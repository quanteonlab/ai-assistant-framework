filename,title,text,len
01-Parallel and High Performance Computing.pdf,01-Parallel and High Performance Computing,"MANNINGRobert Robey\n Yuliana Zamora\nSerial 0.8%\nParallel 99.2%\nParallel and High Performance Computing\nParallel and\nHigh Performance\nComputing\nROBERT  (BOB) ROBEY\nAND YULIANA  (YULIE) ZAMORA\nMANNING\nSHELTER  ISLAND\nFor online information and ordering of this and other Manning books, please visit\nwww.manning.com . The publisher offers discounts on this book when ordered in quantity. \nFor more information, please contact\nSpecial Sales Department\nManning Publications Co.\n20 Baldwin Road\nPO Box 761\nShelter Island, NY 11964\nEmail: orders@manning.com\n©2021 by Manning Publications Co. All rights reserved.\nNo part of this publication may be reproduced, stored in a retrieval system, or transmitted, in \nany form or by means electronic, mechanical, photocopying, or otherwise, without prior written \npermission of the publisher.\nMany of the designations used by manufacturers and sellers to distinguish their products are \nclaimed as trademarks. Where those designations appear in the book, and Manning Publications \nwas aware of a trademark claim, the designations have been printed in initial caps or all caps.\nRecognizing the importance of preserving what has been written, it is Manning’s policy to have \nthe books we publish printed on acid-free paper, and we exert our best efforts to that end. \nRecognizing also our responsibility to conserve the resources of our planet, Manning books\nare printed on paper that is at least 15 percent recycled and processed without the use of \nelemental chlorine.\nDevelopment editor: Marina Michaels\nTechnical development editor: Christopher Haupt\nManning Publications Co. Review editor: Aleksandar Dragosavljevic ´\n20 Baldwin Road Production editor: Deirdre S. Hiam\nPO Box 761 Copy editor: Frances Buran\nShelter Island, NY 11964 Proofreader: Jason Everett\nTechnical proofreader: Tuan A. Tran\nTypesetter: Dennis Dalinnik\nCover designer: Marija Tudor\nISBN: 9781617296468\nPrinted in the United States of America\n To my wife, Peggy, who has supported not only my journey in high performance comput -\ning, but also that of our son Jon and daughter Rachel. Scientific programming is far \nfrom her medical expertise, but she has accompanied me and made it our journey. \nTo my son, Jon, and daughter, Rachel, who have rekindled the flame and for your \npromising future. \n —Bob Robey\n To my husband Rick, who supported me the entire way, thank you for taking the \nearly shifts and letting me work into the night. You never let me give up on myself. \nTo my parents and in-laws, thank you for all your help and support. \nAnd to my son, Derek, for being one of my biggest inspirations; you are the reason \nI leap instead of jump. \n —Yulie Zamora",2729
02-brief contents.pdf,02-brief contents,viibrief contents\nPART 1INTRODUCTION  TO PARALLEL  COMPUTING ................... 1\n1■Why parallel computing? 3\n2■Planning for parallelization 35\n3■Performance limits and profiling 58\n4■Data design and performance models 86\n5■Parallel algorithms and patterns 124\nPART 2CPU: T HE PARALLEL  WORKHORSE ............................. 171\n6■Vectorization: FLOPs for free 175\n7■OpenMP that performs 207\n8■MPI: The parallel backbone 254\nPART 3GPU S: BUILT  TO ACCELERATE ................................... 305\n9■GPU architectures and concepts 309\n10 ■GPU programming model 346\n11 ■Directive-based GPU programming 371\n12 ■GPU languages: Getting down to basics 417\n13 ■GPU profiling and tools 460\nBRIEF CONTENTS viii\nPART 4HIGH PERFORMANCE  COMPUTING  ECOSYSTEMS .......... 489\n14 ■Affinity: Truce with the kernel 491\n15 ■Batch schedulers: Bringing order to chaos 528\n16 ■File operations for a parallel world 547\n17 ■Tools and resources for better code 579,968
03-contents.pdf,03-contents,"ixcontents\npreface xxi\nacknowledgments xxvi\nabout this book xxviii\nabout the authors xxxiii\nPART 1INTRODUCTION  TO PARALLEL  COMPUTING .......... 1\n1 Why parallel computing? 3\n1.1 Why should you learn about parallel computing? 6\nWhat are the potential benefits of parallel computing? 8\nParallel computing cautions 11\n1.2 The fundamental laws of parallel computing 11\nThe limit to parallel computing: Amdahl’s Law 11■Breaking \nthrough the parallel limit: Gustafson-Barsis’s Law 12\n1.3 How does parallel computing work? 15\nWalking through a sample application 16■A hardware model for \ntoday’s heterogeneous parallel systems 22■The application/\nsoftware model for today’s heterogeneous parallel systems 25\n1.4 Categorizing parallel approaches 29\n1.5 Parallel strategies 30\nCONTENTS x\n1.6 Parallel speedup versus comparative speedups: \nTwo different measures 31\n1.7 What will you learn in this book? 32\nAdditional reading 33■Exercises 33\n2 Planning for parallelization 35\n2.1 Approaching a new project: The preparation 36\nVersion control: Creating a safety vault for your parallel code 37\nTest suites: The first step to creating a robust, reliable application 39\nFinding and fixing memory issues 48■Improving code \nportability 50\n2.2 Profiling: Probing the gap between system capabilities and \napplication performance 51\n2.3 Planning: A foundation for success 51\nExploring with benchmarks and mini-apps 52■Design of the core \ndata structures and code modularity 53■Algorithms: Redesign \nfor parallel 53\n2.4 Implementation: Where it all happens 54\n2.5 Commit: Wrapping it up with quality 55\n2.6 Further explorations 55\nAdditional reading 56■Exercises 56\n3 Performance limits and profiling 58\n3.1 Know your application’s potential performance limits 59\n3.2 Determine your hardware capabilities: Benchmarking 62\nTools for gathering system characteristics 62■Calculating \ntheoretical maximum flops 65■The memory hierarchy and \ntheoretical memory bandwidth 66■Empirical measurement of \nbandwidth and flops 67■Calculating the machine balance \nbetween flops and bandwidth 71\n3.3 Characterizing your application: Profiling 71\nProfiling tools 72■Empirical measurement of processor clock \nfrequency and energy consumption 82■Tracking memory \nduring run time 83\n3.4 Further explorations 84\nAdditional reading 84■Exercises 84\nCONTENTS xi\n4 Data design and performance models 86\n4.1 Performance data structures: Data-oriented design 88\nMultidimensional arrays 90■Array of Structures (AoS) versus \nStructures of Arrays (SoA) 94■Array of Structures of Arrays \n(AoSoA) 100\n4.2 Three Cs of cache misses: Compulsory, \ncapacity, conflict 101\n4.3 Simple performance models: A case study 105\nFull matrix data representations 109■Compressed sparse \nstorage representations 112\n4.4 Advanced performance models 116\n4.5 Network messages 119\n4.6 Further explorations 122\nAdditional reading 122■Exercises 123\n5 Parallel algorithms and patterns 124\n5.1 Algorithm analysis for parallel computing \napplications 125\n5.2 Performance models versus algorithmic complexity 126\n5.3 Parallel algorithms: What are they? 130\n5.4 What is a hash function? 131\n5.5 Spatial hashing: A highly-parallel algorithm 132\nUsing perfect hashing for spatial mesh operations 135\nUsing compact hashing for spatial mesh operations 149\n5.6 Prefix sum (scan) pattern and its importance \nin parallel computing 157\nStep-efficient parallel scan operation 158■Work-efficient \nparallel scan operation 159■Parallel scan operations for \nlarge arrays 160\n5.7 Parallel global sum: Addressing the problem \nof associativity 161\n5.8 Future of parallel algorithm research 167\n5.9 Further explorations 167\nAdditional reading 168■Exercises 169\nCONTENTS xii\nPART 2 CPU: T HE PARALLEL  WORKHORSE ...................171\n6 Vectorization: FLOPs for free 175\n6.1 Vectorization and single instruction, multiple data \n(SIMD) overview 176\n6.2 Hardware trends for vectorization 177\n6.3 Vectorization methods 178\nOptimized libraries provide performance for little effort 179■Auto-\nvectorization: The easy way to vectorization speedup (most of the \ntime) 179■Teaching the compiler through hints: Pragmas \nand directives 183■Crappy loops, we got them: Use vector \nintrinsics 190■Not for the faint of heart: Using assembler \ncode for vectorization 195\n6.4 Programming style for better vectorization 196\n6.5 Compiler flags relevant for vectorization for \nvarious compilers 198\n6.6 OpenMP SIMD directives for better portability 203\n6.7 Further explorations 206\nAdditional reading 206■Exercises 206\n7 OpenMP that performs 207\n7.1 OpenMP introduction 208\nOpenMP concepts 208■A simple OpenMP program 211\n7.2 Typical OpenMP use cases: Loop-level, high-level, \nand MPI plus OpenMP 217\nLoop-level OpenMP for quick parallelization 217■High-level \nOpenMP for better parallel performance 218■MPI plus OpenMP \nfor extreme scalability 218\n7.3 Examples of standard loop-level OpenMP 219\nLoop level OpenMP: Vector addition example 220■Stream \ntriad example 222■Loop level OpenMP: Stencil example 224\nPerformance of loop-level examples 226■Reduction example of a \nglobal sum using OpenMP threading 227■Potential loop-level \nOpenMP issues 227\n7.4 Variable scope importance for correctness in \nOpenMP 228\n7.5 Function-level OpenMP: Making a whole function thread \nparallel 229\nCONTENTS xiii\n7.6 Improving parallel scalability with high-level \nOpenMP 231\nHow to implement high-level OpenMP 232■Example of \nimplementing high-level OpenMP 234\n7.7 Hybrid threading and vectorization with OpenMP 237\n7.8 Advanced examples using OpenMP 240\nStencil example with a separate pass for the x and y directions 240\nKahan summation implementation with OpenMP threading 244\nThreaded implementation of the prefix scan algorithm 246\n7.9 Threading tools essential for robust implementations 247\nUsing Allinea/ARM MAP to get a quick high-level profile of your \napplication 248■Finding your thread race conditions with \nIntel® Inspector 248\n7.10 Example of a task-based support algorithm 250\n7.11 Further explorations 251\nAdditional reading 251■Exercises 252\n8 MPI: The parallel backbone 254\n8.1 The basics for an MPI program 255\nBasic MPI function calls for every MPI program 256■Compiler \nwrappers for simpler MPI programs 256■Using parallel startup \ncommands 257■Minimum working example of an MPI \nprogram 257\n8.2 The send and receive commands for process-to-process \ncommunication 259\n8.3 Collective communication: A powerful component \nof MPI 266\nUsing a barrier to synchronize timers 267■Using the broadcast to \nhandle small file input 268■Using a reduction to get a single \nvalue from across all processes 269■Using gather to put order in \ndebug printouts 273■Using scatter and gather to send data out to \nprocesses for work 274\n8.4 Data parallel examples 276\nStream triad to measure bandwidth on the node 276■Ghost cell \nexchanges in a two-dimensional (2D) mesh 277■Ghost cell \nexchanges in a three-dimensional (3D) stencil calculation 285\nCONTENTS xiv\n8.5 Advanced MPI functionality to simplify code \nand enable optimizations 286\nUsing custom MPI data types for performance and code \nsimplification 287■Cartesian topology support in MPI 292\nPerformance tests of ghost cell exchange variants 297\n8.6 Hybrid MPI plus OpenMP for extreme scalability 299\nThe benefits of hybrid MPI plus OpenMP 299■MPI plus \nOpenMP example 300\n8.7 Further explorations 302\nAdditional reading 303■Exercises 303\nPART 3G P U S: BUILT  TO ACCELERATE .........................305\n9 GPU architectures and concepts 309\n9.1 The CPU-GPU system as an accelerated computational \nplatform 311\nIntegrated GPUs: An underused option on commodity-based \nsystems 312■Dedicated GPUs: The workhorse option 313\n9.2 The GPU and the thread engine 313\nThe compute unit is the streaming multiprocessor (or subslice) 316\nProcessing elements are the individual processors 316■Multiple \ndata operations by each processing element 316■Calculating the \npeak theoretical flops for some leading GPUs 316\n9.3 Characteristics of GPU memory spaces 318\nCalculating theoretical peak memory bandwidth 319■Measuring \nthe GPU stream benchmark 321■Roofline performance model for \nGPUs 322■Using the mixbench performance tool to choose the best \nGPU for a workload 324\n9.4 The PCI bus: CPU to GPU data transfer overhead 326\nTheoretical bandwidth of the PCI bus 326■A benchmark \napplication for PCI bandwidth 329\n9.5 Multi-GPU platforms and MPI 332\nOptimizing the data movement between GPUs across the network 333\nA higher performance alternative to the PCI bus 334\n9.6 Potential benefits of GPU-accelerated platforms 334\nReducing time-to-solution 335■Reducing energy use with \nGPUs 337■Reduction in cloud computing costs with \nGPUs 342\nCONTENTS xv\n9.7 When to use GPUs 343\n9.8 Further explorations 343\nAdditional reading 343■Exercises 344\n10 GPU programming model 346\n10.1 GPU programming abstractions: A common \nframework 348\nMassive parallelism 348■Inability to coordinate among tasks 349\nTerminology for GPU parallelism 349■Data decomposition into \nindependent units of work: An NDRange or grid 350■Work groups \nprovide a right-sized chunk of work 353■Subgroups, warps, or \nwavefronts execute in lockstep 353■Work item: The basic unit of \noperation 354■SIMD or vector hardware 354\n10.2 The code structure for the GPU programming model 355\n“Me” programming: The concept of a parallel kernel 356\nThread indices: Mapping the local tile to the global world 357\nIndex sets 358■How to address memory resources in your GPU \nprogramming model 359\n10.3 Optimizing GPU resource usage 361\nHow many registers does my kernel use? 361■Occupancy: \nMaking more work available for work group scheduling 362\n10.4 Reduction pattern requires synchronization across work \ngroups 364\n10.5 Asynchronous computing through queues (streams) 365\n10.6 Developing a plan to parallelize an application \nfor GPUs 366\nCase 1: 3D atmospheric simulation 367■Case 2: Unstructured \nmesh application 368\n10.7 Further explorations 368\nAdditional reading 369■Exercises 370\n11 Directive-based GPU programming 371\n11.1 Process to apply directives and pragmas \nfor a GPU implementation 373\n11.2 OpenACC: The easiest way to run on your GPU 374\nCompiling OpenACC code 375■Parallel compute regions in \nOpenACC for accelerating computations 377■Using directives \nto reduce data movement between the CPU and the GPU 383\nOptimizing the GPU kernels 387■Summary of performance \nCONTENTS xvi\nresults for the stream triad 393■Advanced OpenACC \ntechniques 393\n11.3 OpenMP: The heavyweight champ enters the world of \naccelerators 396\nCompiling OpenMP code 397■Generating parallel work on the \nGPU with OpenMP 398■Creating data regions to control data \nmovement to the GPU with OpenMP 402■Optimizing OpenMP \nfor GPUs 406■Advanced OpenMP for GPUs 411\n11.4 Further explorations 414\nAdditional reading 414■Exercises 416\n12 GPU languages: Getting down to basics 417\n12.1 Features of a native GPU programming language 419\n12.2 CUDA and HIP GPU languages: The low-level \nperformance option 420\nWriting and building your first CUDA application 421\nA reduction kernel in CUDA: Life gets complicated 429\nHipifying the CUDA code 435\n12.3 OpenCL for a portable open source GPU language 438\nWriting and building your first OpenCL application 439\nReductions in OpenCL 445\n12.4 SYCL: An experimental C++ implementation \ngoes mainstream 449\n12.5 Higher-level languages for performance portability 452\nKokkos: A performance portability ecosystem 452■RAJA for a \nmore adaptable performance portability layer 455\n12.6 Further explorations 457\nAdditional reading 457■Exercises 458\n13 GPU profiling and tools 460\n13.1 An overview of profiling tools 460\n13.2 How to select a good workflow 462\n13.3 Example problem: Shallow water simulation 463\n13.4 A sample of a profiling workflow 467\nRun the shallow water application 467■Profile the CPU code to \ndevelop a plan of action 470■Add OpenACC compute directives \nto begin the implementation step 471■Add data movement \ndirectives 473■Guided analysis can give you some suggested \nCONTENTS xvii\nimprovements 474■The NVIDIA Nsight suite of tools can be a \npowerful development aid 476■CodeXL for the AMD GPU \necosystem 478\n13.5 Don’t get lost in the swamp: Focus on the important \nmetrics 479\nOccupancy: Is there enough work? 479■Issue efficiency: Are your \nwarps on break too often? 480■Achieved bandwidth: It always \ncomes down to bandwidth 480\n13.6 Containers and virtual machines provide alternate \nworkflows 480\nDocker containers as a workaround 480■Virtual machines using \nVirtualBox 483\n13.7 Cloud options: A flexible and portable capability 485\n13.8 Further explorations 486\nAdditional reading 486■Exercises 487\nPART 4H IGH PERFORMANCE  COMPUTING  ECOSYSTEMS ...489\n14 Affinity: Truce with the kernel 491\n14.1 Why is affinity important? 492\n14.2 Discovering your architecture 493\n14.3 Thread affinity with OpenMP 495\n14.4 Process affinity with MPI 503\nDefault process placement with OpenMPI 504■Taking control: \nBasic techniques for specifying process placement in OpenMPI 504\nAffinity is more than just process binding: The full picture 509\n14.5 Affinity for MPI plus OpenMP 511\n14.6 Controlling affinity from the command line 516\nUsing hwloc-bind to assign affinity 516■Using likwid-pin: An \naffinity tool in the likwid tool suite 518\n14.7 The future: Setting and changing affinity at run time 520\nSetting affinities in your executable 521■Changing your process \naffinities during run time 522\n14.8 Further explorations 525\nAdditional reading 525■Exercises 526\nCONTENTS xviii\n15 Batch schedulers: Bringing order to chaos 528\n15.1 The chaos of an unmanaged system 529\n15.2 How not to be a nuisance when working on a busy \ncluster 530\nLayout of a batch system for busy clusters 530■How to be \ncourteous on busy clusters and HPC sites: Common HPC pet \npeeves 531\n15.3 Submitting your first batch script 532\n15.4 Automatic restarts for long-running jobs 539\n15.5 Specifying dependencies in batch scripts 543\n15.6 Further explorations 545\nAdditional reading 545■Exercises 546\n16 File operations for a parallel world 547\n16.1 The components of a high-performance filesystem 548\n16.2 Standard file operations: A parallel-to-serial \ninterface 549\n16.3 MPI file operations (MPI-IO) for a more parallel \nworld 551\n16.4 HDF5 is self-describing for better data management 559\n16.5 Other parallel file software packages 568\n16.6 Parallel filesystem: The hardware interface 568\nEverything you wanted to know about your parallel file setup but \ndidn’t know how to ask 568■General hints that apply to all \nfilesystems 572■Hints specific to particular filesystems 574\n16.7 Further explorations 577\nAdditional reading 577■Exercises 578\n17 Tools and resources for better code 579\n17.1 Version control systems: It all begins here 582\nDistributed version control fits the more mobile world 582\nCentralized version control for simplicity and code security 583\n17.2 Timer routines for tracking code performance 583\n17.3 Profilers: You can’t improve what you don’t measure 585\nSimple text-based profilers for everyday use 586■High-level \nprofilers for quickly identifying bottlenecks 587■Medium-level \nprofilers to guide your application development 588■Detailed \nprofilers give the gory details of hardware performance 590\nCONTENTS xix\n17.4 Benchmarks and mini-apps: A window into system \nperformance 591\nBenchmarks measure system performance characteristics 591\nMini-apps give the application perspective 591\n17.5 Detecting (and fixing) memory errors for a robust \napplication 594\nValgrind Memcheck: The open source standby 594■Dr. Memory \nfor your memory ailments 595■Commercial memory tools for \ndemanding applications 597■Compiler-based memory tools for \nconvenience 597■Fence-post checkers detect out-of-bounds memory \naccesses 597■GPU memory tools for robust GPU \napplications 599\n17.6 Thread checkers for detecting race conditions 599\nIntel® Inspector: A race condition detection tool with a GUI 600\nArcher: A text-based tool for detecting race conditions 600\n17.7 Bug-busters: Debuggers to exterminate those bugs 602\nTotalView debugger is widely available at HPC sites 602\nDDT is another debugger widely available at HPC sites 602\nLinux debuggers: Free alternatives for your local development \nneeds 603■GPU debuggers can help crush those GPU bugs 603\n17.8 Profiling those file operations 604\n17.9 Package managers: Your personal system administrator 607\nPackage managers for macOS 607■Package managers for \nWindows 608■The Spack package manager: A package manager \nfor high performance computing 608\n17.10 Modules: Loading specialized toolchains 609\nTCL modules: The original modules system for loading software \ntoolchains 612■Lmod: A Lua-based alternative Modules \nimplementation 612\n17.11 Reflections and exercises 612\nappendix A References 614\nappendix B Solutions to exercises 619\nappendix C Glossary 641\nindex 655",17182
04-preface.pdf,04-preface,,0
05-From the authors.pdf,05-From the authors,,0
06-Bob Robey Los Alamos New Mexico.pdf,06-Bob Robey Los Alamos New Mexico,"xxipreface\nFrom the authors\nBob Robey, Los Alamos, New Mexico\nIt's a dangerous business, Frodo, going out your door. You step onto the road, and\nif you don't keep your feet, there's no knowing where you might be swept off to. \n— Bilbo Baggins\nI could not have foreseen where this journey into parallel computing would take us.\n“Us” because the journey has been shared by numerous colleagues over the years. My\njourney into parallel computing began in the early 1990s, while I was at the University\nof New Mexico. I had written some compressible fluid dynamics codes to model shock\ntube experiments and was running these on every system I could get my hands on. As\na result, I along with Brian Smith, John Sobolewski, and Frank Gilfeather, was asked to\nsubmit a proposal for a high performance computing center. We won the grant and\nestablished the Maui High Performance Computing Center in 1993. My part in the\nproject was to offer courses and lead 20 graduate students in developing parallel com-\nputing at the University of New Mexico in Albuquerque. \n The 1990s were a formative time for parallel computing. I remember a talk by Al\nGeist, one of the original developers of Parallel Virtual Machine (PVM) and a mem-\nber of the MPI standards committee. He talked about the soon-to-be released MPI\nstandard (June, 1994). He said it would never go anywhere because it was too com-\nplex. Al was right about the complexity, but despite that, it took off, and within\nPREFACE xxii\nmonths it was used by nearly every parallel application. One of the reasons for the suc-\ncess of MPI is that there were implementations ready to go. Argonne had been devel-\noping Chameleon, a portability tool that would translate between the message-passing\nlanguages at that time, including P4, PVM, MPL, and many others. The project was\nquickly changed to MPICH, which became the first high-quality MPI implementation.\nFor over a decade, MPI became synonymous with parallel computing. Nearly every\nparallel application was built on top of MPI libraries.\n Now let’s fast forward to 2010 and the emergence of GPUs. I came across a Dr.\nDobb’s article on using a Kahan sum to compensate for the only single-precision arith-\nmetic available on the GPU. I thought that maybe the approach could help resolve a\nlong-standing issue in parallel computing, where the global sum of an array changes\ndepending on the number of processors. To test this out, I thought of a fluid dynamics\ncode that my son Jon wrote in high school. He tested the mass and energy conserva-\ntion in the problem over time and would stop running and exit the program if it\nchanged more than a specified amount. While he was home over Spring break from\nhis freshman year at University of Washington, we tried out the method and were\npleasantly surprised by how much the mass conservation improved. For production\ncodes, the impact of this simple technique would prove to be important. We cover the\nenhanced precision sum algorithm for parallel global sums in section 5.7 in this book.\n In 2011, I organized a summer project with three students, Neal Davis, David Nich-\nolaeff, and Dennis Trujillo, to see if we could get more complex codes like adaptive\nmesh refinement (AMR) and unstructured arbitrary Lagrangian-Eulerian (ALE)\napplications to run on a GPU. The result was CLAMR, an AMR mini-app that ran\nentirely on a GPU. Much of the application was easy to port. The most difficult part\nwas determining the neighbor for each cell. The original CPU code used a k-d tree\nalgorithm, but tree-based algorithms are difficult to port to GPUs. Two weeks into the\nsummer project, the Las Conchas Fire erupted in the hills above Los Alamos and the\ntown was evacuated. We left for Santa Fe, and the students scattered. During the evac-\nuation, I met with David Nicholaeff in downtown Santa Fe to discuss the GPU port. He\nsuggested that we try using a hash algorithm to replace the tree-based code for the\nneighbor finding. At the time, I was watching the fire burning above the town and\nwondering if it had reached my house. In spite of that, I agreed to try it, and the hash-\ning algorithm resulted in getting the entire code running on the GPU. The hashing\ntechnique was generalized by David, my daughter Rachel while she was in high school,\nand myself. These hash algorithms form the basis for many of the algorithms pre-\nsented in chapter 5.\n In following years, the compact hashing techniques were developed by Rebecka\nTumblin, Peter Ahrens, and Sara Hartse. The more difficult problem of compact\nhashing for remapping operations on the CPU and GPU was tackled by Gerald Col-\nlom and Colin Redman when they were just out of high school. With these break-\nthroughs in parallel algorithms for the GPU, the barriers to getting many scientific\napplications running on the GPU were toppling.",4889
07-How we came to write this book.pdf,07-How we came to write this book,"PREFACE xxiii\n In 2016, I started the Los Alamos National Laboratory (LANL) Parallel Computing\nSummer Research Internship (PCSRI) program along with my co-founders, Hai Ah\nNam and Gabe Rockefeller. The goal of the parallel computing program was to\naddress the growing complexity of high-performance computing systems. The pro-\ngram is a 10-week summer internship with lectures on various parallel computing top-\nics, followed by a research project mentored by staff from Los Alamos National\nLaboratory. We have had anywhere from 12 to 18 students participating in the sum-\nmer program, and many have used it as a springboard for their careers. Through this\nprogram, we continue to tackle some of the newest challenges facing parallel and\nhigh performance computing.\nYulie Zamora, University of Chicago, Illinois\nIf there’s a book that you want to read, but it hasn’t been written yet, then you\nmust write it. \n—Toni Morrison\nMy introduction to parallel computing began with, “Before you start, go into the\nroom at the end of the 4th floor and install those Knights Corner processors in our\ncluster.” This request from a professor at Cornell University encouraged me to try\nsomething new. What I thought would be a simple endeavor turned into a tumultuous\njourney into high performance computing. I started with learning the basics of how a\nsmall cluster worked through physically lifting 40-lb servers to working with the BIOS\nand running my first application, and then optimizing these applications across the\nnodes I installed. \n After a short family break, daunting as it was, I applied for a research internship.\nBeing accepted into the first Parallel Computing Summer Research Internship pro-\ngram in New Mexico gave me the opportunity to explore the intricacies of parallel\ncomputing on today’s hardware and that is where I met Bob. I became enthralled with\nthe gains in performance that were possible with just some knowledge of how to prop-\nerly write parallel code. I personally explored how to write more effective OpenMP\ncode. My excitement and progress in optimization of applications opened the door to\nother opportunities, such as attending conferences and presenting my work at the\nIntel User’s Group meeting and at the Intel booth at Supercomputing. I was also\ninvited to attend and present at the 2017 Salishan Conference. That was a great\nopportunity to exchange ideas with some of the leading visionaries of high perfor-\nmance computing.\n Another great experience was applying for and participating in a GPU hackathon.\nAt the hackathon, we ported a code over to OpenACC and, within a week, the code\nachieved a speedup by a factor of 60. Think of this—a calculation that previously took\na month could now be done overnight. Fully diving into the potential of long-term\nresearch, I applied to graduate schools and chose University of Chicago, acknowledging\nPREFACE xxiv\nits close relationship with Argonne National Laboratory. At the University of Chicago,\nI was advised by Ian Foster and Henry Hoffmann. \n From my experiences, I realized how valuable personal interactions are to learning\nhow to write parallel code. I also was frustrated by the lack of a textbook or reference\nthat discusses the current hardware. To fill this gap, we have written this book to make\nit much easier for those new to parallel and high performance computing. Taking on\nthe challenge of creating and teaching an introduction to computer science for\nincoming University of Chicago students helped me gain an understanding of those\nnew to the field. On the other hand, explaining the parallel programming techniques\nas a teaching assistant in the Advanced Distributed Systems course allowed me to work\nwith students with a higher level of understanding. Both these experiences helped me\nto attain the ability to explain complex topics at different levels.\n I believe that everyone should have the opportunity to learn this important mate-\nrial on writing performant code and that it should be easily accessible to everyone. I\nwas lucky enough to have mentors and advisors that steered me to the right website\nlinks or hand me their old manuscripts to read and learn. Though some of the tech-\nniques can be difficult, the greater problem is the lack of a coherent documentation\nor access to leading scientists in the field as mentors. I understand not everyone has\nthe same resources and, therefore, I hope that creating this book fills a void that cur-\nrently exists.\nHow we came to write this book\nBeginning in 2016, a team of LANL scientists led by Bob Robey developed lecture\nmaterials for the Los Alamos National Laboratory (LANL) Parallel Computing Sum-\nmer Research Internships (PCSRI). Much of this material addressed the latest hard-\nware that is quickly coming to market. Parallel computing is changing at a rapid rate\nand there is little documentation to accompany it. A book covering the materials was\nclearly needed. It was at this point that Manning Publications contacted Bob about\nwriting a book on parallel computing. We had a rough draft of the materials, so how\nhard could this be? Thus began a two-year effort to put it all into a high quality format.\n The topics and chapter outline were well-defined at an early stage, based on the\nlectures for our summer program. Many of the ideas and techniques are drawn from\nthe greater high performance computing community as we strive towards an exascale\nlevel of computing—a thousand-fold improvement in computational performance\nover the previous Petascale milestone. This community includes the Department of\nEnergy (DOE) Centers of Excellence, the Exascale Computing Project, and a series of\nPerformance, Portability, and Productivity workshops. The breadth and depth of the\nmaterials in our computing lectures reflect the deep challenges of the complex het-\nerogeneous computing architectures. \n We call the material in this book “an introduction with depth.” It starts at the basics\nof parallel and high-performance computing, but without gaining knowledge of the\ncomputing architecture, it is not possible to achieve optimal performance. We try to\nPREFACE xxv\ngive an insight into a deeper level of understanding as we go along because it is not\nenough to just travel along the trail without any idea of where you are or where you\nare going. We provide the tools to develop a map and to show how far the distance is\nto the goal that we are striving towards.\n At the outset of this book, Joe Schoonover was tapped to write the GPU materials\nand Yulie Zamora the OpenMP chapter. Joe provided the design and layout of the\nGPU sections, but had to quickly drop out. Yulie has written papers and given many\npresentations on how OpenMP fit into this brave new world of exascale computing, so\nthis material was especially well-suited for the OpenMP chapter of the book. Yulie’s\ndeep understanding of the challenges of exascale computing and her ability to break\nit down for newcomers to the field has been a critical contribution to the creation of\nthis book.",7110
08-acknowledgments.pdf,08-acknowledgments,"xxviacknowledgments\nWe’d like to thank all who helped shape this book. First on our list is Joe Schoonover\nof Fluid Dynamics, who has gone above and beyond in teaching parallel computing,\nparticularly with GPUs. Joe was one of the co-leads of our parallel computing program\nand instrumental in formulating what the book should cover. Our other co-leads, Hai\nAh Nam, Gabe Rockefeller, Kris Garrett, Eunmo Koo, Luke Van Roekel, Robert Bird,\nJonas Lippuner, and Matt Turner, have contributed to the success of the parallel com-\nputing school and its content. The founding of the parallel computing summer pro-\ngram would not have occurred without the support and vision of the Institute\nDirector, Stephan Eidenbenz. Also thanks to Scott Runnels and Daniel Israel, who\nhave led the LANL Computational Physics summer school and pioneered the school’s\nconcept, giving us a model to follow.\n We are fortunate to be surrounded by experts in parallel computing and book\npublishing. Thanks to Kate Bowman, whose expertise on writing helped guide the\nrevisions of the early chapters. Kate is incredibly talented in all aspects of publishing\nand has been a book indexer for many years. We have also had informal reviews from\nBob’s son, Jon; daughter, Rachel; and son-in-law, Bob Bird, each of whom have some\nof their technical work mentioned in the book. Yulie’s husband, Rick, helped provide\nexpertise with some topics, and Dov Shlachter reviewed some early drafts and pro-\nvided some helpful feedback.\n We’d also like to acknowledge expertise from collaborators who found their way\ninto specific chapters. This includes Rao Garimella and Shane Fogerty of Los Alamos\nand Matt Martineau of Lawrence Livermore National Laboratory whose work is incor-\nACKNOWLEDGMENTS xxvii\nporated in chapter 4. A special thanks goes to the innovative work of the many stu-\ndents mentioned earlier whose work fills much of chapter 5. Ron Green of Intel has\nfor some years led the effort to document how to use the vectorization provided by the\nIntel compiler, forming the basis for chapter 6. The tsunami simulation in chapter 13\noriginated from the McCurdy High School team composed of Sarah Armstrong,\nJoseph Koby, Juan-Antonio Vigil, and Vanessa Trujillo, participating in the New Mex-\nico Supercomputing Challenge in 2007. Also, thanks to Cristian Gomez for helping\nwith the tsunami illustration. Work on process placement and affinity with Doug\nJacobsen of Intel and Hai Ah Nam and Sam Gutiérrez of Los Alamos National Labora-\ntory laid the foundation for chapter 14. Also, work with the Datalib team of Galen\nShipman and Brad Settlemyer of Los Alamos National Laboratory, Rob Ross, Rob\nLatham, Phil Carns, Shane Snyder of Argonne National Laboratory, and Wei-Keng\nLiao of Northwestern University is reflected in chapter 16 and the section on the Dar-\nshan tool for profiling file operations in chapter 17. \n We also appreciate the efforts of the Manning Publications professionals in creat-\ning a more polished and professional product. Our copy editor, Frances Buran, did a\nremarkable job improving the writing and making it more readable. She handled the\nhighly technical and precise language and did it at an amazing pace. Also thanks to\nDeirdre Hiam, our production editor, for transforming the graphics, formulas, and\ntext into a polished product for our readers. We would also like to thank Jason Everett,\nour proofreader. Paul Wells, the book’s Production Manager, kept all of this effort on\na tight schedule.\n Manning Publications incorporates numerous reviews into the writing process,\nincluding writing style, copyediting, proofreading, and technical content. First is the\nManning Acquisitions Editor, Mike Stephens, who saw the need for a book on this\ntopic. Our Development Editor, Marina Michaels, helped keep us on track for this\nhuge effort. Marina was especially helpful in making the material more accessible to a\ngeneral audience. Christopher Haupt as the Technical Development Editor gave us\nvaluable feedback on the technical content. We especially thank Tuan Tran, our Tech-\nnical Proofer, who reviewed the source code for all the examples. Tuan did a great job\ntackling the difficulties of handling the challenge of high-performance computing\nsoftware and hardware configurations. Our review editor, Aleksandar Dragosavljevic ´,\nrecruited a great set of reviewers that spanned a broad cross-section of readers. These\nreviewers, Alain Couniot, Albert Choy, Alessandro Campeis, Angelo Costa, Arav Kapish\nAgarwal, Dana Robinson, Domingo Salazar, Hugo Durana, Jean-François Morin, Patrick\nRegan, Phillip G Bradford, Richard Tobias, Rob Kielty, Srdjan Santic, Tuan A. Tran,\nand Vincent Douwe gave us valuable feedback which substantially improved the final\nproduct.",4819
09-about this book.pdf,09-about this book,,0
10-How this book is organized A roadmap.pdf,10-How this book is organized A roadmap,"xxviiiabout this book\nOne of the most important tasks for an explorer is to draw a map for those who follow.\nThis is especially true for those of us pushing the boundaries of science and technol-\nogy. Our goal in this book is to provide a roadmap for those just starting to learn\nabout parallel and high performance computing and for those who want to broaden\ntheir knowledge of the field. High performance computing is a rapidly changing\nfield, where languages and technologies are constantly in flux. For this reason, we’ll\nfocus on the fundamentals that stay steady over time. For the computer languages for\nCPUs and GPUs, we stress the common patterns across the many languages, so that\nyou can quickly select the most appropriate language for your current task.\nWho should read this book\nThis book is targeted at both upper division undergraduate parallel computing classes\nand as state-of-the-art literature for computing professionals. If you are interested in per-\nformance, whether it be run time, scale, or power, this book will give you the tools to\nimprove your application and outperform your competition. With processors reaching\nthe limits of scale, heat, and power, we cannot count on the next generation computer\nto speed up our applications. Increasingly, highly skilled and knowledgeable program-\nmers are critical for getting maximum performance from today’s applications. \n In this book, we hope to get across key ideas true for today’s high performance\ncomputing hardware. These are the basic truths of programming for performance.\nThese themes underlie the entire book. \nIn high performance computing, it is not how fast you write the code, it is how\nfast the code you write runs.\nABOUT THIS BOOK xxix\nThis one thought sums up what it means to write applications for high performance\ncomputing. For most other applications, the focus is on how fast you can write an\napplication. Today, computer languages are typically designed to promote quicker\nprogramming rather than better performing code. Although this programming\napproach has long infused high performing computing applications, it has not been\nwidely documented or described. In chapter 4, we discuss this different focus in a pro-\ngramming methodology that has recently been coined as data-oriented design.\nIt is all about memory: how much you use and how often you load it.\nEven when you know that available memory and memory operations are almost always\nthe limiting factor in performance, we still tend to spend a lot of time thinking about\nfloating-point operations. With most current computing hardware capable of 50 float-\ning-point operations for every memory load, floating-point operations are a secondary\nconcern. In almost every chapter, we use our implementation of the STREAM bench-\nmark, a memory performance test, to verify that we are getting reasonable perfor-\nmance from the hardware and programming language.\nIf you load one value, you get eight or sixteen.\nIt’s like buying eggs. You can’t get just one. Memory loads are done by cache lines of\n512 bits. For a double-precision value of 8 bytes, eight values will be loaded whether\nyou want them or not. Plan your program to use more than one value, and preferably\neight contiguous values, for best performance. And while you are at it, use the rest of\nthe eggs.\nIf there are any flaws in your code, parallelization will expose them.\nCode quality requires more attention in high performance computing than a compa-\nrable serial application. This applies to before beginning parallelization, during paral-\nlelization, and after parallelization. With parallelization, you are more likely to trigger\na flaw in your program and will also find debugging more challenging, especially at\nlarge scale. We introduce the techniques for improving software quality in chapter 2,\nthen throughout the chapters we mention important tools, and finally, in chapter 17,\nwe list other tools that can prove valuable. \n These key themes transcend hardware types applying equally to all CPUs and GPUs.\nThese exist because of the current physical constraints imposed on the hardware.\nHow this book is organized: A roadmap\nThis book does not expect that you have any knowledge of parallel programming. It\ndoes expect that readers are proficient programmers, preferably in a compiled, high\nperformance computing language such as C, C++, or Fortran. It is also expected that\nreaders have some knowledge of computing terminology, operating system basics, and\nnetworking. Readers should also be able to find their way around their computer,\nincluding installing software and light system administration tasks. \nABOUT THIS BOOK xxx\n The knowledge of computing hardware is perhaps the most important require-\nment for readers. We recommend opening up your computer, looking at each compo-\nnent, and getting an understanding of its physical characteristics. If you cannot open\nup your computer, see the photos of a typical desktop system at the end of appendix\nC. For example, look at the bottom of the CPU in figure C.2 and at the forest of pins\ngoing into the chip. Can you fit any more pins there? Now you can see why there is a\nphysical limit to how much data can be transferred to the CPU from other parts of the\nsystem. Flip back to these photos and the glossary in appendix A when you need to\nhave a better understanding of the computing hardware or computing terminology.\n We have divided this book into four parts that comprise the world of high perfor-\nmance computing. These are\n■Part 1: Introduction to parallel computing (chapters 1–5)\n■Part 2: Central processing unit (CPU) technologies (chapters 6–8)\n■Part 3: Graphics processing unit (GPU) technologies (chapters 9–13\n■Part 4: High performance computing (HPC) ecosystems (chapters 14–17)\nThe order of topics is oriented towards someone tackling a high performance comput-\ning project. For example, for an application project, the software engineering topics in\nchapter 2 are necessary before starting a project. Once the software engineering is in\nplace, the next decisions are the data structures and algorithms. Then come the imple-\nmentations for the CPU and GPU. Finally, the application is adapted for the parallel file\nsystem and other unique characteristics of a high performance computing system.\n On the other hand, some of our readers are more interested in gaining fundamen-\ntal skills in parallel programming and might want to go directly into the MPI or\nOpenMP chapters. But don’t stop there. Today, there is so much more to parallel\ncomputing. From GPUs that can speed up your application another order of magni-\ntude to tools that can improve your code quality or point out sections of code to opti-\nmize—the potential gains are only limited by your time and expertise. \n If you are using this book for a class on parallel computing, the scope of the material\nis sufficient for at least two semesters. You might think of the book as a buffet of materi-\nals that can be individualized to the audience. By selecting the topics to cover, you can\ncustomize it for your own course objectives. Here is a possible sequence of material:\n■Chapter 1 provides an introduction to parallel computing\n■Chapter 3 approaches measuring hardware and application performance\n■Sections 4.1–4.2 describe the data-oriented design concept of programming,\nmulti-dimensional arrays, and cache basics\n■Chapter 7 covers OpenMP (Open Multi-Processing) to get on-node parallelism\n■Chapter 8 covers MPI (Message Passing Interface) to get distributed parallelism\nacross multiple nodes\n■Sections 14.1–14.5 introduce affinity and process placement concepts\n■Chapters 9 and 10 describe GPU hardware and programming models\n■Sections 11.1–11.2 focus on OpenACC to get applications running on the GPU",7868
11-About the code.pdf,11-About the code,,0
12-liveBook discussion forum.pdf,12-liveBook discussion forum,"ABOUT THIS BOOK xxxi\nYou can add topics such as algorithms, vectorization, parallel file handling, or more\nGPU languages to this list. Or you can remove a topic so that you can spend more time\non the remaining topics. There still are additional chapters to tempt students to con-\ntinue to explore the world of parallel computing on their own.\nAbout the code\nYou cannot learn parallel computing without actually writing code and running it. For\nthis purpose, we provide a large set of examples that accompanies the book. The\nexamples are freely available at https:/ /github.com/EssentialsOfParallelComputing .\nYou can download these examples either as a complete set or individually by chapter. \n With the scope of the examples, hardware, and software, there will inevitably be\nflaws and errors in the accompanying examples. If you find something that is in error\nor just not complete, we encourage contributions to the examples. We have already\nmerged in some change requests from readers, which were greatly appreciated. Addi-\ntionally, the source code repository will be the best place to look for corrections and\nsource code discussions.\nSoftware/hardware requirements\nPerhaps the biggest challenge in parallel and high performance computing is the\nwide range of hardware and software that is involved. In the past, these specialized sys-\ntems were only available at specific sites. Recently, the hardware and software has\nbecome more democratized and widely available even at the desktop or laptop level.\nThis is a substantial shift that can make software for high performance computing\nmuch easier to develop. However, the setup of the hardware and software environ-\nment is the most difficult part of the task. If you have access to a parallel computing\ncluster where these are already set up, we encourage you to take advantage of it. Even-\ntually, you may want to set up your own system. The examples are easiest to use on a\nLinux or Unix system, but should also work on Windows and the MacOS in many\ncases with some additional effort. We have provided alternatives with Docker con-\ntainer templates and VirtualBox setup scripts when you find that the example doesn’t\nrun on your system. \n The GPU exercises require GPUs from the different vendors, including NVIDIA,\nAMD Radeon, and Intel. Anyone who has struggled to get GPU graphics drivers\ninstalled on their system will not be surprised that these present the greatest difficulty\nin setting up your local system for the examples. Some of the GPU languages also can\nwork on the CPU, allowing the development of code on a local system for hardware\nthat you do not have. You may also find that debugging on the CPU is easier. But to\nsee the actual performance, you will have to have the actual GPU hardware.\n Other examples requiring special installation include the batch system and the\nparallel file examples. A batch system requires more than a single laptop or worksta-\ntion to set it up to look like a real installation. Similarly, the parallel file examples work\nbest with a specialized filesystem like Lustre, though the basic examples will work on a\nlaptop or workstation.",3180
13-Other online resources.pdf,13-Other online resources,,0
14-about the authors.pdf,14-about the authors,"ABOUT THIS BOOK xxxii\nliveBook discussion forum\nPurchase of Parallel and High Performance Computing  includes free access to a private\nweb forum run by Manning Publications where you can make comments about the\nbook, ask technical questions, and receive help from the authors and from other\nusers. To access the forum, go to https:/ /livebook.manning.com/#!/book/parallel-\nand-high-performance-computing/discussion . You can also learn more about Man-\nning’s forums and the rules of conduct at https:/ /livebook.manning.com/#!/discussion .\nManning’s commitment to our readers is to provide a venue where a meaningful dia-\nlogue between individual readers and between readers and the authors can take place.\nIt is not a commitment to any specific amount of participation on the part of the\nauthors, whose contribution to the forum remains voluntary (and unpaid). We sug-\ngest you try asking the authors some challenging questions lest their interest stray!\nThe forum and the archives of previous discussions will be accessible from the pub-\nlisher’s website as long as the book is in print.\nOther online resources\nManning Publications also provides an online discussion forum called livebook for\neach book. Our site is at https:/ /livebook.manning.com/book/parallel-and-high-per-\nformance-computing . This is a good place to add comments or expand on the materi-\nals in the chapters. \nAbout the cover illustration\nThe figure on the cover of Parallel and High Performance Computing  is captioned “M’de\nde brosses à Vienne,” or  “Seller of brushes in Vienna.” The illustration is taken from a\ncollection of dress costumes from various countries by Jacques Grasset de Saint-Sau-\nveur (1757-1810), titled Costumes de Différents Pays , published in France in 1797. Each\nillustration is finely drawn and colored by hand. The rich variety of Grasset de Saint-\nSauveur’s collection reminds us vividly of how culturally apart the world’s towns and\nregions were just 200 years ago. Isolated from each other, people spoke different dia-\nlects and languages. In the streets or in the countryside, it was easy to identify where\nthey lived and what their trade or station in life was just by their dress.\n The way we dress has changed since then and the diversity by region, so rich at the\ntime, has faded away. It is now hard to tell apart the inhabitants of different conti-\nnents, let alone different towns, regions, or countries. Perhaps we have traded cultural\ndiversity for a more varied personal life—certainly for a more varied and fast-paced\ntechnological life.\n At a time when it is hard to tell one computer book from another, Manning cele-\nbrates the inventiveness and initiative of the computer business with book covers\nbased on the rich diversity of regional life of two centuries ago, brought back to life by\nGrasset de Saint-Sauveur’s pictures.\nxxxiiiabout the authors\nRobert (Bob) Robey is a technical staff scientist in the Computational Physics Division at\nLos Alamos National Laboratory and an adjunct researcher at the University of New\nMexico. He is a founder of the Parallel Computing Summer Research Internships\nstarted in 2016. He is a member of the NSF/IEEE-TCPP Curriculum Initiative on Par-\nallel and Distributed Computing. Bob is a board member of the New Mexico Super-\ncomputing Challenge, a high school and middle school educational program in its\n30th year. He has mentored hundreds of students over the years and has twice been\nrecognized as a Los Alamos Distinguished Student Mentor. Bob co-taught a parallel\ncomputing class at University of New Mexico and has given guest lectures at other\nuniversities. \n Bob began his scientific career by operating explosive-driven and compressible\ngas-driven shock tubes at the University of New Mexico. This includes the largest\nexplosively-driven shock tube in the world at 20 feet in diameter and over 800 feet\nlong. He conducted hundreds of experiments with explosions and shock waves. To\nsupport his experimental work, Bob has written several compressible fluid dynamics\ncodes since the early 1990s and has authored many articles in international journals\nand publications. Full 3D simulations were a rarity at the time, stressing compute\nresources to the limit. The search for more compute resources led to his involvement\nin high performance computing research.\n B o b wo rke d  12  ye ars  at  t he  Uni ver s it y o f  Ne w Me xi co  co nd uct ing  e xp er im en ts ,\nwriting, and running compressible fluid dynamics simulations, and started a high per-\nformance computing center. He was a lead proposal writer and brought tens of millions\nABOUT THE AUTHORS xxxiv\nof dollars of research grants to the university. Since 1998, he has held a position at the\nLos Alamos National Laboratory. While there, he contributed to large multi-physics\ncodes running on a variety of the latest hardware.\n Bob is a world-class kayaker with first descents down previously unrun rivers in\nMexico and New Mexico. He is also a mountaineer with ascents of peaks on three con-\ntinents up to over 18,000 feet in elevation. He is a leader in the co-ed Los Alamos Ven-\nture crew and helps out with multi-day trips down western rivers. \n Bob is a graduate of Texas A&M University with a Masters in Business Administra-\ntion and a Bachelors degree in Mechanical Engineering. He has taken graduate\ncoursework at University of New Mexico in the Mathematics Department.\n \nYuliana (Yulie) Zamora  is completing her PhD in Computer Science at the University of\nChicago. Yulie is a 2017 fellow at the CERES Center of Unstoppable Computing at the\nUniversity of Chicago and a National Physical Science Consortium (NPSC) graduate\nfellow.\n Yulie has worked at the Los Alamos National Laboratory and interned at Argonne\nNational Laboratory. At Los Alamos National Laboratory, she optimized the Higrad\nFiretec code used for simulating wildland fires and other atmospheric physics for\nsome of the top high performance computing systems. At Argonne National Labora-\ntory, she worked at the intersection of high performance computing and machine\nlearning. She has worked on projects ranging from performance prediction on\nNVIDIA GPUs to machine learning surrogate models for scientific applications. \n Yulie developed and taught an Introduction to Computer Science course for\nincoming University of Chicago students. She incorporated many of the basic con-\ncepts of parallel computing fundamentals into the material. The course was so suc-\ncessful, she was asked to teach it again and again. Wanting to gain more teaching\nexperience, she volunteered for a teaching assistant position for an Advanced Distrib-\nuted Systems course at the University of Chicago. \n Yulie’s Bachelors degree is in Civil Engineering from Cornell University. She fin-\nished her Masters of Computer Science from University of Chicago and will soon com-\nplete her PhD in Computer Science, also from the University of Chicago.",7017
15-Part 1Introduction to parallel computing.pdf,15-Part 1Introduction to parallel computing,"Part 1\nIntroduction to\nparallel computing\nT he first part of this book covers topics of general importance to parallel\ncomputing. These topics include\n■Understanding the resources in a parallel computer\n■Estimating the performance and speedup of applications\n■Looking at software engineering needs particular to parallel computing\n■Considering choices for data structures\n■Selecting algorithms that perform and parallelize well\nWhile these topics should be considered first by a parallel programmer, these\nwill not have the same importance to all readers of this book. For the parallel\napplication developer, all of the chapters in this part address upfront concerns\nfor a successful project. A project needs to select the right hardware, the right\ntype of parallelism, and the right kind of expectations. You should determine\nthe appropriate data structures and algorithms before starting your paralleliza-\ntion efforts; it’s much harder to change these later. \n Even if you are a parallel application developer, you may not need the full\ndepth of material discussed. Those desiring only modest parallelism or serving a\nparticular role on a team of developers might find a cursory understanding of\nthe content sufficient. If you just want to explore parallel computing, we suggest\nreading chapter 1 and chapter 5, then skimming the others to get the terminol-\nogy that is used in discussing parallel computing. \n2 PART 1Introduction to parallel computing\n We include chapter 2 for those who may not have a software engineering back-\nground or for those who just need a refresher. If you are new to all of the details of\nCPU hardware, then you may need to read chapter 3 in small increments. An under-\nstanding of the current computing hardware and your application is important in\nextracting performance, but it doesn’t have to come all at once. Be sure to return to\nchapter 3 when you are ready to purchase your next computing system so you can cut\nthrough all the marketing claims to what is really important for your application.\n The discussion of data design and performance modeling in chapter 4 can be chal-\nlenging because it requires an understanding of hardware details, their performance,\nand compilers to fully appreciate. Although it’s an important topic due to the impact\nthe cache and compiler optimizations have on performance, it’s not necessary for\nwriting a simple parallel program.\n We encourage you to follow along with the accompanying examples for the book.\nYou should spend some time exploring the many examples that are available in these\nsoftware repositories at https:/ /github.com/EssentialsOfParallelComputing . \n The examples are organized by chapter and include detailed information for\nsetup on various hardware and operating systems. For helping to deal with portability\nissues, there are sample container builds for Ubuntu distributions in Docker. There\nare also instructions for setting up a virtual machine through VirtualBox. If you have a\nneed for setting your own system up, you may want to read the section on Docker and\nvirtual machines in chapter 13. But containers and virtual machines come with\nrestricted environments that are not easy to work around. \n Our work is ongoing for the container builds and other system environment setups\nto work properly for the many possible system configurations. Getting the system soft-\nware installed correctly, especially the GPU driver and associated software, is the most\nchallenging part of the journey. The wide variety of operating systems, hardware includ-\ning graphics processing units (GPUs), and the often overlooked quality of installation\nsoftware makes this a difficult task. One alternative is to use a cluster where the software\nis already installed. Still, it is helpful at some point to get some software installed on your\nlaptop or desktop for a more convenient development resource. Now it is time to turn\nthe page and enter the world of parallel computing. It is a world of nearly unlimited\nperformance and potential.",4074
16-1 Why parallel computing.pdf,16-1 Why parallel computing,"3Why parallel computing?\nIn today’s world, you’ll find many challenges requiring extensive and efficient use\nof computing resources. Most of the applications requiring performance tradition-\nally are in the scientific domain. But artificial intelligence (AI) and machine learning\napplications are projected to become the predominant users of large-scale comput-\ning. Some examples of these applications include\nModeling megafires to assist fire crews and to help the public\nModeling tsunamis and storm surges from hurricanes (see chapter 13 for a\nsimple tsunami model)\nVoice recognition for computer interfaces\nModeling virus spread and vaccine developmentThis chapter covers\nWhat parallel computing is and why it’s growing \nin importance\nWhere parallelism exists in modern hardware\nWhy the amount of application parallelism is \nimportant\nSoftware approaches to exploit parallelism\n4 CHAPTER  1Why parallel computing?\nModeling climatic conditions over decades and centuries\nImage recognition for driverless car technology\nEquipping emergency crews with running simulations of hazards such as flooding \nReducing power consumption for mobile devices\nWith the techniques covered in this book, you will be able to handle larger problems\nand datasets, while also running simulations ten, a hundred, or even a thousand times\nfaster. Typical applications leave much of the compute capability of today’s computers\nuntapped. Parallel computing is the key to unlocking the potential of your computer\nresources. So what is parallel computing and how can you use it to supercharge your\napplications?\n Parallel computing  is the execution of many operations at a single instance in time.\nFully exploiting parallel computing does not happen automatically. It requires some\neffort from the programmer. First, you must identify and expose the potential for par-\nallelism in an application. Potential parallelism, or concurrency , means that you certify\nthat it is safe to conduct operations in any order as the system resources become avail-\nable. And, with parallel computing, there is an additional requirement: these opera-\ntions must occur at the same time. For this to happen, you must also properly leverage\nthe resources to execute these operations simultaneously. \n Parallel computing introduces new concerns that are not present in a serial world.\nWe need to change our thought processes to adapt to the additional complexities of\nparallel execution, but with practice, this becomes second nature. This book begins\nyour discovery in how to access the power of parallel computing.\n Life presents numerous examples of parallel processing, and these instances often\nbecome the basis for computing strategies. Figure 1.1 shows a supermarket checkout\nline, where the goal is to have customers quickly pay for the items they want to pur-\nchase. This can be done by employing multiple cashiers to process, or check out, the\ncustomers one at a time. In this case, the skilled cashiers can more quickly execute the\ncheckout process so customers leave faster. Another strategy is to employ many self-\ncheckout stations and allow customers to execute the process on their own. This strat-\negy requires fewer human resources from the supermarket and can open more lanes\nto process customers. Customers may not be able to check themselves out as effi-\nciently as a trained cashier, but perhaps more customers can check out quickly due to\nincreased parallelism resulting in shorter lines.\n We solve computational problems by developing algorithms : a set of steps to achieve\na desired result. In the supermarket analogy, the process of checking out is the algo-\nrithm. In this case, it includes unloading items from a basket, scanning the items to\nobtain a price, and paying for the items. This algorithm is sequential (or serial); it\nmust follow this order. If there are hundreds of customers that need to execute this\ntask, the algorithm for checking out many customers contains a parallelism that can\nbe taken advantage of. Theoretically, there is no dependency between any two cus-\ntomers going through the checkout process. By using multiple checkout lines or self-\ncheckout stations, supermarkets expose parallelism, thereby increasing the rate at which\n5\ncustomers buy goods and leave the store. Each choice in how we implement this paral-\nlelism results in different costs and benefits. \nDEFINITION Parallel computing  is the practice of identifying and exposing par-\nallelism in algorithms, expressing this in our software, and understanding the\ncosts, benefits, and limitations of the chosen implementation.\nIn the end, parallel computing is about performance . This includes more than just\nspeed, but also the size of the problem and energy efficiency. Our goal in this book is\nto give you an understanding of the breadth of the current parallel computing field\nand familiarize you with enough of the most commonly used languages, techniques,\nand tools so that you can tackle a parallel computing project with confidence. Import-\nant decisions about how to incorporate parallelism are often made at the outset of a\nproject. A reasoned design is an important step toward success. Avoiding the design\nstep can lead to problems much later. It is equally important to keep expectations\nrealistic and to know both the available resources and the nature of the project. \n Another goal of this chapter is to introduce the terminology used in parallel com-\nputing. One way to do that is to point you to the glossary in appendix C for a quick ref-\nerence on terminology as you read this book. Because this field and the technology has\ngrown incrementally, the use of many of the terms by those in the parallel community is15 items\nor less\nFigure 1.1 Everyday parallelism in supermarket checkout queues. The checkout cashiers (with caps) \nprocess their queue of customers (with baskets). On the left, one cashier processes four self-checkout \nlanes simultaneously. On the right, one cashier is required for each checkout lane. Each option impacts the \nsupermarket’s costs and checkout rates.",6166
17-1.1 Why should you learn about parallel computing.pdf,17-1.1 Why should you learn about parallel computing,"6 CHAPTER  1Why parallel computing?\noftentimes sloppy and imprecise. With the increased complexity of the hardware and\nof parallelism within applications, it’s important that we establish a clear, unambigu-\nous use of terminology from the start.\n Welcome to the world of parallel computing! As you delve deeper, the techniques\nand approaches become more natural, and you’ll find its power captivating. Problems\nthat you never thought to attempt become commonplace.\n1.1 Why should you learn about parallel computing?\nThe future is parallel. The increase in serial performance has plateaued as processor\ndesigns have hit the limits of miniaturization, clock frequency, power, and even heat.\nFigure 1.2 shows the trends in clock frequency (the rate at which an instruction can be\nexecuted), power consumption, the number of computational cores (or cores for\nshort), and hardware performance over time for commodity processors. \nIn 2005, the number of cores abruptly increased from a single core to multiple cores.\nAt the same time, clock frequency and power consumption flattened out. Theoretical\nperformance steadily increased because performance is proportional to the productPerformance\nsingle thread (kOps)\nFrequency (MHz)\nPower (Watts)\nNo. of cores105\n104\n103\n102\n101\n100\n1970 1990 1980 2000 2010 2020\nYear\nFigure 1.2 Single thread performance, CPU clock frequency (MHz), CPU power consumption \n(watts), and the number of CPU cores from 1970 to 2018. The parallel computing era begins \nabout 2005, when the core count in CPU chips begins to rise, while the clock frequency and \npower consumption plateaus, yet performance steadily increases (Horowitz et al. and Rupp, \nhttps:/ /github.com/karlrupp/microprocessor-trend-data ).\n7 Why should you learn about parallel computing?\nof clock frequency and the number of cores. This shift towards increasing the core\ncount rather than the clock speed indicates that achieving the most ideal performance\nof a central processing unit (CPU) is only available through parallel computing.\n Modern consumer-grade computing hardware comes equipped with multiple cen-\ntral processing units (CPUs) and/or graphics processing units (GPUs) that process\nmultiple instruction sets simultaneously. These smaller systems often rival the comput-\ning power of supercomputers of two decades ago. Making full use of compute resources\n(on laptops, workstations, smart phones, and so forth) requires you, the programmer,\nto have a working knowledge of the tools available for writing parallel applications.\nYou must also understand the hardware features that boost parallelism.\n Because there are many different parallel hardware features, this presents new\ncomplexities to the programmer. One of these features is hyperthreading, introduced\nby Intel. Having two instruction queues interleaving work to the hardware logic units\nallows a single physical core to appear as two cores to the operating system (OS). Vec-\ntor processors are another hardware feature that began appearing in commodity pro-\ncessors in about 2000. These execute multiple instructions at once. The width in bits\nof the vector processor (also called a vector unit) specifies the number of instructions\nto execute simultaneously. Thus, a 256 bit-wide vector unit can execute four 64-bit\n(doubles) or eight 32-bit (single-precision) instructions at one time. \nExample\nLet’s take a 16-core CPU with hyperthreading and a 256 bit-wide vector unit, com-\nmonly found in home desktops. A serial program using a single core and no vector-\nization only uses 0.8% of the theoretical processing capability of this processor! The\ncalculation is\n16 cores × 2 hyperthreads × (256 bit-wide vector unit)/(64-bit double)  =\n 128-way parallelism \nwhere 1 serial path/128 parallel paths = .008 or 0.8%. The following figure shows\nthat this is a small fraction of the total CPU processing power.\nSerial 0.8%\nParallel 99.2%A serial application only accesses \n0.8% of the processing power of a \n16-core CPU.",4043
18-1.1.1 What are the potential benefits of parallel computing.pdf,18-1.1.1 What are the potential benefits of parallel computing,"8 CHAPTER  1Why parallel computing?\nSome improvement in software development tools has helped to add parallelism to\nour toolkits, and currently, the research community is doing more, but it is still a long\nway from addressing the performance gap. This puts a lot of the burden on us, the\nsoftware developers, to get the most from a new generation of processors.\n Unfortunately, software developers have lagged in adapting to this fundamental\nchange in computing power. Further, transitioning current applications to make use\nof modern parallel architectures can be daunting due to the explosion of new pro-\ngramming languages and application programming interfaces (APIs). But a good\nworking knowledge of your application, an ability to see and expose parallelism, and a\nsolid understanding of the tools available can result in substantial benefits. Exactly\nwhat kind of benefits would applications see? Let’s take a closer look.\n1.1.1 What are the potential benefits of parallel computing?\nParallel computing can reduce your time to solution, increase the energy efficiency in\nyour application, and enable you to tackle larger problems on currently existing hard-\nware. Today, parallel computing no longer is the sole domain of the largest computing\nsystems. The technology is now present in everyone’s desktop or laptop, and even on\nhandheld devices. This makes it possible for every software developer to create paral-\nlel software on their local systems, thereby greatly expanding the opportunity for new\napplications.\n Cutting edge research from both industry and academia reveals new areas for\nparallel computing as interest broadens from scientific computing into machine\nlearning, big data, computer graphics, and consumer applications. The emergence\nof new technologies such as self-driving cars, computer vision, voice recognition,\nand AI requires large computational capabilities both within the consumer device\nand in the development sphere, where massive training datasets must be consumed\nand processed. And in scientific computing, which has long been the exclusive\ndomain of parallel computing, there are also new, exciting possibilities. The prolif-\neration of remote sensors and handheld devices that can feed data into larger, more\nrealistic computations to better inform decision-making around natural and man-\nmade disasters allows for more extensive data.\n It must be remembered that parallel computing itself is not the goal. Rather, the\ngoals are what results from parallel computing: reducing run time, performing larger\ncalculations, or decreasing energy consumption.(continued)\nCalculating theoretical and realistic expectations for serial and parallel performance\nas shown in this example is an important skill. We’ll discuss this in more depth in\nchapter 3.\n9 Why should you learn about parallel computing?\nFASTER  RUN TIME WITH MORE COMPUTE  CORES\nReduction of an application’s run time, or the speedup , is often thought to be the pri-\nmary goal of parallel computing. Indeed, this is usually its biggest impact. Parallel\ncomputing can speed up intensive calculations, multimedia processing, and big data\noperations, whether your applications take days or even weeks to process or the results\nare needed in real-time now.\n In the past, the programmer would spend greater efforts on serial optimization to\nsqueeze out a few percentage improvements. Now, there is the potential for orders of\nmagnitude of improvement with multiple avenues to choose from. This creates a new\nproblem in exploring the possible parallel paradigms—more opportunities than pro-\ngramming manpower. But, a thorough knowledge of your application and an aware-\nness of parallelism opportunities can lead you down a clearer path towards reducing\nyour application’s run time.\nLARGER  PROBLEM  SIZES WITH MORE COMPUTE  NODES\nBy exposing parallelism in your application, you can scale up your problem’s size to\ndimensions that were out of reach with a serial application. This is because the\namount of compute resources dictates what can be done, and exposing parallelism\npermits you to operate on larger resources, presenting opportunities that were never\nconsidered before. The larger sizes are enabled by greater amounts of main memory,\ndisk storage, bandwidth over networks and to disk, and CPUs. In analogy with the\nsupermarket as mentioned earlier, exposing parallelism is equivalent to employing\nmore cashiers or opening more self-checkout lanes to handle a larger and growing\nnumber of customers.\nENERGY  EFFICIENCY  BY DOING  MORE WITH LESS\nOne of the new impact areas of parallel computing is energy efficiency. With the\nemergence of parallel resources in handheld devices, parallelism can speed up appli-\ncations. This allows the device to return to sleep mode sooner and permits the use of\nslower, but more parallel processors that consume less power. Thus, moving heavy-\nweight multimedia applications to run on GPUs can have a more dramatic effect on\nenergy efficiency while also resulting in vastly improved performance. The net result\nof employing parallelism reduces power consumption and extends battery life, which\nis a strong competitive advantage in this market niche. \n Another area where energy efficiency is important is with remote sensors, network\ndevices, and operational field-deployed devices, such as remote weather stations.\nOften, without large power supplies, these devices must be able to function in small\npackages with few resources. Parallelism expands what can be done on these devices\nand offloads the work from the central computing system in a growing trend that is\ncalled edge compute . Moving the computation to the very edge of the network enables\nprocessing at the source of the data, condensing it into a smaller result set that can be\nmore easily sent over the network.\n Accurately calculating the energy costs of an application is challenging without\ndirect measurements of power usage. However, you can estimate the cost by multiplying\n10 CHAPTER  1Why parallel computing?\nthe manufacturer’s thermal design power by the application’s run time and the number\nof processors used. Thermal design power is the rate at which energy is expended under\ntypical operational loads. The energy consumption for your application can be esti-\nmated using the formula\nP = (N Processors) × ( R Watts/Processors) × ( T hours)\nwhere P is the energy consumption, N is the number of processors, R is the thermal\ndesign power, and T is the application run time.\nAchieving a reduction in energy cost through accelerator devices like GPUs requires\nthat the application has sufficient parallelism that can be exposed. This permits the\nefficient use of the resources on the device.\nPARALLEL  COMPUTING  CAN REDUCE  COSTS\nActual monetary cost is becoming a more visible concern for software developer\nteams, software users, and researchers alike. As the size of applications and systems\ngrows, we need to perform a cost-benefit analysis on the resources available. For exam-\nple, with the next large High Performance Computing (HPC) systems, the power costs\nare projected to be three times the cost of the hardware acquisition. Example\nIntel’s 16-core Xeon E5-4660 processor has a thermal design power of 120 W. Sup-\npose that your application uses 20 of these processors for 24 hours to run to com-\npletion. The estimated energy usage for your application is \nP = (20 Processors) × (120 W/Processors) × (24 hours) = 57.60 kWhrs\nIn general, GPUs have a higher thermal design power than modern CPUs, but can\npotentially reduce run time or require only a few GPUs to obtain the same result. The\nsame formula can be used as before, where N is now seen as the number of GPUs.\nExample \nSuppose that you’ve ported your application to a multi-GPU platform. You can now\nrun your application on four NVIDIA Tesla V100 GPUs in 24 hrs! NVIDIA’s Tesla V100\nGPU has a maximum thermal design power of 300 W. The estimated energy usage\nfor your application is \nP = (4 GPUs) × (300 W/GPUs) × (24 hrs) = 28.80 kWhrs\nIn this example, the GPU accelerated application runs at half the energy cost as the\nCPU-only version. Note that, in this case, even though the time to solution remains\nthe same, the energy expense is cut in half!",8346
19-1.1.2 Parallel computing cautions.pdf,19-1.1.2 Parallel computing cautions,,0
20-1.2 The fundamental laws of parallel computing.pdf,20-1.2 The fundamental laws of parallel computing,,0
21-1.2.2 Breaking through the parallel limit Gustafson-Barsiss Law.pdf,21-1.2.2 Breaking through the parallel limit Gustafson-Barsiss Law,"11 The fundamental laws of parallel computing\n Usage costs have also promoted cloud computing as an alternative, which is being\nincreasingly adopted across academia, start-ups, and industries. In general, cloud pro-\nviders bill by the type and quantity of resources used and the amount of time spent\nusing these. Although GPUs are generally more expensive than CPUs per unit time,\nsome applications can leverage GPU accelerators such that there are sufficient reduc-\ntions in run time relative to the CPU expense to yield lower costs.\n1.1.2 Parallel computing cautions\nParallel computing is not a panacea. Many applications are neither large enough or\nrequire enough run time to need parallel computing. Some may not even have enough\ninherent parallelism to exploit. Also, transitioning applications to leverage multi-core\nand many-core (GPU) hardware requires a dedicated effort that can temporarily shift\nattention away from direct research or product goals. The investment of time and\neffort must first be deemed worthwhile. It is always more important that the applica-\ntion runs and generates the desired result before making it fast and scaling it up to\nlarger problems.\n We strongly recommend that you start your parallel computing project with a plan.\nIt’s important to know what options are available for accelerating the application,\nthen select the most appropriate for your project. After that, it is crucial to have a rea-\nsonable estimate of the effort involved and the potential payoffs (in terms of dollar\ncost, energy consumption, time to solution, and other metrics that can be important).\nIn this chapter, we begin to give you the knowledge and skills to make decisions on\nparallel computing projects up front.\n1.2 The fundamental laws of parallel computing\nIn serial computing, all operations speed up as the clock frequency increases. In con-\ntrast, with parallel computing, we need to give some thought and modify our applica-\ntions to fully exploit parallel hardware. Why is the amount of parallelism important?\nTo understand this, let’s take a look at the parallel computing laws.\n1.2.1 The limit to parallel computing: Amdahl’s Law\nWe need a way to calculate the potential speedup of a calculation based on the\namount of the code that is parallel. This can be done using Amdahl’s Law, proposed\nby Gene Amdahl in 1967. This law describes the speedup of a fixed-size problem as\nthe processors increase. The following equation shows this, where P is the parallel\nfraction of the code, S is the serial fraction, which means that P + S = 1, and N is the\nnumber of processors:\nSpeedUp (N) = 1\nSP\nN--- -+-----------\n12 CHAPTER  1Why parallel computing?\nAmdahl’s Law highlights that no matter how fast we make the parallel part of the\ncode, we will always be limited by the serial portion. Figure 1.3 visualizes this limita-\ntion. This scaling of a fixed-size problem is referred to as strong scaling.\nDEFINITION Strong scaling  represents the time to solution with respect to the\nnumber of processors for a fixed total size.\n1.2.2 Breaking through the parallel limit: Gustafson-Barsis’s Law\nGustafson and Barsis pointed out in 1988 that parallel code runs should increase the\nsize of the problem as more processors are added. This can give us an alternate way to\ncalculate the potential speedup of our application. If the size of the problem grows\nproportionally to the number of processors, the speedup is now expressed as\nSpeedUp (N) = N – S * (N – 1)\nwhere N is the number of processors, and S is the serial fraction as before. The result\nis that a larger problem can be solved in the same time by using more processors.\nThis provides additional opportunities to exploit parallelism. Indeed, growing the\nsize of the problem with the number of processors makes sense because the applicationIdeal speedup\n90% Parallel fraction\n75% Parallel fraction\n50% Parallel fraction32\n16\n8\n4\n2\n1Speedup\n1 2 4 8 16 32\nNumber of processors\nFigure 1.3 Speedup for a fixed-size problem according to Amdahl’s Law is shown \nas a function of the number of processors. Lines show ideal speedup when 100% of \nan algorithm is parallelized, and for 90%, 75%, and 50%. Amdahl’s Law states that \nspeedup is limited by the fractions of code that remain serial.\n13 The fundamental laws of parallel computing\nuser wants to benefit from more than just the power of the additional processor and\nwants to use the additional memory. The run-time scaling for this scenario, shown in\nfigure 1.4, is called weak scaling.\nDEFINITION Weak scaling  represents the time to solution with respect to the\nnumber of processors for a fixed-sized problem per processor.\nFigure 1.5 shows the difference between strong and weak scaling in a visual represen-\ntation. The weak scaling argument that the mesh size should stay constant on each\nprocessor makes good use of the resources of the additional processor. The strong\nscaling perspective is primarily concerned with speedup of the calculation. In prac-\ntice, both strong scaling and weak scaling are important because these address differ-\nent user scenarios.\n The term scalability  is often used to refer to whether more parallelism can be\nadded in either the hardware or the software and whether there is an overall limit to\nhow much improvement can occur. While the traditional focus is on the run-time scal-\ning, we will make the argument that memory scaling is often more important. \n Figure 1.6 shows an application with limited memory scalability. A replicated array\n(R) is a dataset that is duplicated across all the processors. A distributed array  (D) is\npartitioned and split across the processors. For example, in a game simulation, 100Ideal speedup\n90% Parallel fraction\n75% Parallel fraction\n50% Parallel fraction32\n16\n8\n4\n2\n1\n1 2 4 8 16 32\nNumber of processorsSpeedup\nFigure 1.4 Speedup for when the size of a problem grows with the number of \navailable processors according to Gustafson-Barsis’s Law is shown as a function \nof the number of processors. Lines show ideal speedup when 100% of an algorithm \nis parallelized, and for 90%, 75%, and 50%. \n14 CHAPTER  1Why parallel computing?\ncharacters can be distributed across 4 processors with 25 characters on each proces-\nsor. But the map of the game board might be copied to every processor. In figure 1.6,\nthe replicated array is duplicated across the mesh. Because this figure is for weak scal-\ning, the problem size grows as the number of processors increases. For 4 processors,\nthe array is 4 times as large on each processor. As the number of processors and theStrong scaling\nWeak scalingTotal mesh size stays constant\n1000\n1000500\n500250\n250\n1 processor 4 processors 16 processors\n1000\n10001000\n1000\n1000\n1000Mesh size stays the same for each processor\nTotal mesh size grows by number of processors\nFigure 1.5 Strong scaling keeps the same overall size of a problem and splits it \nacross additional processors. In weak scaling, the size of the mesh stays the same \nfor each processor and the total size increases.\nMemory sizes for weak scaling with replicated and distributed arrays\n100 MB\n100 MB200 MB\n100 MB 100 MB400 MB\n400 MB400 MB\n400 MBArray R\nArray D1 proc 2 proc 4 proc\nProc 0 Proc 0 Proc 1 Proc 1\n200 MBProc 0\n100 MB 100 MB\nProc 2 Proc 3\n100 MB 100 MB\nArray R – Array is replicated (copied) to every processor\nArray D – Array is distributed across processors\nFigure 1.6 Distributed arrays stay the same size as the problem and number of processors doubles \n(weak scaling). But replicated (copied) arrays need all the data on each processor, and memory \ngrows rapidly with the number of processors. Even if the run time weakly scales (stays constant), the \nmemory requirements limit scalability.",7841
22-1.3.1 Walking through a sample application.pdf,22-1.3.1 Walking through a sample application,"15 How does parallel computing work?\nsize of the problem grows, soon there is not enough memory on a processor for the\njob to run. Limited run-time scaling means the job runs slowly; limited memory scal-\ning means the job can’t run at all. It is also the case that if the application’s memory\ncan be distributed, the run time usually scales as well. The reverse, however, is not nec-\nessarily true.\n One view of a computationally intensive job is that every byte of memory gets\ntouched in every cycle of processing, and run time is a function of memory size.\nReducing memory size will necessarily reduce run time. The initial focus in parallel-\nism should thus be to reduce the memory size as the number of processors grows.\n1.3 How does parallel computing work?\nParallel computing requires combining an understanding of hardware, software, and\nparallelism to develop an application. It is more than just message passing or thread-\ning. Current hardware and software give many different options to bring paralleliza-\ntion to your application. Some of these options can be combined to yield even greater\nefficiency and speedup. \n It is important to have an understanding of the parallelization in your application\nand the way different hardware components allow you to expose it. Further, develop-\ners need to recognize that between your source code and the hardware, your applica-\ntion must traverse additional layers, including a compiler and an OS (figure 1.7).\nAs a developer, you are responsible for the application software layer, which includes\nyour source code. In the source code, you make choices about the programming\nlanguage and parallel software interfaces you use to leverage the underlying hard-\nware. Additionally, you decide how to break up your work into parallel units. A com-\npiler is designed to translate your source code into a form the hardware can execute.\nWith these instructions at hand, an OS manages executing these on the computer\nhardware.\n We will show you with an example how to introduce parallelization to an algorithm\nthrough a prototype application. This process takes place in the application software\nlayer but requires an understanding of computer hardware. For now, we’ll refrainOperating systemCompiler\nComputer hardwareApplication software layer\nFigure 1.7 Parallelization is expressed \nin an application software layer that gets \nmapped to the computer hardware \nthrough the compiler and the OS.\n16 CHAPTER  1Why parallel computing?\nfrom discussing the choice in compiler and OS. We will incrementally add each layer\nof parallelization so that you can see how this works. With each parallel strategy, we\nwill explain how the available hardware influences the choices that are made. The\npurpose in doing this is to demonstrate how hardware features influence the parallel\nstrategies. We categorize the parallel approaches a developer can take into\nProcess-based parallelization\nThread-based parallelization \nVectorization\nStream processing\nFollowing the example, we will introduce a model to help you think about modern\nhardware. This model breaks down modern compute hardware into individual com-\nponents and the variety of compute devices. A simplified view of memory is included\nin this chapter. A more detailed look at the memory hierarchy is presented in chapters 3\nand 4. Finally, we will discuss in more detail the application and software layers. \n As mentioned, we categorize the parallel approaches a developer can take into\nprocess-based parallelization, thread-based parallelization, vectorization, and stream\nprocessing. Parallelization based on individual processes with their own memory spaces\ncan be distributed memory on different nodes of a computer or within a node. Stream\nprocessing is generally associated with GPUs. The model for modern hardware and\napplication software will help you better understand how to plan to port your applica-\ntion to current parallel hardware.\n1.3.1 Walking through a sample application\nFor this introduction to parallelization, we will look at a data parallel approach. This is\none of the most common parallel computing application strategies. We’ll perform the\ncomputation on a spatial mesh composed of a regular two-dimensional (2D) grid of\nrectangular elements or cells. The steps (summarized here and described in detail\nlater) to create the spatial mesh and prepare for the calculation are\n1Discretize (break up) the problem into smaller cells or elements\n2Define a computational kernel (operation) to conduct on each element of\nthe mesh\n3Add the following layers of parallelization on CPUs and GPUs to perform the\ncalculation:\n–Vectorization —Work on more than one unit of data at a time\n–Threads —Deploy more than one compute pathway to engage more process-\ning cores\n–Processes —Separate program instances to spread out the calculation into sep-\narate memory spaces\n–Off-loading the calculation to GPUs —Send the data to the graphics processor to\ncalculate\n17 How does parallel computing work?\nWe start with a 2D problem domain of a region of space. For purposes of illustration,\nwe will use a 2D image of the Krakatau volcano (figure 1.8) as our example. The goal of\nour calculation could be to model the volcanic plume, the resulting tsunami, or the\nearly detection of a volcanic eruption using machine learning. For all of these options,\ncalculation speed is critical if we want real-time results to inform our decisions.\nSTEP 1: D ISCRETIZE  THE PROBLEM  INTO SMALLER  CELLS  OR ELEMENTS\nFor any detailed calculation, we must first break up the domain of the problem into\nsmaller pieces (figure 1.9), a process that is called discretization . In image processing,\nthis is often just the pixels in a bitmap image. For a computational domain, these are\ncalled cells or elements. The collection of cells or elements form a computational mesh\nthat covers the spatial region for the simulation. Data values for each cell might be\nintegers, floats, or doubles.\nSTEP 2: D EFINE A COMPUTATIONAL  KERNEL , OR OPERATION , TO CONDUCT  ON EACH ELEMENT  OF THE MESH\nThe calculations on this discretized data are often some form of a stencil operation,\nso-called because it involves a pattern of adjacent cells to calculate the new value for\neach cell. This can be an average (a blur operation, which blurs the image or makes itJava Sea\nKrakatau\nIndian\nOceanFigure 1.8 An example 2D spatial domain for \na numerical simulation. Numerical simulations \ntypically involve stencil operations (see figure 1.11) \nor large matrix-vector systems. These types of \noperations are often used in fluids modeling \nto yield predictions of tsunami arrival times, \nweather forecasts, smoke plume spreading, \nand other processes necessary for informed \ndecisions.\nJavaJava Sea\nIndian\nOceanOceanIndian\nOceanIndian\nOceanIndian\nOceanIndian\nOceanIndian\nOceanIndianJava SeaJava SeaJava SeaJava SeaJava SeaJava SeaJava SeaJava SeaJava SeaJava SeaJava Sea\nIndianIndianIndianIndianIndian\nOceanOceanOceanOceanOcean\nFigure 1.9 The domain is discretized into \ncells. For each cell in the computational \ndomain, properties such as wave height, \nfluid velocity, or smoke density are solved \nfor according to physical laws. Ultimately, a \nstencil operation or a matrix-vector system \nrepresents this discrete scheme.\n18 CHAPTER  1Why parallel computing?\nfuzzier), a gradient (edge-detection, which sharpens the edges in the image), or\nanother more complex operation associated with solving physical systems described\nby partial differential equations (PDEs). Figure 1.10 shows a stencil operation as a\nfive-point stencil that performs a blur operation by using a weighted average of the\nstencil values.\nBut what are these partial differential equations? Let’s go back to our example and\nimagine this time it is a color image composed of separate red, green, and blue arrays\nto make an RGB color model. The term “partial” here means that there is more than\none variable and that we are separating out the change of red with space and time\nfrom that of green and blue. Then we carry out the blur operator separately on each\nof these colors. \n There is one more requirement: we need to apply a rate of change with time and\nspace. In other words, the red would spread at one rate and green and blue at oth-\ners. This could be to produce a special effect on an image, or it can describe how\nreal colors bleed and merge in a photographic image during development. In the\nscientific world, instead of red, green, and blue, we might have mass and x and y\nvelocity. With the addition of a little more physics, we might have the motion of a\nwave or an ash plume. \nFigure 1.10 A five-point stencil operator as a cross pattern on the computational \nmesh. The data marked by the stencil are read in the operation and stored in the \ncenter cell. This pattern is repeated for every cell. The blur operator, one of the \nsimpler stencil operators, is a weighted sum of the five points marked with the large \ndots and updates a value at the central point of the stencil. This type of operation \nis done for smoothing operations or wave propagation numerical simulations.\n19 How does parallel computing work?\nSTEP 3: V ECTORIZATION  TO WORK  ON MORE THAN ONE UNIT OF DATA AT A TIME\nWe start introducing parallelization by looking at vectorization. What is vectorization?\nSome processors have the ability to operate on more than one piece of data at a time;\na capability referred to as vector operations . The shaded blocks in figure 1.11 illustrate\nhow multiple data values are operated on simultaneously in a vector unit in a proces-\nsor with one instruction in one clock cycle.\nSTEP 4: T HREADS  TO DEPLOY  MORE THAN ONE COMPUTE  PATHWAY  TO ENGAGE  MORE PROCESSING  CORES\nBecause most CPUs today have at least four processing cores, we use threading to oper-\nate the cores simultaneously across four rows at a time. Figure 1.12 shows this process.\nSTEP 5: P ROCESSES  TO SPREAD  OUT THE CALCULATION  TO SEPARATE  MEMORY  SPACES\nWe can further split the work between processors on two desktops, often called nodes\nin parallel processing. When the work is split across nodes, the memory spaces for\neach node are distinct and separate. This is indicated by putting a gap between the\nrows as in figure 1.13.\n Even for this fairly modest hardware scenario, there is a potential speedup of 32x.\nThis is shown by the following:\n2 desktops (nodes) × 4 cores × (256 bit-wide vector unit)/(64-bit double) = \n32x potential speedupVector unit\nFigure 1.11 A special vector operation is conducted on four doubles. This \noperation can be executed in a single clock cycle with little additional energy \ncosts to the serial operation.\n20 CHAPTER  1Why parallel computing?\nThreads\nVector unit\nFigure 1.12 Four threads process four rows of vector units simultaneously.\nThreadsProcessesThreads\nVector unit\nFigure 1.13 This algorithm can be parallelized further by distributing the 4×4 \nblocks among distinct processes. Each process uses four threads, each handling \na four-node-wide vector unit in a single clock cycle. Additional white space in the \nfigure illustrates the process boundaries.\n21 How does parallel computing work?\nIf we look at a high-end cluster with 16 nodes, 36 cores per node, and a 512-bit vector\nprocessor, the potential theoretical speedup is 4,608 times faster than a serial process:\n16 nodes × 36 cores × (512 bit-wide vector unit)/(64-bit double) = \n4,608x potential speedup\nSTEP 6: O FF-LOADING  THE CALCULATION  TO GPU S\nThe GPU is another hardware resource for supercharging parallelization. With GPUs,\nwe can harness lots of streaming multiprocessors  for work. For example, figure 1.14 shows\nhow the work can be split up separately into 8x8 tiles. Using the hardware specifica-\ntions for the NVIDIA Volta GPU, these tiles can be operated on by 32 double-preci-\nsion cores spread out over 84 streaming multiprocessors, giving us a total of 2,688\ndouble-precision cores that work simultaneously. If we have one GPU per node in a\n16-node cluster, each with a 2,688 double-precision streaming multiprocessor, this is a\n43,008-way parallelization from 16 GPUs.\nThese are impressive numbers, but at this point, we must temper expectations by\nacknowledging that actual speedup falls far short of this full potential. Our challenge\nnow becomes organizing such extreme and disparate layers of parallelization to obtain\nas much speedup as possible. 84 streaming multiprocessors8×8 work group tile\nOperated on by 32 FP64 cores\nFigure 1.14 On a GPU, the vector length is much larger than on a CPU. \nHere, 8×8 tiles are distributed across GPU work groups.",12800
23-1.3.2 A hardware model for todays heterogeneous parallel systems.pdf,23-1.3.2 A hardware model for todays heterogeneous parallel systems,"22 CHAPTER  1Why parallel computing?\n For this high-level application walk-through, we left out a lot of important details,\nwhich we will cover in later chapters. But even this nominal level of detail highlights\nsome of the strategies for exposing parallelization of an algorithm. To be able to develop\nsimilar strategies for other problems, an understanding of modern hardware and soft-\nware is necessary. We now dive deeper into the current hardware and software models.\nThese conceptual models are simplified representations of the diverse real-world hard-\nware to avoid complexity and maintain generality over quickly evolving systems.\n1.3.2 A hardware model for today’s heterogeneous parallel systems\nTo build a basic understanding of how parallel computing works, we’ll explain the\ncomponents in today’s hardware. To begin, Dynamic Random Access Memory, called\nDRAM, stores information or data. A computational core, or core for short, performs\narithmetic operations (add, subtract, multiply, divide), evaluates logical statements,\nand loads and stores data from DRAM. When an operation is performed on data, the\ninstructions and data are loaded from memory onto the core, operated on, and stored\nback into memory. Modern CPUs, often called processors, are outfitted with many\ncores capable of executing these operations in parallel. It is also becoming common\nto find systems outfitted with accelerator hardware, like GPUs. GPUs are equipped\nwith thousands of cores and a memory space that is separate from the CPU’s DRAM. \n A combination of a processor (or two), DRAM, and an accelerator compose a com-\npute node, which can be referred to in the context of a single home desktop or a\n“rack” in a supercomputer. Compute nodes can be connected to each other with one\nor more networks, sometimes called an interconnect. Conceptually, a node runs a sin-\ngle instance of the OS that manages and controls all of the hardware resources. As\nhardware is becoming more complex and heterogeneous, we’ll start with simplified\nmodels of the system’s components so that each is more obvious.\nDISTRIBUTED  MEMORY  ARCHITECTURE : A CROSS -NODE PARALLEL  METHOD\nOne of the first and most scalable approaches to parallel computing is the distrib-\nuted memory cluster (figure 1.15). Each CPU has its own local memory composed\nof DRAM and is connected to other CPUs by a communication network. Good scalabil-\nity of distributed memory clusters arises from its seemingly limitless ability to incorpo-\nrate more nodes.\n This architecture also provides some memory locality by dividing the total address-\nable memory into smaller subspaces for each node, which makes accessing memory\nDRAMCPUNetwork\nDRAMCPU\nFigure 1.15 The distributed memory \narchitecture links nodes composed of \nseparate memory spaces. These nodes \ncan be workstations or racks.\n23 How does parallel computing work?\noff-node clearly different than on-node. This forces the programmer to explicitly\naccess different memory regions. The disadvantage of this is that the programmer\nmust manage the partitioning of the memory spaces at the outset of the application.\nSHARED  MEMORY  ARCHITECTURE : AN ON-NODE PARALLEL  METHOD\nAn alternative approach connects the two CPUs directly to the same shared memory\n(figure 1.16). The strength of this approach is that the processors share the same\naddress space, which simplifies programming. But this introduces potential memory\nconflicts, resulting in correctness and performance issues. Synchronizing memory\naccess and values between CPUs or the processing cores on a multi-core CPU is com-\nplicated and expensive.\nThe addition of more CPUs and processing cores does not increase the amount of\nmemory available to the application. This and the synchronization costs limit the scal-\nability of the shared memory architecture.\nVECTOR  UNITS: MULTIPLE  OPERATIONS  WITH ONE INSTRUCTION\nWhy not just increase the clock frequency for the processor to get greater throughput\nas done in the past? The biggest limitation in increasing CPU clock frequencies is that\nit requires more power and produces more heat. Whether it is an HPC supercomput-\ning center with limits on installed power lines or your cell phone with limited battery\ncapacity, devices today all have power limitations. This problem is called the power wall . \n Rather than increasing the clock frequency, why not do more than one operation\nper cycle? This is the idea behind the resurgence of vectorization on many processors.\nIt takes only a little more energy to do multiple operations in a vector unit, compared\nto a single operation (more formally called a scalar operation ). With vectorization, we\ncan process more data in a single clock cycle than with a serial process. There is little\nchange to the power requirements for multiple operations (versus just one), and a\nreduction in execution time can lead to a decrease in energy consumption for an\napplication. Much like a four-lane freeway that allows four cars to move simultane-\nously in comparison to a single lane road, the vector operation gives greater processing\nthroughput. Indeed, the four pathways through the vector unit, shown in different shad-\nings in figure 1.17, are commonly called lanes  of a vector operation. \n Most CPUs and GPUs have some capability for vectorization or equivalent opera-\ntions. The amount of data processed in one clock cycle, the vector length , depends onDRAMCPU CPU\nFigure 1.16 The shared \nmemory architecture provides \nparallelization within a node.\n24 CHAPTER  1Why parallel computing?\nthe size of the vector units on the processor. Currently, the most commonly available\nvector length is 256-bits. If the discretized data are 64-bit doubles, then we can do four\nfloating-point operations simultaneously as a vector operation. As figure 1.17 illus-\ntrates, vector hardware units load one block of data at a time, perform a single opera-\ntion on the data simultaneously, and then store the result.\nACCELERATOR  DEVICE : A SPECIAL -PURPOSE  ADD-ON PROCESSOR\nAn accelerator device  is a discrete piece of hardware designed for executing specific tasks\nat a fast rate. The most common accelerator device is the GPU. When used for compu-\ntation, this device is sometimes referred to as a general-purpose graphics processing\nunit (GPGPU). The GPU contains many small processing cores, called streaming mul-\ntiprocessors (SMs). Although simpler than a CPU core, SMs provide a massive amount\nof processing power. Usually, you’ll find a small integrated GPU on the CPU. \n Most modern computers also have a separate, discrete GPU connected to the CPU\nby the Peripheral Component Interface (PCI) bus (figure 1.18). This bus introduces a\ncommunication cost for data and instructions, but the discrete card is often more pow-\nerful than an integrated unit. In high-end systems, for example, NVIDIA uses NVLink\nand AMD Radeon uses their Infinity Fabric to reduce data communication costs, but\nthis cost still is substantial. We will discuss the interesting GPU architecture more in\nchapters 9–12.\nGENERAL  HETEROGENEOUS  PARALLEL  ARCHITECTURE  MODEL\nNow let’s combine all of these different hardware architectures into one model (fig-\nure 1.19). Two nodes, each with two CPUs, share the same DRAM memory. Each CPU is\na dual-core processor with an integrated GPU. A discrete GPU on the PCI bus also\nattaches to one of the CPUs. Though the CPUs share main memory, these are com-\nmonly in different Non-Uniform Memory Access (NUMA) regions. This means that\naccessing the second CPU’s memory is more expensive than getting at it’s own memory.0 1 2 3 4 5 6 7+–*/\nArrayLoad 4 valuesStore 4 values\nNext 4 valuesVector unit\n+–*/ +–*/ +–*/\nFigure 1.17 Vector processing \nexample with four array elements \noperated on simultaneously\nCPU\nGPUGPU\nPCIIntegrated Discrete\nSMFigure 1.18 GPUs come in two varieties: integrated \nand discrete. Discrete or dedicated GPUs typically have \na large number of streaming multiprocessors and their \nown DRAM. Accessing data on a discrete GPU requires \ncommunication over a PCI bus.",8165
24-1.3.3 The applicationsoftware model for todays heterogeneous parallel systems.pdf,24-1.3.3 The applicationsoftware model for todays heterogeneous parallel systems,"25 How does parallel computing work?\nThroughout this hardware discussion, we have presented a simplified model of the\nmemory hierarchy, showing just DRAM or main memory. We’ve shown a cache in the\ncombined model (figure 1.19), but no detail on its composition or how it functions.\nWe reserve our discussion of the complexities of memory management, including\nmultiple levels of cache, for chapter 3. In this section, we simply presented a model\nfor today’s hardware to help you identify the available components so that you can\nselect the parallel strategy best suited for your application and hardware choices.\n1.3.3 The application/software model for today’s heterogeneous \nparallel systems\nThe software model for parallel computing is necessarily motivated by the underlying\nhardware but is nonetheless distinct from it. The OS provides the interface between\nthe two. Parallel operations do not spring to life on their own; rather, source code\nmust indicate how to parallelize work by spawning processes or threads; offloading\ndata, work, and instructions to a compute device; or operating on blocks of data at a\ntime. The programmer must first expose the parallelization, determine the best tech-\nnique to operate in parallel, and then explicitly direct its operation in a safe, correct,\nand efficient manner. The following methods are the most common techniques for\nparallelization, then we’ll go through each of these in detail:\nProcess-based parallelization —Message passing\nThread-based parallelization —Shared data via memory\nVectorization —Multiple operations with one instruction\nStream processing —Through specialized processorsNode\nGPU\nSMPCINetwork\nCache\nDRAMCPU Core\nCoreGPU\nGPU\nSMPCINode\nDiscrete DiscreteMain memory and\ncache hierarchyMain memory and\ncache hierarchyIntegratedCPU Core\nCoreGPU\nIntegratedCPU Core\nCoreGPU\nIntegratedCPU Core\nCoreGPU\nIntegrated\nCache\nDRAM\nFigure 1.19 A general heterogeneous parallel architecture model consisting of two nodes \nconnected by a network. Each node has a multi-core CPU with an integrated and discrete \nGPU and some memory (DRAM). Modern compute hardware normally has some arrangement \nof these components.\n26 CHAPTER  1Why parallel computing?\nPROCESS -BASED  PARALLELIZATION : MESSAGE  PASSING\nThe message passing approach was developed for distributed memory architectures,\nwhich uses explicit messages to move data between processes. In this model, your\napplication spawns separate processes, called ranks  in message passing, with their own\nmemory space and instruction pipeline (figure 1.20). The figure also shows that the\nprocesses are handed to the OS for placement on the processors. The application\nlives in the part of the diagram marked as user space, where the user has permissions\nto operate. The part beneath is kernel space, which is protected from dangerous oper-\nations by the user.\nKeep in mind that processors—CPUs—have multiple processing cores that are not\nequivalent to the processes. Processes are an OS concept, and processors are a hard-\nware component. For however many processes the application spawns, these are\nscheduled by the OS to the processing cores. You can actually run eight processes on\nyour quad-core laptop and these will just swap in and out of the processing cores. For\nthis reason, mechanisms have been developed to tell the OS how to place processes\nand whether to “bind” the process to a processing core. Controlling binding is dis-\ncussed in more detail in chapter 14.\n To move data between processes, you’ll need to program explicit messages into the\napplication. These messages can be sent over a network or via shared memory. TheProcess\nrank 0Process\nrank 1Process\nrank 2Process\nrank 3Process\nrank 5\nOperating systemUser\nspace\nCore\nCPU\nMain memory\nNodeOperating system\nCPU\nMain memory\nNode?? ? ? ?Spawned processes\nCPU CPUOriginating\nprocessMy application +\nMessage passing library\nCore Core Core Core Core Core Core\nFigure 1.20 The message passing library spawns processes. The OS places the processes on the cores of two \nnodes. The question marks indicate that the OS controls the placement of the processes and can move these \nduring run time as indicated by the dashed arrows. The OS also allocates memory for each process from the \nnode’s main memory.\n27 How does parallel computing work?\nmany message-passing libraries coalesced into the Message Passing Interface (MPI)\nstandard in 1992. Since then, MPI has taken over this niche and is present in almost\nall parallel applications that scale beyond a single node. And, yes, you’ll also find\nmany different implementations of MPI libraries as well. \nTHREAD -BASED  PARALLELIZATION : SHARED  DATA VIA MEMORY\nThe thread-based approach to parallelization spawns separate instruction pointers\nwithin the same process (figure 1.21). As a result, you can easily share portions of the\nprocess memory between threads. But this comes with correctness and performance\npitfalls. The programmer is left to determine which sections of the instruction set andDistributed computing versus parallel computing\nSome parallel applications use a lower-level approach to parallelization, called dis-\ntributed computing. We define distributed computing  as a set of loosely-coupled pro-\ncesses that cooperate via OS-level calls. While distributed computing is a subset of\nparallel computing, the distinction is important. Examples of distributed computing\napplications include peer-to-peer networks, the World Wide Web, and internet mail.\nThe Search for Extraterrestial Intelligence (SETI@home) is just one example of many\nscientific distributed computing applications.\nThe location of each process is usually on a separate node and is created through\nthe OS using something like a remote procedure call (RPC) or a network protocol. The\nprocesses then exchange information through the passing of messages between the\nprocesses by inter-process communication  (IPC), of which there are several varieties.\nSimple parallel applications often use a distributed computing approach, but often\nthrough a higher-level language such as Python and specialized parallel modules or\nlibraries.\nOperating system\nCore\nCPU\nMain memory\nNodeUser\nspaceMy application process\nThreads\nCore Core Core\nCPU?? ? ? ?\nSharedFigure 1.21 The application process in a \nthread-based approach to parallelization spawns \nthreads. The threads are restricted to the node’s \ndomain. The question marks show that the OS \ndecides where to place the threads. Some \nmemory is shared between threads.\n28 CHAPTER  1Why parallel computing?\ndata are independent and can support threading. These considerations are discussed\nin more detail in chapter 7, where we will look at OpenMP, one of the leading thread-\ning systems. OpenMP provides the capability to spawn threads and divide up the work\namong the threads.\n There are many varieties of threading approaches, ranging from heavy to light-\nweight and managed by either the user space or the OS. While threading systems are\nlimited to scaling within a single node, these are an attractive option for modest speedup.\nThe memory limitations of the single node, however, have larger implications for the\napplication. \nVECTORIZATION : MULTIPLE  OPERATIONS  WITH ONE INSTRUCTION\nVectorizing an application can be far more cost-effective than expanding compute\nresources at an HPC center, and this method might be absolutely necessary on porta-\nb le  d evi ce s  li ke  c el l p hone s .  Whe n ve ct o ri zing,  w or k i s  do ne  i n b lo cks  of  2 - 16  dat a\nitems at a time. The more formal term for this operation classification is single instruc-\ntion, multiple data (SIMD). The term SIMD is used a lot when talking about vectoriza-\ntion. SIMD is just one category of parallel architectures that will be discussed later in\nsection 1.4.\n Invoking vectorization from a user’s application is most often done through source\ncode pragmas or through compiler analysis. Pragmas and directives are hints given to\nthe compiler to guide how to parallelize or vectorize a section of code. Both pragmas\nand compiler analysis are highly dependent on the compiler capabilities (figure 1.22).\nHere we are dependent on the compiler, where the previous parallel mechanisms\nwere dependent on the OS. Also, without explicit compiler flags, the generated code\nis for the least powerful processor and vector length, significantly reducing the effec-\ntiveness of the vectorization. There are mechanisms where the compiler can be by-\npassed, but these require much more programming effort and are not portable.\nSTREAM  PROCESSING  THROUGH  SPECIALIZED  PROCESSORS\nStream processing is a dataflow concept, where a stream of data is processed by a sim-\npler special-purpose processor. Long used in embedded computing, the technique\nwas adapted for rendering large sets of geometric objects for computer displays in a\nspecialized processor, the GPU. These GPUs were filled with a broad set of arithmeticSource code with\nvector pragmas\nCompiler A Compiler B\n2x 3x\nVector processorCPU Figure 1.22 Vector instructions \nin source code returning different \nperformance levels from compilers",9279
25-1.7.1 Additional reading.pdf,25-1.7.1 Additional reading,"29 Categorizing parallel approaches\noperations and multiple SMs to process geometric data in parallel. Scientific program-\nmers soon found ways to adapt stream processing to large sets of simulation data such\nas cells, expanding the role of the GPU to a GPGPU. \n In figure 1.23, the data and kernel are shown offloaded over the PCI bus to the\nGPU for computation. GPUs are still limited in functionality in comparison to CPUs,\nbut where the specialized functionality can be used, these provide extraordinary com-\npute capability at a lower power requirement. Other specialized processors also fit this\ncategory, though we focus on the GPU for our discussions.\n1.4 Categorizing parallel approaches\nIf you read more about parallel computing, you will encounter acronyms such as\nSIMD (single instruction, multiple data) and MIMD (multiple instruction, multiple\ndata). These terms refer to categories of computer architectures proposed by Michael\nFlynn in 1966 in what has become known as Flynn’s Taxonomy . These classes help to\nview potential parallelization in architectures in different ways. The categorization is\nbased on breaking up instructions and data into either serial or multiple operations\n(figure 1.24). Be aware that though the taxonomy is useful, some architectures and\nalgorithms do not fit neatly within a category. The usefulness comes from recognizing\npatterns in categories such as SIMD that have potential difficulties with conditionals.\nThis is because each data item might want to be in a different block of code, but the\nthreads have to execute the same instruction. \nIn the case where there is more than one instruction sequence, the category is called\nmultiple instruction, single data (MISD). This is not a common architecture; the bestData and\nkernelData\nGPUSMsPCI busCPU\nFigure 1.23 In the stream processing approach, \ndata and compute kernel are offloaded to the GPU \nand its streaming multiprocessors. Processed data, \nor output, transfers back to the CPU for file IO or \nother work.\nSingle MultipleInstruction\nSingleMultipleSISD\nSingle instruction\nsingle dataMISD\nMultiple instruction\nsingle data\nSIMD\nSingle instruction\nmultiple dataMIMD\nMultiple instruction\nmultiple dataDataFigure 1.24 Flynn’s Taxonomy categorizes \ndifferent parallel architectures. A serial \narchitecture is single data, single instruction \n(SISD). Two categories only have partial \nparallelization in that either the instructions \nor data are parallel, but the other is serial. \n30 CHAPTER  1Why parallel computing?\nexample is a redundant computation on the same data. This is used in highly fault-tol-\nerant approaches such as spacecraft controllers. Because spacecraft are in high radia-\ntion environments, these often run two copies of each calculation and compare the\noutput of the two.\n Vectorization is a prime example of SIMD in which the same instruction is per-\nformed across multiple data. A variant of SIMD is single instruction, multi-thread\n(SIMT), which is commonly used to describe GPU work groups. \n The final category has parallelization in both instructions and data and is referred\nto as MIMD. This category describes multi-core parallel architectures that comprise\nthe majority of large parallel systems. \n1.5 Parallel strategies\nSo far in our initial example in section 1.3.1, we looked at data parallelization for cells\nor pixels. But data parallelization can also be used for particles and other data objects.\nData parallelization is the most common approach and often the simplest. Essentially,\neach process executes the same program but operates on a unique subset of data as\nillustrated in the upper right of figure 1.25. The data parallel approach has the advan-\ntage that it scales well as the problem size and number of processors grow. \n Another approach is task parallelism . This includes the main controller with worker\nthreads, pipeline, or bucket-brigade strategies also shown in figure 1.25. The pipeline\napproach is used in superscalar processors where address and integer calculations\nare done with a separate logic unit rather than the floating-point processer, allowing\nthese calculations to be done in parallel. The bucket-brigade uses each processor to\noperate on and transform the data in a sequence of operations. In the main-worker\napproach, one processor schedules and distributes the tasks for all the workers, and\neach worker checks for the next work item as it returns the previous completed task.\nIt is also possible to combine different parallel strategies to expose a greater degree\nof parallelism.\nProcessorPipeline or bucket brigadeMainWorker\nWorkData Proc\nData parallelData Proc\nData Proc\nData Proc\nData Proc\nData ProcWorker\nWorker\nWorker\nProcessor ProcessorFigure 1.25 Various task and data \nparallel strategies, including main-\nworker, pipeline or bucket-brigade \nand data parallelism\n31 Parallel speedup versus comparative speedups: Two different measures\n1.6 Parallel speedup versus comparative speedups: \nTwo different measures\nWe will present a lot of comparative performance numbers and speedups throughout\nthis book. Often the term speedup  is used to compare two different run times with little\nexplanation or context to fully understand what it means. Speedup is a general term\nthat is used in many contexts such as quantifying the effects of optimization, for exam-\nple. To clarify the difference between the two major categories of parallel performance\nnumbers, we’ll define two different terms.\nParallel speedup —We should really call this serial-to-parallel speedup . The speedup\nis relative to a baseline serial run on a standard platform, usually a single CPU.\nThe parallel speedup can be due to running on a GPU or with OpenMP or MPI\non all the cores on the node of a computer system.\nComparative speedup —We should really call this comparative speedup between archi-\ntectures . This is usually a performance comparison between two parallel implemen-\ntations or other comparison between reasonably constrained sets of hardware. For\nexample, it may be between a parallel MPI implementation on all the cores of\nthe node of a computer versus the GPU(s) on a node.\nThese two categories of performance comparisons represent two different goals. The\nfirst is to understand how much speedup can be obtained through adding a particular\ntype of parallelism. It is not a fair comparison between architectures, however. It is\nabout parallel speedup. For example, comparing a GPU run time to a serial CPU run\nis not a fair comparison between a multi-core CPU and the GPU. Comparative speed-\nups between architectures are more appropriate when trying to compare a multi-core\nCPU to the performance of one or more GPUs on a node. \n In recent years, some have normalized the two architectures so that relative perfor-\nmance is compared for similar power or energy requirements rather than an arbitrary\nnode. Still, there are so many different architectures and possible combinations that\nany performance numbers to justify a conclusion can be obtained. You can pick a fast\nGPU and a slow CPU or a quad-core CPU versus a 16-core processor. We are therefore\nsuggesting you add the following terms  in parenthesis  to performance comparisons to\nhelp give these more context: \nAdd (Best 2016) to each term. For example, parallel speedup (Best 2016) and\ncomparative speedup (Best 2016) would indicate that the comparison is between\nthe best hardware released in a particular year (2016 in this example), where\nyou might compare a high-end GPU to a high-end CPU. \nAdd (Common 2016) or (2016) if the two architectures were released in 2016\nbut are not the highest-end hardware. This might be relevant to developers\nand users who have more mainstream parts than that found in the top-end\nsystems.\n32 CHAPTER  1Why parallel computing?\nAdd (Mac 2016) if the GPU and the CPU were released in a 2016 Mac laptop or\ndesktop, or something similar for other brands with fixed components over a\nperiod of time (2016 in this example). Performance comparisons of this type\nare valuable to users of a commonly available system.\nAdd (GPU 2016:CPU 2013) to show that there is a possible mismatch in the\nhardware release year (2016 versus 2013 in this example) of the components\nbeing compared.\nNo qualifications added to comparison numbers. Who knows what the numbers\nmean?\nBecause of the explosion in CPU and GPU models, performance numbers will neces-\nsarily be more of a comparison between apples and oranges rather than a well-defined\nmetric. But for more formal settings, we should at least indicate the nature of the com-\nparison so that others have a better idea of the meaning of the numbers and to be\nmore fair to the hardware vendors.\n1.7 What will you learn in this book?\nThis book is written with the application code developer in mind and no previous\nknowledge of parallel computing is assumed. You should simply have a desire to\nimprove the performance and scalability of your application. The application areas\ninclude scientific computing, machine learning, and analysis of big data on systems\nranging from a desktop to the largest supercomputers.\n To fully benefit from this book, readers should be proficient programmers, prefer-\nably with a compiled, HPC language such as C, C++, or Fortran. We also assume a\nrudimentary knowledge of hardware architectures. In addition, readers should be\ncomfortable with computer technology terms such as bits, bytes, ops, cache, RAM, etc.\nIt is also helpful to have a basic understanding of the functions of an OS and how it\nmanages and interfaces with the hardware components. After reading this book, some\nof the skills you will gain include\nDetermining when message passing (MPI) is more appropriate than threading\n(OpenMP) and vice-versa\nEstimating how much speedup is possible with vectorization\nDiscerning which sections of your application have the most potential for\nspeedup\nDeciding when it might be beneficial to leverage a GPU to accelerate your\napplication\nEstablishing what is the peak potential performance for your application\nEstimating the energy cost for your application\nEven after this first chapter, you should feel comfortable with the different approaches\nto parallel programming. We suggest that you work through the exercises in each\nchapter to help you integrate the many concepts that we present. If you are beginning",10515
26-1.7.2 Exercises.pdf,26-1.7.2 Exercises,,0
27-Summary.pdf,27-Summary,"33 Summary\nto feel a little overwhelmed by the complexity of the current parallel architectures,\nyou are not alone. It’s challenging to grasp all the possibilities. We’ll break it down,\npiece-by-piece, in the following chapters to make it easier for you.\n1.7.1 Additional reading\nA good basic introduction to parallel computing can be found on the Lawrence Liver-\nmore National Laboratory website: \nBlaise Barney, “Introduction to Parallel Computing.” https:/ /computing.llnl.gov/\ntutorials/parallel_comp/ .\n1.7.2 Exercises\n1What are some other examples of parallel operations in your daily life? How\nwould you classify your example? What does the parallel design appear to opti-\nmize for? Can you compute a parallel speedup for this example?\n2For your desktop, laptop, or cell phone, what is the theoretical parallel process-\ning power of your system in comparison to its serial processing power? What\nkinds of parallel hardware are present in it?\n3Which parallel strategies do you see in the store checkout example in figure\n1.1? Are there some present parallel strategies that are not shown? How about\nin your examples from exercise 1?\n4You have an image-processing application that needs to process 1,000 images\ndaily, which are 4 mebibytes (MiB, 220 or 1,048,576 bytes) each in size. It takes\n10 min in serial to process each image. Your cluster is composed of multi-core\nnodes with 16 cores and a total of 16 gibibytes (GiB, 230 bytes, or 1024 mebib-\nytes) of main memory storage per node. (Note that we use the proper binary\nterms, MiB and GiB, rather than MB and GB, which are the metric terms for 106\nand 109 bytes, respectively.)\naWhat parallel processing design best handles this workload?\nbNow customer demand increases by 10x. Does your design handle this? What\nchanges would you have to make?\n5An Intel Xeon E5-4660 processor has a thermal design power of 130 W; this is\nthe average power consumption rate when all 16 cores are used. NVIDIA’s Tesla\nV100 GPU and AMD’s MI25 Radeon GPU have a thermal design power of 300\nW. Suppose you port your software to use one of these GPUs. How much faster\nshould your application run on the GPU to be considered more energy effi-\ncient than your 16-core CPU application?\nSummary\nBecause this is an era where most of the compute capabilities of hardware are\nonly accessible through parallelism, programmers should be well versed in the\ntechniques used to exploit parallelism.\n34 CHAPTER  1Why parallel computing?\nApplications must have parallel work. The most important job of a parallel pro-\ngrammer is to expose more parallelism.\nImprovements to hardware are nearly all-enhancing parallel components. Rely-\ning on increasing serial performance will not result in future speedup. The key\nto increasing application performance will all be in the parallel realm.\nA variety of parallel software languages are emerging to help access the hard-\nware capabilities. Programmers should know which are suitable for different\nsituations.",3031
28-2.1.1 Version control Creating a safety vault for your parallel code.pdf,28-2.1.1 Version control Creating a safety vault for your parallel code,"35Planning for\nparallelization\nDeveloping a parallel application or making an existing application run in parallel\ncan feel challenging at first. Often, developers new to parallelism are unsure of\nwhere to begin and what pitfalls they might encounter. This chapter focuses on a\nworkflow model for developing parallel applications as illustrated in figure 2.1.\nThis model provides the context for where to get started and how to maintainThis chapter covers\nPlanning steps for a parallel project\nVersion control and team development workflows\nUnderstanding performance capabilities and \nlimitations\nDeveloping a plan to parallelize a routine\nProfilePlanCommitPreparation\nParallel\ndevelopment\ncycleImplementFigure 2.1 Our suggested parallel development \nworkflow begins with preparing the application \nand then repeating four steps to incrementally \nparallelize an application. This workflow is \nparticularly suited to an agile project \nmanagement technique.\n36 CHAPTER  2Planning for parallelization\nprogress in developing your parallel application. Generally, it is best to implement\nparallelism in small increments so that if problems are encountered, the last few\ncommits can be reversed. This kind of pattern is suited to agile project management\ntechniques.\n Let’s imagine that you have been assigned a new project to speed up and paral-\nlelize an application from the spatial mesh presented in figure 1.9 (the Krakatau vol-\ncano example). This could be an image detection algorithm, a scientific simulation of\nthe ash plume, or a model of the resulting tsunami waves, or all three of these. What\nsteps can you take to have a successful parallelism project?\n It is tempting to just jump into the project. But without thought and preparation,\nyou greatly reduce your chance of success. As a start, you will need a project plan for\nthis parallelism effort, so we begin here with a high-level overview of the steps in this\nworkflow. Then we’ll dive deeper into each step as this chapter progresses, with a\nfocus on the characteristics typical for a parallel project.\n2.1 Approaching a new project: The preparation\nFigure 2.2 presents the recommended components in the preparation step. These are\nthe items proven to be important specifically for parallelization projects.\n At this stage, you will need to set up version control, develop a test suite for your\napplication, and clean up existing code. Version control allows you to track the changesRapid development: The parallel workflow\nYou first need to prepare your team and your application for rapid development.\nBecause you have an existing serial application that works on the spatial mesh from\nfigure 1.9, there will likely be lots of small changes with frequent tests to ensure that\nthe results do not change. Code preparation includes setting up version control,\ndeveloping a test suite, and ensuring code quality and portability. Team preparation\nwill be centered around processes for the development procedures. As always, proj-\nect management will address task management and scope control.\nTo set the stage for the development cycle, you will need to determine the capabil-\nities of the computing resources available, the demands of your application, and\nyour performance requirements. System benchmarking helps to determine com-\npute resource limitations, while profiling aids your understanding of the application's\ndemands and its most expensive computational kernels. Computational kernels  refer\nto sections of the application that are both computationally intensive and conceptu-\nally self-contained. \nFrom the kernel profiles, you will plan the tasks for parallelizing routines and imple-\nmenting the changes. The implementation stage is only complete once the routine is\nparallelized and the code maintains portability and correctness. With these require-\nments satisfied, the changes will be committed to a version control system. After\ncommitting the incremental changes, the process begins again with an application\nand kernel profile.\n37 Approaching a new project: The preparation\nyou make to your application over time. It permits you to quickly undo mistakes and\ntrack down bugs in your code at a later date. A test suite allows you to verify the cor-\nrectness of your application with each change that is made to your code. When cou-\npled with version control, this can be a powerful setup for rapidly developing your\napplication. \n With version control and code testing in place, you can now tackle the task of\ncleaning up your code. Good code is easy to modify and extend, and does not exhibit\nunpredictable behavior. Good, clean code can be ensured with modularity and checks\nfor memory issues. Modularity  means that you implement kernels as independent sub-\nroutines or functions with well-defined input and output. Memory issues can include\nmemory leaks, out-of-bounds memory access, and use of uninitialized memory. Start-\ning your parallelism work with predictable and quality code promotes rapid progress\nand predictable development cycles. It is hard to match your serial code if the original\nresults are due to a programming error.\n Finally, you will want to make sure your code is portable . This means that multiple\ncompilers can compile your code. Having and maintaining compiler portability allows\nyour application to target additional platforms, beyond the one you may currently\nhave in mind. Further, experience shows that developing code to work with multiple\ncompilers helps to find bugs before these are committed to your code’s version his-\ntory. With the high performance computing landscape changing rapidly, portability\nallows you to adapt to changes much quicker down the line.\n It is not unusual that the preparation time rivals that spent on the actual parallel-\nism, especially for complex code. Including this preparation in your project scope and\ntime estimates avoids frustrations with your project’s progress. In this chapter, we\nassume that you are starting from a serial or prototype application. However, you can\nstill benefit from this workflow strategy even if you’ve already started parallelizing your\ncode. Next, we discuss the four components of project preparation.\n2.1.1 Version control: Creating a safety vault for your parallel code\nIt is inevitable with the many changes that occur during parallelism that you will sud-\ndenly find the code is broken or returning different results. Being able to recover\nfrom this situation by backing up to a working version is critically important. \nNOTE Check to see what kind of version control is in place for your applica-\ntion before beginning any parallelism work.\nFor your image detection project in our scenario, you find that there is already a ver-\nsion control system in place. But the ash plume model never had any version control.TestingPreparation\nVersion\ncontrolCode\nqualityCode\nportabilityFigure 2.2 The recommended preparation \ncomponents address issues that are important \nfor parallel code development. \n38 CHAPTER  2Planning for parallelization\nAs you dig deeper, you find that there are actually four versions of the ash plume code\nin various developer’s directories. When there is a version control system in operation,\nyou may want to review the processes your team uses for day-to-day operations. Per-\nhaps the team thinks it is a good idea to switch to a “pull request” model, where\nchanges are posted for review by other team members before being committed. Or\nyou and your team may feel that the direct commit of the “push” model is more com-\npatible with the rapid, small commits of parallelism tasks. In the push model, commits\nare made directly to the repository without review. In our example of the ash plume\napplication without version control, the priority is to get something in place to tame\nthe uncontrolled divergence of code among developers.\n There are many options for version control. If you have no other preferences, we\nwould suggest Git, the most common distributed version control system. A distributed\nversion control system  is one that allows multiple repository databases, rather than a sin-\ngle centralized system used in centralized version control . Distributed version control is\nadvantageous for open source projects and where developers work on laptops, in\nremote locations, or other situations where they are not connected to a network or\nclose to the central repository. In today’s development environment, this is a huge\nadvantage. But it comes with the cost of additional complexity. Centralized version\ncontrol is still popular and more appropriate for the corporate environment because\nthere is only one place where all the information about the source code exists. Cen-\ntralized control also provides better security and protection for proprietary software.\n There are many good books, blogs, and other resources on how to use Git; we list a\nfew at the end of the chapter. We also list some other common version control systems\nin chapter 17. These include free distributed version control systems such as Mercu-\nrial and Git, commercial systems such as PerForce and ClearCase, and for centralized\nversion control, CVS and SVN. Regardless of which system you use, you and your team\nshould commit frequently. The following scenario is especially common with parallel-\nism tasks:\nI’ll commit after I add the next small change. . . .\nJust one more. . . . Then all of a sudden the code is not working.\nIt’s too late to commit now!\nThis happens to me far too often. So I try to avoid the problem by committing regularly. \nTIP If you do not want lots of small commits in the main repository, you can\ncollapse the commits with some version control systems such as Git, or you\ncan maintain a temporary version control system just for yourself.\nThe commit message is where the commit author can communicate what task is being\naddressed and why certain changes were made, whether for self or for current or\nfuture team members. Every team has their own preference for how detailed these\nmessages should be; we recommend using as much detail as possible in your commit\nmessages. This is your opportunity to save yourself from later confusion by being dili-\ngent today.",10330
29-2.1.2 Test suites The first step to creating a robust reliable application.pdf,29-2.1.2 Test suites The first step to creating a robust reliable application,"39 Approaching a new project: The preparation\n In general, commit messages include a summary and a body. The summary pro-\nvides a short statement indicating clearly what new changes the commit covers. Addi-\ntionally, if you use an issue tracking system, the summary line will reference an issue\nnumber from that system. Finally, the body contains most of the “why” and “how”\nbehind the commit.\nWith a plan for version control and at least a rough agreement on your team’s devel-\nopment processes, we are ready to move on to the next step.\n2.1.2 Test suites: The first step to creating a robust, reliable \napplication\nA test suite  is a set of problems that exercise parts of an application to guarantee that\nrelated parts of the code still work. Test suites are a necessity for all but the simplest of\ncodes. With each change, you should test to see that the results that you get are the\nsame. This sounds simple, but some code can reach slightly different results with dif-\nferent compilers and numbers of processors. Examples of commit messages\nBad commit message: \nFixed a bug\nGood commit message: \nFixed the race condition in the OpenMP version of the blur operator\nGreat commit message:\n[Issue #21] Fixed the race condition in the OpenMP version of the \nblur operator.\n* The race condition was causing non-reproducible results amongst \nGCC, Intel, and PGI compilers. To fix this, an OMP BARRIER was \nintroduced to force threads to synchronize just before calculating \nthe weighted stencil sum.\n* Confirmed that the code builds and runs with GCC, Intel, and PGI \ncompilers and produces consistent results.\nThe first message doesn’t really help anyone understand what bug was fixed. The\nsecond message helps pinpoint the resolution to problems that involve race condi-\ntions in the blur operator. The last message references an issue number (#21) in an\noutside issue tracking system and provides the commit summary on the first line. The\ncommit body, the two bullet points beneath the summary, provides more details\nabout what specifically was needed and why, and indicates to other developers that\nyou took the time to test your version before committing it.\n40 CHAPTER  2Planning for parallelization\nIn the following sections, we’ll discuss why such differences can arise, how to deter-\nmine which variations are reasonable, and how to design tests that catch real bugs\nbefore these are committed to your repository.\nUNDERSTANDING  CHANGES  IN RESULTS  DUE TO PARALLELISM\nThe parallelism process inherently changes the order of operations, which slightly\nmodifies the numerical results. But errors in parallelism also generate small differ-\nences. This is crucial to understand in parallel code development because we need to\ncompare to a single processor run to determine if our parallelism coding is correct.Example: Krakatau scenario test for validated results\nYour project has an ocean wave simulation application that generates validated\nresults. Validated results  are simulation results that are compared to experimental or\nreal-world data. Simulation code that has been validated is valuable. You don’t want\nto lose that while you parallelize the code.\nIn our scenario, you and your team used two different compilers for development and\nproduction. The first is the C compiler in the GNU Compiler Collection (GCC), the ubiq-\nuitous, freely-available compiler dispersed with all Linux distributions and many other\noperating systems. The C compiler is colloquially referred to as the GCC compiler.\nYour application also uses the commercially available Intel C compiler. \nThe following figure shows the hypothetical results for the validated test problem that\npredicts wave height and total mass. The output varies slightly depending on which\ncompiler and the number of processors used in the simulation.\nIn this example, there are variations in the two metrics reported from the program.\nWithout additional information, it is difficult to determine which is correct and which\nvariations in the solution are acceptable. In general, differences in your program out-\nput can be due to\nChanges in the compiler or the compiler version\nChanges in hardware\nCompiler optimizations or small differences between compilers or compiler\nversions\nChanges in the order of operations, especially due to code parallelismWave Height\n4.234732 4\nTotal Mass\n293548. 218Wave Height\n4.234732 5\nTotal Mass\n293548. 219Intel compiler GCC compiler GCC compiler\n1 processor 1 processor 4 processorsWave Height\n4.234732 7\nTotal Mass\n293548. 384\nWhich differences between calculations with various compilers and \nnumbers of processors are acceptable?\n41 Approaching a new project: The preparation\nWe’ll discuss a way to reduce the numerical errors so that the parallelism errors are\nmore obvious in section 5.7, when we discuss techniques for global sums.\n For our test suite, we will need a tool that compares numerical fields with a small\ntolerance for differences. In the past, test suite developers would have to create a tool\nfor this purpose, but a few numerical diff utilities have appeared on the market in\nrecent years. Two such tools are\nNumdiff from https:/ /www.nongnu.org/numdiff/\nndiff from https:/ /www.math.utah.edu/~beebe/software/ndiff/  \nAlternatively, if your code outputs its state in HDF5 or NetCDF files, these formats\ncome with utilities that allow you to compare values stored in the files with varying\ntolerances.\nHDF5® is version 5 of the software originally known as Hierarchical Data For-\nmat, now called HDF. It is freely available from The HDF Group ( https:/ /www\n.hdfgroup.org/ ) and is a common format used to output large data files. \nNetCDF or the Network Common Data Form is an alternate format used by the\nclimate and geosciences community. Current versions of NetCDF are built on\ntop of HDF5. You can find these libraries and data formats at the Unidata Pro-\ngram Center’s website ( https:/ /www.unidata.ucar.edu/software/netcdf/ ).\nBoth of these file formats use binary data for speed and efficiency. Binary data is the\nmachine representation of the data. This format just looks like gibberish to you and\nme, but HDF5 has some useful utilities that allow us to look at what’s inside. The h5ls\nutility lists the objects in the file, such as the names of all the data arrays. The h5dump\nutility dumps the data in each object or array. And most importantly for our purposes\nhere, the h5diff utility compares two HDF files and reports the difference above a\nnumeric tolerance. HDF5 and NetCDF along with other parallel input/output (I/O)\ntopics will be discussed in more detail in chapter 16.\nUSING CM AKE AND CTEST TO AUTOMATICALLY  TEST YOUR CODE\nMany testing systems have become available in recent years. This includes CTest, Goo-\ngle test, pFUnit test, and others. You can find more information on tools in chapter\n17. For now, let’s look at a system created using CTest and ndiff.  \n CTest is a component of the CMake system. CMake is a configuration system that\nadapts generated makefiles to different systems and compilers. Incorporating the\nCTest testing system into CMake couples the two tightly together into a unified system.\nThis provides a lot of convenience to the developer. The process of implementing\ntests using CTest is relatively easy. The individual tests are written as any sequence of\ncommands. To incorporate these into the CMake system requires adding the follow-\ning to the CMakeLists.txt:\nenable_testing()\nadd_test(<testname>  <executable  name>  <arguments  to executable>)\n42 CHAPTER  2Planning for parallelization\nThen you can invoke the tests with make  test , ctest , or you can select individual tests\nwith ctest  -R mpi, where mpi is a regular expression that runs any matching test names.\nLet’s just walk through an example of creating a test using the CTest system.\nMake two source files as shown in listing 2.1 to create applications for this simple test-\ning system. We’ll use a timer to produce small differences in output from both a serial\nand a parallel program. Note that you’ll find the source code for this chapter at\nhttps:/ /github.com/EssentialsofParallelComputing/Chapter2 .\nC Program, TimeIt.c\n 1 #include <unistd.h>\n 2 #include <stdio.h>\n 3 #include <time.h>\n 4 int main(int argc, char *argv[]){\n 5    struct timespec tstart, tstop, tresult;\n 6    clock_gettime(CLOCK_MONOTONIC, &tstart);   \n 7    sleep(10);                                 \n 8    clock_gettime(CLOCK_MONOTONIC, &tstop);    \n 9    tresult.tv_sec = \n         tstop.tv_sec - tstart.tv_sec;      \n10    tresult.tv_usec = \n         tstop.tv_nsec - tstart.tv_nsec;    \n11    printf(""Elapsed time is %f secs\n"", \n         (double)tresult.tv_sec +           \n12       (double)tresult.tv_nsec*1.0e-9);   \n13 }\nMPI Program, MPITimeIt.c\n 1 #include <unistd.h>\n 2 #include <stdio.h>\n 3 #include <mpi.h>\n 4 int main(int argc, char *argv[]){\n 5    int mype;\n 6    MPI_Init(&argc, &argv);                 \n 7    MPI_Comm_rank(MPI_COMM_WORLD, &mype);   \n 8    double t1, t2;Example: CTest prerequisites\nYou will need MPI, CMake, and ndiff installed to run this example. For MPI (Message\nPassing Interface), we’ll use OpenMPI 4.0.0 and CMake 3.13.3 (includes CTest) on\nthe Mac with older versions on Ubuntu. We’ll use the GCC compiler, version 8,\ninstalled on a Mac rather than the default compiler. Then OpenMPI, CMake, and GCC\n(GNU Compiler Collection) are installed with a package manager. We’ll use Home-\nbrew on the Mac and Apt, and Synaptic on Ubuntu Linux. Be sure to get the develop-\nment headers from libopenmpi-dev if these are split out from the run time. ndiff is\ninstalled manually by downloading the tool from https://www.math.utah.edu/~beebe/\nsoftware/ndiff/  and running ./configure , make, and make install .\nListing 2.1 Simple timing programs for demonstrating the testing system\nStarts timer, calls sleep, \nthen stops the timer\nTimer has two values for resolution \nand to prevent overflows.\nPrints calculated time\nInitializes MPI and \ngets processor rank\n43 Approaching a new project: The preparation\n 9    t1 = MPI_Wtime();     \n10    sleep(10);            \n11    t2 = MPI_Wtime();     \n12    if (mype == 0)\n         printf( ""Elapsed time is %f secs\n"",    \n                   t2 - t1);                     \n13    MPI_Finalize();     \n14 }\nNow you need a test script that runs the applications and produces a few different out-\nput files. After these run, there should be numerical comparisons of the output. Here\nis an example of the process you can put in a file called mympiapp.ctest. You should\ndo a chmod  +x to make it executable. \nmympiapp.ctest\n 1 #!/bin/sh\n 2 ./TimeIt > run0.out     \n 3 mpirun -n 1 ./MPITimeIt > run1.out      \n 4 mpirun -n 2 ./MPITimeIt > run2.out      \n 5 ndiff --relative-error 1.0e-4 run1.out run2.out    \n 6 test1=$?   \n 7 ndiff --relative-error 1.0e-4 run0.out run2.out    \n 8 test2=$?   \n 9 exit ""$(($test1+$test2))""    \nThis test first compares the output for a parallel job with 1 and 2 processors with a tol-\nerance of 0.1% on line 5. Then it compares the serial run to the 2 processor parallel\njob on line 7. To get the tests to fail, try reducing the tolerance to 1.0e–5. CTest uses\nthe exit code on line 9 to report pass or fail. The simplest way to add a bunch of CTest\nfiles to the test suite is to use a loop that finds all the files ending in .ctest and adds\nthese to the CTest list. Here is an example of a CMakeLists.txt file with the additional\ninstructions to create the two applications:\nCMakeLists.txt\n 1 cmake_minimum_required (VERSION 3.0)\n 2 project (TimeIt)\n 3\n 4 enable_testing()     \n 5\n 6 find_package(MPI)    \n 7\n 8 add_executable(TimeIt TimeIt.c)      \n 9\n10 add_executable(MPITimeIt MPITimeIt.c)\n11 target_include_directories(MPITimeIt PUBLIC.       \n      ${MPI_INCLUDE_PATH})                            \n12 target_link_libraries(MPITimeIt ${MPI_LIBRARIES})  \n13Starts timer, calls sleep, \nthen stops the timer\nPrints timing output \nfrom first processor\nShuts down MPI\nRuns a \nserial testRuns the first MPI \ntest on 1 processor\nRuns the second MPI \ntest on 2 processors\nCompares the output \nfor the two MPI jobs \nto get the test to failCaptures\nthe status\nset by\nthe ndiff\ncommand Compares the serial \noutput to the 2 \nprocessor runExits with the cumulative \nstatus code so CTest can \nreport pass or fail\nEnables CTest \nfunctionality in CMake\nCMake built-in routine to \nfind most MPI packages\nAdds TimeIt and MPITimeIt build \ntargets with their source code files \nNeeds an include path \nto the mpi.h file and \nto the MPI library\n44 CHAPTER  2Planning for parallelization\n14 file(GLOB TESTFILES RELATIVE\n     ""${CMAKE_CURRENT_SOURCE_DIR}"" ""*.ctest"")        \n15 foreach(TESTFILE ${TESTFILES})                    \n16    add_test(NAME ${TESTFILE} WORKING_DIRECTORY\n         ${CMAKE_BINARY_DIR}                         \n17    COMMAND sh\n         ${CMAKE_CURRENT_SOURCE_DIR}/${TESTFILE})    \n18 endforeach()                                      \n19\n20 add_custom_target(distclean \n      COMMAND rm -rf CMakeCache.txt CMakeFiles     \n21    CTestTestfile.cmake Makefile Testing \n         cmake_install.cmake)                      \nThe find_package(MPI)  command on line 6 defines MPI_FOUND, MPI_INCLUDE_\nPATH, and MPI_LIBRARIES. These variables include the language in newer CMake\nversions of MPI_<lang>_INCLUDE_PATH, and MPI_<lang>_LIBRARIES so that there\nare different paths for C, C++, and Fortran. Now all that remains is to run the test with\nmkdir build && cd build\ncmake ..\nmake\nmake test\nor\nctest\nYou can also get the output for failed tests with\nctest --output-on-failure\nYou should get some results like the following:\nRunning tests...\nTest project /Users/brobey/Programs/RunDiff\n    Start 1: mpitest.ctest\n1/1 Test #1: mpitest.ctest ....................   Passed   30.24 sec\n100% tests passed, 0 tests failed out of 1\nTotal Test time (real) =  30.24 sec\nThis test is based on the sleep function and timers, so it may or may not pass. Test\nresults are in Testing/Temporary/*.\n In this test, we compared the output between individual runs of the application. It\nis also good practice to store a gold standard file from one of the runs along with the\ntest script to compare against as well. This comparison detects changes that will cause\na new version of the application to get different results than earlier versions. WhenGets all files with the \nextension .ctest and \nadds those to the \ntest list for CTest\nA custom command, \ndistclean, removes \ncreated files.\n45 Approaching a new project: The preparation\nthis happens, it is a red flag; check if the new version is still correct. If so, you should\nupdate the gold standard.\n Your test suite should exercise as many parts of the code as is practical. The metric\ncode coverage  quantifies how well the test suite does its task, which is expressed as a per-\ncentage of the lines of source code. There is an old saying from test developers that\nthe part of the code that doesn’t have a test is broken because even if it isn’t now, it\nwill be eventually. With all of the changes made when parallelizing code, breakage is\ninevitable. While high code coverage is important, for our parallelism efforts, it is\nmore critical that there are tests for the parts of the code you are parallelizing. Many\ncompilers have the capability to generate code coverage statistics. For GCC, gcov is the\nprofiling tool, and for Intel, it is Codecov. We’ll take a look at how this works for GCC.\nUNDERSTANDING  THE DIFFERENT  KINDS  OF CODE TESTS\nThere are also different kinds of testing systems. In this section, we’ll cover the follow-\ning types:\nRegression tests —Run at regular intervals to keep the code from backsliding. This\nis typically done nightly or weekly using the cron job scheduler that launches\njobs at specified times. \nUnit tests —Tests the operation of subroutines or other small parts of code during\ndevelopment. \nContinuous integration tests —Gaining in popularity, these tests are automatically\ntriggered to run by a commit to the code. \nCommit tests —A small set of tests that can be run from the command line in a\nfairly short time and are used before commits. \nAll of these testing types are important for a project and, rather than just relying on\none, these should be used together as figure 2.3 illustrates. Testing is particularlyCode coverage with GCC\n1Add the flags -fprofile-arcs  and -ftest-coverage  when compiling and linking\n2Run the instrumented executable on a series of tests\n3Run gcov <source.c>  to get the coverage for each file\nNOTE For builds with CMake, add an extra .c extension to the source file-\nname; for example, gcov CMakeFiles/stream_triad.dir/stream_triad.c.c han-\ndles the extension added by CMake.\n4You will get output something like this:\n88.89% of 9 source lines executed in file <source>.c\nCreating <source>.c.gcov\nThe gcov output file contains a listing with each line prepended with the number of\ntimes it was executed.\n46 CHAPTER  2Planning for parallelization\nimportant for parallel applications because detecting bugs earlier in the development\ncycle means that you are not debugging 1,000 processors 6 hours into a run.\n Unit tests are best created as you develop the code. True aficionados of unit tests\nuse test-driven development (TDD), where the tests are created first and then the\ncode is written to pass these. Incorporating these type of tests into parallel code devel-\nopment includes testing their operation in the parallel language and implementation.\nIdentifying problems at this level is far easier to resolve.\n Commit tests are the first tests that you should add to a project to be heavily used\nin the code modification phase. These tests should exercise all of the routines in the\ncode. By having these tests readily available, team members can run these before mak-\ning a commit to the repository. We recommend that developers invoke these tests\nfrom the command line like a Bash or Python script, or a makefile, prior to a commit.\nExample: A development workflow with commit tests using CMake and CTest\nTo make a commit test within CMakeLists.txt, create the three files shown in the fol-\nlowing listing. Use the Timeit.c from the previous test, but change the sleep interval\nfrom 10 to 30.\nblur_short.ctest\n1 #!/bin/sh\n2 make\nblur_long.ctest\n1 #!/bin/sh\n2 ./TimeIt\nCMakeLists.txt\n 1 cmake_minimum_required (VERSION 3.0)\n 2 project (TimeIt)\n 3\n 4 enable_testing()     \n 5\n 6 add_executable(TimeIt TimeIt.c)Development phase\nCode development\nunit testing\nModiﬁcation phase\nCode changes\ncommit testCommitsContinuous\nintegration\ntestsQuality\ncode\nBug ﬁxes\nAdd tests to verify bug ﬁxes\nregression testingFigure 2.3 The different test \ntypes address different parts of \ncode development to create a \nhigh quality code that is always \nready to release.\nCreating a commit test with CTest\nEnables CTest \nfunctionality in CMake\n47 Approaching a new project: The preparation\nThe commit tests can be run with ctest  -R commit  or with the custom target added to\nthe CMakeLists.txt with make  commit_tests . A make  test  or ctest  command runs all\nthe tests including the long test, which takes a while. The commit test command picks\nout the tests with commit  in the name to get a set of tests that covers critical functional-\nity but runs a little faster. Now the workflow is\n1Edit the source code: vi mysource.c\n2Build code: make\n3Run the commit tests: make  commit_tests\n4Commit the code changes: git commit\nAnd repeat. Continuous integration tests are invoked by a commit to the main code\nrepository. This is an additional guard against committing bad code. The tests can be\nthe same as the commit tests or can be more extensive. Top continuous integration\ntools for these types of tests are\nJenkins ( https:/ /www.jenkins.io )\nTravis CI for GitHub and Bitbucket ( https:/ /travis-ci.com )\nGitLab CI ( https:/ /about.gitlab.com/stages-devops-lifecycle/continuous-\nintegration/ )\nCircleCI ( https:/ /circleci.com )\nRegression tests are usually set up to run overnight through a cron job. This means\nthat the test suites can be more extensive than the test suites for other testing types.\nThese tests can be longer but should complete by the morning report. Additional\ntests, such as memory checks and code coverage are often run as regression tests due\nto the longer run times and the periodicity of the reports. The results of regression\ntests are often tracked over time and a “wall of passes” is considered as an indication\nof the project’s well-being.  7\n 8 add_test(NAME blur_short_commit WORKING_DIRECTORY\n         ${CMAKE_BINARY_DIRECTORY}                   \n 9    COMMAND \n   ${CMAKE_CURRENT_SOURCE_DIR}/blur_short.ctest)     \n10 add_test(NAME blur_long WORKING_DIRECTORY\n         ${CMAKE_BINARY_DIRECTORY}                   \n11    COMMAND \n   ${CMAKE_CURRENT_SOURCE_DIR}/blur_long.ctest)      \n12\n13 add_custom_target(commit_tests\n      COMMAND ctest -R commit DEPENDS <myapp>)    \n14\n15 add_custom_target(distclean \n      COMMAND rm -rf CMakeCache.txt CMakeFiles    \n16      CTestTestfile.cmake Makefile Testing\n        cmake_install.cmake)        Adds two tests, \none with commit \nin the name\nCustom target \ncommit_tests runs all \ntests with “commit” in \nthe name.\nA custom command, \ndistclean, removes the \ncreated files.",21683
30-2.1.3 Finding and fixing memory issues.pdf,30-2.1.3 Finding and fixing memory issues,"48 CHAPTER  2Planning for parallelization\nFURTHER  REQUIREMENTS  OF AN IDEAL TESTING  SYSTEM\nWhile the testing system as described previously is sufficient for most purposes, there\nis more that can be helpful for larger HPC projects. These types of HPC projects can\nhave extensive test suites and might also need to be run in a batch system to access\nlarger resources. \n The Collaborative Testing System (CTS) at https:/ /sourceforge.net/projects/\nctsproject/  provides an example of a system that was developed for these demands. It\nuses a Perl script to run a fixed set of test servers, typically 10, launching the tests in\nparallel to a batch system. As each test completes, it launches the next. This avoids\nflooding the system with jobs all at once. The CTS system also autodetects the batch\nsystem and type of MPI and adjusts the scripts for each system. The reporting system\nuses cron jobs with the tests launched early in the overnight period. The cross-plat-\nform report launches in the morning and then is sent out.\n2.1.3 Finding and fixing memory issues\nGood code quality is paramount. Parallelizing often causes any code flaw to appear;\nthis might be uninitialized memory or memory overwrites. \nUninitialized memory is memory that is accessed before its values are set.  When you allo-\ncate memory to your program, it gets whatever values are in those memory loca-\ntions. This leads to unpredictable behavior if it is used before being set. \nMemory overwrites occur when data is written to a memory location that isn’t owned by a\nvariable.  An example of this is writing past the bounds of an array or string. \nTo catch these sorts of problems, we suggest using memory correctness tools to thor-\noughly check your code. One of the best of these is the freely-available Valgrind pro-\ngram. Valgrind is an instrumentation framework that operates at the machine-code\nlevel by executing instructions through a synthetic CPU. There are many tools that\nhave been developed under the Valgrind umbrella. The first step is to install Valgrind\non your system using a package manager. If you are running the latest version of\nmacOS, you may find that it takes a few months for Valgrind to be ported to the newExample: Krakatau scenario test suite for HPC projects\nAfter reviewing your applications, you find there is a large user base for the image\ndetection application. So your team decides to set extensive regression tests before\nevery commit to avoid impacting the users. Longer running memory correctness tests\nrun overnight, and the performance tracked weekly. The ocean wave simulation is\nnew, however, and has fewer users, but you want to ensure the validated problem\ncontinues to give the same answer. It is too long to run for a commit test, so you run\na shortened version and the full version weekly. \nFor both applications, a continuous integration test is set up to build the code and\nrun a few smaller tests. The ash plume model just started being developed, so you\ndecide to use unit tests to check each new section of code as it is added.\n49 Approaching a new project: The preparation\nkernel. Your best bet for this is to run Valgrind on a different computer, an older\nmacOS or spin up a virtual machine or Docker image. \n To run Valgrind, execute your program as usual, inserting the valgrind  command\nat the front. For MPI jobs, the valgrind  command gets placed after mpirun  and\nbefore your executable name. Valgrind works best with the GCC compiler because\nthat development team adopted it, working to eliminate false positives that can clutter\nthe diagnostic output. It is suggested that when using Intel compilers, compile with-\nout vectorization to avoid warnings about the vector instructions. You can also try the\nother memory correctness tools that are listed in section 17.5.\nUSING VALGRIND  MEMCHECK  TO FIND MEMORY  ISSUES\nThe Memcheck tool is the default tool in the Valgrind tool suite. It intercepts every\ninstruction and checks it for various types of memory errors, generating diagnostics at\nthe start, during, and at the end of the run. This slows down the run by an order of\nmagnitude. If you have not used it before, be prepared for a lot of output. One mem-\nory error leads to many others. The best strategy is to start with the first error, fix it,\nand run again. To see how Valgrind works, try the example code in listing 2.2. To exe-\ncute Valgrind, insert the valgrind  command before the executable name either as\nvalgrind <./my_app>\nor\nmpirun -n 2 valgrind <./myapp>\n 1 #include <stdlib.h>\n 2 \n 3 int main(int argc, char *argv[]){\n 4    int ipos, ival;                   \n 5    int *iarray = (int *) malloc(10*sizeof(int));\n 6    if (argc == 2) ival = atoi(argv[1]);\n 7    for (int i = 0; i<=10; i++){ iarray[i] = ipos; }      \n 8    for (int i = 0; i<=10; i++){ \n 9       if (ival == iarray[i]) ipos = i;    \n10    }  \n11 }\nCompile this code with gcc -g -o test  test.c  and then run it with valgrind  --leak-\ncheck=full  ./test  2. The output from Valgrind is interspersed within the program’s\noutput and can be identified by the prefix with double equal signs (==). The following\nshows some of the more important parts of the output from this example:\n==14324== Invalid write of size 4\n==14324==    at 0x400590: main (test.c:7)\n==14324== \n==14324== Conditional jump or move depends on uninitialized value(s)Listing 2.2 Example code for Valgrind memory errors\nipos is not \ngiven a value.\nLoads uninitialized \nmemory from ipos \ninto iarray\nFlags uninitialized \nmemory",5601
31-2.2 Profiling Probing the gap between system capabilities and application performance.pdf,31-2.2 Profiling Probing the gap between system capabilities and application performance,"50 CHAPTER  2Planning for parallelization\n==14324==    at 0x4005BE: main (test.c:9)\n==14324== \n==14324== Invalid read of size 4\n==14324==    at 0x4005B9: main (test.c:9)\n==14324== \n==14324== 40 bytes in 1 blocks are definitely lost in loss record 1 of 1\n==14324==    at 0x4C29C23: malloc (vg_replace_malloc.c:299)\n==14324==    by 0x40054F: main (test.c:5)\nThis output displays reports on several memory errors. The trickiest one to under-\nstand is the uninitialized memory report. Valgrind reports the error on line 9 when a\ndecision was made with the uninitialized value. The error is actually on line 7 where\niarray  is set to ipos , which was not given a value. It can take some careful analysis in a\nmore complex program to determine the source of the error.\n2.1.4 Improving code portability\nA last code preparation requirement improves code portability to a wider range of\ncompilers and operating systems. Portability begins with the base HPC language, gen-\nerally C, C++, or Fortran. Each of these languages maintains standards for compiler\nimplementations, and new standard releases occur periodically. But this does not\nmean that compilers implement these readily. Often the lag time from release to full\nimplementation by compiler vendors can be long. For example, the Polyhedron Solu-\ntions website ( http:/ /mng.bz/yYne ) reports that no Linux Fortran compiler fully\nimplements the 2008 standard, and less than half fully implement the 2003 standard.\nOf course, what matters is if the compilers have implemented the features that you\nwant. C and C++ compilers are usually more up-to-date in their implementations of\nnew standards, but the lag time can still cause problems for aggressive development\nteams. Also, even if the features are implemented, it does not mean these work in a\nwide variety of settings.\n Compiling with a variety of compilers helps to detect coding errors or identify\nwhere code is pushing the “edge” on language interpretations. Portability provides\nflexibility when using tools that work best in a particular environment. For example,\nValgrind works best with GCC, but Intel® Inspector, a thread correctness tool, works\nbest when you compile the application with Intel compilers. Portability also helps\nwhen using parallel languages. For example, CUDA Fortran is only available with the\nPGI compiler. The current set of implementations of GPU directive-based languages\nOpenACC and OpenMP (with the target  directive) are only available on a small set\nof compilers. Fortunately, MPI and the OpenMP for CPUs are widely available for\nmany compilers and systems. At this point, we need to it make clear that there are\nthree distinct OpenMP capabilities: 1) vectorization through SIMD directives, 2) CPU\nthreading from the original OpenMP model, and 3) offloading to an accelerator, gen-\nerally a GPU, through the new target  directives.",2908
32-2.3.2 Design of the core data structures and code modularity.pdf,32-2.3.2 Design of the core data structures and code modularity,"51 Planning: A foundation for success\n2.2 Profiling: Probing the gap between system capabilities \nand application performance\nProfiling (figure 2.4) determines the hardware performance capabilities and com-\npares that with your application performance. The difference between the capabilities\nand current performance yields the potential for performance improvement.\nThe first part of the profiling process is to determine what is the limiting aspect of\nyour application’s performance. We’ll detail possible performance limitations for\napplications in section 3.1. Briefly, most applications today are limited by memory\nbandwidth or a limitation that closely tracks memory bandwidth. A few applications\nmight be limited by available floating-point operations (flops). We’ll present ways\nto calculate theoretical performance limits in section 3.2. We’ll also describe\nbenchmark programs that can measure the achievable performance for that hard-\nware limitation.\n Once the potential performance is understood, then you can profile your applica-\ntion. We’ll present the process of using some profiling tools in section 3.3. The gap\nbetween the current performance of your application and the hardware capabilities\nfor the limiting aspect of your application then become the target for improvement\nfor the next steps in parallelism.\n2.3 Planning: A foundation for success\nArmed with the information gathered on your application and the targeted platforms,\nit is time to put some details into a plan. Figure 2.5 shows parts of this step. With the\neffort that’s required in parallelism, it is sensible to research prior work before start-\ning the implementation step.Example: Krakatau scenario and code portability\nYour image detection application only compiles with the GCC compiler. Your parallel-\nism project adds OpenMP threading. Your team decides to get it to compile with the\nIntel compiler so that you can use the Intel Inspector to find thread race conditions.\nThe ash plume simulation is written in Fortran and targeted to run on GPUs. Based\non your research of current GPU languages, you decide to include PGI as one of your\ndevelopment compilers so you can use CUDA Fortran.\n• Performance limits\nSection 3.1\n• Benchmarking\nSection 3.2\n• Proﬁling tools\nSection 3.3Profile\nFigure 2.4 The purpose of the profiling step \nis to identify the most important parts of the \napplication code that need to be addressed. \n52 CHAPTER  2Planning for parallelization\nIt is likely that similar problems were encountered in the past. You’ll find many\nresearch articles on parallelism projects and techniques published in recent years. But\none of the richest sources of information includes the benchmarks and mini-apps that\nhave been released. With mini-apps, you have not only the research but also the actual\ncode to study.\n2.3.1 Exploring with benchmarks and mini-apps\nThe high performance computing community has developed many benchmarks, ker-\nnels, and sample applications for use in benchmarking systems, performance experi-\nments, and algorithm development. We’ll list some of these in section 17.4. You can\nuse benchmarks to help select the most appropriate hardware for your application,\nand mini-apps provide help on the best algorithms and coding techniques.\n Benchmarks  are intended to highlight a specific characteristic of hardware perfor-\nmance. Now that you have a sense of what is the performance limit of your applica-\ntion, you should look at the benchmarks most applicable to your situation. If you\ncompute on large arrays that are accessed in a linear fashion, then the stream bench-\nmark is appropriate. If you have an iterative matrix solver as your kernel, then the\nHigh Performance Conjugate Gradient (HPCG) benchmark might be better. Mini-\napps are more focused on a typical operation or pattern found in a class of scientific\napplications. \n It is worthwhile to see if any of these benchmarks or mini-apps are similar to the\nparallel application you are developing. If so, studying how these do similar opera-\ntions can save a lot of effort. Often, a lot of work has been done with the code to\nexplore how to get the best performance, to port to other parallel languages and plat-\nforms, or to quantify performance characteristics. \n Currently benchmarks and mini-apps are predominantly from the field of scien-\ntific computing. We’ll use some of these in our examples, and you are encouraged to\nuse these for your experimentation and as example code. Many of the key operations\nand parallel implementations are demonstrated in these examples.\nExample: Ghost cell updates\nMany mesh-based applications distribute their mesh across processors in a distrib-\nuted memory implementation (see figure 1.13). Because of this, these applications\nneed to update the boundaries of their mesh with values from an adjacent processor.Plan• Research (Mini-apps)\n• Design (Chapter 3)\n• Algorithms (Chapter 4)\nFigure 2.5 The planning steps lay the \nfoundation for a successful project.",5062
33-2.5 Commit Wrapping it up with quality.pdf,33-2.5 Commit Wrapping it up with quality,"53 Planning: A foundation for success\n2.3.2 Design of the core data structures and code modularity\nThe design of data structures has a long ranging impact on your application. This is\none of the decisions that needs to be made up front, realizing that changing the\ndesign later becomes difficult. In chapter 4, we go through some of the considerations\nthat are important, along with a case study that demonstrates the analysis of the per-\nformance of different data structures.\n To begin, focus on the data and data movement. This is the dominant consider-\nation with today’s hardware platforms. It also leads into an effective parallel imple-\nmentation where the careful movement of data becomes even more important. If we\nconsider the filesystem and network as well, data movement dominates everything.\n2.3.3 Algorithms: Redesign for parallel\nAt this point, you should evaluate the algorithms in your application. Can these be\nmodified for parallel coding? Are there algorithms that have better scalability? For\nexample, your application may have a section of code that only takes 5% of the run\ntime but has an N2 algorithmic scaling, while the rest of the code scales with N, where\nN is the number of cells or some other data component. As the problem size grows,\nthe 5% soon becomes 20% and then even higher. Soon it is dominating the run time.\nTo identify these kinds of issues, you might want to profile a larger problem and then\nlook at the growth  in the run time rather than the absolute percentage.This operation is called a ghost cell update . Richard Barrett at Sandia National Labo-\nratories developed the MiniGhost mini-app to experiment with different ways to per-\nform this type of operation. The MiniGhost mini-app is part of the Mantevo suite of\nmini-apps available at https://mantevo.org/default.php .\nExample: Data structure for the ash plume model\nYour ash plume model is in the early stages of development. There are several pro-\nposed data structures and breakdowns of the functional steps. Your team decides to\nspend a week analyzing the alternatives before they become fixed in the code, know-\ning that it will be difficult to change in the future. One of the decisions is which multi-\nmaterial data structure to use and, because many materials will only be in small\nregions of the mesh, whether there is a good way to take advantage of this. You\ndecide to explore a sparse data storage data structure to save memory (some are\ndiscussed in section 4.3.2) and for a faster code.\nExample: Algorithm selection for the wave simulation code\nThe parallelism work on the wave simulation code is projected to add OpenMP and\nvectorization. You have heard of different implementation styles for each of these\n54 CHAPTER  2Planning for parallelization\n2.4 Implementation: Where it all happens\nThis is the step I think of as hand-to-hand combat. Down in the trenches, line-by-line,\nloop-by-loop, and routine-by-routine the code is transformed to parallel code. This is\nwhere all your knowledge of parallel implementations on CPUs and GPUs comes to\neffect. As figure 2.6 shows, this  material will be covered in much of the rest of the\nbook. The chapters on parallel programming languages, chapters 6–8 for CPUs and\nchapters 9–13 for GPUs, begin your journey to developing this expertise.\nDuring the implementation step, it is important to keep track of your overall goals.\nYou may or may not have decided on your parallel language at this point. Even if you\nhave, you should be willing to reassess your choice as you get deeper into the imple-\nmentation. Some of the initial considerations for your choice of direction for the proj-\nect include\nAre your speedup requirements fairly modest? You should explore vectorization\nand shared memory (OpenMP) parallelism in chapters 6 and 7.\nDo you need more memory to scale up? If so, you will want to explore distrib-\nuted memory parallelism in chapter 8.\nDo you need large speedups? Then GPU programming is worth looking into in\nchapters 9–13.\nThe key in this implementation step is to break down the work into manageable\nchunks and divide out the work among your team members. There is both the exhila-\nration of getting an order of magnitude speedup in a routine and realizing that the\noverall impact is small, and there is still a lot of work to do. Perseverance and team-\nwork are important in reaching the goal.(continued)\nparallelism approaches. You assign two team members to review recent papers for\ninsights on the approaches that work best. One of your team members expresses\nconcerns about the parallelism of one of the more difficult routines, which has a com-\nplicated algorithm. The current technique does not look straightforward to parallelize.\nYou agree and ask the team member to look into alternative algorithms that might\nbe different than what is currently being done.\n• CPUs (Chapter 6–8)\n• GPUs (Chap 9–13) ImplementFigure 2.6 The implementation step \nutilizes parallel languages and skills \ndeveloped in the rest of the book.",5077
34-2.6.1 Additional reading.pdf,34-2.6.1 Additional reading,"55 Further explorations\n2.5 Commit: Wrapping it up with quality\nThe commit step finalizes this part of the work with careful checks to verify that code\nquality and portability are maintained. Figure 2.7 shows the components of this step.\nHow extensive these checks are is highly dependent on the nature of the application.\nFor production applications with many users, the tests need to be far more thorough. \nNOTE At this point, it is easier to catch relatively small-scale problems than it\nis to debug complications six days into a run on a thousand processors. \nThe team must buy-in to the commit process and work together to follow it. It is sug-\ngested that there be a team meeting to develop the procedures for all to follow. The\nprocesses used during the initial efforts to improve code quality and portability can be\nexploited in creating your procedures. Lastly, the commit process should be re-evalu-\nated periodically and adapted for the current project needs.\n2.6 Further explorations\nIn this chapter, we have only brushed the surface of how to approach a new project\nand what the available tools can do. For more information, explore the resources and\ntry some of the exercises in the following sections.Example: Reassessment of parallel language\nYour project to add OpenMP and vectorization to the wave simulation code is going\nwell. You have gotten an order of magnitude speedup for typical calculations. But as\nthe application speeds up, your users want to run larger problems and they don’t\nhave enough memory. Your team begins to think about adding MPI parallelism to\naccess additional nodes where more memory is available.\nExample: Re-evaluating your team’s code development process\nYour wave simulation application team has gotten the first increment of work for add-\ning OpenMP to the application. But now the application is occasionally crashing with\nno explanation. One of your team members realizes that it might be due to thread\nrace conditions. Your team implements an additional step to check for these condi-\ntions as part of the commit process.Commit\n• Test\n• Portability\n• Code quality\n• CommitFigure 2.7 The goal of the commit step \nis to create a solid rung on the ladder to \nreaching your end goal.",2263
35-2.6.2 Exercises.pdf,35-2.6.2 Exercises,,0
36-Summary.pdf,36-Summary,"56 CHAPTER  2Planning for parallelization\n2.6.1 Additional reading\nAdditional expertise with today’s distributed version control tools benefits your proj-\nect. At least one member of your team should research the many resources on the web\nthat discuss how to use your chosen version control system. If you use Git, the follow-\ning books from Manning are good resources:\nMike McQuaid, Git in Practice  (Manning, 2014).\nRick Umali, Learn Git in a Month of Lunches  (Manning, 2015).\nTesting is vitally important in the parallel development workflow. Unit testing is per-\nhaps the most valuable but also the most difficult to implement well. Manning has a\nbook that gives a much more thorough discussion of unit testing:\nVladimir Khorikov, Unit Testing Principles, Practices, and Patterns  (Manning, 2020).\nFloating-point arithmetic and precision is an underappreciated topic, despite its\nimportance to every computational scientist. The following is a good read and over-\nview on floating-point arithmetic: \nDavid Goldberg, “What every computer scientist should know about floating-point\narithmetic,” ACM Computing Surveys (CSUR)  23, no. 1 (1991): 5-48.\n2.6.2 Exercises\n1You have a wave height simulation application that you developed during grad-\nuate school. It is a serial application and because it was only planned to be the\nbasis for your dissertation, you didn’t incorporate any software engineering\ntechniques. Now you plan to use it as the starting point for an available tool that\nmany researchers can use. You have three other developers on your team. What\nwould you include in your project plan for this?\n2Create a test using CTest\n3Fix the memory errors in listing 2.2\n4Run Valgrind on a small application of your choice\nThis chapter has covered a lot of ground with many of the details necessary for a par-\nallel project plan. The estimation of performance capabilities and uses of tools to\nextract information on hardware characteristics and application performance give\nsolid, concrete data points to populate the plan. The proper use of these tools and\nskills can help build a solid foundation for a successful parallel project.\nSummary\nCode preparation is a significant part of parallelism work. Every developer is sur-\nprised at the amount of effort spent preparing the code for the project. But this\ntime is well spent in that it is the foundation for a successful parallelism project.\nYou should improve your code quality for parallel code. Code quality must be\nan order of magnitude better than typical serial code. Part of this need for quality\n57 Summary\nresides in the difficulty of debugging at scale and part is due to flaws that are\nexposed in the parallelism process or simply due to the sheer number of itera-\ntions that each line of code is executed. Perhaps this is because the probability\nof encountering a flaw is quite small, but when a thousand processors are run-\nning the code, it becomes a thousand times more likely to occur.\nThe profiling step is important to determine where to focus optimization and\nparallelism work. Chapter 3 provides more details on how to profile your\napplication.\nThere is an overall project plan and another separate plan for each iteration of\ndevelopment. Both of these plans should include some research to include\nmini-apps, data structure designs, and new parallel algorithms to lay the foun-\ndation for the next steps.\nWith the commit step, we need to develop processes to maintain good code\nquality. This should be an ongoing effort and not pushed to later when the\ncode is put into production or when the existing user base starts encountering\nproblems with large, long-running simulations.",3723
37-3.1 Know your applications potential performance limits.pdf,37-3.1 Know your applications potential performance limits,"58Performance limits\nand profiling\nProgrammer resources are scarce. You need to target these resources so that they\nhave the most impact. How do you do this if you don’t know the performance char-\nacteristics of your application and the hardware you plan to run on? That is what\nthis chapter means to address. By measuring the performance of your hardware\nand your application, you can determine where it’s most effective to spend your\ndevelopment time. \nNOTE We encourage you to follow along with the exercises for this chap-\nter. The exercises can be found at https:/ /github.com/EssentialsofParallel\nComputing/Chapter3 .This chapter covers\nUnderstanding the limiting aspect of application \nperformance\nEvaluating performance for the limiting hardware \ncomponents\nMeasuring the current performance of your \napplication\n59 Know your application’s potential performance limits\n3.1 Know your application’s potential performance limits\nComputational scientists still consider floating-point operations (flops) as the primary\nperformance limit. While this might have been true years ago, the reality is that flops\nseldom limit performance in modern architectures. But limits can be for bandwidth\nor for latency. Bandwidth  is the best rate at which data can be moved through a given\npath in the system. For bandwidth to be the limit, the code should use a streaming\napproach, where the memory usually needs to be contiguous and all the values used.\nWhen a streaming approach is not possible, latency is the more appropriate limit.\nLatency  is the time required for the first byte or word of data to be transferred. The fol-\nlowing shows some of the possible hardware performance limits:\nFlops (floating-point operations)\nOps (operations) that include all types of computer instructions\nMemory bandwidth\nMemory latency\nInstruction queue (instruction cache)\nNetworks\nDisk\nWe can break down all of these limitations into two major categories: speeds and\nfeeds. Speeds  are how fast operations can be done. It includes all types of computer\noperations. But to be able to do the operations, you must get the data there. This is\nwhere feeds come in. Feeds  include the memory bandwidth through the cache hierar-\nchy, as well as network and disk bandwidth. For applications that cannot get streaming\nbehavior, the latency of memory, network and disk feed are more important. Latency\ntimes can be orders of magnitude slower than those for bandwidth. One of the biggest\nfactors in whether applications are controlled by latency limits or streaming band-\nwidths is the quality of the programming. Organizing your data so that it can be con-\nsumed in a streaming pattern can yield dramatic speedups.\n The relative performance of different hardware components is shown in figure 3.1.\nLet’s use the 1 word loaded per cycle and 1 flop per cycle marked by the large dot as\nour starting point. Most scalar arithmetic operations like addition, subtraction, and\nmultiplication, can be done in 1 cycle. The division operation can take longer at 3–5\ncycles. In some arithmetic mixes, 2 flops/cycle are possible with the fused multiply-\nadd instruction. The number of arithmetic operations that can be done increases fur-\nther with vector units and multi-core processors. Hardware advances, mostly through\nparallelism, greatly increase the flops/cycle. \n Looking at the sloped memory limits, we see that the performance increase through\na deeper hierarchy of caches means that memory accesses can only match the speedup\nof operations if the data is contained in the L1 cache, typically about 32 KiB. But if we\nonly have that much data, we wouldn’t be so worried about the time it takes. We really\nwant to operate on large amounts of data that can only be contained in main memory\n(DRAM) or even on the disk or network. The net result is that the floating-point\n60 CHAPTER  3Performance limits and profiling\ncapabilities of processors have increased far faster than memory bandwidth. This has\nled to many machine balances  of the order of 50 flops capability for every 8-byte word\nloaded. To understand this impact on applications, we measure its arithmetic intensity.\nArithmetic intensity —In an application, measures the number of flops executed\nper memory operations, where memory operations can be either in bytes or\nwords (a word is 8 bytes for a double and 4 bytes for a single-precision value).\nMachine balance —Indicates for computing hardware the total number of flops\nthat can be executed divided by the memory bandwidth.\nMost applications have an arithmetic intensity close to 1 flop per word loaded, but\nthere are also applications that have a higher arithmetic intensity. The classic example\nof a high arithmetic intensity application uses a dense matrix solver to solve a system\nof equations. The use of these solvers used to be much more common in applications\nthan is true today. The Linpack benchmark uses the kernel from this operation to rep-\nresent this class of applications. The arithmetic intensity for this benchmark is reported\nby Peise to be 62.5 flops/word (see reference in appendix A, Peise, 2017, pg. 201). This\nis sufficient for most systems to max out the floating-point capability. The heavy use of\n103\n102\n101\n100\n10−1\n10−2\n10−3\n10−4\n10−5\n10−4 10−3 10−2 10−1 100 101 102 103 104\nArithmetic intensity [FLOPs/Byte]Performance [GFLP/s]\nNetwork 1 GBPs ethernet: 0.1 GB/sLocal SSD disk: 0.6 GB/sDRAM: 1 word/cycle: 24.0 GB/sL3: 600.0 GB/s75x\nFused multiply add (FMA): 6.0 GFLOP/s\nScalar CPU peak (+,–. ): 3.0 GFLOP/s *Vector (256 bit): 24.0 GFLOP/sMulticore (4 core, 2 hyperthreads): 192.0 GFLOP/s\nL1D or L1l: 1800.0 GB/s\n40x240x8x\n4x\n2x\nStandard system - 1 word loaded per cycle and\n1 ﬂop per cycle on a 3 GHz system\nFigure 3.1 Feeds and speeds shown on a roofline plot. The conventional scalar CPU is close to the 1 word loaded \nper cycle and 1 flop per cycle indicated by the shaded circle. The multipliers for the increase in flops are due to \nthe fused multiply-add instruction, vectorization, multiple cores, and hyperthreads. The relative speeds of memory \nmovement are also shown. We’ll discuss the roofline plot more in section 3.2.4.\n61 Know your application’s potential performance limits\nthe Linpack benchmark for the top 500 ranking of the largest computing systems has\nbecome a leading reason for current machine designs that target a high flop-to-memory\nload ratio.\n For many applications, even achieving the memory bandwidth limit can be diffi-\ncult. Some understanding of the memory hierarchy and architecture is necessary to\nunderstand memory bandwidth. Multiple caches between memory and the CPU help\nhide the slower main memory (figure 3.5 in section 3.2.3) in the memory hierarchy.\nData is transported up the memory hierarchy in chunks called cache lines . If memory is\nnot accessed in a contiguous, predictable fashion, the full memory bandwidth is not\nachieved. Merely accessing data in columns for a 2D data structure that is stored in\nrow order will stride across memory by the row length. This can result in as little as\none value being used out of each cache line. A rough estimate of the memory band-\nwidth from this data access pattern is 1/8th of the stream bandwidth (1 out of every 8\ncache values used). This can be generalized for other cases where more cache usage\noccurs by defining a non-contiguous bandwidth ( Bnc) in terms of the percentage of\ncache used ( Ucache) and the empirical bandwidth ( BE):\nBnc = Ucache × BE = Average Percentage of Cache Used × Empirical Bandwidth\nThere are other possible performance limits. The instruction cache may not be able\nto load instructions fast enough to keep a processor core busy. Integer operations are\nalso a more frequent limiter than commonly assumed, especially with higher dimen-\nsional arrays where the index calculations become more complex.\n For applications that require significant network or disk operations (such as big\ndata, distributed computing, or message passing), network and disk hardware limits\ncan be the most serious concern. To get an idea of the magnitude of these device\nperformance limitations, consider the rule of thumb that for the time taken for the\nfirst byte transferred over a high performance computer network, you can do over\n1,000 flops on a single processor core. Standard mechanical disk systems are orders\nof magnitude slower for the first byte, which has led to the highly asynchronous,\nbuffered operation of today’s filesystems and to the introduction of solid-state stor-\nage devices.\nExample\nYour image detection application has to process a lot of data. Right now it comes in\nover the network and is stored to disk for processing. Your team reviews the perfor-\nmance limits and decides to try and eliminate the storage to disk as an unnecessary\nintermediate operation. One of your team members suggests that you can do addi-\ntional floating-point operations almost for free so the team should consider a more\nsophisticated algorithm. But you think that the limiting aspect of the wave simulation\ncode is memory bandwidth. You add a task to the project plan to measure the perfor-\nmance and confirm your hunch.",9288
38-3.2 Determine your hardware capabilities Benchmarking.pdf,38-3.2 Determine your hardware capabilities Benchmarking,,0
39-3.2.1 Tools for gathering system characteristics.pdf,39-3.2.1 Tools for gathering system characteristics,"62 CHAPTER  3Performance limits and profiling\n3.2 Determine your hardware capabilities: Benchmarking\nOnce you have prepared your application and your test suites, you can begin charac-\nterizing the hardware that you are targeting for production runs. To do this, you need\nto develop a conceptual model for the hardware that allows you to understand its per-\nformance. Performance can be characterized by a number of metrics:\nThe rate at which floating-point operations can be executed (FLOPs/s)\nThe rate at which data can be moved between various levels of memory (GB/s)\nThe rate at which energy is used by your application (Watts)\nThe conceptual models allow you to estimate the theoretical peak performance of var-\nious components of the compute hardware. The metrics you work with in these mod-\nels, and those you aim to optimize, depend on what you and your team value in your\napplication. To complement this conceptual model, you can also make empirical mea-\nsurements on your target hardware. The empirical measurements are made with\nmicro-benchmark applications. One example of a micro-benchmark is the STREAM\nBenchmark that is used for bandwidth-limited cases.\n3.2.1 Tools for gathering system characteristics\nIn determining hardware performance, we use a mixture of theoretical and empirical\nmeasurements. Although complementary, the theoretical value provides an upper\nbound to performance, and the empirical measurement confirms what can be achieved\nin a simplified kernel in close to actual operating conditions.\n It is surprisingly difficult to get hardware performance specifications. The explo-\nsion of processor models and the focus of marketing and media reviews for the\nbroader public often obscure the technical details. Good resources for such include\nFor Intel processors, https:/ /ark.intel.com\nFor AMD processors, https:/ /www.amd.com/en/products/specifications/\nprocessors\nOne of the best tools for understanding the hardware you run is the lstopo program.\nIt is bundled with the hwloc package that comes with nearly every MPI distribution.\nThis command outputs a graphical view of the hardware on your system. Figure 3.2\nshows the output for a Mac laptop. The output can be graphical or text-based. To get\nthe picture in figure 3.2 currently requires a custom installation of hwloc and the\ncairo packages to enable the X11 interface. The text version works with the standard\npackage manager installs. Linux and Unix versions of hwloc usually work as long as\nyou can display an X11 window. A new command, netloc , is being added to the hwloc\npackage to display the network connections. \n \n \n \n63 Determine your hardware capabilities: Benchmarking\nTo install cairo v1.16.0\n1Download cairo from https:/ /www.cairographics.org/releases/\n2Configure it with the following commands: \n./configure --with-x --prefix=/usr/local\nmake\nmake install\nTo install hwloc v2.1.0a1-git\n1Clone the hwloc package from Git: https:/ /github.com/open-mpi/hwloc.git\n2Configure it with the following commands: \n./configure --prefix=/usr/local\nmake\nmake installMachine (16GB total)\nPackage\nNUMANode L#0 P#0 (16GB)\nL3 (8192KB)\nL2 (256KB)\nL1d (32KB)\nL1i (32KB)\nPU L#0\nP#0\nPU L#1\nP#1PU L#4\nP#4\nPU L#5\nP#5PU L#6\nP#6\nPU L#7\nP#7PU L#2\nP#2\nPU L#3\nP#3Core Core Core CoreL2 (256KB)\nL1d (32KB)\nL1i (32KB)L2 (256KB)\nL1d (32KB)\nL1i (32KB)L2 (256KB)\nL1d (32KB)\nL1i (32KB)\nFigure 3.2 Hardware topology for a Mac laptop using the lstopo  command\n64 CHAPTER  3Performance limits and profiling\nSome other commands for probing hardware details are lscpu  on Linux systems,\nwmic  on Windows, and sysctl  or system_profiler  on Mac. The Linux lscpu  com-\nmand outputs a consolidated report of the information from the /proc/cpuinfo file.\nYou can see the full information for every logical core by viewing /proc/cpuinfo\ndirectly. The information from the lscpu  command and the /proc/cpuinfo file helps\nto determine the number of processors, the processor model, the cache sizes, and the\nclock frequency for the system. The flags contain important information on the vector\ninstruction set for the chip. In figure 3.3, we see that the AVX2 and various forms of\nthe SSE vector instruction set are available. We’ll discuss vector instruction sets more\nin chapter 6.\nObtaining information on the devices on the PCI bus can be helpful, particularly for\nidentifying the number and type of the graphics processor. The lspci  command\nreports all the devices (figure 3.4). From the output in the figure, we can see that\nthere is one GPU and that it is an NVIDIA GeForce GTX 960.\nFigure 3.3 Output from lscpu  for a Linux desktop that shows a 4-core i5-6500 CPU \n@ 3.2 GHz with AVX2 instructions",4761
40-3.2.4 Empirical measurement of bandwidth and flops.pdf,40-3.2.4 Empirical measurement of bandwidth and flops,"65 Determine your hardware capabilities: Benchmarking\n3.2.2 Calculating theoretical maximum flops\nLet’s run through the numbers for a mid-2017 MacBook Pro laptop with an Intel Core\ni7-7920HQ processor. This is a 4-core processor running at a nominal frequency of 3.1\nGHz with hyperthreading. With its turbo boost feature, it can run at 3.7 GHz when\nusing four processors and up to 4.1 GHz when using a single processor. The theoreti-\ncal maximum flops ( FT) can be calculated with\nFT = Cv × fc × Ic = Virtual Cores × Clock Rate × Flops/Cycle\nThe number of cores includes the effects of hyperthreads that make the physical cores\n(Ch) appear to be a greater number of virtual or logical cores ( Cv). Here we have two\nh y p e r t h r e a d s  t h a t  m a k e  t h e  v i r t u a l  n u m b e r  o f  p r o c e s s o r s  a p p e a r  t o  b e  e i g h t .  T h e\nclock rate is the turbo boost rate when all the processors are engaged. For the proces-\nsor, it is 3.7 GHz. Finally, the flops per cycle, or more generally instructions per cycle\n(Ic), includes the number of simultaneous operations that can be executed by the vec-\ntor unit. \n To determine the number of operations that can be performed, we take the vector\nwidth ( VW) and divide by the word size in bits ( Wbits). We also include the fused multiply-\nadd ( FMA ) instruction as another factor of two operations per cycle. We refer to this\nas fused operations ( Fops) in the equation. For this specific processor, we get\nIc = VW/Wbits × Fops = (256-bit Vector Unit/64 bits) × (2 FMA ) = 8 Flops/Cycle\nCv = Ch × HT = (4 Hardware Cores × 2 Hyperthreads)\nFT = (8 Virtual Cores) × (3.7 GHz) × (8 Flops/Cycle) = 236.8 GFlops/s\nFigure 3.4 Output from the lspci  command from a Linux desktop that shows an NVIDIA GeForce \nGTX 960 GPU.\n66 CHAPTER  3Performance limits and profiling\n3.2.3 The memory hierarchy and theoretical memory bandwidth\nFor most large computational problems, we can assume that there are large arrays\nthat need to be loaded from main memory through the cache hierarchy (figure 3.5).\nThe memory hierarchy has grown deeper over the years with the addition of more lev-\nels of cache to compensate for the increase in processing speed relative to the main\nmemory access times.\nWe can calculate the theoretical memory bandwidth of the main memory using the\nmemory chips specifications. The general formula is\nBT = MTR  × Mc × Tw × Ns = Data Transfer Rate × \nMemory Channels × Bytes Per Access × Sockets\nProcessors are installed in a socket on the motherboard. The motherboard  is the main\nsystem board of the computer, and the socket  is the location where the processor is\ninserted. Most motherboards are single-socket, where only one processor can be\ninstalled. Dual-socket motherboards are more common in high-performance comput-\ning systems. Two processors can be installed in a dual-socket motherboard, giving us\nmore processing cores and more memory bandwidth. \n The data or memory transfer rate ( MTR ) is usually given in millions of transfers\nper sec (MT/s). The double data rate (DDR) memory performs transfers at the top\nand bottom of the cycle for two transactions per cycle. This means that the memory\nbus clock rate is half of the transfer rate in MHz. The memory transfer width ( Tw) is\n64 bits and because there are 8 bits/byte, 8 bytes are transferred. There are two mem-\nory channels ( Mc) on most desktop and laptop architectures. If you install memory in\nboth memory channels, you will get better bandwidth, but this means you cannot sim-\nply buy another DRAM module and insert it. You will have to replace all the modules\nwith larger modules.\n For the 2017 MacBook Pro with LPDDR3-2133 memory and for two channels, the\ntheoretical memory bandwidth ( BT) can be calculated from the memory transfer rateCPUMain memory\nL1L2L3\n1 ﬂop/cycle400 cycles\n75 cycles\n10 cycles\n4 cyclesIf we are doing big calculations,\nwe should be operating out of\nmain memory.\nA cache line is 64 bytes\nor 8 doubles. If loading\ndirectly from main memory,\nit would take 50 cycles/double.\nThe L3/L2/L caches reduce 1\nthis by an order of magnitude.Figure 3.5 Memory hierarchy \nand access times. Memory is \nloaded into cache lines and \nstored at each level of the \ncache system for reuse.\n67 Determine your hardware capabilities: Benchmarking\n(MTR ) of 2133 MT/s, the number of channels ( Mc), and the number of sockets on\nthe motherboard:\nBT = 2133 MT/s × 2 channels × 8 bytes × 1 socket = 34,128 MiB/s or 34.1 GiB/s\nThe achievable memory bandwidth is lower than the theoretical bandwidth due to the\neffects of the rest of the memory hierarchy. You’ll find complex theoretical models for\nestimating the effects of the memory hierarchy, but that is beyond what we want to\nconsider in our simplified processor model. For this, we will turn to empirical mea-\nsurements of bandwidth at the CPU.\n3.2.4 Empirical measurement of bandwidth and flops\nThe empirical bandwidth  is the measurement of the fastest rate that memory can be\nloaded from main memory into the processor. If a single byte of memory is requested,\nit takes 1 cycle to retrieve it from a CPU register. If it is not in the CPU register, it\ncomes from the L1 cache. If it is not in the L1 cache, the L1 cache loads it from L2\nand so on to main memory. If it goes all the way to main memory, for a single byte of\nmemory, it can take around 400 clock cycles. This time required for the first byte of\ndata from each level of memory is called the memory latency . Once the value is in a\nhigher cache level, it can be retrieved faster until it gets evicted  from that level of the\ncache. If all memory has to be loaded a byte at a time, this would be painfully slow. So\nwhen a byte of memory is loaded, a whole chunk of data (called a cache line ) is loaded\nat the same time. If nearby values are subsequently accessed, these are then already in\nthe higher cache levels.\n The cache lines, cache sizes, and number of cache levels are sized to try to provide\nas much of the theoretical bandwidth of the main memory as possible. If we load con-\ntiguous data as fast as possible to make the best use of the caches, we get the CPU’s\nmaximum possible data transfer rate. This maximum data transfer rate is called the\nmemory  bandwidth . To determine the memory bandwidth, we can measure the time for\nreading and writing a large array. From the following empirical measurements, the\nmeasured bandwidth is about 22 GiB/s. This measured bandwidth is what we’ll use in\nthe simple performance models in the next chapter. \n Two different methods are used for measuring the bandwidth: the STREAM\nBenchmark and the roofline model measured by the Empirical Roofline Toolkit. The\nSTREAM Benchmark was created by John McCalpin around 1995 to support his argu-\nment that memory bandwidth is far more important than the peak floating-point\ncapability. In comparison, the roofline model (see the figure in the sidebar entitled\n“Measuring bandwidth using the empirical Roofline Toolkit” and the discussion later\nin this section) integrates both the memory bandwidth limit and the peak flop rate\ninto a single plot with regions that show each performance limit. The Empirical Roof-\nline Toolkit was created by Lawrence Berkeley National Laboratory to measure and\nplot the roofline model. \n68 CHAPTER  3Performance limits and profiling\n The STREAM Benchmark  measures the time to read and write a large array. For\nthis, there are four variants, depending on the operations performed on the data\nby the CPU as it is being read: the copy, scale, add, and triad measurements. The\ncopy does no floating-point work, the scale and add do one arithmetic operation,\nand the triad does two. These each give a slightly different measure of the maxi-\nmum rate that data can be expected to be loaded from main memory when each\ndata value is only used once. In this regime, the flop rate is limited by how fast\nmemory can be loaded. \n                                         Bytes    Arithmetic Operations\nCopy:       a(i) = b(i)                   16             0\nScale:      a(i) = q*b(i)                 16             1\nSum:        a(i) = b(i) + c(i)            24             1\nTriad:      a(i) = b(i) + q*c(i)          24             2\nThe following exercise shows how to use the STREAM Benchmark to measure band-\nwidth on a given CPU.\nExercise: Measuring bandwidth using the STREAM Benchmark\nJeff Hammond, a scientist at Intel, put the McCalpin STREAM Benchmark code into\na Git repository for more convenience. We use his version in this example. To access\nthe code\n1Clone the image at https:/ /github.com/jeffhammond/STREAM.git\n2Edit the makefile and change the compile line to\n-O3 -march=native -fstrict-aliasing -ftree-vectorize -fopenmp \n    -DSTREAM_ARRAY_SIZE=80000000 -DNTIMES=20\nmake ./stream_c.exe\nHere are the results for the 2017 Mac Laptop:\nFunction    Best Rate MB/s  Avg time     Min time     Max time\nCopy:           22086.5     0.060570     0.057954     0.062090\nScale:          16156.6     0.081041     0.079225     0.082322\nAdd:            16646.0     0.116622     0.115343     0.117515\nTriad:          16605.8     0.117036     0.115622     0.118004\nWe can select the best bandwidth from one of the four measurements as our empir-\nical value of maximum bandwidth.\n69 Determine your hardware capabilities: Benchmarking\nIf a calculation can reuse the data in cache, much higher flop rates are possible. If we\nassume that all data being operated on is in a CPU register or maybe the L1 cache,\nthen the maximum flop rate is determined by the CPU’s clock frequency and how\nmany flops it can do per cycle. This is the theoretical maximum flop rate calculated in\nthe preceding example.\n Now we can put these two together to create a plot of the roofline model. The roof-\nline model has a vertical axis of flops per second and a horizontal axis of arithmetic\nintensity. For high arithmetic intensity, where there are a lot of flops compared to the\ndata loaded, the theoretical maximum flop rate is the limit. This produces a horizon-\ntal line on the plot at the maximum flop rate. As the arithmetic intensity decreases,\nthe time for the memory loads starts to dominate, and we no longer can reach the\nmaximum theoretical flops. This then creates the sloped roof in the roofline model,\nwhere the achievable flop rate slopes down as the arithmetic intensity drops. The hor-\nizontal line on the right of the plot and the sloped line on the left produce the charac-\nteristic shape reminiscent of a roofline and what has become known as the roofline\nmodel or plot. You can determine the roofline plot for a CPU or even a GPU as shown\nin the following exercise.\nExercise: Measuring bandwidth using the empirical Roofline Toolkit\nTo prepare for this exercise, install either OpenMPI or MPICH to get a working MPI.\nInstall gnuplot v4.2 and Python v3.0. On Macs, download the GCC compiler to replace\nthe default compiler. These installs can be done using a package manager (brew on\nMac and apt or synaptic on Ubuntu Linux).\n1Clone the Roofline Toolkit from Git: \ngit clone https://bitbucket.org/berkeleylab/cs-roofline-toolkit.git\n2Then type\ncd cs-roofline-toolkit/Empirical_Roofline_Tool-1.1.0\ncp Config/config.madonna.lbl.gov.01 Config/MacLaptop2017\n3Edit Config/MacLaptop2017. (The following figure shows the file for a 2017 Mac\nlaptop.)\n4Run tests ./ert Config/MacLaptop2017 .\n5View the Results.MacLaptop2017/Run.001/roofline.ps file.\n70 CHAPTER  3Performance limits and profiling\n(continued)\nThe following figure shows the roofline for the 2017 Mac laptop. The empirical mea-\nsurement of the maximum flops is a little higher than we calculated analytically. This\nis probably due to a higher clock frequency for a short period of time. Trying different\nconfiguration parameters like turning off vectorization or running one process can\nhelp to determine whether you have the right hardware specifications. The sloped\nlines are the bandwidth limits at different arithmetic intensities. Because these are\ndetermined empirically, the labels for each slope might not be correct and extra lines\nmay be present. \nFrom these two empirical measurements, we get a similar maximum bandwidth\nthrough the cache hierarchy of around 22 MB/s or about 65% of the theoretical band-\nwidth at the DRAM chips (22 GiB/s / 34.1 GiB/s).# Mac Laptop, MPI and OpenMP (4-core Intel Core I7 3.1 GHz)\nERT_RESULTS Results.MacLaptop.01\nERT_DRIVER  driver1\nERT_KERNEL kernel1\nERT_MPI True\nERT_MPI_CFLAGS     -I/usr/local/Cellar/open-mpi/4.0.0/include\nERT_MPI_LDFLAGS   -L/usr/local/opt/libevent/lib -L/usr/local/Cellar/open-mpi/4.0.0/lib -lmpi\nERT_OPENMP True\nERT_OPENMP_CFLAGS  -fopenmp\nERT_OPENMP_LDFLAGS -fopenmp\nERT_FLOPS   1,2,4,8,16\nERT_ALIGN   64\nERT_CC      gcc-8\nERT_CFLAGS  -O3 -march=native -fstrict-aliasing -ftree-vectorize\nERT_LD      gcc-8\nERT_LDFLAGS\nERT_LDLIBS\nERT_RUN     export OMP_NUM_THREADS=ERT_OPENMP_THREADS; mpirun -np ERT_MPI_PROCS ERT_CODE\nERT_PROCS_THREADS 1-8\nERT_MPI_PROCS 1,2,4\nERT_OPENMP_THREADS 1,2,4,8\nERT_NUM_EXPERIMENTS 3\nERT_MEMORY_MAX 1073741824\nERT_WORKING_SET_MIN 1\nERT_TRIALS_MIN 1\nERT_GNUPLOT gnuplotmpicc --show can help with these paths.\n-march=native to compile for this CPU’s vector unit\nSets number of MPI ranks and threads. The\nlspci command may help. Core I7 has\nhyperthreads, I5 does not.\nConfig/MacLaptop2017",13533
41-3.2.5 Calculating the machine balance between flops and bandwidth.pdf,41-3.2.5 Calculating the machine balance between flops and bandwidth,,0
42-3.3.1 Profiling tools.pdf,42-3.3.1 Profiling tools,"71 Characterizing your application: Profiling\n3.2.5 Calculating the machine balance between flops and bandwidth\nNow we can determine the machine balance. The machine balance  is the flops divided\nby the memory bandwidth. We can calculate both a theoretical machine balance ( MB T)\nand an empirical machine balance ( MB E) like so:\nMB T = FT / BT = 236.8 GFlops/s / 34.1 GiB/s × (8 bytes/word) = 56 Flops/word \nMB E = FE / BE = 264.4 GFlops/s / 22 GiB/s × (8 bytes/word) = 96 Flops/word\nIn the roofline figure in the previous section, the machine balance is the intersection\nof the DRAM bandwidth line with the horizontal flop limit line. We see that intersec-\ntion is just above 10 Flops/Byte. Multiplying by 8 would give a machine balance above\n80 Flops/word. We get a few different estimates of the machine balance from these\ndifferent methods, but the conclusion for most applications is that we are in the band-\nwidth-bound regime.\n3.3 Characterizing your application: Profiling\nNow that you have some sense of what performance you can get with the hardware,\nyou need to determine what are the performance characteristics of your application.\nAdditionally, you should develop an understanding of how different subroutines and\nfunctions depend on each other.Roofline for 2017 Mac Laptop shows the maximum FLOPs as a horizontal line, and the maximum \nbandwidth from various cache and memory levels as sloped lines.0.1 1\nFLOPs/ByteEmpirical rooﬂine graph (Results.MacLaptop.01/Run.010)\nGFLOPs/s\n10 100 0.0110100\nL4 - 34.4 GB/s\nDRAM - 22.5 GB/s264.4 GFLOPs/s (Maximum)\nL2 - 895.1 GB/sL1 - 1800.8 GB/s\nL3 - 523.3 GB/s\n72 CHAPTER  3Performance limits and profiling\n3.3.1 Profiling tools\nWe’ll focus on profiling tools that produce a high-level view and that also provide\nadditional information or context. There are a lot of profiling tools, but many pro-\nd u c e  m o r e  i n f o r m a t i o n  t h a n  c a n  b e  a b s o r b e d .  A s  t i m e  p e r m i t s ,  y o u  m a y  w a n t  t o\nexplore the other profiling tools listed in section 17.3. We’ll also present a mix of\nfreely available tools and commercial tools so that you have options depending on\nyour available resources.\n It is important to remember your goal here is to isolate where it is best to spend\nyour time parallelizing your application. The goal is not to understand every last detail\nof your current performance. It is easy to make the mistake of either not using these\ntools at all or getting lost in the tools and the data those produce.\nUSING CALL GRAPHS  FOR HOT-SPOT AND DEPENDENCY  ANALYSIS\nWe’ll start with tools that highlight hot spots and graphically display how each subrou-\ntine relates to others within code. Hot spots  are kernels that occupy the largest amount\nof time during execution. Additionally, a call graph  is a diagram that shows which rou-\ntines call other routines. We can merge these two sets of information for an even more\npowerful combination as we will see in the next exercise.\n A number of tools can generate call graphs, including valgrind’s cachegrind tool.\nCachegrind’s call graphs highlight both hot spots and display subroutine dependen-\ncies. This type of graph is useful for planning development activities to avoid merge\nconflicts. A common strategy is to segregate tasks among the team so that work done\nby each team member takes place in a single call stack. The following exercise shows\nhow to produce a call graph with the Valgrind tool suite and Callgrind. Another tool\nin the Valgrind suite, either KCacheGrind or QCacheGrind, then displays the results.\nThe only difference is that one uses X11 graphics and the other uses Qt graphics.\n \n \n Example: Profiling the Krakatu tsunami wave simulation\nYou decide to profile your wave simulation application to see where the time is\nspent and decide how to parallelize and speed up the code. Some high-fidelity sim-\nulations can take days to run, so your team wants to understand how parallelization\nwith OpenMP and vectorization might improve the performance. You decide to study\na similar mini-app, CloverLeaf, that solves the compressible fluid dynamics equa-\ntions. These equations are just slightly more complicated than the ones in your\nwave simulation application. CloverLeaf has versions in several parallel languages.\nFor this profiling study, your team wants to compare the serial version to the paral-\nlel version with OpenMP and vectorization. Understanding CloverLeaf performance\ngives you a good frame of reference for the second step of profiling your serial wave\nsimulation code.\n73 Characterizing your application: Profiling\n \n \n Exercise: Call graph using cachegrind\nFor this exercise, the first step is to generate a call graph file using the Callgrind tool\nand then visualize it with KCacheGrind. \n1Install Valgrind and KcacheGrind or QCacheGrind using a package manager\n2Download the CloverLeaf miniapp from https://github.com/UK-MAC/CloverLeaf\ngit clone --recursive https://github.com/UK-MAC/CloverLeaf.git\n3Build the serial version of CloverLeaf\ncd CloverLeaf/CloverLeaf_Serial\nmake COMPILER=GNU IEEE=1 C_OPTIONS=""-g -fno-tree-vectorize"" \\n              OPTIONS=""-g -fno-tree-vectorize""\n4Run Valgrind with the Callgrind tool\ncp InputDecks/clover_bm256_short.in clover.in\nedit clover.in and change cycles from 87 to 10\nvalgrind --tool=callgrind -v ./clover_leaf\n5Start QCacheGrind with the command qcachegrind\n6Load a specific callgrind.out. XXX file into the QCacheGrind GUI\n7Right click Call Graph and change the image settings\nThe following figure shows the CloverLeaf call graph. Each box in the call graph shows\nthe name of the kernel and the percentage of time consumed by the kernel at each\nlevel of the call stack. A call stack  is the chain of routines that call the present loca-\ntion in the code. As each routine calls a subroutine, it pushes its address onto the\nstack. At the end of the routine, the program simply pops the address off the stack\nas it returns to the prior calling routine. Each of the other “leaves” of the tree have\ntheir own call stacks. The call stack describes the hierarchy of data sources for the\nvalues in the “leaf” routine with the variables being passed down through the call\nchain. Timings can be either exclusive , where each routine excludes the timing of the\nroutines it calls, or inclusive , where it includes the timing of all the routines below.\nThe timings shown in the figure in the sidebar entitled “Measuring bandwidth using\nthe empirical Roofline Toolkit” are inclusive with each level containing the levels\nbelow and summing up to 100% at the main routine.\nIn the figure, the call hierarchy is shown for the most expensive routines, along with\nthe number of times called and the percentage of run time. We can see from this that\nthe majority of the run time is in the advection  routines that move materials and\nenergy from one cell to another. We need to focus our efforts there. The call graph is\nalso helpful in tracing the path through the source code to follow.\n74 CHAPTER  3Performance limits and profiling\nAnother useful profiling tool is Intel® Advisor. This is a commercial tool with helpful\nfeatures for getting the most performance from your application. Intel Advisor is part\nof the Parallel Studio package that also bundles the Intel compilers, Intel Inspector,\nand VTune. There are options for a student, educator, open source developer and\ntrial licenses at https:/ /software.intel.com/en-us/qualify-for-free-software/student .\nThese Intel tools have also been released for free in the OneAPI package at https:/ /\nsoftware.intel.com/en-us/oneapi . Recently, Intel Advisor has added a profiling fea-\nture incorporating the roofline model. Let’s take a look at it in operation.\n \n \n (continued)\nCall graph for CloverLeaf from KCacheGrind shows the largest contributors to run time.\n\n75 Characterizing your application: Profiling\nExercise: Intel® Advisor\nThis exercise shows how to generate the roofline for the CloverLeaf mini-app, a regular grid\ncompressible fluid dynamics (CFD) hydrocode.\n1Build the OpenMP version of CloverLeaf:\ngit clone --recursive https://github.com/UK-MAC/CloverLeaf.git\ncd CloverLeaf/CloverLeaf_OpenMP\nmake COMPILER=INTEL IEEE=1 C_OPTIONS=""-g -xHost"" OPTIONS=""-g -xHost""\nor\nmake COMPILER=GNU IEEE=1 C_OPTIONS=""-g -march=native"" \\n       OPTIONS=""g -march=native""\n2Run the application in the Intel Advisor tool:\ncp InputDecks/clover_bm256_short.in clover.in\nadvixe-gui\n3Set the executable to clover_leaf in the CloverLeaf_OpenMP directory. The working direc-\ntory can be set to the application directory or CloverLeaf_OpenMP.\naFor the GUI operation, select the Start Survey Analysis pull-down menu and choose\nStart Roofline Analysis\nbOn the command line, type the following:\nadvixe-cl --collect roofline --project-dir ./advixe_proj -- ./clover_leaf\n4Start the GUI and click the folder icon to load the run data. \n5To view the results, click Survey and Roofline, then click on the far left side of the top\npanel of performance results (where it says roofline in vertical text).\nThe following figure shows the summary statistics for the Intel Advisor profiler. It reports an\narithmetic intensity of approximately .11 FLOPS/byte or .88 FLOPS/word. The floating-point\ncomputational rate is 36 GFLOPS/s.\nSummary output from Intel Advisor reporting an arithmetic intensity of 0.11 FLOPS/byte.\n\n76 CHAPTER  3Performance limits and profiling\nWe can also use the freely available likwid tool suite to get an arithmetic intensity. Lik-\nwid is an acronym for “Like I Knew What I’m Doing” and is authored by Treibig,\nHager, and Wellein at the University of Erlangen-Nuremberg. It is a command-line\ntool that only runs on Linux and utilizes the machine-specific registers (MSR). The\nMSR module must be enabled with modprobe msr . The tool uses hardware counters to(continued)\nThe next figure shows the roofline plot from Intel Advisor for the CloverLeaf mini-app. The per-\nformance of various kernels is shown as points relative to the roofline performance of the Sky-\nlake processor. The size and color of the points indicate the percentage of overall time for\neach of the kernels. Even at a glance, it is clear that the algorithm is bandwidth limited and\nfar to the left of the compute-bound region. Because this mini-app uses double precision, mul-\ntiply the arithmetic intensity of .01 by 8 to get an arithmetic intensity well below 1 flop/word.\nThe machine balance is the intersection of the double-precision FMA peak and the DRAM\nbandwidth.\nIn this plot, the machine balance is above 10 flops/byte or, multiplying by 8, greater than 80\nflops/word, where the word size is a double. The sections of code that are most important for\nperformance are identified by the names associated with each dot. The routines that have the\nmost potential for improvement can be determined by how far these are below the bandwidth\nlimit. We also can see that it would be helpful to improve the arithmetic intensity in the kernels.\nRoofline from Intel® Advisor for Cloverleaf (clover_bm256_short.in) on a Skylake Gold_6152 processor\n77 Characterizing your application: Profiling\nmeasure and report various information from the system, including run time, clock\nfrequency, energy and power usage, and memory read and write statistics.\nExercise: likwid perfctr\n1Install likwid from package manager or with the following commands:\ngit clone https://github.com/RRZE-HPC/likwid.git\ncd likwid\nedit config.mk\nmake\nmake install\n2Enable MSR with sudo modprobe msr\n3Run likwid-perfctr -C 0-87 -g MEM_DP ./clover_leaf \n(In the output, there’s also min and max columns. These have been removed to\nsave space.)\n+----------------------------------------+-------------+-----------+\n|                 Metric                 |     Sum     |    Avg    |\n+----------------------------------------+-------------+-----------+\n|        Runtime (RDTSC) [s] STAT        |  47646.0600 |  541.4325 |\n|        Runtime unhalted [s] STAT       |  56963.3936 |  647.3113 |\n|            Clock [MHz] STAT            | 223750.6676 | 2542.6212 |\n|                CPI STAT                |    170.1285 |    1.9333 |\n|             Energy [J] STAT            | 151590.4909 | 1722.6192 |\n|             Power [W] STAT             |    279.9804 |    3.1816 |\n|          Energy DRAM [J] STAT          |  37986.9191 |  431.6695 |\n|           Power DRAM [W] STAT          |     70.1601 |    0.7973 |\n|             DP MFLOP/s STAT            |  22163.8134 |  251.8615 |\n|           AVX DP MFLOP/s STAT          |   4777.5260 |   54.2901 |\n|           Packed MUOPS/s STAT          |   1194.3827 |   13.5725 |\n|           Scalar MUOPS/s STAT          |  17386.2877 |  197.5715 |\n|  Memory read bandwidth [MBytes/s] STAT |  96817.7018 | 1100.2012 |\n|  Memory read data volume [GBytes] STAT |  52420.2526 |  595.6847 |\n| Memory write bandwidth [MBytes/s] STAT |  26502.2674 |  301.1621 |\n| Memory write data volume [GBytes] STAT |  14349.1896 |  163.0590 |\n|    Memory bandwidth [MBytes/s] STAT    | 123319.9692 | 1401.3633 |\n|    Memory data volume [GBytes] STAT    |  66769.4422 |  758.7437 |\n|       Operational intensity STAT       |      0.3609 |    0.0041 |\n+----------------------------------------+-------------+-----------+\nComputation Rate = (22163.8134+4*4777.5260) = 41274 MFLOPs/sec = 41.3 \nGFLOPs/sec\nArithmetic Intensity = 41274/123319.9692 = .33 FLOPs/byte\nOperational Intensity = .3608 FLOPs/byte\nFor a serial run:\nComputation Rate = 2.97 GFLOPS/sec\nOperational intensity = 0.2574 FLOPS/byte\nEnergy = 212747.7787 Joules\nEnergy DRAM = 49518.7395 Joules\n78 CHAPTER  3Performance limits and profiling\nWe can also use the output from likwid to calculate the energy reduction for Clover-\nLeaf due to running in parallel.\nINSTRUMENT  SPECIFIC  SECTIONS  OF CODE WITH LIKWID -PERFCTR  MARKERS\nMarkers can be used in likwid to get performance for one or multiple sections of\ncode. This capability will be used in the next chapter in section 4.2.\n1Compile the code with -DLIKWID_PERFMON -I<PATH_TO_LIKWID>/include\n2Link with -L<PATH_TO_LIKWID>/lib  and -llikwid\n3Insert the lines from listing 3.1 into your code\nLIKWID_MARKER_INIT;             \nLIKWID_MARKER_THREADINIT;\nLIKWID_MARKER_REGISTER(""Compute"")     \nLIKWID_MARKER_START(""Compute"");\n// ...  Your code to measure\nLIKWID_MARKER_STOP(""Compute"");\nLIKWID_MARKER_CLOSE;            \nGENERATING  YOUR OWN ROOFLINE  PLOTS\nCharlene Yang, NERSC, has created and released a Python script for generating a\nroofline plot. This is extremely convenient for generating a high-quality, custom\ngraphic with data from your explorations. For these examples, you may want to install\nthe anaconda3 package. It contains the matplotlib library and Jupyter notebook sup-\nport. Use the following code to customize a roofline plot using Python and matplotlib:\ngit clone https://github.com/cyanguwa/nersc-roofline.git\ncd nersc-roofline/Plotting\nmodify data.txt\npython plot_roofline.py data.txt\nWe’ll use modified versions of this plotting script in a couple of exercises. In this first\none, we embedded parts of the roofline plotting script into a Jupyter notebook. Jupy-\nter notebooks ( https:/ /jupyter.org/install.html ) allow you to intersperse Markdown\ndocumentation with Python code for an interactive experience. We use this to dynam-\nically calculate the theoretical hardware performance and then create a roofline plot\nof your arithmetic intensity and performance. Exercise: Calculate the energy savings for parallel run relative to serial\nEnergy reduction is (212747.7787 - 151590.4909) / 212747.7787 = 28.7 %.\nDRAM energy reduction is (49518.7395 - 37986.9191) / 49518.7395 = 23.2 %.\nListing 3.1 Inserting markers into code to instrument specific sections of code\nA single\nthreaded\nregionRequires daemon \nwith suid (root) \npermissions\n79 Characterizing your application: Profiling\nExercise: Plotting script embedded in a Jupyter notebook\nInstall Python3 using a package manager. Then use the Python installer, pip, to install NumPy,\nSciPy, matplotlib, and Jupyter:\nbrew install python3\npip install numpy scipy matplotlib jupyter\nRun the Jupyter notebook:\n1Download the Jupyter notebook at https:/ /github.com/EssentialsofParallelComputing/\nChapter3\n2Open the Jupyter notebook  HardwarePlatformCharacterization.ipynb\n3In HardwarePlatformCharacterization.ipynb, change the settings for the hardware in the\nfirst section for your platform of interest as shown in the following figure:\nOnce you change the hardware settings, you are ready to run the calculations for the theoret-\nical hardware characteristics. Run all cells in the notebook and look for calculations in the next\npart of the notebook as shown in the next figure. \n\n80 CHAPTER  3Performance limits and profiling\n(continued)\nThe next notebook section contains the measured performance data you want to plot on the\nroofline plot. Enter this data from performance measurements. We use the data collected\nusing the likwid performance counters for a serial run of CloverLeaf and one with OpenMP and\nvectorization.\nNow the notebook starts the code to plot the roofline using matplotlib. Shown here is the first\nhalf of the plotting script. You can change the plot extent, scales, labels, and other settings.\nThe plotting script then finds the “elbows” where the lines intersect to plot only the relative\nsegments. It also works out the location and orientation of the text to be placed on the plot.\n\n81 Characterizing your application: Profiling\nPlotting this arithmetic intensity and computation rate gives the result in figure 3.6.\nBoth the serial and the parallel runs are plotted on the roofline. The parallel run is\nabout 15 times faster and with slightly higher operational (arithmetic) intensity.\n There are a couple more tools that can measure arithmetic intensity. The Intel®\nSoftware Development Emulator (SDE) package ( https:/ /software.intel.com/en-us/\narticles/intel-software-development-emulator ) generates lots of information that can\nbe used to calculate arithmetic intensity. The Intel® Vtune™ performance tool (part\nof the Parallel Studio package) can also be used to gather performance information.\n When we compare the results from Intel Advisor and likwid, there is a difference\nin the arithmetic intensity. There are many different ways to count operations, count-\ning the whole cache line when loaded or just the data used. Similarly, the counters can\ncount the entire vector width and not just the part that is used. Some tools count just",18765
43-3.4 Further explorations.pdf,43-3.4 Further explorations,"82 CHAPTER  3Performance limits and profiling\nfloating-point operations, whereas others count different types of operations (such as\ninteger) as well.\n3.3.2 Empirical measurement of processor clock frequency and \nenergy consumption\nRecent processors have a lot of hardware performance counters and control capabili-\nties. These include processor frequency, temperature, power, and many others. New\nsoftware applications and libraries are emerging to make accessing this information\neasier. These applications ease the programming difficulty, but these may also help\nwork around the need for elevated permissions so that the data is more accessible to\nnormal users. This is a welcome development because programmers cannot optimize\nwhat they cannot see. \n With the aggressive management of processor frequency, processors seldom are at\ntheir nominal frequency setting. The clock frequency is reduced when processors are\nat idle and increased to a turbo-boost mode when busy. Two easy interactive com-\nmands to see the behavior of the processor frequency are\nwatch -n 1 ""lscpu | grep MHz""\nwatch -n 1 ""grep MHz /proc/cpuinfo""10−1\n10−3 10−2 10−1 100 101 102100101102103104\nPerformance [GFLOP/s]\nCloverLeaf w/OpenMP and Vectorization CloverLeaf Serial\nArithmetic intensity [FLOPs/Byte]DP Vector FMA peak: 2801.2 GFLOP/s\nDP Vector add peak: 1400.3 GFLOP/s\nL1 Bandwidth: 21000.0 GiB/s\nL2 Bandwidth: 9961.2 GiB/s\nL3 Bandwidth: 1171.5 GiB/s\nDRAM Bandwidth: 224.1 GiB/s\nFigure 3.6 Overall performance of Clover Leaf on a Skylake Gold processor\n83 Characterizing your application: Profiling\nThe likwid tool suite also has a command-line tool, likwid-powermeter, to look at pro-\ncessor frequencies and power statistics. The likwid-perfctr tool also reports some of\nthese statistics in a summary report. Another handy little app is the Intel® Power Gad-\nget, with versions for the Mac and Windows and a more limited one for Linux. It\ngraphs frequency, power, temperature, and utilization.\n The CLAMR mini-app ( http:/ /www.github.com/LANL/CLAMR.git ) is developing\na small library, PowerStats, that will track energy and frequency from within an appli-\ncation and report it at the end of the run. Currently, PowerStats works on the Mac,\nusing the Intel Power Gadget library interface. A similar capability is being developed\nfor Linux systems. The application code needs to add just a few calls as shown in the\nfollowing listing.\npowerstats_init();    \npowerstats_sample();      \npowerstats_finalize();    \nWhen run, the following table is printed:\nProcessor      Energy(mWh) =   94.47181\nIA             Energy(mWh) =   70.07562\nDRAM           Energy(mWh) =    3.09289\nProcessor      Power (W)   =   71.07833\nIA             Power (W)   =   54.73608\nDRAM           Power (W)   =    2.32194\nAverage Frequency          = 3721.19422\nAverage Temperature (C)    =   94.78369\nTime Expended (secs)       =   12.13246\n3.3.3 Tracking memory during run time\nMemory usage is also another aspect of performance that isn’t easily visible to the pro-\ngrammer. You can use the same sort of interactive command for processor frequency\nas in the previous listing, but for memory statistics instead. First, get your process ID\nfrom the top or the ps command. Then use one of the following commands to track\nmemory usage:\nwatch -n 1 ""grep VmRSS /proc/<pid>/status""\nwatch -n 1 ""ps <pid> ""\ntop -s 1 -p <pid>\nTo integrate this into your program, perhaps to see what happens with memory in dif-\nferent phases, the MemSTATS library in CLAMR provides four different memory-\ntracking calls:Listing 3.2 PowerStats code to track energy and frequency\nDeclare once at start up\nDeclare periodically during \ncalculation (for example, \nevery 100 iterations) or \nfor different phasesDeclare once at program end",3831
44-3.4.1 Additional reading.pdf,44-3.4.1 Additional reading,,0
45-4 Data design and performance models.pdf,45-4 Data design and performance models,"84 CHAPTER  3Performance limits and profiling\nlong long memstats_memused()\nlong long memstats_mempeak()\nlong long memstats_memfree()\nlong long memstats_memtotal()\nInsert these calls into your program to return the current memory statistics at the\npoint of the call. MemSTATS is a single C source and header file, so it should be easy\nto integrate into your program. To get the source, go to http:/ /github.com/LANL/\nCLAMR/  and look in the MemSTATS directory. It is also available at https:/ /github\n.com/EssentialsofParallelComputing/Chapter3  in the code samples.\n3.4 Further explorations\nThis chapter only brushes the surface of what all these tools can do. For more infor-\nmation, explore the following resources in the additional reading section and try\nsome of the exercises.\n3.4.1 Additional reading\nYou can find more information and data on the STREAM Benchmark here:\nJohn McCalpin. 1995. “STREAM: Sustainable Memory Bandwidth in High Perfor-\nmance Computers.” https:/ /www.cs.virginia.edu/stream/ .\nThe roofline model originated at Lawrence Berkeley National Laboratory. Their web-\nsite has many resources exploring its use:\n“Roofline Performance Model.” https:/ /crd.lbl.gov/departments/computer-science/\nPAR/research/roofline/ .\n3.4.2 Exercises\n1Calculate the theoretical performance of a system of your choice. Include the\npeak flops, memory bandwidth, and machine balance in your calculation.\n2Download the Roofline Toolkit from https:/ /bitbucket.org/berkeleylab/\ncs-roofline-toolkit.git  and measure the actual performance of your selected\nsystem.\n3With the Roofline Toolkit, start with one processor and incrementally add opti-\nmization and parallelization, recording how much improvement you get at\neach step.\n4Download the STREAM Benchmark from https:/ /www.cs.virginia.edu/stream/\nand measure the memory bandwidth of your selected system.\n5Pick one of the publicly available benchmarks or mini-apps listed in section 17.1\nand generate a call graph using KCacheGrind.\n6Pick one of the publicly available Benchmarks or mini-apps listed in section 17.1\nand measure its arithmetic intensity with either Intel Advisor or the likwid tools.\n85 Summary\n7Using the performance tools presented in this chapter, determine the average\nprocessor frequency and energy consumption for a small application.\n8Using some of the tools from section 3.3.3, determine how much memory an\napplication uses.\nThis chapter has covered a lot of ground with many necessary details for a parallel\nproject plan. Estimating performance capabilities and using tools to extract informa-\ntion on hardware characteristics and application performance give solid, concrete\ndata points to populate the plan. The proper use of these tools and skills can help\nbuild a foundation for a successful parallel project.\nSummary\nThere are several possible performance limitations for an application. These\nrange from the peak number of floating-point operations (flops) to memory\nbandwidth and hard disk reads and writes.\nApplications on current computing systems are generally more limited by mem-\nory bandwidth than flops. Although identified two decades ago, it has become\neven more true than projected at that time. But computational scientists have\nbeen slow to adapt their thinking to this new reality.\nYou can use profiling tools to measure your application performance and to\ndetermine where to focus optimization and parallelization work. This chapter\nshows examples using Intel® Advisor, Valgrind, Callgrind, and likwid, but there\nare many other tools including Intel® VTune, Open|Speedshop (O|SS), HPC\nToolkit, or Allinea/ARM MAP. (A more complete list is given in section 17.3.)\nHowever, the most valuable tools are those that provide actionable information\nrather than quantity.\nYou can use hardware performance utilities and apps to determine energy con-\nsumption, processor frequency, memory usage, and much more. By making these\nperformance attributes more visible, it becomes easier to optimize for these\nconsiderations.\n86Data design and\nperformance models\nThis chapter has two topics that are intimately coupled: (1) the introduction of per-\nformance models increasingly dominated by data movement and, thus, necessarily\n(2) the underlying design and structure of data. Although it may seem secondary to\nperformance, the data structure and its design are critical. This must be deter-\nmined in advance because it dictates the entire form of the algorithms, code, and\nlater, the parallel implementation. \n The choice of data structures and, thereby, the data layout often determines the\nperformance that you can achieve and in ways that are not always obvious whenThis chapter covers\nWhy real applications struggle to achieve \nperformance\nAddressing kernels and loops that significantly \nunderperform\nChoosing data structures for your application\nAssessing different programming approaches \nbefore writing code\nUnderstanding how the cache hierarchy delivers \ndata to the processor\n87\nthe design decisions are made. Thinking about the data layout and its performance\nimpacts is at the core of a new and growing programming approach called data-oriented\ndesign . This approach considers the patterns of how data will be used in the program and\nproactively designs around it. Data-oriented design gives us a data-centric view of the\nworld, which is also consistent with our focus on memory bandwidth rather than floating-\npoint operations (flops). In summary, for performance, our approach is to think about\nData rather than code\nMemory bandwidth rather than flops\nCache line instead of individual data elements\nOperations prioritized on data already in cache\nSimple performance models based on the data structures and the algorithms that nat-\nurally follow can roughly predict performance. A performance model  is a simplified rep-\nresentation of how a computer system executes the operations in a kernel of code. We\nuse simplified models because reasoning about the full complexity of the computer\noperation is difficult and obscures the key aspects we need to think about for perfor-\nmance. These simplified models should capture the computer’s operational aspects\nthat are most important for performance. Also, every computer system varies in the\ndetails of its operation. Because we want our application to run on a wide range of sys-\ntems, we need a model that abstracts a general view of the operations that all systems\nhave in common. \n A model helps us to understand the current functioning of our kernel perfor-\nmance. It helps build expectations for the performance and how it might improve\nwith changes to the code. Changes to the code can be a lot of work, and we’ll want to\nknow what the result should be before embarking on the effort. It also helps us to\nfocus on the critical factors and resources for our application’s performance.\n A performance model is not limited to flops, and indeed, we will focus on the data\nand memory aspects. In addition to flops and memory operations, integer operations,\ninstructions and instruction types can be important and should be counted. But the\nlimits associated with these additional considerations usually track memory perfor-\nmance and can be treated as a small reduction in the performance from that limit.\n The first part of the chapter looks at simple data structures and how these impact\nperformance. Next, we’ll introduce performance models to use for quickly making\ndesign decisions. These performance models are then put to use in a case study to\nlook at more complicated data structures for compressed, sparse multi-material arrays\nto assess which data structure is likely to perform well. The impact of these decisions\non data structures often shows up much later in the project when changes are far\nmore difficult. The last portion of this chapter focuses on advanced programming\nmodels; it introduces the more complex models that are appropriate for deeper dives\ninto performance issues or understanding how computer hardware and its design\ninfluences performance. Let’s dig into what this means when looking at your code\nand performance issues.",8240
46-4.1 Performance data structures Data-oriented design.pdf,46-4.1 Performance data structures Data-oriented design,"88 CHAPTER  4Data design and performance models\nNOTE We encourage you to follow along with the examples for this chapter\nat https:/ /github.com/EssentialsofParallelComputing/Chapter4 .\n4.1 Performance data structures: Data-oriented design\nOur goal is to design data structures that lead to good performance. We’ll start with a\nway to allocate multidimensional arrays and then move on to more complex data\nstructures. To achieve this goal requires\nUnderstanding how data is laid out in the computer\nHow data is loaded into cache lines and then the CPU\nHow data layout impacts performance\nThe increasing importance of data movement to performance in today’s\ncomputers\nIn most modern programming languages, data is grouped in structures of one kind or\nanother. For example, the use of data structures in C or classes in object-oriented pro-\ngramming (also called OOP) bring related items together for the convenience of\norganizing the source code. The members of the class are gathered together with the\nmethods that operate on it. While the philosophy of object-oriented programming\noffers a lot of value from a programmer’s perspective, it completely ignores how the\nCPU operates. Object-oriented programming results in frequent method calls with\nfew lines of code in between (figure 4.1). \n For method invocation, the class must first be brought into the cache. Next the\ndata is brought into cache and then adjacent elements of the class. This is convenient\nwhen you are operating on one object. But for applications with intensive computa-\ntions, there are large numbers of each item. For these situations, we don’t want to\ninvoke a method on one item at a time with each invocation requiring the transversal\nof a deep call stack. These lead to instruction cache misses, poor data cache usage,\nbranching, and lots of function call overhead.\nDraw_Window\nSet_Window_Trim\nDraw_Square\nfor (all sides)\nDraw_Line\nSet_Window_Active\nDraw_Window_Toolbar\nDraw_Square\nfor (all sides)\nDraw_LineDraw_Window\nC call stack C++ call stackSet_Pen\nDraw_Line(ymax, xmin, ymin, xmin)\nDraw_Line(ymin,xmin,ymin,xmax)\nDraw_Line(ymin,xmax,ymax,xmax)\nDraw_Line(ymax,xmax,ymax,xmin)\nymin = ymax-10;\nDraw_Line(ymax, xmin, ymax, xmin)\nDraw_Line(ymin,xmin,ymax,xmax)\nDraw_Line(ymin,xmax,ymax,xmax)\nDraw_Line(ymax,xmax,ymax,xmin)\nFigure 4.1 Object-oriented languages have deep call stacks with lots of \nmethod calls (shown on the left), while procedural languages have long \nsequences of operations at one level of the call stack.\n89 Performance data structures: Data-oriented design\nC++ methods make it much easier to write concise code, but nearly every line is a\nmethod invocation as figure 4.1 illustrates. In numerical simulation code, the\nDraw_Line  call would more than likely be a complex mathematical expression. But\neven here, if the Draw_Line  function is inlined into the source code, there will be no\njumps into functions for the C code. Inlining  is where the compiler copies the source\nfrom a subroutine into the location where it is used rather than making a call to it.\nThe compiler can only inline for simple, short routines, however. But object-oriented\ncode has method calls that won’t inline because of complexity and deep call stacks.\nThis causes instruction cache misses and other performance issues. If we are only\ndrawing one window, the loss in performance is offset by the simpler programming. If\nwe are going to draw a million windows, we can’t afford the performance hit.\n So let’s flip this around and design our data structures for performance rather\nthan programming convenience. Object-oriented programming and other modern\nprogramming styles are powerful but introduce many performance traps. At CppCon\nin 2014, Mike Acton’s presentation, “Data-oriented design and C++,” summarized\nwork from the gaming industry that identified why modern programming styles impede\nperformance. Advocates of the data-oriented design programming style address this\nissue by creating a programming style that focuses squarely on performance. This\napproach is coined data-oriented design , which focuses on the best data layout for the\nCPU and the cache. This style has much in common with the techniques long used by\nhigh-performance computing (HPC) developers. In HPC, data-oriented design is the\nnorm; it follows naturally from the way people wrote programs in Fortran. So, what\ndoes data-oriented design look like? It\nOperates on arrays, not individual data items, avoiding the call overhead and\nthe instruction and data cache misses\nPrefers arrays rather than structures for better cache usage in more situations\nInlines subroutines rather than transversing a deep call hierarchy\nControls memory allocation, avoiding undirected reallocation behind the scenes\nUses contiguous array-based linked lists to avoid the standard linked list imple-\nmentations used in C and C++, which jump all over memory with poor data\nlocality and cache usage\nAs we move into parallelization in the next chapters, we’ll note that our experience\nshows that large data structures or classes also cause problems with shared memory\nparallelization and vectorization. In shared memory programming, we need to be able\nto mark variables as private to a thread or as global across all threads. But currently, all\nthe items in the data structure have the same attribute. The problem is particularly\nacute during incremental introduction of OpenMP parallelization. When implement-\ning vectorization, we want long arrays of homogeneous data, while classes usually\ngroup heterogeneous data. This complicates things.",5668
47-4.1.1 Multidimensional arrays.pdf,47-4.1.1 Multidimensional arrays,"90 CHAPTER  4Data design and performance models\n4.1.1 Multidimensional arrays\nIn this section, we’ll cover the ubiquitous multidimensional array data structure in sci-\nentific computing. Our goal will be to understand\nHow to lay out multidimensional arrays in memory\nHow to access the arrays to avoid performance problems\nHow to call numerical libraries that are in Fortran from a C program\nHandling multidimensional arrays is the most common problem with regard to per-\nformance. The first two subfigures in figure 4.2 show the conventional C and Fortran\ndata layouts.\nThe C data order is referred to as row major , where data across the row varies faster\nthan data in the column. This means that row data is contiguous in memory. In con-\ntrast, the Fortran data layout is column major , where the column data varies fastest.\nPractically, as programmers, we must remember which index should be in the inner\nloop to leverage the contiguous memory in each situation (figure 4.3).A[j][i]\ny\nx\n0 1 2 30123\nj\ni0 1 2 34 5 6 78 9 10 1112 13 14 15\nC ordering\nrow majorA(j,i)\ny\nx\n1 2 3 41234\nj\ni0123\n4567\n891011\n12131415A(i,j)\ny\nx\n1 2 3 41234\nj\ni0 1 2 34 5 6 78 9 10 1112 13 14 15\nFortran ordering\ncolumn majorSwitching fortran\nindex order\nFigure 4.2 Conventional C ordering is row major while Fortran ordering is column \nmajor. Switching either the Fortran or C index order makes these compatible. Note \nthat convention has Fortran array indices starting at 1 while C starts at 0. Also, \nC convention numbers the elements from 0 to 15 in contiguous order.\nfor (int j=0; j<jmax; j++){\nfor (int i=0; i<imax; i++){\nA[j][i] = 0.0;\n}\n}\nIn C, the last index is\nfastest and should\nbe the inner loop.do i=1, imax\ndo j=1, jmax\nA(j,i) = 0.0\nenddo\nenddo\nIn Fortran, the first index\nis fastest and should\nbe the inner loop.\nFigure 4.3 For C, the important thing to remember is that the last index \nvaries fastest and should be the inner loop of a nested loop. For Fortran, the \nfirst index varies fastest and should be the inner loop of a nested loop.\n91 Performance data structures: Data-oriented design\nBeyond the differences in data ordering between languages, there is a further issue\nthat must be considered. Is the memory for the whole 2D array contiguous? Fortran\ndoesn’t guarantee that the memory is contiguous unless you use the CONTIGUOUS  attri-\nbute on the array as this example shows:\nreal, allocatable, contiguous :: x(:,:)\nIn practice, using the contiguous attribute is not as critical as it might seem. All popu-\nlar Fortran compilers allocate contiguous memory for arrays with or without this attri-\nbute. The possible exceptions are padding for cache performance or passing an array\nthrough a subroutine interface with a slice operator. A slice operator  is a construct in\nFortran that allows you to refer to a subset of an array as in the example of a copy of a\nrow of a 2D array to a 1D array with the syntax y(:)=  x(1,:) . Slice operators can also\nbe used in a subroutine call; for example, \ncall write_data_row(x(1,:))\nSome research compilers handle this by simply modifying the stride  between data ele-\nments in the dope vector for the array. In Fortran, the dope vector  is the metadata for\nthe array containing the start location, length of the array, and the stride between ele-\nments for each dimension. Dope in this context is from “give me the dope (info)” on\nsomeone or something (in this case, the array). Figure 4.4 illustrates the concepts of a\ndope vector, the slice operator, and stride. The idea is that by modifying the stride in\nthe dope vector from 1 to 4, the data is then traversed as a row rather than a column.\nBut in practice, production Fortran compilers usually make a copy of the data and\npass it into the subroutine to avoid breaking code that is expecting contiguous data.\nThis also means that you should avoid using the slice operator in calling Fortran sub-\nroutines because of the hidden copy and its resulting performance cost.\nA(j,i)y\nx\n1 2 3 41234\nj\ni0123\n4567\n891011\n12131415\nStride of 1048 1 2 01232D dope vector\ny dimension:\nStart address is 0\nStride is 1\nLength is 4\nx dimension:\nStart address is 0\nStride is 4\nLength is 4\nFortran ordering\ncolumn majorSlice operator can be done by\neither modifying the dope vector\nor by making a copy.\nSlice operator x(:,1)\nDope vector\nStart address is 0\nStride is 1\nLength is 4Slice operator x(:,1)\nDope vector\nStart address is 0\nStride is 4\nLength is 4Stride of 4\nFigure 4.4 Different views of a Fortran array created by modifying the dope vector, a set of \nmetadata describing the start, stride, and length in each dimension. The slice operator returns a \nsection of a Fortran array with all of the elements in the dimension with the colon (:). More \ncomplicated sections can be created, such as the lower four elements with A(1:2,1:2), where the \nupper and lower bounds are specified with the colon.\n92 CHAPTER  4Data design and performance models\nC has its own issues with contiguous memory for a 2D array. This is due to the conven-\ntional way of dynamically allocating a 2D array in C as shown in the following listing.\n 8 double **x = \n      (double **)malloc(jmax*sizeof(double *));    \n 9\n10 for (j=0; j<jmax; j++){\n11    x[j] = \n         (double *)malloc(imax*sizeof(double));    \n12 }\n13\n14 // computation \n15\n16 for (j=0; j<jmax; j++){\n17    free(x[j]);    \n18 }\n19 free(x);          \nThis listing uses 1+jmax  allocations, and each allocation can come from a different\nplace in the heap. With larger-sized 2D arrays, the layout of the data in memory has\nonly a small impact on cache efficiency. The bigger problem is that the use of noncon-\ntiguous arrays is severely limited; it’s impossible to pass these to Fortran, write those in\na block to a file, and then pass these to a GPU or to another processor. Instead, each\nof these operations needs to be done row by row. Fortunately, there is an easy way to\nallocate a contiguous block of memory for C arrays. Why isn’t it standard practice? It’s\nbecause everyone learns the conventional method as in listing 4.1 and doesn’t think\nabout it. The following listing shows how to allocate a contiguous block of memory for\na 2D array.\n 8 double **x = \n 9    (double **)malloc(jmax*sizeof(double *));     \n10 \n11 x[0] = (void *)malloc(jmax*imax*sizeof(double));     \n12\n13 for (int j = 1; j < jmax; j++) {\n14    x[j] = x[j-1] + imax;    \n15 }\n16\n17 // computation\n18\n19 free(x[0]);    \n20 free(x);       \nThis method not only gives you a contiguous memory block, but it also only takes two\nmemory allocations! We can optimize this even further by bundling the row pointersListing 4.1 Conventional way of allocating a 2D array in C\nListing 4.2 Allocating a contiguous 2D array in CAllocates a column of \npointers of type \npointer to double\nAllocates each \nrow of data\nDeallocates memory\nAllocates a block of memory \nfor the row pointers\nAllocates a block \nof memory for \nthe 2D array\nAssigns the memory \nlocation to point to the \ndata block for each row \npointer\nDeallocates memory\n93 Performance data structures: Data-oriented design\ninto the memory block at the start of the contiguous memory allocation on line 11 of\nlisting 4.2, thereby combining the two memory allocations into one (figure 4.5).\nThe following listing shows the implementation of a single contiguous memory alloca-\ntion for a 2D array in malloc2D.c.\nmalloc2D.c\n 1 #include <stdlib.h>\n 2 #include ""malloc2D.h""\n 3\n 4 double **malloc2D(int jmax, int imax)\n 5 {\n 6    double **x = (double **)malloc(jmax*sizeof(double *) + \n 7                 jmax*imax*sizeof(double));      \n 8\n 9    x[0] = (double *)x + jmax;        \n10\n11    for (int j = 1; j < jmax; j++) {\n12       x[j] = x[j-1] + imax;    \n13    }\n14\n15    return(x);\n16 }\nmalloc2D.h\n1 #ifndef MALLOC2D_H\n2 #define MALLOC2D_H\n3 double **malloc2D(int jmax, int imax);\n4 #endif\nNow we have only one memory block, including the row pointer array. This should\nimprove memory allocation and cache efficiency. The array can also be indexed as a\n1D or a 2D array as shown in listing 4.4. The 1D array reduces the integer address cal-\nculation and is easier to vectorize or thread (when we come to that in chapters 6 and 7).\nThe listing also shows a manual 2D index calculation into a 1D array.\n Listing 4.3 Single contiguous memory allocation for a 2D arrayContiguous memory blockRows\nRow\npointers\nFigure 4.5 A contiguous block of \nmemory becomes a 2D array in C.\nAllocates a block of \nmemory for the row \npointers and the 2D array\nAssigns the start of the \nmemory block for the 2D \narray after the row pointers\nAssigns the memory location \nto point to the data block for \neach row pointer",8863
48-4.1.2 Array of Structures AoS versus Structures of Arrays SoA.pdf,48-4.1.2 Array of Structures AoS versus Structures of Arrays SoA,"94 CHAPTER  4Data design and performance models\ncalc2d.c\n 1 #include ""malloc2D.h""\n 2 \n 3 int main(int argc, char *argv[])\n 4 {\n 5    int i, j;\n 6    int imax=100, jmax=100;\n 7 \n 8    double **x = (double **)malloc2D(jmax,imax);\n 9\n10    double *x1d=x[0];                 \n11    for (i = 0; i< imax*jmax; i++){   \n12       x1d[i] = 0.0;                  \n13    }                                 \n14 \n15    for (j = 0; j< jmax; j++){      \n16       for (i = 0; i< imax; i++){   \n17          x[j][i] = 0.0;            \n18       }                            \n19    }                               \n20\n21    for (j = 0; j< jmax; j++){      \n22       for (i = 0; i< imax; i++){   \n23          x1d[i + imax * j] = 0.0;  \n24       }                            \n25    }                               \n26 }\nFortran programmers take for granted the first-class treatment of multidimensional\narrays in the language. Although C and C++ have been around for decades, these still\ndo not have a native multidimensional array built into the language. There are pro-\nposals to the C++ standard to add native multidimensional array support for the 2023\nrevision (see the Hollman, et al. reference in appendix A). Until then, the multidi-\nmensional array memory allocation covered in listing 4.4 is essential.\n4.1.2 Array of Structures (AoS) versus Structures of Arrays (SoA)\nIn this section, we’ll cover the implications of structures and classes on data layout.\nOur goals are to understand\nThe different ways structures can be laid out in memory\nHow to access arrays to avoid performance problems\nThere are two different ways to organize related data into data collections. These are\nthe Array of Structures  (AoS), where the data is collected into a single unit at the lowest\nlevel and then an array is made of the structure, or the Structure of Arrays  (SoA), where\neach data array is at the lowest level and then a structure is made of the arrays. A thirdListing 4.4 1D and 2D access of contiguous 2D array\n1D access of the \ncontiguous 2D array\n2D access of the \ncontiguous 2D array\nManual 2D index \ncalculation for a 1D array\n95 Performance data structures: Data-oriented design\nway, which is a hybrid of these two data structures, is the Array of Structures of Arrays\n(AoSoA). We will discuss this hybrid data structure in section 4.1.3.\n One common example of an AoS is the color values used to draw graphic objects.\nThe following listing shows the red, green, blue (RGB) color system structure in C.\n1 struct RGB {\n2    int R;      \n3    int G;      \n4    int B;      \n5 };\n6 struct RGB polygon_color[1000];     \nListing 4.5 shows an AoS where the data is laid out in memory (figure 4.6). In the fig-\nure, note the blank space at bytes 12, 28, and 44 where the compiler inserts padding\nto get the memory alignment on a 64-bit boundary (128 bits or 16 bytes). A 64-byte\ncache line holds four values of the structure. Then in line 6, we create the polygon_\ncolor  array composed of 1,000 of the RGB data structure type. This data layout is rea-\nsonable because, generally, the RGB values are used together to draw each polygon.\nThe SoA is an alternative data layout. The following listing shows the C code for this.\n 1 struct RGB {\n 2    int *R;      \n 3    int *G;      \n 4    int *B;      \n 5 };\n 6 struct RGB polygon_color;     \n 7\n 8 polygon_color.R = (int *)malloc(1000*sizeof(int));\n 9 polygon_color.G = (int *)malloc(1000*sizeof(int));\n10 polygon_color.B = (int *)malloc(1000*sizeof(int));\n11\n12 free(polygon_color.R);\n13 free(polygon_color.G);\n14 free(polygon_color.B);\nThe memory layout has all 1,000 R values in contiguous memory. The G and B color\nvalues could follow the R values in memory, but these can also be elsewhere in the\nheap, depending on where the memory allocator finds space. The heap is a separate\nregion of memory that is used to allocate dynamic memory with the malloc  routine orListing 4.5 Array of Structures (AoS) in C\nListing 4.6 Structure of Arrays (SoA) in CDefines a scalar \ncolor value\nDefines an Array of \nStructures (AoS)\nRGB RGB RGB\n0 4 8 12 16 20 24 28 32 36 40 44 bytesFigure 4.6 Layout in memory of an RGB color \nmodel in an Array of Structures (AoS).\nDefines an integer \narray of a color value\nDefines a Structure \nof Arrays (SoA)\n96 CHAPTER  4Data design and performance models\nthe new operator. We can also use the contiguous memory allocator (listing 4.3) to\nforce the memory to be located together.\n Our concern here is performance. Each of these data structures is equally reason-\nable to use from the programmer’s perspective, but the important questions are how\ndoes the data structure appear to the CPU and how does it affect performance. Let’s\nlook at the performance of these data structures in a couple of different scenarios.\nARRAY OF STRUCTURES  (AOS) PERFORMANCE  ASSESSMENT\nIn our color example, assume that when the data is read, all three components for a\npoint are accessed but not a single R, G, or B value, so the AoS representation works\nwell. And for graphics operations, this data layout is commonly used. \nNOTE If the compiler adds padding, it increases the number of memory\nloads by 25% for the AoS representation, but not all compilers insert this pad-\nding. Still it is worth considering for those compilers that do. \nIf only one of the RGB values is accessed in a loop, the cache usage would be poor\nbecause the loop skips over unneeded values. When this access pattern is vectorized\nby the compiler, it would need to use a less efficient gather/scatter operation.\nSTRUCTURE  OF ARRAYS  (SOA) PERFORMANCE  ASSESSMENT\nFor the SoA layout, the RGB values have separate cache lines (figure 4.7). Thus, for\nsmall data sizes where all three RGB values are needed, there’s good cache usage. But\nas the arrays grow larger and more arrays are presented, the cache system struggles,\ncausing performance to suffer. In these cases, the interactions of the data and the\ncache become too complicated to fully predict the performance.\nAnother data layout and access pattern that is often encountered is the use of vari-\nables as 3D spatial coordinates in a computational application. The following listing\nshows the typical C structure definition for this.\n1 struct point {\n2    double x, y, z;     \n3 };Listing 4.7 Spatial coordinates in a C Array of Structures (AoS) R R R\n0 4 8 12 16 20 24 28 32 36 40 44 bytes\n...\nG G G\n0 4 8 12 16 20 24 28 32 36 40 44 bytes\n...\nB B B\n0 4 8 12 16 20 24 28 32 36 40 44 bytes\n...RGB struct pointersRRRRRRRR\nGGGGGGGG\nBBBBBBBB&R\n&G\n&BFigure 4.7 In the Structure of \nArrays (SoA) data layout, the \npointers are adjacent in memory, \npointing to separate contiguous \narrays for each color.\nDefines the spatial \ncoordinate of point\n97 Performance data structures: Data-oriented design\n4 struct point cell[1000];      \n5 double radius[1000];\n6 double density[1000];\n7 double density_gradient[1000];\nOne use of this data structure is to calculate the distance from the origin (radius) as\nfollows:\n10 for (int i=0; i < 1000; i++){\n11    radius[i] = sqrt(cell[i].x*cell[i].x + cell[i].y*cell[i].y + \ncell[i].z*cell[i].z);\n12 }\nThe values of x, y, and z are brought in together in one cache line and written out to\nthe radius variable in a second cache line. The cache usage for this case is reasonable.\nBut in a second plausible case, a computational loop might use the x location to calcu-\nlate a gradient in density in the x-direction like this:\n20 for (int i=1; i < 1000; i++){\n21   density_gradient[i] = (density[i] - density[i-1])/\n                           (cell[i].x - cell[i-1].x);\n22 }\nNow the cache access for x skips over the y and z data so that only one-third (or even\none-quarter if padded) of the data in the cache is used. Thus, the optimal data layout\ndepends entirely on usage and the particular data access patterns. \n In mixed use cases, which are likely to appear in real applications, sometimes the\nstructure variables are used together and sometimes not. Generally, the AoS layout\nperforms better overall on CPUs, while the SoA layout performs better on GPUs. In\nreported results, there is enough variability that it is worth testing for a particular\nusage pattern. In the density gradient case, the following listing shows the SoA code.\n 1 struct point{\n 2      double *x, *y, *z;     \n 3 };\n 4 struct point cell;        \n 5 cell.x = (double *)malloc(1000*sizeof(double));\n 6 cell.y = (double *)malloc(1000*sizeof(double));\n 7 cell.z = (double *)malloc(1000*sizeof(double));\n 8 double *radius = (double *)malloc(1000*sizeof(double));\n 9 double *density = (double *)malloc(1000*sizeof(double));\n10 double *density_gradient = (double *)malloc(1000*sizeof(double));\n11 // ... initialize data\n12\n13 for (int i=0; i < 1000; i++){              \n14    radius[i] = sqrt(cell.x[i]*cell.x[i] +\n                       cell.y[i]*cell.y[i] +\n                       cell.z[i]*cell.z[i]);Listing 4.8 Spatial coordinate Structure of Arrays (SoA)Defines an array \nof point locations\nDefines arrays of \nspatial locations\nDefines structure of \ncell spatial locations\nThis loop uses contiguous \nvalues of arrays.\n98 CHAPTER  4Data design and performance models\n15 }\n16\n17 for (int i=1; i < 1000; i++){      \n18    density_gradient[i] = (density[i] - density[i-1])/\n                            (cell.x[i] - cell.x[i-1]);\n19 }\n20\n21 free(cell.x);\n22 free(cell.y);\n23 free(cell.z);\n24 free(radius);\n25 free(density);\n26 free(density_gradient);\nWith this data layout, each variable is brought in on a separate cache line, and cache\nusage will be good for both kernels. But as the number of required data members\nget sufficiently larger, the cache has difficulty efficiently handling the multitude of\nmemory streams. In a C++ object-oriented implementation, you should be wary of\nother pitfalls. The next listing presents a cell class with the cell spatial coordinate\nand the radius as its data components and a method to calculate the radius from x,\ny, and z. \n 1 class Cell{\n 2       double x;\n 3       double y;\n 4       double z;\n 5       double radius;\n 6    public:\n 7       void calc_radius() {\n              radius = sqrt(x*x + y*y + z*z);     \n         }\n 8       void big_calc();\n 9 }\n10\n11 Cell my_cells[1000];      \n12 \n13 for (int i = 0; i < 1000; i++){\n14    my_cells[i].calc_radius();\n15 }\n16\n17 void Cell::big_calc(){\n18    radius = sqrt(x*x + y*y + z*z);\n19    // ... lots more code, preventing in-lining\n20 }\nRunning this code results in a couple of instruction cache misses and overhead from\nsubroutine calls for each cell. Instruction cache misses occur when the sequence of\ninstructions jumps and the next instruction is not in the instruction cache. There are\ntwo level 1 caches: one for the program data and the second for the processor’sListing 4.9 Spatial coordinate class example with C++This loop uses contiguous \nvalues of arrays.\nInvokes radius \nfunction for each cell\nDefines an array of objects \nas an array of structs\n99 Performance data structures: Data-oriented design\ninstructions. Subroutine calls require the additional overhead to push the arguments\nonto the stack before the call and an instruction jump. Once in the routine, the argu-\nments need to be popped off the stack and then, at the end of the routine, there is\nanother instruction jump. In this case, the code is simple enough that the compiler\ncan inline the routine to avoid these costs. But in more complex cases, such as with a\nbig_calc  routine, it cannot. Additionally, the cache line pulls in x, y, z, and the\nradius. The cache helps speed up the load of the position coordinates that actually\nneed to be read. But the radius, which needs to be written, is also in the cache line. If\ndifferent processors are writing the values for the radius, this could invalidate the\ncache lines and require other processors to reload the data into their caches.\n There are many features of C++ that make programming easier. These should gen-\nerally be used at a higher level in the code, using the simpler procedural style of C and\nFortran where performance counts. In the previous listing, the radius calculation can\nbe done as an array instead of as a single scalar element. The class pointer can be\ndereferenced once at the start of the routine to avoid repeated dereferencing and\npossible instruction cache misses. Dereferencing  is an operation where the memory\naddress is obtained from the pointer reference so that the cache line is dedicated to\nthe memory data instead of the pointer. Simple hash tables can also use a structure\nto group the key and value together as the following listing shows.\n1 struct hash_type {\n2    int key;\n3    int value;\n4 };\n5 struct hash_type hash[1000];\nThe problem with this code is that it reads multiple keys until it finds one that\nmatches and then reads the value for that key. But the key and value are brought into\na single cache line, and the value ignored until the match occurs. It is better to have\nthe key as one array and the value as another to facilitate a faster search through the\nkeys as shown in the next listing.\n1 struct hash_type {\n2    int *key;\n3    int *value;\n4 } hash;\n5 hash.key   = (int *)malloc(1000*sizeof(int));\n6 hash.value = (int *)malloc(1000*sizeof(int));\nAs a final example, take a physics state structure that contains density, 3D momentum,\nand total energy. The following listing shows this structure.Listing 4.10 Hash Array of Structures (AoS) \nListing 4.11 Hash Structure of Arrays (SoA)",13713
49-4.2 Three Cs of cache misses Compulsory capacity conflict.pdf,49-4.2 Three Cs of cache misses Compulsory capacity conflict,"100 CHAPTER  4Data design and performance models\n1 struct phys_state {\n2    double density;\n3    double momentum[3];\n4    double TotEnergy;\n5 };\nWhen processing only density, the next four values in cache go unused. Again, it is\nbetter to have this as an SoA. \n4.1.3 Array of Structures of Arrays (AoSoA)\nThere are cases where hybrid groupings of structures and arrays are effective. The\nArray of Structures of Arrays (AoSoA) can be used to “tile” the data into vector\nlengths. Let’s introduce the notation A[len/4]S[3]A[4]  to represent this layout. A[4]\nis an array of four data elements and is the inner, contiguous block of data. S[3]  rep-\nresents the next level of the data structure of three fields. The combination of\nS[3]A[4]  gives the data layout that figure 4.8 shows.\nWe need to repeat the block of 12 data values A[len/4]  times to get all the data. If we\nreplace the 4 with a variable, we get\nA[len/V]S[3]A[V], where V=4\nIn C or Fortran, respectively, the array could be dimensioned as\nvar[len/V][3][V], var(1:V,1:3,1:len/V)\nIn C++, this would be implemented naturally as the following listing shows.\n 1 const int V=4;       \n 2 struct SoA_type{\n 3    int R[V], G[V], B[V];\n 4 }; \n 5\n 6 int main(int argc, char *argv[])\n 7 {\n 8    int len=1000;\n 9    struct SoA_type AoSoA[len/V];    \n10 \n11    for (int j=0; j<len/V; j++){     \n12       for (int i=0; i<V; i++){           Listing 4.12 Physics state Array of Structures (AoS) \nListing 4.13 RGB Array of Structures of Arrays (AoSoA) RGB RGB RGB\n0 4 8 12 16 20 24 28 32 36 40 44 bytesRG B ...Figure 4.8 An Array of Structures of Arrays (AoSoA) is \nused with the last array length, matching the vector \nlength of the hardware for a vector length of four.\nSets vector length\nDivides the array length \nby vector length\nLoops over array length\nLoops over vector length, \nwhich should vectorize\n101 Three Cs of cache misses: Compulsory, capacity, conflict\n13          AoSoA[j].R[i] = 0;\n14          AoSoA[j].G[i] = 0;\n15          AoSoA[j].B[i] = 0;\n16       }\n17    }\n18 }\nBy varying V to match the hardware vector length or the GPU work group size, we cre-\nate a portable data abstraction. In addition, by defining V=1 or V=len , we recover the\nAoS and SoA data structures, respectively. This data layout then becomes a way to\nadapt for the hardware and the program’s data use patterns. \n There are many details to address about the implementation of this data structure\nto minimize indexing costs and decide whether to pad the array for performance. The\nAoSoA data layout has some of the properties of both the AoS and SoA data structures\nso the performance is generally close to the better of the two as shown in a study by\nRobert Bird from Los Alamos National Laboratory (figure 4.9).\n4.2 Three Cs of cache misses: Compulsory, \ncapacity, conflict\nCache efficiency dominates the performance of intensive computations. As long as\nthe data is cached, the computation proceeds quickly. When the data is not cached,\na cache miss  occurs. The processor then has to pause and wait for the data to be\nloaded. The cost of a cache miss is on the order of 100 to 400 cycles; 100s of flops\nFigure 4.9 Performance of the Array of Structures of Arrays (AoSoA) generally matches the best of the \nAoS and SoA performances. The 1, 8 and NP array length in the x-axis legend is the value for the last \narray in AoSoA. These values mean that the first set reduces to an AOS, the last set reduces to an SoA, \nand the middle set has a second array length of 8 to match the vector length of the processor.\n102 CHAPTER  4Data design and performance models\ncan be done in the same time! For performance, we must minimize cache misses.\nBut minimizing cache misses requires an understanding of how data moves from\nmain memory to the CPU. This is done with a simple performance model that sepa-\nrates cache misses into three C’s: compulsory , capacity , and conflict . First we must\nunderstand how the cache works.\n When data is loaded, it is loaded in blocks, called cache lines , that are typically 64\nbytes long. These are then inserted into a cache location based on its address in mem-\nory. In a direct-mapped cache , there is only one location to load data into the cache. This\nis important when two arrays get mapped to the same location. With a direct-mapped\ncache, only one array can be cached at a time. To avoid this, most processors have an\nN-way set associative cache  that provides N locations into which data are loaded. With\nregular, predictable memory accesses of large arrays, it is possible to prefetch data . That\nis, you can issue an instruction to preload data before it is needed so that it’s already\nin the cache. This can be done either in hardware or in software by the compiler. \n Eviction  is the removal of a cache line from one or more cache levels. This can be\ncaused by the load of a cache line at the same location ( cache conflict ) or the limited\nsize of the cache ( capacity miss ). A store operation  by an assignment in the loop causes a\nwrite allocate in cache, where a new cache line is created and modified. This cache\nline is evicted (stored) to main memory, although it may not happen immediately.\nThere are various write policies used that affect the details of write operations. The\nthree C’s of caches are a simple approach to understanding the source of the cache\nmisses that dominate run-time performance for intensive computations. \nCompulsory —Cache misses that are necessary to bring in the data when it is first\nencountered.\nCapacity —Cache misses that are caused by a limited cache size, which evicts data\nfrom the cache to free up space for new cache line loads.\nConflict —When data is loaded at the same location in the cache. If two or more\ndata items are needed at the same time but are mapped to the same cache line,\nboth data items must be loaded repeatedly for each data element access.\nWhen cache misses occur due to capacity or conflict evictions followed by reloads of\nthe cache lines, this is sometimes referred to as cache thrashing , which can lead to poor\nperformance. From these definitions, we can easily calculate a few characteristics of a\nkernel and at least get an idea of the expected performance. For this, we will use the\nblur operator kernel from figure 1.10. \n Listing 4.14 shows the stencil.c kernel. We also use the 2D contiguous memory allo-\ncation routine in malloc2D.c from section 4.1.1. The timer code is not shown here but\nis in the online source code. Included are timers and calls to the likwid (“Like I Knew\nWhat I’m Doing”) profiler. Between iterations, there is a write to a large array to flush\nthe cache so that there is no relevant data in it that can distort the results.\n \n \n103 Three Cs of cache misses: Compulsory, capacity, conflict\nstencil.c\nIf we have a perfectly effective cache, once the data is loaded into memory, it is kept\nthere. Of course, this is far from reality in most cases. But with this model, we can cal-\nculate the following:\nTotal memory used = 2000 × 2000 × (5 references + 1 store) × 8 bytes = 192 MB\nCompulsory memory loaded and stored = 2002 × 2002 × 8 bytes × 2 arrays =\n64.1 MB\nArithmetic intensity = 5 flops × 2000 × 2000 / 64.1 Mbytes = .312 flops/byte or\n2.5 flops/wordListing 4.14 Stencil kernel for the Krakatau blur operator\n#include <stdio.h>\n#include <stdlib.h>\n#include <sys/time.h>\n#include ""malloc2D.h""\n#include ""timer.h""\n#include ""likwid.h""\n#define SWAP_PTR(xnew,xold,xtmp) (xtmp=xnew, xnew=xold, xold=xtmp)\nint main(int argc, char *argv[]){\nLIKWID_MARKER_INIT;\nLIKWID_MARKER_REGISTER(""STENCIL"");\nstruct timeval tstart_cpu, tstop_cpu;\ndouble cpu_time;\nint imax=2002, jmax = 2002;\ndouble **xtmp, *xnew1d, *x1d;\ndouble **x = malloc2D(jmax, imax);\ndouble **xnew = malloc2D(jmax, imax);\nint *flush = (int *)malloc(jmax*imax*sizeof(int)*10);\nxnew1d = xnew[0]; x1d = x[0];\nfor (int i = 0; i < imax*jmax; i++){\nxnew1d[i] = 0.0; x1d[i] = 5.0;}\nfor (int j = jmax/2 - 5; j < jmax/2 + 5; j++){\nfor (int i = imax/2 - 5; i < imax/2 -1; i++){\nx[j][i] = 400.0;}}\nfor (int iter = 0; iter < 10000; iter++){\nfor (int l = 1; l < jmax*imax*10; l++){ flush[l] = 1.0; }\ncpu_timer_start(&tstart_cpu);\nLIKWID_MARKER_START(""STENCIL"");\nfor (int j = 1; j < jmax-1; j++){\nfor (int i = 1; i < imax-1; i++){\nxnew[j][i] = ( x[j][i] + x[j][i-1] + x[j][i+1] + x[j-1][i] + x[j+1][i] ) / 5.0;\n}  }\nLIKWID_MARKER_STOP(""STENCIL"");\ncpu_time += cpu_timer_stop(tstart_cpu);\nSWAP_PTR(xnew, x, xtmp);\nif (iter%100 == 0) printf(""Iter %d\n"",iter);\n}\nprintf(""Timing is %f\n"",cpu_time);\nfree(x); free(xnew); free(flush);\nLIKWID_MARKER_CLOSE;\n}Initializing likwid and\nregistering marker name\nUsing that 1D array pointer\ntrick to initialize arrays\nInitializing a block of memory\nin center to a larger value\nFlushing cache\nCalculational\nkernelMemory loads - 5\nloads and store 1\nClosing likwidMarker\nstart\nand\nstopFloating-point operations = 5 flops\n104 CHAPTER  4Data design and performance models\nThe program is then compiled with the likwid library and run on a Skylake 6152 pro-\ncessor with the following command:\nlikwid-perfctr -C 0 -g MEM_DP -m ./stencil\nThe result that we need is at the end of the performance table printed at the conclu-\nsion of the run:\n+-----------------------------------+------------+\n|  ...                              |            |\n|             DP MFLOP/s            |  3923.4952 |\n|           AVX DP MFLOP/s          |  3923.4891 |\n|  ...                              |            |\n|       Operational intensity       |     0.247  |\n+-----------------------------------+------------+\nThe performance data for the stencil kernel is presented as a roofline plot using a\nPython script (available in the online materials) and shown in figure 4.10. The roof-\nline plot, as introduced in section 3.2.4, shows the hardware limits of the maximum\nfloating-point operations and the maximum bandwidth as a function of arithmetic\nintensity.\nThis roofline plot shows the compulsory data limit to the right side of the measured\narithmetic intensity of 0.247 (shown with a large dot in figure 4.10). The kernel cannot\n104\n103\n102\n101\n100\n10−1Performance [GFLOP/s]\n10−3 10−2 10−1 100\nArithmetic intensity [FLOPs/Byte]101 102StencilDRAM: 224.1 GB/sL3: 1171.5 GB/sL2: 9961.2 GB/sL1: 21000.0 GB/sDP Vector FMA peak 2801.2 GFLOP/s\nDP Vector add peak 1400.3 GFLOP/s\nCompulsory=.312 Flops/byte\nFigure 4.10 The roofline plot of the stencil kernel for the Krakatau example in \nchapter 1 shows the compulsory upper bound to the right of the measured \nperformance.",10708
50-4.3 Simple performance models A case study.pdf,50-4.3 Simple performance models A case study,"105 Simple performance models: A case study\ndo better than the compulsory limit if it has a cold cache. A cold cache  is one that does\nnot have any relevant data in it from whatever operations were being done before\nentering the kernel. The distance between the large dot and the compulsory limit\ngives us an idea of how effective the cache is in this kernel. The kernel in this case is\nsimple, and the capacity and conflict cache loads are only about 15% greater than the\ncompulsory cache loads. Thus, there is not much room for improvement for the ker-\nnel performance. The distance between the large dot and the DRAM roofline is because\nthis is a serial kernel with vectorization, while the rooflines are parallel with OpenMP.\nThus, there is a potential to improve performance by adding parallelism.\n Because this is a log-log plot, differences are greater than they might appear. Look-\ning closely, the possible improvement from parallelism is nearly an order of magni-\ntude. Improving cache usage can be accomplished by using other values in the cache\nline or reusing data multiple times while it is in the cache. These are two different\ncases, referred to as either spatial locality or temporal locality:\nSpatial locality  refers to data with nearby locations in memory that are often ref-\nerenced close together.\nTemporal locality  refers to recently referenced data that is likely to be referenced\nagain in the near future.\nFor the stencil kernel (listing 4.14), when the value of x[1][1]  is brought into cache,\nx[1][2]  is also brought into cache. This is spatial locality. In the next iteration of the\nloop to calculate x[1][2] , x[1][1]  is needed. It should still be in the cache and gets\nreused as a case of temporal locality. \n A fourth C is often added to the three C’s mentioned earlier that will become\nimportant in later chapters. This is called coherency.\nDEFINITION Coherency  applies to those cache updates needed to synchronize\nthe cache between multiprocessors when data that is written to one proces-\nsor’s cache is also held in another processor’s cache.\nThe cache updates required to maintain coherency can sometimes lead to heavy traf-\nfic on the memory bus and are sometimes referred to as cache update storms . These\ncache update storms can lead to slowdowns in performance rather than speedups\nwhen additional processors are added to a parallel job.\n4.3 Simple performance models: A case study\nThis section looks at an example of using simple performance models to make\ninformed decisions on what data structure to use for multi-material calculations in a\nphysics application. It uses a real case study to show the effects of:\nSimple performance models for a real programming design question\nCompressed sparse data structures to stretch your computational resources\nSome segments of computational science have long used compressed sparse matrix\nrepresentations. Most notable is the Compressed Sparse Row (CSR) format used for\n106 CHAPTER  4Data design and performance models\nsparse matrices since the mid 1960s with great results. For the compressed sparse data\nstructure evaluated in this case study, the memory savings are greater than 95%, and\nthe run time approaches 90% faster than the simple 2D array design. The simple per-\nformance models used predicted the performance within a 20–30% error of actual\nmeasured performance (see Fogerty, Mattineau, et al., in the section on additional\nreading later in this chapter). But there is a cost to using this compressed scheme—\nprogrammer effort. We want to use the compressed sparse data structure where its\nbenefits outweigh the costs. Making this decision is where the simple performance\nmodel really shows its usefulness. \nSimple performance models are useful to the application developer when addressing\nmore complex programming problems than just a doubly-nested loop over a 2D array.\nThe goal of these models is to get a rough assessment of performance through simple\ncounts of operations in a characteristic kernel to make decisions on programming\nalternatives. Simple performance models are slightly more complicated than the\nthree C’s model. The basic process is to count and note the following:\nMemory loads and stores (memops)\nFloating-point operations (flops)\nContiguous versus non-contiguous memory loads\nPresence of branches\nSmall loops\nWe’ll count memory loads and stores (collectively referred to as memops ) and flops,\nbut we’ll also note whether the memory loads are contiguous and if there are branches\nthat might affect the performance. We’ll also use empirical data such as stream band-\nwidth and generalized operation counts to transform the counts into performance\nestimates. If the memory loads are not contiguous, only 1 out of 8 values in the cache\nline are used, so we divide the stream bandwidth by up to 8 for those cases. \n For the serial part of this study, we’ll use the hardware performance of a MacBook\nPro with 6 MB L3 cache. The processor frequency ( v) is 2.7 GHz. The measured\nstream bandwidth is 13,375 MB/s using the technique introduced in section 3.2.4\nwith the stream benchmark code.\n In algorithms with branching, if we take the branch almost all the time, the branch\ncost is low. When the branch taken is infrequent, we add a branch prediction cost ( Bc)\nand possibly a missed prefetch cost ( Pc). A simple model of the branch predictor usesExample: Modeling the Krakatau ash plume\nYour team is considering the modeling of the ash plume in the example from chap-\nter 1. They realize that the ash materials in the plume may eventually number in the\n10s or even up to 100, yet these materials do not need to be in every cell. Could a\ncompressed sparse representation be useful for this situation? \n107 Simple performance models: A case study\nthe most frequent case in the last few iterations as the likely path. This lowers the cost\nif there is some clustering in branch paths due to data locality. The branch penalty\n(Bp) becomes NbBf(Bc + Pc)/v. For typical architectures, the branch prediction cost\n(Bc) is about 16 cycles and the missing prefetch cost ( Pc) is empirically determined to\nbe about 112 cycles. Nb is the number of times the branch is encountered and Bf is the\nbranch miss frequency . Loop overhead for small loops of unknown length are also\nassigned a cost ( Lc) to account for branching and control. The loop cost is estimated\nat about 20 cycles per exit. The loop penalty ( Lp) becomes Lc/v.\n We will use simple performance models in a design study looking at possible multi-\nmaterial data structures for physics simulations. The purpose of this design study is to\ndetermine which data structures would give the best performance before writing any\ncode. In the past, the choice was made on subjective judgement rather than an objec-\ntive basis. The particular case that is being examined is the sparse case, where there\nare many materials in the computational mesh but only one or few materials in any\ncomputational cell. We’ll reference the small sample mesh with four materials in fig-\nure 4.11 in the discussion of possible data layouts. Three of the cells have only a single\nmaterial, whereas cell 7 has four materials.\nThe data structure is only half the story. We also need to evaluate the data layout in a\ncouple of representative kernels by\n1Computing pavg[C] , the average density of materials in cells of a mesh\n2Evaluating p[C][m] , the pressure in each material contained in each cell using\nthe ideal gas law: p( p,t) = nrt/v\nBoth of these computations have an arithmetic intensity of 1 flop per word or lower.\nWe also expect that these kernels will be bandwidth limited. We’ll use two large dataCell 0 Cell 1 Cell 2Cell 3 Cell 4 Cell 5Cell 6 Cell 7 Cell 8\nMat 1 Mat 2Mat 3 Mat 4\nFigure 4.11 A 3×3 computational mesh \nshows that cell 7 contains four materials.\n108 CHAPTER  4Data design and performance models\nsets to test the performance of the kernels. Both are 50 material ( Nm), 1 million cell\nproblems ( Nc) with four state arrays ( Nv). The state arrays are density ( p), tempera-\nture ( t), pressure (p), and volume fraction ( Vf). The two data sets are\nGeometric Shapes Problem —A mesh initialized from nested rectangles of materials\n(figure 4.12). The mesh is a regular rectangular grid. With the materials in sep-\narate rectangles rather than scattered, most cells only have one or two materi-\nals. The result is that there are 95% pure cells ( Pf) and 5% mixed cells ( Mf).\nThis mesh has some data locality so the branch prediction miss ( Bp) is roughly\nestimated to be 0.7.\nRandomly Initialized Problem —A randomly initialized mesh with 80% pure cells\nand 20% mixed cells. Because there is little data locality, the branch prediction\nmiss ( Bp) is estimated to be 1.0.\nIn the performance analysis in sections 4.3.1 and 4.3.2, there are two major design\nconsiderations: data layout and loop order. We refer to the data layout as either cell- or1\n0.8\n0.6\n0.4\n0.2\n0\n0 0.2 0.4 0.6 0.8 1\nFigure 4.12 Fifty nested half rectangles used to initialize mesh for the \ngeometric shapes test case",9191
51-4.3.1 Full matrix data representations.pdf,51-4.3.1 Full matrix data representations,"109 Simple performance models: A case study\nmaterial-centric, depending on the larger organizing factor in the data. The data lay-\nout factor has the large stride in the data order. We refer to the loop access pattern as\neither cell- or material-dominant to indicate which is the outer loop. The best situa-\ntion occurs when the data layout is consistent with the loop access pattern. There is no\nperfect solution; one of the kernels prefers one layout and the second kernel prefers\nthe other.\n4.3.1 Full matrix data representations\nThe simplest data structure is a full matrix  storage representation. This assumes that\nevery material is in every cell. These full matrix representations are similar to the 2D\narrays discussed in the previous section.\nFULL MATRIX  CELL-CENTRIC  STORAGE\nFor the small problem in figure 4.11 (the 3x3 computa-\ntional mesh), figure 4.13 shows the cell-centric data lay-\nout. The data order follows the C language convention\nwith the materials stored contiguously for each cell. In\nother words, the programming representation is vari-\nable[C][m]  with m varying fastest. In the figure, the\nshaded elements are mixed materials in a cell. Pure cells\njust have a 1.0 entry. The elements with dashes indicate\nthat none of that material is in the cell and is therefore\ngiven a zero in this representation. In this simple exam-\nple, about half of the entries have zeros, but in the big-\nger problem, the number of entries that are zero will be\ngreater than 95%. The number of non-zero entries is\nreferred to as the filled fraction  (Ff), and for our design\nscenario is typically less than 5%. Thus, if a compressed\nsparse storage scheme is used, the memory savings will\nbe greater than 95%, even accounting for the additional\nstorage overhead of the more complex data structures.\n The full matrix data approach has the advantage that\nit is simpler and, thus, easier to parallelize and optimize.\nThe memory savings is substantial enough that it is probably worth using the com-\npressed sparse data structure. But what are the performance implications of the\nmethod? We can guess that having more memory for data potentially increases the\nmemory bandwidth and makes the full matrix representation slower. But what if we\ntest for the volume fraction and, if it is zero, we skip the mixed material access? Fig-\nure 4.14 shows how we tested this approach, where the pseudo-code for the cell-\ndominant loop is shown along with the counts for each operation to the left of the\nline of code. The cell-dominant loop structure has the cell index in the outer loop,\nwhich matches with the cell index as the first index in the cell-centric data structure.4----0.2\n----0.2\n----\n0.75-- -- 1.0\n0.05 0.1 0.18\n7\n6\n5\n4\n3\n2\n1\n00.1 -- 0.7\n--0.55 0.45\n123\nMaterialsCells0.4 0.55 0.05\n0.8 -- --\n-- 1.0 --\n0.6 0.4 --\n1.0 -- --\nFigure 4.13 The cell-centric, \nfull matrix data structure with \nmaterials stored contiguously \nfor each cell\n110 CHAPTER  4Data design and performance models\nThe counts are summarized from the line notes (beginning with #) in figure 4.14 as:\nmemops  = Nc(Nm + 2FfNm + 2) = 54.1 Mmemops\nflops = Nc(2FfNm + 1) = 3.1 Mflops\nNc = 1e6; Nm = 50; Ff = .021\nI f  we  l oo k at f l op s , w e  wo ul d co nc lud e that  w e hav e b e e n ef f ic ie nt a nd t he  pe rf o r-\nmance would be great. But this algorithm is clearly going to be dominated by memory\nbandwidth. For estimating memory bandwidth performance, we need to factor in the\nbranch prediction miss. Because the branch is taken so infrequently, the probability of\na branch prediction miss is high. The geometric shapes problem has some locality, so\nthe miss rate is estimated to be 0.7. Putting this all together, we get the following for\nour performance model (PM):\nPM = Nc(Nm + FfNm + 2) * 8/Stream + BpFfNcNm = 67.2 ms\nBf = 0.7; Bc = 16; Pc = 16;  = 2.7\nThe cost of the branch prediction miss makes the run time high; higher than if we just\nskipped the conditional and added in zeros. Longer loops would amortize the penalty\ncost, but clearly a conditional that is rarely taken is not the best scenario for perfor-\nmance. We could also insert a prefetch operation before the conditional to force load-\ning the data in case the branch is taken. But this would increase the memops so the\nactual performance improvement would be small. It would also increase the traffic on\nthe memory bus, causing congestion that would trigger other problems, especially\nwhen adding thread parallelism.\nFULL MATRIX  MATERIAL -CENTRIC  STORAGE\nNow let’s take a look at the material-centric data structure (figure 4.15). The C nota-\ntion for this is variable[m][C]  with the rightmost index of C (or cells) varying fastest.\nIn the figure, the dashes indicate elements that are filled with zeros. Many of the char-\nacteristics of this data structure are similar to the cell-centric full matrix data represen-\ntation, but with the indices of the storage flipped.1: cells, up tofor all C, Ncdo\n2: 0.0ave ←\n5: ave ← ave + ρ f [C][m] *   [C][m]\n8: ρave ← ave/V C[C] [ ]6: end if\n7:end for\n9:end for3: material IDs, , up tofor all mNmdo\n4: ifVf[ ][ ] > 0.0C  m then#NcNm V loads (f)\n#BρNcNmbranch penalty\n# 2FfNcNmloads ( ρ, f)\n# 2FfNcNm +, ﬂops ( *)\n#Ncstores ( ρave),Ncloads ( ) V\n#Nc l ﬂops ( )Figure 4.14 Modified \ncell-dominant algorithm to \ncompute average density of \ncells using the full matrix \nstorage\n111 Simple performance models: A case study\nThe algorithm for computing the average density of each of the cells can be done with\ncontiguous memory loads and a little thought. The natural way to implement this\nalgorithm is to have the outer loop over the cells, initialize it to zero there, and divide\nby the volume at the end. But this strides over the data in a non-contiguous fashion.\nWe want to loop over cells in the inner loop, requiring separate loops before and after\nthe main loop. Figure 4.16 shows the algorithm along with annotations for memops\nand flops.\nCollecting all the annotations for operations, we get\nmemops  = 4Nc(Nm + 1) = 204 Mmemops\nflops = 2NcNm + Nc = 101 Mflops\nThis kernel is bandwidth limited, so the performance model is\nPM = 4Nc(Nm + 1) * 8/Stream = 122 ms\nThe performance of this kernel is half of what the cell-centric data structure achieved.\nBut this computational kernel favors the cell-centric data layout, and the situation is\nreversed for the pressure calculation.CellsMaterials\n123\n012345678----\n1.00.4--\n0.61.0--\n------\n0.80.550.05\n0.40.550.45\n----0.7\n0.10.10.1\n0.05--1.0\n---- -- -- 0.2 -- -- 0.2 0.75 4 --\nFigure 4.15 The material-centric full matrix \ndata structure stores cells contiguously for \neach material. The array indexing in C would \nbe density[m][C]  with the cell index \ncontiguous. The cells with dashes are filled \nwith zeros.\n1: cells, up tofor all C, Ncdo\n5: cells, up tofor all C, Ncdo\n9: cells, up tofor all C, Ncdo3:end for3:end for\n7:end for8:end for\n8:end for\n11:end for4: m terial IDs, , up tofor all a mNmdo2:ρave[ ] 0.0C←\n#NcNmstores ( ρave)\n# 3NcNmloads ( ρave,ρ,Vf)\n# 2NcNm +, ﬂops ( *)\n# 3Ncloads/stores ( ρave’V)\n#Ncﬂops (/)#Ncstores ( ρave)\n6: ρaveC←[] ρave[ ] +Cρ [ ][ ]*Vm  Cf[] []m  C\n10: ρaveC←[] ρave[] /[]C  V CFigure 4.16 Material-\ndominant algorithm to \ncompute average density \nof cells using full matrix \nstorage",7421
52-4.3.2 Compressed sparse storage representations.pdf,52-4.3.2 Compressed sparse storage representations,"112 CHAPTER  4Data design and performance models\n4.3.2 Compressed sparse storage representations\nNow we’ll discuss the advantages and limitations of a couple of compressed storage\nrepresentations. The compressed sparse storage data layouts clearly save memory, but\nthe design for both cell- and material-centric layouts takes some thought. \nCELL-CENTRIC  COMPRESSED  SPARSE  STORAGE\nThe standard approach is a linked list of materials for each cell. But linked lists are\ngenerally short and jump all over memory. The solution is to put the linked list into a\ncontiguous array with the link pointing to the start of the material entries. The next\ncell will have its materials follow right afterwards. Thus, during normal traversal of the\ncells and materials, these will be accessed in contiguous order. Figure 4.17 shows the\ncell-centric data storage scheme. The values for pure cells are kept in cell state arrays.\nIn the figure, 1.0 is the volume fraction of the pure cells, but it can also be the pure\ncell values for density, temperature, and pressure. The second array is the number of\nmaterials in the mixed cell. A –1 indicates that it is a pure cell. Then the material\nlinked list index, imaterial , is in the third array. If it is less than 1, the absolute value of\nthe entry is the index into the mixed data storage arrays. If it is 1 or greater, then it is\nthe index into the compressed pure cell arrays.\nMixed data storage arrays are basically a linked list implemented in a standard array so\nthat the data is contiguous for good cache performance. The mixed data starts with an\narray called nextfrac , which points to the next material for that cell. This enables theCells\n0 1 2 3 4 5 6 7 8\n1.0 1.0\n1 0 2 –2 –4 –6 –8 –11 3\n121412231341\n0.6 0.4 0.2 0.4 0.55 0.45 0.7 0.2Material\n0 1 2 3 4 5 6 7 8 9 10 11 12\nMixed data storage arrays0.8 0.551.0\n0.1\n13 14 150.05 0.1 0.1 0.752 3 4Number of materials, nmats 2 2 3 2 3 4 –1 –1 –1\nfrac2cellnextfrac 1 –1 3 –1 5 15 7 –1 9 10 –1 12 13 14 –1\n1133 44\n3\n0.05–1\n4 5 5 6 6 6 7 7 7 7State arrays\nV, , t, p ρ\nMaterial/Linked list index,\nimaterial\nState arrays\nVf, , t, p ρ\nFigure 4.17 The mixed material arrays for the cell-centric data structure use a linked list \nimplemented in a contiguous array. The different shading at the bottom indicates the \nmaterials that belong to a particular cell and match the shading used in figure 4.13.\n113 Simple performance models: A case study\naddition of new materials in the cell by adding these to the end of the array. Figure 4.17\nshows this with the mixed material list for cell 4, where the arrow shows the third mate-\nrial to be added at the end. The frac2cell  array is a backward mapping to the cell that\ncontains the material. The third array, material , contains the material number for the\nentry. These are the arrays that provide the navigation around the compressed sparse\ndata structure. The fourth array is the set of state arrays for each material in each cell\nwith the volume fraction ( Vf), density ( ), temperature ( t) and pressure ( p).\n The mixed material arrays keep extra memory at the end of the array to quickly\nadd new material entries on the fly. Removing the data link and setting it to zero\ndeletes the materials. To give better cache performance, the arrays are periodically\nreordered back into contiguous memory.\n Figure 4.18 shows the algorithm for the calculation of the average density for each\ncell for the compressed sparse data layout. We first retrieve the material index, imate-\nrial , to see if this is a cell with mixed materials by testing if it is zero or less. If it is a\npure cell, we do nothing because we already have the density in the cell array. If it is\na mixed material cell, we enter a loop to sum up the density multiplied by the volume\nfraction for each of the materials. We test for the end condition of the index becom-\ning negative and use the nextfrac  array to get the next entry. Once we reach the end\nof the list, we calculate the cell’s density ( ). To the right side of the lines of code are\nthe annotations for the operational costs.\nTo the right of the lines of code are the annotations for the operational costs. For this\nanalysis, we will have 4-byte integer loads, so we convert memops  to membytes . Collecting\nthe counts, we obtain\nmembytes  = (4 + 2 Mf * 8) Nc + (2 * 8 + 4)= 6.74 Mbytes\nflops = 2ML + MfNc = .24 Mflops\nMf = .04895; ML = 979701: cells, up tofor all C, Ncdo\n3: [ ]ix ← imaterial  C\n7: [ ] ix ← nextfrac ix4: ix 0if then<=\n5: - ,Untill 0 for doix ← ix ix <2: 0.0ave ←\n6: ave ← ave + ρ [ix]*Vf[]ix\n8: end for\n10: end if\n11:end for9: ρ[] []C ← ave/V C#Ncloads (imaterial )\n#Lρsmall loop overhead\n# 2MLloads ( ρ, Vf)\n#MLﬂops ( +,*)\n#MLloads ( ) nextfrac\n#MfNcstores ( ρave)\n#MfNcloads ( ) V\n#MfNcﬂops ( ) IFigure 4.18 Cell-dominant \nalgorithm to compute average \ncell density using compact \nstorage\n114 CHAPTER  4Data design and performance models\nAgain, this algorithm is memory bandwidth limited. The estimated run time from the\nperformance model is a 98% reduction from the full cell-centric matrix.\nPM = membytes /Stream + LpMfNc = .87 ms\nLp = 20/2.7e6; Mf = .04895\nMATERIAL -CENTRIC  COMPRESSED  SPARSE  STORAGE\nThe material-centric compressed sparse data structure subdivides everything into sep-\narate materials. Returning to the small test problem in figure 4.9, we see that there are\nsix cells with material 1: 0, 1, 3, 4, 6, and 7 (shown in figure 4.19 in subset 1). There\nare two mappings in the subset: one from mesh to subset, mesh2subset , and one from\nthe subset back to the mesh, subset2mesh . The list in the subset to mesh has the indi-\nces of the six cells. The mesh array contains –1 for each cell that does not have the\nmaterial and numbers the ones that do sequentially to map to the subset. The nmats\narray at the top of figure 4.19 has the number of materials contained in each cell. The\nvolume fraction ( Vf), and density ( ) arrays on the right side of the figure have values\nfor each cell in that material. The C nomenclature for this would be Vf[imat][icell]\n10.0 10.0 10.0 10.0 10.0 10.0\n0.01 0.01 0.01 0.01 0.01\n5.1 5.1 5.1 5.1 5.1\n1.0 1.0 1.01.0 0.6 0.8 0.4 0.1 0.05\n0.4 1.0 0.55 0.55 0.1\n0.05 0.45 0.7 0.1 1.0\n0.2 0.2 0.75\n1. 1. 1. 1. 1. 1. 1. 1. 1. V121232341 nmats\n1 –1 –1 –1 1 2 –1 –1 2 –1 –1 –1 1 4 –1 –1 1 2 3 –1 2\n–1\nVf 1\nVf 2\nVf 3Vf\nVf 4\nρ1\nρ2\nρ3ρ\nρ4–1\n01 – 1 23 – 1 45 – 11 4 3 –1 1 2 3 4 3 –1 –1 –13 matids\nmesh2subset\n0 1 3 4 6 7 subset2mesh Subset 1\n–1 0 1 –1 2 3 –1 4 –1 mesh2subset\n1 2 4 5 7 subset2mesh Subset 2\n–1 –1 –1 –1 0 1 2 3 4 mesh2subset\n4 5 6 7 8 subset2mesh Subset 3\n–1 –1 –1 0 –1 –1 1 2 –1 mesh2subset\n3 6 7 subset2mesh Subset 4\nFigure 4.19 The material-centric compressed sparse data layout is organized around materials. For each \nmaterial, there is a variable-length array with a list of the cells that contain the material. The shading \ncorresponds to the shading in figure 4.15. The illustration maps between the full mesh and subsets and the \nvolume fraction and density variables for each subset.\n115 Simple performance models: A case study\nand p[imat][icell] . Because there are relatively few materials with long lists of cells,\nwe can use regular 2D array allocations rather than forcing these to be contiguous. To\noperate on this data structure, we mostly work with each material subset in sequence.\n The material-dominant algorithm in figure 4.20 for the compressed sparse algo-\nrithm looks like the algorithm in figure 4.13 with the addition of the retrieval of the\npointers in lines 5, 6, and 8. But the loads and flops in the inner loop are only done\nfor the material subset of the mesh rather than the full mesh. This provides consider-\nable savings in flops and memops. Collecting all the counts, we get\nmembytes  = 5 * 8 * FfNmNc + 4 * 8 * Nc + (8 + 4) * Nm = 74 Mbytes\nflops = (2 FfNm + 1) Nc = 3.1 Mflops\nThe performance model shows more than a 95% reduction in estimated run time\nfrom the material-centric full matrix data structure:\nPM = membytes/Stream  = 5.5 ms\nTable 4.1 summarizes the results for these four data structures. The difference between\nthe estimated and measured run time is remarkably small. This shows that even rough\ncounts of memory loads can be a good predictor of performance. \nTable 4.1 The sparse data structures are faster and use less memory than the full 2D matrices.\nMemory load (MBs) Flops Estimated run time Measured run time\nCell-centric full 424 3.1 67.2 108\nMaterial-centric full 1632 101 122 164\nCell-centric \ncompressed sparse6.74 .24 .87 1.4\nMaterial-centric \ncompressed sparse74 3.1 5.5 9.61: cells, up tofor all C, Ncdo\n2:ρaveC←[ ] 0.0\n5:ncmat ← ncellsmat m []\n8: C ← subset c []6: [ ]Subset ← Subset2mesh m\n10: end for\n14:end for11:end for\n12: cells,for all C, up to Ncdo4: material IDs, up tofor all mNmdo3:end for\n7: cells, , up to ncmatfor all do c\n9: ρave[]C← ρave[ ] +Cρ [ ][ ] * Vm  cf[] [ ]m  c\n13: ρave[]C← ρave[ ]/V[ ]CC#Ncstores\n#Nmloads (ncellsmat)\n#Nmloads (subset2mesh)\n#FfNcNmloads (subset)\n# 3FfNcNmloads ( ρave’ρ,Vf)\n#FfNcNmstores ( ρave)\n# 2FfNcNm +, ﬂops ( *)\n# 2Ncloads ( ρave’V)\n#Ncstores ( ρave)\n#Nc l ﬂops ( )Figure 4.20 Material-\ndominant algorithm \ncomputes the average \ndensity of cells using the \nmaterial-centric compact \nstorage scheme.",9438
53-4.4 Advanced performance models.pdf,53-4.4 Advanced performance models,"116 CHAPTER  4Data design and performance models\nThe advantage of the compressed sparse representations is dramatic, with savings in\nboth memory and performance. Because the kernel we analyzed was more suited for\nthe cell-centric data structures, the cell-centric compressed sparse data structure is\nclearly the best performer both in memory and run time. If we look at the other ker-\nnel that shows the material-centric data layout, the results are slightly in favor of the\nmaterial-centric data structures. But the big takeaway is that either of the compressed\nsparse representations is a vast improvement over the full matrix representations.\n While this case study focused on multi-material data representations, there are\nmany diverse applications with sparse data that can benefit from the addition of a\ncompressed sparse data structure. A quick performance analysis similar to the one\ndone in this section can determine whether the benefits are worth the additional\neffort in these applications.\n4.4 Advanced performance models\nThere are more advanced performance models that better capture aspects of the\ncomputer hardware. We will briefly cover these advanced models to understand what\nthese offer and the possible lessons to be learned. The details of the performance\nanalysis are not as important as the takeaways.\n In this chapter, we focused primarily on bandwidth-limited kernels because these\nrepresent the performance limitations of most applications. We counted the bytes\nloaded and stored by the kernel and estimated the time required for this data move-\nment based on the stream benchmark or roofline model (chapter 3). By now, you\nshould realize that the unit of operation for computer hardware is not really bytes or\nwords but cache lines, and we can improve the performance models by counting the\ncache lines that need to be loaded and stored. At the same time, we can estimate how\nmuch of the cache line is used. \n The stream benchmark is actually composed of four individual kernels: the copy,\nscale, add, and triad kernels. So why the variation in the bandwidth (16156.6–22086.5\nMB/s) among these kernels as seen in the STREAM Benchmark exercise in 3.2.4? It\nwas implied then that the cause was the difference in arithmetic intensity among the\nkernels shown in the table in section 3.2.4. This is only partly true. The small differ-\nence in arithmetic operations is really a pretty minor influence as long as we are in the\nbandwidth-limited regime. The correlation with the arithmetic operations is also not\nhigh. Why does the scale operation have the lowest bandwidth? The real culprits are\nthe details in the cache hierarchy of the system. The cache system is not like a pipe\nwith water flowing steadily through it as might be implied by the stream benchmark. It\nis more like a bucket brigade ferrying data up the cache levels with varying numbers\nof buckets and sizes as figure 4.21 shows. This is exactly what the Execution Cache\nMemory (ECM) model developed by Treibig and Hager tries to capture. Although it\nrequires knowledge of the hardware architecture, it can predict the performance\nextremely well for streaming kernels. Movement between levels can be limited by the\nnumber of operations (µops), called micro-ops, that can be performed in a single\n117 Advanced performance models\ncycle. The ECM model works in terms of cache lines and cycles, modeling the move-\nment between the different cache levels.\n Let’s just take a quick look at the ECM model for the stream triad ( A[i]  = B[i]  +\ns*C[i] ) to see how this model works (figure 4.22). This calculation must be done for\nthe specific kernel and hardware. We’ll use a Haswell EP system for the hardware for this\nanalysis. We start at the computational core with the equation Tcore = max( TnOL,TOL),\nwhere T is time in cycles. TOL is generally the arithmetic operations that overlap the\ndata transfer time, and TnOL is the non-overlapping data transfer time.\n For the stream triad, we have a cache line of multiply-add operations. If this is\ndone with a scalar operation, it takes 8 cycles to complete. But we can do this with the\nnew Advanced Vector Extensions (AVX) instructions. The Haswell chip has two fusedRegisters\nL3L2L1\nDRAMCPU\nFigure 4.21 The movement of data between \ncache levels is a series of discrete operations, \nmore like a bucket brigade than a flow through \na pipe. The details of the hardware and how \nmany loads can be issued at each level and in \neach direction largely impact the efficiency of \nloading data through the cache hierarchy.\nRegistersCPU\nL3L2L1\nDRAM1 cycle\n2 AVX FMA opsECM model for stream triad\n4 AVX cache linesTcore= max(3,1) = 3\nL2-L1 5 cycles\nL3-L2 8 cyclesL1-CPU 3 cycles\nL3\nDRAM-L3 21.7 cycles4 cache lines\nat 5.4 cycle/cache line4 cache lines\nat 32 bytes/cycle3 cache lines at 64 bytes/cycle\nplus 1 store (eviction) at 32 bytes/cycle\nFigure 4.22 The Execution Cache Memory (ECM) model for the \nHaswell processor provides a detailed timing for the data transfer of \nthe stream triad computation between cache levels. If the data is in \nmain memory, the time it takes to get the data to the CPU is the \nsum of the transfer times between each cache level or 21.7 + 8 + 5 \n+ 3 = 37.7 cycles. The floating-point operations only take 3 cycles, \nso the memory loads are the limiting aspect for the stream triad. \n118 CHAPTER  4Data design and performance models\nmultiply-add (FMA) AVX 256-bit vector units. Each of these units processes four dou-\nble-precision values. There are eight values in a cache line, so two FMA AVX vector\nunits can process this in one cycle. TnOL is the data transfer time. We need to load\ncache lines for B and C, and we need to load and store a cache line for A. This takes 3\ncycles for the Haswell chip because of a limitation of the address generation units\n(AGUs). \n Moving the four cache lines from L2 to L1 at 64 bytes/cycle takes 4 cycles. But the\nuse of A[i]  is a store operation. A store generally requires a special load called a write-\nallocate , where the memory space is allocated in the virtual data manager, and the cache\nline created at the necessary cache levels. Then the data is modified and evicted  (stored)\nfrom the cache. This can only operate at 32 bytes/cycle at this level of cache, resulting in\nan additional cycle or a total of 5 cycles. From L3–L2, the data transfer is 32 bytes/cycle,\nso it takes 8 cycles. And finally, using the measured bandwidth of 27.1 GB/s, the number\nof cycles to move the cache lines from main memory is about 21.7 cycles. ECM uses this\nspecial notation to summarize these numbers:\n{TOL || TnOL | TL1L2 | TL2L3 | TL3Mem } = {1 || 3 | 5 | 8 | 21.7} cycles\nThe Tcore is shown by the TOL || TnOL in the notation. These are essentially the times\n(in cycles) to move between each level, with a special case for the Tcore, where some of\nthe operations on the computational core can overlap some of the data transfer oper-\nations from L1 to the registers. Then the model predicts the number of cycles it would\ntake to load from each level of the cache by summing the data transfer time, including\nthe non-overlapping data transfers from L1 to registers. The max of the TOL and the\ndata transfer time is then used as the predicted time:\nTECM = max( TnOL + Tdata, TOL)\nThis special ECM notation shows the resulting prediction for each cache level:\n{3⎤8⎤16⎤37.7} cycles\nThis notation says that the kernel takes 3 cycles when it operates out of the L1 cache, 8\nout of L2 cache, 16 out of L3, and 37.7 cycles when the data has to be retrieved from\nmain memory. \n What can be learned from this example is that bumping up against a discrete hard-\nware limit on a particular chip with a particular kernel can force another cycle or two\nat one of the transfers between cache levels, causing slower performance. A slightly\ndifferent version of the processor might not have the same problem. For example,\nlater versions of Intel chips add another AGU, which changes the L1-register cycles\nfrom 3 to 2.\n This example also demonstrates that the vector units have value for both arithme-\ntic operations and data movement. The vector load, also known as a quad-load opera-\ntion, is not new. Much of the focus in the discussion on vector processors is on the",8362
54-4.5 Network messages.pdf,54-4.5 Network messages,"119 Network messages\narithmetic operations. But for bandwidth-limited kernels, it is likely that the vector\nmemory operations are more important. An analysis by Stengel, et al. using the\nECM model shows that the AVX vector instructions can give a two times perfor-\nmance improvement over loops that the compiler naively schedules. This is perhaps\nbecause the compiler does not have enough information available. More recent vec-\ntor units also implement a gather/scatter  memory load operation where the data\nloaded into the vector unit does not have to be in contiguous memory locations\n(gather) and the store from the vector to memory does not have to be contiguous\nmemory locations (scatter). \nNOTE This new gather/scatter memory load feature is welcomed as many\nreal numerical simulation codes need it to perform well. But there are still\nperformance issues with the current gather/scatter implementation and more\nimprovement is needed.\nWe can also analyze the performance of the cache hierarchy with the streaming store .\nThe streaming store bypasses the cache system and writes directly to main memory.\nThere is an option in most compilers to use streaming stores, and some invoke it as an\noptimization on their own. Its effect is to reduce the number of cache lines being\nmoved between levels of the cache hierarchy, reducing congestion and the slower evic-\ntion operation between levels of the cache. Now that you have seen the effect of the\ncache-line movement, you should be able to appreciate its value.\n The ECM model is used to evaluate and optimize stencil kernels by several research-\ners. Stencil kernels are streaming operations and can be analyzed with these techniques.\nIt gets a little messy to keep track of all the cache lines and hardware characteristics with-\nout making mistakes, so performance counting tools can help. We’ll refer you to a cou-\nple of references listed in appendix A for further information on these.\n The advanced models are great for understanding the performance of relatively\nsimple streaming kernels. Streaming kernels  are those that load data in a nearly optimal\nway to effectively use the cache hierarchy. But kernels in scientific and HPC applica-\ntions are often complex with conditionals, imperfectly nested loops, reductions, and\nloop-carried dependencies. In addition, compilers can transform the high-level lan-\nguage to assembler operations in unexpected ways, which complicate the analysis.\nThere are usually a lot of kernels and loops to deal with as well. It is not feasible to\nanalyze these complex kernels without specialized tools, so we try to develop general\nideas from the simple kernels that we can apply to the more complex ones. \n4.5 Network messages\nWe can extend our data transfer models for use in analyzing the computer network. A\nsimple network performance model between nodes of a cluster or an HPC system is\nTime (ms) = latency (µsecs) + bytes_moved (MBytes) /(bandwidth (GB/s) \n      (with unit conversions)\n120 CHAPTER  4Data design and performance models\nNote that this is a network bandwidth rather than the memory bandwidth we have\nbeen using. There is an HPC benchmark site for latency and bandwidth at\nhttp:/ /icl.cs.utk.edu/hpcc/hpcc_results_lat_band.cgi  \nWe can use the network micro-benchmarks from the HPC benchmark site to get typi-\ncal latency and bandwidth numbers. We’ll use 5 µsecs for the latency and 1 GB/s for\nthe bandwidth. This gives us the plot shown in figure 4.23. For larger messages, we can\nestimate about 1 s for every MB transferred. But the vast majority of messages are\nsmall. We look at two different communication examples, first a larger message and\nthen a smaller one, to understand the importance of latency and bandwidth in each.\nExample: Ghost cell communication\nLet’s take a 1000×1000 mesh. We need to communicate the outside cells of our\nprocessor as shown in the following figure to the adjacent processor so that it can\ncomplete its calculation. The extra cells placed on the outside of the mesh for the\nprocessor are called ghost cells . \n1000 elements in outside cells * 8 bytes = 8 KB\nCommunication time = 5 µsecs + 8 ms0.0 0.4 0.2 0.6 0.8 1.0\nMBytes movedTime (msecs)\n02004006008001000 5 microsec latency, 1 GB/s bandwidth\nFigure 4.23 Typical network transfer time as a function of the size of the message \ngives us a rule of thumb: 1 MB takes 1 s (second), 1 KB takes 1 ms (millisecond), \nor 1 byte takes 1 microsec.\n121 Network messages\nOuter cell data is exchanged with adjacent processors so that the stencil calculation is operating \non current values. The dashed cells are called ghost cells because these hold duplicated data from \nanother processor. The arrows only show the data exchange for every other cell for clarity.\nExample: Sum number of cells across processors\nWe need to transfer the number of cells to the adjacent processor to sum and then\nreturn the sum. There are two communications of a 4-byte integer. \nCommunication time = (5 µsecs + 4 µsecs) * 2 = 18 µsecs\nIn this case, the latency is significant in the overall time taken for the transmission\nof the message.",5180
55-4.6 Further explorations.pdf,55-4.6 Further explorations,,0
56-4.6.2 Exercises.pdf,56-4.6.2 Exercises,"122 CHAPTER  4Data design and performance models\nThe last sum example is a reduction operation in computer science lingo. An array of\ncell counts across the processors is reduced into a single value. More generally, a reduc-\ntion operation  is any operation where a multidimensional array from 1 to N dimensions\nis reduced to at least one dimension smaller and often to a scalar value. These are\ncommon operations in parallel computing and involve cooperation among the pro-\ncessors to complete. Also, the reduction sum in the last example can be performed in\npair-wise fashion in a tree-like pattern with the number of communication hops being\nlog2N, where N is the number of ranks (processors). When the number of processors\nreaches into the thousands, the time for the operation grows larger. Perhaps more\nimportantly, all of the processors have to synchronize at the operation, leading to\nmany of those waiting for the other processors to get to the reduction call.\n There are more complex models for network messages that might be useful for\nspecific network hardware. But the details of network hardware vary enough that\nthese may not shed much light on the general behavior across all possible hardware.\n4.6 Further explorations\nHere are some resources for exploring the topics in this chapter, including data-oriented\ndesign, data structures, and performance models. Most application developers find\nthe additional materials on data-oriented design to be interesting. Many applications\ncan exploit sparsity, and we can learn how from the case study on compressed sparse\ndata structures. \n4.6.1 Additional reading\nThe following two references give good descriptions of the data-oriented design\napproach developed in the gaming community for building performance into pro-\ngram design. The second reference also gives the location of the video of Acton’s pre-\nsentation at CppCon.\nNoel Llopis, “Data-oriented design (or why you might be shooting yourself in the\nfoot with OOP)” (Dec, 2009). Accessed February 21, 2021. http:/ /gamesfromwithin\n.com/data-oriented-design .\nMike Acton and Insomniac Games, “Data-oriented design and C++.” Presenta-\ntion at CppCon (September, 2014):\n–P o w e r p o i n t  a t  https:/ /github.com/CppCon/CppCon2014\n– Video at https:/ /www.youtube.com/watch?v=rX0ItVEVjHc\nThe following reference is good for going into more detail on the case study of com-\npressed sparse data structures using simple performance models. You’ll also find mea-\nsured performance results on multi-core and GPUs:\nShane Fogerty, Matt Martineau, et al., “A comparative study of multi-material data\nstructures for computational physics applications.” In Computers & Mathematics with\nApplications  Vol. 78, no. 2 (July, 2019): 565–581. The source code is available at\nhttps:/ /github.com/LANL/MultiMatTest .",2858
57-5.2 Performance models versus algorithmic complexity.pdf,57-5.2 Performance models versus algorithmic complexity,"123 Summary\nThe following paper introduces the shorthand notation used for the Execution Cache\nModel: \nHolger Stengel, Jan Treibig, et al., “Quantifying performance bottlenecks of stencil\ncomputations using the execution-cache-memory model.” In Proceedings of the 29th\nACM on International Conference on Supercomputing  (ACM, 2015): 207–216.\n4.6.2 Exercises\n1Write a 2D contiguous memory allocator for a lower-left triangular matrix.\n2Write a 2D allocator for C that lays out memory the same way as Fortran.\n3Design a macro for an Array of Structures of Arrays (AoSoA) for the RGB color\nmodel in section 4.1.\n4Modify the code for the cell-centric full matrix data structure to not use a condi-\ntional and estimate its performance.\n5How would an AVX-512 vector unit change the ECM model for the stream\ntriad?\nSummary\nData structures are at the foundation of application design and often dictate\nperformance and the resulting implementation of parallel code. It is worth a lit-\ntle additional effort to develop a good design for the data layout.\nYou can use the concepts of data-oriented design to develop higher performing\napplications.\nThere are ways to write contiguous memory allocators for multidimensional\narrays or special situations to minimize memory usage and improve performance.\nYou can use compressed storage structures to reduce your application’s mem-\nory usage while also improving performance.\nSimple performance models based on counting loads and stores can predict\nthe performance of many basic kernels.\nMore complex performance models shed light on the performance of the\ncache hierarchy with respect to low-level details in the hardware architecture.\n124Parallel algorithms\nand patterns\nAlgorithms are at the core of computational science. Along with data structures,\ncovered in the previous chapter, algorithms form the basis of all computational\napplications. For this reason, it is important to give careful thought to the key algo-\nrithms in your code. To begin, let’s define what we mean by parallel algorithms and\nparallel patterns.\nA parallel algorithm is a well-defined, step-by-step computational procedure that empha-\nsizes concurrency to solve a problem.  Examples of algorithms include sorting,\nsearching, optimization, and matrix operations.\nA parallel pattern is a concurrent, separable fragment of code that occurs in diverse sce-\nnarios with some frequency.  By themselves, these code fragments generally do\nnot solve complete problems of interest. Some examples include reductions,\nprefix scans, and ghost cell updates.This chapter covers\nWhat parallel algorithms and patterns are and \ntheir importance\nHow to compare the performance of different \nalgorithms\nWhat distinguishes parallel algorithms from \nother algorithms\n125 Algorithm analysis for parallel computing applications\nWe will show the reduction in section 5.7, the prefix scan in section 5.6, and ghost cell\nupdates in section 8.4.2. In one context, a parallel procedure can be considered an\nalgorithm, and in another, it can be a pattern. The real difference is whether it is\naccomplishing the main goal or just part of a larger context. Recognizing patterns\nthat are “parallel friendly” is important to prepare for later parallelization efforts. \n5.1 Algorithm analysis for parallel computing applications\nThe development of parallel algorithms is a young field. Even the terminology and\ntechniques to analyze parallel algorithms are still stuck in the serial world. One of the\nmore traditional ways to evaluate algorithms is by looking at their algorithmic com-\nplexity. Our definition of algorithmic complexity follows.\nDEFINITION Algorithmic complexity  is a measure of the number of operations\nthat it would take to complete an algorithm. Algorithmic complexity is a\nproperty of the algorithm and is a measure of the amount of work or opera-\ntions in the procedure.\nComplexity is usually expressed in asymptotic notation . Asymptotic notation is a type of\nexpression that specifies the limiting bounds of performance. Basically, the notation\nidentifies whether the run time grows linearly or whether it progresses at a more accel-\nerated rate with the problem’s size. The notation uses various forms of the letter O,\nsuch as O(N), O(N log N) or O(N2). N is the size of a long array such as the number of\ncells, particles, or elements. The combination of O() and N refers to how the cost of\nthe algorithm scales as the size N of the array grows. The O can be thought of as\n“order” as in “scales on the order of.” Generally, a simple loop over N items will be\nO(N), a double-nested loop will be O(N2), and a tree-based algorithm will be O(N log\nN). By convention, the leading constants are dropped. The most commonly used\nasymptotic notations are\nBig O —This is the worst case limit of an algorithm’s performance. Examples are\na doubly nested for loop for a large array of size N, which would be O(N2) com-\nplexity. \nBig Ω (Big Omega) —The best case performance of an algorithm. \nBig Θ (Big Theta) —The average case performance of an algorithm.\nTraditional analysis of algorithms uses algorithmic complexity, computational com-\nplexity, and time complexity interchangeably. We will define the terms a bit differently\nto help us evaluate algorithms on today’s parallel computing hardware. Time doesn’t\nscale with the amount of work and neither does the computational effort or cost. We\nthus make the following adjustments to the definitions for computational complexity\nand time complexity: \nComputational complexity (also called step complexity) is the number of steps that are\nneeded to complete an algorithm.  This complexity measurement is an attribute of\nthe implementation and the type of hardware that is used for the calculation. It\n126 CHAPTER  5Parallel algorithms and patterns\nincludes the amount of parallelism that is possible. If you’re using a vector or\nmulti-core computer, a step (cycle) can be four or more floating-point opera-\ntions. Can you use these additional operations to reduce the number of steps?\nTime complexity takes into account the actual cost of an operation on a typical modern\ncomputing system.  The largest adjustment for time is to consider the cost of mem-\nory loads and the caching of data.\nWe’ll use complexity analysis for some of our algorithm comparisons, such as the pre-\nfix sum algorithms in section 5.5. But for applied computer scientists, the asymptotic\ncomplexity of an algorithm is somewhat one-dimensional and of limited use. It only\ntells us the cost of an algorithm in the limit as it grows larger. In an applied setting, we\nneed a more complete model of an algorithm. We’ll see why in the next section.\n5.2 Performance models versus algorithmic complexity\nWe first introduced performance models in chapter 4 to analyze the relative perfor-\nmance of different data structures. In a performance model, we build a much more\ncomplete description of the performance of an algorithm than in algorithmic com-\nplexity analysis. The biggest difference is that we don’t hide the constant multiplier in front\nof the algorithm . But there is also a difference in the terms such as log N for scaling. The\nactual count of operations is from a binary tree and should be log 2 N. \n In traditional algorithmic complexity analysis, the difference between the two log-\narithmic terms is a constant that gets absorbed into the constant multiplier. In com-\nmon world problems, these constants could matter and don’t cancel out; therefore,\nwe need to use a performance model to differentiate between different approaches of\na similar algorithm. To further understand the benefits of using performance models,\nlet’s start with an example from everyday life.\nReturning to the first algorithm in the example, let’s assume for simplicity’s sake that\nthe folders remain after the packets are handed out to a participant, such that the\nnumber of folders stays constant at the original number. There are N participants and\nN folders, creating a doubly nested loop. The computation is of order N2 operations,Example \nYou are one of the organizers of a conference with 100 participants. You want to\nhand out registration packets to each participant. Here are some algorithms that you\ncan use:\n1Have all 100 participants form a line as you search through 100 folders to find\nthe packet to hand to each participant in turn. The worst case is that you will\nhave to look through all 100 folders. On average, you will look through 50. If\nthere is no packet, you have to look through all 100.\n2Presort the packets alphabetically prior to registration. Now you can use a bisec-\ntion search to find each folder.\n127 Performance models versus algorithmic complexity\nor O(N2) in Big O asymptotic notation for the worst case. If the folders decreased\neach time, the computation would be ( N + N – 1 + N – 2 . . .) or an O(N2) algorithm.\nThe second algorithm can exploit the sorted order of the folders with a bisection\nsearch so the algorithm can be done for the worst case in O(N log N) operations. \n Asymptotic complexity tells us how the algorithm performs as we reach large sizes,\nsuch as one million participants. But we won’t ever have one million participants. We\nwill have a finite size of 100 participants. For finite sizes, a more complete picture of\nthe algorithmic performance is needed. \n To illustrate this, we apply a performance model to one of the most basic com-\nputer algorithms to see how it might give us more insight. We’ll use a time-based\nmodel where we include the real hardware costs rather than an operation-based\ncount. In this example, we look at a bisection search, also known as a binary search. It\nis one of the most common computer algorithms and numerical optimization tech-\nniques. Conventional asymptotic analysis says that the binary search is much faster\nthan a linear search. We’ll show that accounting for how real computers function, the\nincrease in speed is not as much as might be expected. This analysis also helps to\nexplain the table lookup results in section 5.5.1.\nThough asymptotic complexity is used to understand performance as an algorithm\nscales, it does not provide an equation for absolute performance. For a given prob-\nlem, linear search, which scales linearly, might outperform a bisection search, which\nscales logarithmically. This is especially true when you parallelize the algorithm as it isExample: Bisection search versus linear search \nThe bisection search algorithm can be used to find the correct entry in a sorted array\nof 256 integer elements. This algorithm takes the midpoint, bisecting the remaining\npossible range in a recursive manner. In a performance model, a bisection search\nwould have log 2 256 steps or 8, while a linear search would have a worst case of 256\nsteps and an average of 128. If we count cache line loads for a 4-byte integer array,\nthe linear search would only be 16 cache line loads for the worst case and 8 on aver-\nage. The binary search would require 4 cache line loads for the worst case and about\n4 for the average case. It is imperative to highlight that this linear search would only\nbe two times slower than a binary search, instead of the 16 times slower that we\nmight expect. \nIn this analysis, we assume any operation on data in the cache is essentially free\n(actually a couple of cycles), while a cache line load is on the order of about 100\ncycles. We just count the cache line loads and ignore the comparison operations. For\nour time-based performance model, we would say the cost of the linear search is\n(n / 16) / 2 = 8 and the bisection search is log 2(n / 16) = 4.\nHow the cache behaves changes the result by a large amount for these short array\nlengths.  The bisection search is still faster, but not by as much as the simpler anal-\nysis would have us expect.\n128 CHAPTER  5Parallel algorithms and patterns\nmuch simpler to scale a linearly scaled algorithm than a logarithmically scaled algo-\nrithm. In addition, the computer is designed to linearly walk through an array and\nprefetch data, which can speedup performance a little more. Finally, the specific prob-\nlem can have the item occurring at the beginning of the array, where a bisection\nsearch performs much worse than a linear search.\n As an example of other parallel considerations, let’s look at the implementation of\nthe search on 32 threads of a multi-core CPU or a GPU. The set of threads must wait\nfor the slowest to complete during each operation. The bisection search always takes 4\ncache loads. The linear search varies in the number of cache lines required for each\nthread. The worst case controls how long the operation takes, making the cost closer\nto 16 cache lines than the average of 8 cache lines.\n So you ask, how does this work in practice? Let’s look at the two variations of the\ntable lookup code described in the example. You can test the following algorithms on\nyour system with the perfect hash code included in the accompanying source code for\nthis chapter at https:/ /github.com/EssentialsofParallelComputing/Chapter5 . First, the\nfollowing listing shows the linear search algorithm version of the table lookup code.\nPerfectHash/table.c\n268 double *interpolate_bruteforce(int isize, int xstride,\n       int d_axis_size, int t_axis_size, double *d_axis, double *t_axis,\n269    double *dens_array, double *temp_array, double *data)\n270 {\n271   int i;\n272 \n273   double *value_array=(double *)malloc(isize*sizeof(double));\n274 \n275   for (i = 0; i<isize; i++){\n276     int tt, dd;\n277 \n276     int tt, dd; \n277 \n278     for (tt=0; tt<t_axis_size-2 &&                \n             temp_array[i] > t_axis[tt+1]; tt++);     \n279     for (dd=0; dd<d_axis_size-2 &&                \n             dens_array[i] > d_axis[dd+1]; dd++);     \n280 \n281     double xf = (dens_array[i]-d_axis[dd])/    \n                    (d_axis[dd+1]-d_axis[dd]);     \n282     double yf = (temp_array[i]-t_axis[tt])/    \n                    (t_axis[tt+1]-t_axis[tt]);     \n283     value_array[i] =\n                 xf *     yf *data(dd+1,tt+1)      \n284       + (1.0-xf)*     yf *data(dd,  tt+1)      \n285       +      xf *(1.0-yf)*data(dd+1,tt)        \n286       + (1.0-xf)*(1.0-yf)*data(dd,  tt);       \n287 \n288   }Listing 5.1 Linear search algorithm in a table lookup \nSpecifies a linear \nsearch from 0 to \naxis_size\nInterpolation\n129 Performance models versus algorithmic complexity\n289 \n290   return(value_array);\n291 }\nThe linear search of the two axes is done in lines 278 and 279. The coding is simple\nand straightforward, resulting in a cache-friendly implementation. Now let’s look at\nthe bisection search in the following listing.\nPerfectHash/table.c\n293 double *interpolate_bisection(int isize, int xstride,\n       int d_axis_size, int t_axis_size, double *d_axis, double *t_axis,\n294    double *dens_array, double *temp_array, double *data)\n295 {\n296   int i;\n297 \n298   double *value_array=(double *)malloc(isize*sizeof(double));\n299 \n300   for (i = 0; i<isize; i++){\n301     int tt = bisection(t_axis, t_axis_size-2,    \n                           temp_array[i]);           \n302     int dd = bisection(d_axis, d_axis_size-2,    \n                           dens_array[i]);           \n303 \n304     double xfrac = (dens_array[i]-d_axis[dd])/     \n                       (d_axis[dd+1]-d_axis[dd]);      \n305     double yfrac = (temp_array[i]-t_axis[tt])/     \n                       (t_axis[tt+1]-t_axis[tt]);      \n306     value_array[i] =\n                xfrac *     yfrac *data(dd+1,tt+1)     \n307      + (1.0-xfrac)*     yfrac *data(dd,  tt+1)     \n308      +      xfrac *(1.0-yfrac)*data(dd+1,tt)       \n309      + (1.0-xfrac)*(1.0-yfrac)*data(dd,  tt);      \n310   }\n311 \n312   return(value_array);\n313 }\n314 \n315 int bisection(double *axis, int axis_size, double value)\n316 {\n317   int ibot = 0;                    \n318   int itop = axis_size+1;          \n319 \n320   while (itop - ibot > 1){         \n321     int imid = (itop + ibot) /2;   \n322     if ( value >= axis[imid] )     \n323       ibot = imid;                 \n324     else                           \n325       itop = imid;                 \n326   }                                \n327   return(ibot);\n328 }Listing 5.2 Bisection search algorithm in a table lookup \nBisection \ncalls\nInterpolation\nBisection \nalgorithm",16562
58-5.5 Spatial hashing A highly-parallel algorithm.pdf,58-5.5 Spatial hashing A highly-parallel algorithm,"130 CHAPTER  5Parallel algorithms and patterns\nThe bisection code is slightly longer than the linear search (listing 5.1), but it should\nhave less operational complexity. We’ll look at other table search algorithms in section\n5.5.1 and show their relative performance in figure 5.8. \n Spoiler:  the bisection search is not much faster than the linear search as you might\nexpect, even accounting for the cost of the interpolation.  Although this is the case,\nthis analysis shows the linear search is not as slow as you might expect either. \n5.3 Parallel algorithms: What are they?\nNow let’s take another example from everyday life to introduce some ideas for parallel\nalgorithms. This first example demonstrates how an algorithmic approach that is com-\nparison-free and less synchronous can be easier to implement and can perform better\nfor highly parallel hardware. We discuss additional examples in the following sections\nthat highlight spatial locality, reproducibility, and other important attributes for paral-\nlelism and then summarize all of the ideas in section 5.8 at the end of the chapter. \nExample: Comparison sort versus hash sort\nYou want to sort the 100 participants in the auditorium from the previous example.\nFirst, you try a comparison sort.\n1Sort a room by having each person compare their last name to their neighbor in\ntheir row and move left if their last name is earlier in alphabetical sequence and\nright if later.\n2Continue for up to N steps, where N is the number of people in the room.\nFor GPUs, a workgroup cannot communicate with another workgroup. Let’s assume\neach row in the auditorium is a workgroup. When you reach the end of the row, it is\nlike you have reached the limit of the GPU workgroup, and you have to exit the kernel\nto do a comparison with the next row. Having to exit the kernel means that it takes\nmultiple kernel invocations, adding to coding complexity and to run time. There will\nbe more on how a GPU functions in chapters 9 and 10.\nNow, let’s look at a different sorting algorithm: the hash sort. The best way to under-\nstand a hash sort is through the following example. We’ll go into more detail on the\nelements of a hash function in section 5.4. \n1Place a sign for each letter at the front of the room.\n2Each person goes to the table with the first letter of their last name.\n3For letters with large numbers of participants, repeat for the second letter in the\nlast name. For small numbers of participants, do any simple sort, including the\none described previously.\nThe first sort is a comparison sort using a bubble sort algorithm, which generally\nperforms poorly. The bubble sort  steps through a list, compares adjacent elements,\nand swaps these if they are in an incorrect order. The algorithm goes repeatedly\nthrough the list until the list is sorted. The best comparison sort has an algorithmic\ncomplexity limit of O(N log N). The hash sort  breaks this barrier because it doesn’t\n131 What is a hash function?\n5.4 What is a hash function?\nIn this section, we will discuss the importance of a hash function. Hashing techniques\noriginated in the 1950s and 60s, but have been slow to be adapted to many application\nareas. Specifically, we will go through what constitutes a perfect hash, spatial hashing,\nperfect spatial hashing, along with all the promising use cases.\n A hash function  maps from a key to a value, much like a dictionary uses a word as\nthe lookup key to its definition. In figure 5.1, the word Romero  is the key that is hashed\nto look up the value, which in this case is the moniker or username. Unlike a physical\ndictionary, a computer needs at least 26 possible storage locations times the maximum\nlength of the dictionary key. So for a computer, it is absolutely necessary to encode the\nkey into a shorter form called a hash. The term hash or hashing  refers to “chopping\nup” the key into a shorter form to use as an index to store the value. The location for\nstoring the collection of values for a specific key is called a bucket  or bin. There are\nmany different ways to generate a hash from a key; the best approaches are generally\nproblem-specific.\nA perfect hash  is one where there is one entry in each bucket at most. Perfect hashes are\nsimple to handle, but can take more memory. A minimal perfect hash  is just one entry inuse comparisons. On average, the hash sort is an Θ(1) operation for each participant\nand Θ(N) for all participants. The faster performance is significant; more importantly,\nthe operations for each participant are completely independent. Having the opera-\ntions completely independent makes the algorithm easy to parallelize, even on less\nsynchronous GPU architectures.\nCombining this all together, we can use the hash sort to put the participants and fold-\ners in alphabetical order and add parallelism by having multiple lines divide up by the\nalphabet at the registration table. The two hash sorts will be Θ(N) and in parallel will\nbe Θ(N/P) where P is the number of processors. For 16 processors and 100 partici-\npants, the parallel speedup from the serial, brute-force method is 1002/(100/16) =\n1,600x. We recognize that the hash design is similar to what we see in well-organized\nconferences or school registration lines.\nRomero\nLandryKey Value\n59\nHash key calculation\n1st letter ASCII code – 64 + 26\n+\n2nd letter ASCII code – 6413\ndarsalHash\nFigure 5.1 Hash table to lookup a computer moniker \nby last name. In ASCII, R is 82 and O is 79. We can \nthen calculate the first hash key with 82 – 64 + 26 + \n79 – 4 = 59. The value stored in the hash table is the \nusername, sometimes called a moniker.\n132 CHAPTER  5Parallel algorithms and patterns\neach bucket and with no empty buckets. It takes longer to calculate minimal perfect\nhashes, but for example, for fixed sets of programming keywords, the extra time is\nworth it. For most of the hashes we’ll discuss here, the hashes will be created on the\nfly, queried, and thrown away, so a faster creation time is more important than mem-\nory size. Where a perfect hash is not feasible or takes too much memory, a compact\nhash can be employed. A compact hash  compresses the hash so that it requires less stor-\nage memory. As always, there are tradeoffs in programming complexity, run time, and\nrequired memory among the different hashing methods.\n The load factor  is the fraction of the hash that is filled. It is computed by n/k, where\nn is the number of entries in the hash table and k is the number of buckets. Compact\nhashes still work at load factors of .8 to .9, but the efficiency drops off after that due to\ncollisions. Collisions  occur when more than one key wants to store its value in the same\nbucket. It is important to have a good hash function that distributes keys more uni-\nformly, avoiding the clustering of entries, thereby allowing higher load factors. With a\ncompact hash, both the key and the value are stored so that on retrieval, the key can\nbe checked to see if it is the right entry.\n In the previous examples, we used the first letter of the last name as a simple hash\nkey. While effective, there are certainly flaws with using the first letter. One is that the\nnumber of last names starting with each letter in the alphabet is not evenly distrib-\nuted, leading to unequal numbers of entries in each bucket. We could instead use the\ninteger representation of the string, which produces a hash for the first four letters of\nthe name. But the character set gives only 52 possible values for the 256 storage loca-\ntions for each byte, leading to only a fraction of possible integer keys. A special hash\nfunction that expects only characters would need far fewer storage locations. \n5.5 Spatial hashing: A highly-parallel algorithm\nOur discussion in chapter 1 used a uniform-sized, regular grid from the Krakatau\nexample in figure 1.9. For this discussion on parallel algorithms and spatial hashing,\nwe need to use more complex computational meshes. In scientific simulations, more\ncomplex meshes define with more detail the areas that we are interested in. In big\ndata, specifically image analysis and categorization, these more complex meshes are\nnot widely adopted. Yet the technique would have great value there; when a cell in the\nimage has mixed characteristics, just split the cell. \n The biggest impediment to using more complex meshes is that coding becomes\nmore complicated and we must incorporate new computational techniques. For com-\nplex meshes, it is a greater challenge to find methods that work and scale well on par-\nallel architectures. In this section, we’ll show you how you can handle some of the\ncommon spatial operations with highly parallel algorithms.\n \n \n \n \n133 Spatial hashing: A highly-parallel algorithm\nCell-based adaptive mesh refinement (AMR) belongs to a class of unstructured mesh\ntechniques that no longer have the simplicity of a structured grid to locate data. In\ncell-based AMR (figure 5.2), the cell data arrays are one-dimensional, and the data\ncan be in any order. The mesh locations are carried along in additional arrays that\nhave the size and location information for each cell. Thus, there is some structure to\nthe grid, but the data is completely unstructured. Taking this further into unstruc-\ntured territory, a fully-unstructured mesh could have cells of triangles, polyhedra, or\nother complex shapes. This allows the cells to “fit” the boundaries between land and\nocean, but at the cost of more complex numerical operations. Because many of the\nsame parallel algorithms for unstructured data apply to both, we’ll work mostly with\nthe cell-based AMR example.\nAMR techniques can be broken down into patch, block, and cell-based approaches. The\npatch and block methods use various size patches or fixed-size blocks that can at least\npartially exploit the regular structure of these groups of cells. Cell-based AMR has truly\nunstructured data that can be in any order. A shallow-water, cell-based AMR mini-app,\nCLAMR ( https:/ /github.com/lanl/CLAMR.git ), was developed by Davis, Nicholaeff,\nand Trujillo while they were summer students in 2011 at Los Alamos National Labora-\ntory. They wanted to see if cell-based AMR applications could run on GPUs. In theExample: Krakatau wave simulation\nYour team is working on their wave application and decides they need more resolu-\ntion for the simulation in some areas at the wave front and at the shoreline as shown\nin figure 1.9. They don’t, however, need fine resolution in other parts of the grid. The\nteam decides to take a look at adaptive mesh refinement (AMR), where they can put\nfiner mesh resolution in areas that need it.\nFigure 5.2 A cell-based AMR mesh for a wave \nsimulation from the CLAMR mini-app. The black \nsquares are the cells and the variously-shaded \nsquares represent the height of a wave radiating \noutward from the upper right corner. \n134 CHAPTER  5Parallel algorithms and patterns\nprocess, they found breakthrough parallel algorithms that also made CPU implemen-\ntations run faster. The most important of these was a spatial hash.\n Spatial hashing  is a technique where the key is based on spatial information. The\nhashing algorithm retains the same average algorithmic complexity of Θ(1) opera-\ntions for each lookup. All spatial queries can be performed with a spatial hash; many\nare much faster than alternative methods. The basic principle is to map objects onto a\ngrid of buckets arranged in a regular pattern. \n A spatial hash is shown in the center of figure 5.3. The sizing of the buckets is\nselected based on the characteristic size of the objects to map. For a cell-based AMR\nmesh, the minimum cell size is used. For particles or objects, as shown on the right in\nthe figure, the cell size is based on the interaction distance. This choice means that\nonly the cells immediately adjacent need to be queried for interaction or collision cal-\nculations. Collision calculations are one of the great application areas for spatial\nhashes, not only in scientific computing for smooth particle hydrodynamics, molecu-\nlar dynamics, and astrophysics, but also in gaming engines and computer graphics.\nThere are many situations where we can exploit spatial locality to reduce computa-\ntional costs.\nBoth the AMR and the unstructured mesh on the left in the figure are referred to as\ndifferential discretized data  because the cells are smaller where the gradients are steeper\nto better resolve the physical phenomena. But these have a limit to how much smaller\nthe cells can get. The limit keeps the bucket sizes from getting too small. Both meshes\nstore their cell indices in all the underlying buckets of the spatial hash. For the parti-\ncles and geometric objects, the particle indices and object identifiers are stored in the\nAiAi\nAMR and unstructured meshDifferential discretized data used\nas a computational meshSpatial hash based on\nminimum cell size\n2D spatial hashBk\nInteraction\nDistance\nδ = Cell_Sizemin\nAi\nAi{B0,B1,B14,B15}Binning computational elements based\non interaction distance in cell lists\nParticle-based computations\nlocality-enforcing schemes\nFigure 5.3 Computational meshes, particles, and objects mapped onto a spatial hash. The polyhedra of the \nunstructured mesh and the rectangular cells of the cell-based adaptive refinement mesh can be mapped to a \nspatial hash for spatial operations. Particles and geometric objects can also benefit from being mapped to a \nspatial hash to provide information about their spatial locality so that only nearby items need to be considered.",13696
59-5.5.1 Using perfect hashing for spatial mesh operations.pdf,59-5.5.1 Using perfect hashing for spatial mesh operations,"135 Spatial hashing: A highly-parallel algorithm\nbuckets. This provides a form of locality that keeps the computational cost from\nincreasing as the problem size increases. For example, if the problem domain is\nincreased on the left and top, the interaction calculation in the lower right of the spa-\ntial hash stays the same. The algorithmic complexity thus stays Θ(N) for the particle\ncalculations instead of growing to Θ(N2). The following listing shows the pseudo code\nfor the interaction loop, which is over nearby locations for the inner loop instead of\nhaving to search through all the particles.\n1 forall particles, ip, in NParticles{\n2    forall particles, jp, in Adjacent_Buckets{\n3       if (distance between particles < interaction_distance){\n4          perform collision or interaction calculation\n5       }\n6    }\n7 }\n5.5.1 Using perfect hashing for spatial mesh operations\nWe’ll first look at perfect hashing to focus on the use of hashing rather than the\nmechanics internal to hashing. These methods all rely on being able to guarantee that\nthere will be only one entry in each bucket, avoiding the issues of handling collisions\nwhere a bucket might have more than one data entry. For perfect hashing, we’ll inves-\ntigate the four most important spatial operations:\nNeighbor finding —Locating the one or two neighbors on each side of a cell\nRemapping —Mapping another AMR mesh onto a current AMR mesh\nTable lookup —Locating the intervals in the 2D table to perform the interpolation\nSorting —A 1D or 2D sort of the cell data\nAll of the source code for the examples for the four operations in 1D and 2D is avail-\nable at https:/ /github.com/lanl/PerfectHash.git  under an open source license. The\nsource is also linked into the examples for this chapter. The perfect hash code uses\nCMake and tests for the availability of OpenCL. If you do not have OpenCL capability,\nthe code detects that and will not compile the OpenCL implementations. The rest of\nthe cases on the CPU will still run.\nNEIGHBOR  FINDING  USING  A SPATIAL  PERFECT  HASH\nNeighbor finding is one of the most important spatial operations. In scientific com-\nputing, the material moved out of one cell has to move into the adjacent cell. We need\nto know which cell to move to in order to compute the amount of material and move\nit. In image analysis, the characteristics of the adjacent cell can give important infor-\nmation on the composition of the current cell.\n The rules for the AMR mesh in CLAMR are that there can be only a single-level\njump in refinement across a face of a cell. Also, the neighbor list of each cell on each\nside is just one of the neighbor cells, and the choice is to be the lower cell or the cellListing 5.3 Particle interaction pseudo-code\n136 CHAPTER  5Parallel algorithms and patterns\nto the left of each pair as figure 5.4 shows. The second of the pair is found by using\nthe neighbor list of the first cell; for example, ntop[nleft[ic]] . The problem then\nbecomes setting up the neighbor arrays for every cell.\nWhat are the possible algorithms for finding neighbors? The naive way is to search all\nthe other cells for the cell that is adjacent. This can be done by looking at the i, j, and\nlevel variables in each cell. The naive algorithm is O(N2). It performs well with small\nnumbers of cells, but the run-time complexity grows large quickly. Some common\nalternative algorithms are tree-based, such as the k-D tree and quadtree algorithms\n(octree in three dimensions). These are comparison-based algorithms that scale as\nO(N log N), which are defined later. The code for the 2D neighbor calculation,\nincluding the k-D tree, brute force, CPU, and GPU hash implementations is available\nat https:/ /github.com/lanl/PerfectHash.git , along with the other spatial perfect hash\napplications discussed later in this chapter.\n The k-D tree splits the mesh into two equal halves in the x-dimension and then two\nequal halves in the y-dimension, repeating until it finds the object. The algorithm to\nbuild the k-D tree is O(N log N), and each search is also O(N log N). \n The quadtree has four children for each parent, one for each quadrant. This\nexactly maps to the subdivision of the cell-based AMR mesh. A full quadtree starts\nfrom the top, or root, with one cell and subdivides to the finest level of the AMR\nmesh. A “truncated” quadtree starts from the coarsest level of the mesh and has a\nquadtree for each coarse cell to map down to the finest level. The quadtree algorithm\nis a comparison-based algorithm: O(N log N). \n The limitation of just one level jump across a face is called a graded mesh . In cell-\nbased AMR, graded meshes are common, but other quadtree applications such as n-body\napplications in astrophysics result in much larger jumps in the quadtree data structure.ntop\nnlft[ic]\nnbot[ic]\nnrhtnrhtic\nLevel0\n1 2 3 41\n12i for level 1i for level 0\n011234\n12j for level 1j for level 0Cell ic is at i= , j= , level= 11 1\nBottom neighbors are:\nnbot[ic] & nrht[nbot[ic]]\nntop[nlft[ic]]\n1 level jump\nin reﬁnement\nnrht[nbot[ic]]ntop\nFigure 5.4 The left neighbor is the lower \ncell of the two to the left, and the bottom \nneighbor is the cell to the left of the two \nbelow. Similarly, the right neighbor is the \nlower cell of the two to the right, and the top \nneighbor is to the left of the two cells above.\n137 Spatial hashing: A highly-parallel algorithm\nThe one-level jump in refinement allows us to improve the algorithm design for find-\ning neighbors. We can start our search at the leaf that represents our cell and, at most,\nwe only have to go up two levels of the tree to find our neighbor. For searching for a\nnear neighbor of similar size, the search should start at the leaves and use a quadtree.\nFor searches for large irregular objects, the k-D tree should be used and the search\nshould start from the root of the tree. Proper use of tree-based search algorithms can\nprovide a viable implementation on CPUs, but the comparisons and tree construction\npresent difficulties on GPUs, where comparisons beyond the work group cannot be\ndone easily.\n This sets the stage for the design of a spatial hash to perform the neighbor finding\noperation. We can guarantee that there are no collisions in our spatial hash by making\nthe buckets in the hash the size of the finest cells in the AMR mesh. The algorithm\nthen becomes\nAllocate a spatial hash the size of the finest level of the cell-based AMR mesh\nFor each cell in the AMR mesh, write the cell number to the hash buckets\nunderlying the cell\nCompute the index for a finer cell one cell outside the current cell on each side\nRead the value placed in the hash bucket at that location\nFor the mesh shown in figure 5.5, the write phase is followed by a read phase to look\nup the index of the right neighbor cell. \nEach cell writes its cell number to the hash buckets it covers.\nRight neighbor of cell 2 is at col 8, row 3. Look up in hash and it is cell 26. 1034676871 70\n56669\n6563\n6454\n1061\n15\n16\n17 1814 191312116059 625655 57\n58\n20 21\n2232 3435 3338 3637 3951 5052 4953 48 47\n45 46\n44\n27\n25\n24 232631 2829 3041 4243 40\n2\n187 6\n9\n543210\n1 1 1 0 9876543210012345\n01234567891011\n566\n6563\n64\n1061\n15\n16 14 191312116059 625655 57\n58\n20 21\n2232 3435 3338 3637 3951 5052 49\n31 2829 3041 4243 40\n87 6\n9\n1 1 1 0 987654321001234567891011\n000\n0111\n12 222\n333 3444 4\n17\n17 1717 18\n18 1818 23\n232626 26\n26\n23\n23 24 2424 2425 2525 2527 2727 2744 4444 4446 45 45 4646 46 45 4548 48 47 4747 47 48 48 53\n53 5353 54\n54 5454 70\n70\n69\n69 69697070 71\n71\n68\n68\n67\n67 676768687171\nLevelAMR mesh Hash table\nij\nFigure 5.5 Finding the right neighbor of cell 21 using a spatial perfect hash\n138 CHAPTER  5Parallel algorithms and patterns\nThis algorithm is well suited to GPUs and is shown in listing 5.5. The first implementa-\ntion took less than a day to port from the CPU to the GPU. The original k-D tree\nwould take weeks or months to implement on the GPU. The algorithmic complexity\nalso breaks the O(log N) threshold and is, on average, Θ(N) for N cells. \n This first implementation of the perfect hash neighbor calculation was an order of\nmagnitude faster on the CPU than the k-D tree method, and an additional order\nof magnitude faster on the GPU than a single core of a CPU for a total of 3,157x\nspeedup (figure 5.6). The algorithm performance study was done on an NVIDIA\nV100 GPU and a Skylake Gold 5118 CPU with a nominal clock frequency of 2.30 GHz.\nAll the results in this chapter used this architecture as well. The CPU core and GPU\narchitecture are the best available around 2018, giving a Best(2018)  parallel speedup\ncomparison (see section 1.6 for speedup notation). But it isn’t an architecture com-\nparison between the CPU and the GPU. If the 24 virtual cores on this CPU were uti-\nlized, the CPU would also see a considerable parallel speedup.\nHow hard is it to write the code for this kind of performance? Let’s take a look at the\ncode for the hash table in listing 5.4 for the CPU. The input to the routine are the 1D\narrays, i, j, and level , where level  is the refinement level, and i and j are the row\nand column of the cell in the mesh at that cell’s refinement level. The whole listing is\nabout a dozen lines.\n \n 3,157\n66\n1\nk-D tree Hash single core (CPU) Parallel hash (GPU)100101102103104\nSpeedup relative to base algorithm2D neighbor calculation with various algorithms\nFour levels reﬁnement\n256×256 coarse mesh\n2.9 million cells\nFigure 5.6 The algorithm and parallel speedup total 3,157x. The new \nalgorithm enables the parallel speedup on the GPU.\n139 Spatial hashing: A highly-parallel algorithm\nneigh2d.c from PerfectHash\n452 int *levtable = (int *)malloc(levmx+1);    \n453 for (int lev=0; lev<levmx+1; lev++)        \n       levtable[lev] = (int)pow(2,lev);        \n454\n455 int jmaxsize = mesh_size*levtable[levmx];    \n456 int imaxsize = mesh_size*levtable[levmx];    \n457 int **hash = (int **)genmatrix(jmaxsize,    \n                 imaxsize, sizeof(int));        \n458\n459 for(int ic=0; ic<ncells; ic++){          \n460    int lev = level[ic];\n461    for (int jj=j[ic]*levtable[levmx-lev];\n            jj<(j[ic]+1)*levtable[levmx-lev]; jj++) {\n462       for (int ii=i[ic]*levtable[levmx-lev];\n               ii<(i[ic]+1)*levtable[levmx-lev]; ii++) {\n463          hash[jj][ii] = ic;\n464       }\n465    }\n466 }\nThe loops at lines 459, 461, and 462 reference the 1D arrays i, j, and level ; level  is\nthe refinement level where 0 is the coarse level and 1 to levmax  are the levels of refine-\nment. The arrays i and j are the row and column of the cell in the mesh at that cell’s\nrefinement level.\n Listing 5.5 shows the code for writing out the spatial hash in OpenCL for the GPU,\nwhich is similar to listing 5.4. Although we haven’t covered OpenCL yet, the simplicity of\nthe GPU code is clear, even without understanding all the details. Let’s do a brief com-\nparison to get a sense of how code has to change for the GPU. We define a macro to\nhandle the 2D indexing and to make the code look more like the CPU version. Then\nthe biggest difference is that there is no cell loop. This is typical of GPU code, where the\nouter loops are removed and are instead handled by the kernel launch. The cell index is\nprovided for each thread by a call to the get_global_id  intrinsic. There will be more on\nthis example and writing OpenCL code, in general, in chapter 12.\nneigh2d_kern.cl from PerfectHash\n 77 #define hashval(j,i) hash[(j)*imaxsize+(i)]\n 78\n 79 __kernel void hash_setup_kern(\n 80       const uint isize,\n 81       const uint mesh_size,\n 82       const uint levmx,\n 83       __global const int  *levtable,   \n 84       __global const int  *i,          \n 85       __global const int  *j,          \n 86       __global const int  *level,      Listing 5.4 Writing out a spatial hash table for the CPU \nListing 5.5 Writing out a spatial hash table on the GPU in OpenCL Constructs a table \nof powers of two \n(1, 2, 4, …)\nSets the number of rows and \ncolumns at the finest level\nAllocates the hash table\nMaps cells to \nhash table\nPasses in the table \nof powers of 2 along \nwith i, j, and level\n140 CHAPTER  5Parallel algorithms and patterns\n 87       __global int  *hash\n 88       ) { \n 89 \n 90    const uint ic = get_global_id(0);     \n 91    if (ic >= isize) return;            \n 92 \n 93    int imaxsize = mesh_size*levtable[levmx];\n 94    int lev = level[ic];\n 95    int ii = i[ic];\n 96    int jj = j[ic];\n 97    int levdiff = levmx - lev;\n 98\n 99    int iimin =  ii   *levtable[levdiff];    \n100    int iimax = (ii+1)*levtable[levdiff];    \n101    int jjmin =  jj   *levtable[levdiff];    \n102    int jjmax = (jj+1)*levtable[levdiff];    \n103 \n104    for (   int jjj = jjmin; jjj < jjmax; jjj++) {\n105       for (int iii = iimin; iii < iimax; iii++) {\n106          hashval(jjj, iii) = ic;       \n107       }   \n108    }   \n109 }\nThe code for retrieving the neighbor indexes is also simple as shown in listing 5.6,\nwith just a loop across the cells and a read of the hash table at where the neighbor\nlocation would be on the finest level of the mesh. You can find the locations of the\nneighbors by incrementing the row or column by one cell in the direction needed.\nFor the left or bottom neighbor, the increment is 1, while for the right or top neigh-\nbor, the increment is the full width of the mesh in the x-direction or imaxsize .\nneigh2d.c from PerfectHash\n472 for (int ic=0; ic<ncells; ic++){\n473   int ii = i[ic];\n474   int jj = j[ic];\n475   int lev = level[ic];\n476   int levmult = levtable[levmx-lev];\n477   int nlftval = \n         hash[     jj   *levmult              ]      \n             [MAX( ii   *levmult-1,0         )];     \n478   int nrhtval = \n         hash[     jj   *levmult              ]      \n             [MIN((ii+1)*levmult,  imaxsize-1)];     \n480   int nbotval = \n         hash[MAX( jj   *levmult-1,0)         ]      \n             [      ii   *levmult             ];     \n481   int ntopval = \n         hash[MIN((jj+1)*levmult,  jmaxsize-1)]      \n             [      ii   *levmult             ];     Listing 5.6 Finding neighbors from a spatial hash table on the CPU The loop across the cells is \nimplied by the GPU kernel; \neach thread is a cell.\nThe return is important \nto avoid reading past \nend of the arrays.\nComputes the bounds \nof the underlying hash \nbuckets to set\nSets the hash table \nvalue to the thread \nID (the cell number)\nCalculates the \nneighbor cell location \nfor the query, using a \nmax/min to keep it in \nbounds\n141 Spatial hashing: A highly-parallel algorithm\n482   neigh2d[ic].left  = nlftval;    \n483   neigh2d[ic].right = nrhtval;    \n484   neigh2d[ic].bot   = nbotval;    \n485   neigh2d[ic].top   = ntopval;    \n486}\nFor the GPU, we again remove the loop for the cells and replace it with a get_\nglobal_id  call as shown in the following listing. \nneigh2d_kern.cl from PerfectHash\n113 #define hashval(j,i) hash[(j)*imaxsize+(i)]\n114\n115 __kernel void calc_neighbor2d_kern(\n116       const int isize,\n117       const uint mesh_size,\n118       const int levmx,\n119       __global const int *levtable,\n120       __global const int *i, \n121       __global const int *j, \n122       __global const int *level,\n123       __global const int *hash,\n124       __global struct neighbor2d *neigh2d\n125       ) {\n126\n127    const uint ic  = get_global_id(0);    \n128    if (ic >= isize) return;\n129\n130    int imaxsize = mesh_size*levtable[levmx];\n131    int jmaxsize = mesh_size*levtable[levmx];\n132\n133    int ii = i[ic];      \n134    int jj = j[ic];\n135    int lev = level[ic];\n136   int levmult = levtable[levmx-lev];\n137\n138    int nlftval = hashval(     jj   *levmult              ,\n                             max( ii   *levmult-1,0         )); \n139    int nrhtval = hashval(     jj   *levmult              ,\n                             min((ii+1)*levmult,  imaxsize-1));\n140    int nbotval = hashval(max( jj   *levmult-1,0)         ,\n                                  ii   *levmult              );  \n141    int ntopval = hashval(min((jj+1)*levmult,  jmaxsize-1),\n                                  ii   *levmult              );  \n142    neigh2d[ic].left   = nlftval;\n143    neigh2d[ic].right  = nrhtval;\n144    neigh2d[ic].bottom = nbotval;\n145    neigh2d[ic].top    = ntopval;\n146 }\nCompare the simplicity of this code to the k-D tree code for the CPU, which is a thou-\nsand lines long! Listing 5.7 Finding neighbors from a spatial hash table on the GPU in OpenCLAssigns the neighbor \nvalue for output \narrays\nGets the cell ID \nfor the thread\nThe rest of code is similar \nto the CPU version.\n142 CHAPTER  5Parallel algorithms and patterns\nREMAP CALCULATIONS  USING  A SPATIAL  PERFECT  HASH\nAnother important numerical mesh operation is a remap from one mesh to another.\nFast remaps can permit different physics to be performed on meshes optimized for\ntheir individual needs. \n In this case, we will look at remapping the values from one cell-based AMR mesh to\nanother cell-based AMR mesh. Mesh remaps can also involve unstructured meshes or\nparticle-based simulations, but the techniques are more complicated. The setup phase\nis identical to the neighbor case, where the cell index for every cell is written to the\nspatial hash. In this case, the spatial hash is created for the source mesh. Then the\nread phase, shown in listing 5.8, queries the spatial hash for the cell numbers underly-\ning each cell of the target mesh and sums up the values from the source mesh into the\ntarget mesh after adjusting for the size difference of the cells. For this demonstration,\nwe have simplified the source code from the example at https:/ /github.com/Essentials\nofParallelComputing/Chapter5.git . \nremap2.c from PerfectHash\n211 for(int jc = 0; jc < ncells_target; jc++) {\n212    int ii = mesh_target.i[jc];              \n213    int jj = mesh_target.j[jc];              \n214    int lev = mesh_target.level[jc];         \n215    int lev_mod = two_to_the(levmx - lev);\n216    double value_sum = 0.0;\n217    for(int jjj = jj*lev_mod;                  \n               jjj < (jj+1)*lev_mod; jjj++) {     \n218       for(int iii = ii*lev_mod;               \n                  iii < (ii+1)*lev_mod; iii++) {  \n219          int ic = hash_table[jjj*i_max+iii];\n220          value_sum += value_source[ic] /       \n                (double)four_to_the(               \n                   levmx-mesh_source.level[ic]     \n                );                                 \n221       }\n222    }\n223    value_remap[jc] += value_sum;\n224 }\nFigure 5.7 shows the performance improvement from the remap using the spatial per-\nfect hash. There is a speedup due to the algorithm and then an additional parallel\nspeedup running on the GPU for a total speedup of over 1,000 times faster. The paral-\nlel speedup on the GPU is made possible by the ease of the algorithm implementation\non the GPU. Good parallel speedup should also be possible on the multi-core proces-\nsor as well.\n Listing 5.8 The read phase of the remapping of a value on the CPU \nGets the location \nof the target \nmesh cell\nQueries the spatial \nhash for source \nmesh cells\nSums the values \nfrom the source \nmesh, adjusting for \nrelative cell sizes\n143 Spatial hashing: A highly-parallel algorithm\nTABLE LOOKUPS  USING  A SPATIAL  PERFECT  HASH\nThe operation of looking up values from tabular data presents a different kind of\nlocality that can be exploited by a spatial hash. You can use hashing for searching for\nthe intervals on both axes for the interpolation. For this example, we used a 51x23\nlookup table of equation-of-state values. The two axes are density and temperature,\nwith an equal spacing used between values on each axis. We will use n for the length of\nthe axis and N for the number of table lookups that are to be performed. We used\nthree algorithms in this study:\nThe first is a linear search (brute force) starting at the first column and row.\nThe brute force should be an O(n) algorithm for each data query or for all N,\nO(N * n), where n is the number of columns or rows, respectively, for each axis.\nThe second is a bisection search that looks at the midpoint value of the possible\nrange and recursively narrows the location for the interval. The bisection\nsearch should be an O(log n) algorithm for each data query. \nFinally, we used a hash to do an O(1) lookup of the interval for each axis. We\nmeasured the performance of the hash on both a single core of a CPU and a\nGPU. The test code searches for the interval on both axes and a simple interpo-\nlation of the data values from the table to get the result.\nFigure 5.8 shows the performance results for the different algorithms. The results\nhave some surprises. The bisection search is no faster than the brute force (linear\nsearch) despite being an O(N log n) algorithm instead of an O(N*n) algorithm. This1,217\n21\n1\nk-D tree Hash single core (CPU) Parallel hash (GPU)100101102103104\nSpeedup relative to base algorithm2D remap calculation with various algorithms\nFour levels reﬁnement\n256×256 coarse mesh\n2.9 million cells\nFigure 5.7 The speedup of the remap algorithm due to the change of the \nalgorithm from a k-D tree to a hash on a single core of the CPU and then \nported to the GPU for a parallel speedup.\n144 CHAPTER  5Parallel algorithms and patterns\nseems to be contrary to the simple performance model, which indicates that the\nspeedup should be 4–5x for the search on each axis. With the interpolation, we’d still\nexpect around a 2x improvement. But there is a simple explanation, which you might\nguess from our discussions in section 5.2. \n The search for the interval on each axis requires, at most, only two cache loads on\none axis and four on the other for the linear search! The bisection would need the\nsame number of cache loads. By considering cache loads, we would expect no differ-\nence in performance. The hash algorithm could directly go to the correct interval,\nbut it would still need a cache load. The reduction in cache loads would be about a\nfactor of 3x. The additional improvement is probably due to the reduction in the con-\nditionals for the hash algorithm. The observed performance is in line with the expec-\ntations once we include the effect of the cache hierarchy.\nPorting the algorithm to the GPU is a bit more involved and shows what perfor-\nmance enhancements are possible in the process. To understand what was done,\nlet’s first look at the hash implementation on the CPU in listing 5.9. The code loops\nover all of the 16 million values, finding the intervals on each axis, and then interpo-\nlates the data in the table to get the resulting value. By using the hashing technique,\nwe can find the interval locations by using a simple arithmetic expression with no\nconditionals.\n \n Table lookup calculation with various algorithms\nBrute Bisection Hash single core Parallel hash (GPU)1.000 1.0054.0561,326.14316.8 million cells104\n103\n102\n101\n100Speedup relative to base algorithm\nFigure 5.8 The algorithms used for table lookup show a large speedup for \nthe hash algorithm on the GPU.\n145 Spatial hashing: A highly-parallel algorithm\ntable.c from PerfectHash\n272 double dens_incr = \n       (d_axis[50]-d_axis[0])/50.0;    \n273 double temp_incr = \n       (t_axis[22]-t_axis[0])/22.0;    \n274\n275 for (int i = 0; i<isize; i++){\n276    int tt = (temp[i]-t_axis[0])/temp_incr;    \n277    int dd = (dens[i]-d_axis[0])/dens_incr;    \n278\n279    double xf = (dens[i]-d_axis[dd])/          \n280                (d_axis[dd+1]-d_axis[dd]);     \n281    double yf = (temp[i]-t_axis[tt])/          \n282                (t_axis[tt+1]-t_axis[tt]);     \n283    value_array[i] =      \n                xf *     yf *data(dd+1,tt+1)    \n284      + (1.0-xf)*     yf *data(dd,  tt+1)    \n285      +      xf *(1.0-yf)*data(dd+1,tt)      \n286      + (1.0-xf)*(1.0-yf)*data(dd,  tt);     \n287 }\nWe could simply port this to the GPU as was done in the earlier cases by removing the\nfor loop and replacing it with a call to get_global_id . But the GPU has a small local\nmemory cache that is shared by each work group, which can hold about 4,000 double-\nprecision values. We have 1,173 values in the table and 51+23 axis values. These can fit\nin the local memory cache that can be accessed quickly and shared among all the\nthreads in the workgroup. The code in listing 5.10 shows how this is done. The first\npart of the code cooperatively loads the data values into local memory using all of the\nthreads. A synchronization is then required to guarantee that all the data is loaded\nbefore moving on to the interpolation kernel. The remaining code looks much the\nsame as the code for the CPU in listing 5.9.\ntable_kern.cl from PerfectHash\n45 #define dataval(x,y) data[(x)+((y)*xstride)]\n46\n47 __kernel void interpolate_kernel(\n48    const uint isize,\n49    const uint xaxis_size,\n50    const uint yaxis_size,\n51    const uint dsize,\n52    __global const double *xaxis_buffer,\n53    __global const double *yaxis_buffer,\n54    __global const double *data_buffer,\n55    __local        double *xaxis,\n56    __local        double *yaxis,\n57    __local        double *data,\n58    __global const double *x_array,\n59    __global const double *y_array,Listing 5.9 The table interpolation code for the CPU \nListing 5.10 The table interpolation code in OpenCL for the GPU Computes a constant increment \nfor each axis data lookup\nDetermines the interval \nfor interpolation and \nthe fraction in the \ninterval\nBi-linear interpolation \nto fill the value_array \nwith the results\n146 CHAPTER  5Parallel algorithms and patterns\n60    __global       double *value\n61    )\n62 {\n63    const uint tid = get_local_id(0);\n64    const uint wgs = get_local_size(0);\n65    const uint gid = get_global_id(0);\n66\n67    if (tid < xaxis_size) \n          xaxis[tid]=xaxis_buffer[tid];    \n68    if (tid < yaxis_size)\n          yaxis[tid]=yaxis_buffer[tid];    \n69\n70    for (uint wid = tid; wid<d_size; wid+=wgs){   \n71       data[wid] = data_buffer[wid];              \n72    }                                             \n73\n74    barrier(CLK_LOCAL_MEM_FENCE);    \n75    \n76    double x_incr = (xaxis[50]-xaxis[0])/50.0;   \n77    double y_incr = (yaxis[22]-yaxis[0])/22.0;   \n78   \n79    int xstride = 51;\n80\n81    if (gid < isize) {\n82       double xdata = x_array[gid];    \n83       double ydata = y_array[gid];    \n84\n85       int is = (int)((xdata-xaxis[0])/x_incr);   \n86       int js = (int)((ydata-yaxis[0])/y_incr);   \n87       double xf = (xdata-xaxis[is])/             \n                        (xaxis[is+1]-xaxis[is]);    \n88       double yf = (ydata-yaxis[js])/             \n                        (yaxis[js+1]-yaxis[js]);    \n89    \n90       value[gid] =\n                 xf *     yf *dataval(is+1,js+1)    \n91        + (1.0-xf)*     yf *dataval(is,  js+1)    \n92        +      xf *(1.0-yf)*dataval(is+1,js)      \n93        + (1.0-xf)*(1.0-yf)*dataval(is,  js);     \n94    }\n95 }\nThe performance result for the GPU hash code shows the impact of this optimiza-\ntion with a larger speedup than from the single core CPU performance for the other\nkernels.\nSORTING  MESH DATA USING  A SPATIAL  PERFECT  HASH\nThe sort operation is one of the most studied algorithms and forms the basis for many\nother operations. In this section, we look at the special case of sorting spatial data. You\ncan use a spatial sort to find the nearest neighbors, eliminate duplicates, simplify\nrange finding, graphics output, and a host of other operations. \n For simplicity, we’ll work with 1D data with a minimum cell size of 2.0. All cells\nmust be a power of two larger than the minimum cell size. The test case allows up toLoads the axis \ndata values\nLoads the \ndata table\nNeeds to synchronize \nbefore table queries\nComputes a constant increment \nfor each axis data lookup\nLoads the next \ndata value\nDetermines the interval \nfor interpolation and the \nfraction in the interval\nBi-linear \ninterpolation\n147 Spatial hashing: A highly-parallel algorithm\nfour levels of coarsening, in addition to the minimum cell size for the following possi-\nbilities: 2.0, 4.0, 8.0, 16.0, and 32.0. Cell sizes are randomly generated, and the cells\nrandomly ordered. The sort is performed with a quicksort and then with a hash sort\non the CPU and the GPU. The calculation for the spatial hash sort exploits the infor-\nmation about the 1D data. We know the minimum and maximum value for X and the\nminimum cell size. With this information, we can calculate a bucket index that guar-\nantees a perfect hash with\nwhere bk is the bucket to place the entry, Xi is the x coordinate for the cell, Xmin is the\nminimum value of X, and Δmin is the minimum distance between any two adjacent val-\nues of X.\n We can demonstrate the hash sort operation (figure 5.9). The minimum differ-\nence between values is 2.0, so the bucket size of 2 guarantees that there are no colli-\nsions. The minimum value is 0, so the bucket location can be calculated with Bi =\nXi/Δmin = Xi/2.0. We could store either the value or the index in the hash table. For\nexample, 8, the first key, could be stored in bucket 4, or the original index location of\n0 could also be stored. If the value is stored, we retrieve the 8 with hash[4] . If the\nindex is stored, then we retrieve the value with keys[hash[4]] . Storing the index loca-bkXiXmin–\nmin----------------------=\nKeys Hash function Hash table\nBuckets8\n21\n16\n18\n13\n0\n5\n10\n23\n20\n0\n2\n5\n8\n10\n13\n16\n18\n21\n232\n4\n6\n8\n10\n12\n14\n16\n18\n20\n22\n24\nFigure 5.9 Sorting using a spatial perfect hash. This method stores the \nvalue in the hash with a bucket, but it could also store the index location of \nthe value in the original array. Note that the bucket size of 2 with a range of \n0 to 24 is indicated by the small numbers on the left of the hash table.\n148 CHAPTER  5Parallel algorithms and patterns\ntion is a little slower in this case, but it is more general. It can also be used to reorder\nall the arrays in a mesh. In the test case for the performance study, we use the method\nof storing the index.\n The spatial hash sort algorithm is Θ(N), while the quicksort is Θ(N log N). But the\nspatial hash sort is more specialized to the problem at hand and can temporarily take\nmore memory. The remaining questions are how difficult is this algorithm to write\nand how does it perform? The following listing shows the code for the write phase of\nthe spatial hash implementation. \nsort.c from PerfectHash\n283 uint hash_size = \n       (uint)((max_val - min_val)/min_diff);      \n284 hash = (int*)malloc(hash_size*sizeof(int));   \n285 memset(hash, -1, hash_size*sizeof(int));    \n286 \n287 for(uint i = 0; i < length; i++) {\n288     hash[(int)((arr[i]-min_val)/min_diff)] = i;    \n289 }\n290    \n291 int count=0;                            \n292 for(uint i = 0; i < hash_size; i++) {   \n293     if(hash[i] >= 0) {                  \n294         sorted[count] = arr[hash[i]];   \n295         count++;                        \n296     }                                   \n297 }                                       \n298    \n299 free(hash);\nNote that the code in the listing is barely more than a dozen lines. Compare this to a\nquicksort code that is five times as long and far more complicated.\n Figure 5.10 shows the performance of the spatial sort on both a single core of the\nCPU and the GPU. As we shall see, the parallel implementation on the CPU and GPU\ntakes some effort for good performance. The read phase of the algorithm needs a well\nimplemented prefix sum so that the retrieval of the sorted values can be done in par-\nallel. The prefix sum is an important pattern for many algorithms; we’ll discuss it fur-\nther in section 5.6. \n The GPU implementation for this example uses a well implemented prefix sum,\nand the performance of the spatial hash sort is excellent as a result. In earlier tests\nwith an array size of two million, this GPU sort was 3x faster than the fastest general\nGPU sort, and the serial CPU version was 4x faster than the standard quicksort. With\ncurrent CPU architectures and a larger array size of 16 million, our spatial hash sort is\nshown to be nearly 6x faster (figure 5.10). It is remarkable that our sort written in two\nor three months is much faster than the current fastest reference sorts on the CPU\nand GPU, especially since the reference sorts are the results of decades of research\nand the effort of many researchers!Listing 5.11 The spatial hash sort on the CPU\nCreates a hash table with \nbuckets of size min_diff\nSets all the elements \nof hash array to – 1\nPlaces the index of \ncurrent array element \ninto hash according to \nwhere the arr value goesSweeps \nthrough hash \nand puts set \nvalues in a \nsorted array",33260
60-5.5.2 Using compact hashing for spatial mesh operations.pdf,60-5.5.2 Using compact hashing for spatial mesh operations,"149 Spatial hashing: A highly-parallel algorithm\n5.5.2 Using compact hashing for spatial mesh operations\nWe are not done yet with exploring hashing methods. The algorithms in the perfect\nhashing section can be greatly improved. In the previous section, we explored using\ncompact hashes for the neighbor finding and the remap operations. The key observa-\ntions are that we don’t need to write to every spatial hash bin, and we can improve the\nalgorithms by handling collisions. This thereby allows the spatial hashes to be com-\npressed and use less memory. This gives us more options on algorithm choice with dif-\nferent memory requirements and run times.\nNEIGHBOR  FINDING  WITH WRITE  OPTIMIZATIONS  AND COMPACT  HASHING\nThe previous simple perfect hash algorithm for finding neighbors performs well for\nsmall numbers of mesh refinement levels in an AMR mesh. But when there are six or\nmore levels of refinement, a coarse cell writes to 64 hash buckets, and a fine cell only\nhas to write to one, leading to a load imbalance and a problem with thread divergence\nfor parallel implementations. \n Thread divergence  is when the amount of work for each thread varies and the\nthreads end up waiting for the slowest. We can improve the perfect hash algorithm\nfurther with the optimizations shown in figure 5.11. The first optimization is realizing\nthat the neighbor queries only sample the outer hash buckets of a cell, so there is no\nneed to write to the interior. Further analysis shows that only the corners or midpoints\nof the cell’s representation in the hash will be queried, reducing the needed writes\neven further. In the figure, the example shown to the far right of the sequence furtherSpeedup relative to base algorithm103\n102\n101\n10016.8 million cellsSort calculation with various algorithms\nHash single core Parallel hash (GPU) Quicksort1.05.9246.4\nFigure 5.10 Our spatial hash sort shows a speedup on a single core of the \nCPU and a further parallel speedup on the GPU. Our sort is 6x faster than the \ncurrent fastest sort.\n150 CHAPTER  5Parallel algorithms and patterns\noptimizes the writes to only one per cell and does multiple reads where the entry\nexists for a finer, same size, or coarser neighbor cell. This last technique requires ini-\ntializing the hash table to a sentinel value such as –1 to indicate no entry.\n But now that less data is written to the hash, we have a lot of empty space, or spar-\nsity, and can compress the hash table to as low as 1.25x times the number of entries,\ngreatly reducing the memory requirements of the algorithm. The inverse of the size\nmultiplier is known as the hash load factor  and is defined as the number of filled hash\ntable entries divided by the hash table size. For a 1.25 size multiplier, the hash load fac-\ntor is 0.8. We typically use a much smaller load factor, typically around 0.333 or a size\nmultiplier of 3. This is because in parallel processing, we want to avoid one processor\nbeing slower than the others. Hash sparsity  represents the empty space in the hash.\nSparsity indicates the opportunity for compression.\n Figure 5.12 shows the process of creating a compact hash. Because of the compres-\nsion to a compact hash, two entries try to store their value in bucket 1. The second25811\n91 012 2\n2\n2\n2\n2\n2\n22222222555588\n8 5 2\n5 2 6 6 7 7\n77 555566 2\n3 3 3 3 4\n4\n4\n444 444 4 2\n3 2\n35\n5\n3\n3 2\n3 3 3 3 22222222\n0000000\n0\n0\n0\n0\n0\n0\n000000000000000\n111111111111111\n111111111111189 1 011 12 2 2\n2\n2\n00\n00 1\n00 0 11 1122 3\n1144 4 3558 1 1\n9\n7\n77\n71012 11\n7 6 5\n34\n1 0289 1 012\n8\n6\n68\n8\n6\n6\n44\n4455\n55 2\n335\n3 367\n34\n0 1\nFigure 5.11 Optimizing the neighbor-finding calculation using the perfect spatial hash by reducing the number \nof writes and reads\nSpatial data\ni = 0 2 3 4\nj = 0\n1\n2\n3Key Value\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n152.67\n3.52 6.17\n3.14 8.24\n5.792.67\n3.52\n6.17\n3.14\n8.24\n5.79Perfect hash\nCompression\nfunctionProbing\nsequenceKey Value\n0,9,12\n6,7\n2,11\n8,5\n4,15\n10\n13,3\n1,142.67\n3.14\n8.24\n5.79Compact hash\nKey Value\n2.67\n3.52\n6.17\n3.14\n8.24\n5.79Compact hashBucket\nindex\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15Bucket\nindex\n0\n1\n2\n3\n4\n5\n6\n70\n1\n2\n3\n4\n5\n6\n70,9,12\n6\n2,11, 7\n8,5\n4,15\n10\n13,3\n1,14Sparsity = 10 / 16 = .625\nLoad factor = 6 / 16 = .3753.52,6.17\nSparsity = 10 / 16 = .25\nLoad Factor = 6 / 8 = .75Bucket\nindex\nFigure 5.12 This sequence from left to right shows the storing of spatial data in a perfect spatial hash, \ncompressing it into a smaller hash and then, where there is a collision, looking for the next available empty \nslot to store it.\n151 Spatial hashing: A highly-parallel algorithm\nentry sees that there is already a value there, so it looks for the next open slot in a\ntechnique called open addressing . In open addressing, we look for the next open slot in\nthe hash table and store the value in that slot. There are other hashing methods than\nopen addressing, but these often require the ability to allocate memory during an\noperation. Allocating memory is more difficult on the GPU, so we stick with open\naddressing where collisions are resolved by finding alternate storage locations within\nthe already allocated hash table.\n In open addressing, there are a few choices that we can use as the trial for the next\nopen slot. These are\nLinear probing —Where the next entry is just the next bucket in sequence until\nan open bucket is found\nQuadratic probing —Where the increment is squared so that the attempted buck-\nets are +1, +4, +9, and so forth from the original location\nDouble hashing —Where a second hashing function is used to jump to a deter-\nministic, but pseudo-random distance from the first trial location\nThe reason for the more complex choices for the next trial is to avoid clustering of\nvalues in part of the hash table, leading to longer store and query sequences. We use\nthe quadratic probing method because the first couple of tries are in the cache, which\nleads to better performance. Once a slot is found, both the key and the value are\nstored. When reading the hash table, the stored key is compared to the read key, and\nif they aren’t the same, then the read tries the next slot in the table.\n We could make a performance estimate of the improvement of these optimizations\nby counting the number of writes and reads. But we need to adjust these write and\nread numbers to account for the number of cache lines and not just the raw number\nof values. Also, the code with the optimizations has more conditionals. Thus, the run-\ntime improvement is modest and only better for higher levels of mesh refinement.\nThe parallel code on the GPU shows more benefit because the thread divergence is\nreduced. \n Figure 5.13 shows the measured performance results for the different hash table\noptimizations for a sample AMR mesh that has a relatively modest sparsity factor of 30.\nThe code is available at https:/ /github.com/lanl/CompactHash.git . The last perfor-\nmance numbers shown in figure 5.13 for both the CPU and GPU are for compact\nhash runs. The cost of the compact hash is offset by not having as much memory to\ninitialize to the sentinel value of –1. The effect is that the compact hash has a compet-\nitive performance compared to the perfect hashing methods. With more sparsity in\nthe hash table than the 30x compression factor in this test case, the compact hash can\neven be faster than the perfect hash methods. Cell-based AMR methods in general\nshould have at least a 10x compression and can often exceed 100x. \n These hashing methods have been implemented in the CLAMR mini-app. The\ncode switches between a perfect hash algorithm for low levels of sparsity and the com-\npact hash when there is a lot of empty space in the hash.\n152 CHAPTER  5Parallel algorithms and patterns\nFACE NEIGHBOR  FINDING  FOR UNSTRUCTURED  MESHES\nSo far, we haven’t discussed algorithms for unstructured meshes because it’s hard to\nguarantee that a perfect hash can easily be created for these. The most practical meth-\nods require a way to handle collisions and, thus, compact hashing techniques. Let’s\nexplore one case where the use of a hash is fairly straightforward. Finding the neigh-\nbor face for a polygonal mesh can be an expensive search procedure. Many unstruc-\ntured codes store the neighbor map because it is so expensive. The technique we show\nnext is so fast that the neighbor map can be calculated on the fly.\nExample: Finding the face neighbor for an unstructured mesh\nThe following figure presents a small part of an unstructured mesh with polygonal\ncells. One of the computational challenges for this type of mesh is to find the con-\nnectivity map for each face of the polygons. A brute force search of every other ele-\nment seems reasonable for small numbers of polygons, but for larger numbers of\ncells, this can soon take minutes or hours. A k-D tree search reduces the time, but\nis there an even faster way? Let’s try a hash-based method instead. We overlay the\nfigure on top of the hash table to the right of the figure. The algorithm is as follows:\nWe place a dot for the center of each face in the hash bucket where it falls.\nEvery cell writes its cell number into the bin at the center of each face. If the\nface is to the left and up from the center, it writes its index to the first of two\nplaces in the bin; otherwise, it writes to the second place.\nEvery cell checks for each face to see if there is a number in the other bucket.\nIf there is, it is the neighbor cell. If not, it is an external face with no neighbor.k-D tree Full Opt 1 Opt 2 Opt 3Optimized 2D neighbor calculation with various algorithms\nCompact GPU GPU 1 GPU 2 GPU 3 G Comp1283 301464 39329915,87328,22939,52227,854 30,716Six levels reﬁnement\n256×256 coarse mesh\n8.6 million cells\nSpeedup relative to base algorithm\n100101102103104105\nFigure 5.13 The optimized versions shown for the CPU and GPU correspond to the methods shown \nin figure 5.11. Compact is the CPU compact, and G Comp is the GPU compact for the last method in \neach set. The compact method is faster than the original perfect hash, requiring considerably less \nmemory. At higher levels of refinement, the methods that reduce the number of writes show some \nperformance benefit as well.\n153 Spatial hashing: A highly-parallel algorithm\nThe proper size of the hash table is difficult to specify. The best solution is to pick a\nreasonable size based on the number of faces or the minimum face length and then\nhandle collisions if these occur.\nREMAPS  WITH WRITE  OPTIMIZATIONS  AND COMPACT  HASHING\nAnother operation, the remap, is a little more difficult to optimize and set up for a\ncompact hash because the perfect hash approach reads all the underlying cells. First,\nwe have to come up with a way that doesn’t require every hash bucket to be filled. \n We write the cell indices for each cell to the lower left corner of the underlying\nhash. Then, during the read, if a value is not found or the level of the cell in the input\nmesh is not correct, we look for where a cell in the input mesh would write if it were at\nthe next coarser level. Figure 5.14 shows this approach, where cell 1 in the output\nmesh queries the hash location (0,2) and finds a –1, so it then looks for where the\nnext coarser cell would be at (0,0) and finds the cell index of 1. The density of cell 1We have found our neighbors in a single write and single read!\nFinding the neighbor for each face of each cell using a spatial hash. Each face writes to one of two \nbins in the spatial hash. If the face is towards the left and up from center, it writes to the first bin. \nIf towards the right and down, it writes to the second. In the read pass, it looks to see if the other \nbin has been filled and if it is, that cell number is its neighbor. (Graphic and algorithm courtesy of \nRachel Robey.)\nInput mesh\n0\n1 234\n98 76 5Hash\nRead 1\nRead 2 1-1-1-10-1-1-1\n-1\n-1-1-1-1-1-1-1\n-1\n-1-1-1-1-1-1-1\n-1\n-1-1-1-1-1-1-1\n-1\n-1-1-1-1-1-1-1\n-1\n-1-1-19-175\n-1\n-1-1-1-1-186\n-1\n2-1-13-14-1Output mesh\n53\n6472 109\n8\nFigure 5.14 A single write, multiple read implementation of a spatial hash remap. \nThe first query is where a cell of the same size from the input mesh would write, and \nthen if no value is found, the next query looks for where a cell at the next coarser \nlevel would write.\n154 CHAPTER  5Parallel algorithms and patterns\nin the output mesh is then set to the density of cell 1 in the input mesh. For cell 9 in\nthe output mesh, it looks in the hash at (4,4) and finds an input cell index of 3. It then\nlooks up the level of cell 3 in the input mesh, and because the input mesh cell level is\nfiner, it must also query hash locations (6,4) to get the cell index of 9 and location\n(4,6), which returns cell index 4 and location (6,6) to get cell index of 7. The first two\ncell indices are at the same level, so these do not need to go any further. The cell\nindex of 7 is at a finer level, so we must recursively descend into that location to find\ncell indices of 8, 5, and 6. Listing 5.12 shows the code.\nsinglewrite_remap.cc and meshgen.cc from CompactHashRemap/AMR_remap\n 47 #define two_to_the(ishift)    (1u <<(ishift) )   \n 48\n 49 typedef struct {      \n 50    uint ncells;       \n 51    uint ibasesize;      \n 52    uint levmax;      \n 53    uint *dist;      \n 54    uint *i;\n 55    uint *j;\n 56    uint *level;\n 57    double *values;\n 58 } cell_list;\n 59\n 60 cell_list icells, ocells;            \n 61\n<... lots of code to create mesh ...>    \n120 size_t hash_size = icells.ibasesize*two_to_the(icells.levmax)*\n121                    icells.ibasesize*two_to_the(icells.levmax);\n122 int *hash = (int *) malloc(hash_size *        \n                               sizeof(int));      \n123 uint i_max = icells.ibasesize*two_to_the(icells.levmax);\nBefore the write, a perfect hash table is allocated and initialized to the sentinel value\nof –1 (figure 5.10). Then the cell indices from the input mesh are written to the hash\n(listing 5.13). The code is available at https:/ /github.com/lanl/CompactHashRemap\n.git in the file AMR_remap/singlewrite_remap.cc, along with variants for using a com-\npact hash table and OpenMP. The OpenCL version for the GPU is in AMR_remap/\nh_remap_kern.cl.\nAMR_remap/singlewrite_remap.cc from CompactHashRemap\n127 for (uint i = 0; i < icells.ncells; i++) {       \n128     uint lev_mod =                     \n           two_to_the(icells.levmax –      \n                      icells.level[i]);    Listing 5.12 The setup phase for the single-write spatial hash remap on the CPU\nListing 5.13 The write phase for the single-write spatial hash remap on the CPUDefines 2n power \nfunction as the shift \noperator for speedStructure to hold \ncharacteristics of a mesh\nNumber of cells \nin the mesh\nNumber of coarse cells \nacross the x-dimension \nNumber of refinement levels \nin addition to the base meshDistribution\nof cells\nacross\nlevels of\nrefinement\nSets up input \nand output \nmeshes\nAllocates the hash table \nfor a perfect hash\nThe actual read part of the \nhash write is just four lines.\nThe multiplier to convert \nbetween mesh levels\n155 Spatial hashing: A highly-parallel algorithm\n129     hash[((icells.j[i] * lev_mod) * i_max)    \n            + (icells.i[i] * lev_mod)] = i;       \n130 }\nThe code for the read phase (listing 5.14) has an interesting structure. The first part is\nbasically split into two cases: the cell at the same location in the input mesh is the\nsame level or coarser or it is a set of finer cells. In the first case, we loop up the levels\nuntil we find the right level and set the value in the output mesh to the value in the\ninput mesh. If it is finer, we recurse down the levels, summing up the values as we go.\nAMR_remap/singlewrite_remap.cc from CompactHashRemap\n132 for (uint i = 0; i < ocells.ncells; i++) {\n133     uint io = ocells.i[i];\n134     uint jo = ocells.j[i];\n135     uint lev = ocells.level[i];\n136 \n137     uint lev_mod = two_to_the(ocells.levmax - lev);\n138     uint ii = io*lev_mod;\n139     uint ji = jo*lev_mod;\n140 \n141     uint key = ji*i_max + ii;\n142     int probe = hash[key];\n144 \n145     if (lev > ocells.levmax){lev = ocells.levmax;}\n146 \n147     while(probe < 0 && lev > 0) {    \n148         lev--;\n149         uint lev_diff = ocells.levmax - lev;\n150         ii >>= lev_diff;\n151         ii <<= lev_diff;\n152         ji >>= lev_diff;\n153         ji <<= lev_diff;\n154         key = ji*i_max + ii;\n155         probe = hash[key];\n156     }\n157     if (lev >= icells.level[probe]) {\n158         ocells.values[i] = icells.values[probe];    \n159     } else {\n160         ocells.values[i] =                  \n               avg_sub_cells(icells, ji, ii,    \n                             lev, hash);        \n161     }\n162 }\n163 double avg_sub_cells (cell_list icells, uint ji, uint ii,\n           uint level, int *hash) {\n164     uint key, i_max, jump;\n165     double sum = 0.0;\n166     i_max = icells.ibasesize*two_to_the(icells.levmax);\n167     jump = two_to_the(icells.levmax - level - 1);\n168    \n169     for (int j = 0; j < 2; j++) {\n170         for (int i = 0; i < 2; i++) {Listing 5.14 The read phase for the single-write spatial hash remap on the CPUComputes the index \nfor the 1D hash table\nIf a sentinel value is found, \ncontinues to coarser levels\nBecause this is at the \nsame level or coarser, \nsets the value of the \nfound cell ID in the \ninput mesh\nFor finer cells, recursively \ndescends and sums the \ncontributors\n156 CHAPTER  5Parallel algorithms and patterns\n171             key = ((ji + (j*jump)) * i_max) + (ii + (i*jump));\n172             int ic = hash[key];\n173             if (icells.level[ic] == (level + 1)) {\n174                 sum += icells.values[ic];     \n175             } else {\n176                 sum += avg_sub_cells(icells, ji + (j*jump), \n                    ii + (i*jump), level+1, hash);    \n177             }\n178         }\n179     }\n180  \n181     return sum/4.0;\n182 }\nOk, this seems fine for the CPU, but how is it going to work on the GPU? Supposedly,\nrecursion is not supported on the GPU. There doesn’t seem to be any easy way to\nwrite this without recursion. But we tested it on the GPU and found that it works. It\nruns fine on all of the GPUs that we tried for the limited number of levels of refine-\nment that will be used in any practical mesh. Evidently, a limited amount of recursion\nworks on a GPU! We then implemented compact hash versions of this approach and\nthese show good performance.\nHIERARCHICAL  HASH TECHNIQUE  FOR THE REMAP  OPERATION\nAnother innovative approach to using hashing for a remap operation involves a hier-\narchical set of hashes and a “breadcrumb” technique. A breadcrumb trail of sentinel\nvalues has the benefit that we do not need to initialize the hash tables to a sentinel\nvalue at the start (figure 5.15).\nThe first step is to allocate a hash table for each level of the mesh. Then the cell indi-\nces are written to the appropriate level hash and recurse upward through the coarserAccumulates to \nthe new value\nRecursively \ndescends \nagain\nInput mesh (i)\n53\n64Output mesh (o)\navg (i4,i3,i9,avg(i5,i6,i7,i8))72 1\n89 053\n6472 1\n89 0\n0\n1 24\n395\n76\n8\n5\n76\n84\n3–1\n91 20– 1\nFigure 5.15 A hierarchical hash \ntable with a separate hash for each \nlevel. When a write is done in one of \nthe finer levels, a sentinel value is \nplaced in each level above to form a \n“breadcrumb” trail to inform queries \nthat there is data at finer levels.",19815
61-5.7 Parallel global sum Addressing the problem of associativity.pdf,61-5.7 Parallel global sum Addressing the problem of associativity,"157 Prefix sum (scan) pattern and its importance in parallel computing\nhashes, leaving a sentinel value so that queries know there are values in the finer-level\nhash tables. Looking at figure 5.15 for cell 9 in the input mesh, we see that\nThe cell index is written to the mid-level hash table, then a sentinel value is writ-\nten to the hash bins in the coarser hash table. \nThe read operation for cell 9 first goes to the coarsest level of the hash table,\nwhere it finds a sentinel value of –1. It now knows that it must go to the finer\nlevels. \nIt finds three cells at the mid-level hash table and another sentinel value to tell\nthe read operation to recursively descend to the finest level, where it finds four\nmore values to add to the summation. \nThe other queries are all found in the coarsest hash table, and the values\nassigned to the output mesh.\nEach of the hash tables can be either a perfect hash or a compact hash. The method\nhas a recursive structure, similar to the previous technique. It also runs fine on GPUs.\n5.6 Prefix sum (scan) pattern and its importance \nin parallel computing\nThe prefix sum was a critical element of making the hash sort work in parallel in sec-\ntion 5.5.1. The prefix sum operation, also known as a scan, is a common operation in\ncomputations with irregular sizes. Many computations with irregular sizes need to\nknow where to start writing to be able to operate in parallel. A simple example is\nwhere each processor has a different number of particles. To be able to write to the\noutput array or access data on other processors or threads, each processor needs to\nknow the relationship of the local indices to the global indices. In the prefix sum, the\noutput array, y, is a running sum of all of the numbers previous to it in the original\narray:\nThe prefix sum can either be an inclusive scan, where the current value is included, or\nan exclusive scan, where it isn’t included. The previous equation is for an exclusive\nscan. Figure 5.16 shows both an exclusive and an inclusive scan. The exclusive scan is\nthe starting index for the global array, while the inclusive scan is the ending index for\neach process or thread.\n The following listing shows the standard serial code for the scan operation.yj xi\ni0=j1–\n=\n3 4 6 3 8 7 5 4x\n0 3 7 13 16 24 31 36y Exclusive scan\n3 7 13 16 24 31 36y Inclusive scan 40Figure 5.16 The array x gives the number of \nparticles in each cell. The exclusive and inclusive \nscan of an array gives the starting and ending \naddress in the global data set.\n158 CHAPTER  5Parallel algorithms and patterns\n1 y[0] = x[0];\n2 for (int i=1; i<n; i++){\n3    y[i] = y[i-1] + x[i];\n4 }\nOnce the scan operation is complete, each process is free to perform its operation in\nparallel because the process knows where to put its result. The scan operation itself,\nthough, appears to be intrinsically serial. Each iteration is dependent on the previ-\nous. But there are effective ways to parallelize it. We’ll look at a step-efficient, a\nwork-efficient, and a large array algorithm in this section.\n5.6.1 Step-efficient parallel scan operation\nA step-efficient algorithm uses the fewest number of steps. But this might not be the\nfewest number of operations because a different number of operations is possible with\neach step. This was discussed earlier when defining computational complexity in sec-\ntion 5.1.\n The prefix sum operation can be made parallel with a tree-based reduction pat-\ntern as figure 5.17 shows. Rather than waiting for the previous element to sum up\nits values, each element sums its value and the preceding value. Then it does the\nsame operation, but with the value two elements over, four elements over, and so\non. The end result is an inclusive scan; during the operation all of the processes\nhave been busy.\nNow we have a parallel prefix that operates in just log 2n steps, but the amount of work\nincreases from the serial algorithm. Can we design a parallel algorithm that has the\nsame amount of work? Listing 5.15 The serial inclusive scan operation\nStep 1\nStep 2\nStep 3X x0x1x2x3x4x5x6x7\nΣ(x0...x0)Σ ( x0...x1)\nΣ(x0...x1)\nΣ(x0...x1)Σ(x1...x2)\nΣ(x0...x2)\nΣ(x0...x2)Σ(x2...x3)\nΣ(x0...x3)\nΣ(x0...x3)Σ(x3...x4)\nΣ(x1...x4)\nΣ(x0...x4)Σ(x4...x5)\nΣ(x2...x5)\nΣ(x0...x5)Σ(x5...x6)\nΣ(x3...x6)\nΣ(x0...x6)Σ(x6...x7)\nΣ(x4...x7)\nΣ(x0...x7)Σ(x0...x0)\nΣ(x0...x0)\nFigure 5.17 The step-efficient inclusive scan uses O(log 2n) steps to \ncompute a prefix sum in parallel.\n159 Prefix sum (scan) pattern and its importance in parallel computing\n5.6.2 Work-efficient parallel scan operation\nA work-efficient algorithm uses the least number of operations. This might not be the\nfewest number of steps because a different number of operations is possible with each\nstep. The choice of a work-efficient or a step-efficient algorithm is dependent on the\nnumber of parallel processes that can exist.\n The work-efficient parallel scan operation uses two sweeps through the arrays. The\nfirst sweep is called an upsweep, though it is more of a right sweep. It is shown in fig-\nure 5.18 from top to bottom, rather than the traditional bottom to top for easier com-\nparison to the step-efficient algorithm.\nThe second phase, known as the downsweep phase, is more of a left sweep. It starts by\nsetting the last value to zero and then does another tree-based sweep (figure 5.19) to\nget the final result. The amount of work is reduced significantly, but with the require-\nment of more steps.Step 1\nStep 2\nStep 3X x0x1x2x3x4x5x6x7\nx0\nx0\nx0Σ(x0...x1)\nΣ(x0...x1)\nΣ(x0...x1)x2\nx2\nx2Σ(x2...x3)\nΣ(x0...x3)\nΣ(x0...x3)x4\nx4\nx4Σ(x4...x5)\nΣ(x4...x5)\nΣ(x4...x5)x6\nx6\nx6Σ(x6...x7)\nΣ(x4...x7)\nΣ(x0...x7)\nFigure 5.18 The upsweep phase of the work-efficient scan shown from \ntop to bottom, which has far fewer operations than the step-efficient scan. \nEssentially, every other value is left unmodified.\nInsert\nZero\nStep 0\nStep 1\nStep 2x0x2x4x6Σ(x0...x1)Σ ( x0...x3)Σ ( x4...x5)Σ ( x0...x7)\nΣ(x0...x3)\nΣ(x0...x5)\nΣ(x0...x6) Σ(x0...x5)x0\nx0\nx0\n0x2\nx2\nx2x4\nx4\nx4x6\nx6\nx6Σ(x0...x1)\nΣ(x0...x1)\nΣ(x0...x0)Σ ( x0...x1)00Σ(x0...x3)\nΣ(x0...x1)\nΣ(x0...x2) Σ(x0...x3)Σ(x4...x5)\nΣ(x4...x5)\nΣ(x0...x3)\nΣ(x0...x4)0\nFigure 5.19 The downsweep phase of the work-efficient exclusive scan \noperation has far fewer operations than the step-efficient scan.\n160 CHAPTER  5Parallel algorithms and patterns\nWhen shown this way, the work-efficient scan has an interesting pattern, with a right\nsweep starting with half the threads and decreasing until only one is operating. Then\nit begins a sweep back to the left with one thread at the start and finishing with all\nthreads busy. The additional steps allow the earlier calculations to be reused so that\nthe total operations are only O(N).\n These two parallel prefix sum algorithms give us a couple of different options on\nhow to incorporate parallelism in this essential operation. But both of these are lim-\nited to the number of threads available in a workgroup on the GPU or the number of\nprocessors on a CPU. \n5.6.3 Parallel scan operations for large arrays\nFor larger arrays, we also need an algorithm that is parallel. Figure 5.20 shows such an\nalgorithm using three kernels for the GPU. The first kernel starts with a reduction\nsum on each workgroup and stores the result in a temporary array that is smaller than\nthe original large array by the number of threads in the workgroup. On the GPU, the\nnumber of threads in a workgroup is typically as high as 1,024. The second kernel\nthen loops across the temporary array performing a scan on each work group-sized\nblock. This results in the temporary array now holding the offsets for each work group.\nA third kernel is then invoked to perform the scan operation on work group-sized\nchunks of the original array, and an offset calculated for each thread at this level.\nWG 1 WG 2 WG 3 WG 4 WG 5 WG 6 WG 7 WG 8\nRead work group-sized blocks\nOﬀsetsSum on each work group\nDetermine offset for each thread and applyExclusive sum by work group\nFigure 5.20 The large array scan proceeds in three stages and as three kernels for the GPU. \nThe first stage does a reduction sum to an intermediate array. The second stage does a scan \nto create the offsets for the work groups. Then the third phase scans the original array and \napplies the work group offsets to get the scan results for each element of the array.\n161 Parallel global sum: Addressing the problem of associativity\nBecause the parallel prefix sum is so important in operations like sorts, it is heavily\noptimized for GPU architectures. We don’t go into that level of detail in this book.\nInstead, we suggest that application developers use libraries or freely available imple-\nmentations for their work. For the parallel prefix scan available for CUDA, you’ll find\nimplementations such as the CUDA Data Parallel Primitives Library (CUDPP), avail-\nable at https:/ /github.com/cudpp/cudpp . For OpenCL, we suggest either the imple-\nmentation from its parallel primitives library, CLPP, or the scan implementation from\nour hash-sorting code available in the sort_kern.cl file at https:/ /github.com/LANL/\nPerfectHash.git . We’ll present a version of the prefix scan for OpenMP in chapter 7.\n5.7 Parallel global sum: Addressing the problem \nof associativity\nNot all parallel algorithms are about speeding up calculations. The global sum is a\nprime example of such a case. Parallel computing has been plagued since the earliest\ndays with the non-reproducibility of sums across processors. In this section, we show\none example of an algorithm that improves the reproducibility of a parallel calcula-\ntion so that it gets nearer to the results of the original serial calculation.\n Changing the order of additions changes the answer in finite-precision arithmetic.\nThis is problematic because a parallel calculation changes the order of the additions.\nThe problem is due to finite-precision arithmetic not being associative. And the problem\ngets worse as the problem size gets larger because the addition of the last value\nbecomes a smaller and smaller part of the overall sum. Eventually the addition of the last\nvalue might not change the sum at all. There is even a worse case for additions of finite-\nprecision values when adding two values that are almost identical, but of different signs.\nThis subtraction of one value from another when these are nearly the same causes a cata-\nstrophic cancellation . The result is only a few significant digits with noise filling the rest.\nThe result in the example has only a couple of significant digits left! And where do the\nrest of the digits in the printed value come from? The problem in parallel computing\nis that instead of the sum being a linear addition of the values in the array, on two\nprocessors the sum is a linear sum of half of the array and then the two partial sumsExample: Catastrophic cancellation\nThe subtraction of two nearly similar values will have a result with only a small num-\nber of significant figures. Put the following code in a file called catastrophic.py and\nrun the program with python catastrophic.py .\nx = 12.15692174374373 - 12.15692174374372\nprint x         Catastrophic cancellation in a short Python code\nReturns \n1.0658 1410364e– 14\n162 CHAPTER  5Parallel algorithms and patterns\nadded at the end. The change in the order causes the global sum to be different. The\ndifference can be small, but now the question is whether the parallelization of the\ncode has been done properly. Exacerbating the problem is that all of the new paral-\nlelization techniques and hardware, such as vectorization and threads, also cause this\nproblem. The pattern for this global sum operation is called a reduction.\nDEFINITION A reduction  is an operation where an array of one or more dimen-\nsions is reduced to at least one dimension less and often to a scalar value. \nThis operation is one of the most common in parallel computing and is often a con-\ncern for performance, and in this case, correctness. An example of this is calculating\nthe total mass or energy in a problem. This takes a global array of the mass in each cell\nand results in a single scalar value.\n As with all computer calculations, the results of the global sum reduction are not\nexact. In serial calculations, this does not pose a serious problem because we always\nget the same inexact result. In parallel, we most likely get a more accurate result with\nmore correct significant digits, but it is different than the serial result. This is known\nas the global sum issue . Anytime the results between the serial and parallel versions\nwere slightly different, the cause was attributed to this problem. But often, when time\nwas taken to dig more deeply into the code, the problem turned out to be a subtle par-\nallel programming error such as failing to update the ghost cells between processors.\nGhost cells  are cells that hold the adjacent processor values needed by the local proces-\nsor, and if they are not updated, the slightly older values cause a small error compared\nto the serial run. \n For years, I thought, like other parallel programmers, that the only solution was to\nsort the data into a fixed order and sum it up in a serial operation. But because this\nwas too expensive, we just lived with the problem. In about 2010, several parallel pro-\ngrammers, including myself, realized that we were looking at the problem wrong. It is\nnot solely an order problem, but also a precision problem. In real number arithmetic,\naddition is associative! So adding precision is also a way to solve the problem and at a\nfar lower cost than sorting the data.\n To gain a better understanding of the problem and how to solve it, let’s take a look\nat a problem from compressible fluid dynamics called the Leblanc problem, also\nknown as “the shock tube from hell.” In the Leblanc problem, a high pressure region\nis separated from a low pressure region by a diaphragm that is removed at time zero.\nIt is a challenging problem because of the strong shock that results. But the feature we\nare most interested in is the large dynamic range in both the density and energy vari-\nables. We’ll use the energy variable with a high value at 1.0e–1 and a low value of 1.0e–10.\nThe dynamic range  is the range of the working set of real numbers, or in this case, the\nratio of the maximum and the minimum values. The dynamic range is nine orders of\nmagnitude, which means that when adding the small value to the large value for double-\nprecision, floating-point numbers with about 16 significant digits, in reality, we only\nhave about 7 significant digits in the result. \n163 Parallel global sum: Addressing the problem of associativity\n Let’s look at a problem size of 134,217,728 on a single processor with half the val-\nues at the high energy state and the other half at the low energy state. These two\nregions are separated by a diaphragm at the beginning of the problem. The problem\nsize is large for a single processor, but for a parallel computation, it is relatively small.\nIf the high energy values are summed first, the next single low value that is added will\nhave few significant digits to contribute. Reversing the order of the sum so that the\nlow energy values are summed first makes the small values of near equal size in their\nsum, and by the time the high energy value is added, there will be more significant\ndigits, thus a more accurate sum. This gives us a possible sorting-based solution. Just\nsort the values in order from the lowest magnitude to the highest and you will get a\nmore accurate sum. There are several solutions for addressing the global sum that are\nmuch more tractable than the sorting technique. The list of possible techniques pre-\nsented here includes \nLong-double data type\nPairwise summation\nKahan summation\nKnuth summation\nQuad-precision summation\nYou can try the various methods in the exercises that accompany the chapter at\nhttps:/ /github.com/EssentialsOfParallelComputing/Chapter5.git . The original study\nlooked at parallel OpenMP implementations and truncation techniques that we won’t\ngo into here.\n The easiest solution is to use the long-double data type on a x86 architecture. On\nthis architecture, a long-double is implemented as an 80-bit floating-point number in\nhardware giving an extra 16-bits of precision. Unfortunately, this is not a portable\ntechnique. The long double on some architectures and compilers is only 64-bits, and\non others it’s 128-bits and implemented in software. Some compilers also force round-\ning between operations to maintain consistency with other architectures. Check your\ncompiler documentation carefully on how it implements a long-double when using\nthis technique. The code shown in the next listing is simply a regular sum with the\ndata type of the accumulator set to long double.\nGlobalSums/do_ldsum.c\n1 double do_ldsum(double *var, long ncells)\n2 {\n3    long double ldsum = 0.0;\n4    for (long i = 0; i < ncells; i++){\n5       ldsum += (long double)var[i];     \n6    }\n7    double dsum = ldsum;     \n8    return(dsum);      \n9 }Listing 5.16 Long-double data type sum on x86 architectures\nvar is an array of doubles, \nwhile the accumulator is a \nlong double.\nThe return type of the function \ncan also be long double and the \nvalue of ldsum returned.Returns \na double\n164 CHAPTER  5Parallel algorithms and patterns\nAt line 8 in the listing, a double is returned to stay consistent with the concept of a\nhigher precision accumulator returning the same data type as the array. We see how this\nperforms later, but first let’s cover the other methods for addressing the global sum.\n The pairwise summation is a surprisingly simple solution to the global sum prob-\nlem, especially within a single processor. The code is relatively straightforward as the\nfollowing listing shows but requires an additional array half the size of the original.\nGlobalSums/do_pair_sum.c\n 4 double do_pair_sum(double *var, long ncells)\n 5 {\n 6    double *pwsum =\n       (double *)malloc(ncells/2*sizeof(double));    \n 7\n 8    long nmax = ncells/2;\n 9    for (long i = 0; i<nmax; i++){       \n10       pwsum[i] = var[i*2]+var[i*2+1];   \n11    }                                    \n12\n13    for (long j = 1; j<log2(ncells); j++){      \n14       nmax /= 2;                               \n15       for (long i = 0; i<nmax; i++){           \n16          pwsum[i] = pwsum[i*2]+pwsum[i*2+1];   \n17       }                                        \n18    }                                           \n19    double dsum = pwsum[0];      \n20    free(pwsum);     \n21    return(dsum);\n22 }\nThe simplicity of the pairwise summation becomes a little more complicated when\nworking across processors. If the algorithm remains true to its basic structure, a com-\nmunication may be needed at each step of the recursive sum.\n Next is the Kahan summation. The Kahan summation is the most practical method\nof the possible global sum methods. It uses an additional double variable to carry the\nremainder of the operation, in effect doubling the effective precision. The technique\nwas developed by William Kahan in 1965 (Kahan later became one of the key contrib-\nutors to the early IEEE floating-point standards). The Kahan summation is most\nappropriate for a running summation when the accumulator is the larger of two val-\nues. The following listing shows this technique.\nGlobalSums/do_kahan_sum.c\n 1 double do_kahan_sum(double *var, long ncells)\n 2 {\n 3    struct esum_type{       \n 4       double sum;          \n 5       double correction;   \n 6    };                      Listing 5.17 Pairwise summation on processor\nListing 5.18 Kahan summationNeeds temporary space \nto do the pairwise \nrecursive sums\nAdds the initial \npairwise sum into \nnew array\nRecursively sums the \nremaining log 2 steps, \nreducing array size by \ntwo for each step\nAssigns the result \nto a scalar value \nfor returnFrees \ntemporary \nspace\nDeclares a double-\ndouble data type\n165 Parallel global sum: Addressing the problem of associativity\n 7\n 8    double corrected_next_term, new_sum;\n 9    struct esum_type local;\n10 \n11    local.sum = 0.0;\n12    local.correction = 0.0;\n13    for (long i = 0; i < ncells; i++) {\n14       corrected_next_term = var[i] + local.correction;\n15       new_sum          = local.sum + local.correction;\n16       local.correction = corrected_next_term -       \n                            (new_sum - local.sum);      \n17       local.sum        = new_sum;\n18    }\n19\n20   double dsum = local.sum + local.correction;    \n21   return(dsum);                                  \n22 }\nThe Kahan summation takes about four floating-point operations instead of one. But\nthe data can be kept in registers or the L1 cache, making the operation less expensive\nthan we might initially expect. Vectorized implementations can make the operation\ncost the same as the standard summation. This is an example where we use the excess\nfloating-point capability of the processor to get a better answer. \n We’ll look at a vector implementation of the Kahan sum in section 6.3.4. Some new\nnumerical methods are attempting a similar approach, using the excess floating-point\ncapability of current processors. These view the current machine balance of 50 flops\nper data load as an opportunity and implement higher-order methods that require\nmore floating-point operations to exploit the unused floating-point resource because\nit is essentially free.\n The Knuth summation method handles additions where either term can be larger.\nThe technique was developed by Donald Knuth in 1969. It collects the error for both\nterms at a cost of seven floating-point operations as the following listing shows. \nGlobalSums/do_knuth_sum.c\n 1 double do_knuth_sum(double *var, long ncells)\n 2 {\n 3    struct esum_type{        \n 4       double sum;           \n 5       double correction;    \n 6    };                       \n 7\n 8   double u, v, upt, up, vpp;\n 9   struct esum_type local;\n10 \n11   local.sum = 0.0;\n12   local.correction = 0.0;\n13   for (long i = 0; i < ncells; i++) {\n14      u = local.sum;\n15      v = var[i] + local.correction;Listing 5.19 Knuth summationComputes the remainder to \ncarry to the next iteration\nReturns the double-\nprecision result\nDefines a double-\ndouble data type\n166 CHAPTER  5Parallel algorithms and patterns\n16      upt = u + v;\n17      up = upt - v;      \n18      vpp = upt - up;    \n19      local.sum = upt;\n20      local.correction = (u - up) + (v - vpp);      \n21   }\n22\n23   double dsum = local.sum + local.correction;    \n24   return(dsum);                                  \n25 }\nThe last technique, the quad-precision sum, has the advantage of simplicity in coding,\nbut because the quad-precision types are almost always done in software, it is expen-\nsive. Portability is also something to beware of as not all compilers have implemented\nthe quad-precision type. The following listing presents this code.\nGlobalSums/do_qdsum.c\n1 double do_qdsum(double *var, long ncells)\n2 {\n3    __float128 qdsum = 0.0;                 \n4    for (long i = 0; i < ncells; i++){\n5       qdsum += (__float128)var[i];      \n6    }\n7    double dsum =qdsum;\n8    return(dsum);\n9 }\nNow on to the assessment of how these different approaches work. Because half the\nvalues are 1.0e–1 and the other half are 1.0e–10, we can get an accurate answer to\ncompare against by multiplying instead of adding:\naccurate_answer = ncells/2 * 1.0e-1 + ncells/2 * 1.0e-10\nTable 5.1 shows the results of comparing the global sum values actually obtained ver-\nsus the accurate answer and measuring the run time. We essentially get nine digits of\naccuracy with a regular summation of doubles. The long double on a system with an\n80-bit floating-point representation improves it somewhat, but doesn’t completely\neliminate the error. The pairwise, Kahan and Knuth summations all reduce the error\nto zero with a modest increase in run time. A vectorized implementation of the Kahan\nand Knuth summation (shown in section 6.3.4) eliminates the increase in run time.\nEven so, when considering cross-processor communications and the cost of MPI calls,\nthe increase in run time is insignificant.\n Now that we understand the behavior of the global sum techniques on a processor,\nwe can consider the problem when the arrays are distributed across multiple proces-\nsors. We need some understanding of MPI to tackle this problem, so we will show how\nto do this in section 8.3.3, after learning the basics of MPI.Listing 5.20 Quad precision global sumCarries the values \nfor each termCombined into \none correction\nReturns the double-\nprecision result\nQuad precision \ndata type\nCasts the input \nvalue from array \nto quad precision",25063
62-5.8 Future of parallel algorithm research.pdf,62-5.8 Future of parallel algorithm research,,0
63-5.9.2 Exercises.pdf,63-5.9.2 Exercises,"167 Further explorations\n5.8 Future of parallel algorithm research\nWe have seen some of the characteristics of parallel algorithms including those suit-\nable for extremely parallel architectures. Let’s summarize these so that we can look\nfor them in other situations:\nLocality —Often-used term in describing good algorithms but without any defi-\nnition. It can have multiple meanings. Here are a couple:\n–Locality for cache —Keeps the values that will be used together close together\nso that cache utilization is improved.\n–Locality for operations —Avoids operating on all the data when not all is needed.\nThe spatial hash for particle interactions is a classic example that keeps an\nalgorithm’s complexity O(N) instead of O(N2).\nAsynchronous —Avoids coordination between threads that can cause synchro-\nnization.\nFewer conditionals —Besides the additional performance hit from conditional\nlogic, thread divergence can be a problem on some architectures.\nReproducibility —Often a highly parallel technique violates the lack of associativ-\nity of finite-precision arithmetic. Enhanced-precision techniques can help\ncounter this issue.\nHigher arithmetic intensity —Current architectures have added floating-point capa-\nbility faster than memory bandwidth. Algorithms that increase arithmetic inten-\nsity can make good use of parallelism such as the vector operations.\n5.9 Further explorations\nThe development of parallel algorithms is still a young field of research, and there are\nmany new algorithms to be discovered. But there are also many known techniques\nthat have not been widely disseminated or used. Particularly challenging is that the\nalgorithms are often in wildly different fields of computer or computational science.Table 5.1 Precision and run-time results for various global sum techniques\nMethod Error Run time\nDouble –1.99e–09 0.116\nLong double –1.31e–13 0.118\nPairwise summation 0.0 0.402\nKahan summation 0.0 0.406\nKnuth summation 0.0 0.704\nQuad double 5.55e–17 3.010\n168 CHAPTER  5Parallel algorithms and patterns\n5.9.1 Additional reading\nFor more on algorithms, we recommend a popular textbook:\nThomas Cormen, et al., Introduction to Algorithms , 3rd ed (MIT Press, 2009).\nFor more information on patterns and algorithms, here are two good books for fur-\nther reading:\nMichael McCool, Arch D. Robison, and James Reinders, Structured Parallel Pro-\ngramming: Patterns for Efficient Computation  (Morgan Kaufmann, 2012).\nTimothy G. Mattson, Beverly A. Sanders, and Berna L. Massingill, Patterns for\nParallel Programming  (Addison-Wesley, 2004).\nThe concepts of spatial hashing have been developed by some of my students ranging\nfrom high school level through graduate students. The section on perfect hashing in\nthe following resource draws from work by Rachel Robey and David Nicholaeff. David\nalso implemented spatial hashing in the CLAMR mini-app.\nRachel N. Robey, David Nicholaeff, and Robert W. Robey, “Hash-based algorithms\nfor discretized data,” SIAM Journal on Scientific Computing  35, no. 4 (2013): C346–\nC368.\nThe ideas for parallel compact hashing for neighbor finding came from Rebecka Tum-\nblin, Peter Ahrens, and Sara Hartse. These were built from the methods to reduce the\nwrites and reads developed by David Nicholaeff.\nRebecka Tumblin, Peter Ahrens, et al., “Parallel compact hash algorithms for com-\nputational meshes,” SIAM Journal on Scientific Computing  37, no. 1 (2015): C31–C53.\nDeveloping optimized methods for the remap operation was much more challenging.\nGerald Collom and Colin Redman tackled the problem and came up with some really\ninnovative techniques and implementations on the GPU and in OpenMP. This chap-\nter only touches on some of these. There are far more ideas in their paper:\nGerald Collom, Colin Redman, and Robert W. Robey, “Fast Mesh-to-Mesh Remaps\nUsing Hash Algorithms,” SIAM Journal on Scientific Computing  40, no. 4 (2018):\nC450–C476.\nI first developed the concept of enhanced-precision global sums in about 2010. Jona-\nthan Robey implemented the technique in his Sapient hydrocode and Rob Aulwes,\nLos Alamos National Laboratory, helped develop the theoretical foundations. The fol-\nlowing two references give more details on the method:\nRobert W. Robey, Jonathan M. Robey, and Rob Aulwes, “In search of numerical\nconsistency in parallel programming,” Parallel Computing  37, no. 4–5 (2011):\n217–229.",4444
64-Summary.pdf,64-Summary,"169 Summary\nRobert W. Robey, “Computational Reproducibility in Production Physics Appli-\ncations,” Numerical Reproducibility at Exascale Workshop (NRE2015), Interna-\ntional Conference for High Performance Computing, Networking, Storage and\nAnalysis, 2015. Available at https:/ /github.com/lanl/ExascaleDocs/blob/master/\nComputationalReproducibilityNRE2015.pdf\n5.9.2 Exercises\n1A cloud collision model in an ash plume is invoked for particles within a 1 mm\ndistance. Write pseudocode for a spatial hash implementation. What complex-\nity order is this operation?\n2How are spatial hashes used by the postal service?\n3Big data uses a map-reduce algorithm for efficient processing of large data sets.\nHow is it different than the hashing concepts presented here?\n4A wave simulation code uses an AMR mesh to better refine the shoreline. The\nsimulation requirements are to record the wave heights versus time for speci-\nfied locations where buoys and shore facilities are located. Because the cells are\nconstantly being refined, how could you implement this?\nSummary\nAlgorithms and patterns are one of the foundations of computational applica-\ntions. Selecting algorithms that have low computational complexity and lend\nthemselves to parallelization is important when first developing an application.\nA comparison-based algorithm has a lower complexity limit of O(N log N).\nNon-comparison algorithms can break this lower algorithmic limit. \nHashing is a non-comparison technique that has been used in spatial hashing to\nachieve Θ(N) complexity for spatial operations.\nFor any spatial operation, there is a spatial hashing algorithm that scales as\nO(N). In this chapter, we provide examples of techniques that can be used in\nmany scenarios.\nCertain patterns have been shown to be adaptable to parallelism and the asyn-\nchronous nature of GPUs. The prefix scan and hashing techniques are two such\npatterns. The prefix scan is important for parallelizing irregular-sized arrays.\nHashing is a non-comparison, asynchronous algorithm that is highly scalable.\nReproducibility is an important attribute in developing robust production\napplications. This is especially important for reproducible global sums and for\ndealing with finite-precision arithmetic operations that are not associative. \nEnhanced precision is a new technique that restores associativity, allowing reor-\ndering of operations, and thus, more parallelism.",2454
65-Part 2CPU The parallel workhorse.pdf,65-Part 2CPU The parallel workhorse,"Part 2\nCPU: The parallel workhorse\nT oday, every developer should understand the growing parallelism available\nwithin modern CPU processors. Unlocking the untapped performance of CPUs\nis a critical skill for parallel and high performance computing applications. To\nshow how to take advantage of CPU parallelism, we cover\n■Using vector hardware\n■Using threads for parallel work across multi-core processors\n■Coordinating work on multiple CPUs and multi-core processors with mes-\nsage passing\nThe CPU’s parallel capabilities need to be at the core of your parallel strategy.\nBecause it’s the central workhorse, the CPU controls all the memory allocations,\nmemory movement, and communication. The application developer’s knowledge\nand skill are the most important factors for fully using the CPU’s parallelism. CPU\noptimization is not automatically done by some magic compiler. Commonly, many\nof the parallel resources on the CPU go untapped by applications. We can break\ndown the available CPU parallelism into three components in increasing order of\neffort. These are\n■Vectorization —Exploits the specialized hardware that can do more than\none operation at a time\n■Multi-core and threading —Spreads out work across the many processing\ncores in today’s CPUs\n■Distributed memory —Harnesses multiple nodes into a single, cooperative\ncomputing application\n172 PART 2CPU: The parallel workhorse\nThus, we begin with vectorization. Vectorization is a highly underused capability with\nnotable gains when implemented. Though compilers can do some vectorization, com-\npilers don’t do enough. The limitations are especially noticeable for complicated\ncode. Compilers are just not there yet. Although compilers are improving, there is not\nsufficient funding or manpower for this to happen quickly. Consequently, the applica-\ntion programmer has to help in a variety of ways. Unfortunately, there is little docu-\nmentation on vectorization. In chapter 6, we present an introduction to the arcane\nknowledge of getting more from vectorization for your application. \n With the explosion in processing cores on each CPU, the need and knowledge for\nexploiting on-node parallelism is growing rapidly. Two common CPU resources for this\ninclude threading and shared memory. There are dozens of different threading systems\nand shared memory approaches. In chapter 7, we present a guide to using OpenMP, the\nmost commonly used threading package for high performance computing.\n The dominant language for parallelism across nodes, and even within nodes, is the\nopen source standard, the Message Passing Interface (MPI). The MPI standard grew\nout of a consolidation of many message-passing libraries from the early days of parallel\nprogramming. MPI is a well-designed language that has withstood the test of time and\nchanges to hardware architectures. It has also adapted with new features and improve-\nments that have been incorporated into its implementations. Still, most application\nprogrammers just use the most basic features of the language. In chapter 8, we give an\nintroduction to the basics of MPI, as well as some advanced features that can be useful\nin many scientific and big data applications.\n The key to getting high performance on the CPU is to pay attention to memory\nbandwidth, supplying the data to parallel engines. Good parallel performance begins\nwith good serial performance (and an understanding of the topics presented in the\nfirst five chapters of this book). CPUs provide the most general parallelism for the\nwidest variety of applications. From modest parallelism through extreme scale, the\nCPU often delivers the goods. The CPU is also where you must begin your journey\ninto the parallel world. Even in solutions that use accelerators, the CPU remains an\nessential component in the system.\n Up until now, the solution to increasing performance was to add more compute\npower in the form of physically adding more nodes to your cluster or high perfor-\nmance computer. The parallel and high performance computing community has\ngone as far as it can with that approach and is beginning to hit power and energy con-\nsumption limits. Additionally, the number of nodes and processors cannot continue\nto grow without running into the limitations of scaling applications. In response to\nthis, we must turn to other avenues to improve performance. Within the processing\nnode, there are a lot of underutilized parallel hardware capabilities. As we first men-\ntioned in section 1.1, parallelism within the node will continue to grow. \n Even with continuing limitations of compute power and other looming thresholds,\nkey insights and knowledge of lesser-known tools can unlock substantial performance.\nThrough this book and your studies, we can help tackle these challenges. In the end,\n173 CPU: The parallel workhorse>\nyour skills and knowledge are important commodities for unlocking the promises of\nparallel performance.\n The examples that accompany the three chapters in part 2 of this book are at\nhttps:/ /github.com/EssentialsofParallelComputing,  with a separate repository for\neach chapter. Docker container builds for each chapter should install and work well\non any operating system. The container builds for the first two chapters in this part\n(chapters 6 and 7) use a graphical interface to allow the use of performance and cor-\nrectness tools.",5422
66-6.3.1 Optimized libraries provide performance for little effort.pdf,66-6.3.1 Optimized libraries provide performance for little effort,"175Vectorization:\nFLOPs for free\nProcessors have special vector units that can load and operate on more than one data\nelement at a time. If we’re limited by floating-point operations, it is absolutely neces-\nsary to use vectorization to reach peak hardware capabilities. Vectorization  is the pro-\ncess of grouping operations together so more than one can be done at a time. But,\nadding more flops to hardware capability when an application is memory bound has\nlimited benefit. Take note, most applications are memory bound. Compilers can be\npowerful, but as you will see, real performance gain with vectorization might not be\nas easy as the compiler documentation suggests. Still, the performance gain from vec-\ntorization can be achieved with a little effort and should not be ignored. \n In this chapter, we will show how programmers, with a little bit of effort and\nknowledge, can achieve a performance boost through vectorization. Some of theseThis chapter covers\nThe importance of vectorization\nThe kind of parallelization provided by \na vector unit\nDifferent ways you can access vector \nparallelization\nPerformance benefits you can expect\n176 CHAPTER  6Vectorization: FLOPs for free\ntechniques simply require the use of the right compiler flags and programming styles,\nwhile others require much more work. Real-world examples demonstrate the various\nways vectorization is achieved. \nNOTE We encourage you to follow along with the examples for this chapter\nat https:/ /github.com/EssentialsofParallelComputing/Chapter6 .\n6.1 Vectorization and single instruction, multiple data \n(SIMD) overview\nWe introduced the single instruction, multiple data (SIMD) architecture in section\n1.4 as one component of Flynn’s Taxonomy. This taxonomy is used as a parallelization\nclassification of instruction and data streams on an architecture. In the SIMD case, as\nthe name indicates, there is a single instruction that is executed across multiple data\nstreams. One vector add instruction replaces eight individual scalar add instructions in\nthe instruction queue, which reduces the pressure on the instruction queue and\ncache. The biggest benefit is that it takes about the same power to perform eight addi-\ntions in a vector unit as one scalar addition. Figure 6.1 shows a vector unit that has a\n512-bit vector width, offering a vector length of eight double-precision values.\nLet’s briefly summarize vectorization terminology:\nVector (SIMD) lane —A pathway through a vector operation on vector registers\nfor a single data element much like a lane on a multi-lane freeway.\nVector width —The width of the vector unit, usually expressed in bits.\nVector length —The number of data elements that can be processed by the vector\nin one operation.\nVector (SIMD) instruction sets —The set of instructions that extend the regular sca-\nlar processor instructions to utilize the vector processor. \nVectorization is produced through both a software and a hardware component. The\nrequirements are\nGenerate instructions —The vector instructions must be generated by the com-\npiler or manually specified through intrinsics or assembler coding. \nMatch instructions to the vector unit of the processor —If there is a mismatch between\nthe instructions and the hardware, newer hardware can usually process theCache line Cache line512 bits\n(64 bytes)Vector lane\na[0:7]\nb[0:7]+a[i]\nb[i]+\nc[0:7] c[i]1 instruction, 8 additions Scalar operation\n= =1 cycle\n1 instruction\n1 additionFigure 6.1 A scalar operation does a \nsingle double-precision addition in one \ncycle. It takes eight cycles to process \na 64-byte cache line. In comparison, a \nvector operation on a 512-bit vector unit \ncan process all eight double-precision \nvalues in one cycle. \n177 Hardware trends for vectorization\ninstructions, but older hardware will just fail to run. (AVX instructions do not\nrun on ten-year-old chips. Sorry!)\nThere is no fancy process that converts regular scalar instructions on the fly. If you use\nan older version of your compiler, as many programmers do, it will not have the capa-\nbility to generate the instructions for the latest hardware. Unfortunately, it takes time\nfor compiler writers to include new hardware capabilities and instruction sets. It can\nalso take a while for the compiler writers to optimize these capabilities. \nTake away: When you use the latest processors, make sure to use the latest versions of the\ncompiler.\nYou should also specify the appropriate vector instruction set to generate. By default,\nmost compilers take the safe route and generate SSE2 (Streaming SIMD Extensions)\ninstructions so that the code works on any hardware. SSE2 instructions only execute\ntwo double-precision operations at a time instead of the four or eight operations that\ncan be done on more recent processors. For performance applications, there are bet-\nter choices: \nYou can compile for the architecture you are running on.\nYou can compile for any architecture manufactured within the last 5 or 10\nyears. Specifying AVX (Advanced Vector Extensions) instructions would give a\n256-bit width vector and would run on any hardware since 2011.\nYou can ask the compiler to generate more than one vector instruction set. It\nthen falls back to the best one for the hardware being used.\nTake away: Specify the most advanced vector instruction set in your compiler flags that\nyou can reasonably use.\n6.2 Hardware trends for vectorization\nTo implement the choices discussed previously, it is helpful to know the historical\ndates of hardware and instruction set release for selecting which vector instruction set\nto use. Table 6.1 highlights the key releases, and figure 6.2 shows the trends in the vec-\ntor unit size.\nTable 6.1 The vector hardware releases over the last decade have dramatically improved vector\nfunctionality.\nRelease Functionality\nMMX (trademark with no \nofficial meaning)Targeted towards the graphics market, but GPUs soon took over this func-\ntion. Vector units shifted their focus to computation rather than graphics. \nAMD released its version under the name 3DNow! with single-precision \nsupport.\nSSE (Streaming SIMD \nExtensions)First Intel vector unit to offer floating-point operations with single-precision \nsupport\nSSE2 Double-precision support added\n178 CHAPTER  6Vectorization: FLOPs for free\n \n6.3 Vectorization methods\nThere are several ways to achieve vectorization in your program. In ascending order of\nprogrammer effort, these include\nOptimized libraries\nAuto-vectorization\nHints to the compiler\nVector intrinsics\nAssembler instructionsAVX (Advanced Vector \nExtensions)Twice the vector length. AMD added a fused multiply-add FMA vector \ninstruction in its competing hardware, effectively doubling the performance \nfor some loops.\nAVX2 Intel added a fused multiply-add (FMA) to its vector processor. \nAVX512 First offered on the Knights Landing processor; it came to the main-line \nmulti-core processor hardware lineup in 2017.\nFrom the years 2018 and on, Intel and AMD (Advanced Micro Devices, Inc.) \nhave created multiple variants of AVX512 as incremental improvements to \nvector hardware architectures.Table 6.1 The vector hardware releases over the last decade have dramatically improved vector\nfunctionality.  (continued)\nRelease Functionality\n512\n256\nVector unit size bits128\n64\n1997 2001 2005 2009\nDate of releaseVector unit trends in commodity processors\nMMXSSE SSE2 SSE3\nSSE4AVX2AVXAVX512\n2013 2017\nFigure 6.2 The appearance of vector unit hardware for commodity \nprocessors began around 1997 and has slowly grown over the last twenty \nyears, both in vector width (size) and in types of operations supported.",7763
67-6.3.2 Auto-vectorization The easy way to vectorization speedup most of the time.pdf,67-6.3.2 Auto-vectorization The easy way to vectorization speedup most of the time,"179 Vectorization methods\n6.3.1 Optimized libraries provide performance for little effort\nFor the least effort to achieve vectorization, programmers should research what librar-\nies are available that they can use for their application. Many low-level libraries pro-\nvide highly-optimized routines for programmers seeking performance. Some of the most\ncommonly used libraries include\nBLAS (Basic Linear Algebra System)—A base component of high-performance\nlinear algebra software\nLAPACK—A linear algebra package\nSCALAPACK—A scalable linear algebra package\nFFT (Fast Fourier transform)—Various implementation packages available\nSparse Solvers—Various implementations of sparse solvers available\nThe Intel® Math Kernel Library (MKL) implements optimized versions of the BLAS,\nLAPACK, SCALAPACK, FFTs, sparse solvers, and mathematical functions for Intel pro-\ncessors. Though available as a part of some Intel commercial packages, the library is\nalso offered freely. Many other library developers release packages for a variety of pur-\nposes. Additionally, hardware vendors supply optimized libraries for their hardware\nunder different licensing arrangements.\n6.3.2 Auto-vectorization: The easy way to vectorization speedup \n(most of the time1)\nAuto-vectorization is the recommended choice for most programmers because imple-\nmentation requires the least amount of programming effort. That being said, compil-\ners cannot always recognize where vectorization can be applied safely. In this section,\nwe first look at what kind of code a compiler might automatically vectorize. Then, we\nshow how to verify that you get the actual vectorization you expect. You will also learn\nabout programming styles that make it possible for the compiler to vectorize code and\nperform other optimizations. This includes the use of the restrict  keyword for C\nand __restrict  or __restrict__  attributes for C++. \n With ongoing improvements of architectures and compilers, auto-vectorization\ncan provide significant performance improvement. The proper compiler flags and\nprogramming style can improve this further. \nDEFINITION Auto-vectorization  is the vectorization of the source code by the\ncompiler for standard C, C++, or Fortran languages.\n1It is important to note that although auto-vectorization often yields significant performance gains, it can some-\ntimes slow down the code. This is due to the overhead of setting up the vector instructions being greater than\nthe performance gain. The compiler generally makes a decision to vectorize using a cost function. The com-\npiler vectorizes if the cost function shows that the code would be faster, but it is guessing at the array lengths\nand assumes all the data is from the first level of the cache.\n180 CHAPTER  6Vectorization: FLOPs for free\nWe will discuss compiler flags in more detail in section 6.4, and the timer.c and\ntimer.h files in section 17.2. Compiling the stream_triad.c file with version 8 of the\nGCC compiler gives the following compiler feedback:\nstream_triad.c:19:7: note: loop vectorized\nstream_triad.c:12:4: note: loop vectorizedExample: Auto-vectorization\nLet’s see how auto-vectorization works on the simple loop from the STREAM Triad\nfrom the STREAM Benchmark introduced in section 3.2.4. We separate out the triad\ncode from the STREAM Benchmark into a standalone test problem with this listing.\nautovec/stream_triad.c\n 1 #include <stdio.h>\n 2 #include <sys/time.h>\n 3 #include ""timer.h""\n 4\n 5 #define NTIMES 16\n 6 #define STREAM_ARRAY_SIZE 80000000        \n 7 static double a[STREAM_ARRAY_SIZE],       \n                    b[STREAM_ARRAY_SIZE],    \n                    c[STREAM_ARRAY_SIZE];    \n 8\n 9 int main(int argc, char *argv[]){\n10    struct timeval tstart;\n11    double scalar = 3.0, time_sum = 0.0;        \n12    for (int i=0; i<STREAM_ARRAY_SIZE; i++) {   \n13       a[i] = 1.0;                              \n14       b[i] = 2.0;                              \n15    }                                           \n16 \n17    for (int k=0; k<NTIMES; k++){\n18       cpu_timer_start(&tstart);\n19       for (int i=0; i<STREAM_ARRAY_SIZE; i++){   \n20          c[i] = a[i] + scalar*b[i];              \n21       }                                          \n22       time_sum += cpu_timer_stop(tstart);\n23       c[1] = c[2];                        \n24    }\n25    printf(""Average runtime is %lf msecs\n"", time_sum/NTIMES);\n26 }\nMakefile for the GCC compiler\nCFLAGS=-g -O3 -fstrict-aliasing \\n       -ftree-vectorize -march=native -mtune=native \\n       -fopt-info-vec-optimized\nstream_triad: stream_triad.o timer.oAuto vectorization of stream_triad.c\nLarge enough \nto force into \nmain memory\nInitializes \ndata and \narrays\nStream triad loop has \nthree operands with a \nmultiply and an add.\nKeeps the compiler from \noptimizing out the loop\n181 Vectorization methods\nGCC vectorizes both the initialization loop and the stream triad loop! We can run the\nstream triad with\n./stream_triad\nWe can verify that the compiler uses vector instructions with the likwid tool (section 3.3.1).\nlikwid-perfctr -C 0 -f -g MEM_DP ./stream_triad\nLook in the report output from this command for these lines:\n| FP_ARITH_INST_RETIRED_128B_PACKED_DOUBLE |   PMC0  |          0 |\n|    FP_ARITH_INST_RETIRED_SCALAR_DOUBLE   |   PMC1  |         98 |\n| FP_ARITH_INST_RETIRED_256B_PACKED_DOUBLE |   PMC2  |  640000000 |\n| FP_ARITH_INST_RETIRED_512B_PACKED_DOUBLE |   PMC3  |          0 |\nIn the output, you can see most of the operation counts are in the 256B_PACKED_DOUBLE\ncategory on the third line. Why all the 256 bit operations? Some versions of the GCC\ncompiler, including the 8.2 version used in this test, generate 256 bits instead of the\n512-bit vector instructions for the Skylake processor. Without a tool like likwid, we\nwould need to carefully check the vectorization reports or inspect the generated\nassembler instructions to find out the compiler was not generating the proper instruc-\ntions. For the GCC compiler, we can change the generated instructions by adding the\ncompiler flag -mprefer-vector-width=512  and then try again. Now we’ll get AVX512\ninstructions with eight double-precision values computed at once:\n| FP_ARITH_INST_RETIRED_256B_PACKED_DOUBLE |   PMC2  |          0 |\n| FP_ARITH_INST_RETIRED_512B_PACKED_DOUBLE |   PMC3  |  320000000 |\nExample: Auto-vectorization in a function\nIn this example, we try a slightly more complicated version of the code from the pre-\nvious example with the stream triad loop in a separate function. The following listing\nshows how.\nautovec_function/stream_triad.c\n 1 #include <stdio.h>\n 2 #include <sys/time.h>\n 3 #include ""timer.h""\n 4\n 5 #define NTIMES 16\n 6 #define STREAM_ARRAY_SIZE 80000000\n 7 static double a[STREAM_ARRAY_SIZE], b[STREAM_ARRAY_SIZE],\n                 c[STREAM_ARRAY_SIZE];\n 8Stream triad loop in a separate function\n182 CHAPTER  6Vectorization: FLOPs for free\nLet’s look at the output from the GCC compiler for the code in the previous stream\ntriad loop listing:\nstream_triad.c:10:4: note: loop vectorized\nstream_triad.c:10:4: note: loop versioned for vectorization because of\n   possible aliasing\nstream_triad.c:10:4: note: loop vectorized\nstream_triad.c:18:4: note: loop vectorized\nThe compiler cannot tell if the arguments to the function point to the same or to\noverlapping data. This causes the compiler to create more than one version and pro-\nduces code that tests the arguments to determine which to use. We can fix this by add-\ning to the function definition the restrict  attribute as part of the arguments. The\nC99 standard added this keyword. Unfortunately, C++ has not standardized the\nrestrict  keyword, but the __restrict  attribute works for GCC, Clang, and Visual\nC++. Another common form of the attribute in C++ compilers is __restrict__ :\n9 void stream_triad(double* restrict a, double* restrict b, \n                     double* restrict c, double scalar){\nWe used GCC to compile the code with the restrict  keyword added and got(continued)\n 9 void stream_triad(double* a, double* b,      \n         double* c, double scalar){             \n10    for (int i=0; i<STREAM_ARRAY_SIZE; i++){\n11       a[i] = b[i] + scalar*c[i];\n12    }\n13 }\n14\n15 int main(int argc, char *argv[]){\n16    struct timeval tstart;\n17    double scalar = 3.0, time_sum = 0.0;\n18    for (int i=0; i<STREAM_ARRAY_SIZE; i++) {\n19       a[i] = 1.0;\n20       b[i] = 2.0;\n21    }\n22\n23    for (int k=0; k<NTIMES; k++){\n24       cpu_timer_start(&tstart);\n25       stream_triad(a, b, c, scalar);      \n26       time_sum += cpu_timer_stop(tstart);\n27       // to keep the compiler from optimizing out the loop\n28       c[1] = c[2];\n29    }\n30    printf(""Average runtime is %lf msecs\n"", time_sum/NTIMES);\n31 }Stream triad loop in \na separate function\nStream triad \nfunction call",8946
68-6.3.3 Teaching the compiler through hints Pragmas and directives.pdf,68-6.3.3 Teaching the compiler through hints Pragmas and directives,"183 Vectorization methods\nstream_triad.c:10:4: note: loop vectorized\nstream_triad.c:10:4: note: loop vectorized\nstream_triad.c:18:4: note: loop vectorized\nNow this compiler generates fewer versions of the function. We need to also point out\nthat the -fstrict-aliasing  flag tells the compiler to aggressively generate code with\nthe assumption that there is no aliasing. \nDEFINITION Aliasing  is where pointers point to overlapping regions of mem-\nory. In this situation, the compiler cannot tell if it is the same memory, and it\nwould be unsafe to generate vectorized code or other optimizations.\nIn recent years, the strict aliasing option has become the default with GCC and other\ncompilers (optimization levels -O2 and -O3 set -fstrict-aliasing ). This broke a lot\nof code where aliased variables actually existed. As a result, compilers have dialed back\nhow aggressively they generate more efficient code. All of this is to tell you that you\nmay get different results with various compilers and even different compiler versions. \n By using the restrict  attribute, you make a promise to the compiler that there is\nno aliasing. We recommend using both the restrict  attribute and the -fstrict-\naliasing  compiler flag. The attribute is portable with the source across all architec-\ntures and compilers. You’ll need to apply the compiler flags for each compiler, but\nthese affect all of your source.\n From these examples, it would seem that the best course of action for program-\nmers to get vectorization is to just let the compiler auto-vectorize. While compilers are\nimproving, for more complex code compilers often fail to recognize that they can\nsafely vectorize the loop. Thus, the programmer needs to help the compiler with hints.\nWe discuss this technique next.\n6.3.3 Teaching the compiler through hints: Pragmas and directives\nOk, the compiler is not quite able to figure it out and generate vectorized code; is\nthere something we can do? In this section, we will present how to give more precise\ndirections to the compiler. In return, this gives you more control over the vectoriza-\ntion process of your code. Here, you will learn how to use pragmas and directives to\nconvey information to the compiler for portable implementation of vectorization.\nDEFINITION Pragma  is an instruction to a C or C++ compiler to help it inter-\npret the source code. The form of the instruction is a preprocessor statement\nstarting with #pragma . (In Fortran, where it is called a directive, the form is a\ncomment line starting with !$).\nExample: Using manual hints to the compiler for vectorization\nWe need an example where the compiler does not vectorize the code without help.\nFor this we use the example code in the following listings. Together these calculate the\nwave speed in a cell to determine the timestep to use in the calculation. The timestep\n184 CHAPTER  6Vectorization: FLOPs for free\n(continued)\ncan be no greater than the minimum time it takes for a wave to cross any cell in the\nmesh.\ntimestep/main.c\n 1 #include <stdio.h>\n 2 #include ""timestep.h""\n 3 #define NCELLS 10000000\n 4 static double H[NCELLS], U[NCELLS], V[NCELLS], dx[NCELLS], \ndy[NCELLS];\n 5 static int celltype[NCELLS];\n 6\n 7 int main(int argc, char *argv[]){\n 8    double mymindt;\n 9    double g = 9.80, sigma = 0.95;       \n10    for (int ic=0; ic<NCELLS ; ic++) {   \n11       H[ic] = 10.0;                     \n12       U[ic] = 0.0;                      \n13       V[ic] = 0.0;                      \n14       dx[ic] = 0.5;                     \n15       dy[ic] = 0.5;                     \n16       celltype[ic] = REAL_CELL;         \n17    }                                    \n18    H[NCELLS/2] = 20.0;                  \n19\n20    mymindt = timestep(NCELLS, g, sigma,    \n                celltype, H, U, V, dx, dy);   \n21\n22    printf(""Minimum dt is %lf\n"",mymindt);\n23 }\ntimestep/timestep.c\n 1 #include <math.h>\n 2 #include ""timestep.h""\n 3 #define REAL_CELL 1\n 4\n 5 double timestep(int ncells, double g, double sigma, int* celltype,\n 6           double* H, double* U, double* V, double* dx, double* dy){\n 7    double wavespeed, xspeed, yspeed, dt; \n 8    double mymindt = 1.0e20;\n 9    for (int ic=0; ic<ncells ; ic++) {\n10       if (celltype[ic] == REAL_CELL) {\n11          wavespeed = sqrt(g*H[ic]);          \n12          xspeed = (fabs(U[ic])+wavespeed)/dx[ic];\n13          yspeed = (fabs(V[ic])+wavespeed)/dy[ic];\n14          dt=sigma/(xspeed+yspeed);               Timestep calculation using a minimum reduction loop in timestep/main.c\nInitializes \narrays and \ndata\nCalls the timestep \ncalculation\nTimestep calculation using a minimum reduction loop in timestep/timestep.c\nThe velocity \nof wave\nTime for wave to \ncross cell times \nsafety factor \n(sigma)\n185 Vectorization methods\nFor this example, we added the -fopt-info-vec-missed  compiler flag to get a report\non the missed loop vectorizations. Compiling this code gives us\nmain.c:10:4: note: loop vectorized\ntimestep.c:9:4: missed: couldn't vectorize loop\ntimestep.c:9:4: missed: not vectorized: control flow in loop.\nThis vectorization report tells us that the timestep loop was not vectorized due to the\nconditional in the loop. Let’s see if we can get the loop to optimize by adding a\npragma. Add the following line just before the for loop in timestep.c (at line 9):\n#pragma omp simd reduction(min:mymindt)\nNow compiling the code shows conflicting messages about whether the timestep loop\nwas vectorized:\nmain.c:10:4: note: loop vectorized\ntimestep_opt.c:9:9: note: loop vectorized\ntimestep_opt.c:11:7: note: not vectorized: control flow in loop.\nWe need to check the executable with a performance tool such as likwid to see if it\nactually vectorizes:\nlikwid-perfctr -g MEM_DP -C 0 ./timestep_opt\nThe output from the likwid tool shows that no vector instructions are being executed:\n|             DP MFLOP/s            |  451.4928 |\n|           AVX DP MFLOP/s          |         0 |\n|           Packed MUOPS/s          |         0 |\nWith the GCC 9.0 version of the compiler, we have been able to get this to vectorize by\nadding the -fno-trapping-math  flag. If there is a division in a conditional block, this\nflag tells the compiler not to worry about throwing an error exception, so it will then15          if (dt < mymindt) mymindt = dt;     \n16       }   \n17    }   \n18    return(mymindt);\n19 }\nMakefile for GCC\nCFLAGS=-g -O3 -fstrict-aliasing -ftree-vectorize -fopenmp-simd \\n  -march=native -mtune=native -mprefer-vector-width=512 \\n  -fopt-info-vec-optimized -fopt-info-vec-missed\n  \nstream_triad: main.o timestep.o timer.oGets the minimum \ntime for all cells \nand uses for \ntimestep\n186 CHAPTER  6Vectorization: FLOPs for free\nvectorize. If there is a sqrt  in the conditional block, the -fno-math-errno  flag will\nallow the compiler to vectorize. For better portability, the pragma should also tell the\ncompiler that some variables are not preserved across loop iterations and, hence, are\nnot a flow or anti-flow dependency. These dependencies will be discussed after the list-\ning in the following example. \n#pragma omp simd private(wavespeed, xspeed, yspeed, dt) reduction(min:mymindt)\nAn even better way to indicate that the scope of the variables is limited to each itera-\ntion of the loop is to declare the variables in the scope of the loop:\ndouble wavespeed = sqrt(g*H[ic]);\ndouble xspeed = (fabs(U[ic])+wavespeed)/dx[ic];\ndouble yspeed = (fabs(V[ic])+wavespeed)/dy[ic];\ndouble dt=sigma/(xspeed+yspeed);\nNow we can remove the private  clause and the declaration of the variables prior to\nthe loop. We can also add the restrict  attribute to the function interface to inform\nthe compiler the pointers do not overlap:\ndouble timestep(int ncells, double g, double sigma, int* restrict  celltype,\n                double* restrict H, double* restrict U, double* restrict V,\n                double* restrict dx, double* restrict dy);\nEven with all of these changes, we were not able to get the GCC compiler to vectorize\nthe code. With further investigation using version 9 of the GCC compiler, we finally\nwere successful by adding the -fno-trapping-math  flag. If there is a division in a\nconditional block, this flag tells the compiler not to worry about throwing an error\nexception so it will then vectorize. If there is a sqrt  in the conditional block, the\n-fno-math-errno  flag allows the compiler to vectorize. The Intel compiler, however,\nvectorizes all of the versions. \n One of the more common operations is a sum of an array. Back in section 4.5, we\nintroduced this type of operation as a reduction. We’ll add a little complexity to the\noperation by including a conditional that limits the sum to the real cells in a mesh.\nHere real cells  are considered elements not on the boundary or ghost cells from other\nprocessors. We discuss ghost cells in chapter 8. \nmass_sum/mass_sum.c\n 1 #include ""mass_sum.h""\n 2 #define REAL_CELL 1\n 3\n 4 double mass_sum(int ncells, int* restrict celltype, double* restrict H,\n 5                 double* restrict dx, double* restrict dy){\n 6    double summer = 0.0;                                 \n 7 #pragma omp simd reduction(+:summer)    \n 8    for (int ic=0; ic<ncells ; ic++) {Listing 6.1 Mass sum calculation using a sum reduction loop\nSets the reduction\nvariable to zero\nThread loop treats summer \nas a reduction variable.\n187 Vectorization methods\n 9       if (celltype[ic] == REAL_CELL) {      \n10          summer += H[ic]*dx[ic]*dy[ic];\n11       }   \n12    }   \n13    return(summer);\n14 }\nThe OpenMP SIMD pragma should automatically set the reduction variable to zero,\nbut when the pragma is ignored, the initialization on line 6 is necessary. The OpenMP\nSIMD pragma on line 7 tells the compiler that we use the summer  variable in a reduc-\ntion sum. In the loop, the conditional on line 9 can be implemented in the vector\noperations with a mask. Each vector lane has its own copy of summer  and these will\nthen be combined at the end of the for loop.\n The Intel compiler successfully recognizes the sum reduction and automatically\nvectorizes the loop without the OpenMP SIMD pragma. GCC also vectorizes with ver-\nsions 9 and later of the compiler.\nExample: Using the compiler vectorization report as a guide for adding \npragmas\nBecause the Intel compiler generates better vectorization reports, we will use it for\nthis example. The source code for this example is taken from listing 4.14. The main\nloop is shown in this listing with line numbers.\n56   for (int j = 1; j < jmax-1; j++){\n57      for (int i = 1; i < imax-1; i++){\n58         xnew[j][i] = (x[j][i] + x[j  ][i-1] + x[j  ][i+1]\n                                 + x[j-1][i  ] + x[j+1][i  ])/5.0;\n59      }  \n60   }  \nFor this example, we used the Intel v19 compiler with these compiler flags:\nCFLAGS=-g -O3 -std=c99 -qopenmp-simd -ansi-alias -xHost \\n   -qopt-zmm-usage=high -qopt-report=5 -qopt-report-phase=vec,loop\nThe vectorization report shows that the compiler did not vectorize the inner loop at\nline 57 and the outer loop at line 56: \nLOOP BEGIN at stencil.c(56,7)\n   remark #15344: loop was not vectorized: vector dependence prevents \n                  vectorization\n   remark #15346: vector dependence: assumed OUTPUT dependence between\n                  xnew[j][i] (58:13)and xnew[j][i] (58:13)\n   remark #15346: vector dependence: assumed OUTPUT dependence between\n                  xnew[j][i] (58:13)and xnew[j][i] (58:13)The conditional can be \nimplemented with a mask.\nMain loop of stencil example from listing 4.14\n188 CHAPTER  6Vectorization: FLOPs for free\nIn the previous example, the flow and anti-flow dependencies arise due to the possibil-\nity of aliasing between x and xnew . The compiler is being more conservative in this\ncase than it needs to be. The output dependency is only called out in the attempt to\nvectorize the outer loop. The compiler cannot be certain that the subsequent itera-\ntions of the inner loop won’t write to the same location as a prior iteration. Before we\ncontinue, let’s define a few terms:\nFlow dependency —A variable within the loop is read after being written, known as\na read-after-write (RAW).\nAnti-flow dependency —A variable within the loop is written after being read,\nknown as a write-after-read (WAR).\nOutput dependency —A variable is written to more than once in the loop.\nFor the GCC v8.2 compiler, the vectorization report is\nstencil.c:57:10: note: loop vectorized\nstencil.c:57:10: note: loop versioned for vectorization because of \n                 possible aliasing\nstencil.c:51:7: note: loop vectorized\nstencil.c:37:7: note: loop vectorized\nstencil.c:37:7: note: loop versioned for vectorization because of \n                possible aliasing\nThe GCC compiler chooses to create two versions and tests which to use at run time.\nThe report is nice enough to give us a clear idea of the cause of the problem. There\nare two ways that we can fix these problems. We can help guide the compiler by add-\ning a pragma before the loop at line 57 like this:\n#pragma omp simd\n         for (int i = 1; i < imax-1; i++){\nAnother approach to solving this problem is to add a restrict  attribute to the defini-\ntion of x and xnew :\ndouble** restrict x = malloc2D(jmax, imax);\ndouble** restrict xnew = malloc2D(jmax, imax);(continued)\n   LOOP BEGIN at stencil.c(57,10)\n      remark #15344: loop was not vectorized: vector dependence prevents\n                     vectorization\n      remark #15346: vector dependence: assumed FLOW dependence between\n                     xnew[j][i] (58:13)and x[j][i] (58:13)\n      remark #15346: vector dependence: assumed ANTI dependence between\n                     x[j][i] (58:13)and xnew[j][i] (58:13)\n      remark #25438: unrolled without remainder by 4\n   LOOP END\nLOOP END\n189 Vectorization methods\nThe vectorization report for Intel now shows that the inner loop is vectorized with a\nvectorized peel loop, main vectorized loop, and a vectorized remainder loop. This\ncalls for a few more definitions.\nPeel loop —A loop to execute for misaligned data so that the main loop would\nthen have aligned data. Often the peel loop is conditionally executed at run\ntime if the data is discovered to be misaligned.\nRemainder loop —A loop that executes after the main loop to handle a partial set\nof data that is too small for a full vector length.\nThe peel loop is added to deal with the unaligned data at the start of the loop, and the\nremainder loop takes care of any extra data at the end of the loop. The reports for all\nthree loops look similar. Looking at the main loop report, we see that the estimated\nspeedup is over six times faster:\nLOOP BEGIN at stencil.c(55,21)\n   remark #15388: vec support: reference xnew[j][i] has aligned access\n   [ stencil.c(56,13) ]\n   remark #15389: vec support: reference x[j][i] has unaligned access\n   [ stencil.c(56,28) ]\n   remark #15389: vec support: reference x[j][i-1] has unaligned access\n   [ stencil.c(56,38) ]\n   remark #15389: vec support: reference x[j][i+1] has unaligned access\n   [ stencil.c(56,50) ]\n   remark #15389: vec support: reference x[j-1][i] has unaligned access\n   [ stencil.c(56,62) ]\n   remark #15389: vec support: reference x[j+1][i] has unaligned access\n   [ stencil.c(56,74) ]\n   remark #15381: vec support: unaligned access used inside loop body\n   remark #15305: vec support: vector length 8\n   remark #15399: vec support: unroll factor set to 2\n   remark #15309: vec support: normalized vectorization overhead 0.236\n   remark #15301: OpenMP SIMD LOOP WAS VECTORIZED\n   remark #15449: unmasked aligned unit stride stores: 1\n   remark #15450: unmasked unaligned unit stride loads: 5\n   remark #15475: --- begin vector cost summary ---\n   remark #15476: scalar cost: 43\n   remark #15477: vector cost: 6.620\n   remark #15478: estimated potential speedup: 6.370\n   remark #15486: divides: 1\n   remark #15488: --- end vector cost summary ---\n   remark #25015: Estimate of max trip count of loop=125\nLOOP END\nTake note that the estimated speedup is carefully labeled as potential speedup . It is\nunlikely that you will get the full estimated speedup unless \nYour data is in a high level of the cache.\nThe actual array length is long.\nYou are not bandwidth-limited with data loads from main memory.",16514
69-6.3.4 Crappy loops we got them Use vector intrinsics.pdf,69-6.3.4 Crappy loops we got them Use vector intrinsics,"190 CHAPTER  6Vectorization: FLOPs for free\nIn the preceeding implementation, the actual measured speedup on a Skylake Gold\nprocessor with the Intel compiler is 1.39 times faster than the unvectorized version.\nThis vectorization report is for the speedup of the processor, but we still have a mem-\nory bandwidth-limited kernel from main memory to contend with.\n For the GCC compiler, the SIMD pragma is successful at eliminating the version-\ning of the two loops. In fact, adding the restrict  clause had no effect, and both loops\nare still versioned. Additionally, because there is a vectorized version in all these cases,\nperformance doesn’t change. To understand speedup, we can compare the perfor-\nmance to a version where the vectorization is turned off to find that the speedup for\nvectorization with GCC is about 1.22 times faster.\n6.3.4 Crappy loops, we got them: Use vector intrinsics\nFor troublesome loops that just don’t vectorize even with hints, vector intrinsics are\nanother option. In this section, we’ll see how to use intrinsics for more control over\nvectorization. The downside of vector intrinsics is that these are less portable. Here,\nwe will look at some examples that use vector intrinsics for successful vectorization to\nshow the use of intrinsics to vectorize the Kahan sum introduced in section 5.7. In that\nsection, we said the cost of the Kahan sum in a normal sum operation was about four\nfloating-point operations instead of one. But if we can vectorize the Kahan sum opera-\ntion, the cost becomes much less. \n The implementations in these examples use a 256-bit vector intrinsic to speed up\nthe operation by nearly a factor of four over the serial version. We show three differ-\nent ways to implement a Kahan sum kernel in the listings for the following examples.\nYou will find the full implementation at https:/ /github.com/lanl/GlobalSums.git ,\nwhich is extracted from the global sums example. It is included in the GlobalSumsVec-\ntorized directory in the code for this chapter.\nExample: Implementation of the Kahan sum using vector intrinsics\nThe first intrinsics example, by Andy Dubois and Brett Neuman of Los Alamos\nNational Laboratory, uses the Intel x86 vector intrinsics. This is the most commonly\nused set of intrinsics and can run on both Intel and AMD processors that support AVX\nvector instructions.\nGlobalSumsVectorized/kahan_intel_vector.c\n 1 #include <x86intrin.h>                   \n 2\n 3 static double \n      sum[4] __attribute__ ((aligned (64)));    \n 4\n 5 double do_kahan_sum_intel_v(double* restrict var, long ncells)\n 6 {\n 7    double const zero = 0.0;Intel x86 vector intrinsics version of the enhanced precision Kahan sum \nInclude file with Intel and AMD \nx86 vector intrinsics definitions\nDefines a regularly aligned \narray of four double-\nprecision values\n191 Vectorization methods\n 8    __m256d local_sum = _mm256_broadcast_sd(     \n                         (double const*) &zero);   \n 9    __m256d local_corr = _mm256_broadcast_sd(    \n                          (double const*) &zero);  \n10\n11    #pragma simd                          \n12    #pragma vector aligned                \n13    for (long i = 0; i < ncells; i+=4) {\n14        __m256d var_v = _mm256_load_pd(&var[i]);     \n15        __m256d corrected_next_term =       \n                    var_v + local_corr;       \n16        __m256d new_sum =                   \n                    local_sum + local_corr;   \n17        local_corr = corrected_next_term –  \n                    (new_sum - local_sum);    \n18        local_sum = new_sum;                \n19    }\n20    __m256d sum_v;                          \n21    sum_v  = local_corr;                    \n22    sum_v += local_sum;                     \n23    _mm256_store_pd(sum, sum_v);   \n24\n25    struct esum_type{\n26       double sum;\n27       double correction;\n28    } local;\n29    local.sum = 0.0;\n30    local.correction = 0.0;\n31\n32    for (long i = 0; i < 4; i++) {             \n33       double corrected_next_term_s =          \n                  sum[i] + local.correction;     \n34       double new_sum_s =                      \n                  local.sum + local.correction;  \n35       local.correction =                      \n                  corrected_next_term_s –        \n                  (new_sum_s - local.sum);       \n36       local.sum          = new_sum_s;         \n37    }                                          \n38    double final_sum = \n         local.sum + local.correction;           \n39    return(final_sum);\n40 }\nExample: Implementation of the Kahan sum using GCC vector intrinsics\nThe second implementation of the Kahan sum uses the GCC vector extensions. These\nvector instructions can support other architectures than the AVX vector units on x86\narchitectures. But the GCC vector extensions’ portability is limited to where you can use\nthe GCC compiler. If a longer vector length is specified than the hardware supports, the\ncompiler generates instructions using combinations of shorter vector lengths.Fills a four-wide, double-\nprecision vector variable \nwith zeros\nPragmas to instruct the compiler to \noperate on the aligned vector variables\nLoads four values \nfrom a standard \narray into a vector \nvariable\nDoes the standard \nKahan operation on \nall four-wide vector \nvariables\nStores the four vector lanes \ninto a regularly aligned \narray of four values\nSums the four sums \nfrom the four vector \nlanes using scalar \nvariables\n192 CHAPTER  6Vectorization: FLOPs for free\n(continued)\nGlobalSumsVectorized/kahan_gcc_vector.c\n 1 static double\n      sum[4] __attribute__ ((aligned (64)));    \n 2\n 3 double do_kahan_sum_gcc_v(double* restrict var, long ncells)\n 4 {\n 5    typedef double vec4d __attribute__       \n         ((vector_size(4 * sizeof(double))));  \n 6\n 7    vec4d local_sum = {0.0};    \n 8    vec4d local_corr = {0.0};   \n 9\n10    for (long i = 0; i < ncells; i+=4) {\n11        vec4d var_v = *(vec4d *)&var[i];     \n12        vec4d corrected_next_term =         \n             var_v + local_corr;              \n13        vec4d new_sum =                     \n             local_sum + local_corr;          \n14        local_corr = corrected_next_term –  \n             (new_sum - local_sum);           \n15        local_sum = new_sum;                \n16    }                                       \n17    vec4d sum_v;                            \n18    sum_v  = local_correction;              \n19    sum_v += local_sum;                     \n20    *(vec4d *)sum = sum_v;    \n21\n22    struct esum_type{\n23       double sum;\n24       double correction;\n25    } local;\n26    local.sum = 0.0;\n27    local.correction = 0.0;\n28\n29    for (long i = 0; i < 4; i++) {       \n30       double corrected_next_term_s =    \n            sum[i] + local.correction;     \n31       double new_sum_s =                \n            local.sum + local.correction;  \n32       local.correction =                \n            corrected_next_term_s          \n            (new_sum_s - local.sum);       \n33       local.sum = new_sum_s;            \n34    }                                    \n35    double final_sum = \n            local.sum + local.correction;  \n36    return(final_sum);\n37 }GCC vector extensions version of the enhanced precision Kahan sum\nDefines a regularly \naligned array of four \ndouble-precision values\nDeclares the vec4d \nvector data type\nFills a four-wide, double-precision \nvector variable with zeros\nLoads four values from \na standard array into a \nvector variable\nThe standard Kahan \noperation is done on \nall four-wide vector \nvariables.\nStores the four vector lanes \ninto a regularly aligned \narray of four values\nSums the four sums \nfrom the four vector \nlanes using scalar \nvariables\n193 Vectorization methods\nExample: Implementation of the Kahan sum using C++ vector intrinsics\nFor C++ code, Agner Fog of the Technical University of Denmark wrote a C++ vector\nclass library under an open source license. This class library is portable across hard-\nware architectures and automatically adapts to older hardware with shorter vector\nlengths. The Fog vector class library is well-designed and has an extensive manual.\nMore details on this library are given in the additional reading section at the end of\nthe chapter. \nIn this example, we’ll write the vector version of the Kahan sum in C++ and then call\nit from our C main program. We’ll also handle a remainder block of values that\ndoesn’t fit a full vector width by using the partial_load  function. We won’t always\nhave arrays that are evenly divided into 4-wide groupings. We can sometimes pad the\narrays with extra zeros to make an array the right size, but the better approach is to\nhandle the remaining values in a separate block of code at the end of the loop. Note\nthat the Vec4d  data type is defined in the vector class header file.\nGlobalSumsVectorized/kahan_fog_vector.cpp\n 1 #include ""vectorclass.h""            \n 2  \n 3 static double                                \n      sum[4] __attribute__ ((aligned (64)));    \n 4\n 5 extern ""C"" {                               \n 6 double do_kahan_sum_agner_v(double* var,   \n                               long ncells);  \n 7 }                                          \n 8\n 9 double do_kahan_sum_agner_v(double* var, long ncells)\n10 {\n11    Vec4d local_sum(0.0);     \n12    Vec4d local_corr(0.0);    \n13    Vec4d var_v;\n14\n15    int ncells_main=(ncells/4)*4;\n16    int ncells_remainder=ncells%4;\n17    for (long i = 0; i < ncells_main; i+=4) {\n18        var_v.load(var+i);                   \n19        Vec4d corrected_next_term =          \n             var_v + local_corr;               \n20        Vec4d new_sum =                      \n             local_sum + local_corr;           \n21        local_corr = corrected_next_term –   \n             (new_sum - local_sum);            \n22        local_sum = new_sum;                 \n23    }\n24    if (ncells_remainder > 0) {\n25        var_v.load_partial(ncells_remainder,var+ncells_main);\n26        Vec4d corrected_next_term = var_v + local_corr;\n27        Vec4d new_sum = local_sum + local_corr;Agner Fog’s C++ vector class library for enhanced precision for Kahan sum \nIncludes the vector \nclass header file\nDefines a regularly aligned array \nof four double-precision values\nSpecifies that we \nwant a C-style \nsubroutine\nFills a four-wide, double-precision \nvector variable with zeros\nDefines a vector \ndouble-precision \nvariable of 256 bits \n(or four doubles)\nLoads four values \nfrom a standard \narray into a \nvector variable\n194 CHAPTER  6Vectorization: FLOPs for free\nWe then tested the Kahan sum implemented with the three vector intrinsics against\nthe original serial sum and original Kahan sum. We used version 8.2 of the GCC com-\npiler and ran the tests on a Skylake Gold processor. The GCC compiler fails to vector-\nize the serial sum and the original Kahan sum code. Adding an OpenMP pragma gets\nthe serial sum to vectorize, but the loop-carried dependency in the Kahan sum pre-\nvents the compiler from vectorizing the code. \n It is important to note in the following performance results that the vectorized ver-\nsions for serial and Kahan sums with all three vector intrinsics (bolded) have nearly\nidentical run times. We can do more floating-point operations in the same time and\nsimultaneously reduce the numerical error. This is a great example that with some\neffort, floating-point operations can come for free.\nSETTINGS INFO -- ncells 1073741824 log 30\nInitializing mesh with Leblanc problem, high values first\n  relative diff  runtime    Description(continued)\n28        local_corr = corrected_next_term - (new_sum - local_sum);\n29        local_sum = new_sum;\n30    }\n31\n32    Vec4d sum_v;           \n33    sum_v  = local_corr;   \n34    sum_v += local_sum;    \n35    sum_v.store(sum);        \n36\n37    struct esum_type{\n38       double sum;\n39       double correction;\n40    } local;\n41    local.sum = 0.0;\n42    local.correction = 0.0;\n43   for (long i = 0; i < 4; i++) {\n44      double corrected_next_term_s =     \n           sum[i] + local.correction;      \n45      double new_sum_s =                 \n           local.sum + local.correction;   \n46      local.correction =                 \n           corrected_next_term_s –         \n           (new_sum_s - local.sum);        \n47      local.sum = new_sum_s;             \n48   }\n49   double final_sum = \n        local.sum + local.correction;      \n50   return(final_sum);\n51 }\nThe load and store  commands are more natural in this vector class library than\nsome of the other vector-intrinsic syntax. The standard Kahan \noperation is done on all \nfour-wide vector variables.\nStores the four vector lanes \ninto a regularly aligned \narray of four values\nSums the four sums \nfrom the four vector \nlanes using scalar \nvariables",13040
70-6.4 Programming style for better vectorization.pdf,70-6.4 Programming style for better vectorization,"195 Vectorization methods\n    8.423e-09    1.273343   Serial sum\n            0    3.519778   Kahan sum with double double accumulator\n 4 wide vectors serial sum\n   -3.356e-09    0.683407   Intel vector intrinsics Serial sum\n   -3.356e-09    0.682952   GCC vector intrinsics Serial sum\n   -3.356e-09    0.682756   Fog C++ vector class Serial sum\n 4 wide vectors Kahan sum\n            0    1.030471   Intel Vector intrinsics Kahan sum\n            0    1.031490   GCC vector extensions Kahan sum\n            0    1.032354   Fog C++ vector class Kahan sum\n 8 wide vector serial sum\n   -1.986e-09    0.663277   Serial sum (OpenMP SIMD pragma)\n   -1.986e-09    0.664413   8 wide Intel vector intrinsic Serial sum\n   -1.986e-09    0.664067   8 wide GCC vector intrinsic Serial sum\n   -1.986e-09    0.663911   8 wide Fog C++ vector class Serial sum\n 8 wide vector Kahan sum\n   -1.388e-16    0.689495   8 wide Intel Vector intrinsics Kahan sum\n   -1.388e-16    0.689100   8 wide GCC vector extensions Kahan sum\n   -1.388e-16    0.689472   8 wide Fog C++ vector class Kahan sum\n6.3.5 Not for the faint of heart: Using assembler code for vectorization\nIn this section, we will cover when it is appropriate to write vector assembly in your appli-\ncation. We’ll also discuss what vector assembler code looks like, how to disassemble your\ncompiled code, and how to tell which vector instruction set the compiler generated.\n Programming vector units directly with vector assembly instructions has the great-\nest opportunity to achieve maximum performance. But it takes a deep understanding\nof the performance behavior of the large number of vector instructions across many\ndifferent processors. Programmers without this expertise will probably get better per-\nformance from using vector intrinsics as shown in the previous section than from\ndirectly writing vector assembler instructions. In addition, the portability of vector\nassembly code is limited; it will only work on a small set of processor architectures. For\nthese reasons, it is rare that writing vector assembly instructions makes sense.\nExample: Looking at vector assembly instructions\nTo see what vector assembly instructions look like, we can display these from the\nobject file using the objdump  command. The listing shows the output from the follow-\ning command:\nobjdump -d -M=intel --no-show-raw-insn <object code file.o>\n 0000000000000000 <do_kahan_sum_gcc_v>:\n 0:   test   %rsi,%rsi\n 3:   jle    90 <do_kahan_sum_gcc_v+0x90>\n 9:   vxorpd %xmm2,%xmm2,%xmm2\n d:   xor    %eax,%eax\n f:   vmovapd %ymm2,%ymm0Assembler code for GCC vector extensions version of Kahan sum\n196 CHAPTER  6Vectorization: FLOPs for free\nBecause it seldom makes sense to do more than simply look at the assembler instruc-\ntions that the compiler generates, we won’t go through a programming example of\nwriting a routine in assembler from scratch.\n6.4 Programming style for better vectorization\nWe suggest that you adopt a programming style that is more compatible with the\nneeds of vectorization and other forms of loop parallelization. Based on the lessons\nlearned from the examples throughout the chapter, certain programming styles can\nhelp the compiler generate vectorized code. Adopting the following programming\nstyles leads to better performance out of the box and less work needed for optimiza-\ntion efforts.(continued)\n13:   nopl   0x0(%rax,%rax,1)\n18:   vmovapd %ymm0,%ymm1\n1c:   vaddpd %ymm2,%ymm0,%ymm0\n20:   vaddpd (%rdi,%rax,8),%ymm2,%ymm3\n25:   add    $0x4,%rax\n29:   vsubpd %ymm1,%ymm0,%ymm1\n2d:   vsubpd %ymm1,%ymm3,%ymm2\n31:   cmp    %rax,%rsi\n34:   jg     18 <do_kahan_sum_gcc_v+0x18>\n36:   vaddpd %ymm2,%ymm0,%ymm0\n3a:   vmovapd %ymm0,0x0(%rip)             # 42 <do_kahan_sum_gcc_v+0x42>\n42:   vxorpd %xmm2,%xmm2,%xmm2\n46:   vaddsd 0x0(%rip),%xmm2,%xmm0        # 4e <do_kahan_sum_gcc_v+0x4e>\n4e:   vaddsd %xmm2,%xmm0,%xmm2\n52:   vaddsd 0x0(%rip),%xmm0,%xmm0        # 5a <do_kahan_sum_gcc_v+0x5a>\n5a:   vsubsd %xmm2,%xmm0,%xmm0\n5e:   vaddsd %xmm2,%xmm0,%xmm3\n62:   vaddsd 0x0(%rip),%xmm0,%xmm1        # 6a <do_kahan_sum_gcc_v+0x6a>\n6a:   vsubsd %xmm2,%xmm3,%xmm2\n6e:   vsubsd %xmm2,%xmm1,%xmm1\n72:   vaddsd %xmm1,%xmm3,%xmm2\n76:   vaddsd 0x0(%rip),%xmm1,%xmm0        # 7e <do_kahan_sum_gcc_v+0x7e>\n7e:   vsubsd %xmm3,%xmm2,%xmm3\n82:   vsubsd %xmm3,%xmm0,%xmm0\n86:   vaddsd %xmm2,%xmm0,%xmm0\n8a:   vzeroupper\n8d:   retq\n8e:   xchg   %ax,%ax\n90:   vxorpd %xmm0,%xmm0,%xmm0\n94:   jmp    3a <do_kahan_sum_gcc_v+0x3a>\nIf you see ymm registers, vector instructions were generated. zmm registers indicate that\nthere are AVX512 vector instructions. The xmm registers are generated for both scalar\nand SSE vector instructions. We can tell the listing was from the kahan_gcc_vector.c.o\nfile because there are no zmm instructions in the output. If we look at the kahan_gcc\n_vector8.c.o file that generates 512-bit instructions, you’ll see zmm instructions.\n197 Programming style for better vectorization\nGeneral suggestions:\nUse the restrict  attribute on pointers in function arguments and declarations\n(C and C++).\nUse pragmas or directives where needed to inform the compiler.\nBe careful with optimizing for the compiler with #pragma  unroll  and other tech-\nniques; you might limit the possible options for the compiler transformations.2\nPut exceptions and error checks with print statements in a separate loop.\nConcerning data structures:\nTry to use a data structure with a long length for the innermost loop.\nUse the smallest data type needed ( short  rather than int).\nUse contiguous memory accesses. Some newer instruction sets implement\ngather/scatter memory loads, but these are less efficient.\nUse Structure of Arrays (SOA) rather than Array of Structures (AOS).\nUse memory-aligned data structures where possible.\nRelated to loop structures:\nUse simple loops without special exit conditions.\nMake loop bounds a local variable by copying global values and then using them.\nUse the loop index for array addresses when possible.\nExpose the loop bound size so it is known to the compiler. If the loop is only\nthree iterations long, the compiler might unroll the loop rather than generate\na four-wide vector instruction.\nAvoid array syntax in performance-critical loops (Fortran).\nIn the loop body:\nDefine local variables within a loop so that it is clear that these are not carried\nto subsequent iterations (C and C++).\nVariables and arrays within a loop should be write-only or read-only (only on\nthe left side of the equal sign or on the right side, except for reductions).\nDon’t reuse local variables for a different purpose in the loop—create a new\nvariable. The memory space you waste is far less important than the confusion\nthis creates for the compiler.\nAvoid function calls and inline instead (manually or with the compiler).\nLimit conditionals within the loop and, where necessary, use simple forms that\ncan be masked.\n2A #pragma  unroll  tells the compiler to insert the specified number of statements to replace iterations of the\nloop and to reduce the loop iterations in the loop control statement or remove it altogether.",7220
71-6.5 Compiler flags relevant for vectorization for various compilers.pdf,71-6.5 Compiler flags relevant for vectorization for various compilers,"198 CHAPTER  6Vectorization: FLOPs for free\nConcerning compiler settings and flags:\nUse the latest version of a compiler and prefer compilers that do better vector-\nization.\nUse a strict aliasing compiler flag.\nGenerate code for the most powerful vector instruction set you can get away with.\n6.5 Compiler flags relevant for vectorization for \nvarious compilers\nTables 6.2 and 6.3 show the compiler flags that are recommended for vectorization for\nthe latest version of various compilers. Compiler flags for vectorization frequently\nchange, so check the documentation for the compiler version that you are using.\n The strict aliasing flag listed in column two of table 6.2 should help with auto-\nvectorization for C and C++, but verify that it doesn’t break any code. Column three in\ntable 6.2 has the various options to specify which vectorization instruction set to use for\nsome of the compilers. The ones shown in the table should be a good starting point. Vec-\ntorization reports can be generated with the compiler flags in column two of table 6.2.\nThe compiler reports are still improving for most of the compilers and are likely to\nchange. For GCC, the optimized and missed flags are recommended. Getting the loop\noptimization reports at the same time as the vectorization can be helpful so that you can\nsee if loops have been unrolled or interchanged. If not using the rest of OpenMP, but\nusing OpenMP SIMD directives, the flags in the last column of table 6.3 should be used.\nTable 6.2 Vectorization flags for various compilers\nCompiler Strict aliasing Vectorization Floating-point flags\nGCC, G++, \nGFortran v9-fstrict-aliasing -ftree-vectorize\n-march=native \n-mtune=native\nver 8.0+: \n-mprefer-vector\n-width=512-fno-trapping-math\n-fno-math-errno\nClang v9 -fstrict-aliasing -fvectorize\n-march=native\n-mtune=native-fno-math-errno\nIntel icc v19 -ansi-alias -restrict\n-xHost\n-vecabi=cmdtarget\nver 18.0+:   \n-qopt-zmm-usage=high\nMSVC Not implemented On by default\nIBM XLC v16 -qalias=ansi\n-qalias=restrict-qsimd=auto\n-qhot \n-qarch=pwr9\n-qtune=pwr9\n199 Compiler flags relevant for vectorization for various compilers\nYou can set the vector instructions to any single set such as AVX2 or multiple sets.\nWe’ll show you how to do both. For a single instruction set, the flags shown in the pre-\nvious tables request that the compiler should use the vector instruction set for the pro-\ncessor that is used for compiling (march=native , -xHost , and -qarch=pwer9 ). Without\nthis flag, the compiler uses the SSE2 set. If you are interested in running across a wide\nrange of processors, you may want to specify an older instruction set or just use the\ndefault. There is some loss in performance from older sets. \n Support for more than one vector instruction set can be added with the Intel com-\npiler. This is common practice for the Intel Knights Landing processor, where the\ninstruction set for the host processor might be different. For this, you must specify\nboth instruction sets:\n-axmic-avx512 -xcore-avx2\nThe -ax adds the additional set. Note that the host  keyword cannot be used when\nrequesting two instruction sets.Cray -h restrict=[a,f] -h vector3\n-h preferred_vector_width=#\nwhere # can be [64,128,256,512]\nTable 6.3 OpenMP SIMD and vectorization report flags for various compilers\nCompiler Vectorization report OpenMP SIMD\nGCC, G++, \nGFortran v9-fopt-info-vec-optimized[=file]\n-fopt-info-vec-missed[=file]\n-fopt-info-vec-all[=file] \nSame for loop optimizations\nreplace vec with loop-fopenmp-simd\nClang v9 -Rpass-analysis=loop-vectorize -fopenmp-simd\nIntel icc v19 -qopt-report=[1-5]\n-qopt-report-phase=vec,loop\n-qopt-report-filter=\n   ”filename,ln1-ln2”-qopenmp-simd\nMSVC -Qvec-report:[1,2] -openmp:experimental\nIBM XLC v16 -qreport -qopenmp\nCray -h msgs -h negmsgs\n-h list=a-h omp (default)Table 6.2 Vectorization flags for various compilers\nCompiler Strict aliasing Vectorization Floating-point flags\n200 CHAPTER  6Vectorization: FLOPs for free\n We briefly mentioned the use of a floating-point flag to encourage vectorization in\nthe sum reduction kernel when discussing listing 6.1. When vectorizing loops with a\nconditional, the compiler inserts a mask that uses only part of the vector results. But\nthe masked operations can generate a floating-point error by dividing by zero or tak-\ning the square root of a negative number. GCC and Clang compilers require that the\nextra floating-point flags shown in the last column of table 6.2 be set to vectorize loops\nwith conditionals and any potentially problematic floating-point operations.\n There are some situations where you might want to turn off vectorization. Turning\noff vectorization allows you to see the improvement and speedup you achieved with\nvectorization. It allows you to check that you get the same answer with and without\nvectorization. Sometimes auto-vectorization gives you the wrong answer and, thus, you\nwould want to turn it off. You may also want to only vectorize the computationally\nintensive files and skip the rest.\nThe vectorization and performance results in this chapter were with GCC v8 and v9\nand with the Intel compiler v19. As noted in table 6.1, the 512-bit vector support was\nadded to GCC starting in version 8 and in Intel in version 18. So the capability for the\nnew 512-bit vector hardware is recent.\nA CMAKE  MODULE  FOR SETTING  COMPILER  FLAGS\nSetting all of the flags for compiler vectorization is messy and difficult to get right. So we\nhave created a CMake module that you can use, which is similar to the FindOpenMP\n.cmake and FindMPI.cmake modules. Then the main CMakeLists.txt file just needs\nfind_package(Vector)\nif (CMAKE_VECTOR_VERBOSE)\n   set(VECTOR_C_FLAGS ""${VECTOR_C_FLAGS} ${VECTOR_C_VERBOSE}"")\nendif()\nset(CMAKE_C_FLAGS ""${CMAKE_C_FLAGS} ${VECTOR_C_FLAGS}"")\nThe CMake module is shown in FindVector.cmake in the main directory for this chap-\nter’s examples at https:/ /github.com/EssentialsofParallelComputing/Chapter6.git .\nAlso see the GlobalSumsVectorized  code example for using the FindVector.cmakeTable 6.4 Compiler flags to turn off vectorization\nCompiler Flag\nGCC -fno-tree-vectorize  (default is on at –O3)\nClang -fno-vectorize  (default is on)\nIntel -no-vec  (default is on with –O2 or higher)\nMSVC There is no compiler flag to turn off vectorization (by default is on)\nXLC -qsimd=noauto  (default is on at –O3 level)\nCray -h vector0 -hpf0 or -hfp1  (default vectorization level is –h vector2 )\n201 Compiler flags relevant for vectorization for various compilers\nmodule. We’ll migrate the module to other examples to help clean up our CMake-\nLists.txt file as well. The following listing is an excerpt from the module for the C\ncompiler. The flags for C++ and Fortran are also set with similar code in the Find-\nVector.cmake module.\nFindVector.cmake\n  8 # Main output flags\n  9 #   VECTOR_<LANG>_FLAGS      \n 10 #   VECTOR_NOVEC_<LANG>_FLAGS      \n 11 #   VECTOR_<LANG>_VERBOSE         \n 12 # Component flags\n 13 #   VECTOR_ALIASING_<LANG>_FLAGS     \n 14 #   VECTOR_ARCH_<LANG>_FLAGS            \n 15 #   VECTOR_FPMODEL_<LANG>_FLAGS     \n 16 #   VECTOR_NOVEC_<LANG>_OPT       \n 17 #   VECTOR_VEC_<LANG>_OPTS    \n...\n 25 if(CMAKE_C_COMPILER_LOADED)\n 26   if (""${CMAKE_C_COMPILER_ID}"" STREQUAL ""Clang"") # using Clang\n 27     set(VECTOR_ALIASING_C_FLAGS ""${VECTOR_ALIASING_C_FLAGS} \n          -fstrict-aliasing"")\n 28     if (""${CMAKE_SYSTEM_PROCESSOR}"" STREQUAL ""x86_64"")\n 29       set(VECTOR_ARCH_C_FLAGS ""${VECTOR_ARCH_C_FLAGS} \n            -march=native -mtune=native"")\n 30     elseif (""${CMAKE_SYSTEM_PROCESSOR}"" STREQUAL ""ppc64le"")\n 31       set(VECTOR_ARCH_C_FLAGS ""${VECTOR_ARCH_C_FLAGS}\n            -mcpu=powerpc64le"")\n 32     elseif (""${CMAKE_SYSTEM_PROCESSOR}"" STREQUAL ""aarch64"")\n 33       set(VECTOR_ARCH_C_FLAGS ""${VECTOR_ARCH_C_FLAGS}\n            -march=native -mtune=native"")\n 34     endif (""${CMAKE_SYSTEM_PROCESSOR}"" STREQUAL ""x86_64"")\n 35 \n 36     set(VECTOR_OPENMP_SIMD_C_FLAGS ""${VECTOR_OPENMP_SIMD_C_FLAGS}\n          -fopenmp-simd"")\n 37     set(VECTOR_C_OPTS ""${VECTOR_C_OPTS} -fvectorize"")\n 38     set(VECTOR_C_FPOPTS ""${VECTOR_C_FPOPTS} -fno-math-errno"")\n 39     set(VECTOR_NOVEC_C_OPT ""${VECTOR_NOVEC_C_OPT} -fno-vectorize"")\n 40     set(VECTOR_C_VERBOSE ""${VECTOR_C_VERBOSE} -Rpass=loop-vectorize\n           -Rpass-missed=loop-vectorize -Rpass-analysis=loop-vectorize"")\n 41 \n 42   elseif (""${CMAKE_C_COMPILER_ID}"" STREQUAL ""GNU"") # using GCC\n 43     set(VECTOR_ALIASING_C_FLAGS ""${VECTOR_ALIASING_C_FLAGS}\n          -fstrict-aliasing"")\n 44     if (""${CMAKE_SYSTEM_PROCESSOR}"" STREQUAL ""x86_64"")\n 45       set(VECTOR_ARCH_C_FLAGS ""${VECTOR_ARCH_C_FLAGS}\n            -march=native -mtune=native"")\n 46     elseif (""${CMAKE_SYSTEM_PROCESSOR}"" STREQUAL ""ppc64le"")\n 47       set(VECTOR_ARCH_C_FLAGS ""${VECTOR_ARCH_C_FLAGS}\n            -mcpu=powerpc64le"")Listing 6.2 Excerpt from FindVector.cmake for C compiler\nSets all flags and turns\non vectorizationSets all flags but \ndisables vectorization\nTurns on verbose messages \nwhen compiling for \nvectorization feedback\nStricter aliasing option to \nhelp auto-vectorizationSet to\ncompile for\narchitecture\nthat it’s on Set so that Kahan sum does not get \noptimized out (unsafe optimizations)\nTurns off vectorization for debugging \nand performance measurement Turns on\nvectorization\n202 CHAPTER  6Vectorization: FLOPs for free\n 48     elseif (""${CMAKE_SYSTEM_PROCESSOR}"" STREQUAL ""aarch64"")\n 49       set(VECTOR_ARCH_C_FLAGS ""${VECTOR_ARCH_C_FLAGS}\n            -march=native -mtune=native”)\n 50     endif (“${CMAKE_SYSTEM_PROCESSOR}” STREQUAL “x86_64”)\n 51 \n 52     set(VECTOR_OPENMP_SIMD_C_FLAGS “${VECTOR_OPENMP_SIMD_C_FLAGS}\n             -fopenmp-simd"")\n 53     set(VECTOR_C_OPTS ""${VECTOR_C_OPTS} -ftree-vectorize"")\n 54     set(VECTOR_C_FPOPTS ""${VECTOR_C_FPOPTS} -fno-trapping-math\n          -fno-math-errno"")\n 55     if (""${CMAKE_SYSTEM_PROCESSOR}"" STREQUAL ""x86_64"")\n 56       if (""${CMAKE_C_COMPILER_VERSION}"" VERSION_GREATER ""7.9.0"")\n 57         set(VECTOR_C_OPTS ""${VECTOR_C_OPTS} -mprefer-vector-width=512"")\n 58       endif (""${CMAKE_C_COMPILER_VERSION}"" VERSION_GREATER ""7.9.0"")\n 59     endif (""${CMAKE_SYSTEM_PROCESSOR}"" STREQUAL ""x86_64"")\n 60 \n 61     set(VECTOR_NOVEC_C_OPT ""${VECTOR_NOVEC_C_OPT} -fno-tree-vectorize"")\n 62     set(VECTOR_C_VERBOSE ""${VECTOR_C_VERBOSE} -fopt-info-vec-optimized \n           -fopt-info-vec-missed -fopt-info-loop-optimized\n           -fopt-info-loop-missed"")\n 63 \n 64   elseif (""${CMAKE_C_COMPILER_ID}"" STREQUAL ""Intel"") # using Intel C\n 65     set(VECTOR_ALIASING_C_FLAGS ""${VECTOR_ALIASING_C_FLAGS}\n          -ansi-alias"")\n 66     set(VECTOR_FPMODEL_C_FLAGS ""${VECTOR_FPMODEL_C_FLAGS}\n          -fp-model:precise"")\n 67 \n 68     set(VECTOR_OPENMP_SIMD_C_FLAGS ""${VECTOR_OPENMP_SIMD_C_FLAGS}\n          -qopenmp-simd"")\n 69     set(VECTOR_C_OPTS ""${VECTOR_C_OPTS} -xHOST"")\n 70     if (""${CMAKE_C_COMPILER_VERSION}"" VERSION_GREATER ""17.0.4"")\n 71       set(VECTOR_C_OPTS ""${VECTOR_C_OPTS} -qopt-zmm-usage=high"")\n 72     endif (""${CMAKE_C_COMPILER_VERSION}"" VERSION_GREATER ""17.0.4"")\n 73     set(VECTOR_NOVEC_C_OPT ""${VECTOR_NOVEC_C_OPT} -no-vec"")\n 74     set(VECTOR_C_VERBOSE ""${VECTOR_C_VERBOSE} -qopt-report=5\n           -qopt-report-phase=openmp,loop,vec"")\n 75 \n 76   elseif (CMAKE_C_COMPILER_ID MATCHES ""PGI"")\n 77     set(VECTOR_ALIASING_C_FLAGS ""${VECTOR_ALIASING_C_FLAGS}\n          -alias=ansi"")\n 78     set(VECTOR_OPENMP_SIMD_C_FLAGS ""${VECTOR_OPENMP_SIMD_C_FLAGS}\n          -Mvect=simd"")\n 79 \n 80     set(VECTOR_NOVEC_C_OPT ""${VECTOR_NOVEC_C_OPT} -Mnovect "")\n 81     set(VECTOR_C_VERBOSE ""${VECTOR_C_VERBOSE} -Minfo=loop,inline,vect"")\n 82 \n 83   elseif (CMAKE_C_COMPILER_ID MATCHES ""MSVC"")\n 84     set(VECTOR_C_OPTS ""${VECTOR_C_OPTS}"" "" "")\n 85 \n 86     set(VECTOR_NOVEC_C_OPT ""${VECTOR_NOVEC_C_OPT}"" "" "")\n 87     set(VECTOR_C_VERBOSE ""${VECTOR_C_VERBOSE} -Qvec-report:2"")\n 88 \n 89   elseif (CMAKE_C_COMPILER_ID MATCHES ""XL"")\n 90     set(VECTOR_ALIASING_C_FLAGSS ""${VECTOR_ALIASING_C_FLAGS}\n          -qalias=restrict"")",12188
72-6.6 OpenMP SIMD directives for better portability.pdf,72-6.6 OpenMP SIMD directives for better portability,"203 OpenMP SIMD directives for better portability\n 91     set(VECTOR_FPMODEL_C_FLAGSS ""${VECTOR_FPMODEL_C_FLAGS} -qstrict"")\n 92     set(VECTOR_ARCH_C_FLAGSS ""${VECTOR_ARCH_C_FLAGS} -qhot -qarch=auto\n          -qtune=auto"")\n 93 \n 94     set(CMAKE_VEC_C_FLAGS ""${CMAKE_VEC_FLAGS} -qsimd=auto"")\n 95     set(VECTOR_NOVEC_C_OPT ""${VECTOR_NOVEC_C_OPT} -qsimd=noauto"")\n 96     # ""long vector"" optimizations\n 97     #set(VECTOR_NOVEC_C_OPT ""${VECTOR_NOVEC_C_OPT} -qhot=novector"")\n 98     set(VECTOR_C_VERBOSE ""${VECTOR_C_VERBOSE} -qreport"")\n 99 \n100   elseif (CMAKE_C_COMPILER_ID MATCHES ""Cray"")\n101     set(VECTOR_ALIASING_C_FLAGS ""${VECTOR_ALIASING_C_FLAGS}\n          -h restrict=a"")\n102     set(VECTOR_C_OPTS ""${VECTOR_C_OPTS} -h vector=3"")\n103 \n104     set(VECTOR_NOVEC_C_OPT ""${VECTOR_NOVEC_C_OPT} -h vector=0"")\n105     set(VECTOR_C_VERBOSE ""${VECTOR_C_VERBOSE} -h msgs -h negmsgs\n          -h list=a"")\n106 \n107   endif()\n108 \n109   set(VECTOR_BASE_C_FLAGS ""${VECTOR_ALIASING_C_FLAGS}\n         ${VECTOR_ARCH_C_FLAGS} ${VECTOR_FPMODEL_C_FLAGS}"")\n110   set(VECTOR_NOVEC_C_FLAGS ""${VECTOR_BASE_C_FLAGS}\n         ${VECTOR_NOVEC_C_OPT}"")\n111   set(VECTOR_C_FLAGS ""${VECTOR_BASE_C_FLAGS} ${VECTOR_C_OPTS}\n         ${VECTOR_C_FPOPTS} ${VECTOR_OPENMP_SIMD_C_FLAGS}"")\n112 endif()\n6.6 OpenMP SIMD directives for better portability\nWith the release of the OpenMP 4.0 standard, we have the option of using a more por-\ntable set of SIMD directives. These directives are implemented as commands rather\nthan hints. We have already seen the use of these directives in section 6.3.3. The direc-\ntives can be used to only request vectorization, or these can be combined with the\nfor/do directive to request both threading and vectorization. The general syntax for\nC and C++ pragmas is\n#pragma omp simd      / Vectorizes the following loop or block of code\n#pragma omp for simd  / Threads and vectorizes the following loop\nThe general syntax for Fortran directives is\n!$omp simd       / Vectorizes the following loop or block of code\n!$omp do simd    / Threads and vectorizes the following loop\nThe basic SIMD directive can be supplemented with additional clauses to communi-\ncate more information. The most common additional clause is some variant of the\nprivate  clause. This clause breaks false dependencies by creating a separate, private\nvariable for each vector lane. An example of the syntax is\n204 CHAPTER  6Vectorization: FLOPs for free\n#pragma omp simd private(x)\n   for (int i=0; i<n; i++){\n      x=array(i);\n      y=sqrt(x)*x;\n   }\nFor a simple private  clause, the recommended approach for C and C++ program-\nmers is to just define the variable in the loop to make clear your intent:\ndouble x=array(i);\nThe firstprivate  clause initializes the private variable for each thread with the value\ncoming into the loop, while the lastprivate  clause sets the variable after the loop to\nthe logically last value it would have in a sequential form of the loop.\n The reduction  clause creates a private variable for each lane and then performs\nthe specified operation between the values for each lane at the end of the loop. The\nreduction variables are initialized for each vector lane as would make sense for the\nspecified operation.\n The aligned  clause tells the compiler that the data is aligned on a 64-byte boundary\nso that peel loops do not need to be generated. Aligned data can be loaded into vector\nregisters more efficiently. But first, the memory needs to be allocated with memory\nalignment. There are many different functions that you can use to get aligned memory,\nbut there are still issues with portability. Here are some of the possibilities:\nvoid *memalign(size_t alignment, size_t size);\nint posix_memalign(void **memptr, size_t alignment, size_t size);\nvoid *aligned_alloc(size_t alignment, size_t size);\nvoid *aligned_malloc(size_t alignment, size_t size);\nYou can also use attributes to a memory definition to specify memory alignment:\ndouble x[100] __attribute__((aligned(64)));\nAnother important modifier is the collapse  clause. It tells the compiler to combine\nnested loops into a single loop for the vectorized implementation. The argument to\nthe clause indicates how many loops to collapse: \n#pragma omp collapse(2)\n   for (int j=0; j<n; j++){\n      for (int i=0; i<n; i++){\n         x[j][i] = 0.0;\n      }\n   }\nThe loops are required to be perfectly  nested. Perfectly nested loops only have state-\nments in the innermost loop, with no extraneous statements before or after each loop\nblock. The following clauses are for more specialized cases: \n205 OpenMP SIMD directives for better portability\nThe linear  clause says that the variable changes for every iteration by some lin-\near function. \nThe safelen  clause tells the compiler that the dependencies are separated by\nthe specified length, which allows the compiler to vectorize for vector lengths\nshorter than or equal to the safe length clause argument. \nThe simdlen  clause generates vectorization of the specified length instead of\nthe default length.\nOPENMP SIMD FUNCTIONS\nWe can also vectorize an entire function or subroutine so that it can be called from\nwithin a vectorized region of the code. The syntax is a little different for C/C++ and\nFortran. For C/C++, we’ll use an example where the radial distance of an array of\npoints is calculated using the Pythagorean theorem:\n#pragma omp declare simd\ndouble pythagorean(double a, double b){\n   return(sqrt(a*a + b*b));\n}\nFor Fortran, the subroutine or function name must be specified as an argument to the\nSIMD clause:\nsubroutine pythagorean(a, b, c)\n!$omp declare simd(pythagorean)\nreal*8 a, b, c\n   c = sqrt(a**2+b**2)\nend subroutine pythagorean\nThe OpenMP SIMD function directive can also take some of the same clauses and\nsome new ones as follows:\nThe inbranch  or notinbranch  clause informs the compiler whether the func-\ntion is called from within a conditional or not.\nThe uniform  clause says that the argument specified in the clause stays constant\nfor all calls and does not need to be set up as a vector in the vectorized call.\nThe linear(ref,  val,  uval)  clause specifies to the compiler that the variable\nin the clause argument is linear in some form. For example, Fortran passes\narguments by reference and when it passes subsequent array locations. In the\nprevious Fortran example, the clause would look like this:\n!$omp declare simd(pythagorean) linear(ref(a, b, c))\nThe clause can also be used to specify that the value is linear and whether the\nstep is a larger constant as might occur in a strided access.\nThe aligned  and simdlen  clauses are similar to the uses in the loop-oriented\nOpenMP SIMD clauses.",6778
73-6.7 Further explorations.pdf,73-6.7 Further explorations,,0
74-6.7.1 Additional reading.pdf,74-6.7.1 Additional reading,,0
75-6.7.2 Exercises.pdf,75-6.7.2 Exercises,,0
76-7.1 OpenMP introduction.pdf,76-7.1 OpenMP introduction,"206 CHAPTER  6Vectorization: FLOPs for free\n6.7 Further explorations\nYou won’t find a lot of available materials on vectorization. The best approach for fur-\nther explorations is to try vectorizing a smaller code block and experiment with the\ncompilers that you commonly use. That being said, Intel has a lot of brief vectoriza-\ntion guides that are the best and most current resources. Look on the Intel website for\nthe latest materials.\n6.7.1 Additional reading\nJohn Levesque, Cray Corporation, has authored a recent book with a good chapter on\nvectorization:\nJohn Levesque and Aaron Vose, Programming for Hybrid Multi/Manycore MPP Systems ,\n(CRC Press, 2017).\nAgner Fog has some of the best references on vectorization in his optimization guides,\nfor example:\nAgner Fog, “Optimizing software in C++: An optimization guide for Windows,\nLinux and Mac platforms,” 2004–2018 (last updated Aug, 2018).\nAgner Fog, “VCL C++ vector class library,” v. 1.30 (2012–2017) available as a\nPDF at https:/ /www.agner.org/optimize/vectorclass.pdf .\n6.7.2 Exercises\n1Experiment with auto-vectorizing loops from the multimaterial code in section\n4.3 ( https:/ /github.com/LANL/MultiMatTest.git ). Add the vectorization and\nloop report flags and see what your compiler tells you.\n2Add OpenMP SIMD pragmas to help the compiler vectorize loops to the loop\nyou selected in the first exercise.\n3For one of the vector-intrinsic examples, change the vector length from four\ndouble-precision values to an eight-wide vector width. Check the source code\nfor this chapter for examples of working code for eight-wide implementations.\n4If you are on an older CPU, does your program from exercise 3 successfully\nrun? What is the performance impact?\nSummary\nBoth auto- and manual vectorization can provide significant performance improve-\nments for your code. To underscore this\nWe show several different methods for vectorizing code with different levels of\ncontrol, effort, and performance.\nWe summarize the proper compiler flags and their usage.\nWe provide a list of programming styles to achieve vectorization.\n207OpenMP that performs\nAs many-core architectures grow in size and popularity, the details of thread-level\nparallelism become a critical factor in software performance. In this chapter, we\nfirst introduce the basics of Open Multi-Processing (OpenMP), a shared memory\nprogramming standard, and why it’s important to have a fundamental understand-\ning of how OpenMP functions. We will look at sample problems ranging in diffi-\nculty from a simple common “Hello World” example to a complex split-direction\nstencil implementation with OpenMP parallelization. We will thoroughly analyze\nthe interaction between OpenMP directives and the underlying OS kernel, as well as\nthe memory hierarchy and hardware features. Finally, we will investigate a promising\nhigh-level approach to OpenMP programming for future extreme-scale applications.This chapter covers\nPlanning and designing a correct and performant \nOpenMP program\nWriting loop-level OpenMP for modest parallelism\nDetecting correctness problems and improving \nrobustness\nFixing performance issues with OpenMP \nWriting scalable OpenMP for high performance",3251
77-7.1.1 OpenMP concepts.pdf,77-7.1.1 OpenMP concepts,"208 CHAPTER  7OpenMP that performs\nWe show that high-level OpenMP is efficient for algorithms containing many short\nloops of computational work. \n When compared to more standard-threading approaches, the high-level OpenMP\nparadigm leads to a reduction in thread overhead costs, synchronization waits, cache\nthrashing, and memory usage. Given these advantages, it is essential that the modern\nparallel computing programmer (you) knows both shared and distributed memory\nprogramming paradigms. We discuss the distributed memory programming paradigm\nin chapter 8 on the Message Passing Interface (MPI).\nNOTE You’ll find the accompanying source code for this chapter at https:/ /\ngithub.com/EssentialsofParallelComputing/Chapter7 .\n7.1 OpenMP introduction\nOpenMP is one of the most widely supported open standards for threads and shared-\nmemory parallel programming. In this section, we will explain the standard, ease of\nuse, expected gains, difficulties, and the memory models. \n The version of OpenMP that you see today took some time to develop and is still\nevolving. The origin of OpenMP began when several hardware vendors introduced\ntheir implementations in the early 1990s. A failed attempt was made in 1994 to stan-\ndardize these implementations in the ANSI X3H5 draft standard. It was not until the\nintroduction of wide-scale, multi-core systems in the late ’90s that a re-emergence of\nthe OpenMP approach was spurred, leading to the first OpenMP standard in 1997. \n Today, OpenMP provides a standard and portable API for writing shared-memory\nparallel programs using threads; it’s known to be easy to use, allowing for fast imple-\nmentation, and requires only a small increase in code, normally seen in the context of\npragmas or directives. A pragma (C/C++) or directive (Fortran) indicates to the com-\npiler where to initiate OpenMP threads. These terms, pragma and directive, are often\nused interchangeably. Pragmas  are preprocessor statements in C and C++. Directives  are\nwritten as comments in Fortran in order for the program to retain the standard lan-\nguage syntax when OpenMP is not used. Although using OpenMP requires a compiler\nthat supports it, most compilers come standard with that support.\n OpenMP makes parallelization achievable for a beginner, thus allowing for an easy\nand fun introduction to scaling an application beyond one core. With the easy use of\nOpenMP pragmas and directives, a block of code can be quickly executed in parallel.\nIn figure 7.1, you can see a conceptual view of the effort required and the perfor-\nmance obtained for OpenMP and MPI (discussed in chapter 8). Using OpenMP will\noften be the first exciting step into scaling an application.\n7.1.1 OpenMP concepts\nAlthough it is easy to achieve modest parallelism with OpenMP, thorough optimiza-\ntion can be a challenge. The source of the difficulty is the relaxed memory model that\npermits thread race conditions to exist. By relaxed , we mean that the value of the vari-\nables in main memory are not updated immediately. It would be too expensive to do\n209 OpenMP introduction\nso for every change in variables. Because of the delay in the updates, minor timing dif-\nferences between memory operations by each thread on shared variables have the\npotential to cause different results from run-to-run. Let’s look at some definitions:\nRelaxed memory model —The value of the variables in main memory or caches of\nall the processors are not updated immediately.\nRace condition —A situation where multiple outcomes are possible, and the\nresult is dependent on the timing of the contributors.\nOpenMP was initially used to parallelize highly regular loops using threads on shared\nmemory multiprocessors. Within a threaded parallel construct, each variable can be\neither shared or private. The terms shared  and private  have a particular meaning for\nOpenMP. Here are their definitions:\nPrivate variable —In the context of OpenMP, a private variable is local and only\nvisible to its thread.\nShared variable —In the context of OpenMP, a shared variable is visible and mod-\nifiable by any thread. \nTruly understanding these terms requires a fundamental view of how memory is man-\naged for a threaded application. As figure 7.2 shows, each thread has a private mem-\nory in its stack and shares memory in the heap.\n OpenMP directives specify work sharing but say nothing about the memory or data\nlocation. As a programmer, you must understand the implicit rules for the memory\nscope of variables. The OS kernel can use several techniques to manage memory for\nOpenMP and threading. The most common technique is the first touch concept,\nwhere memory is allocated nearest to the thread where it is first touched. We define\nwork sharing and first touch as\nWork sharing —To split the work across a number of threads or processes. \nFirst touch —The first touch of an array causes the memory to be allocated. The\nmemory is allocated near the thread location where the touch occurs. Prior to\nthe first touch, the memory only exists as an entry in a virtual memory table.\nThe physical memory that corresponds to the virtual memory is created when it\nis first accessed.MPI\nOpenMP\nTotal eﬀortPerformance\nFigure 7.1 Conceptual visualization \nof the programming effort required to \nimprove performance using either MPI \nor OpenMP\n210 CHAPTER  7OpenMP that performs\nThe reason that first touch is important is that on many high-end, high-performance\ncomputing nodes, there are multiple memory regions. When there are multiple mem-\nory regions, there is often Non-Uniform Memory Access (NUMA) from a CPU and its\nprocesses to different portions of memory, adding an important consideration for\noptimizing code performance.\nDEFINITION On some computing nodes, blocks of memory are closer to some\nprocessors than others. This situation is called Non-Uniform Memory Access\n(NUMA). This is often the case when a node has two CPU sockets with each\nsocket having its own memory. A processor’s access to memory in the other\nNUMA domain typically takes twice the time (penalty) as it does to access its\nown memory.\nMoreover, because OpenMP has a relaxed memory model, an OpenMP barrier or\nflush operation is required for the memory view of a thread to be communicated to\nother threads. A flush operation  guarantees that a value moves between two threads,\npreventing race conditions. An OpenMP barrier flushes all the locally modified values\nand synchronizes the threads. How this updating of the values is done is a complicated\noperation in the hardware and operating system. \n On a shared-memory, multi-core system, the modified values in cache must be\nflushed to the main memory and updated. Newer CPUs use specialized hardware to\ndetermine what actually changed, so the cache in dozens of cores only updates if nec-\nessary. But it is still an expensive operation and forces threads to stall while waiting for\nupdates. In many ways, it is a similar kind of operation to what you need to do when\nyou want to remove a thumb drive from your computer; you have to tell the operating\nsystem to flush all the thumb drive caches and then wait. Codes that use frequent bar-\nriers and flushes combined with smaller parallel regions often have excessive synchro-\nnization leading to poor performance.\n OpenMP addresses a single node, not multiple nodes with distributed memory\narchitectures. Thus, its memory scalability is limited to the memory on the node. ForStack\nStack\nStack\nStackTop of memory\nBottom of memoryEach thread has a\nseparate stack pointer\nand memory.\nHeap is shared\nbetween threads\nExecutable instructionsHeap (dynamic data)Grows upward\nStatic data\nLiteralsFigure 7.2 The threaded memory model helps \nwith understanding which variables are shared \nand which are private. Each thread, shown by the \nsquiggly lines, has its own instruction pointer, \nstack pointer, and stack memory but shares \nthe heap and static memory data.",8024
78-7.1.2 A simple OpenMP program.pdf,78-7.1.2 A simple OpenMP program,"211 OpenMP introduction\nparallel applications that have larger memory requirements, OpenMP needs to be\nused in conjunction with a distributed-memory parallel technique. We discuss the\nmost common of these, the MPI standard, in chapter 8.\n Table 7.1 shows some common OpenMP concepts, terminology, and directives. We\nwill demonstrate the use of these in the rest of the chapter.\n7.1.2 A simple OpenMP program\nNow we’ll show you how to apply each of the OpenMP concepts and directives. In this\nsection, you will learn how to create a region of code with multiple threads using\nthe OpenMP parallel pragma on a traditional “Hello World” problem distributed\namong threads. You will see how easy it is to use OpenMP and, potentially, to achieveTable 7.1 Roadmap of OpenMP topics in this chapter\nOpenMP topic OpenMP pragma Description\nParallel  regions  \n(see listing 7.2)#pragma  omp parallel  Spawns threads within the region \nfollowing this directive.\nLoop  work  sharing  \n(see listing 7.7)#pragma omp for  \nFor Fortran: \n#pragma  do forSplits work equally between threads. \nScheduling clauses include static , \ndynamic , guided , and auto .\nCombined  parallel  \nregion  and work  \nsharing  \n(see listing 7.7)#pragma  omp \nparallel  for Directives can also be combined for \nspecific calls within routines.\nReduction  \n(see section 7.3.5) #pragma  omp \nparallel  for reduction  \n(+: sum) , (min:  xmin) , or \n(max:  xmax)\nSynchronization  \n(see listing 7.15) #pragma  omp barrier  With multiple threads running, this call \ncreates a stopping point so that all the \nthreads can regroup before moving to \nthe next section.\nSerial  sections  \n(see listing 7.4 and 7.5 )#pragma  omp masked  \nexecutes on thread zero with no \nbarrier at the enda\n#pragma  omp single )\none thread with an implicit barrier \nat the end of the block\na  #pragma  omp masked  was #pragma  omp master . With the release of OpenMP standard v 5.1 in Nov. 2020, \nthe term “master” was changed to “masked” to address concerns that it is offensive to many in the technical community. \nWe are strong advocates of inclusion and, thus, use the new syntax throughout this chapter. Readers are warned that \ncompilers may take some time to implement the change. Note that the examples that accompany the chapter will use the \nolder syntax until most compilers are updated.This directive prevents multiple \nthreads from executing the code. \nUse this directive when you have a \nfunction within a parallel region that \nyou only want to run on one thread. \nLocks  #pragma  omp critical  or \natomic  For advanced implementations and \nused only in special cases\n212 CHAPTER  7OpenMP that performs\nperformance gains. There are several ways to control how many threads you have in\nthe parallel region. These are\nDefault —The default is usually the maximum number of threads for the node,\nbut it can be different, depending on the compiler and if MPI ranks exist.\nEnvironment variable —Set the size with the OMP_NUM_THREADS environ-\nment variable; for example\nexport OMP_NUM_THREADS=16\nFunction call —Call the OpenMP function omp_set_threads , for example\nomp_set_threads(16)\nPragma —For example, #pragma  omp parallel  num_threads(16)\nThe simple example in listings 7.1 through 7.6 shows how to get your thread ID and\nthe number of threads. Listing 7.1 shows our first attempt at writing a Hello World\nprogram.\nHelloOpenMP/HelloOpenMP.c\n 1 #include <stdio.h>\n 2 #include <omp.h>       \n 3\n 4 int main(int argc, char *argv[]){\n 5    int nthreads, thread_id;\n 6    nthreads = omp_get_num_threads();      \n 7    thread_id = omp_get_thread_num();      \n 8    printf(""Goodbye slow serial world and Hello OpenMP"");\n 9    printf(""I have %d thread(s) and my thread id is %d\n"",nthreads,thread_id);\n10 }\nTo compile with GCC\ngcc -fopenmp -o HelloOpenMP HelloOpenMP.c\nwhere -fopen  is the compiler flag to turn on OpenMP.\n Next, we’ll set the number of threads for the program to use by setting an environ-\nment variable. We could also use the function call omp_set_num_threads()  or just let\nOpenMP pick the number of threads based on the hardware that we are running on.\nTo set the number of threads, use this command to set the environment variable:\nexport OMP_NUM_THREADS=4\nNow, run your executable with ./HelloOpenMP, we get\nGoodbye slow serial world and Hello OpenMP!\n  I have 1 thread(s) and my thread id is 0Listing 7.1 A simple hello OpenMP program that prints Hello  OpenMP\nIncludes OpenMP header file for the \nOpenMP function calls (mandatory)\nFunction calls to get \nthe number of threads \nand the thread ID\n213 OpenMP introduction\nNot quite what we wanted; there is only one thread. We have to add a parallel region\nto get multiple threads. Listing 7.2 shows how to add the parallel region. \nNOTE In listings throughout the chapter, you’ll see the annotations >> Spawn\nthreads  >> and Implied  Barrier  Implied  Barrier.  These are visual cues to\nshow where threads are spawned and where barriers are inserted by the\ncompiler. In later listings, we'll use the same annotations for Explicit  Barrier\nExplicit  Barrier  where we have inserted a barrier directive.\nHelloOpenMP/HelloOpenMP_fix1.c\n 1 #include <stdio.h>\n 2 #include <omp.h>\n 3 \n 4 int main(int argc, char *argv[]){\n 5   int nthreads, thread_id;\n 6   #pragma omp parallel >> Spawn threads >>    \n 7   {\n 8      nthreads = omp_get_num_threads();\n 9      thread_id = omp_get_thread_num();\n10      printf(""Goodbye slow serial world and Hello OpenMP!\n"");\n11      printf(""  I have %d thread(s) and my thread id is \n%d\n"",nthreads,thread_id);\n12   } Implied Barrier      Implied Barrier\n13}\nWith these changes, we get the following output:\nGoodbye slow serial world and Hello OpenMP!\n  I have 4 thread(s) and my thread id is 3\nGoodbye slow serial world and Hello OpenMP!\n  I have 4 thread(s) and my thread id is 3\nGoodbye slow serial world and Hello OpenMP!\nGoodbye slow serial world and Hello OpenMP!\n  I have 4 thread(s) and my thread id is 3\n  I have 4 thread(s) and my thread id is 3\nAs you can see, all of the threads report that they are thread number 3. This is because\nnthreads  and thread_id  are shared variables. The value that is assigned at run time\nto these variables is the one written by the last thread to execute the instruction. This\nis a typical race condition as figure 7.3 illustrates. It is a common issue in threaded\nprograms of any type.\n Also note that the order of the printout is random, depending on the order of the\nwrites from each processor and how they get flushed to the standard output device. To\nget the right thread numbers, we define the thread_id  variable in the loop so that the\nscope of the variable becomes private to the thread as the following listing shows.\n \n Listing 7.2 Adding a parallel region to Hello OpenMP\nAdds the \nparallel region\n214 CHAPTER  7OpenMP that performs\nHelloOpenMP/HelloOpenMP_fix2.c\n 1 #include <stdio.h>\n 2 #include <omp.h>\n 3 \n 4 int main(int argc, char *argv[]){\n 5    #pragma omp parallel >> Spawn threads >>\n 6    {\n 7       int nthreads = omp_get_num_threads();    \n 8       int thread_id = omp_get_thread_num();    \n 9       printf(""Goodbye slow serial world and Hello OpenMP!\n"");\n10       printf(""  I have %d thread(s) and my thread id is \n%d\n"",nthreads,thread_id);\n11    } Implied Barrier      Implied Barrier\n12 }\nAnd we get\nGoodbye slow serial world and Hello OpenMP!\nGoodbye slow serial world and Hello OpenMP!\n  I have 4 thread(s) and my thread id is 2     \nGoodbye slow serial world and Hello OpenMP!\n  I have 4 thread(s) and my thread id is 3     \nGoodbye slow serial world and Hello OpenMP!\n  I have 4 thread(s) and my thread id is 0     \n  I have 4 thread(s) and my thread id is 1     \nSay we really didn’t want every thread printing out. Let’s minimize the output and put\nthe print statement in a single OpenMP clause as the following listing shows, so only\none thread writes output.\n \n Listing 7.3 Defining variables where these are used in Hello OpenMP\nThread\n0Thread\n1Thread\n2Thread\n3\nStack\nnthreads = omp_get_num_threads();\nthread_id = omp_get_thread_num();\n*Final valueHeap nthreads\nthread_id+1 cycle\n+2 cycle\n+3 cycle\n+4 cycle\n+5 cycle\n+6 cycle\n+7 cycle\n+8 cycle4\n4\n4*4\n0\n12\nTime\nFigure 7.3 Variables in the previous example are defined before the parallel region, \nthus these are shared variables in the heap. Each thread writes to these, and the final \nvalue is determined by which one writes last. The shading represents progression \nthrough time with writes at different clock cycles by various threads in a non-\ndeterministic fashion. This situation and similar situations are called race \nconditions  because the results can vary from run to run. \nDefinition of nthreads \nand thread_id moved \ninto the parallel region.\nNow we get a \ndifferent thread ID \nfor each thread.\n215 OpenMP introduction\nHelloOpenMP/HelloOpenMP_fix3.c\n 1 #include <stdio.h>\n 2 #include <omp.h>\n 3 \n 4 int main(int argc, char *argv[]){\n 5    #pragma omp parallel >> Spawn threads >>\n 6    {\n 7       int nthreads = omp_get_num_threads();     \n 8       int thread_id = omp_get_thread_num();     \n 9       #pragma omp single                                 \n10       {                                                  \n11          printf(""Number of threads is %d\n"",nthreads);   \n12          printf(""My thread id %d\n"",thread_id);          \n13       } Implied Barrier      Implied Barrier             \n14    } Implied Barrier      Implied Barrier\n15 }\nAnd the output is now:\nGoodbye slow serial world and Hello OpenMP!\n  I have 4 thread(s) and my thread id is 2\nThe thread ID is a different value on each run. Here, we really wanted the thread that\nprints out to be the first thread, so we change the OpenMP clause in the next listing to\nuse masked  instead of single .\nHelloOpenMP/HelloOpenMP_fix4.c\n 1 #include <stdio.h>\n 2 #include <omp.h>\n 3 \n 4 int main(int argc, char *argv[]){\n 5    #pragma omp parallel >> Spawn threads >>\n 6    {\n 7       int nthreads = omp_get_num_threads();\n 8       int thread_id = omp_get_thread_num();\n 9       #pragma omp masked                   \n10       {\n11          printf(""Goodbye slow serial world and Hello OpenMP!\n"");\n12          printf(""  I have %d thread(s) and my thread id is \n%d\n"",nthreads,thread_id);\n13       }\n14    } Implied Barrier      Implied Barrier\n15 }\nRunning this code now returns what we were first trying to do:\nGoodbye slow serial world and Hello OpenMP!\n  I have 4 thread(s) and my thread id is 0Listing 7.4 Adding a single pragma to print output for Hello OpenMP\nListing 7.5 Changing a single pragma to a masked  pragma in Hello OpenMPVariables defined in \na parallel region \nare private.\nPlaces output \nstatements into \nan OpenMP single \npragma block\nAdds directive \nto run only on \nmain thread\n216 CHAPTER  7OpenMP that performs\nWe can make this operation even more concise and use fewer pragmas as we show in\nlisting 7.6. The first print statement does not need to be in the parallel region. Also,\nwe can limit the second printout to thread zero by simply using a conditional on the\nthread number. The implied barrier is from the omp parallel  pragma.\nHelloOpenMP/HelloOpenMP_fix5.c\n 1 #include <stdio.h>\n 2 #include <omp.h>\n 3 \n 4 int main(int argc, char *argv[]){\n 5    printf(""Goodbye slow serial world and Hello OpenMP!\n"");     \n 6    #pragma omp parallel >> Spawn threads >>                    \n 7    if (omp_get_thread_num() == 0) {                           \n 8       printf(""  I have %d thread(s) and my thread id is %d\n"",\n             omp_get_num_threads(), omp_get_thread_num());\n 9    } \n10    Implied Barrier      Implied Barrier\n11 }\nWe have learned a few important things from this example: \nVariables that are defined outside a parallel region are by default shared  in the\nparallel region.\nWe should always strive to have the smallest program scope for a variable that is\nstill correct. By defining the variable in the loop, the compiler can better under-\nstand our intent and handle it correctly.\nUsing the masked  clause is more restrictive than the single  clause because it\nrequires thread 0 to execute the code block. The masked  clause also does not\nhave an implicit barrier at the end.\nWe need to watch out for possible race conditions between the operations of\ndifferent threads.\nOpenMP is continuously updating and releasing new versions. Before using an OpenMP\nimplementation, you should know the version and the features that are supported.\nOpenMP started with the ability to harness threads across a single node. New capabili-\nties, such as vectorization, and offloading tasks to accelerators, such as GPUs, have\nbeen added to the OpenMP standard. The following table shows some of the major\nfeatures added in the last decade.Listing 7.6 Reducing the number of pragmas in Hello OpenMP\nVersion 3.0 (2008) Introduction of task parallelism, and improvements to loop parallelism. These \nimprovements to loop parallelism include loop collapse and nested parallelism.\nVersion 3.1 (2011) Adds reduction min and max operators to C and C++ (other operators already in \nC and C++; min and max already in Fortran) and thread binding controlMoves print\nstatement out\nof parallel\nregion\nPragma applies to next statement or a \nscoping block delimited by curly braces.Replaces OpenMP \nmasked pragma \nwith conditional \nfor thread zero",13604
79-7.2 Typical OpenMP use cases Loop-level high-level and MPI plus OpenMP.pdf,79-7.2 Typical OpenMP use cases Loop-level high-level and MPI plus OpenMP,,0
80-7.2.2 High-level OpenMP for better parallel performance.pdf,80-7.2.2 High-level OpenMP for better parallel performance,"217 Typical OpenMP use cases: Loop-level, high-level, and MPI plus OpenMP\nOne thing to note is that to deal with the substantial changes in hardware that are\noccurring, the pace of changes to OpenMP has increased since 2011. While the\nchanges in version 3.0 and 3.1 dealt mostly with the standard CPU threading model,\nsince then the changes in versions 4.0, 4.5, and 5.0 have mostly dealt with other forms\nof hardware parallelism, such as accelerators and vectorization.\n7.2 Typical OpenMP use cases: Loop-level, high-level, \nand MPI plus OpenMP\nOpenMP has three specific use-case scenarios to meet the needs of three different\ntypes of users. The first decision you need to make is which scenario is appropriate for\nyour situation. The strategy and techniques vary for each of these cases: loop-level\nOpenMP, high-level OpenMP, and OpenMP to enhance MPI implementations. In the\nfollowing sections, we will elaborate on each of these, when to use them and why, and\nhow to use them. Figure 7.4 shows the recommended material to be carefully read for\neach of the use cases. \n7.2.1 Loop-level OpenMP for quick parallelization\nA standard use case for loop-level OpenMP is when your application only needs a\nmodest speedup and has plenty of memory resources. By this we mean that its require-\nments can be satisfied by the memory on a single hardware node. In this use case, it\nmight be sufficient to use loop-level OpenMP. The following list summarizes the appli-\ncation characteristics of loop-level OpenMP:\nModest parallelism\nHas plenty of memory resources (low memory requirements)\nExpensive part of calculation is in just a few for or do loopsVersion 4.0 (2013) Adds OpenMP SIMD (vectorization) directive, target directive for offloading to \nGPUs and other accelerator devices, and thread affinity control\nVersion 4.5 (2015) Substantial improvements to accelerator device support for GPUs\nVersion 5.0 (2018) Further improvements to accelerator device support\nLoop-level OpenMP High-level OpenMP MPI plus OpenMP\nSection 7.3 Loop-level OpenMP Section 7.3 Loop-level OpenMP\nChapter 8 Hybrid MPI plus OpenMP\nSection 7.4 Variable scope\nSection 7.5 Function-level OpenMP\nSection 7.6 High-level OpenMPSection 7.3 Loop-level OpenMP\nSection 7.4 Variable scope\nSection 7.5 Function-level OpenMP\nSection 7.6 High-level OpenMP\nFigure 7.4 The recommended reading for each of the scenarios depends on the use case for your \napplication.",2462
81-7.3.1 Loop level OpenMP Vector addition example.pdf,81-7.3.1 Loop level OpenMP Vector addition example,"218 CHAPTER  7OpenMP that performs\nWe use loop-level OpenMP in these cases because it takes little effort and can be done\nquickly. With separate parallel  for pragmas, the issue of thread race conditions is\nreduced. By placing OpenMP parallel  for pragmas or parallel  do directives before\nkey loops, the parallelism of the loop can be easily achieved. Even when the end goal\nis a more efficient implementation, this loop-level approach is often the first step\nwhen introducing thread parallelism to an application. \nNOTE If your use case requires only modest speedup, go to section 7.3 for\nexamples of this approach.\n7.2.2 High-level OpenMP for better parallel performance\nNext we discuss a different scenario, high-level OpenMP, where higher performance is\ndesired. Our high-level OpenMP design has a radical difference from the strategies\nfor standard loop-level OpenMP. Standard OpenMP starts from the bottom-up and\napplies the parallelism constructs at the loop level. Our high-level OpenMP approach\ntakes a whole system view to the design with a top-down approach that addresses the\nmemory system, the system kernel, and the hardware. The OpenMP language does\nnot change, but the method of its use does. The end result is that we eliminate many\nof the thread startup costs and the costs of synchronization that hobble the scalability\nof loop-level OpenMP.\n If you need to extract every last bit of performance out of your application, then\nhigh-level OpenMP is for you. Begin by learning loop-level OpenMP in section 7.3 as a\nstarting point for your application. Then you will need to gain a deeper understand-\ning of OpenMP variable scope from sections 7.4 and 7.5. Finally, dive into section 7.6\nfor a look at how the diametrically opposite approach of high-level OpenMP from the\nloop-level approach results in better performance. In that section, we’ll look at the\nimplementation model and a step-by-step method to reach the desired structure. This\nis followed by detailed examples of implementations for high-level OpenMP.\n7.2.3 MPI plus OpenMP for extreme scalability\nWe can also use OpenMP to supplement distributed memory parallelism (as dis-\ncussed in chapter 8). The basic idea of using OpenMP on a small subset of processes\nadds another level of parallel implementation that helps for extreme scaling. This\ncould be within the node, or better yet, the set of processors that uniformly share\nquick access to shared memory, commonly referred to as a Non-Uniform Memory\nAccess (NUMA) region. \n We first discussed NUMA regions in OpenMP concepts in section 7.1.1 as an addi-\ntional consideration for performance optimization. By using threading only within\none memory region where all memory accesses have the same cost, some of the com-\nplexity and performance traps of OpenMP are avoided. In a more modest hybrid\nimplementation, OpenMP can be used to harness the two-to-four hyperthreads for\neach processor. We’ll discuss this scenario, the hybrid MPI + OpenMP, in chapter 8\nafter describing the basics of MPI. \n219 Examples of standard loop-level OpenMP\n For the OpenMP skills needed for this hybrid approach with small thread counts,\nit is sufficient to learn the loop-level OpenMP techniques in section 7.3. Then move\nincrementally to a more efficient and scalable OpenMP implementation, which allows\nmore and more threads to replace MPI ranks. This requires at least some of the steps\non the path to high-level OpenMP as presented in section 7.6. Now that you know\nwhat sections are important for your application’s use case, let’s jump into the details\nof how to make each strategy work.\n7.3 Examples of standard loop-level OpenMP\nIn this section, we will look at examples of loop-level parallelization. The loop-level\nuse case was introduced in section 7.2.1; here we will show you the implementation\ndetails. Let’s begin.\n Parallel regions are initiated by inserting pragmas around blocks of code that can be\ndivided among independent threads (e.g., do loops, for loops). OpenMP relies on the\nOS kernel for its memory handling. This reliance for memory handling can often be an\nimportant factor that limits OpenMP from reaching its peak potential. We’ll look at why\nthis happens. Each variable within a parallel construct can be either shared or private.\nMoreover, OpenMP has a relaxed memory model. Each thread has a temporary view of\nmemory so that it doesn’t have the cost of storing memory with every operation. When\nthe temporary view finally must be reconciled with main memory, an OpenMP barrier\nor flush operation is required to synchronize memory. Each of these synchronizations\ncomes with a cost, due to the time that it takes to perform the flush, but also because it\nrequires fast threads to wait for slower ones to complete. An understanding of how\nOpenMP functions can reduce these performance bottlenecks.\n Performance is not the only concern for an OpenMP programmer. You should\nalso watch for correctness issues caused by thread race conditions. Threads might\nprogress at different speeds on the processors and, in combination with the relaxed\nmemory synchronization, serious errors can suddenly occur in even well-tested code.\nCareful programming and the use of specialized tools as discussed in section 7.9.2 is\nessential for robust OpenMP applications. \n In this section, we’ll take a look at a few loop-level OpenMP examples to get an\nidea of how it is used in practice. The source code that accompanies the chapter has\nmore variants of each example. We strongly encourage you to experiment with each\nof these on the architecture and compiler that you commonly work with. We ran each\nof the examples on a Skylake Gold 6152 dual socket system, as well as a 2017 Mac lap-\ntop. Threads are allocated by cores, and thread binding is enabled using the following\nOpenMP environment variables to reduce the performance variation of runs:\nexport OMP_PLACES=cores\nexport OMP_CPU_BIND=true\nWe’ll explore thread placement and binding more in chapter 14. For now, to help you\nget experience with loop-level OpenMP, we’ll present three different examples: vector\n220 CHAPTER  7OpenMP that performs\naddition, stream triad, and a stencil code. We’ll show the parallel speedup of the three\nexamples after the last example in section 7.3.4.\n7.3.1 Loop level OpenMP: Vector addition example\nIn the vector addition example (listing 7.7), you can see the interaction between the\nthree components: OpenMP work-sharing directives, implied variable scope, and mem-\nory placement by the operating system. These three components are necessary for\nOpenMP program correctness and performance.\nVecAdd/vecadd_opt1.c\n 1 #include <stdio.h>\n 2 #include <time.h>\n 3 #include <omp.h>\n 4 #include ""timer.h""\n 5\n 6 #define ARRAY_SIZE 80000000      \n 7 static double a[ARRAY_SIZE], b[ARRAY_SIZE], c[ARRAY_SIZE];\n 8\n 9 void vector_add(double *c, double *a, double *b, int n);\n10\n11 int main(int argc, char *argv[]){\n12    #pragma omp parallel >> Spawn threads >>\n13       if (omp_get_thread_num() == 0)\n14          printf(""Running with %d thread(s)\n"",omp_get_num_threads());\n      Implied Barrier      Implied Barrier\n15\n16    struct timespec tstart;\n17    double time_sum = 0.0;\n18    for (int i=0; i<ARRAY_SIZE; i++) {   \n19       a[i] = 1.0;                       \n20       b[i] = 2.0;                       \n21    }                                    \n22\n23    cpu_timer_start(&tstart);\n24    vector_add(c, a, b, ARRAY_SIZE);\n25    time_sum += cpu_timer_stop(tstart);\n26\n27    printf(""Runtime is %lf msecs\n"", time_sum);\n28 }\n29\n30 void vector_add(double *c, double *a, double *b, int n)\n31 {\n32    #pragma omp parallel for >> Spawn threads >>     \n33    for (int i=0; i < n; i++){                  \n34       c[i] = a[i] + b[i];                      \n35    }                                           \n      Implied Barrier      Implied Barrier\n36 }Listing 7.7 Vector add with a simple loop-level OpenMP pragma\nArray is large \nenough to force \ninto main memory.\nInitialization \nloop\nSingle-combined \nOpenMP parallel \nfor pragma\nVector \nadd loop\n221 Examples of standard loop-level OpenMP\nThis particular implementation style produces modest parallel performance on a sin-\ngle node. Take note, this implementation could be better. All the array memory is first\ntouched by the main thread during the initialization prior to the main loop as shown\non the left in figure 7.5. This can cause the memory to be located in a different mem-\nory region, where the memory access time is greater for some of the threads. \nNow, to improve the OpenMP performance, we insert pragmas in the initialization\nloops as listing 7.8 shows. The loops are distributed in the same static threading parti-\ntion, so the threads that touch the memory in the initialization loop will have the mem-\nory located near to them by the operating system (shown on the right side of figure 7.5).\nVecAdd/vecadd_opt2.c\n11 int main(int argc, char *argv[]){\n12    #pragma omp parallel >> Spawn threads >>\n13       if (omp_get_thread_num() == 0)\n14          printf(""Running with %d thread(s)\n"",omp_get_num_threads());\n      Implied Barrier      Implied Barrier\n15\n16    struct timespec tstart;\n17    double time_sum = 0.0;\n18    #pragma omp parallel for >> Spawn threads >>     \n19    for (int i=0; i<ARRAY_SIZE; i++) {      \n20       a[i] = 1.0;                          \n21       b[i] = 2.0;                          \n22    }                                       \n      Implied Barrier      Implied BarrierListing 7.8 Vector add with first touchFirst touch allocates on main thread a, bShared a b c n Private i\nFirst touch allocates close to thread a, bShared a b c n Private i\nThread 0 Thread 1 Thread 2 Thread 3\nSimple OpenMP loop ignores\nmemory allocation.\nMemory ends up on main thread.Distributing initialization loop mimics\nthread access for proper ﬁrst touch.\nMemory ends up close to threads.a,b c,n a, b, c, nThread 0 Thread 1 Thread 2 Thread 3\nFigure 7.5 Adding a single OpenMP pragma on the main vector add computation loop \n(on the left) results in the a and b arrays being touched first by the main thread; the \ndata is allocated near thread zero. The c array is first touched during the computation \nloop and, therefore, the memory for the c array is close to each thread. On the right, \nadding an OpenMP pragma on the initialization loop results in the memory for the a \nand b arrays being placed near the thread where the work is done.\nInitialization in a “parallel \nfor” pragma so first touch \ngets memory in the \nproper location\nInitializes the \na and b arrays",10729
82-7.3.2 Stream triad example.pdf,82-7.3.2 Stream triad example,"222 CHAPTER  7OpenMP that performs\n23\n24    cpu_timer_start(&tstart);\n25    vector_add(c, a, b, ARRAY_SIZE);\n26    time_sum += cpu_timer_stop(tstart);\n27\n28    printf(""Runtime is %lf msecs\n"", time_sum);\n29 }\n30\n31 void vector_add(double *c, double *a, double *b, int n)\n32 {\n33    #pragma omp parallel for  >> Spawn threads >>       \n34    for (int i=0; i < n; i++){    \n35       c[i] = a[i] + b[i];        \n36    }                             \n      Implied Barrier      Implied Barrier\n37 }\nThe threads in the second NUMA region no longer have a slower memory access\ntime. This improves the memory bandwidth for the threads in the second NUMA\nregion and also improves the load balance across the threads. First touch is an OS\npolicy that was mentioned earlier in section 7.1.1. Good first touch implementa-\ntions may often gain a 10 to 20% performance improvement. For evidence that this\nis the case, see table 7.2 in section 7.3.4 for the performance improvement on these\nexamples.\n If NUMA is enabled in the BIOS, the Skylake Gold 6152 CPU has a factor of about\ntwo decrease in performance when accessing remote memory. As with most tunable\nparameters, the configuration of individual systems can vary. To see your configura-\ntion, you can use the numactl  and numastat  commands for Linux. You may have to\ninstall the numactl-libs or numactl-devel packages for these commands. \n Figure 7.6 shows the output for the Skylake Gold test platform. The node distances\nlisted at the end of the output roughly capture the cost of accessing memory on a\nremote node. You can think of this as the relative number of hops to get to memory.\nHere the memory access cost is a little over a factor of two (21 versus 10). Note that\nsometimes two NUMA region systems are listed with a cost of 20 versus 10 as a default\nconfiguration instead of their real costs.\n The NUMA configuration information can tell you what is important to optimize.\nIf you only have one NUMA region, or the difference in memory access costs is small,\nyou may not need to worry as much about first touch optimizations. If the system is\nconfigured for interleaved memory accesses to the NUMA regions, optimizing for the\nfaster local memory accesses will not help. In the absence of specific information or\nwhen trying to optimize in general for larger HPC systems, you should use first touch\noptimizations to get local, faster memory accesses.\n7.3.2 Stream triad example\nThe following listing shows another similar example for the stream triad benchmark.\nThis example runs multiple iterations of the kernel to get an average performance. OpenMP for pragma \nto distribute work \nfor vector add loop \nacross threads\nVector \nadd loop\n223 Examples of standard loop-level OpenMP\nStreamTriad/stream_triad_opt2.c\n 1 #include <stdio.h>\n 2 #include <time.h>\n 3 #include <omp.h>\n 4 #include ""timer.h""\n 5\n 6 #define NTIMES 16\n 7 #define STREAM_ARRAY_SIZE 80000000     \n 8 static double a[STREAM_ARRAY_SIZE], b[STREAM_ARRAY_SIZE], \nc[STREAM_ARRAY_SIZE];\n 9\n10 int main(int argc, char *argv[]){\n11    #pragma omp parallel >> Spawn threads >>\n12       if (omp_get_thread_num() == 0)\n13          printf(""Running with %d thread(s)\n"",omp_get_num_threads());\n      Implied Barrier      Implied Barrier\n14\n15    struct timeval tstart;\n16    double scalar = 3.0, time_sum = 0.0;            \n17    #pragma omp parallel for >> Spawn threads >>\n18    for (int i=0; i<STREAM_ARRAY_SIZE; i++) {       \n19       a[i] = 1.0;                                  \n20       b[i] = 2.0;                                  \n21    }                                               \n      Implied Barrier      Implied BarrierListing 7.9 Loop-level OpenMP threading of the stream triad\nFigure 7.6 Output from the numactl  and numastat  commands. The distance between memory \nregions is highlighted. Note that the NUMA utilities use the term “node” differently than we have \ndefined it. In their terminology, each NUMA region is a node. We reserve the node terminology for a \nseparate distributed memory system such as another desktop or tray in a rack-mounted system.\nLarge enough to force \ninto main memory\nInitializes data \nand arrays",4239
83-7.3.3 Loop level OpenMP Stencil example.pdf,83-7.3.3 Loop level OpenMP Stencil example,"224 CHAPTER  7OpenMP that performs\n22\n23    for (int k=0; k<NTIMES; k++){\n24       cpu_timer_start(&tstart);\n25       #pragma omp parallel for >> Spawn threads >>\n26       for (int i=0; i<STREAM_ARRAY_SIZE; i++){     \n27          c[i] = a[i] + scalar*b[i];                \n28       }                                            \n         Implied Barrier      Implied Barrier\n29       time_sum += cpu_timer_stop(tstart);\n30       c[1]=c[2];      \n31    }\n32\n33    printf(""Average runtime is %lf msecs\n"", time_sum/NTIMES);\n34 }\nAgain, we just need one pragma to implement the OpenMP threaded computation at\nline 25. A second pragma inserted at line 17 further improves performance because of\nthe better memory placement obtained by a proper first touch technique.\n7.3.3 Loop level OpenMP: Stencil example\nThe third example of loop-level OpenMP is the stencil operation first introduced in\nchapter 1 (figure 1.10). This stencil operator adds the surrounding neighbors and\ntakes an average for the new value of the cell. Listing 7.10 has more complex memory\nread access patterns and, as we optimize the routine, it shows us the effect of threads\naccessing memory written by other threads. In this first loop-level OpenMP implemen-\ntation, each parallel  for block is synchronized by default, which prevents potential\nrace conditions. In later, more optimized versions of the stencil, we’ll add explicit syn-\nchronization directives.\nStencil/stencil_opt2.c\n 1 #include <stdio.h>\n 2 #include <stdlib.h>\n 3 #include <time.h>\n 4 #include <omp.h>\n 5\n 6 #include ""malloc2D.h""\n 7 #include ""timer.h""\n 8\n 9 #define SWAP_PTR(xnew,xold,xtmp) (xtmp=xnew, xnew=xold, xold=xtmp)\n10\n11 int main(int argc, char *argv[])\n12 {\n13    #pragma omp parallel >> Spawn threads >>\n14    #pragma omp masked\n15       printf(""Running with %d thread(s)\n"",omp_get_num_threads());\n      Implied Barrier      Implied Barrier\n16\n17    struct timeval tstart_init, tstart_flush, tstart_stencil, tstart_total;Listing 7.10 Loop-level OpenMP threading in the stencil example with first touchStream \ntriad loop\nKeeps the compiler from \noptimizing the loop\n225 Examples of standard loop-level OpenMP\n18    double init_time, flush_time, stencil_time, total_time;\n19    int imax=2002, jmax = 2002;\n20    double** xtmp;\n21    double** x = malloc2D(jmax, imax);\n22    double** xnew = malloc2D(jmax, imax);\n23    int *flush = (int *)malloc(jmax*imax*sizeof(int)*4);\n24\n25    cpu_timer_start(&tstart_total);\n26    cpu_timer_start(&tstart_init);\n27    #pragma omp parallel for >> Spawn threads >>     \n28    for (int j = 0; j < jmax; j++){\n29       for (int i = 0; i < imax; i++){\n30          xnew[j][i] = 0.0;\n31          x[j][i] = 5.0;\n32       }   \n33    } Implied Barrier      Implied Barrier \n34\n35    #pragma omp parallel for >> Spawn threads >>     \n36    for (int j = jmax/2 - 5; j < jmax/2 + 5; j++){\n37       for (int i = imax/2 - 5; i < imax/2 -1; i++){\n38          x[j][i] = 400.0;\n39       }   \n40    } Implied Barrier      Implied Barrier \n41    init_time += cpu_timer_stop(tstart_init);\n42\n43    for (int iter = 0; iter < 10000; iter++){\n44       cpu_timer_start(&tstart_flush);\n45       #pragma omp parallel for >> Spawn threads >>    \n46       for (int l = 1; l < jmax*imax*4; l++){\n47           flush[l] = 1.0;\n48       } Implied Barrier      Implied Barrier  \n49       flush_time += cpu_timer_stop(tstart_flush);\n50       cpu_timer_start(&tstart_stencil);\n51       #pragma omp parallel for >> Spawn threads >>    \n52       for (int j = 1; j < jmax-1; j++){\n53          for (int i = 1; i < imax-1; i++){\n54             xnew[j][i]=(x[j][i] + x[j][i-1] + x[j][i+1] + \n                                     x[j-1][i] + x[j+1][i])/5.0;\n55          }   \n56       } Implied Barrier      Implied Barrier  \n57       stencil_time += cpu_timer_stop(tstart_stencil);\n58 \n59       SWAP_PTR(xnew, x, xtmp);\n60       if (iter%1000 == 0) printf(""Iter %d\n"",iter);\n61    }   \n62    total_time += cpu_timer_stop(tstart_total);\n63 \n64    printf(""Timing: init %f flush %f stencil %f total %f\n"",\n65           init_time,flush_time,stencil_time,total_time);\n66\n67    free(x);\n68    free(xnew);\n69    free(flush);\n70 }Initializes with \nOpenMP pragma \nfor first-touch \nmemory allocation\nInserts parallel \nfor pragma to \nthread loop",4384
84-7.3.5 Reduction example of a global sum using OpenMP threading.pdf,84-7.3.5 Reduction example of a global sum using OpenMP threading,"226 CHAPTER  7OpenMP that performs\nFor this example, we inserted a flush loop at line 46 to empty the cache of the x and\nxnew  arrays. This is to mimic the performance where a code does not have the vari-\nables in cache from a prior operation. The case without data in cache is termed a cold\ncache , and when the data is in cache it is called a warm cache . Both cold and warm\ncaches are valid cases to analyze for different use-case scenarios. Simply, both cases are\npossible in a real application, and it may be difficult to even know which will happen\nwithout a deep analysis. \n7.3.4 Performance of loop-level examples\nLet’s review the performance of the earlier examples in this section. As seen in list-\nings 7.8, 7.9, and 7.10, introducing loop-level OpenMP requires few changes to the\nsource code. As table 7.2 demonstrates, the performance improvement is on the\norder of 10x faster. This is a pretty good performance return for the effort required.\nBut for a system with 88 threads, the achieved parallel efficiency is modest, at about\n19% as calculated below, giving us some room for improvement. To calculate the\nspeedup, we first take the serial run time divided by the parallel run time like this: \nStencil speedup = (serial run-time)/(parallel run-time) = 17.0 times faster\nIf we get perfect speedup on 88 threads, it would be 88. We take the actual speedup\nand divide by the ideal speedup of 88 to calculate the parallel efficiency:\nStencil parallel efficiency = (stencil speedup)/(ideal speedup) = 17 / 88 = 19%\nParallel efficiency is much better at smaller thread counts; at four threads, parallel\nefficiency is at 85%. The effect of getting memory allocated close to the thread is\nsmall, but significant. In the timings in table 7.2, the first optimization, simple loop-\nlevel OpenMP, has just OpenMP parallel  for pragmas on the computation loops.\nThe second optimization with first touch adds the OpenMP parallel  for pragmas for\nthe initialization loops. Table 7.2 summarizes the performance improvements for sim-\nple OpenMP with the addition of a first touch optimization. The timings used OMP_\nPLACES=cores  and OMP_CPU_BIND=true .\nProfiling the stencil application threaded with OpenMP, we observe that 10-15% of\nthe run time is consumed by OpenMP overhead, consisting of thread waits and threadTable 7.2 Shown are the run times in msecs. The speedup on a Skylake Gold 6152 dual socket node\nwith the GCC version 8.2 compiler is a factor of ten on 88 threads. Adding an OpenMP pragma on the\ninitialization to get proper first-touch memory allocation returns an additional speedup.\nSerial Simple loop-level OpenMP Adding first touch\nVector Add 0.253 0.0301 0.0175\nStream Triad 0.164 0.0203 0.0131\nStencil 62.56 3.686 3.311",2778
85-7.5 Function-level OpenMP Making a whole function thread parallel.pdf,85-7.5 Function-level OpenMP Making a whole function thread parallel,"227 Examples of standard loop-level OpenMP\nstartup costs. We can reduce the OpenMP overhead by adopting a high-level OpenMP\ndesign as we’ll discuss in section 7.6.\n7.3.5 Reduction example of a global sum using OpenMP threading\nAnother common type of loop is a reduction. Reductions are a common pattern in\nparallel programming that were introduced in section 5.7. Reductions  are any opera-\ntion that starts with an array and calculates a scalar result. In OpenMP, this can also be\nhandled easily in the loop-level pragma with the addition of a reduction  clause as the\nfollowing listing shows.\nGlobalSums/serial_sum_novec.c\n 1 double do_sum_novec(double* restrict var, long ncells)\n 2 {\n 3    double sum = 0.0;                        \n 4    #pragma omp parallel for reduction(+:sum)     \n 5    for (long i = 0; i < ncells; i++){       \n 6       sum += var[i];                        \n 7    }                                        \n 8 \n 9    return(sum);\n10 }\nThe reduction operation computes a local sum on each thread and then sums all the\nthreads together. The reduction variable, sum, is initialized to the appropriate value\nfor the operation. In the code in listing 7.11, the reduction variable is initialized to\nzero. The initialization of the sum variable to zero on line 3 is still needed for proper\noperation when we don’t use OpenMP.\n7.3.6 Potential loop-level OpenMP issues\nLoop-level OpenMP can be applied to most, but not all loops. The loop must have a\ncanonical form so that the OpenMP compiler can apply the work-sharing operation.\nThe canonical form is the traditional, straightforward loop implementation that is\nfirst learned by programmers. The requirements are that\nThe loop index variable must be an integer.\nThe loop index cannot be modified in the loop.\nThe loop must have standard exit conditions.\nThe loop iterations must be countable.\nThe loop must not have any loop-carried dependencies.\nYou can test the last requirement by reversing the order of the loop or by changing\nthe order of the loop operations. If the answer changes, the loop has loop-carried\ndependencies. There are similar restrictions on loop-carried dependencies for vec-\ntorization on the CPU and threading implementations on the GPU. The similarities\nof this loop-carried dependency requirement have been described as fine-grainedListing 7.11 Global sum with OpenMP threading\nGlobal sum\nreduction\ncodeOpenMP parallel \nfor loop with \nreduction clause\n228 CHAPTER  7OpenMP that performs\nparallelization versus the coarse-grained  structure used in a distributed-memory, mes-\nsage-passing approach. Here are some definitions:\nFine-grained parallelization —A type of parallelism where computational loops or\nother small blocks of code are operated on by multiple processors or threads\nand may need frequent synchronization.\nCoarse-grained parallelization —A type of parallelism where the processor operates\non large blocks of code with infrequent synchronization.\nMany programming languages have proposed a modified loop-type that tells the com-\npiler that loop-level parallelism is allowed to be applied in some form. For now, sup-\nplying a pragma or directive before the loop supplies this information. \n7.4 Variable scope importance for correctness in OpenMP\nTo convert an application or routine to high-level OpenMP, you need to understand\nvariable scope. The OpenMP specifications are vague on many scoping details. Fig-\nure 7.7 shows the scoping rules for compilers. Generally, a variable on the stack is con-\nsidered private, and those that are placed in the heap are shared (figure 7.2). For\nhigh-level OpenMP, the most important case is how to manage scope in a called rou-\ntine in a parallel region.\nWhen determining the scope of variables, you should put more focus on variables on\nthe left-hand side of an expression. The scope for variables that are being written to is\nmore important to get correct. Note that private variables are undefined at entry and\nafter the exit of a parallel region as listing 7.12 shows. The firstprivate  and last-\nprivate  clauses can modify this behavior in special cases. If a variable is private, we\nshould see it set before it is used in the parallel block and not used after the parallel\nregion. If a variable is intended to be private, it is best to declare the variable withinScoping rules\nShared Private Reduction\nReduction clauseSpecification\nParallel\nconstruct\nFortran\nroutine\nC routine\nParallel regionArguments\nAlways All loop indices will be privateInherited from calling environmentFile scope variables,\nextern static , orAll local variables or\ndeclared .threadprivate\nAlso dynamically\nallocatedAutomatic variables\nwithin parallel construct\nor in a orprivate\nfirstprivate clause\nsave attribute, initialized\nvariables, common block,\nmodule variablesVariables declared outside\nparallel construct or in a\nshared clause\nFigure 7.7 Summary of thread scoping rules for OpenMP applications\n229 Function-level OpenMP: Making a whole function thread parallel\nthe loop because a locally declared variable has exactly the same behavior as a private\nOpenMP variable. Long story short, declaring the variable within the loop eliminates\nany confusion on what the behavior should be. It does not exist before the loop or\nafterward, so incorrect uses are not possible.\n1    double x;      \n2    #pragma omp parallel for private(x) >> Spawn threads >>     \n3    for (int i=0; i < n; i++){\n4       x = 1.0                                 \n5       double y = x*2.0;                    \n6    } Implied Barrier      Implied Barrier\n7\n8    double z = x;     \nOn the directive in line 4 in listing 7.11, we added a reduction clause to note the spe-\ncial treatment needed for the sum variable. On line 2 of listing 7.12, we showed the\nprivate  directive. There are other clauses that can be used on the parallel directive\nand other program blocks, for example:\nshared(var,var)\nprivate(var,var)\nfirstprivate(var,var)\nlastprivate(var,  var)\nreduction([+,min,max]:<var,var>)\n*threadprivate  (a special directive used in a thread-parallel function)\nWe highly recommend using tools such as Intel® Inspector and Allinea/ARM MAP to\ndevelop more efficient code and to implement high-level OpenMP. We discuss some\nof these tools in section 7.9. Becoming familiar with a variety of essential tools is neces-\nsary before beginning the implementation of high-level OpenMP. After running your\napplication through these tools, a better understanding of the application allows for a\nsmoother transition to the implementation of high-level OpenMP.\n7.5 Function-level OpenMP: Making a whole function \nthread parallel\nWe will introduce the concept of high-level OpenMP in section 7.6. But before we\nattempt high-level OpenMP, it is necessary to see how the loop-level implementations\ncan be expanded to cover larger sections of code. The purpose for expanding the\nloop-level implementation is to lower the overhead and increase parallel efficiency.\nWhen expanding the parallel region, it eventually covers an entire subroutine. Once\nwe convert the whole function into an OpenMP parallel region, OpenMP provides\nfar less control over the thread scope of the variables. The clauses for a parallelListing 7.12 Private variable entering the OpenMP parallel block \nVariable declared outside \nparallel for block.Private clause \non parallel for \nblock\nX will not be defined, \nso it must be set first.\nDeclared private \nvariable within loop \nis better style.X is not \ndefined here.\n230 CHAPTER  7OpenMP that performs\nregion no longer help as there is no place to add scoping clauses. So how do we con-\ntrol variable scope?\n While the defaults for variable scope in functions usually work well, there are\ncases where they don’t. The only OpenMP pragma control for functions is the\nthreadprivate  directive that makes a declared variable private. Most variables in a\nfunction are on the stack and are already private. If there is an array dynamically allo-\ncated in the routine, the pointer it is assigned to is a local variable on the stack, which\nmeans it is private and different for every thread. We want this array to be shared, but\nthere is no directive for that. Using the specific compiler scoping rules from figure 7.7,\nwe add a save  attribute to the pointer declaration in Fortran, making the compiler\nput the variable in the heap and, thus, sharing the variable among the threads. In C,\nthe variable can be declared static or made file scope. The following listing shows\nsome examples of the thread scope of variables for Fortran, and listing 7.14 shows\nexamples for C and C++.\n 4 subroutine function_level_OpenMP(n, y)\n 5    integer :: n\n 6    real :: y(n)        \n 7\n 8    real, allocatable :: x(:)      \n 9    real x1                          \n10    real :: x2 = 0.0              \n11    real, save :: x3                       \n12    real, save, allocatable :: z          \n13 \n14    if (thread_id .eq. 0) allocate(x(100))  \n15\n16 !  lots of code                                                                                                                      \n17                                                                                                                                      \n1 8     i f  ( t h r e a d _ i d  . e q .  0 )  d e a l l o c a t e ( x )                                                                                                \n19 end subroutine function_level_OpenMP\nThe pointer for array y on line 6 is the scope of the variable at the location of the sub-\nroutine. In this case, it is in a parallel region, making it private. Both the pointer for x\nand the variable x1 are private. The scope of variable x2 on line 10 is more compli-\ncated. It is shared in Fortran 90 and private in Fortran 77. Initialized variables in For-\ntran 90 are on the heap and are only initialized (to zero in this case) on their first\noccurrence! The variables x3 and z on lines 11 and 12 are shared because these are in\nthe heap. The memory allocated for x on line 14 is on the heap and shared, but the\npointer is private, which results in memory only accessible on thread zero.\n 5 void function_level_OpenMP(int n, double *y)      \n 6 {Listing 7.13 Function-level variable scope in Fortran\nListing 7.14 Function-level variable scope in C/C++Pointer for array y and its array \nelements that are private Pointer for allocatable \narray x that is private\nVariable x 1 is on the \nstack, so it is private.\nVariable x2 is shared \nin Fortran 90.\nVariable x3 is placed on \nthe heap, so it is shared.\nPointer for z array is on \nthe heap and is shared.\nThe x array memory is shared, but the pointer to x is private.\nThe pointer to \narray y is private.",10882
86-7.6.1 How to implement high-level OpenMP.pdf,86-7.6.1 How to implement high-level OpenMP,"231 Improving parallel scalability with high-level OpenMP\n 7    double *x;                   \n 8    static double *x1;             \n 9\n10    int thread_id;\n11 #pragma omp parallel\n12    thread_id = omp_get_thread_num();\n13 \n14    if (thread_id == 0) x = (double *)malloc(100*sizeof(double));       \n15    if (thread_id == 0) x1 = (double *)malloc(100*sizeof(double));    \n16 \n17 // lots of code\n18    if (thread_id ==0) free(x); \n19    if (thread_id ==0) free(x1); \n20 }\nThe pointer to array y in the argument list on line 5 is on the stack. It has the scope of\nthe variable at the calling location. In a parallel region, the pointer to y is private. The\nmemory for the x array is on the heap and shared, but the pointer is private, so the\nmemory is only accessible from thread zero. Memory for the x1 array is on the heap\nand shared, and the pointer is shared so the memory is accessible and shared across\nall the threads.\n You need to be always on guard for unexpected effects of variable declarations and\ndefinitions that impact the thread scope. For example, initializing a local variable with\na value in a Fortran 90 subroutine automatically gives the variable the save  attribute\nand the variable is now shared.1 We recommend explicitly adding the save  attribute to\nthe declaration to avoid any issues or confusion.\n7.6 Improving parallel scalability with high-level OpenMP\nWhy use high-level OpenMP? The central high-level OpenMP strategy is to improve\non standard loop-level parallelism by minimizing fork/join overhead and memory\nlatency. Reduction of thread wait times is often seen as another major motivating fac-\ntor of high-level OpenMP implementations. By explicitly dividing the work among\nthe threads, threads are no longer implicitly waiting on other threads and can there-\nfore go on to the next part of the calculation. This allows explicit control of the synchro-\nnization point. In figure 7.8, unlike the typical fork-join model of standard OpenMP,\nhigh-level OpenMP keeps the threads dormant but alive, thus reducing overhead\ntremendously. \n In this section, we’ll review the explicit steps needed to implement high-level\nOpenMP. Then we’ll show you how to go from a loop-level implementation to a high-\nlevel implementation.\n1This is not the case under the Fortran 77 standard! But even with Fortran 77, some compilers such as the DEC\nFortran compiler mandate that every variable in a routine have the save  attribute, causing obscure bugs and\nportability problems. Knowing this, we could make sure we are compiling with the Fortran 90 standard and\npotentially fix the private scoping issue by initializing the array pointer, which causes it to be moved to the\nheap, making the variable shared.The pointer to \narray x is private.\nThe pointer to \narray x 1 is shared.\nMemory for the x\narray is shared.Memory for the x 1\narray is shared.\n232 CHAPTER  7OpenMP that performs\n7.6.1 How to implement high-level OpenMP\nImplementation of high-level OpenMP is often more time-consuming because it\nrequires the use of advanced tools and extensive testing. Implementing high-level\nOpenMP can also be difficult as it is more prone to race conditions than the stan-\ndard loop-level implementation. Additionally, it is often not apparent how to get\nfrom the starting point (loop-level implementation) to the ending point (high-level\nimplementation). \n The common use for the more tedious high-level open implementation would be\nwhen you want more efficiency and want to get rid of thread spawning and synchroni-\nzation costs. For more information on high-level OpenMP, see section 7.11. You can\nimplement efficient high-level OpenMP by having a good understanding of the\nmemory bounds of all loops in your application, enabling the use of profiling tools,\nand methodically working through the following steps. We suggest and show an imple-\nmentation strategy that is incremental, methodical, and can provide a successful, smooth\ntransition to a high-level OpenMP implementation. Steps to a high-level OpenMP imple-\nmentation include\nBase implementation —Implement loop-level OpenMP\nStep 1: Reduce thread start up —Merge the parallel regions and join all the loop-\nlevel parallel constructs into larger parallel regions\nStep 2: Synchronization —Add nowait  clauses to for loops, where synchronization\nis not needed, and calculate and manually partition the loops across the\nthreads, which allows for removal of barriers and required synchronization.\nStep 3: Optimize —Make arrays and variables private to each thread when possible.\nStep 4: Code correctness —Check thoroughly for race conditions (after every step).\nFigures 7.9 and 7.10 show the pseudocode corresponding to the previous four steps,\nstarting with a typical loop-level implementation using omp parallel  do pragmas and\ntransitioning to more efficient high-level parallelism.\n In our steps to a high-level OpenMP implementation, the thread start-up time is\nreduced in the first step of high-level OpenMP. The entire code is placed in a singleMemorySubroutine\nSerial\nsectionResultsDo i = ltb, utb\nDo i = ltb, utb\nDo i = ltb, utb\nDo i = ltb, utbSubroutine\nDo i = ltb, utb\nDo i = ltb, utb\nDo i = ltb, utb\nDo i = ltb, utbThread 0\nThread 1\nThread 2\nThread 3Thread 0ltb\nutb\nDriverltb\nutb\nltb\nutb\nltb\nutbThread 1\nThread 2\nThread 3Dominantthreads\nFigure 7.8 Visualization of high-level OpenMP threading. Threads are spawned once and left dormant when not \nneeded. Thread bounds are specified manually and synchronization is minimized.\n233 Improving parallel scalability with high-level OpenMP\nparallel region in order to minimize the overhead of forking and joining. In high-level\nOpenMP, threads are generated by the parallel  directive once, at the beginning of the\nexecution of the program. Unused threads do not die but remain dormant when run-\nning through a serial portion. To guarantee this, the serial portion is executed by the\nFigure 7.9 High-level OpenMP starts with a loop-level OpenMP implementation and merges parallel regions \ntogether to reduce the cost of thread spawning. We use the animal images to represent where the changes are \nmade and the relative speed of the actual implementation. The conventional loop level OpenMP shown with the \nturtle is faster, but there is overhead with each parallel  do that limits speedup. The dog represents the relative \ngain in speed from merging parallel regions. \nFigure 7.10 The next steps for high-level OpenMP add nowait  clauses to do or for loops, which reduce \nsynchronization costs. Then we calculate the loop bounds ourselves and explicitly use these in the loops to avoid \neven more synchronization. Here, the cheetah and the hawk identify the changes made in both implementations. \nThe hawk (on the right) is faster than the cheetah (on the left) as the overhead of the OpenMP is reduced.",6943
87-7.6.2 Example of implementing high-level OpenMP.pdf,87-7.6.2 Example of implementing high-level OpenMP,"234 CHAPTER  7OpenMP that performs\nmain thread, enabling few to no changes in the serial portion of the code. Once the pro-\ngram finishes running through the serial portion or starts a parallel region again, the\nsame threads forked at the beginning of the program are invoked or reused.\n Step 2 addresses the synchronization added to every for loop in OpenMP by\ndefault. The easiest way to reduce synchronization cost is to add nowait  clauses to all\nloops where it is possible, while maintaining correctness. A further step is to explicitly\ndivide the work among threads. The typical code for explicitly dividing the work for C\nis shown here. (The Fortran equivalent accounting for arrays starting at 1 is shown in\nfigure 7.10.)\ntbegin = N *  threadID   /nthreads\ntend   = N * (threadID+1)/nthreads\nThe impact of the manual partitioning of the arrays is that it reduces cache thrashing\nand race conditions by not allowing threads to share the same space in memory. \n Step 3, optimization, means that we explicitly state whether certain variables are\nshared or private. By giving the threads a specific space in memory, the compiler (and\nprogrammer) can forgo guessing about the state of the variables. This can be done by\napplying the variable scoping rules from figure 7.7. Furthermore, compilers cannot\nproperly parallelize loops that include complex loop-carried dependencies and loops\nthat are not in canonical form. High-level OpenMP helps the compiler by being more\nexplicit about the thread scoping of variables, thus allowing complex loops to be par-\nallelized. This leads into the last part of this step for the high-level OpenMP approach.\nArrays will be partitioned across the threads. Explicit partitioning of the arrays guaran-\ntees that a thread only touches memory assigned to it and allows us to start fixing\nmemory locality issues.\n And in the last step, code correctness,  it is important to use the tools listed in\nsection 7.9 to detect and fix race conditions. In the next section, we will show you\nthe process of implementing the steps we described. The programs found in the\nGitHub source for this chapter will prove to be useful in following along with the\nstepwise process.\n7.6.2 Example of implementing high-level OpenMP\nYou can complete a full implementation of high-level OpenMP in a series of steps. You\nshould first look at where the bottleneck(s) of the code are in your application, in\naddition to finding the most compute-intensive loop in the code. You can then find\nthe innermost level loop of the code and add the standard loop-based OpenMP direc-\ntives. The scoping of the variables in the most intensive loops and inner loops needs\nto be understood, referring to figure 7.7 for guidance.\n In step 1, you should focus on reducing the thread start-up costs. This is done in\nlisting 7.15 by merging parallel regions to include the entire iteration loop in a single\nparallel region. We start slowly moving the OpenMP directives outward, expanding\nthe parallel region. The original OpenMP pragmas on lines 49 and 57 can be merged\n235 Improving parallel scalability with high-level OpenMP\ninto one parallel region between lines 44 to 70. The extent of the parallel region is\ndefined by the curly braces on lines 45 and 70 thus only starting the parallel region\nonce instead of 10,000 times.\nHighLevelOpenMP_stencil/stencil_opt4.c\n44 #pragma omp parallel >> Spawn threads >>     \n45 {\n46    int thread_id = omp_get_thread_num();\n47    for (int iter = 0; iter < 10000; iter++){\n48       if (thread_id ==0) cpu_timer_start(&tstart_flush);\n49       #pragma omp for nowait                  \n50       for (int l = 1; l < jmax*imax*4; l++){\n51          flush[l] = 1.0;\n52       }\n53       if (thread_id == 0){\n54          flush_time += cpu_timer_stop(tstart_flush);\n55          cpu_timer_start(&tstart_stencil);\n56       }\n57       #pragma omp for >> Spawn threads >>         \n58       for (int j = 1; j < jmax-1; j++){\n59          for (int i = 1; i < imax-1; i++){\n60             xnew[j][i]=(x[j][i] + x[j][i-1] + x[j][i+1] + x[j-1][i] + \nx[j+1][i])/5.0;\n61          }\n62       } Implied Barrier      Implied Barrier\n63       if (thread_id == 0){\n64          stencil_time += cpu_timer_stop(tstart_stencil);\n65 \n66          SWAP_PTR(xnew, x, xtmp);\n67          if (iter%1000 == 0) printf(""Iter %d\n"",iter);\n68       }\n69    }\n70 } // end omp parallel\n   Implied Barrier      Implied Barrier\nPortions of the code that are required to be run in serial are placed in control of the\nmain thread, allowing for the parallel region to be expanded across large portions of\nthe code that encompass both serial and parallel regions. With each step, use the tools\ndiscussed in section 7.9 to make sure that the application still runs correctly.\n In the second part of the implementation, you begin the transition to high-level\nOpenMP by carrying the main OpenMP parallel loop to the beginning of the pro-\ngram. After that, you can move on to calculating upper and lower loop bounds. List-\ning 7.16 (and the online examples in stencil_opt5.c and stencil_opt6.c) shows how\nyou calculate the upper and lower bounds specific to the parallel region. Remember,\narrays start at different points depending on language: Fortran starts at 1 and C starts\nat 0. Loops with the same upper and lower bound can use the same thread without\nhaving to recalculate the bounds. Listing 7.15 Merging parallel regions into a single parallel region\nSingle OpenMP \nparallel region\nOpenMP for pragma \nwith no synchronization \nbarrier at end of loop\nSecond OpenMP \nfor pragma\n236 CHAPTER  7OpenMP that performs\nNOTE You must be careful to insert barriers in required locations to prevent\nrace conditions. Much care also needs to be taken when placing these prag-\nmas as too many could become detrimental to the overall performance of the\napplication.\nHighLevelOpenMP_stencil/stencil_opt6.c\n29 #pragma omp parallel >> Spawn threads >>\n30 {\n31    int thread_id = omp_get_thread_num();\n32    int nthreads = omp_get_num_threads();\n33\n34    int jltb = 1 + (jmax-2) * ( thread_id     ) / nthreads;     \n35    int jutb = 1 + (jmax-2) * ( thread_id + 1 ) / nthreads;     \n36\n37    int ifltb = (jmax*imax*4) * ( thread_id     ) / nthreads;   \n38    int ifutb = (jmax*imax*4) * ( thread_id + 1 ) / nthreads;   \n39\n40    int jltb0 = jltb;                                           \n41    if (thread_id == 0) jltb0--;                                \n42    int jutb0 = jutb;                                           \n43    if (thread_id == nthreads-1) jutb0++;                       \n44\n45    int kmin = MAX(jmax/2-5,jltb);                              \n46    int kmax = MIN(jmax/2+5,jutb);                              \n47\n48    if (thread_id == 0) cpu_timer_start(&tstart_init);          \n49    for (int j = jltb0; j < jutb0; j++){      \n50       for (int i = 0; i < imax; i++){\n51          xnew[j][i] = 0.0;\n52          x[j][i] = 5.0;\n53       }\n54    }\n55\n56    for (int j = kmin; j < kmax; j++){        \n57       for (int i = imax/2 - 5; i < imax/2 -1; i++){\n58          x[j][i] = 400.0;\n59       }\n60    }\n61    #pragma omp barrier      \n      Explicit Barrier      Explicit Barrier\n62    if (thread_id == 0) init_time += cpu_timer_stop(tstart_init);\n63\n64    for (int iter = 0; iter < 10000; iter++){\n65       if (thread_id == 0) cpu_timer_start(&tstart_flush);      \n66       for (int l = ifltb; l < ifutb; l++){\n67          flush[l] = 1.0;\n68       }\n69       if (thread_id == 0){                                     \n70          flush_time += cpu_timer_stop(tstart_flush);           \n71          cpu_timer_start(&tstart_stencil);                     \n72       }                                                        \n73       for (int j = jltb; j < jutb; j++){     Listing 7.16 Precalculating loop lower and upper bounds\nComputes \nloop \nbounds\nUses thread \nID instead \nof OpenMP \nmasked pragma \nto eliminate \nsynchronizationUses\nmanually\ncalculated\nloop boundsBarrier to synchronize \nwith other threads",8146
88-7.7 Hybrid threading and vectorization with OpenMP.pdf,88-7.7 Hybrid threading and vectorization with OpenMP,"237 Hybrid threading and vectorization with OpenMP\n74          for (int i = 1; i < imax-1; i++){\n75             xnew[j][i]=( x[j][i] + x[j][i-1] + x[j][i+1] + x[j-1][i] + \nx[j+1][i] )/5.0;\n76          }\n77       }\n78       #pragma omp barrier    \n         Explicit Barrier      Explicit Barrier\n79       if (thread_id == 0){                                 \n80          stencil_time += cpu_timer_stop(tstart_stencil);   \n81\n82          SWAP_PTR(xnew, x, xtmp);                          \n83          if (iter%1000 == 0) printf(""Iter %d\n"",iter);     \n84       }                                                    \n85       #pragma omp barrier    \n         Explicit Barrier      Explicit Barrier\n86    }\n87 } // end omp parallel\n    Implied Barrier      Implied Barrier\nTo obtain a correct answer, it is crucial to start from the innermost loop and have an\nunderstanding of which variables need to stay private or become shared among the\nthreads. As you start enlarging the parallel region, serial portions of the code will be\nplaced into a masked region. This region has one thread that does all the work, while\nthe other threads remain alive but dormant. Zero or only a few changes are required\nwhen placing serial portions of the code into a main thread. Once the program fin-\nishes running through the serial region or gets into a parallel region, the past dor-\nmant threads start working again to parallelize the current loop.\n For the final step, comparing results for steps along the way to a high-level OpenMP\nimplementation, in listings 7.14 and 7.15 and the provided online stencil examples,\nyou can see that the number of pragmas is greatly reduced while also yielding better\nperformance (figure 7.11).\n7.7 Hybrid threading and vectorization with OpenMP\nIn this section, we will combine topics from chapter 6 with what you have learned in\nthis chapter. This combination yields to better parallelization and utilizes the vector\nprocessor. The OpenMP threaded loop can be combined with the vectorized loop byBarrier to\nsynchronize\nwith other\nthreadsUses thread \nID instead \nof OpenMP \nmasked pragma \nto eliminate \nsynchronization\nFigure 7.11 Optimizing the OpenMP pragmas both reduces the number of pragmas required and \nimproves the performance of the stencil kernel.\n238 CHAPTER  7OpenMP that performs\nadding the simd  clause to the parallel  for as in #pragma  omp parallel  for simd . The\nfollowing listing shows this for the stream triad.\nStreamTriad/stream_triad_opt3.c\n 1 #include <stdio.h>\n 2 #include <time.h>\n 3 #include <omp.h>\n 4 #include ""timer.h""\n 5 \n 6 #define NTIMES 16\n 7 #define STREAM_ARRAY_SIZE 80000000      \n 8 static double a[STREAM_ARRAY_SIZE], b[STREAM_ARRAY_SIZE], \nc[STREAM_ARRAY_SIZE];\n 9\n10 int main(int argc, char *argv[]){\n11    #pragma omp parallel >> Spawn threads >>\n12       if (omp_get_thread_num() == 0)\n13          printf(""Running with %d thread(s)\n"",omp_get_num_threads());\n       Implied Barrier      Implied Barrier\n14 \n15    struct timeval tstart;\n16    double scalar = 3.0, time_sum = 0.0;               \n17    #pragma omp parallel for simd >> Spawn threads >>\n18    for (int i=0; i<STREAM_ARRAY_SIZE; i++) {          \n19       a[i] = 1.0;                                     \n20       b[i] = 2.0;                                     \n21    }                                                  \n      Implied Barrier      Implied Barrier\n22    for (int k=0; k<NTIMES; k++){\n23       cpu_timer_start(&tstart);\n24       #pragma omp parallel for simd >> Spawn threads >>\n25       for (int i=0; i<STREAM_ARRAY_SIZE; i++){          \n26          c[i] = a[i] + scalar*b[i];                     \n27       }                                                 \n         Implied Barrier      Implied Barrier\n28       time_sum += cpu_timer_stop(tstart);\n29       c[1]=c[2];      \n30    }\n31\n32    printf(""Average runtime is %lf msecs\n"", time_sum/NTIMES);\n33}\nThe hybrid implementation of the stencil example with both threading and vectoriza-\ntion puts the for pragma on the outer loop and the simd  pragma on the inner loop as\nthe following listing shows. Both the threaded and the vectorized loops work best with\nloops over large arrays as would usually be the case for the stencil example.\nHybridOpenMP_stencil/stencil_hybrid.c\n26 #pragma omp parallel >> Spawn threads >>\n27 {Listing 7.17 Loop-level OpenMP threading and vectorization of the stream triad\nListing 7.18 Stencil example with both threading and vectorizationLarge enough to \nforce into main \nmemory\nInitializes \ndata and \narrays\nStream \ntriad loop\nKeeps the compiler from \noptimizing out the loop\n239 Hybrid threading and vectorization with OpenMP\n28    int thread_id = omp_get_thread_num();\n29    if (thread_id == 0) cpu_timer_start(&tstart_init);\n30    #pragma omp for\n31    for (int j = 0; j < jmax; j++){\n32       #ifdef OMP_SIMD\n33       #pragma omp simd       \n34       #endif\n35       for (int i = 0; i < imax; i++){\n36          xnew[j][i] = 0.0;\n37          x[j][i] = 5.0;\n38       }\n39    } Implied Barrier      Implied Barrier\n40\n41    #pragma omp for\n42    for (int j = jmax/2 - 5; j < jmax/2 + 5; j++){\n43       for (int i = imax/2 - 5; i < imax/2 -1; i++){\n44          x[j][i] = 400.0;\n45       }\n46    } Implied Barrier      Implied Barrier\n47    if (thread_id == 0) init_time += cpu_timer_stop(tstart_init);\n48\n49    for (int iter = 0; iter < 10000; iter++){\n50       if (thread_id ==0) cpu_timer_start(&tstart_flush);\n51       #ifdef OMP_SIMD\n52       #pragma omp for simd nowait     \n53       #else\n54       #pragma omp for nowait\n55       #endif\n56       for (int l = 1; l < jmax*imax*10; l++){\n57          flush[l] = 1.0;\n58       }\n59       if (thread_id == 0){\n60          flush_time += cpu_timer_stop(tstart_flush);\n61          cpu_timer_start(&tstart_stencil);\n62       }\n63       #pragma omp for\n64       for (int j = 1; j < jmax-1; j++){\n65          #ifdef OMP_SIMD\n66          #pragma omp simd                                      \n67          #endif\n68          for (int i = 1; i < imax-1; i++){\n69             xnew[j][i]=(x[j][i] + x[j][i-1] + x[j][i+1] + \n                                     x[j-1][i] + x[j+1][i])/5.0;\n70          }\n71       } Implied Barrier      Implied Barrier\n72       if (thread_id == 0){\n73          stencil_time += cpu_timer_stop(tstart_stencil);\n74\n75          SWAP_PTR(xnew, x, xtmp);\n76          if (iter%1000 == 0) printf(""Iter %d\n"",iter);\n77       }\n78       #pragma omp barrier\n79    }\n80 } // end omp parallel\n   Implied Barrier      Implied Barrier Adds\nOpenMP\nSIMD\npragma\nfor inner\nloopsAdds additional OpenMP \nSIMD pragma to for \npragma on single loop",6783
89-7.8 Advanced examples using OpenMP.pdf,89-7.8 Advanced examples using OpenMP,,0
90-7.8.1 Stencil example with a separate pass for the x and y directions.pdf,90-7.8.1 Stencil example with a separate pass for the x and y directions,"240 CHAPTER  7OpenMP that performs\nFor the GCC compiler, the results with and without vectorization show a significant\nspeedup with vectorization: \n4 threads, GCC 8.2 compiler, Skylake Gold 6152\nThreads only:      Timing init 0.006630 flush 17.110755 stencil 17.374676 \ntotal 34.499799\nThreads & vectors: Timing init 0.004374 flush 17.498293 stencil 13.943251 \ntotal 31.454906\n7.8 Advanced examples using OpenMP\nThe examples shown so far have been simple loops over a set of data with relatively\nfew complications. In this section, we show you how to handle three advanced exam-\nples that require more effort:\nSplit-direction, two-step stencil —Advanced handling for thread scoping of variables\nKahan summation —A more complex reduction loop\nPrefix scan —Explicitly handling partitioning work among threads\nThe examples in this section reveal the various ways to handle more difficult situations\nand give you a deeper understanding of OpenMP.\n7.8.1 Stencil example with a separate pass for the x and y directions\nHere we will look at the potential difficulties that arise when implementing OpenMP\nfor a split-direction, two-step stencil operator where a separate pass is made for each\nspatial direction. Stencils  are building-blocks for numerical scientific applications and\nused to calculate dynamic solutions to partial differential equations. \n In a two-step stencil, where values are calculated on the faces, data arrays have dif-\nferent data-sharing requirements. Figure 7.12 represents such a stencil with 2D-face\ndata arrays. Furthermore, it is common that one of the dimensions of these 2D arrays\nneeds to be shared among all the threads or processes. The x-face data is simpler to\ndeal with because it is aligned with the thread data decomposition, but we don’t need\nthe full x-face array on every thread. The y-face data has a different problem because the\ndata is across threads, necessitating sharing of the y-face 2D array. High-level OpenMP\nallows for a quick privatization of the dimension needed. Figure 7.12 shows how cer-\ntain dimensions of a matrix can be kept either private, shared, or both.\n The first touch principle of most kernels (defined in section 7.1.1) says that mem-\nory will most likely be local to the thread (except at the edges between threads on\npage boundaries). We can improve the memory locality by making the array sections\ncompletely private to the thread where possible, such as the x-face data. Due to the\nincreasing number of processors, increasing the data locality is essential in minimiz-\ning the increasing speed gap between processors and memory. The following listing\nshows a serial implementation to begin with.\n \n \n241 Advanced examples using OpenMP\nSplitStencil/SplitStencil.c\n 58 void SplitStencil(double **a, int imax, int jmax)\n 59 {\n 60    double** xface = malloc2D(jmax, imax);    \n 61    double** yface = malloc2D(jmax, imax);    \n 62    for (int j = 1; j < jmax-1; j++){               \n 63       for (int i = 0; i < imax-1; i++){            \n 64          xface[j][i] = (a[j][i+1]+a[j][i])/2.0;    \n 65       }                                            \n 66    }                                               \n 67    for (int j = 0; j < jmax-1; j++){                 \n 68       for (int i = 1; i < imax-1; i++){              \n 69          yface[j][i] = (a[j+1][i]+a[j][i])/2.0;      \n 70       }                                              \n 71    }                                                 Listing 7.19 Split-direction stencil operatorXface variable memory distribution\nStencil operation\nThread 0Yface variable memory distribution\nStencil operation\nThread 1Thread 1 Thread 0\nStack\nStack\nExecutable instructionsHeap is shared\nbetween threads.Top of memory\nBottom of memoryHeap\nStatic data\nLiteralsGrows upwardXface[0][0]Xface[0][0]Xface[2][6]\nXface[2][6]\nYface[0][0]Yface[6][5]double **xface;\ndouble **xface;\ndouble **yface;xface mem xface mem\nyface memvoid stencil_op(void){\ndouble **xface = malloc(18*sizeof(double));\n... operations on xface ...\n}\n}\n... operations on yface ...\n}Each thread has a\nseparate stack pointer\nand memory.\nvoid stencil_op(void){\nif (thread_id == 0){\nstatic double **yface = malloc(18*sizeof(double));\nFigure 7.12 The x face of a stencil aligned with the threads needs private storage for each thread. The \npointer should be on the stack, and each thread should have a different pointer. The y face needs to share \nthe data, so we define one pointer in the static data region where both threads can access it.\nCalculates values \non x and y faces \nof cells\nx-face calculation \nrequires only adjacent \ncells in the x direction.\ny-face calculation \nrequires adjacent cells \nin the y direction.\n242 CHAPTER  7OpenMP that performs\n 72    for (int j = 1; j < jmax-1; j++){                         \n 73       for (int i = 1; i < imax-1; i++){                      \n 74          a[j][i] = (a[j][i]+xface[j][i]+xface[j][i-1]+\n 75                             yface[j][i]+yface[j-1][i])/5.0;  \n 76       }                                                      \n 77    }                                                         \n 78    free(xface);\n 79    free(yface);\n 80 }\nWhen using OpenMP with the stencil operator, you must determine whether the mem-\nory for each thread needs to be private or shared. In listing 7.18 (previously), the\nmemory for the x-direction can be all private, allowing for faster calculations. In\nthey-direction (figure 7.12), the stencil requires access to the adjacent thread’s data;\ntherefore, this data must be shared among the threads. This leads us to the imple-\nmentation shown in the following listing.\nSplitStencil/SplitStencil_opt1.c\n 86 void SplitStencil(double **a, int imax, int jmax)\n 87 {\n 88    int thread_id = omp_get_thread_num();\n 89    int nthreads = omp_get_num_threads();\n 90 \n 91    int jltb = 1 + (jmax-2) * ( thread_id     ) / nthreads;    \n 92    int jutb = 1 + (jmax-2) * ( thread_id + 1 ) / nthreads;    \n 93 \n 94    int jfltb = jltb;              \n 95    int jfutb = jutb;              \n 96    if (thread_id == 0) jfltb--;   \n 97 \n 98    double** xface = (double **)malloc2D(jutb-jltb, imax-1);   \n 99    static double** yface;   \n100    if (thread_id == 0) yface = (double **)malloc2D(jmax+2, imax);   \n101 #pragma omp barrier                     \n       Explicit Barrier      Explicit Barrier\n102    for (int j = jltb; j < jutb; j++){                  \n103       for (int i = 0; i < imax-1; i++){                \n104          xface[j-jltb][i] = (a[j][i+1]+a[j][i])/2.0;   \n105       }                                                \n106    }                                                   \n107    for (int j = jfltb; j < jfutb; j++){           \n108       for (int i = 1; i < imax-1; i++){           \n109          yface[j][i] = (a[j+1][i]+a[j][i])/2.0;   \n110       }                                           \n111    }                                              \n112 #pragma omp barrier                        \n       Explicit Barrier      Explicit BarrierListing 7.20 Split-direction stencil operator with OpenMPAdds in \ncontributions \nfrom all the \nfaces of the cell\nManually \ncalculates \ndistribution of \ndata across \nthreads\nThe y faces have \none less data value \nto distribute.Allocates a \nprivate portion \nof the x-face \ndata for each \nthreadDeclares the\ny-face data\npointer as\nstatic so it\nhas shared\nscope.\nAllocates one version of the\ny-face array to be shared\nacross threadsInserts an OpenMP barrier \nso that all threads have \nthe allocated memoryDoes the local \nx-face calculation \non each thread\nThe y-face calculation \nhas a j+1 and thus \nneeds a shared array.\nWe need an OpenMP synchronization\nbecause the next loop uses an\nadjacent thread work.\n243 Advanced examples using OpenMP\n113    for (int j = jltb; j < jutb; j++){                              \n114       for (int i = 1; i < imax-1; i++){                            \n115          a[j][i] = (a[j][i]+xface[j-jltb][i]+xface[j-jltb][i-1]+   \n116                             yface[j][i]+yface[j-1][i])/5.0;        \n117       }                                                            \n118    }                                                               \n119    free(xface);                             \n120 #pragma omp barrier           \n       Explicit Barrier      Explicit Barrier\n121    if (thread_id == 0) free(yface);     \n122 }\nTo define the memory on the stack as shown in the x-direction, we need a pointer to a\npointer to a double ( double  **xface ) so that the pointer is on the stack and private to\neach thread. Then we allocate the memory using a custom 2D malloc  call at line 98 in\nlisting 7.20. We only need enough memory for each thread, so we compute the thread\nbounds in lines 91 and 92 and use these in the 2D malloc  call. The memory is allo-\ncated from the heap and can be shared, but each thread only has its own pointer;\ntherefore, each thread can’t access the other threads’ memory.\n Rather than allocating memory from the heap, we could have used the automatic\nallocation, such as double  xface[3][6] , where the memory is automatically allocated\non the stack. The compiler automatically sees this declaration and pushes the memory\nspace onto the stack. In cases where the arrays are large, the compiler might move the\nmemory requirement to the heap. Each compiler has a different threshold on deciding\nwhether to place memory on the heap or on the stack. If the compiler moves the mem-\nory to the heap, only one thread has the pointer to this location. In effect, it is private,\neven though it is in shared memory space. \n For the y-faces, we define a static pointer to a pointer ( static  double  **yface ),\nwhere all threads can access the same pointer. In this case, only one thread needs to\ndo this memory allocation, and all remaining threads can access this pointer and\nthe memory itself. For this example, you can use figure 7.7 to see the different\noptions of making the memory shared. In this case, you would go to the Parallel\nRegion -> C Routine and pick one of the file scope variables, extern  or static , to\nmake the pointer shared among the threads. It is easy to get something wrong such\nas in the variable scope, the memory allocation, or the synchronization. For exam-\nple, what happens if we just define a regular double  **yfaces  pointer. Now each\nthread has its own private pointer and only one of these gets memory allocated. The\npointer for the second thread would not point to anything, generating an error\nwhen it is used.\n Figure 7.13 shows the performance for running the threaded version of the code\non the Skylake Gold processor. For a small number of threads, we get a super-linear\nspeedup before falling off at above eight threads. Super-linear speedup happens on\noccasion because the cache performance improves as the data is partitioned across\nthreads or processors.Combines the \nwork from the \nprevious x-face \nand y-face \nloops into a \nnew cell value Frees local x-face \narray for each thread\nA barrier ensures all \nthreads are done with \nthe shared y-face array.\nFrees the y-face array\non only one processor",11322
91-7.8.2 Kahan summation implementation with OpenMP threading.pdf,91-7.8.2 Kahan summation implementation with OpenMP threading,"244 CHAPTER  7OpenMP that performs\nDEFINITION Super-linear speedup  is performance that’s better than the ideal scal-\ning curve for strong scaling. This can happen because the smaller array sizes fit\ninto a higher level of the cache, resulting in better cache performance.\n7.8.2 Kahan summation implementation with OpenMP threading\nFor the enhanced-precision Kahan summation algorithm, introduced in section 5.7,\nwe cannot use a pragma to get the compiler to generate a multi-threaded implementa-\ntion because of the loop-carried dependencies. Therefore, we’ll follow a similar algo-\nrithm as we used in the vectorized implementation in section 6.3.4. We first sum up\nthe values on each thread in the first phase of the calculation. Then we sum the values\nacross the threads to get the final sum as the following listing shows.\nGlobalSums/kahan_sum.c\n 1 #include <stdlib.h>\n 2 #include <omp.h>\n 3 \n 4 double do_kahan_sum(double* restrict var, long ncells)\n 5 {\n 6    struct esum_type{\n 7       double sum;\n 8       double correction;\n 9    };\n10 \n11    int nthreads = 1;      \n12    int thread_id   = 0;   \n13 #ifdef _OPENMP\n14    nthreads = omp_get_num_threads();\n15    thread_id = omp_get_thread_num();\n16 #endif\n17 Listing 7.21 An OpenMP implementation of the Kahan summation16\n14\n1012\n8\n6\n4\n2Stencil speedup\n2 4 6 8 10 12 14 16\nNumber of threadsIdeal speedup\nStencil speedup\nFigure 7.13 The threaded \nversion of the split stencil has \na super-linear speedup for two \nto eight threads.\nGets the total number of \nthreads and thread_id\n245 Advanced examples using OpenMP\n18    struct esum_type local;\n19    local.sum = 0.0;\n20    local.correction = 0.0;\n21 \n22    int tbegin = ncells * ( thread_id     ) / nthreads;    \n23    int tend   = ncells * ( thread_id + 1 ) / nthreads;    \n24 \n25    for (long i = tbegin; i < tend; i++) {\n26       double corrected_next_term = var[i] + local.correction;\n27       double new_sum             = local.sum + local.correction;\n28       local.correction   = corrected_next_term - (new_sum - local.sum);\n29       local.sum          = new_sum;\n30    }\n31 \n32    static struct esum_type *thread;     \n33    static double sum;                   \n34 \n35 #ifdef _OPENMP         \n36 #pragma omp masked\n37    thread = malloc(nthreads*sizeof(struct esum_type));   \n38 #pragma omp barrier\n      Explicit Barrier      Explicit Barrier\n39 \n40    thread[thread_id].sum = local.sum;                 \n41    thread[thread_id].correction = local.correction;   \n42 \n43 #pragma omp barrier                      \n      Explicit Barrier      Explicit Barrier\n44 \n45    static struct esum_type global;\n46 #pragma omp masked                 \n47    {\n48       global.sum = 0.0;\n49       global.correction = 0.0;\n50       for ( int i = 0 ; i < nthreads ; i ++ ) {\n51          double corrected_next_term = thread[i].sum + \n52                 thread[i].correction + global.correction;\n53          double new_sum    = global.sum + global.correction;\n54          global.correction = corrected_next_term - \n                              (new_sum - global.sum);\n55          global.sum = new_sum;\n56    }\n57 \n58       sum = global.sum + global.correction;\n59       free(thread);\n60    } // end omp masked\n61 #pragma omp barrier\n      Explicit Barrier      Explicit Barrier\n62 #else\n63    sum = local.sum + local.correction;\n64 #endif\n65 \n66    return(sum);\n67 }Computes the range \nfor which this thread \nis responsible\nPuts the variables \nin shared memory\nDefines the compiler variable \n_OPENMP when using OpenMPAllocates one \nthread in shared \nmemory\nStores the summation \nof each thread in array\nWaits until all threads \nget here and then sums \nacross threads\nUses a single thread to \ncompute the beginning \noffset for each thread",3851
92-7.9.1 Using AllineaARM MAP to get a quick high-level profile of your application.pdf,92-7.9.1 Using AllineaARM MAP to get a quick high-level profile of your application,"246 CHAPTER  7OpenMP that performs\n7.8.3 Threaded implementation of the prefix scan algorithm\nIn this section, we look at the threaded implementation of the prefix scan operation.\nThe prefix scan operation, introduced in section 5.6, is important for algorithms with\nirregular data. This is because a count to determine the starting location for ranks or\nthreads allows the rest of the calculation to be done in parallel. As discussed in that\nsection, the prefix scan can also be done in parallel, yielding another parallelization\nbenefit. The implementation process has three phases:\nAll threads —Calculates a prefix scan for each thread’s portion of the data\nSingle thread —Calculates the starting offset for each thread’s data\nAll threads —Applies the new thread offset across all the data for each thread\nThe implementation in listing 7.22 works for a serial application and when called\nfrom within an OpenMP parallel region. This has the benefit that you can use the\ncode in the listing for both serial and threaded cases, reducing the code duplication\nfor this operation.\nPrefixScan/PrefixScan.c\n 1 void PrefixScan (int *input, int *output, int length)\n 2 {  \n 3    int nthreads = 1;                    \n 4    int thread_id   = 0;                 \n 5 #ifdef _OPENMP\n 6    nthreads = omp_get_num_threads();    \n 7    thread_id = omp_get_thread_num();    \n 8 #endif\n 9   \n10    int tbegin = length * ( thread_id     ) / nthreads;   \n11    int tend   = length * ( thread_id + 1 ) / nthreads;   \n12   \n13    if ( tbegin < tend ) {      \n14        output[tbegin] = 0;                             \n15        for ( int i = tbegin + 1 ; i < tend ; i++ ) {   \n16           output[i] = output[i-1] + input[i-1];        \n17        }\n18    }\n19    if (nthreads == 1) return;   \n20   \n21 #ifdef _OPENMP\n22 #pragma omp barrier                    \n      Explicit Barrier      Explicit Barrier\n23   \n24    if (thread_id == 0) {                          \n25       for ( int i = 1 ; i < nthreads ; i ++ ) {\n26          int ibegin = length * ( i - 1 ) / nthreads;\n27          int iend   = length * ( i     ) / nthreads;\n28      \n29          if ( ibegin < iend ) \n30             output[iend] = output[ibegin] + input[iend-1];Listing 7.22 An OpenMP implementation of the prefix scan \nGets the total number \nof threads and \nthread_id\nComputes the range \nfor which this thread \nis responsibleOnly\nperforms\nthis\noperation\nif there is\na positive\nnumber\nof entries.Does an \nexclusive scan \nfor each thread\nFor multiple threads only, do the \nadjustment to prefix scan for the \nbeginning value for each thread\nWaits\nuntil all\nthreads\nget here Uses the main \nthread to compute \nthe beginning offset \nfor each thread\n247 Threading tools essential for robust implementations\n31      \n32          if ( ibegin < iend - 1 )\n33             output[iend] += output[iend-1];\n34       }\n35    }\n36 #pragma omp barrier                       \n      Explicit Barrier      Explicit Barrier\n37\n38 #pragma omp simd                             \n39    for ( int i = tbegin + 1 ; i < tend ; i++ ) {    \n40       output[i] += output[tbegin];                  \n41    }                                                \n42 #endif\n43}\nThis algorithm should theoretically scale as\nParallel_timer = 2 * serial_time/nthreads\nThe performance on the Skylake Gold 6152 architecture peaks at about 44 threads,\n9.4 times faster than the serial version.\n7.9 Threading tools essential for robust implementations\nDeveloping a robust OpenMP implementation is difficult without using specialized\ntools for detecting thread race conditions and performance bottlenecks. The use of\ntools becomes much more important as you try to get a higher performance OpenMP\nimplementation. There are both commercial and openly available tools. The typical\ntool list when integrating advanced implementations of OpenMP in your application\nincludes:\nValgrind —A memory tool introduced in section 2.1.3. It also works with OpenMP\nand helps in finding uninitialized memory or out-of-bounds accesses in threads.\nCall graph —The cachegrind tool produces a call graph and a profile of your\napplication. A call graph determines which functions call other functions to\nclearly show the call hierarchy and code path. An example of the cachegrind\ntool was presented in section 3.3.1.\nAllinea/ARM Map —A high-level profiler to get an overall cost of thread starts\nand barriers (for OpenMP apps).\nIntel® Inspector —To detect thread race conditions (for OpenMP apps). \nWe described the first two tools in earlier chapters; they can be referred to there. In\nthis section, we will discuss the last two tools as these relate more to an OpenMP\napplication. These tools are needed to profile the bottlenecks and understand where\nthey lie within your application and, thus, are essential in knowing where to best\nstart changing your code in an efficient manner. Ends calculation \non main thread \nwith barrier\nStarts all \nthreads again\nApplies the offset to the \nrange for this thread",5111
93-7.9.2 Finding your thread race conditions with Intel Inspector.pdf,93-7.9.2 Finding your thread race conditions with Intel Inspector,"248 CHAPTER  7OpenMP that performs\n7.9.1 Using Allinea/ARM MAP to get a quick high-level profile \nof your application\nOne of the better tools to get a high-level application profile is Allinea/ARM MAP.\nFigure 7.14 shows a simplified view of its interface. For an OpenMP application, it\nshows the cost for thread starts and waits, highlights the application’s bottlenecks, and\nshows the usage of memory CPU floating point utilization. The profiler makes it easy\nto compare the gains made before and after code changes. Allinea/ARM MAP excels\nat producing a quick, high-level view of your application, but there are many other\nprofilers that can be used. Some of these are reviewed in section 17.3.\n7.9.2 Finding your thread race conditions with Intel® Inspector\nIt is essential to find and eliminate thread race conditions in an OpenMP implementa-\ntion to produce a robust, production-quality application. For this purpose, tools are\nessential because it is impossible for even the best programmer to catch all the thread\nrace conditions. As the application begins to scale, memory errors occur more fre-\nquently and can cause an application to break. Catching these memory errors early on\nsaves time and energy on future runs.\nFigure 7.14 These are results from Allinea/ARM MAP showing the majority of the compute time on the \nhighlighted line of code. We often use indicators like this to show us the location of bottlenecks.\n249 Threading tools essential for robust implementations\n There are not many tools that are effective at finding thread race conditions. We\nshow the use of one of these tools, the Intel® Inspector, to detect and pinpoint the\nlocation of these race conditions. Having tools to understand thread race conditions\nin memory is also useful when scaling to larger thread counts. Figure 7.15 provides a\nsample screenshot of Intel® Inspector.\nBefore changes in the initial application are made, it is critical to complete regression\ntesting. Ensuring correctness is crucial to the successful implementation of OpenMP\nthreading. A correct OpenMP code cannot be implemented unless an application or a\nwhole subroutine is in its proper working state. This also requires that the section of\ncode that is being threaded with OpenMP must also be exercised in a regression test.\nWithout being able to do regression testing, it becomes difficult to make steady prog-\nress. In summary, these tools, along with regression testing, create a better understand-\ning of the dependencies, efficiency, and correctness in most applications.\nFigure 7.15 Intel® Inspector report showing detection of thread race conditions. Here the items listed as Data \nrace under the Type heading on the panel to the upper left show all the places where there is currently a race \ncondition.",2812
94-7.11 Further explorations.pdf,94-7.11 Further explorations,"250 CHAPTER  7OpenMP that performs\n7.10 Example of a task-based support algorithm\nThe task-based parallel strategy was first introduced in chapter 1 and illustrated in fig-\nure 1.25. Using a task-based approach, you can divide work into separate tasks that can\nthen be parceled out to individual processes. Many algorithms are more naturally\nexpressed in terms of a task-based approach. OpenMP has supported this type of\napproach since its version 3.0. In the subsequent standard releases, there have been\nfurther improvements to the task-based model. In this section we’ll show you a simple\ntask-based algorithm to illustrate the techniques in OpenMP.\n One of the approaches to a reproducible global sum is to sum up the values in a\npairwise manner. The normal array approach requires the allocation of a working\narray and some complicated indexing logic. Using a task-based approach as in fig-\nure 7.16 avoids the need for a working array by recursively splitting the data in half in\nthe downward sweep, until an array length of 1 is reached, and then summing up the\npairs in the upward sweep.\nListing 7.23 shows the code for the task-based approach. The spawning of the task\nneeds to be done in a parallel region but by only one thread, leading to the nested\nblocks of pragmas in lines 8 to 14.\nPairwiseSumByTask/PairwiseSumByTask.c\n 1 #include <omp.h>\n 2 \n 3 double PairwiseSumBySubtask(double* restrict var, long nstart, long nend);\n 4 \n 5 double PairwiseSumByTask(double* restrict var, long ncells)\n 6 {\n 7    double sum;\n 8    #pragma omp parallel >> Spawn threads >>     \n 9    {Listing 7.23 A pair-wise summation using OpenMP tasks0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 150 1 2 3 4 5 6 7 8 9 10 11 12 13 14 150 1 2 3 4 5 6 7 8 9 10 11 12 13 14 150 1 2 3 4 5 6 7 8 9 10 11 12 13 14 150 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15\n+++++++++++++++\nFigure 7.16 The task-based implementation recursively splits the array into half on \nthe downward sweep. Once an array size of 1 occurs, the task sums pairs of data in \nthe upward sweep.\nLaunches \nparallel region",2087
95-7.11.2 Exercises.pdf,95-7.11.2 Exercises,"251 Further explorations\n10       #pragma omp masked                               \n11       {\n12          sum = PairwiseSumBySubtask(var, 0, ncells);   \n13       }\n14    } Implied Barrier      Implied Barrier\n15    return(sum);\n16 }\n17 \n18 double PairwiseSumBySubtask(double* restrict var, long nstart, long nend)\n19 {\n20    long nsize = nend - nstart;\n21    long nmid = nsize/2;      \n22    double x,y;\n23    if (nsize == 1){          \n24       return(var[nstart]);   \n25    }\n26 \n27    #pragma omp task shared(x) mergeable final(nsize > 10)   \n28    x = PairwiseSumBySubtask(var, nstart, nstart + nmid);    \n29    #pragma omp task shared(y) mergeable final(nsize > 10)   \n30    y = PairwiseSumBySubtask(var, nend - nmid, nend);        \n31    #pragma omp taskwait       \n32 \n33    return(x+y);    \n34 }\nGetting good performance with a task-based algorithm takes a lot more tuning to pre-\nvent too many threads from being spawned and to keep granularity of the tasks rea-\nsonable. For some algorithms, task-based algorithms are a much more appropriate\nparallel strategy.\n7.11 Further explorations\nThere are many materials on traditional thread-based OpenMP programming. With\nnearly every compiler supporting OpenMP, the best learning approach is to simply\nstart adding OpenMP directives to your code. There are many training opportunities\ncovering OpenMP, including the annual Supercomputing Conference held in Novem-\nber. For information, see https:/ /sc21.supercomputing.org/ . For those who are even\nmore interested in OpenMP, there is an International Workshop on OpenMP held\nevery year that covers the latest developments. For information, see http:/ /www\n.iwomp.org/ .\n7.11.1 Additional reading\nBarbara Chapman is one of the leading writers and authorities on OpenMP. Her\nbook is the standard reference for OpenMP programming, especially for the thread-\ning implementation in OpenMP as of 2008:\nBarbara Chapman, Gabriele Jost, and Ruud Van Der Pas, Using OpenMP: portable\nshared memory parallel programming , vol. 10 (MIT Press, 2008).Starts main task \non one thread\nSubdivides the array \ninto two parts\nInitializes sum at leaf with \nsingle value from array\nLaunches a pair of \nsubtasks with half of \nthe data for each\nWaits for \ntwo tasks to \ncomplete Sums the values from the \ntwo subtasks and returns \nto the calling thread",2390
96-Summary.pdf,96-Summary,"252 CHAPTER  7OpenMP that performs\nThere are many researchers working on developing more efficient techniques of\nimplementing OpenMP, which has come to be called high-level OpenMP . Here is a link\nto slides going into more detail on high-level OpenMP:\nYuliana Zamora, “Effective OpenMP Implementations on Intel’s Knights Landing,”\nLos Alamos National Laboratory Technical Report LA-UR-16-26774, 2016. Avail-\nable at: https:/ /www.osti.gov/biblio/1565920-effective-openmp-implementations-in\ntel-knights-landing.\nA good textbook on OpenMP and MPI is one written by Peter Pacheco. It has some\ngood examples of OpenMP code:\nPeter Pacheco, An introduction to parallel programming  (Elsevier, 2011).\nBlaise Barney at Lawrence Livermore National Laboratory has authored a well-written\nOpenMP reference that’s also available online:\nBlaise Barney, OpenMP Tutorial , https:/ /computing.llnl.gov/tutorials/openMP/\nThe OpenMP Architecture Review Board (ARB) maintains a website that is the authori-\ntative location for all things OpenMP, from specifications to presentations and tutorials:\nOpenMP Architecture Review Board, OpenMP , https:/ /www.openmp.org.\nFor a deeper discussion on the difficulties with threading:\nEdward A Lee, “The problem with threads.” Computer  39, no. 5 (2006): 33-42.\n7.11.2 Exercises\n1Convert the vector add example in listing 7.8 into a high-level OpenMP follow-\ning the steps in section 7.2.2.\n2Write a routine to get the maximum value in an array. Add an OpenMP pragma\nto add thread parallelism to the routine.\n3Write a high-level OpenMP version of the reduction in the previous exercise.\nWe covered a substantial amount of material in this chapter. This solid foundation will\nhelp you in developing an effective OpenMP application. \nSummary\nLoop-level implementations of OpenMP can be quick and easy to create.\nAn efficient implementation of OpenMP can achieve promising application\nspeed-up.\nGood first-touch implementations can often gain a 10–20% performance\nimprovement.\nUnderstanding variable scope across threads is important in getting OpenMP\ncode to work.\n253 Summary\nHigh-level OpenMP can boost performance on current and upcoming many-\ncore architectures.\nThreading and debugging tools are essential when implementing more com-\nplex versions of OpenMP.\nSome of the style guidelines that are suggested in this chapter include\n– Declaring variables where these are used so that they automatically become\nprivate, which is generally correct.\n– Modifying declarations to get the right threading scope for variables rather\nthan using an extensive list in private and public clauses.\n– Avoiding the critical  clause or other locking constructs where possible.\nPerformance is generally impacted heavily by these constructs.\n– Reducing synchronization by adding nowait  clauses to for loops and limit-\ning the use of #pragma  omp barrier  to only where necessary.\n– Merging small parallel regions into fewer, larger parallel regions to reduce\nOpenMP overhead.",3039
97-8.1.1 Basic MPI function calls for every MPI program.pdf,97-8.1.1 Basic MPI function calls for every MPI program,"254MPI: The parallel\nbackbone\nThe importance of the Message Passing Interface (MPI) standard is that it allows a\nprogram to access additional compute nodes and, thus, run larger and larger prob-\nlems by adding more nodes to the simulation. The name message passing  refers to\nthe ability to easily send messages from one process to another. MPI is ubiquitous\nin the field of high-performance computing. Across many scientific fields, the use\nof supercomputers entails an MPI implementation. \n MPI was launched as an open standard in 1994 and, within months, became the\ndominant parallel computing library-based language. Since 1994, the use of MPI\nhas led to scientific breakthroughs from physics to machine learning to self-drivingThis chapter covers\nSending messages from one process to another\nPerforming common communication patterns with \ncollective MPI calls\nLinking meshes on separate processes with \ncommunication exchanges\nCreating custom MPI data types and using MPI \nCartesian topology functions\nWriting applications with hybrid MPI plus OpenMP \n255 The basics for an MPI program\ncars! Several implementations of MPI are now in widespread use. MPICH from Argonne\nNational Laboratories and OpenMPI are two of the most common. Hardware vendors\noften have customized versions of one of these two implementations for their plat-\nforms. The MPI standard, now up to version 3.1 as of 2015, continues to evolve and\nchange.\n In this chapter, we’ll show you how to implement MPI in your application. We’ll\nstart with a simple MPI program and then progress to a more complicated example of\nhow to link together separate computational meshes on separate processes through\ncommunicating boundary information. We’ll touch on some advanced techniques\nthat are important for well-written MPI programs, such as building custom MPI data\ntypes and the use of MPI Cartesian topology functions. Last, we’ll introduce combin-\ning MPI with OpenMP (MPI plus OpenMPI) and vectorization to get multiple levels\nof parallelism. \nNOTE We encourage you to follow along with the examples for this chapter\nat https:/ /github.com/EssentialsofParallelComputing/Chapter8 .\n8.1 The basics for an MPI program\nIn this section, we will cover the basics that are needed for a minimal MPI program.\nSome of these basic requirements are specified by the MPI standard, while others are\nprovided by convention by most MPI implementations. The basic structure and opera-\ntion of MPI has stayed remarkably consistent since the first standard.\n To begin, MPI is a completely library-based language. It does not require a special\ncompiler or accommodations from the operating system. All MPI programs have a\nbasic structure and process as figure 8.1 shows. MPI always begins with an MPI_Init\ncall right at the start of the program and an MPI_Finalize  at the program’s exit. This\nis in contrast to OpenMP, as discussed in chapter 7, which needs no special startup\nand shutdown commands and just places parallel directives around key loops.\nWrite MPI program:\n#include <mpi.h>\nint main(int argc, char *argv[])\n{\nMPI_Init(&argc, &argv);\nMPI_Finalize();\nreturn(0);\n}\nCompile:\nWrappers: mpicc, mpiCC, mpif90\nor\nManual: include mpi.h and link\nin MPI library\nRun:\nmpirun -n <#procs> my_prog.x\nAlternate names for mpirun are\nmpiexec, aprun, srunFigure 8.1 The MPI approach is \nlibrary-based. Just compile, linking \nin the MPI library, and launch with a \nspecial parallel startup program.",3513
98-8.1.3 Using parallel startup commands.pdf,98-8.1.3 Using parallel startup commands,"256 CHAPTER  8MPI: The parallel backbone\nO n c e  y o u  w r i t e  a n  M P I  p a r a l l e l  p r o g r a m ,  i t  i s  c o m p i l e d  w i t h  a n  i n c l u d e  f i l e  a n d\nlibrary. Then it is executed with a special startup program that establishes the parallel\nprocesses across nodes and within the node.\n8.1.1 Basic MPI function calls for every MPI program\nThe basic MPI function calls include MPI_Init  and MPI_Finalize . The call to\nMPI_Init  should be right after program startup, and the arguments from the main\nroutine must be passed to the initialization call. Typical calls look like the following\nand may or may not use the return  variable:\niret = MPI_Init(&argc, &argv);\niret = MPI_Finalize();\nMost programs will need the number of processes and the process rank within the\ngroup that can communicate, called a communicator . One of the main functions of\nMPI is to start up remote processes and lash these up so messages can be sent\nbetween the processes. The default communicator is MPI_COMM_WORLD , which is set\nup at the beginning of every parallel job by MPI_Init . Let’s take a moment to look at\na few definitions:\nProcess —An independent unit of computation that has ownership of a portion\nof memory and control over resources in user space.\nRank —A unique, portable identifier to distinguish the individual process within\nthe set of processes. Normally this would be an integer within the set of integers\nfrom zero to one less than the number of processes.\nThe calls to get these important variables are\niret = MPI_Comm_rank(MPI_COMM_WORLD, &rank);\niret = MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n8.1.2 Compiler wrappers for simpler MPI programs\nAlthough MPI is a library, we can treat it like a compiler through the use of the MPI\ncompiler wrappers. This makes the building of MPI applications easier because you\ndon’t need to know which libraries are required and where the libraries are located.\nThese are especially convenient for small MPI applications. There are compiler wrap-\npers for each programming language:\nmpicc —Wrapper for C code\nmpicxx —Wrapper for C++ (also can be mpiCC  or mpic++ )\nmpifort —Wrapper for Fortran (also can be mpif77  or mpif90 )\nUsing these wrappers is optional. If you are not using the compiler wrappers, they\ncan still be valuable for identifying the compile flags necessary for building your appli-\ncation. The mpicc  command has options that output this information. You can find",2491
99-8.1.4 Minimum working example of an MPI program.pdf,99-8.1.4 Minimum working example of an MPI program,"257 The basics for an MPI program\nthese options for your MPI with man mpicc . For the two most popular MPI implemen-\ntations, we list the command-line options for mpicc , mpicxx , and mpifort  here.\nFor OpenMPI, use these command options:\n–--showme\n–--showme:compile\n–--showme:link\nFor MPICH, use these command options:\n–-show\n–-compile_info\n–-link_info\n8.1.3 Using parallel startup commands\nThe startup of the parallel processes for MPI is a complex operation that is handled by\na special command. At first, this command was often mpirun . But with the release of\nthe MPI 2.0 standard in 1997, the startup command was recommended to be mpiexec ,\nto try and provide more portability. Yet this attempt at standardization was not com-\npletely successful, and today there are several names used for the startup command: \nmpirun  -n <nprocs>\nmpiexec  -n <nprocs>\naprun\nsrun\nMost MPI startup commands take the option -n for the number of processes, but\nothers might take -np. With the complexity of recent computer node architectures,\nthe startup commands have a myriad of options for affinity, placement, and environ-\nment (some of which we will discuss in chapter 14). These options vary with each\nMPI implementation and even with each release of their MPI libraries. The simplic-\nity of the options available from the original startup commands has morphed into a\nconfusing morass of options that have not yet stabilized. Fortunately, for the begin-\nning MPI user, most of these options can be ignored , but they are important for advanced\nuse and tuning.\n8.1.4 Minimum working example of an MPI program\nNow that we have learned all the basic components, we can combine them into the\nminimum working example that listing 8.1 shows: we start the parallel job and print\nout the rank and number of processes from each process. In the call to get the rank\nand size, we use the MPI_COMM_WORLD  variable that is the group of all the MPI processes\nand is predefined in the MPI header file. Note that the displayed output can be in any\norder; the MPI program leaves it up to the operating system for when and how the\noutput is displayed.\n \n258 CHAPTER  8MPI: The parallel backbone\nMinWorkExampleMPI.c\n 1 #include <mpi.h>     \n 2 #include <stdio.h>\n 3 int main(int argc, char **argv)\n 4 {\n 5    MPI_Init(&argc, &argv);   \n 6 \n 7    int rank, nprocs;\n 8    MPI_Comm_rank(MPI_COMM_WORLD, &rank);       \n 9    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);    \n10\n11    printf(""Rank %d of %d\n"", rank, nprocs);\n12\n13    MPI_Finalize();     \n14    return 0;\n15 }\nListing 8.2 defines a simple makefile to build this example using the MPI compiler\nwrappers. In this case, we use the mpicc  wrapper to supply the location of the mpi.h\ninclude file and the MPI library.\nMinWorkExample/Makefile.simple\ndefault:        MinWorkExampleMPI\nall:    MinWorkExampleMPI\nMinWorkExampleMPI: MinWorkExampleMPI.c Makefile\n        mpicc MinWorkExampleMPI.c  -o MinWorkExampleMPI\nclean:\n        rm -f MinWorkExampleMPI MinWorkExampleMPI.o\nFor more elaborate builds on a variety of systems, you might prefer CMake. The fol-\nlowing listing shows the CMakeLists.txt file for this program.\nMinWorkExample/CMakeLists.txt\ncmake_minimum_required(VERSION 2.8)\nproject(MinWorkExampleMPI)\n# Require MPI for this project:\nfind_package(MPI REQUIRED)       \nadd_executable(MinWorkExampleMPI MinWorkExampleMPI.c)Listing 8.1 MPI minimum working example\nListing 8.2 Simple makefile using MPI compiler wrappers\nListing 8.3 The CMakeLists.txt for building with CMakeInclude file for MPI \nfunctions and variables\nInitializes after program start, \nincluding program arguments\nGets the rank number \nof the process\nGets the number of \nranks in the program \ndetermined by the \nmpirun command Finalizes MPI to \nsynchronize ranks \nand then exits\nCalls a special module to \nfind MPI and sets variables",3927
100-8.2 The send and receive commands for process-to-process communication.pdf,100-8.2 The send and receive commands for process-to-process communication,"259 The send and receive commands for process-to-process communication\ntarget_include_directories(MinWorkExampleMPI    \n   PRIVATE ${MPI_C_INCLUDE_PATH})               \ntarget_compile_options(MinWorkExampleMPI        \n   PRIVATE ${MPI_C_COMPILE_FLAGS})              \ntarget_link_libraries(MinWorkExampleMPI         \n   ${MPI_C_LIBRARIES} ${MPI_C_LINK_FLAGS})      \n# Add a test:\nenable_testing()\nadd_test(MPITest ${MPIEXEC} ${MPIEXEC_NUMPROC_FLAG}   \n   ${MPIEXEC_MAX_NUMPROCS}                            \n   ${MPIEXEC_PREFLAGS}                                \n   ${CMAKE_CURRENT_BINARY_DIR}/MinWorkExampleMPI      \n   ${MPIEXEC_POSTFLAGS})                              \n# Cleanup\nadd_custom_target(distclean COMMAND rm -rf CMakeCache.txt CMakeFiles\n                  Makefile cmake_install.cmake CTestTestfile.cmake Testing)\nNow using the CMake build system, let’s configure, build, and then run the test with\nthese commands:\ncmake .\nmake\nmake test\nThe write operation from the printf  command displays output in any order. Finally,\nto clean up after the run, use these commands:\nmake clean\nmake distclean\n8.2 The send and receive commands for \nprocess-to-process communication\nThe core of the message-passing approach is to send a message from point-to-point or,\nperhaps more precisely, process-to-process. The whole point of parallel processing is\nto coordinate work. To do this, you need to send messages either for control or work\ndistribution. We’ll show you how these messages are composed and properly sent.\nThere are many variants of the point-to-point routines; we’ll cover those that are rec-\nommended to use in most situations.\n Figure 8.2 shows the components of a message. There must be a mailbox at either\nend of the system. The size of the mailbox is important. The sending side knows the\nsize of the message, but the receiving side does not. To make sure there is a place for\nthe message to be stored, it is usually better to post the receive  first. This avoids delay-\ning the message by the receiving process having to allocate a temporary space to\nstore the message until a receive is posted and it can copy it to the right location.\nFor an analogy, if the receive (mailbox) is not posted (not there), the postman has\nto hangout until someone puts one up. Posting the receive first avoids the possibilityModifies \nthe compile \nflags\nCreates a \nportable \nMPI test\n260 CHAPTER  8MPI: The parallel backbone\nof insufficient memory space on the receiving end to allocate a temporary buffer to\nstore the message.\n The message itself is always composed of a triplet at both ends: a pointer to a mem-\nory buffer, a count, and a type. The type sent and type received can be different types\nand counts. The rationale for using types and counts is that it allows the conversion\nof types between the processes at the source and at the destination. This permits a\nmessage to be converted to a different form at the receiving end. In a heteroge-\nneous environment, this might mean converting lower-endian to big-endian, a low-\nlevel difference in the byte order of data stored on different hardware vendors. Also,\nthe receive size can be greater than the amount sent. This permits the receiver to\nquery how much data is sent so it can properly handle the message. But the receiving\nsize cannot be smaller than the sending size because it would cause a write past the\nend of the buffer.\n The envelope also is composed of a triplet. It defines who the message is from, who\nit is sent to, and a message identifier to keep from getting multiple messages con-\nfused. The triplet consists of the rank, tag, and communication group. The rank is for\nthe specified communication group. The tag helps the programmer and MPI distin-\nguish which message goes to which receive. In MPI, the tag is a convenience. It can be\nset to MPI_ANY_TAG  if an explicit tag number is not desired. MPI uses a context created\ninternally within the library to separate the messages correctly. Both the communica-\ntor and the tag must match for a message to complete.\nNOTE One of the strengths of the message-passing approach is the memory\nmodel. Each process has clear ownership of its data plus the control and syn-\nchronization over when the data changes. You can be guaranteed that some\nother process cannot change your memory while your back is turned.Sending mailbox with\nmessage ready to goReceiving mailbox with space\navailable for receiptMessage contains\npointer to memory\ncount data type.\nRank\nTag\nComm Group\nFigure 8.2 A message in MPI is always composed of a pointer to memory, a count, and a type. The \nenvelope has an address composed of a rank, a tag, and a communication group along with an internal \nMPI context.\n261 The send and receive commands for process-to-process communication\nNow let’s try an MPI program with a simple send/receive. We have to send data on\none process and receive data on another. There are different ways that we could issue\nthese calls on a couple of processes (figure 8.3). Some of the combinations of basic\nblocking send and receives are not safe and can hang, such as the two combinations\non the left of figure 8.3. The third combination requires careful programming with\nconditionals. The method to the far right is one of several safe methods to schedule\ncommunications by using non-blocking sends and receives. These are also called asyn-\nchronous  or immediate calls, which explains the I character preceding the send  and\nreceive  keywords (the case shown on the far right of the figure).\nThe most basic MPI send and receive is MPI_Send  and MPI_Recv . The basic send and\nreceive functions have the following prototypes:\nMPI_Send(void *data, int count, MPI_Datatype datatype, int dest, int tag,\n         MPI_COMM comm)\nMPI_Recv(void *data, int count, MPI_Datatype datatype, int source, int tag, \n         MPI_COMM comm, MPI_Status *status)\nNow let’s go through each of the four cases in figure 8.3 to understand why some\nhang and some work fine. We’ll begin with the MPI_Send  and MPI_Receive  that were\nshown in the previous function prototypes and in the left-most example in the figure.\nBoth of these routines are blocking . Blocking means that these do not return until a\nspecific condition is fulfilled. In the case of these two calls, the condition for return is\nthat the buffer is safe to use again. On the send, the buffer must have been read and\nis no longer needed. On the receive, the buffer must be filled. If both processes in a\ncommunication are blocking, a situation known as a hang  can occur. A hang occurs\nwhen one or more processes are waiting on an event that can never occur.Send\nReceiveBlocking send ﬁrst Blocking receive ﬁrst Alternating send receive\nSend\nReceiveTime\nSendReceive\nSendReceive Send\nReceive SendReceiveRank 0 Rank 0 Rank 1 Rank 0 Rank 1 Rank 1\nIsendIreceive\nIsendIreceiveRank 0 Rank 1\nWaitall Waitall\nBlocking communication calls\nNon-blocking communication callsNon-blocking\nsend receive\nAll operations are\nposted and now we\nwait for competion.Operation\ncompleted,\ncan\ncontinueOperation\ncompleted,\ncan\ncontinueIf buﬀer is\navailable,\ncopy and\ncontinue.If buﬀer is\navailable,\ncopy and\ncontinue.Blocked,\nwaiting\nfor dataBlocked,\nwaiting\nfor data\nFigure 8.3 The ordering of blocking send and receives is tricky to do correctly. It is much safer and faster to \nuse the non-blocking or immediate forms of the send and receive operations and then wait for completion.\n262 CHAPTER  8MPI: The parallel backbone\nExample: A blocking send/receive program that hangs\nThis example highlights a common problem in parallel programming. You must always\nbe on guard to avoid a situation that might hang (deadlock). In the following listing,\nwe look at how this might occur so that we can avoid it.\nSend_Recv/SendRecv1.c\n 1 #include <mpi.h>\n 2 #include <stdio.h>\n 3 #include <stdlib.h>\n 4 int main(int argc, char **argv)\n 5 {\n 6    MPI_Init(&argc, &argv);\n 7 \n 8    int count = 10;\n 9    double xsend[count], xrecv[count];\n10    for (int i=0; i<count; i++){\n11       xsend[i] = (double)i;\n12    }\n13 \n14    int rank, nprocs;\n15    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n16    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n17    if (nprocs%2 == 1){\n18       if (rank == 0){\n19          printf(""Must be called with an even number of processes\n"");\n20       }\n21       exit(1);\n22    }\n23 \n24    int tag = rank/2;     \n25    int partner_rank = (rank/2)*2 + (rank+1)%2;    \n26    MPI_Comm comm = MPI_COMM_WORLD;\n27\n28    MPI_Recv(xrecv, count, MPI_DOUBLE,    \n               partner_rank, tag, comm,     \n               MPI_STATUS_IGNORE);          \n29    MPI_Send(xsend, count, MPI_DOUBLE,    \n               partner_rank, tag, comm);    \n30 \n31    if (rank == 0) printf(""SendRecv successfully completed\n"");\n32\n33    MPI_Finalize();\n34    return 0;\n35 }\nThe tag and rank of the communication partner are calculated through integer and\nmodulo arithmetic that pairs up the tags for each send and receive and gets the rank\nof the other member of the pair. Then the receives are posted for every process with\nits partner. These are blocking receives that do not complete (return) until the bufferA simple send/receive example in MPI (always hangs)\nInteger division \npairs up the tags for \nthe send and receive \npartners.Partner rank is the \nopposite member \nof the pair.\nReceives are \nposted first.\nSends are done \nafter the receives.\n263 The send and receive commands for process-to-process communication\nLet’s try reversing the order of the sends and receives. We list the changed lines in the\nfollowing listing from the original listing in the previous example.\nSend_Recv/SendRecv2.c\n28    MPI_Send(xsend, count, MPI_DOUBLE,    \n               partner_rank, tag, comm);    \n29    MPI_Recv(xrecv, count, MPI_DOUBLE,   \n               partner_rank, tag, comm,    \n               MPI_STATUS_IGNORE);         \nSo does this one fail? Well, it depends. The send call returns after the use of the send\ndata buffer is complete. Most MPI implementations will copy the data into preallo-\ncated buffers on the sender or receiver if the size is small enough. In this case, the\nsend completes and the receive is called. If the message is large, the send waits for\nthe receive call to allocate a buffer to put the message into before returning. But the\nreceive never gets called, so the program hangs. We could alternate the posting of\nsends and receives by ranks so that hangs do not occur. We have to use a conditional\nfor this variant as the following listing shows. \nSend_Recv/SendRecv3.c\n28    if (rank%2 == 0) {        \n29       MPI_Send(xsend, count, MPI_DOUBLE, partner_rank, tag, comm);\n30       MPI_Recv(xrecv, count, MPI_DOUBLE, partner_rank, tag, comm,\n                  MPI_STATUS_IGNORE);\n31    } else {                                            \n32       MPI_Recv(xrecv, count, MPI_DOUBLE, partner_rank, tag, comm,\n                  MPI_STATUS_IGNORE);\n33       MPI_Send(xsend, count, MPI_DOUBLE, partner_rank, tag, comm);\n34    }\nBut this is complicated to get right in more complex communication and requires care-\nful use of conditionals. A better way to implement this is by using the MPI_Sendrecv\ncall as the next listing shows. By using this call, you hand-off the responsibility for cor-\nrectly executing the communication to the MPI library. This is a pretty good deal for\nthe programmer.\n is filled. Because the send is not called until after the receives complete, the program\nhangs. Note that we wrote the send and receive calls without if statements (condi-\ntionals) based on rank. Conditionals are the source of many bugs in parallel code, so\nthese are generally good to avoid.\nListing 8.4 A simple send/receive example in MPI (sometimes fails)\nListing 8.5 Send/receive with alternating sends and receives by rankFirst calls send \noperation\nThen calls receive operation \nafter send completes\nEven ranks post \nthe send first.\nOdd ranks do the receive first.\n264 CHAPTER  8MPI: The parallel backbone\nSend_Recv/SendRecv4.c\n28    MPI_Sendrecv(xsend, count, MPI_DOUBLE,   \n                   partner_rank, tag,          \n29                 xrecv, count, MPI_DOUBLE,   \n                   partner_rank, tag, comm,    \n                   MPI_STATUS_IGNORE);         \nThe  MPI_Sendrecv  call is a good example of the advantages of using the collective\ncommunication calls that we’ll present in section 8.3. It is good practice to use the col-\nlective communication calls when possible because these delegate responsibility for\navoiding hangs and deadlocks , as well as the responsibility for good performance to\nthe MPI library.\n As an alternative to the blocking communication calls in previous examples, we\nlook at using the MPI_Isend  and MPI_Irecv  in listing 8.7. These are called immediate\n(I) versions because these return immediately. This is often referred to as asynchro-\nnous or non-blocking calls. Asynchronous  means that the call initiates the operation but\ndoes not wait for the completion of the work.\nSend_Recv/SendRecv5.c\n27    MPI_Request requests[2] =\n         {MPI_REQUEST_NULL, MPI_REQUEST_NULL};    \n28 \n29    MPI_Irecv(xrecv, count, MPI_DOUBLE,    \n                partner_rank, tag, comm,     \n                &requests[0]);               \n30    MPI_Isend(xsend, count, MPI_DOUBLE,       \n                partner_rank, tag, comm,        \n                &requests[1]);                  \n31    MPI_Waitall(2, requests, MPI_STATUSES_IGNORE);       \nEach process waits at the MPI_Waitall  on line 31 of the listing for message comple-\ntion. You should also see a measurable improvement in program performance by\nreducing the number of places that block from every send and receive call to just the\nsingle MPI_Waitall . But you must be careful not to modify the send buffer or access\nthe receive buffer until the operation completes. There are other combinations that\nwork. Let’s look at the following listing, which uses one possibility.\nSend_Recv/SendRecv6.c\n27    MPI_Request request;\n28 \n29    MPI_Isend(xsend, count, MPI_DOUBLE,    \n                partner_rank, tag, comm,     \n                &request);                   Listing 8.6 Send/receive with the MPI_Sendrecv  call\nListing 8.7 A simple send/receive example using Isend  and Irecv\nListing 8.8 A mixed immediate and blocking send/receive exampleA combined send/receive \ncall replaces the individual \nMPI_Send and MPI_Recv.\nDefines an array of requests and \nsets to null so these are defined \nwhen tested for completion\nThe Irecv is \nposted first.\nThe Isend is then called \nafter the Irecv completes.\nCalls a Waitall to wait for the send and receive to complete\nPosts the send \nwith an MPI_Isend \nso that it returns\n265 The send and receive commands for process-to-process communication\n30    MPI_Recv(xrecv, count, MPI_DOUBLE,   \n               partner_rank, tag, comm,    \n               MPI_STATUS_IGNORE);         \n31    MPI_Request_free(&request);     \nWe start the communication with an asynchronous send and then block with a blocking\nreceive. Once the blocking receive completes, this process can continue even if the send\nhas not completed. You still must free the request handle with an MPI_Request_free  or\nas a side-effect of a call to MPI_Wait  or an MPI_Test  to avoid a memory leak. You can\nalso call the MPI_Request_free  immediately after the MPI_Isend .\n Other variants of send/receive might be useful in special situations. The modes\nare indicated by a one- or two-letter prefix, similar to that seen in the immediate vari-\nant, as listed here:\nB (buffered)\nS (synchronous)\nR (ready)\nIB (immediate buffered)\nIS (immediate synchronous)\nIR (immediate ready)\nThe list of predefined MPI data types for C is extensive; the data types map to nearly\nall the types in the C language. MPI also has types corresponding to Fortran data\ntypes. We list just the most common ones for C:\nMPI_CHAR  (a 1-byte C character type)\nMPI_INT  (a 4-byte integer type)\nMPI_FLOAT  (a 4-byte real type)\nMPI_DOUBLE  (an 8-byte real type)\nMPI_PACKED  (a generic byte-sized data type, usually used for mixed data types)\nMPI_BYTE  (a generic byte-sized data type)\nThe MPI_PACKED  and MPI_BYTE  are special types and match any other type. MPI_BYTE\nindicates an untyped value and the count specifies the number of bytes. It bypasses\nany data conversion operations in heterogeneous data communications. MPI_PACKED\nis used with the MPI_PACK  routine as the ghost exchange example in section 8.4.3\nshows. You can also define your own data type to use in these calls. This is also demon-\nstrated in the ghost exchange example. There are also many communication comple-\ntion testing routines, which include\nint MPI_Test(MPI_Request *request, int *flag, MPI_Status *status)\nint MPI_Testany(int count, MPI_Request requests[], int *index, int *flag,\n                MPI_Status *status)\nint MPI_Testall(int count, MPI_Request requests[], int *flag, \n                MPI_Status statuses[])Calls the blocking receive. \nThis process can continue \nas soon as it returns.\nFrees the request handle \nto avoid a memory leak",17373
101-8.3.3 Using a reduction to get a single value from across all processes.pdf,101-8.3.3 Using a reduction to get a single value from across all processes,"266 CHAPTER  8MPI: The parallel backbone\nint MPI_Testsome(int incount, MPI_Request requests[], int *outcount, \n                 int indices[], MPI_Status statuses[])\nint MPI_Wait(MPI_Request *request, MPI_Status *status)\nint MPI_Waitany(int count, MPI_Request requests[], int *index, \n                MPI_Status *status)\nint MPI_Waitall(int count, MPI_Request requests[], MPI_Status statuses[])\nint MPI_Waitsome(int incount, MPI_Request requests[], int *outcount, \n                 int indices[], MPI_Status statuses[])\nint MPI_Probe(int source, int tag, MPI_Comm comm, MPI_Status *status)\nThere are additional variants of the MPI_Probe  that are not listed here. MPI_Waitall  is\nshown in several examples in this chapter. The other routines are useful in more spe-\ncialized situations. The name of the routines gives a good idea of the capabilities that\nthese provide. \n8.3 Collective communication: \nA powerful component of MPI\nIn this section, we’ll look at the rich set of collective communication calls in MPI. Col-\nlective communications operate on a group of processes contained in an MPI commu-\nnicator. To operate on a partial set of processes, you can create your own MPI\ncommunicator for a subset of MPI_COMM_WORLD  such as every other process. Then you\ncan use your communicator in place of MPI_COMM_WORLD  in collective communication\ncalls. Most of the collective communication routines operate on data. Figure 8.4 gives\na visual idea of what each collective operation does.\nProc 0\nProc 1\nProc 2\nProc 3Proc 0 1.0 1.0\n1.0\n1.0\n1.0MPI_Bcast\nProc 0\nProc 1\nProc 2\nProc 3Proc 0 4.0 1.0\n1.0\n1.0\n1.0MPI_Reduce\n+Proc 0\nProc 1\nProc 2\nProc 3Proc 0 4.0 1.0\n1.0\n1.0\n1.0MPI_Allreduce\n+Proc 1 4.0\nProc 2 4.0\nProc 3 4.0\n1.0 2.0 3.0 4.0\nProc 0\nProc 1\nProc 2\nProc 31.0\n2.0\n3.0\n4.0Proc 0MPI_Scatter\n1.0 2.0 3.0 4.0Proc 0\nProc 1\nProc 2\nProc 31.0\n2.0\n3.0\n4.0\nProc 0MPI_Gather\n1.0 2.0 3.0 4.0Proc 0\nProc 1\nProc 2\nProc 31.0\n2.0\n3.0\n4.0\nProc 3MPI_Allgather\n1.0 2.0 3.0 4.0 Proc 21.0 2.0 3.0 4.0 Proc 11.0 2.0 3.0 4.0 Proc 0\nFigure 8.4 The data movement of the most common MPI collective routines provide important functions \nfor parallel programs. Additional variants MPI_Scatterv , MPI_Gatherv , and MPI_Allgatherv  allow \na variable amount of data to be sent or received from the processes. Not shown are some additional \nroutines such as the MPI_Alltoall  and similar functions. \n267 Collective communication: A powerful component of MPI\nWe’ll present examples of how to use the most commonly used collective operations\nas these might be applied in an application. The first example (in section 8.3.1) shows\nhow you might use the barrier. It is the only collective routine that does not operate\non data. Then we’ll show some examples with the broadcast (section 8.3.2), reduction\n(section 8.3.3), and finally, scatter/gather operations (sections 8.3.4 and 8.3.5). MPI\nalso has a variety of all-to-all routines. But these are costly and rarely used, so we won’t\ncover those here. These collective operations all operate on a group of processes rep-\nresented by a communication group. All members of a communication group must\ncall the collective or your program will hang.\n8.3.1 Using a barrier to synchronize timers\nThe simplest collective communication call is MPI_Barrier . It is used to synchronize\nall of the processes in an MPI communicator. In most programs, it should not be nec-\nessary, but it is often used for debugging and for synchronizing timers. Let’s look at\nhow MPI_Barrier  could be used to synchronize timers in the following listing. We also\nuse the MPI_Wtime  function to get the current time.\nSynchronizedTimer/SynchronizedTimer1.c\n 1 #include <mpi.h>\n 2 #include <unistd.h>\n 3 #include <stdio.h>\n 4 int main(int argc, char *argv[])\n 5 {\n 6    double start_time, main_time;\n 7 \n 8    MPI_Init(&argc, &argv);\n 9    int rank;\n10    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n11    \n12    MPI_Barrier(MPI_COMM_WORLD);           \n13    start_time = MPI_Wtime();         \n14\n15    sleep(30);   \n16\n17    MPI_Barrier(MPI_COMM_WORLD);           \n18    main_time = MPI_Wtime() - start_time;                  \n19    if (rank == 0) printf(""Time for work is %lf seconds\n"", main_time);\n20 \n21    MPI_Finalize();\n22    return 0;\n23 }\nThe barrier is inserted before starting the timer and then just before stopping the\ntimer. This forces the timers on all of the processes to start at about the same time. By\ninserting the barrier before stopping the timer, we get the maximum time across all of\nthe processes. Sometimes using a synchronized timer gives a less confusing measure\nof time, but in others, an unsynchronized timer is better. Listing 8.9 Using MPI_Barrier  to synchronize a timer in an MPI program\nSynchronizes all the \nprocesses so these start \nat about the same time\nGets the starting value \nof the timer using the \nMPI_Wtime routine\nRepresents\nwork being\ndoneSynchronizes the \nprocesses to get the \nlongest time taken\nGets the timer value and subtracts the\nstarting value to get the elapsed time\n268 CHAPTER  8MPI: The parallel backbone\nNOTE Synchronized timers and barriers should not be used in production\nruns; these can cause serious slowdowns in an application.\n8.3.2 Using the broadcast to handle small file input\nThe broadcast sends data from one processor to all of the others. This operation is\nshown in figure 8.4 in the upper left. One of the uses of the broadcast, MPI_Bcast , is\nto send values read from an input file to all other processes. If every process tries to\nopen a file at large process counts, it can take minutes to complete the file open.\nThis is because file systems are inherently serial and one of the slower components\nof a computer system. For these reasons, for small file input, it is a good practice to\nonly open and read a file from a single process. The following listing shows the way\nto do this.\nFileRead/FileRead.c\n 1 #include <stdio.h>\n 2 #include <string.h>\n 3 #include <stdlib.h>\n 4 #include <mpi.h>\n 5 int main(int argc, char *argv[])\n 6 {\n 7    int rank, input_size;\n 8    char *input_string, *line;\n 9    FILE *fin;\n10 \n11    MPI_Init(&argc, &argv);\n12    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n13 \n14    if (rank == 0){\n15       fin = fopen(""file.in"", ""r"");\n16       fseek(fin, 0, SEEK_END);     \n17       input_size = ftell(fin);     \n18       fseek(fin, 0, SEEK_SET);       \n19       input_string = (char *)malloc((input_size+1)*sizeof(char));\n20       fread(input_string, 1, input_size, fin);       \n21       input_string[input_size] = '\0';      \n22    }\n23 \n24    MPI_Bcast(&input_size, 1, MPI_INT, 0,    \n                MPI_COMM_WORLD);               \n25    if (rank != 0)                         \n         input_string =                      \n            (char *)malloc((input_size+1)*   \n                           sizeof(char));    \n26    MPI_Bcast(input_string, input_size,        \n                MPI_CHAR, 0, MPI_COMM_WORLD);    \n27 \n28    if (rank == 0) fclose(fin);\n29 \n30    line = strtok(input_string,""\n"");Listing 8.10 Using MPI_Bcast  to handle small file input\nGets the file size to \nallocate an input buffer\nResets the file pointer \nto the start of file\nReads entire file\nNull terminating input buffer\nBroadcasts size \nof input buffer\nAllocates input buffer \non other processes\nBroadcasts \ninput buffer\n269 Collective communication: A powerful component of MPI\n31    while (line != NULL){\n32       printf(""%d:input string is %s\n"",rank,line);\n33       line = strtok(NULL,""\n"");\n34    }\n35    free(input_string);\n36 \n37    MPI_Finalize();\n38    return 0;\n39 }\nIt is better to broadcast larger chunks of data than it is to broadcast many small indi-\nvidual values. We therefore broadcast the entire file. To do this, we need to first broad-\ncast the size so that every process can allocate an input buffer and then broadcast the\ndata. The file read and broadcasts are done from rank 0, generally referred to as\nthe main process. \n MPI_Bcast  takes a pointer for the first argument, so when sending a scalar variable,\nwe send the reference by using the ampersand ( &) operator to get the address of the\nvariable. Then comes the count and the type to fully define the data to be sent. The\nnext argument specifies the originating process. It is 0 in both of these calls because\nthat is the rank where the data resides. All other processes in the MPI_COMM_WORLD\ncommunication then receive the data. This technique is for small input files. For\nlarger file input or output, there are ways to conduct parallel file operations. The\ncomplex world of parallel input and output is discussed in chapter 16.\n8.3.3 Using a reduction to get a single value from across all processes\nThe reduction pattern, discussed in section 5.7, is one of the most important parallel\ncomputing patterns. The reduction operation is shown in figure 8.4 in the upper mid-\ndle. An example of the reduction in Fortran array syntax is xsum  = sum(x(:)) , where\nthe Fortran sum intrinsic sums the x array and puts it in the scalar variable xsum . The\nMPI reduction calls take an array or multi-dimensional array and combine the values\ninto a scalar result. There are many operations that can be done during the reduction.\nThe most common are\nMPI_MAX  (maximum value in an array)\nMPI_MIN  (minimum value in an array)\nMPI_SUM  (sum of an array)\nMPI_MINLOC  (index of minimum value)\nMPI_MAXLOC  (index of maximum value)\nThe following listing shows how we can use MPI_Reduce  to get the minimum, maxi-\nmum, and average of a variable from every process.\nSynchronizedTimer/SynchronizedTimer2.c\n 1 #include <mpi.h>\n 2 #include <unistd.h>Listing 8.11 Using reductions to get min, max, and avg timer results\n270 CHAPTER  8MPI: The parallel backbone\n 3 #include <stdio.h>\n 4 int main(int argc, char *argv[])\n 5 {\n 6    double start_time, main_time, min_time, max_time, avg_time;\n 7 \n 8    MPI_Init(&argc, &argv);\n 9    int rank, nprocs;\n10    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n11    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n12 \n13    MPI_Barrier(MPI_COMM_WORLD);    \n14    start_time = MPI_Wtime();       \n15 \n16    sleep(30);    \n17 \n18    main_time = MPI_Wtime() - start_time;     \n19    MPI_Reduce(&main_time, &max_time, 1,          \n         MPI_DOUBLE, MPI_MAX, 0, MPI_COMM_WORLD);   \n20    MPI_Reduce(&main_time, &min_time, 1,          \n         MPI_DOUBLE, MPI_MIN, 0,MPI_COMM_WORLD);    \n21    MPI_Reduce(&main_time, &avg_time, 1,          \n         MPI_DOUBLE, MPI_SUM, 0,MPI_COMM_WORLD);    \n22    if (rank == 0) \n         printf(""Time for work is Min: %lf  Max: %lf  Avg:  %lf seconds\n"",\n23              min_time, max_time, avg_time/nprocs);\n24 \n25    MPI_Finalize();\n26    return 0;\n27 }\nThe reduction result, the maximum in this case, is stored on rank 0 (argument 6 in\nthe MPI_Reduce  call), which in this case is the main process. If we wanted to just print\nit out on the main process, this would be appropriate. But if we wanted all of the pro-\ncesses to have the value, we would use the MPI_Allreduce  routine. \n You can also define your own operator. We’ll use the example of the Kahan\nenhanced-precision summation we have been working with and first introduced in\nsection 5.7. The challenge in a distributed memory parallel environment is to carry\nthe Kahan summation across process ranks. We start by looking at the main program\nin the following listing before looking at two other parts of the program in listings\n8.13 and 8.14.\nGlobalSums/globalsums.c\n57 int main(int argc, char *argv[])\n58 {\n59    MPI_Init(&argc, &argv);\n60    int rank, nprocs;\n61    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n62    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n63 \n64    init_kahan_sum();                      Listing 8.12 An MPI version of the Kahan summationSynchronizes all the processes so \nthese start at about the same time\nRepresents work \nbeing doneGets the timer value and \nsubtracts the starting value \nto get the elapsed time\nUses reduction calls to \ncompute the max, min, \nand average time\nInitializes the new MPI \ndata type and creates \na new operator\n271 Collective communication: A powerful component of MPI\n65 \n66    if (rank == 0) printf(""MPI Kahan tests\n"");\n67 \n68    for (int pow_of_two = 8; pow_of_two < 31; pow_of_two++){\n69       long ncells = (long)pow((double)2,(double)pow_of_two);\n70 \n71       int nsize;\n72       double accurate_sum;\n73       double *local_energy =          \n           init_energy(ncells, &nsize,   \n           &accurate_sum);               \n74 \n75       struct timespec cpu_timer;\n76       cpu_timer_start(&cpu_timer);\n77 \n78       double test_sum =                          \n           global_kahan_sum(nsize, local_energy);   \n79 \n80       double cpu_time = cpu_timer_stop(cpu_timer);\n81 \n82       if (rank == 0){\n83          double sum_diff = test_sum-accurate_sum; \n84          printf(""ncells %ld log %d acc sum %-17.16lg sum %-17.16lg "",\n85                 ncells,(int)log2((double)ncells),accurate_sum,test_sum);\n86          printf(""diff %10.4lg relative diff %10.4lg runtime %lf\n"",\n87                 sum_diff,sum_diff/accurate_sum, cpu_time);\n88       }\n89       \n90       free(local_energy);\n91    }\n92    \n93    MPI_Type_free(&EPSUM_TWO_DOUBLES);   \n94    MPI_Op_free(&KAHAN_SUM);             \n95    MPI_Finalize();\n96    return 0;\n97 }\nThe main  program shows that the new MPI data type is created once at the start of the\nprogram and freed at the end, before MPI_Finalize . The call to perform the global\nKahan summation is done multiple times within the loop, where the data size is\nincreased by powers of two. Now let’s look at the next listing to see what needs to be\ndone to initialize the new data type and operator.\nGlobalSums/globalsums.c\n14 struct esum_type{        \n15    double sum;           \n16    double correction;    \n17 };                       \n18 \n19 MPI_Datatype EPSUM_TWO_DOUBLES;         \n20 MPI_Op KAHAN_SUM;                           Listing 8.13 Initializing new MPI data type and operator for Kahan summationGets a distributed \narray to work with\nCalculates the Kahan \nsummation of the energy \narray across all processes\nFrees the custom data \ntype and operator\nDefines an esum_type \nstructure to hold the \nsum and correction termDeclares a new MPI \ndata type composed \nof two doubles\nDeclares a new Kahan \nsummation operator\n272 CHAPTER  8MPI: The parallel backbone\n21 \n22 void kahan_sum(struct esum_type * in, \n                  struct esum_type * inout, int *len,\n23     MPI_Datatype *EPSUM_TWO_DOUBLES)                \n24 {\n25    double corrected_next_term, new_sum;\n26    corrected_next_term = in->sum + (in->correction + inout->correction);\n27    new_sum = inout->sum + corrected_next_term;\n28    inout->correction = corrected_next_term - (new_sum - inout->sum);\n29    inout->sum = new_sum;\n30 }\n31 \n32 void init_kahan_sum(void){\n33    MPI_Type_contiguous(2, MPI_DOUBLE,         \n                          &EPSUM_TWO_DOUBLES);   \n34    MPI_Type_commit(&EPSUM_TWO_DOUBLES);       \n35 \n36    int commutative = 1;                             \n37    MPI_Op_create((MPI_User_function *)kahan_sum,    \n                    commutative, &KAHAN_SUM);          \n38 }\nWe first create the new data type, EPSUM_TWO_DOUBLES , by combining two of the basic\nMPI_DOUBLE  data type in line 33. We have to declare the type outside the routine at\nline 19 so that it is available to use by the summation routine. To create the new oper-\nator, we first write the function to use as the operator in lines 22-30. We then use\nesum_type  to pass both double values in and back out. We also need to pass in the\nlength and the data type that it will operate on as the new EPSUM_TWO_DOUBLES  type.\n In the process of creating a Kahan sum reduction operator, we showed you how to\ncreate a new MPI data type and a new MPI reduction operator. Now let’s move on to\nactually calculating the global sum of the array across MPI ranks as the following list-\ning shows.\nGlobalSums/globalsums.c\n40 double global_kahan_sum(int nsize, double *local_energy){\n41    struct esum_type local, global;\n42    local.sum = 0.0;                \n43    local.correction = 0.0;         \n44 \n45    for (long i = 0; i < nsize; i++) {           \n46       double corrected_next_term =              \n            local_energy[i] + local.correction;    \n47       double new_sum =                          \n            local.sum + local.correction;          \n48       local.correction = corrected_next_term -  \n                           (new_sum - local.sum);  \n49       local.sum = new_sum;                      \n50    }                                            \n51 Listing 8.14 Performing an MPI Kahan summationDefines a function for \nthe new operator using \na predefined signature\nCreates the type \nand commits it\nCreates the new \noperator and \ncommits it\nInitializes both members \nof the esum_type to zero\nPerforms the \non-process Kahan \nsummation",17235
102-8.3.5 Using scatter and gather to send data out to processes for work.pdf,102-8.3.5 Using scatter and gather to send data out to processes for work,"273 Collective communication: A powerful component of MPI\n52    MPI_Allreduce(&local, &global, 1, EPSUM_TWO_DOUBLES, KAHAN_SUM,  \n                    MPI_COMM_WORLD);\n53 \n54    return global.sum;\n55 }\nCalculating the global Kahan summation is relatively easy now. We can do the local\nKahan sum as shown in section 5.7. But we have to add MPI_Allreduce  at line 52 to\nget the global result. Here, we defined the allreduce operation to end with the result\non all processors as shown in figure 8.4 in the upper right.\n8.3.4 Using gather to put order in debug printouts\nA gather operation can be described as a collate operation, where data from all pro-\ncessors is brought together and stacked into a single array as shown in figure 8.4 in the\nlower center. You can use this collective communication call to bring order to the out-\nput to the console from your program. By now, you should have noticed that the out-\nput printed from multiple ranks of an MPI program comes out in random order,\nproducing a jumbled, confusing mess. Let’s look at a better way to handle this so that\nthe only output is from the main process. By printing the output from only the main\nprocess, the order will be correct. The next listing shows a sample program that gets\ndata from all of the processes and prints it out in a nice, orderly output.\nDebugPrintout/DebugPrintout.c\n 1 #include <stdio.h>\n 2 #include <time.h>\n 3 #include <unistd.h>\n 4 #include <mpi.h>\n 5 #include ""timer.h""\n 6 int main(int argc, char *argv[])\n 7 {\n 8    int rank, nprocs;\n 9    double total_time;\n10    struct timespec tstart_time;\n11 \n12    MPI_Init(&argc, &argv);\n13    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n14    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n15 \n16    cpu_timer_start(&tstart_time);\n17    sleep(30);                            \n18    total_time += cpu_timer_stop(tstart_time);\n19 \n20    double times[nprocs];                      \n21    MPI_Gather(&total_time, 1, MPI_DOUBLE,        \n         times, 1, MPI_DOUBLE, 0, MPI_COMM_WORLD);  \n22    if (rank == 0) {              \n23       for (int i=0; i<nprocs; i++){    Listing 8.15 Using a gather to print debug messagesPerforms the reduction with the\nnew KAHAN_SUM operator\nGets unique values \non each process \nfor our example\nNeeds an array to \ncollect all the times\nUses a gather to \nbring all the values \nto process zero\nOnly prints on the \nmain processLoops\nover the\nprocesses\nfor the\nprint\n274 CHAPTER  8MPI: The parallel backbone\n24          printf(""%d:Work took %lf secs\n"",    \n                   i, times[i]);                 \n25       }\n26    }\n27 \n28    MPI_Finalize();\n29    return 0;\n30 }\nMPI_Gather  takes the standard triplet describing the data source. We need to use the\nampersand to get the address of the scalar variable total_time . The destination is also a\ntriplet with the destination array of times . An array is already an address, so no amper-\nsand is needed. The gather is done to process 0 of the MPI world communication group.\nFrom there, it requires a loop to print the time for each process. We prepend every line\nwith a number in the format #: so that it is clear which process the output refers to.\n8.3.5 Using scatter and gather to send data out to processes for work\nThe scatter operation, shown in figure 8.4 in the lower left, is the opposite of the\ngather operation. For this operation, the data is sent from one process to all the oth-\ners in the communication group. The most common use for a scattering operation is\nin the parallel strategy distributing data arrays out to other processes for work. This is\nprovided by the MPI_Scatter  and MPI_Scatterv  routines. The following listing shows\nthe implementation.\nScatterGather/ScatterGather.c\n 1 #include <stdio.h>\n 2 #include <stdlib.h>\n 3 #include <mpi.h>\n 4 int main(int argc, char *argv[])\n 5 {\n 6    int rank, nprocs, ncells = 100000;\n 7 \n 8    MPI_Init(&argc, &argv);\n 9    MPI_Comm comm = MPI_COMM_WORLD;\n10    MPI_Comm_rank(comm, &rank);\n11    MPI_Comm_size(comm, &nprocs);\n12 \n13    long ibegin = ncells *(rank  )/nprocs;   \n14    long iend   = ncells *(rank+1)/nprocs;   \n15    int  nsize  = (int)(iend-ibegin);        \n16 \n17    double *a_global, *a_test;\n18    if (rank == 0) {\n19       a_global = (double *)               \n            malloc(ncells*sizeof(double));   \n20       for (int i=0; i<ncells; i++) {      \n21          a_global[i] = (double)i;         \n22       }                                   \n23    }Listing 8.16 Using scatter to distribute data and gather to bring it backPrints the time \nfor each process\nComputes the size \nof the array on \nevery process\nSets up data \non the main \nprocess\n275 Collective communication: A powerful component of MPI\n24 \n25    int nsizes[nprocs], offsets[nprocs];          \n26    MPI_Allgather(&nsize, 1, MPI_INT, nsizes,     \n                    1, MPI_INT, comm);              \n27    offsets[0] = 0;                               \n28    for (int i = 1; i<nprocs; i++){               \n29       offsets[i] = offsets[i-1] + nsizes[i-1];   \n30    }                                             \n31 \n32    double *a = (double *)                           \n         malloc(nsize*sizeof(double));                 \n33    MPI_Scatterv(a_global, nsizes, offsets,          \n34       MPI_DOUBLE, a, nsize, MPI_DOUBLE, 0, comm);   \n35 \n36    for (int i=0; i<nsize; i++){     \n37       a[i] += 1.0;                  \n38    }                                \n39 \n40    if (rank == 0) {\n41       a_test = (double *)                 \n            malloc(ncells*sizeof(double));   \n42    }\n43 \n44    MPI_Gatherv(a, nsize, MPI_DOUBLE,      \n45                a_test, nsizes, offsets,   \n                  MPI_DOUBLE, 0, comm);      \n46 \n47    if (rank == 0){\n48       int ierror = 0;\n49       for (int i=0; i<ncells; i++){\n50          if (a_test[i] != a_global[i] + 1.0) {\n51             printf(""Error: index %d a_test %lf a_global %lf\n"",\n52                    i,a_test[i],a_global[i]);\n53             ierror++;\n54          }\n55       }\n56       printf(""Report: Correct results %d errors %d\n"",\n                ncells-ierror,ierror);\n57    }\n58 \n59    free(a);\n60    if (rank == 0) {\n61       free(a_global);\n62       free(a_test);\n63    }\n64 \n65    MPI_Finalize();\n66    return 0;\n67 }\nWe first need to calculate the size of the data on each process. The desired distribution\nis to be as equal as possible. A simple way to calculate the size is shown in lines 13–15,\nusing simple integer arithmetic. Now we need the global array, but we only need it on\nthe main process. So we allocate and set it up on this process in lines 18-23. In order toGets the sizes \nand offsets into \nglobal arrays for \ncommunication\nDistributes the \ndata onto the \nother processes\nDoes the \ncomputation\nReturns array data \nto the main process, \nperhaps for output",6966
103-8.4 Data parallel examples.pdf,103-8.4 Data parallel examples,,0
104-8.4.2 Ghost cell exchanges in a two-dimensional 2D mesh.pdf,104-8.4.2 Ghost cell exchanges in a two-dimensional 2D mesh,"276 CHAPTER  8MPI: The parallel backbone\ndistribute or gather the data, the sizes and offsets for all processes must be known. We\nsee the typical calculation for this in lines 25-30. The actual scatter is done with an\nMPI_Scatterv  on lines 32–34. The data source is described with the arguments buf-\nfer, counts , offsets , and the data type. The destination is handled with the standard\ntriplet. Then the source rank that will send the data is specified as rank 0. Finally, the\nlast argument is comm , the communication group that will receive the data.\n MPI_Gatherv  does the opposite operation, as shown in figure 8.4. We only need\nthe global array on the main process, and so it is only allocated there on lines 40-42.\nThe arguments to MPI_Gatherv  start with the description of the source with the stan-\ndard triplet. Then the destination is described with the same four arguments as were\nused in the scatter. The destination rank is the next argument, followed by the com-\nmunication group.\n It should be noted that the sizes and offsets used in the MPI_Gatherv  call are all of\ninteger type. This limits the size of the data that can be handled. There was an attempt\nto change the data type to the long data type so larger data sizes could be handled in\nversion 3 of the MPI standard. It was not approved because it would break too many\napplications. Stay tuned for the addition of new calls that provide support for a long\ninteger type in one of the next MPI standards.\n8.4 Data parallel examples\nThe data parallel strategy, defined in section 1.5, is the most common approach in\nparallel applications. We’ll look at a few examples of this approach in this section.\nFirst, we’ll look at a simple case of the stream triad where no communication is neces-\nsary. Then we’ll look at the more typical ghost cell exchange techniques used to link\ntogether the subdivided domains distributed to each process.\n8.4.1 Stream triad to measure bandwidth on the node\nThe STREAM Triad is a bandwidth testing benchmark code introduced in section 3.2.4.\nThis version uses MPI to get more processes working on the node and, possibly, on\nmultiple nodes. The purpose of having more processes is to see what the maximum\nbandwidth is for the node when all processors are used. This gives a target bandwidth\nto aim for with more complicated applications. As listing 8.17 shows, the code is sim-\nple because no communication between ranks is required. The timing is only reported\non the main process. You can run this first on one processor and then on all the pro-\ncessors on your node. Do you get the full parallel speedup that you would expect from\nthe increase in processors? How much does the system memory bandwidth limit your\nspeedup?\nStreamTriad/StreamTriad.c\n  1 #include <stdio.h>\n  2 #include <stdlib.h>Listing 8.17 The MPI version of the STREAM Triad\n277 Data parallel examples\n  3 #include <time.h>\n  4 #include <mpi.h>\n  5 #include ""timer.h""\n  6 \n  7 #define NTIMES 16\n  8 #define STREAM_ARRAY_SIZE 80000000    \n  9 \n 10 int main(int argc, char *argv[]){\n 11 \n 12    MPI_Init(&argc, &argv);\n 13 \n 14    int nprocs, rank;\n 15    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n 16    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n 17    int ibegin = STREAM_ARRAY_SIZE *(rank  )/nprocs;\n 18    int iend   = STREAM_ARRAY_SIZE *(rank+1)/nprocs;\n 19    int nsize = iend-ibegin;\n 20    double *a = malloc(nsize * sizeof(double));\n 21    double *b = malloc(nsize * sizeof(double));\n 22    double *c = malloc(nsize * sizeof(double));\n 23 \n 24    struct timespec tstart;\n 25    double scalar = 3.0, time_sum = 0.0;   \n 26    for (int i=0; i<nsize; i++) {          \n 27       a[i] = 1.0;                         \n 28       b[i] = 2.0;                         \n 29    }                                      \n 30 \n 31    for (int k=0; k<NTIMES; k++){\n 32       cpu_timer_start(&tstart);\n 33       for (int i=0; i<nsize; i++){    \n 34          c[i] = a[i] + scalar*b[i];   \n 35       }                               \n 36       time_sum += cpu_timer_stop(tstart);\n 37       c[1]=c[2];    \n 38    }\n 39 \n 40    free(a);\n 41    free(b);\n 42    free(c);\n 43 \n 44    if (rank == 0) \n          printf(""Average runtime is %lf msecs\n"", time_sum/NTIMES);\n 45    MPI_Finalize();\n 46    return(0);\n 47 }\n8.4.2 Ghost cell exchanges in a two-dimensional (2D) mesh\nGhost  cells are the mechanism that we use to link the meshes on adjacent processors.\nThese are used to cache values from adjacent processors so that fewer communica-\ntions are needed. The ghost cell technique is the single most important method for\nenabling distributed memory parallelism in MPI.Large enough to \nforce into main \nmemory\nInitializes \ndata and \narrays\nThe stream \ntriad loop\nKeeps the compiler \nfrom optimizing \nout the loop\n278 CHAPTER  8MPI: The parallel backbone\n Let’s talk a little bit about the terminology of halos and ghost cells. Even before the\nage of parallel processing, a region of cells surrounding the mesh was often used to\nimplement boundary conditions. These boundary conditions could be reflective,\ninflow, outflow, or periodic. For efficiency, programmers wanted to avoid if state-\nments in the main computational loop. To do this, they added cells surrounding the\nmesh and set those to appropriate values before the main computational loop. These\ncells had the appearance of a halo, so the name stuck. Halo cells  are any set of cells sur-\nrounding a computational mesh regardless of their purpose. A domain-boundary halo  is\nthen halo cells used for imposing a specific set of boundary conditions.\n Once applications were parallelized, a similar outer region of cells was added to\nhold values from the neighboring meshes. These cells are not real cells but only exist\nas an aid to reduce communication costs. Because these are not real, these were soon\ngiven the name ghost cells . The real data for a ghost cell is on the adjacent processor\nand the local copy is just a ghost value. The ghost cells also look like halos and are also\nreferred to as halo cells. Ghost cell updates  or exchanges  refer to the updating of the\nghost cells and are only needed for parallel, multi-process runs when you need\nupdates of real values from adjacent processes. \n The boundary conditions need to be done for both serial and parallel runs. Confu-\nsion exists because these operations are often referred to as halo updates , although it’s\nunclear exactly what is meant. In our terminology, halo updates  refers to both the\ndomain boundary updates and ghost cell updates. For optimizing MPI communica-\ntion, we only need to look at the ghost cell updates or exchanges and put aside the\nboundary conditions calculations for the present.\n Let’s now look at how to set up ghost cells for the borders of the local mesh on\neach process and perform the communication between the subdomains. By using\nghost cells, the needed communications are grouped into a fewer number of com-\nmunications than if a single communication is done every time a cell’s value is\nneeded from another process. This is the most common technique to make the data\nparallel approach perform well. In the implementations of the ghost cell updates,\nwe’ll demonstrate the use of the MPI_Pack  routine and load a communication buffer\nwith a simple cell-by-cell array assignment. In later sections, we’ll also see how to do\nthe same communication with MPI data types, using the MPI topology calls for setup\nand communication.\n Once we implement the ghost cell updates in a data parallel code, most of the\nneeded communication is handled. This isolates the code that provides the parallel-\nism into a small section of the application. This small section of the code is important\nto optimize for parallel efficiency. Let’s look at some implementations of this function-\nality, starting with the setup in listing 8.18 and the work done by the stencil loops in\nlisting 8.19. You may want to look at the full code in the GhostExchange/GhostEx-\nchange_Pack directory of the example code for the chapter at https:/ /github.com/\nEssentialsOfParallelComputing/Chapter8 .\n \n279 Data parallel examples\nGhostExchange/GhostExchange_Pack/GhostExchange.cc\n30    int imax = 2000, jmax = 2000;          \n31    int nprocx = 0, nprocy = 0;                      \n32    int nhalo = 2, corners = 0;                \n33    int do_timing;     \n      ....\n40    int xcoord = rank%nprocx;     \n41    int ycoord = rank/nprocx;     \n42 \n43    int nleft = (xcoord > 0       ) ?            \n                  rank - 1      : MPI_PROC_NULL;   \n44    int nrght = (xcoord < nprocx-1) ?            \n                  rank + 1      : MPI_PROC_NULL;   \n45    int nbot  = (ycoord > 0       ) ?            \n                  rank - nprocx : MPI_PROC_NULL;   \n46    int ntop  = (ycoord < nprocy-1) ?            \n                  rank + nprocx : MPI_PROC_NULL;   \n47 \n48    int ibegin = imax *(xcoord  )/nprocx;    \n49    int iend   = imax *(xcoord+1)/nprocx;    \n50    int isize  = iend - ibegin;              \n51    int jbegin = jmax *(ycoord  )/nprocy;    \n52    int jend   = jmax *(ycoord+1)/nprocy;    \n53    int jsize  = jend - jbegin;              \nWe do memory allocation for the local size plus room for the halos on each process.\nTo make the indexing a little simpler, we offset the memory indexing to start at -nhalo\nand end at isize+nhalo . The real cells then are always from 0 to isize-1 , regardless\nof the width of the halo. \n The following lines show a call to a special malloc2D  with two additional argu-\nments that offset the array addressing so that the real part of the array is from 0,0 to\njsize,isize . This is done with some pointer arithmetic that moves the starting loca-\ntion of each pointer.\n64    double** x    = malloc2D(jsize+2*nhalo, isize+2*nhalo, nhalo, nhalo);\n65    double** xnew = malloc2D(jsize+2*nhalo, isize+2*nhalo, nhalo, nhalo);\nWe use the simple stencil calculation from the blur operator introduced in figure 1.10\nto provide the work. Many applications have far more complex computations that\ntake much more time. The following listing shows the stencil calculation loops.\nGhostExchange/GhostExchange_Pack/GhostExchange.cc\n 91   for (int iter = 0; iter < 1000; iter++){     Listing 8.18 Setup for ghost cell exchanges in a 2D mesh\nListing 8.19 Work is done in a stencil iteration loopInput settings: -i <imax> -j\n<jmax> are the sizes of the grid.-x <nprocx> -y <nprocy> \nare the number of processes \nin x- and y-directions.\n-h <nhalo> -c is the \nnumber of halo cells and \n-c includes corner cells.-t do_timing \nsynchronizes timing.\nxcoord and ycoord of processes. \nRow index varies fastest.\nNeighbor rank \nfor each process \nfor neighbor \ncommunication\nSize of computational \ndomain for each \nprocess and the global \nbegin and end index\nIteration loop\n280 CHAPTER  8MPI: The parallel backbone\n 92      cpu_timer_start(&tstart_stencil);\n 93\n 94      for (int j = 0; j < jsize; j++){            \n 95         for (int i = 0; i < isize; i++){         \n 96           xnew[j][i]=                            \n                (x[j][i] + x[j][i-1] + x[j][i+1] +   \n                 x[j-1][i] + x[j+1][i])/5.0;         \n 97         }                                        \n 98      }                                           \n 99\n100      SWAP_PTR(xnew, x, xtmp);     \n101\n102      stencil_time += cpu_timer_stop(tstart_stencil);\n103\n104      boundarycondition_update(x, nhalo, jsize,\n          isize, nleft, nrght, nbot, ntop);\n105      ghostcell_update(x, nhalo, corners,         \n          jsize, isize, nleft, nrght, nbot, ntop);   \n106   }      \nNow we can look at the critical ghost cell update code. Figure 8.5 shows the required\noperation. The width of the ghost cell region can be one, two, or more cells in depth.\nThe corner cells may also be needed for some applications. Four processes (or\nranks) each need data from this rank; to the left, right, top, and bottom. Each of\nthese processes requires a separate communication and a separate data buffer. The\nwidth of the halo region varies in different applications, as well as whether the cor-\nner cells are needed. \n Figure 8.5 shows an example of a ghost cell exchange for a 4-by-4 mesh on nine\nprocesses with a one-cell-wide halo and the corners included. The outer boundary\nhalos are updated first and then a horizontal data exchange, a synchronization, and\nthe vertical data exchange. If corners are not needed, the horizontal and vertical\nexchanges can be done at the same time. If the corners are desired, a synchronization\nis necessary between the horizontal and vertical exchanges.\n A key observation of the ghost cell data updates is that in C, the row data is contig-\nuous, whereas the column data is separated by a stride that is the size of the row. Send-\ning individual values for the columns is expensive and so we need to group these\ntogether somehow. \n You can perform the ghost cell update with MPI in several ways. In this first version\nin listing 8.20, we’ll look at an implementation using the MPI_Pack  call to pack the col-\numn data. The row data is sent with just a standard MPI_Isend  call. The width of the\nghost cell region is specified by the nhalo  variable, and corners can be requested with\nthe proper input.\n \n \n \n Stencil \ncalculation\nPointer swap for old \nand new x arrays\nGhost cell update call \nrefreshes ghost cells.\nIteration loop\n281 Data parallel examples\n–1 0 1 2 3 4–101234\n–1 0 1 2 3 4 –1 0 1 2 3 4–101234–101234\n–101234–101234–101234\n–1 0 1 2 3 4 –1 0 1 2 3 4 –1 0 1 2 3 4\nFigure 8.5 The corner cell version of the ghost cell update first exchanges data to the left \nand right (on the top half of the figure), followed by a top and bottom exchange (on the bottom \nhalf of the figure). With care, the left and right exchange can be smaller with just the real cells \nplus the outer boundary cells, although there is no harm in making it the full vertical size of the \nmesh. The updating of the boundary cells surrounding the mesh is done separately.\n282 CHAPTER  8MPI: The parallel backbone\nGhostExchange/GhostExchange_Pack/GhostExchange.cc\n167 void ghostcell_update(double **x, int nhalo,     \n       int corners, int jsize, int isize,            \n168    int nleft, int nrght, int nbot, int ntop,     \n       int do_timing)                                \n169 {\n170    if (do_timing) MPI_Barrier(MPI_COMM_WORLD);\n171 \n172    struct timespec tstart_ghostcell;\n173    cpu_timer_start(&tstart_ghostcell);\n174 \n175    MPI_Request request[4*nhalo];\n176    MPI_Status status[4*nhalo];\n177 \n178    int jlow=0, jhgh=jsize;\n179    if (corners) {\n180       if (nbot == MPI_PROC_NULL) jlow = -nhalo;\n181       if (ntop == MPI_PROC_NULL) jhgh = jsize+nhalo;\n182    }\n183    int jnum = jhgh-jlow;\n184    int bufcount = jnum*nhalo;\n185    int bufsize = bufcount*sizeof(double);\n186 \n187    double xbuf_left_send[bufcount];\n188    double xbuf_rght_send[bufcount];\n189    double xbuf_rght_recv[bufcount];\n190    double xbuf_left_recv[bufcount];\n191 \n192    int position_left;                          \n193    int position_right;                         \n194    if (nleft != MPI_PROC_NULL){                \n195       position_left = 0;                       \n196       for (int j = jlow; j < jhgh; j++){       \n197         MPI_Pack(&x[j][0], nhalo, MPI_DOUBLE,  \n198           xbuf_left_send, bufsize,             \n              &position_left,  MPI_COMM_WORLD);    \n199       }                                        \n200    }                                           \n201 \n202    if (nrght != MPI_PROC_NULL){                \n203       position_right = 0;                      \n204       for (int j = jlow; j < jhgh; j++){       \n205         MPI_Pack(&x[j][isize-nhalo], nhalo,    \n              MPI_DOUBLE, xbuf_rght_send,          \n206           bufsize, &position_right,            \n              MPI_COMM_WORLD);                     \n207       }                                        \n208    }                                           \n209 Listing 8.20 Ghost cell update routine for 2D mesh with MPI_Pack\nThe update of the \nghost cells from \nadjacent processes\nPacks buffers for \nghost cell update \nfor left and right \nneighbors\n283 Data parallel examples\n210    MPI_Irecv(&xbuf_rght_recv, bufsize,       \n                 MPI_PACKED, nrght, 1001,        \n211              MPI_COMM_WORLD, &request[0]);   \n212    MPI_Isend(&xbuf_left_send, bufsize,       \n                 MPI_PACKED, nleft, 1001,        \n213              MPI_COMM_WORLD, &request[1]);   \n214 \n215    MPI_Irecv(&xbuf_left_recv, bufsize,       \n                 MPI_PACKED, nleft, 1002,        \n216              MPI_COMM_WORLD, &request[2]);   \n217    MPI_Isend(&xbuf_rght_send, bufsize,       \n                 MPI_PACKED, nrght, 1002,        \n218              MPI_COMM_WORLD, &request[3]);   \n219    MPI_Waitall(4, request, status);          \n220 \n221    if (nrght != MPI_PROC_NULL){               \n222       position_right = 0;                     \n223       for (int j = jlow; j < jhgh; j++){      \n224         MPI_Unpack(xbuf_rght_recv, bufsize,   \n              &position_right, &x[j][isize],      \n225           nhalo, MPI_DOUBLE, MPI_COMM_WORLD); \n226       }                                       \n227    }                                          \n228 \n229    if (nleft != MPI_PROC_NULL){               \n230       position_left = 0;                      \n231       for (int j = jlow; j < jhgh; j++){      \n232         MPI_Unpack(xbuf_left_recv, bufsize,   \n              &position_left,  &x[j][-nhalo],     \n233           nhalo, MPI_DOUBLE, MPI_COMM_WORLD); \n234       }                                       \n235    }                                          \n236\n237    if (corners) {\n238       bufcount = nhalo*(isize+2*nhalo);\n239       MPI_Irecv(&x[jsize][-nhalo],           \n             bufcount, MPI_DOUBLE, ntop, 1001,   \n240          MPI_COMM_WORLD, &request[0]);       \n241       MPI_Isend(&x[0    ][-nhalo],           \n             bufcount, MPI_DOUBLE, nbot, 1001,   \n242            MPI_COMM_WORLD, &request[1]);     \n243\n244       MPI_Irecv(&x[     -nhalo][-nhalo],     \n             bufcount, MPI_DOUBLE, nbot, 1002,   \n245          MPI_COMM_WORLD, &request[2]);       \n246       MPI_Isend(&x[jsize-nhalo][-nhalo],     \n             bufcount, MPI_DOUBLE,ntop, 1002,    \n247          MPI_COMM_WORLD, &request[3]);       \n248       MPI_Waitall(4, request, status);   \n249    } else {Communication \nfor left and right \nneighbors\nUnpacks buffers \nfor left and right \nneighbors\nGhost cell updates \nin one contiguous \nblock for bottom \nand top neighbors\nWaits for all \ncommunication \nto complete\n284 CHAPTER  8MPI: The parallel backbone\n250       for (int j = 0; j<nhalo; j++){             \n251          MPI_Irecv(&x[jsize+j][0],               \n               isize, MPI_DOUBLE, ntop, 1001+j*2,    \n252            MPI_COMM_WORLD, &request[0+j*4]);     \n253          MPI_Isend(&x[0+j    ][0],               \n               isize, MPI_DOUBLE, nbot, 1001+j*2,    \n254            MPI_COMM_WORLD, &request[1+j*4]);     \n255\n256          MPI_Irecv(&x[     -nhalo+j][0],         \n               isize, MPI_DOUBLE, nbot, 1002+j*2,    \n257            MPI_COMM_WORLD, &request[2+j*4]);     \n258          MPI_Isend(&x[jsize-nhalo+j][0],         \n               isize, MPI_DOUBLE, ntop, 1002+j*2,    \n259            MPI_COMM_WORLD, &request[3+j*4]);     \n260       }                                          \n261       MPI_Waitall(4*nhalo, request, status);     \n262    }\n263 \n264    if (do_timing) MPI_Barrier(MPI_COMM_WORLD);\n265 \n266    ghostcell_time += cpu_timer_stop(tstart_ghostcell);\n267 }\nThe MPI_Pack  call is particularly useful when there are multiple data types that need\nto be communicated in the ghost update. The values are packed into a type-agnostic\nbuffer and then unpacked on the other side. The neighbor communication in the ver-\ntical direction is done with contiguous row data. When there are corners included, a\nsingle buffer works well. Without corners, individual halo rows are sent. There are\nusually only one or two halo cells, so this is a reasonable approach.\n Another way to load the buffers for the communication is with an array assign-\nment. Array assignments are a good approach when there is a single, simple data type\nlike the double-precision float type used in this example. The following listing shows\nthe code for replacing the MPI_Pack  loops with array assignments.\nGhostExchange/GhostExchange_ArrayAssign/GhostExchange.cc\n190    int icount;\n191    if (nleft != MPI_PROC_NULL){                     \n192       icount = 0;                                   \n193       for (int j = jlow; j < jhgh; j++){            \n194          for (int ll = 0; ll < nhalo; ll++){        \n195             xbuf_left_send[icount++] = x[j][ll];    \n196          }                                          \n197       }                                             \n198    }                                                \n199    if (nrght != MPI_PROC_NULL){                     \n200       icount = 0;                                   \n201       for (int j = jlow; j < jhgh; j++){            \n202          for (int ll = 0; ll < nhalo; ll++){        Listing 8.21 Ghost cell update routine for 2D mesh with array assignmentsGhost cell updates \none row at a time \nfor bottom and \ntop neighbors\nWaits for all \ncommunication \nto complete\nFills the send \nbuffers",21787
105-8.5.1 Using custom MPI data types for performance and code simplification.pdf,105-8.5.1 Using custom MPI data types for performance and code simplification,"285 Data parallel examples\n203             xbuf_rght_send[icount++] =              \n                   x[j][isize-nhalo+ll];                \n204          }                                          \n205       }                                             \n206    }                                                \n207 \n208    MPI_Irecv(&xbuf_rght_recv, bufcount,      \n                 MPI_DOUBLE, nrght, 1001,        \n209              MPI_COMM_WORLD, &request[0]);   \n210    MPI_Isend(&xbuf_left_send, bufcount,      \n                 MPI_DOUBLE, nleft, 1001,        \n211              MPI_COMM_WORLD, &request[1]);   \n212    \n213    MPI_Irecv(&xbuf_left_recv, bufcount,      \n                 MPI_DOUBLE, nleft, 1002,        \n214              MPI_COMM_WORLD, &request[2]);   \n215    MPI_Isend(&xbuf_rght_send, bufcount,      \n                 MPI_DOUBLE, nrght, 1002,        \n216              MPI_COMM_WORLD, &request[3]);   \n217    MPI_Waitall(4, request, status);          \n218    \n219    if (nrght != MPI_PROC_NULL){                \n220       icount = 0;                              \n221       for (int j = jlow; j < jhgh; j++){       \n222          for (int ll = 0; ll < nhalo; ll++){   \n223             x[j][isize+ll] =                   \n                   xbuf_rght_recv[icount++];       \n224          }                                     \n225       }                                        \n226    }                                           \n227    if (nleft != MPI_PROC_NULL){                \n228       icount = 0;                              \n229       for (int j = jlow; j < jhgh; j++){       \n230          for (int ll = 0; ll < nhalo; ll++){   \n231             x[j][-nhalo+ll] =                  \n                   xbuf_left_recv[icount++];       \n232          }                                     \n233       }                                        \n234    }                                           \nThe MPI_Irecv  and MPI_Isend  calls now use a count and the MPI_DOUBLE  data type\nrather than the generic byte type of MPI_Pack . We also need to know the data type for\ncopying data into and out of the communication buffer.\n8.4.3 Ghost cell exchanges in a three-dimensional (3D) \nstencil calculation\nYou can also do a ghost cell exchange for a 3D stencil calculation. We’ll do that in list-\ning 8.22. The setup is a little more complicated, however. The process layout is first\ncalculated as xcoord , ycoord , and zcoord  values. Then the neighbors are determined,\nand the sizes of the data on each processor calculated.\n Fills the send \nbuffers\nPerforms the \ncommunication \nbetween left and \nright neighbors\nCopies the \nreceive buffers \ninto the ghost \ncells\n286 CHAPTER  8MPI: The parallel backbone\nGhostExchange/GhostExchange3D_*/GhostExchange.cc\n 63    int xcoord = rank%nprocx;            \n 64    int ycoord = rank/nprocx%nprocy;     \n 65    int zcoord = rank/(nprocx*nprocy);   \n 66 \n 67    int nleft = (xcoord > 0       ) ?             \n          rank - 1      : MPI_PROC_NULL;             \n 68    int nrght = (xcoord < nprocx-1) ?             \n          rank + 1      : MPI_PROC_NULL;             \n 69    int nbot  = (ycoord > 0       ) ?             \n          rank - nprocx : MPI_PROC_NULL;             \n 70    int ntop  = (ycoord < nprocy-1) ?             \n          rank + nprocx : MPI_PROC_NULL;             \n 71    int nfrnt = (zcoord > 0       ) ?             \n          rank - nprocx * nprocy : MPI_PROC_NULL;    \n 72    int nback = (zcoord < nprocz-1) ?             \n          rank + nprocx * nprocy : MPI_PROC_NULL;    \n 73 \n 74    int ibegin = imax *(xcoord  )/nprocx;         \n 75    int iend   = imax *(xcoord+1)/nprocx;         \n 76    int isize  = iend - ibegin;                   \n 77    int jbegin = jmax *(ycoord  )/nprocy;         \n 78    int jend   = jmax *(ycoord+1)/nprocy;         \n 79    int jsize  = jend - jbegin;                   \n 80    int kbegin = kmax *(zcoord  )/nprocz;         \n 81    int kend   = kmax *(zcoord+1)/nprocz;         \n 82    int ksize  = kend - kbegin;                   \nThe ghost cell update, including the array copies into buffers, communication, and\ncopying out is a couple of hundred lines long and can’t be shown here. Refer to the\ncode examples ( https:/ /github.com/EssentialsofParallelComputing/Chapter8 ) that\naccompany the chapter for the detailed implementation. We’ll show an MPI data type\nversion of the ghost cell update in section 8.5.1.\n8.5 Advanced MPI functionality to simplify code \nand enable optimizations\nThe excellent design of MPI becomes apparent as we see how basic MPI components\ncan be combined into higher-level functionality. We got a taste of this in section 8.3.3,\nwhen we created a new double-double type and a new reduction operator. This exten-\nsibility gives MPI important capabilities. We’ll look at a couple of these advanced func-\ntions that are useful in common data parallel applications. These include\nMPI custom data types —Builds new data types from the basic MPI type building\nblocks.\nTopology support —A basic Cartesian regular grid topology and a more general\ngraph topology are both available. We’ll just look at the simpler MPI Cartesian\nfunctions.Listing 8.22 Setup for a 3D mesh\nSets up the process \ncoordinates\nCalculates \nthe neighbor \nprocesses for \neach process\nCalculates the \nbeginning and \nending index for \neach process and \nthen the size\n287 Advanced MPI functionality to simplify code and enable optimizations\n8.5.1 Using custom MPI data types for performance \nand code simplification\nMPI has a rich set of functions to create new, custom MPI data types from the basic\nMPI types. This allows the encapsulation of complex data into a single custom data\ntype that you can use in communication calls. As a result, a single communication call\ncan send or receive many smaller pieces of data as a unit. Here is a list of some of the\nMPI data type creation functions:\nMPI_Type_contiguous —Makes a block of contiguous data into a type.\nMPI_Type_vector —Creates a type out of blocks of strided data. \nMPI_Type_create_subarray —Creates a rectangular subset of a larger array.\nMPI_Type_indexed  or MPI_Type_create_hindexed —Creates an irregular set of\nindices described by a set of block lengths and displacements. The hindexed\nversion expresses the displacements in bytes instead of a data type for more\ngenerality.\nMPI_Type_create_struct —Creates a data type encapsulating the data items in\na structure in a portable way that accounts for padding by the compiler.\nYou’ll find a visual illustration to be helpful in understanding some of these data types.\nFigure 8.6 shows some of the simpler and more commonly used functions including\nMPI_Type_  contiguous , MPI_Type_vector , and MPI_Type_create_subarray .\nOnce a data type is described and made into a new data type, it must be initialized\nbefore it is used. For this purpose, there are a couple of additional routines to commit\nand free the types. A type must be committed before use and it must be freed to avoid\na memory leak. The routines include\nMPI_Type_Commit —Initializes the new custom type with needed memory alloca-\ntion or other setup\nMPI_Type_Free —Frees any memory or data structure entries from the creation\nof the data typeCountMPI_Type_contiguous\nBlock\nlengthMPI_Type_vector\nStride\n12 3\nCountMPI_Type_create_subarray\nStart sizes = {2,2}\nArray sizes = {7,10}\nSubsizes = {3,4}0123456\n0123456789\nFigure 8.6 Three MPI custom data types with illustrations of the arguments used in \ntheir creation\n288 CHAPTER  8MPI: The parallel backbone\nWe can greatly simplify the ghost cell communication by defining a custom MPI data\ntype as was shown in figure 8.6 to represent the column of data and to avoid the\nMPI_Pack  calls. By defining an MPI data type, an extra data copy can be avoided. The\ndata can be copied from its regular location straight into the MPI send buffers. Let’s\nsee how this is done in listing 8.23. Listing 8.24 shows the second part of the program. \n We first set up the custom data types. We use the MPI_Type_vector  call for sets of\nstrided array accesses. For the contiguous data for the vertical type when we include\ncorners, we use the MPI_Type_contiguous  call, and in lines 139 and 140, we free the\ndata type at the end before the MPI_Finalize .\nGhostExchange/GhostExchange_VectorTypes/GhostExchange.cc\n 56    int jlow=0, jhgh=jsize;\n 57    if (corners) {\n 58       if (nbot == MPI_PROC_NULL) jlow = -nhalo;\n 59       if (ntop  == MPI_PROC_NULL) jhgh = jsize+nhalo;\n 60    }  \n 61    int jnum = jhgh-jlow;\n 62 \n 63    MPI_Datatype horiz_type;\n 64    MPI_Type_vector(jnum, nhalo, isize+2*nhalo,\n                       MPI_DOUBLE, &horiz_type);\n 65    MPI_Type_commit(&horiz_type);\n 66 \n 67    MPI_Datatype vert_type;\n 68    if (! corners){\n 69       MPI_Type_vector(nhalo, isize, isize+2*nhalo,\n                          MPI_DOUBLE, &vert_type);\n 70    } else {\n 71       MPI_Type_contiguous(nhalo*(isize+2*nhalo),\n                              MPI_DOUBLE, &vert_type);\n 72    }\n 73    MPI_Type_commit(&vert_type);\n...\n139    MPI_Type_free(&horiz_type);\n140    MPI_Type_free(&vert_type);\nYou can then write the ghostcell_update  more concisely and with better perfor-\nmance using the MPI data types as in the following listing. If we need to update cor-\nners, a synchronization is needed between the two communication passes.\nGhostExchange/GhostExchange_VectorTypes/GhostExchange.cc\n197    int jlow=0, jhgh=jsize, ilow=0, waitcount=8, ib=4;\n198    if (corners) {\n199       if (nbot == MPI_PROC_NULL) jlow = -nhalo;\n200       ilow = -nhalo;\n201       waitcount = 4;\n202       ib = 0;Listing 8.23 Creating a 2D vector data type for the ghost cell update\nListing 8.24 2D ghost cell update routine using the vector data type\n289 Advanced MPI functionality to simplify code and enable optimizations\n203    }\n204 \n205    MPI_Request request[waitcount];\n206    MPI_Status status[waitcount];\n207 \n208    MPI_Irecv(&x[jlow][isize], 1,        \n          horiz_type, nrght, 1001,          \n209       MPI_COMM_WORLD, &request[0]);     \n210    MPI_Isend(&x[jlow][0],     1,        \n          horiz_type, nleft, 1001,          \n211       MPI_COMM_WORLD, &request[1]);     \n212\n213    MPI_Irecv(&x[jlow][-nhalo],      1,  \n          horiz_type, nleft, 1002,          \n214       MPI_COMM_WORLD, &request[2]);     \n215    MPI_Isend(&x[jlow][isize-nhalo], 1,  \n          horiz_type, nrght, 1002,          \n216       MPI_COMM_WORLD, &request[3]);     \n217 \n218    if (corners)                          \n          MPI_Waitall(4, request, status);   \n219 \n220    MPI_Irecv(&x[jsize][ilow],   1,      \n          vert_type, ntop, 1003,            \n221       MPI_COMM_WORLD, &request[ib+0]);  \n222    MPI_Isend(&x[0    ][ilow],   1,      \n          vert_type, nbot, 1003,            \n223       MPI_COMM_WORLD, &request[ib+1]);  \n224 \n225    MPI_Irecv(&x[     -nhalo][ilow], 1,  \n          vert_type, nbot, 1004,            \n226       MPI_COMM_WORLD, &request[ib+2]);  \n227    MPI_Isend(&x[jsize-nhalo][ilow], 1,  \n          vert_type, ntop, 1004,            \n228       MPI_COMM_WORLD, &request[ib+3]);  \n229 \n230    MPI_Waitall(waitcount, request, status);\nThe reason for using MPI data types is usually given as better performance. It does\nallow MPI implementation to avoid an extra copy in some cases. But from our per-\nspective, the biggest reason for MPI data types is the cleaner, simpler code and fewer\nopportunities for bugs.\n The 3D version using MPI data types is a little more complicated. We use MPI_\nType_create_subarray  in the following listing to create three custom MPI data types\nto be used in the communication. \nGhostExchange/GhostExchange3D_VectorTypes/GhostExchange.cc   \n109    int array_sizes[] = {ksize+2*nhalo, jsize+2*nhalo, isize+2*nhalo};\n110    if (corners) {Listing 8.25 Creating an MPI subarray data type for 3D ghost cellsSend left and right \nusing the custom \nhoriz_type MPI \ndata type\nSynchronize if \ncorners are sent.\nUpdates ghost \ncells on top \nand bottom\n290 CHAPTER  8MPI: The parallel backbone\n111       int subarray_starts[] = {0, 0, 0};         \n112       int hsubarray_sizes[] =                    \n             {ksize+2*nhalo, jsize+2*nhalo,          \n              nhalo};                                \n113       MPI_Type_create_subarray(3,                \n             array_sizes, hsubarray_sizes,           \n114          subarray_starts, MPI_ORDER_C,           \n             MPI_DOUBLE, &horiz_type);               \n115\n116       int vsubarray_sizes[] =                 \n             {ksize+2*nhalo, nhalo,               \n              isize+2*nhalo};                     \n117       MPI_Type_create_subarray(3,             \n             array_sizes, vsubarray_sizes,        \n118          subarray_starts, MPI_ORDER_C,        \n             MPI_DOUBLE, &vert_type);             \n119\n120       int dsubarray_sizes[] =              \n             {nhalo, jsize+2*nhalo,            \n              isize+2*nhalo};                  \n121       MPI_Type_create_subarray(3,          \n             array_sizes, dsubarray_sizes,     \n122          subarray_starts, MPI_ORDER_C,     \n             MPI_DOUBLE, &depth_type);         \n123    } else {\n124       int hsubarray_starts[] = {nhalo,nhalo,0};  \n125       int hsubarray_sizes[] = {ksize, jsize,     \n                                   nhalo};           \n126       MPI_Type_create_subarray(3,                \n             array_sizes, hsubarray_sizes,           \n127          hsubarray_starts, MPI_ORDER_C,          \n             MPI_DOUBLE, &horiz_type);               \n128\n129       int vsubarray_starts[] = {nhalo, 0,     \n                                    nhalo};       \n130       int vsubarray_sizes[] = {ksize, nhalo,  \n                                   isize};        \n131       MPI_Type_create_subarray(3,             \n             array_sizes, vsubarray_sizes,        \n132          vsubarray_starts, MPI_ORDER_C,       \n             MPI_DOUBLE, &vert_type);             \n133\n134       int dsubarray_starts[] = {0, nhalo,    \n                                    nhalo};      \n135       int dsubarray_sizes[] = {nhalo, ksize, \n                                   isize};       \n136       MPI_Type_create_subarray(3,            \n             array_sizes, dsubarray_sizes,       \n137          dsubarray_starts, MPI_ORDER_C,      \n             MPI_DOUBLE, &depth_type);           \n138    }\n139 \n140    MPI_Type_commit(&horiz_type);\n141    MPI_Type_commit(&vert_type);\n142    MPI_Type_commit(&depth_type);Creates a horizontal \ndata type using \nMPI_Type_create_\nsubarray\nCreates a\nvertical data\ntype using\nMPI_Type\n_create\n_subarrayCreates a \ndepth data \ntype using \nMPI_Type\n_create\n_subarray\nCreates a \ndepth data \ntype using \nMPI_Type\n_create\n_subarray\n291 Advanced MPI functionality to simplify code and enable optimizations\nThe following listing shows that the communication routine using these three MPI\ndata types is pretty concise.\nGhostExchange/GhostExchange3D_VectorTypes/GhostExchange.cc   \n334    int waitcount = 12, ib1 = 4, ib2 = 8;\n335    if (corners) {\n336       waitcount=4;\n337       ib1 = 0, ib2 = 0;\n338    }\n339 \n340    MPI_Request request[waitcount*nhalo];\n341    MPI_Status status[waitcount*nhalo];\n342 \n343    MPI_Irecv(&x[-nhalo][-nhalo][isize],   1,  \n                 horiz_type, nrght, 1001,         \n344              MPI_COMM_WORLD, &request[0]);    \n345    MPI_Isend(&x[-nhalo][-nhalo][0],       1,  \n                 horiz_type, nleft, 1001,         \n346              MPI_COMM_WORLD, &request[1]);    \n347    \n348    MPI_Irecv(&x[-nhalo][-nhalo][-nhalo],  1,  \n                 horiz_type, nleft, 1002,         \n349              MPI_COMM_WORLD, &request[2]);    \n350    MPI_Isend(&x[-nhalo][-nhalo][isize-1], 1,  \n                 horiz_type, nrght, 1002,         \n351              MPI_COMM_WORLD, &request[3]);    \n352    if (corners)                            \n          MPI_Waitall(4, request, status);     \n353   \n354    MPI_Irecv(&x[-nhalo][jsize][-nhalo],   1,    \n                 vert_type, ntop, 1003,             \n355              MPI_COMM_WORLD, &request[ib1+0]);  \n356    MPI_Isend(&x[-nhalo][0][-nhalo],       1,    \n                 vert_type, nbot, 1003,             \n357              MPI_COMM_WORLD, &request[ib1+1]);  \n358    \n359    MPI_Irecv(&x[-nhalo][-nhalo][-nhalo],  1,    \n                 vert_type, nbot, 1004,             \n360              MPI_COMM_WORLD, &request[ib1+2]);  \n361    MPI_Isend(&x[-nhalo][jsize-1][-nhalo], 1,    \n                 vert_type, ntop, 1004,             \n362              MPI_COMM_WORLD, &request[ib1+3]);  \n363    if (corners)                            \n          MPI_Waitall(4, request, status);     \n364    \n365    MPI_Irecv(&x[ksize][-nhalo][-nhalo],   1,    \n                 depth_type, nback, 1005,           \n366              MPI_COMM_WORLD, &request[ib2+0]);  \n367    MPI_Isend(&x[0][-nhalo][-nhalo],       1,    \n                 depth_type, nfrnt, 1005,           \n368              MPI_COMM_WORLD, &request[ib2+1]);  \n369    Listing 8.26 The 3D ghost cell update using MPI data types \nGhost cell \nupdate for the \nhorizontal \ndirection.\nSynchronize \nif corners are \nneeded in the \nupdate.Ghost cell \nupdate for \nthe vertical \ndirection.\nGhost cell \nupdate for the \ndepth direction.",17670
106-8.5.2 Cartesian topology support in MPI.pdf,106-8.5.2 Cartesian topology support in MPI,"292 CHAPTER  8MPI: The parallel backbone\n370    MPI_Irecv(&x[-nhalo][-nhalo][-nhalo],  1,      \n                 depth_type, nfrnt, 1006,             \n371              MPI_COMM_WORLD, &request[ib2+2]);    \n372    MPI_Isend(&x[ksize-1][-nhalo][-nhalo], 1,      \n                 depth_type, nback, 1006,             \n373              MPI_COMM_WORLD, &request[ib2+3]);    \n374    MPI_Waitall(waitcount, request, status);       \n8.5.2 Cartesian topology support in MPI\nIn this section, we’ll show you how the topology functions in MPI work. The opera-\ntion is still the ghost exchange shown in figure 8.5, but we can simplify the coding by\nusing Cartesian functions. Not covered are general graph functions for unstruc-\ntured applications. We’ll start with the setup routines before moving on to the com-\nmunication routines. \n The setup routines need to set the values for the process grid assignments and\nthen to set the neighbors as was done in listings 8.18 and 8.22. As shown in listing 8.24\nfor 2D and listing 8.25 for 3D, the process sets the dims  array to the number of proces-\nsors to use in each dimension. If any of the values in the dims  array are zero, the\nMPI_Dims_create  function calculates some values that will work. Note that the num-\nber of processes in each direction does not take into account the mesh size and may\nnot produce good values for long, narrow problems. Consider the case of a mesh that\nis 8x8x1000 and give it 8 processors; the process grid will be 2x2x2, resulting in a mesh\ndomain of 4x4x500 on each process.\n MPI_Cart_create  takes the resulting dims  array and an input array, periodic ,\nthat declares whether a boundary wraps to the opposite side and vice-versa. The last\nargument is the reorder argument that lets MPI reorder processes. It is zero (false)\nin this example. Now we have a new communicator that contains information about\nthe topology.\n Getting the process grid layout is just a call to MPI_Cart_coords . Getting neighbors\nis done with a call to MPI_Cart_shift  with the second argument specifying the direc-\ntion and the third argument the displacement or number of processes in that direc-\ntion. The output is the ranks of the adjacent processors.\nGhostExchange/CartExchange_Neighbor/CartExchange.cc\n43 int dims[2] = {nprocy, nprocx};\n44 int periodic[2]={0,0};\n45 int coords[2];\n46 MPI_Dims_create(nprocs, 2, dims);\n47 MPI_Comm cart_comm;\n48 MPI_Cart_create(MPI_COMM_WORLD, 2, dims, periodic, 0, &cart_comm);\n49 MPI_Cart_coords(cart_comm, rank, 2, coords);\n50\n51 int nleft, nrght, nbot, ntop;\n52 MPI_Cart_shift(cart_comm, 1, 1, &nleft, &nrght);\n53 MPI_Cart_shift(cart_comm, 0, 1, &nbot,  &ntop);Listing 8.27 2D Cartesian topology support in MPIGhost cell \nupdate for the \ndepth direction.\n293 Advanced MPI functionality to simplify code and enable optimizations\nThe 3D Cartesian topology setup is similar but with three dimensions as the following\nlisting shows.\nGhostExchange/CartExchange3D_Neighbor/CartExchange.cc\n65 int dims[3] = {nprocz, nprocy, nprocx};\n66 int periods[3]={0,0,0};\n67 int coords[3];\n68 MPI_Dims_create(nprocs, 3, dims);\n69 MPI_Comm cart_comm;\n70 MPI_Cart_create(MPI_COMM_WORLD, 3, dims, periods, 0, &cart_comm);\n71 MPI_Cart_coords(cart_comm, rank, 3, coords);\n72 int xcoord = coords[2];\n73 int ycoord = coords[1];\n74 int zcoord = coords[0];\n75 \n76 int nleft, nrght, nbot, ntop, nfrnt, nback;\n77 MPI_Cart_shift(cart_comm, 2, 1, &nleft, &nrght);\n78 MPI_Cart_shift(cart_comm, 1, 1, &nbot,  &ntop);\n79 MPI_Cart_shift(cart_comm, 0, 1, &nfrnt, &nback);\nIf we compare this code to the versions in listing 8.19 and 8.23, we see that the topol-\nogy functions do not save a lot of lines of code or greatly reduce the programming\ncomplexity in this relatively simple example for the setup. We can also leverage the\nCartesian communicator created in line 70 of listing 8.28 to do the neighbor commu-\nnication as well. That is where the greatest reduction of lines of code is seen. The MPI\nfunction has the following arguments:\nint MPI_Neighbor_alltoallw(const void *sendbuf,\n                           const int sendcounts[],\n                           const MPI_Aint sdispls[],\n                           const MPI_Datatype sendtypes[],\n                           void *recvbuf,\n                           const int recvcounts[],\n                           const MPI_Aint rdispls[],\n                           const MPI_Datatype recvtypes[],\n                           MPI_Comm comm)\nThere are a lot of arguments in the neighbor call, but once we get these all set up, the\ncommunication is concise and done in a single statement. We’ll go over all the argu-\nments in detail because these can be difficult to get right.\n The neighbor communication call can use either a filled buffer for the sends and\nreceives or do the operation in place. We’ll show the in-place method. The send\nand receive buffers are the 2D x array. We will use an MPI data type to describe the\ndata block, so the counts will be an array with the value of one for all four Cartesian\nsides for 2D or six sides for 3D. The order of the communication for the sides is bot-\ntom, top, left, right for 2D and front, back, bottom, top, left, right for 3D, and is the\nsame for both send and receive types. Listing 8.28 3D Cartesian topology support in MPI\n294 CHAPTER  8MPI: The parallel backbone\n The data block is different for each direction: horizontal, vertical, and depth. We\nuse the convention of standard perspective drawings with x going to the right, y\nupwards, and z (the depth) going back into the page. But within each direction, the\ndata block is the same but with different displacements to the start of the data block.\nThe displacements are in bytes, which is why you will see the offsets multiplied by 8,\nthe data type size of a double-precision value. Now let’s look at how all this gets put\ninto code for the setup of the communication for the 2D case in the following listing.\nGhostExchange/CartExchange_Neighbor/CartExchange.c\n55    int ibegin = imax *(coords[1]  )/dims[1];    \n56    int iend   = imax *(coords[1]+1)/dims[1];    \n57    int isize  = iend - ibegin;                  \n58    int jbegin = jmax *(coords[0]  )/dims[0];    \n59    int jend   = jmax *(coords[0]+1)/dims[0];    \n60    int jsize  = jend - jbegin;                  \n61 \n62    int jlow=nhalo, jhgh=jsize+nhalo,                    \n          ilow=nhalo, inum = isize;                        \n63    if (corners) {                                       \n64       int ilow = 0, inum = isize+2*nhalo;               \n65       if (nbot == MPI_PROC_NULL) jlow = 0;              \n66       if (ntop == MPI_PROC_NULL) jhgh = jsize+2*nhalo;  \n67    }                                                    \n68    int jnum = jhgh-jlow;                                \n69 \n70    int array_sizes[] = {jsize+2*nhalo, isize+2*nhalo};\n71 \n72    int subarray_sizes_x[] = {jnum, nhalo};     \n73    int subarray_horiz_start[] = {jlow, 0};     \n74    MPI_Datatype horiz_type;                    \n75    MPI_Type_create_subarray (2, array_sizes,   \n         subarray_sizes_x, subarray_horiz_start,  \n76       MPI_ORDER_C, MPI_DOUBLE, &horiz_type);   \n77    MPI_Type_commit(&horiz_type);               \n78    \n79    int subarray_sizes_y[] = {nhalo, inum};     \n80    int subarray_vert_start[] = {0, jlow};      \n81    MPI_Datatype vert_type;                     \n82    MPI_Type_create_subarray (2, array_sizes,   \n         subarray_sizes_y, subarray_vert_start,   \n83       MPI_ORDER_C, MPI_DOUBLE, &vert_type);    \n84    MPI_Type_commit(&vert_type);                \n85    \n86    MPI_Aint sdispls[4] = {\n         nhalo  *(isize+2*nhalo)*8,       \n87       jsize  *(isize+2*nhalo)*8,    \n88       nhalo  *8,                \n89       isize  *8};            \n90    MPI_Aint rdispls[4] = {\n         0,      Listing 8.29 2D Cartesian neighbor communication setup\nCalculates the \nglobal begin and \nend indices and the \nlocal array size\nIncludes the corner \nvalues if these are \nrequested\nCreates the data block \nto communicate in the \nhorizontal direction using \nthe subarray function\nCreates the data block \nto communicate in the \nvertical direction using \nthe subarray functionBottom\nrow is\nnhalo\nabove\nstart.Displacements are from bottom left \ncorner of memory block in bytes.\nTop row\nis jsize\nabove\nstart.Left column is nhalo right of start.\nRight column is isize \nright of start.\nBottom ghost row is 0 above start.\n295 Advanced MPI functionality to simplify code and enable optimizations\n91       (jsize+nhalo)  *(isize+2*nhalo)*8,      \n92       0,                        \n93       (isize+nhalo)*8};                      \n94    MPI_Datatype sendtypes[4] = {vert_type,   \n         vert_type, horiz_type, horiz_type};    \n95    MPI_Datatype recvtypes[4] = {vert_type,   \n         vert_type, horiz_type, horiz_type};    \nThe setup for the 3D Cartesian neighbor communication uses the MPI data types\nfrom listing 8.25. The data types define the block of data to be moved, but we need to\ndefine the offset in bytes to the start location of the data block for the send and\nreceive. We also need to define the arrays for the sendtypes  and recvtypes  in the\nproper order as in the next listing.\nGhostExchange/CartExchange3D_Neighbor/CartExchange.c\n154    int xyplane_mult = (jsize+2*nhalo)*(isize+2*nhalo)*8;\n155    int xstride_mult = (isize+2*nhalo)*8;\n156    MPI_Aint sdispls[6] = {\n          nhalo  *xyplane_mult,      \n157       ksize  *xyplane_mult,    \n158       nhalo  *xstride_mult,    \n159       jsize  *xstride_mult,    \n160       nhalo  *8,            \n161       isize  *8};   \n162    MPI_Aint rdispls[6] = {\n          0,                            \n163       (ksize+nhalo)  *xyplane_mult,   \n164       0,     \n165       (jsize+nhalo)  *xstride_mult,   \n166       0,                            \n167       (isize+nhalo)*8};           \n168    MPI_Datatype sendtypes[6] = {    \n          depth_type, depth_type,       \n          vert_type, vert_type,         \n          horiz_type, horiz_type};      \n169    MPI_Datatype recvtypes[6] = {    \n          depth_type, depth_type,       \n          vert_type, vert_type,         \n          horiz_type, horiz_type};      \nThe actual communication is done with a single call to the MPI_Neighbor_alltoallw\nas shown in listing 8.31. There is also a second block of code for the corner cases that\nrequires a couple of calls with a synchronization in between to ensure the corners are\nproperly filled. The first call does only the horizontal direction and then waits for\ncompletion before doing the vertical direction.\n Listing 8.30 3D Cartesian neighbor communication setupTop ghost row is \njsize+nhalo above start.\nLeft ghost column \nis 0 right of start.Right ghost row is \njsize+nhalo right \nof start.\nSend types are ordered \nbottom, top, left, and \nright neighbors.Receive types are ordered bottom,\ntop, left, and right neighbors.\nDisplacements are from bottom left \ncorner of memory block in bytes.Front is\nnhalo\nbehind\nfront.\nBack is\nksize\nbehind\nfront.Bottom row is nhalo above start.\nTop row is jsize above start.\nLeft column is nhalo right of start.\nRight\ncolumn\nis isize.Front ghost is 0 from front.\nBack ghost is ksize+nhalo behind front.\nBottom ghost row is 0 above start.\nTop ghost row is jsize+nhalo above start.\nLeft ghost column is 0 right of start.\nRight ghost row is jsize+nhalo \nright of start.Send and\nreceive types\nare ordered\nfront, back,\nbottom, top,\nleft, and\nright.\n296 CHAPTER  8MPI: The parallel backbone\nGhostExchange/CartExchange_Neighbor/CartExchange.c\n224 if (corners) {\n225    int counts1[4] = {0, 0, 1, 1};     \n226    MPI_Neighbor_alltoallw (           \n          &x[-nhalo][-nhalo], counts1,    \n             sdispls, sendtypes,          \n227          &x[-nhalo][-nhalo], counts1, \n             rdispls, recvtypes,          \n228       cart_comm);                     \n229 \n230    int counts2[4] = {1, 1, 0, 0};   \n231    MPI_Neighbor_alltoallw (         \n          &x[-nhalo][-nhalo], counts2,  \n             sdispls, sendtypes,      \n232       &x[-nhalo][-nhalo], counts2,  \n             rdispls, recvtypes,      \n233       cart_comm);                 \n234 } else {\n235    int counts[4] = {1, 1, 1, 1};     \n236    MPI_Neighbor_alltoallw (         \n          &x[-nhalo][-nhalo], counts,   \n             sdispls, sendtypes,        \n237       &x[-nhalo][-nhalo], counts,   \n             rdispls, recvtypes,        \n238       cart_comm);                   \n239 }\nThe 3D Cartesian neighbor communication is similar but with the addition of the z\ncoordinate (depth). The depth comes first in the counts and types arrays. In the\nphased communication for corners, the depth comes after horizontal and vertical\nghost cell exchanges as the next listing shows.\nGhostExchange/CartExchange3D_Neighbor/CartExchange.c\n346 if (corners) {\n347    int counts1[6] = {0, 0, 0, 0, 1, 1};      \n348    MPI_Neighbor_alltoallw(                   \n          &x[-nhalo][-nhalo][-nhalo], counts1,   \n             sdispls, sendtypes,                 \n349       &x[-nhalo][-nhalo][-nhalo], counts1,   \n             rdispls, recvtypes,                 \n350       cart_comm);                            \n351       \n352    int counts2[6] = {0, 0, 1, 1, 0, 0};     \n353    MPI_Neighbor_alltoallw(                  \n          &x[-nhalo][-nhalo][-nhalo], counts2,  \n             sdispls, sendtypes,                \n354       &x[-nhalo][-nhalo][-nhalo], counts2,  \n             rdispls, recvtypes,                \n355       cart_comm);                           Listing 8.31 2D Cartesian neighbor communication\nListing 8.32 3D Cartesian neighbor communicationSet counts to 1 for the \nhorizontal direction\nHorizontal \ncommunication\nSets counts to 1 for \nthe vertical direction\nVertical communicationSets all the counts to 1 \nfor all the directions\nAll the neighbor \ncommunication \nis done in one \ncall.\nHorizontal ghost \nexchange\nVertical ghost \nexchange",14196
107-8.5.3 Performance tests of ghost cell exchange variants.pdf,107-8.5.3 Performance tests of ghost cell exchange variants,"297 Advanced MPI functionality to simplify code and enable optimizations\n356       \n357    int counts3[6] = {1, 1, 0, 0, 0, 0};     \n358    MPI_Neighbor_alltoallw(                  \n          &x[-nhalo][-nhalo][-nhalo], counts3,  \n             sdispls, sendtypes,                \n359       &x[-nhalo][-nhalo][-nhalo], counts3,  \n             rdispls, recvtypes,                \n360       cart_comm);                           \n361 } else {\n362    int counts[6] = {1, 1, 1, 1, 1, 1};      \n363    MPI_Neighbor_alltoallw(                  \n          &x[-nhalo][-nhalo][-nhalo], counts,   \n             sdispls, sendtypes,                \n364       &x[-nhalo][-nhalo][-nhalo], counts,   \n             rdispls, recvtypes,                \n365       cart_comm);                           \n366 }\n8.5.3 Performance tests of ghost cell exchange variants\nLet’s try out these ghost cell exchange variants on a test system. We’ll use two Broad-\nwell nodes (Intel® Xeon® CPU E5-2695 v4 at 2.10GHz) with 72 virtual cores each. We\ncould run this on more compute nodes with different MPI library implementations,\nhalo sizes, mesh sizes, and with higher performance communication interconnects for\na more comprehensive view of how each ghost cell exchange variant performs. Here’s\nthe code:\nmpirun -n 144 --bind-to hwthread ./GhostExchange -x 12 -y 12 -i 20000 \\n       -j 20000 -h 2 -t -c\nmpirun -n 144 --bind-to hwthread ./GhostExchange -x 6 -y 4 -z 6 -i 700 \\n       -j 700 -k 700 -h 2 -t -c\nThe options to the GhostExchange program are\n-x (processes in the x-direction)\n-y (processes in the y-direction)\n-z (processes in the z-direction)\n-i (mesh size in the i- or x-direction)\n-j (mesh size in the j- or y-direction)\n-k (mesh size in the k- or z-direction)\n-h (width of halo cells, usually 1 or 2)\n-c (include the corner cells)\nExercise: Ghost cell tests\nThe accompanying source code is set up to run the whole set of ghost cell exchange\nvariations. In the batch.sh file, you can change the halo size and whether you want\ncorners. The file is set up to run all of the test cases 11 times on two Skylake Gold\nnodes with 144 total processes. Depth ghost \nexchange\nAll neighbors \nat once\n298 CHAPTER  8MPI: The parallel backbone\n(continued)\ncd GhostExchange\n./build.sh\n./batch.sh |& tee results.txt\n./get_stats.sh > stats.out\nYou can then generate plots with the provided scripts. You need the matplotlib library\nfor the Python plotting scripts. The results are for the median run time.\npython plottimebytype.py\npython plottimeby3Dtype.py\nThe following figure shows the plots for the small test cases. The MPI data type versions\nappear a little faster even at this small scale, indicating that perhaps a data copy is\nbeing avoided. The larger gain for the MPI Cartesian topology calls and MPI data types\nis that the ghost exchange code is greatly simplified. The use of these more advanced\nMPI calls does require more setup effort, but this is just done once at startup.\n2D ghost exchange run time (s)1.4\n1.2\n1.0\n0.8\n0.6\n0.4\n0.2\n0.0\n2D neighbor communication methodArray Pack MPI types CNeighbor CMPI types CPack\n1214\n10\n8\n6\n4\n2\n03D ghost exchange run time (s)\n3D neighbor communication methodArray MPI types CArray CNeighbor CMPI typesThe relative performance of \n2D and 3D ghost exchanges \non two nodes with a total of \n144 processes. The MPI data \ntypes in the MPI types and \nCNeighbor are a little faster \nthan the buffer explicitly filled \nby loops of array assignments. \nIn the 2D ghost exchanges, \nthe pack routines are slower, \nalthough in this case, the \nexplicitly filled buffer is faster \nthan the MPI data types.",3706
108-8.6 Hybrid MPI plus OpenMP for extreme scalability.pdf,108-8.6 Hybrid MPI plus OpenMP for extreme scalability,,0
109-8.6.2 MPI plus OpenMP example.pdf,109-8.6.2 MPI plus OpenMP example,"299 Hybrid MPI plus OpenMP for extreme scalability\n8.6 Hybrid MPI plus OpenMP for extreme scalability\nThe combination of two or more parallelization techniques is called a hybrid paral-\nlelization, in contrast to all MPI implementations that are also called pure MPI or\nMPI-everywhere. In this section, we’ll look at Hybrid MPI plus OpenMP, where MPI\nand OpenMP are used together in an application. This usually amounts to replacing\nsome MPI ranks with OpenMP threads. For larger parallel applications reaching into\nthousands of processes, replacing MPI ranks with OpenMP threads potentially\nreduces the total size of the MPI domain and the memory needed for extreme scale.\nHowever, the added performance of the thread-level parallelism layer might not\nalways be worth the added complexity and development time. For this reason, hybrid\nMPI plus OpenMP implementations are normally the domain of extreme applications\nin both size and performance needs.\n8.6.1 The benefits of hybrid MPI plus OpenMP\nWhen performance becomes critical enough for the added complexity of hybrid par-\nallelism, there can be several advantages of adding an OpenMP parallel layer to MPI-\nbased code. For example, these advantages might be\nFewer ghost cells to communicate between nodes\nLower memory requirements for MPI buffers\nReduced contention for the NIC\nReduced size of tree-based communications\nImproved load balancing\nAccessing all hardware components\nSpatially-decomposed parallel applications using subdomains with ghost (halo) cells\nwill have fewer total ghost cells per node when you add thread-level parallelism. This\nleads to a reduction in both memory requirements and communication costs, espe-\ncially on a many-core architecture like Intel’s Knights Landing (KNL). Using shared-\nmemory parallelism can also improve performance by reducing contention for the\nnetwork interface card (NIC) by avoiding the unnecessary copying of data used by\nMPI for on-node messages. Additionally, many MPI algorithms are tree-based, scaling\nas log 2n. Reducing the run time by 2 n threads decreases the depth of the tree and\nincrementally improves performance. While the remaining work still has to be done\nby threads, it impacts performance by allowing less synchronization and communica-\ntion latency costs. Threads can also be used to improve load balance within a NUMA\nregion or a compute node. \n In some cases, a hybrid parallel approach is not only advantageous, but necessary\nto access the full hardware performance potential. For example, some hardware, and\nperhaps memory controller functionality, can only be accessed by threads and not\nprocesses (MPI ranks). The many-core architectures of Intel’s Knights Corner and\nKnights Landing architectures have had these concerns. In MPI + X + Y, where X is\nthreading and Y is a GPU language, we often match the ranks to the number of GPUs.\n300 CHAPTER  8MPI: The parallel backbone\nOpenMP allows the application to continue to access the other processors for on-CPU\nwork. There are other solutions to this, such as MPI_COMM groups and MPI-shared\nmemory functionality or simply driving the GPU from multiple MPI ranks.\n In summary, while it can be attractive to run codes with MPI-everywhere on mod-\nern many-core systems, there are concerns about scalability as the number of cores\ngrows. If you are looking for extreme scalability, you will want an efficient implemen-\ntation of OpenMP in your application. We covered our design of high-level OpenMP\nthat is much more efficient in the previous chapter in sections 7.2.2 and 7.6.\n8.6.2 MPI plus OpenMP example\nThe first steps to a hybrid MPI plus OpenMP implementation is to let MPI know what\nyou will be doing. This is done in the MPI_Init  call right at the beginning of the pro-\ngram. You should replace the MPI_Init  call with the MPI_Init_thread  call like this:\nMPI_Init_thread(&argc, &argv, int thread_model required, \n                int *thread_model_provided);\nThe MPI standard defines four thread models. These models give different levels of\nthread safety with the MPI calls. In increasing order of thread safety:\nMPI_THREAD_SINGLE —Only one thread is executed (standard MPI)\nMPI_THREAD_FUNNELED —Multithreaded but only the main thread makes MPI\ncalls\nMPI_THREAD_SERIALIZED —Multithreaded but only one thread at a time makes\nMPI calls\nMPI_THREAD_MULTIPLE —Multithreaded with multiple threads making MPI calls\nMany applications perform communication at the main loop level, and OpenMP\nthreads are applied to key computational loops. For this pattern, MPI_THREAD_FUN-\nNELED  works just fine.\nNOTE It’s best to use the lowest level of thread safety that you need. Each\nhigher level imposes a performance penalty because the MPI library has to\nplace mutexes or critical blocks around send and receive queues and other\nbasic parts of MPI. \nNow let’s see what changes are needed to our stencil example to add OpenMP thread-\ning. We chose the CartExchange_Neighbor example to modify for this exercise. The\nfollowing listing shows that the first change is to modify the MPI initialization.\nHybridMPIPlusOpenMP/CartExchange.cc\n26 int provided;\n27 MPI_Init_thread(&argc, &argv,         \n      MPI_THREAD_FUNNELED, &provided);   \n28 Listing 8.33 MPI initialization for OpenMP threading\nMPI initialization for \nOpenMP threading\n301 Hybrid MPI plus OpenMP for extreme scalability\n29 int rank, nprocs;\n30 MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n31 MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n32 if (rank == 0) {\n33    #pragma omp parallel\n34    #pragma omp master\n35       printf(""requesting MPI_THREAD_FUNNELED”    \n                  "" with %d threads\n"",             \n36              omp_get_num_threads());             \n37    if (provided != MPI_THREAD_FUNNELED){       \n38       printf(""Error: MPI_THREAD_FUNNELED”\n                  "" not available. Aborting ...\n"");\n39       MPI_Finalize();\n40       exit(0);\n41    }\n42 }\nThe mandatory change is using MPI_Init_thread  instead of MPI_Init  on line 27. The\nadditional code checks that the requested thread safety level is available and exits if it\nis not. We also print the number of threads on the main thread of rank zero. Now\nonto the changes in the computational loop shown in the next listing.\nHybridMPIPlusOpenMP/CartExchange.cc\n157  #pragma omp parallel for          \n158  for (int j = 0; j < jsize; j++){\n159     #pragma omp simd              \n160     for (int i = 0; i < isize; i++){\n161        xnew[j][i] = ( x[j][i] + x[j][i-1] + x[j][i+1]\n                                  + x[j-1][i] + x[j+1][i] )/5.0;\n162     }\n163  }\nThe changes required to add OpenMP threading are the addition of a single pragma\nat line 157. As a bonus, we show how to add vectorization for the inner loop with\nanother pragma inserted at line 159.\n You can now try running this hybrid MPI plus OpenMP+Vectorization example on\nyour system. But to get good performance, you will need to control the placement of\nthe MPI ranks and the OpenMP threads. This is done by setting affinity, a topic that\nwe will cover in greater depth in chapter 14.\nDEFINITION Affinity  assigns a preference for the scheduliing of a process, rank,\nor thread to a particular hardware component. This is also called pinning  or\nbinding .\nSetting the affinity for your ranks and threads becomes more important as the com-\nplexity of the node increases and with hybrid parallel applications. In earlier exam-\nples, we used --bind-to  core  and --bind-to  hwthread  to improve performance and\nreduce variability in run-time performance caused by ranks migrating from one coreListing 8.34 Addition of OpenMP threading and vectorization to computational loopsPrints number of \nthreads to check if \nwhat we want\nChecks if this MPI \nsupports our requested \nthread safety level\nAdds OpenMP threading \nfor outer loop\nAdds SIMD vectorization \nfor inner loop",7970
110-8.7.1 Additional reading.pdf,110-8.7.1 Additional reading,"302 CHAPTER  8MPI: The parallel backbone\nto another. In OpenMP, we used environment variables to set placement and affini-\nties. An example is\nexport OMP_PLACES=cores\nexport OMP_CPU_BIND=true\nFor now, start with pinning the MPI ranks to sockets so that the threads can spread to\nother cores as we showed in our ghost cell test example for the Skylake Gold proces-\nsor. Here’s how:\nexport OMP_NUM_THREADS=22\nmpirun -n 4 --bind-to socket ./CartExchange -x 2 -y 2 -i 20000 -j 20000 \\n-h 2 -t -c\nWe run 4 MPI ranks that each spawn 22 threads as specified by the OMP_NUM_THREADS\nenvironment variable for a total of 88 processes. The --bind-to  socket  o p t i o n  t o\nmpirun  tells it to bind the processes to the socket where these are placed.\n8.7 Further explorations\nAlthough we have covered a lot of material in this chapter, there are still many more\nfeatures that are worth exploring as you get more experience with MPI. Some of the\nmost important are mentioned here and left for your own study.\nComm groups —MPI has a rich set of functions that create, split, and otherwise\nmanipulate the standard MPI COMM_WORLD communicator into new group-\nings for specialized operations like communication within a row or task-based\nsubgroups. For some examples of the use of communicator groups, see listing\n16.4 in section 16.3. We use communication groups to split the file output into\nmultiple files and break the domain into row and column communicators.\nUnstructured mesh boundary communications —An unstructured mesh needs to\nexchange boundary data in a similar manner to that covered for a regular, Car-\ntesian mesh. These operations are more complex and not covered here. There\nare many sparse, graph-based communication libraries that support unstruc-\ntured mesh applications. One example of such a library is the L7 communica-\ntion library developed by Richard Barrett now at Sandia National Laboratories.\nIt is included with the CLAMR mini-app; see the l7 subdirectory at https:/ /\ngithub.com/LANL/CLAMR .\nShared memory —The original MPI implementations sent data over the network\ninterface in nearly all cases. As the number of cores grew, MPI developers real-\nized that they could do some of the communication in shared memory. This is\ndone behind the scenes as a communication optimization. Additional shared\nmemory functionality continues to be added with MPI shared memory “win-\ndows.” This functionality had some problems at first, but it is becoming mature\nenough to use in applications.",2537
111-Part 3GPUs Built to accelerate.pdf,111-Part 3GPUs Built to accelerate,"303 Further explorations\nOne-sided communication —Responding to other programming models, MPI\nadded one-sided communication in the form of MPI_Puts  and MPI_Gets . Con-\ntrary to the original MPI message-passing model, where both the sender and\nreceiver have to be active participants, the one-sided model allows just one or\nthe other to conduct the operation.\n8.7.1 Additional reading\nIf you want more introductory material on MPI, the text by Peter Pacheco is a classic:\nPeter Pacheco, An introduction to parallel programming  (Elsevier, 2011).\nYou can find thorough coverage of MPI authored by members of the original MPI\ndevelopment team:\nWilliam Gropp, et al., “Using MPI: portable parallel programming with the mes-\nsage-passing interface,” Vol. 1 (MIT Press, 1999).\nFor a presentation of MPI plus OpenMP, there is a good lecture from a course by Bill\nGropp, one of the developers of the original MPI standard. Here’s the link:\nhttp:/ /wgropp.cs.illinois.edu/courses/cs598-s16/lectures/lecture36.pdf\n8.7.2 Exercises\n1Why can’t we just block on receives as was done in the send/receive in the ghost\nexchange using the pack or array buffer methods in listings 8.20 and 8.21,\nrespectively?\n2Is it safe to block on receives as shown in listing 8.8 in the vector type version of\nthe ghost exchange? What are the advantages if we only block on receives?\n3Modify the ghost cell exchange vector type example in listing 8.21 to use block-\ning receives instead of a waitall . Is it faster? Does it always work?\n4Try replacing the explicit tags in one of the ghost exchange routines with MPI_\nANY_TAG . Does it work? Is it any faster? What advantage do you see in using\nexplicit tags?\n5Remove the barriers for the synchronized timers in one of the ghost exchange\nexamples. Run the code with the original synchronized timers and the unsyn-\nchronized timers.\n6Add the timer statistics from listing 8.11 to the stream triad bandwidth measure-\nment code in listing 8.17.\n7Apply the steps to convert high-level OpenMP to the hybrid MPI plus OpenMP\nexample in the code that accompanies the chapter (HybridMPIPlusOpenMP\ndirectory). Experiment with the vectorization, number of threads, and MPI ranks\non your platform.\n304 CHAPTER  8MPI: The parallel backbone\nSummary\nUse the proper send and receive point-to-point messages. This avoids hangs and\ngets good performance.\nUse collective communication for common operations. This makes for concise\nprogramming, avoids hangs, and improves performance.\nUse ghost exchanges to link together subdomains from various processors. The\nexchanges make the subdomains act as a single global computational mesh.\nAdd more levels of parallelism through combining MPI with OpenMP threads\nand vectorization. The additional parallelism helps give better performance.\nPart 3\nGPUs: Built to accelerate\nT he following chapters on GPU computing discuss using GPUs for scientific\ncomputing. The topics include\n■In chapter 9, you’ll gain an understanding of the GPU architecture and its\nbenefits for general-purpose computation.\n■In chapter 10, you’ll learn how to build a mental representation of the\nprogramming model for GPUs.\n■In chapters 11 and 12, you’ll explore the available GPU programming lan-\nguages. In chapter 11, we present basic examples in OpenACC and OpenMP,\nand in chapter 12, we cover a broad range of GPU languages, from lower\nlevel native languages like CUDA, OpenCL, and HIP to higher level ones\nlike SYCL, Kokkos, and Raja.\n■In chapter 13, you’ll learn about profiling tools and developing a work-\nflow model that enhances programmer productivity.\nGPUs were built to accelerate computation. With a single-minded focus on\nimproving the frame rate for computer animation, GPU hardware developers\nwent to great lengths to increase numerical operation throughput. These devices\nare simply general-purpose extreme accelerators for any massively parallel oper-\nation. They are called graphics processing units because they were developed for\nthat application.\n Fast forward to today, and many software developers have realized that the\nacceleration provided by GPUs is just as applicable for a wide variety of applica-\ntion domains. While at the University of North Carolina in 2002, Mark Harris\ncoined the term general-purpose graphics processing units  (GPGPUs) to try and\n306 PART 3GPUs: Built to accelerate\ncapture the idea that GPUs are suitable for more than graphics alone. Major markets\nfor GPUs have sprung up for bitcoin mining, machine learning, and high-performance\ncomputing. Small modifications to the GPU hardware, such as double-precision floating-\npoint units and tensor operations, have customized the basic GPU designs for each of\nthese markets. No longer are GPUs just for graphics. \n Each GPU model that is released targets a different market segment. With high-\nend GPU models commanding prices of up to $10,000, these are not the same hard-\nware models that you will find in mass-market computers. Though GPUs are used in\nmany more applications than they were originally intended for, it is still difficult to see\nthem completely replacing the general-purpose functionality of a CPU within the\nnear future, as a single operation is better suited for the CPU. \n If we were to come up with a new name for GPUs, how would we capture their\nfunctionality in a broad sense? The commonality across all the use domains is that, in\norder for it to make sense to use GPUs, there must be a lot of work that can be done\nsimultaneously. And by a lot, we mean thousands or tens of thousands of simultaneous\nparallel operations. GPUs are really parallel accelerators . Maybe we should call them Par-\nallel Processing Units (PPUs) or Parallel Processing Accelerators (PPAs) to capture a\nbetter idea of their functionality. But, we’ll stick with the term GPUs with the under-\nstanding that they are so much more. Seeing GPUs in this light, you can understand\nwhy they are of such importance for the parallel computing community.\n GPUs are simpler devices to design and manufacture than CPUs, so the design\ncycle time is half that of the CPUs. The crossover point for many applications for GPU\nperformance relative to CPUs was about 2012. Since then, GPU performance has\nbeen improving at about twice the rate of CPUs. In very rough numbers, GPUs today\ncan provide a ten times speedup over CPUs. Of course, there is a lot of variability in\nthis speedup by the type of application and the quality of code implementation. The\ntrends are clear—GPUs will continue to show greater speedups for those applications\nthat can fit its massively parallel architecture.\n To help you understand these new hardware devices, we go over the essential\nparts of their hardware design in chapter 10. We then try to help you develop a men-\ntal model of how to approach them. It is important to gain this understanding before\ntackling a project for GPUs. We have seen numerous porting efforts fail because\nprogrammers thought they could just move the most expensive loop to the GPU and\nthey would see fantastic speedups. Then when their application runs slower, they\nabandon the effort. Transferring data to the GPU is expensive; therefore, large parts\nof your application must be ported to the device to see any benefit. A simple perfor-\nmance model and analysis before the GPU implementation would have tempered\nprogrammers’ initial expectations and cautioned them to plan for sufficient time\nand effort to achieve success.\n Perhaps the biggest impediment to beginning a GPU implementation is the con-\nstantly shifting landscape of programming languages. It seems like a new language\ngets released every few months. Although these languages bring a high degree of\n307 GPUs: Built to accelerate\ninnovation, this constant evolution makes it difficult for application developers. How-\never, taking a closer look at the languages shows that there are more similarities than\ndifferences, often converging on a couple of common designs. While we expect a few\nmore years of language thrashing, many of the language variations are akin to dialects\nrather than completely different languages. In chapter 11, we cover the pragma-based\nlanguages. In chapter 12, we survey native GPU languages and a new class of perfor-\nmance portability languages that heavily leverage C++ constructs. Though we present\na variety of language implementations, we suggest you initially pick a couple of lan-\nguages to get some hands-on experience with those. Much of your decision on which\nlanguages to experiment with will be dependent on the hardware that you have read-\nily available. \n You can check out the examples at https:/ /github.com/EssentialsofParallelCom\nputing  for each of these chapters. (In fact, we highly encourage you to do so.) One of\nthe barriers for GPU programming is access to hardware and getting it set up prop-\nerly. The installation of system software to support the GPUs can sometimes be diffi-\ncult. The examples have lists of the software packages for the GPUs from the vendors\nthat can get you started. But you will want to install the software for the GPU in your\nsystem and, in the examples, comment out the rest. It will take some trial and error. In\nchapter 13, we discuss different workflows and alternatives such as setting up Docker\ncontainers and virtual machines (VMs). These options may provide a way to set up a\ndevelopment environment on your laptop or desktop, especially if you are using Win-\ndows or MacOS.\n If you don’t have local hardware available, you can try out a cloud service with\nGPUs. Some such as Google cloud ($200-300 credit) even have free trials. These ser-\nvices even have marketplace add-ons that let you set up an HPC cluster with GPUs.\nOne example of an HPC cloud service with GPUs is the Fluid Numerics Google Cloud\nPlatform. For testing out Intel GPUs, Intel has set up cloud services for trial services as\nwell. For information, we recommend these sites:\n■Fluid-Slurm Google Cloud Cluster at https:/ /console.cloud.google.com/mar\nketplace/details/fluid-cluster-ops/fluid-slurm-gcp\n■Intel cloud version of oneAPI and DPCPP at https:/ /software.intel.com/en-us/\noneapi  (you must register to use).",10301
112-9 GPU architectures and concepts.pdf,112-9 GPU architectures and concepts,"309GPU architectures\nand concepts\nWhy do we care about graphics processing units (GPUs) for high-performance com-\nputing? GPUs provide a massive source of parallel operations that can greatly exceed\nthat which is available on the more conventional CPU architecture. To exploit their\ncapabilities, it is essential that we understand GPU architectures. Though GPUs have\noften been used for graphical processing, GPUs are also used for general-purpose\nparallel computing. This chapter provides an overview of the hardware on a GPU-\naccelerated platform. \n What systems today are GPU accelerated? Virtually every computing system pro-\nvides the powerful graphics capabilities expected by today’s users. These GPUs\nrange from small components of the main CPU to large peripheral cards taking upThis chapter covers\nUnderstanding the GPU hardware and connected \ncomponents\nEstimating the theoretical performance of \nyour GPU\nMeasuring the performance of your GPU\nDifferent application uses for effectively using \na GPU\n310 CHAPTER  9GPU architectures and concepts\na large part of space in a desktop case. HPC systems are increasingly coming equipped\nwith multiple GPUs. On occasion, even personal computers used for simulation or\ngaming can sometimes connect two GPUs for higher graphics performance. In this\nchapter, we present a conceptual model that identifies key hardware components of a\nGPU accelerated system. Figure 9.1 shows these components.\nDue to inconsistent terminology in the community, there is added complexity in under-\nstanding GPUs. We will use the terminology established by the OpenCL standard\nbecause it was agreed to by multiple GPU vendors. We will also note alternate termi-\nnology that is in common use, such as that used by NVIDIA. Let’s look at a few defini-\ntions before continuing our discussion:\nCPU—The main processor that is installed in the socket of the motherboard.\nCPU RAM —The “memory sticks” or dual in-line memory modules (DIMMs)\ncontaining Dynamic Random-Access Memory (DRAM) that are inserted into\nthe memory slots in the motherboard.\nGPU—A large peripheral card installed in a Peripheral Component Intercon-\nnect Express (PCIe) slot on the motherboard.\nGPU RAM —Memory modules on the GPU peripheral card for exclusive use of\nthe GPU.\nPCI bus —The wiring that connects the peripheral cards to the other compo-\nnents on the motherboard.\nWe’ll introduce each component in a GPU-accelerated system and show how to calcu-\nlate the theoretical performance for each. We’ll then examine their actual perfor-\nmance with small micro-benchmark applications. This will help to establish how some\nhardware components can cause bottlenecks that prevent you from accelerating an\napplication with GPUs. Armed with this information, we’ll conclude the chapter with\na discussion of the types of applications that benefit most from GPU acceleration and\nwhat your goals should be to see performance gains when porting an application to\nrun on GPUs. For this chapter, you’ll find the source code at https:/ /github.com/\nEssentialsofParallelComputing/Chapter9 .CPU RAM\nGPU RAM\nGPUCPUPCI busFigure 9.1 Block diagram of \nGPU-accelerated system using \na dedicated GPU. The CPU and \nGPU each have their own \nmemory. The CPU and GPU \ncommunicate over a PCI bus.",3335
113-9.1.2 Dedicated GPUs The workhorse option.pdf,113-9.1.2 Dedicated GPUs The workhorse option,"311 The CPU-GPU system as an accelerated computational platform\n9.1 The CPU-GPU system as an accelerated \ncomputational platform\nGPUs are everywhere. They can be found in cell phones, tablets, personal computers,\nconsumer-grade workstations, gaming consoles, high performance computing cen-\nters, and cloud computing platforms. GPUs provide additional compute power on\nmost modern hardware and accelerate many operations you may not even be aware\nof. As the name suggests, GPUs were designed for graphics-related computations.\nConsequently, GPU design focuses on processing large blocks of data (triangles or\npolygons) in parallel, which is a requirement for graphics applications. Compared to\nCPUs that can handle tens of parallel threads or processes in a clock cycle, GPUs are\ncapable of processing thousands of parallel threads simultaneously. Because of this\ndesign, GPUs offer a considerably higher theoretical peak performance that can\npotentially reduce the time to solution and the energy footprint of an application. \n Computational scientists, always on the lookout for computational horsepower,\nwere attracted to using GPUs to perform more general-purpose computing tasks.\nBecause GPUs were designed for graphics, the languages originally developed to pro-\ngram them, like OpenGL, focused on graphics operations. To implement algorithms\non GPUs, programmers had to reframe their algorithms in terms of these operations,\nwhich was time-consuming and error-prone. Extending the use of the graphics proces-\nsor to non-graphics workloads became known as general-purpose graphics processing\nunit (GPGPU) computing. \n The continued interest and success of GPGPU computing led to the introduction\nof a flurry of GPGPU languages. The first to gain wide adoption was the Compute Uni-\nfied Device Architecture (CUDA) programming language for NVIDIA GPUs, which was\nfirst introduced in 2007. The dominant open standard GPGPU computing language is\nthe Open Computing Language (OpenCL), developed by a group of vendors led by\nApple and released in 2009. We’ll cover both CUDA and OpenCL in chapter 12. \n Despite the continual introduction of GPGPU languages, or maybe because of it,\nmany computational scientists have found the original, native, GPGPU languages diffi-\ncult to use. As a result, higher-level approaches using directive-based APIs gained a large\nfollowing and spurred corresponding development efforts by vendors. We’ll cover\nexamples of directive-based languages like OpenACC and OpenMP (with the new tar-\nget directive) in chapter 11. For now, we summarize the new directive-based GPGPU\nlanguages, OpenACC and OpenMP, as an unqualified success. These languages and\nAPIs have allowed programmers to focus more on developing their applications, rather\nthan expressing their algorithm in terms of graphics operations. The end result has\noften been tremendous speedups in scientific and data science applications.\n GPUs are best described as accelerators, long used in the computing world. First\nlet’s define what we mean by an accelerator.\nDEFINITION An accelerator  (hardware) is a special-purpose device that supple-\nments the main general-purpose CPU in speeding up certain operations. \n312 CHAPTER  9GPU architectures and concepts\nA classic example of an accelerator is the original PC that came with the 8088 CPU. It\nhad the option and a socket for the 8087 coprocessor that would do floating-point\noperations in hardware rather than software. Today, the most common hardware\naccelerator is the graphics processor, which can be either a separate hardware compo-\nnent or integrated on the main processor. The distinction of being called an accelera-\ntor is that it is a special-purpose rather than a general-purpose device, but that\ndifference is not always clear-cut. A GPU is an additional hardware component that\ncan perform operations alongside a CPU. GPUs come in two flavors:\nIntegrated GPUs —A graphics processor engine that is contained on the CPU\nDedicated GPUs —A GPU contained on a separate peripheral card\nIntegrated GPUs are built directly into the CPU chip. Integrated GPUs share RAM\nresources with the CPU. Dedicated GPUs are attached to the motherboard via a\nPeripheral Component Interconnect (PCI) slot. The PCI slot is a physical component\nthat allows data to be transmitted between the CPU and GPU. It is commonly referred\nto as the PCI bus. \n9.1.1 Integrated GPUs: An underused option on \ncommodity-based systems\nIntel® has long included an integrated GPU with their CPUs for the budget market.\nThey fully expected that users wanting real performance would buy a discrete GPU.\nThe Intel integrated GPUs have historically been relatively weak in comparison to\nAMD’s (Advanced Micro Devices, Inc.) integrated version. This has recently changed\nwith Intel claiming that the integrated graphics on their Ice Lake processor are on a\npar with AMD integrated GPUs.\n The AMD integrated GPUs are called Accelerated Processing Units (APUs). These\nare a tightly coupled combination of the CPU and a GPU. The source of the GPU\ndesign originally came from the AMD purchase of the ATI graphics card company in\n2006. In the AMD APU, the CPU and GPU share the same processor memory. These\nGPUs are smaller than a discrete GPU, but still (proportionally) give GPU graphics\n(and compute) performance. The real target for AMD for APUs is to provide a more\ncost-effective, but performant system for the mass market. The shared memory is also\nattractive because it eliminates the data transfer over the PCI bus, which is often a seri-\nous performance bottleneck.\n The ubiquitous nature of the integrated GPU is important. For us, it means that\nnow many commodity desktops and laptops have the ability to accelerate computations.\nThe goal on these systems is a relatively modest performance boost and, perhaps, to\nreduce the energy cost or to improve battery life. But for extreme performance, the dis-\ncrete GPUs are still the undisputed performance champions.",6057
114-9.2 The GPU and the thread engine.pdf,114-9.2 The GPU and the thread engine,"313 The GPU and the thread engine\n9.1.2 Dedicated GPUs: The workhorse option\nIn this chapter, we will focus primarily on GPU accelerated platforms with dedicated\nGPUs, also called discrete GPUs . Dedicated GPUs generally offer more compute power\nthan integrated GPUs. Additionally, these GPUs can be isolated to execute general-\npurpose computing tasks. Figure 9.1 conceptually illustrated a CPU-GPU system with a\ndedicated GPU. A CPU has access to its own memory space (CPU RAM) and is con-\nnected to a GPU via a PCI bus. It is able to send data and instructions over the PCI bus\nfor the GPU to work with. The GPU has its own memory space, separate from the CPU\nmemory space.\n In order for work to be executed on the GPU, at some point, data must be trans-\nferred from the CPU to the GPU. When the work is complete, and the results are\ngoing to be written to file, the GPU must send data back to the CPU. The instructions\nthe GPU must execute are also sent from CPU to GPU. Each one of these transactions\nis mediated by the PCI bus. Although we won’t discuss how to make these actions hap-\npen in this chapter, we’ll discuss the hardware performance limitations of the PCI bus.\nDue to these limitations, a poorly designed GPU application can potentially have\nworse performance than that with CPU-only code. We’ll also discuss the internal\narchitecture of the GPU and the performance of the GPU with regards to memory\nand floating-point operations.\n9.2 The GPU and the thread engine\nFor those of us who have done thread programming over the years on a CPU, the\ngraphics processor is like the ideal thread engine. The components of this thread\nengine are\nA seemingly infinite number of threads\nZero time cost for switching or starting threads \nLatency hiding of memory accesses through automatic switching between work\ngroups\nLet’s look at the hardware architecture of a GPU to get an idea of how it performs this\nmagic. To show a conceptual model of a GPU, we abstract the common elements from\ndifferent GPU vendors and even between design variations from the same vendor. We\nmust remind you that there are hardware variations that are not captured by these\nabstract models. Adding to this plethora of terminology currently in use, it is not sur-\nprising that it is difficult for a newcomer to the field to understand GPU hardware and\nprogramming languages. Still, this terminology is relatively sane compared to the\ngraphics world with vertex shaders, texture mapping units, and fragment generators.\nTable 9.1 summarizes the rough equivalence of terminology, but beware that because\nthe hardware architectures are not exactly the same, the correspondence in terminol-\nogy varies depending on the context and user.\n \n314 CHAPTER  9GPU architectures and concepts\nThe last row in table 9.1 shows the hardware layer that implements a single instruction\non multiple data, commonly referred to as SIMD. Strictly speaking, the NVIDIA hard-\nware does not have vector hardware, or SIMD, but emulates this through a collection\nof threads in what it calls a warp in a single instruction, multi-thread (SIMT) model.\nYou may want to refer back to our initial discussion of parallel categories in section 1.4\nto refresh your memory on these different approaches. Other GPUs can also perform\nSIMT operations on what OpenCL and AMD call subgroups, which are equivalent to\nthe NVIDIA warps. We’ll discuss this more in chapter 10, which explicitly looks at GPU\nprogramming models. This chapter, however, will focus on the GPU hardware, its\narchitecture, and concepts.\n Often, GPUs also have hardware blocks of replication, some of which are listed in\ntable 9.2, to simplify the scaling of their hardware designs to more units. These units\nof replication are a manufacturing convenience, but often show up in the specifica-\ntion lists and discussions.\nFigure 9.2 depicts a simplified block diagram of a single node system with a single\nmultiprocessor CPU and two GPUs. A single node can have a wide variety of configu-\nrations, composed of one or more multiprocessor CPUs with an integrated GPU, and\nfrom one to six discrete GPUs. In OpenCL nomenclature, each GPU is a compute\ndevice. But compute devices can also be CPUs in OpenCL.\nDEFINITION A compute device  in OpenCL is any computational hardware that\ncan perform computation and supports OpenCL. This can include GPUs,\nCPUs, or even more exotic hardware such as embedded processors or field-\nprogrammable gate arrays (FPGAs).Table 9.1 Hardware terminology: A rough translation\nHost OpenCL AMD GPU NVIDIA/CUDA Intel Gen11\nCPU Compute device GPU GPU GPU\nMultiprocessor Compute unit (CU) Compute unit (CU) Streaming multi-\nprocessor (SM)Subslice\nProcessing core \n(Core for short)Processing \nelement (PE)Processing \nelement (PE)Compute cores \nor CUDA coresExecution units \n(EU)\nThread Work Item Work Item Thread\nVector or SIMD Vector Vector Emulated with \nSIMT warpSIMD\nTable 9.2 GPU hardware replication units by vendor\nAMD NVIDIA/CUDA Intel Gen11\nShader Engine (SE) Graphics processing cluster Slice\n315 The GPU and the thread engine\nThe simplified diagram in figure 9.2 is our model for describing the components of a\nGPU and is also useful when understanding how a GPU processes data. A GPU is com-\nposed of \nGPU RAM (also known as global memory )\nWorkload distributor\nCompute units (CUs) (SMs in CUDA)\nCUs have their own internal architecture, often referred to as the microarchitecture .\nInstructions and data received from the CPU are processed by the workload distribu-\ntor. The distributor coordinates instruction execution and data movement onto and\noff of the CUs. The achievable performance of a GPU depends on \nGlobal memory bandwidth\nCompute unit bandwidth \nThe number of CUs\nIn this section, we’ll explore each of the components for our model of a GPU. With\neach component, we will also discuss models for theoretical peak bandwidth.Figure 9.2 A simplified block diagram of a GPU system showing two \ncompute devices, each having separate GPU, GPU memory, and multiple \ncompute units (CUs). The NVIDIA CUDA terminology refers to CUs as \nstreaming multiprocessors (SMs).",6224
115-9.2.1 The compute unit is the streaming multiprocessor or subslice.pdf,115-9.2.1 The compute unit is the streaming multiprocessor or subslice,,0
116-9.2.2 Processing elements are the individual processors.pdf,116-9.2.2 Processing elements are the individual processors,,0
117-9.2.3 Multiple data operations by each processing element.pdf,117-9.2.3 Multiple data operations by each processing element,,0
118-9.2.4 Calculating the peak theoretical flops for some leading GPUs.pdf,118-9.2.4 Calculating the peak theoretical flops for some leading GPUs,"316 CHAPTER  9GPU architectures and concepts\nAdditionally, we’ll show how to use micro-benchmark tools to measure actual perfor-\nmance of components.\n9.2.1 The compute unit is the streaming multiprocessor (or subslice)\nA GPU compute device has multiple CUs. (CU, compute unit , is the term agreed to by\nthe community for the OpenCL standard.) NVIDIA calls them streaming multiprocessors\n(SMs), and Intel refers to them as subslices .\n9.2.2 Processing elements are the individual processors\nEach CU contains multiple graphics processors called processing elements (PEs) in\nOpenCL, or CUDA cores (or Compute Cores) as NVIDIA calls them. Intel refers\nto them as execution units (EUs), and the graphics community calls them shader\nprocessors. \n Figure 9.3 shows a simplified conceptual diagram of a PE. These processors are not\nequivalent to a CPU processor; they are simpler designs, needing to perform graphics\noperations. But the operations needed for graphics include nearly all the arithme-\ntic operations that a programmer uses on a regular processor.\n9.2.3 Multiple data operations by each processing element\nWithin each PE, it might be possible to perform an operation on more than one data\nitem. Depending on the details of the GPU microprocessor architecture and the GPU\nvendor, these are referred to as SIMT, SIMD, or vector operations. A similar type of\nfunctionality can be provided by ganging PEs together.\n9.2.4 Calculating the peak theoretical flops for some leading GPUs\nWith an understanding of the GPU hardware, we can now calculate the peak theoreti-\ncal flops for some recent GPUs. These include the NVIDIA V100, AMD Vega 20, AMD\nArcturus, and the integrated Gen11 GPU on the Intel Ice Lake CPU. Table 9.3 lists theThread\nscheduler\nLocal memory\nPrivate memory(Shared memory)\n(Registers)\nProcessing elements\n(Compute cores)\nFigure 9.3 Simplified block diagram of \na compute unit (CU) with a large number \nof processing elements (PEs).\n317 The GPU and the thread engine\nspecifications for these GPUs. We’ll use these specifications to calculate the theoretical\nperformance of each device. Then, knowing the theoretical performance, you can\nmake comparisons on how each performs. This can help you with purchasing deci-\nsions or with estimating how much faster or slower another GPU might be with your\ncalculations. Hardware specifications for many GPU cards can be found at Tech-\nPowerUp: https:/ /www.techpowerup.com/gpu-specs/ . \n For NVIDIA and AMD, the GPUs targeted to the HPC market have the hardware\ncores to perform one double-precision operation for every two single-precision opera-\ntions. This relative flop capability can be expressed as a ratio of 1:2, where double pre-\ncision is 1:2 of single precision on top-end GPUs. The importance of this ratio is that it\ntells you that you can roughly double your performance by reducing your precision\nrequirements from double precision to single. For many GPUs, half precision has a\nratio of 2:1 to single precision or double the flop capability. The Intel integrated GPU\nhas 1:4 double precision relative to single precision, and some commodity GPUs have\n1:8 ratios of double precision to single precision. GPUs with these lower ratios of dou-\nble precision are targeted at the graphics market or for machine learning. To get\nthese ratios, take the FP64 row and divide by the FP32 row.\nThe peak theoretical flops can be calculated by taking the clock rate times the num-\nber of processors times the number of floating-point operations per cycle. The flopsTable 9.3 Specifications for recent discrete GPUs from NVIDIA, AMD, and an integrated Intel GPU\nGPUNVIDIA V100 \n(Volta)NVIDIA A100 \n(Ampere)AMD Vega 20 \n(MI50)AMD Arcturus \n(MI100)Intel Gen11 \nIntegrated\nCompute units \n(CUs)80 108 60 120 8\nFP32 cores/CU 64 64 64 64 64\nFP64 cores/CU 32 32 32 32\nGPU clock \nnominal/boost1290/1530 MHz 1410 MHz 1200/1746 MHz 1000/1502 MHz 400/1000 MHz\nSubgroup or \nwarp size32 32 64 64\nMemory clock 876 MHz 1215 MHz 1000 MHz 1200 MHz Shared memory\nMemory type HBM2 (32 GB) HBM2(40 GB) HBM2 HBM2 (32 GB) LPDDR4X-3733\nMemory data \nwidth4096 bits 5120 bits 4096 bits 4096 bits 384 bits\nMemory bus \ntypeNVLink or \nPCIe 3.0x16NVLink or \nPCIe Gen 4Infinity Fabric or \nPCIe 4.0x16Infinity Fabric or \nPCIe 4.0x16Shared memory\nDesign Power 300 watts 400 watts 300 watts 300 watts 28 watts",4415
119-9.3.1 Calculating theoretical peak memory bandwidth.pdf,119-9.3.1 Calculating theoretical peak memory bandwidth,"318 CHAPTER  9GPU architectures and concepts\nper cycle accounts for the fused-multiply add (FMA), which does two operations in\none cycle.\nPeak Theoretical Flops (GFlops/s)\n= Clock rate MHZ × Compute Units × Processing units\n× Flops/cycle\nBoth the NVIDIA V100 and the AMD Vega 20 give impressive floating-point peak per-\nformance. The Ampere shows some additional improvement in floating-point perfor-\nmance, but it is the memory performance that promises greater increases. The MI100\nfrom AMD shows a bigger jump in floating-point performance. The Intel integrated\nGPU is also quite impressive given that it is limited by the available silicon space and\nlower nominal design power of a CPU. With Intel developing plans for discrete graphics\ncards for several market segments, expect to see even more GPU options in the future.\n9.3 Characteristics of GPU memory spaces\nA typical GPU has different types of memory. Using the right memory space can make\na big impact on performance. Figure 9.4 shows these memories as a conceptual dia-\ngram. It helps to see the physical locations of each level of memory to understand how\nit should behave. Although a vendor can put the GPU memory wherever they want, it\nmust behave as shown in this diagram.Example: Peak theoretical flop for some leading GPUs\nTheoretical Peak Flops for NVIDIA V100:\n2 × 1530 × 80 × 64 /10^6 = 15.6 TFlops (single precision)\n2 × 1530 × 80 × 32 /10^6 = 7.8 TFlops (double precision) \nTheoretical Peak Flops for NVIDIA Ampere:\n2 × 1410 × 108 × 64 /10^6 = 19.5 TFlops (single precision)\n2 × 1410 × 108 x× 32 /10^6 = 9.7 TFlops (double precision) \nTheoretical Peak Flops for AMD Vega 20 (MI50):\n2 × 1746 × 60 × 64 /10^6 = 13.4 TFlops (single precision)\n2 × 1746 × 60 × 32 /10^6 = 6.7 TFlops (double precision) \nTheoretical Peak Flops for AMD Arcturus (MI100):\n2 × 1502 × 120 × 64 /10^6 = 23.1 TFlops (single precision)\n2 × 1502 × 120 × 32 /10^6 = 11.5 TFlops (double precision) \nTheoretical Peak Flops for Intel Integrated Gen 11 on Ice Lake:\n2 × 1000 × 64 × 8 /10^6 = 1.0 TFlops (single precision)\n319 Characteristics of GPU memory spaces\nThe list of the GPU memory types and their properties are as follows.\nPrivate memory (register memory) —Immediately accessible by a single PE and only\nby that PE.\nLocal memory —Accessible to a single CU and all of the PEs on that CU. Local\nmemory can be split between a scratchpad that can be used as a programmable\ncache and, by some vendors, a traditional cache on GPUs. Local memory is\naround 64-96 KB in size.\nConstant memory —Read-only memory accessible and shared across all of the CUs.\nGlobal memory —Memory that’s located on the GPU and accessible by all of the CUs.\nOne of the factors that makes GPUs fast is that they use specialized global memory\n(RAM), which provides higher bandwidth, whereas current CPUs use DDR4 memory\nand are just now moving to DDR5. GPUs use a special version called GDDR5 that gives\nhigher performance. The latest GPUs are now moving to High-Bandwidth Memory\n(HBM2) that provides even higher bandwidth. Besides increasing bandwidth, HBM\nalso reduces power consumption.\n9.3.1 Calculating theoretical peak memory bandwidth\nYou can calculate the theoretical peak memory bandwidth for a GPU from the mem-\nory clock rate on the GPU and the width of the memory transactions in bits. Table 9.4\nshows some of the higher values for each memory type. We also need to multiply by aProcessing\nelement MPrivate\nmemory M\nProcessing\nelement 1Private\nmemory 1Compute unit 1\nLocal memory 1Compute device\nData cachePrivate to GPU only\nGlobal memory\nConstant memory\nReadable/Writeable by hostProcessing\nelement MPrivate\nmemory M\nProcessing\nelement 1Private\nmemory 1Compute unit N\nLocal memory N\nFigure 9.4 Rectangles show each component of the GPU and the memory that is \nat each hardware level. The host writes and reads the global and constant memory. \nEach of the CUs can read and write from the global memory and read from the \nconstant memory.\n320 CHAPTER  9GPU architectures and concepts\nfactor of two for the double data rate, which retrieves memory at both the top of the\ncycle and at the bottom. Some DDR memory can even do more transactions per cycle.\nTable 9.4 also shows some of the transaction multipliers for different kinds of graphics\nmemory.\nCalculating the theoretical memory bandwidth takes the memory clock rate times the\nnumber of transactions per cycle and then multiplies by the number of bits retrieved\non each transaction:\nTheoretical Bandwidth = Memory Clock Rate (GHz) × Memory bus (bits) × \n   (1 byte/8 bits) × transaction multiplier\nSome specification sheets give the memory transaction rate in Gbps rather than the\nmemory clock frequency. This rate is the transactions per cycle times the clock rate.\nGiven this specification, the bandwidth equation becomes\nTheoretical Bandwidth = Memory Transaction Rate(Gbps) × Memory bus (bits) × \n   (1 byte/8 bits)Table 9.4 Specifications for common GPU memory types\nGraphics \nMemory TypeMemory \nClock (MHz)Memory \nTransactions \n(GT/s)Memory Bus \nWidth (bits)Transaction \nMultiplierTheoretical \nBandwidth \n(GB/s)\nGDDR3 1000 2.0 256 2 64\nGDDR4 1126 2.2 256 2 70\nGDDR5 2000 8.0 256 4 256\nGDDR5X 1375 11.0 384 8 528\nGDDR6 2000 16.0 384 8 768\nHBM1 500 1000.0 4096 2 512\nHBM2 1000 2000.0 4096 2 1000\nExample: Theoretical bandwidth calculations\nFor the NVIDIA V100 that operates at 876 MHz and uses HBM2 memory:\nTheoretical Bandwidth = 0.876 × 4096 × 1/8 × 2 = 897 GB/s\nFor the AMD Radeon Vega20 (MI50) GPU that operates at 1000 MHz:\nTheoretical Bandwidth = 1.000 × 4096 × 1/8 × 2 = 1024 GB/s",5697
120-9.3.3 Roofline performance model for GPUs.pdf,120-9.3.3 Roofline performance model for GPUs,"321 Characteristics of GPU memory spaces\n9.3.2 Measuring the GPU stream benchmark\nBecause most of our applications scale with memory bandwidth, the STREAM Bench-\nmark that measures memory bandwidth is one of the most important micro-benchmarks.\nWe first used the STREAM Benchmark to measure the bandwidth on CPUs in section\n3.2.4. The benchmark process is similar for the GPUs, but we need to rewrite the stream\nkernels in GPU languages. Fortunately, this has been done by Tom Deakin at the Univer-\nsity of Bristol for a variety of GPU languages and hardware in his Babel STREAM code.\n The Babel STREAM Benchmark code measures the bandwidth of a variety of hard-\nware with different programming languages. We use it here to measure the bandwidth\nof an NVIDIA GPU using CUDA. Also available are versions in OpenCL, HIP, Open-\nACC, Kokkos, Raja, SYCL, and OpenMP with GPU targets. These are all different lan-\nguages that can be used for GPU hardware like NVIDIA, AMD, and Intel GPUs.\nExercise: Measuring bandwidth using the Babel STREAM Benchmark\nThe steps to using the stream benchmark for CUDA on an NVIDIA GPU are\n1Clone the Babel STREAM Benchmark with\ngit clone git@github.com:UoB-HPC/BabelStream.git\n2Then type\nmake -f CUDA.make\n./cuda-stream\nThe results for an NVIDIA V100 GPU are\nFunction    MBytes/sec  Min (sec)   Max         Average    \nCopy        800995.012  0.00067     0.00067     0.00067    \nMul         796501.837  0.00067     0.00068     0.00068    \nAdd         838993.641  0.00096     0.00097     0.00096    \nTriad       840731.427  0.00096     0.00097     0.00096    \nDot         866071.690  0.00062     0.00063     0.00063\nThe process is similar for the AMD GPU:\n1Edit the OpenCL.make file and add the paths to your OpenCL header files and\nlibraries.\n2Then type\nmake -f OpenCL.make\n./ocl-stream\nFor the AMD Vega 20, the GPU bandwidth is slightly lower than the NVIDIA GPU. \nUsing OpenCL device gfx906+sram-ecc\nFunction    MBytes/sec  Min (sec)   Max         Average     \nCopy        764889.965  0.00070     0.00077     0.00072     \n322 CHAPTER  9GPU architectures and concepts\n9.3.3 Roofline performance model for GPUs\nWe introduced the roofline performance model for CPUs in section 3.2.4. This model\naccounts for both memory bandwidth and flop performance limits of the system. It is\nsimilarly useful for GPUs to understand their performance limits. (continued)\nMul         764182.281  0.00070     0.00076     0.00072     \nAdd         764059.386  0.00105     0.00134     0.00109     \nTriad       763349.620  0.00105     0.00110     0.00108     \nDot         670205.644  0.00080     0.00088     0.00083     \nExercise: Measuring bandwidth using the Empirical Roofline Toolkit\nFor this exercise, you will need access to an NVIDIA and/or an AMD GPU. A similar\nprocess can be used for other GPUs.\n1Get the roofline toolkit with \ngit clone https:/  /bitbucket.org/berkeleylab/cs-roofline-toolkit.git\n2Then type\ncd cs-roofline-toolkit/Empirical_Roofline_Tool-1.1.0\ncp Config/config.voltar.uoregon.edu Config/config.V100_gpu\n3Edit these settings in Config/config.V100_gpu \nERT_RESULTS Results.V100_gpu\nERT_PRECISION FP64\nERT_NUM_EXPERIMENTS 5\n4Run \ntests ./ert Config/config.V100_gpu\n5View the Results.config.V100_gpu/Run.001/roofline.ps file\ncp Config/config.odinson-ocl-fp64.01 Config/config.Vega20_gpu\n6Edit these settings in Config/config.Vega20_gpu\nERT_RESULTS Results.Vega20_gpu\nERT_CFLAGS  -O3 -x c++ -std=c++11 -Wno-deprecated-declarations \n            -I<path to OpenCL headers>\nERT_LDLIBS  -L<path to OpenCL libraries> -lOpenCL\n7Run \ntests ./ert Config/config.Vega20_gpu\n8View the output in Results.config.Vega20_gpu/Run.001/roofline.ps\n323 Characteristics of GPU memory spaces\nFigure 9.5 shows the results of the roofline benchmarks for both the NVIDIA V100\nand the AMD Vega20 GPUs. \n10100100010000\n0.01 0.1 10 1 100GFLOPs/s\nFLOPs/ByteEmpirical rooﬂine graph (Results.V100_gpupu/Run.001)\n6648.2 GFLOPs/sec (FP64 maximum)\nDRAM - 793.1 GB/sL1 - 2846.3 GB/s\n10100100010000\n0.01 0.1 10 1 100GFLOPs/s\nFLOPs/ByteEmpirical rooﬂine graph (Results.Vega20_gpu/Run.001)\n6236.1 GFLOPs/sec (FP64 maximum)\nL1 - 2082.7 GB/s\nDRAM - 744.0 GB/s\nFigure 9.5 Roofline plots for NVIDIA V100 and AMD Vega 20 showing the bandwidth \nand flop limits for the two GPUs.",4349
121-9.3.4 Using the mixbench performance tool to choose the best GPU for a workload.pdf,121-9.3.4 Using the mixbench performance tool to choose the best GPU for a workload,"324 CHAPTER  9GPU architectures and concepts\n9.3.4 Using the mixbench performance tool to choose the best GPU \nfor a workload\nThere are many GPU options for cloud services and in the HPC server market. Is\nthere a way to figure out the best value GPU for your application? We’ll look at a per-\nformance model that can help you select the best GPU for your workload.\n By changing the independent variable in the roofline plot from arithmetic intensity\nto the memory bandwidth, the performance limits of an application relative to each\nGPU device are highlighted. The mixbench tool was developed to draw out the differ-\nences between the performance of different GPU devices. This information is really no\ndifferent than that shown in the roofline model, but visually it has a different impact.\nLet’s go through an exercise using the mixbench tool to show what you can learn.\nThe results of the benchmark are plotted as the compute rate in GFlops/sec with\nrespect to the memory bandwidth in GB/sec (figure 9.6). Basically, the benchmark\nresults in a horizontal line at the peak flop rate and a vertical dropoff at the memory\nbandwidth limit. The maximum of each of these values is taken and used to plot the\nsingle point at the upper right that captures both the peak flop and peak bandwidth\ncapabilities of the GPU device.\n We can run the mixbench tool for a variety of GPU devices and get their peak per-\nformance characteristics. GPU devices designed for the HPC market have a high dou-\nble-precision floating-point capability, and GPUs for other markets like graphics and\nmachine learning focus on single-precision hardware. Exercise: Getting both the peak flop rate and bandwidth using the \nmixbench tool\n1Get the mixbench code\ngit clone https:/  /github.com/ekondis/mixbench.git\n2Check for CUDA or OpenCL and install if necessary.\ncd mixbench; edit Makefile\n3Fix the path to the CUDA and/or OpenCL installations.\n4Set the executables to build. You can override the path to the CUDA installa-\ntion with\nmake CUDA_INSTALL_PATH=<path>\n5Run either of the following:\n./mixbench-cuda-ro \n./mixbench-ocl-ro\n325 Characteristics of GPU memory spaces\nWe can plot each GPU device in figure 9.7 along with a line representing the arithme-\ntic or operational intensity of an application. Most typical applications are around a 1\nflop/load intensity. At the other extreme, matrix multiplication has an arithmetic\nintensity of 65 flops/load. We show a sloped line for both of these types of applica-\ntions in figure 9.7. If the GPU point is above the application line, we draw a vertical\nline down to the application line to find what the achievable application performance\nwill be. For a device to the right and lower than the application line, we use a horizon-\ntal line to find the performance limit.\nWhat the plot makes clear is the match in the GPU device characteristics relative to\nthe application requirements. For the typical application that has a 1 flop/load arith-\nmetic intensity, GPUs like the GeForce GTX 1080Ti, built for the graphics market, are7000\n0 200 400 600 800 1000V100\n6000\n5000\n4000\n3000\n2000Compute rate (GFlops/s)\nMemory bandwidth (GB/s)1000Figure 9.6 The data output from \na run of mixbench on a V100 \n(shown as a line plot). The \nmaximum bandwidth and floating-\npoint rate is found and used to \nplot the V100 performance point \nin the upper right of the plot. \n7000\n0 200 400 600 800 10006000\n5000\n4000\n3000\n2000Compute rate (GFlops/s)\nMemory bandwidth (GB/s)1000Tesla S2050Quadro K6000P100Vega20 V100\nMatrix multiplication application\nGeForce GTX1080Ti\nTypical 1 ﬂop/word applicationFigure 9.7 A collection of \nperformance points for GPU devices \n(shown on the plot on the right) \nalong with the application \narithmetic intensity (shown as \nstraight lines). Values above the \nline indicate that the application is \nmemory-bound and below the line \nindicates it is compute-bound.",3973
122-9.4 The PCI bus CPU to GPU data transfer overhead.pdf,122-9.4 The PCI bus CPU to GPU data transfer overhead,,0
123-9.4.1 Theoretical bandwidth of the PCI bus.pdf,123-9.4.1 Theoretical bandwidth of the PCI bus,"326 CHAPTER  9GPU architectures and concepts\na good match. The V100 GPU is more suited for the Linpack benchmark used for the\nTOP500 ranking of large computing systems because it’s basically composed of matrix\nmultiplications. GPUs like the V100 are specialized hardware specifically built for the\nHPC market and command a high price premium. For some applications with lower\narithmetic intensity, the commodity GPUs designed for the graphics market can be a\nbetter value. \n9.4 The PCI bus: CPU to GPU data transfer overhead\nThe PCI bus shown in figure 9.1 is needed to transfer data from the CPU to the GPU\nand back. The cost of the data transfer can be a significant limitation on the perfor-\nmance of operations moved to the GPU. It is often critical to limit the amount of data\nthat gets transferred back and forth to get any speedup from the GPU.\n The current version of the PCI bus is called PCI Express (PCIe). It has been\nrevised several times in generations from 1.0 to 6.0 as of this writing. You should know\nwhich generation of the PCIe bus you have in your system to understand its perfor-\nmance limitation. In this section, we show two methods for estimating the bandwidth\nof a PCI bus:\nA back-of-the-envelope theoretical peak performance model\nA micro-benchmark application\nThe theoretical peak performance model is useful for quickly estimating what you\nmight expect on a new system. The model has the benefit that you don’t need to run\nany applications on the system. This is useful when you are just starting a project and\nwould like to quickly estimate by hand the possible performance bottlenecks. In addi-\ntion, the benchmark example shows that the peak bandwidth you can reach depends\non how you make use of the hardware. \n9.4.1 Theoretical bandwidth of the PCI bus\nOn dedicated GPU platforms, all data communication between the GPU and CPU\noccurs over the PCI bus. Because of this, it is a critical hardware component that can\nheavily influence the overall performance of your application. In this section, we go\nover the key features and descriptors of a PCI bus that you need to be aware of in\norder to calculate the theoretical PCI bus bandwidth. Knowing how to calculate this\nnumber on the fly is useful for estimating possible performance limitations when port-\ning your application to GPUs.\n The PCI bus is a physical component that attaches dedicated GPUs to the CPU and\nother devices. It allows for communication between the CPU and GPU. Communica-\ntion occurs over multiple PCIe lanes. We’ll start by presenting a formula for the theo-\nretical bandwidth, and then explain each term. \nTheoretical Bandwidth (GB/s) = Lanes × TransferRate (GT/s) × \n    OverheadFactor(Gb/GT) × byte/8 bits\n327 The PCI bus: CPU to GPU data transfer overhead\nThe theoretical bandwidth is measured in units of gigabytes per second (GB/s). It is\ncalculated by multiplying the number of lanes and the maximum transfer rate for\neach lane and then converting from bits to bytes. The conversion is left in the formula\nbecause the transfer rates are usually reported in GigaTransfers per second (GT/s).\nThe overhead factor is due to an encoding scheme used to ensure data integrity,\nreducing the effective transfer rate. For generation 1.0 devices, the encoding scheme\nhad a cost of 20%, so the overhead factor would be 100%–20% or 80%. From genera-\ntion 3.0 onward, the encoding scheme overhead drops to just 1.54%, so the achieved\nbandwidth becomes essentially the same as the transfer rate. Let’s now dive into each\nof the terms in the bandwidth equation.\nPCIE LANES\nThe number of lanes of a PCI bus can be found by looking through manufacturer\nspecifications, or you can use a number of tools available on Linux platforms. Keep\nin mind that some of these tools might require root privileges. If you do not have\nthese privileges, it is best to consult your system administrator to find out this infor-\nmation. Nonetheless, we will present two options for determining the number of\nPCIe lanes.\n A common utility available on Linux systems is lspci. This utility lists all the compo-\nnents attached to the motherboard. We can use the grep regular expression tool to\nfilter out only the PCI bridge. The following command shows you the vendor infor-\nmation and the device name with the number of PCIe lanes. For this example, (x16)\nin the output indicates that there are 16 lanes.\n$ lspci -vmm | grep ""PCI bridge"" -A2\nClass:     PCI bridge\nVendor:    Intel Corporation\nDevice:    Sky Lake PCIe Controller (x16)\nAlternatively, the dmidecode  command provides similar information:\n$ dmidecode | grep ""PCI""\nPCI is supported\nType: x16 PCI Express\nDETERMINING  THE MAXIMUM  TRANSFER  RATE\nThe maximum transfer rates for each lane in a PCIe bus can directly be determined\nby its design generation. Generation is a specification for the required performance\nof the hardware, much like 4G is an industry standard for cell-phones. The PCI Spe-\ncial Interest Group (PCI SIG) represents industry partners and establishes a PCIe\nspecification that is commonly referred to as generation  or gen for short. Table 9.5\nshows the maximum transfer rate per PCI lanes and direction.\n \n \n328 CHAPTER  9GPU architectures and concepts\nIf you don’t know the generation of your PCIe bus, you can use lspci to get this infor-\nmation. In all of the information output by lspci, we are looking for the link capacity\nfor the PCI bus. In this output, link capacity is abbreviated LnkCap :\n$ sudo lspci -vvv | grep -E 'PCI|LnkCap'\nOutput:\n00:01.0 PCI bridge: \n        Intel Corporation Sky Lake PCIe Controller (x16) (rev 07)\nLnkCap: Port #2, Speed 8GT/s, Width x16, ASPM L0s L1, Exit Latency L0s\nNow that we know the maximum transfer rate from this output, we can use this in the\nbandwidth formula. It’s also helpful to know that this speed can also be aligned with\nthe generation. In this case, the output indicates that we are working with a Gen3\nPCIe system.\nNOTE On some systems, the output from lspci and other system utilities may\nnot give much information. The output is system specific and just reports the\nidentification from each device. If you are unable to determine the character-\nistics from these utilities, your fallback might be to use the PCI benchmark\ncode given in section 9.4.2 to determine the capabilities of your system.\nOVERHEAD  RATES\nTransmitting data across the PCI bus requires additional overhead. Generation 1 and\n2 standards stipulate that 10 bytes are transmitted for every 8 bytes of useful data.\nStarting with generation 3, the transfer transmits 130 bytes for every 128 bytes of data.\nThe overhead factor is the ratio of the number of usable bytes over the total bytes\ntransmitted (table 9.5).Table 9.5 PCI Express (PCIe) specifications by generation\nPCIe GenerationMaximum \nTransfer Rate \n(bi-directional)Encoding \nOverheadOverhead factor\n(100%-encoding \noverhead)Theoretical \nBandwidth\n16 lanes - GB/s\nGen1 2.5 GT/s 20% 80% 4\nGen2 5.0 GT/s 20% 80% 8\nGen3 8.0 GT/s 1.54% 98.46% 15.75\nGen4 16.0 GT/s 1.54% 98.46% 31.5\nGen5 (2019) 32.0 GT/s 1.54% 98.46% 63\nGen6 (2021) 64.0 GT/s 1.54% 98.46% 126",7220
124-9.4.2 A benchmark application for PCI bandwidth.pdf,124-9.4.2 A benchmark application for PCI bandwidth,"329 The PCI bus: CPU to GPU data transfer overhead\nREFERENCE  DATA FOR PCI E THEORETICAL  PEAK BANDWIDTH\nNow that we have all of the necessary information, let’s estimate the theoretical band-\nwidth through an example, using output shown in the previous sections.\n9.4.2 A benchmark application for PCI bandwidth\nThe equation for the theoretical PCI bandwidth gives the expected best peak band-\nwidth. In other words, this is the highest possible bandwidth that an application can\nachieve for a given platform. In practice, the achieved bandwidth can depend on a\nnumber of factors, including the OS, system drivers, other hardware components on\nthe compute node, GPU programming API, and the size of the data block sent across\nthe PCI bus. On most systems you have access to, it is likely that all but the last two\nchoices are out of your control to modify. When developing your application, you are\nin control of the programming API and the size of the data blocks that are transmitted\nacross the PCI bus. \n With this in mind, we are left with the question how does the data block size influ-\nence the achieved bandwidth? This type of question is typically answered with a micro-\nbenchmark.  A micro-benchmark is a small program that is meant to exercise a single\nprocess or piece of hardware that a larger application will use. Micro-benchmarks\nhelp provide some indication of system performance. \n In our situation, we want to devise a micro-benchmark that copies data from the\nCPU to the GPU and vice-versa. Because the data copy is expected to happen in\nmicroseconds to tens of microseconds, we will measure the time it takes to complete\nthe data copy 1,000 times. This time will then be divided by 1,000 to obtain the aver-\nage time to copy data between the CPU and GPU. \n We will do a step-by-step walk through using a benchmark application to measure\nPCI bandwidth. Listing 9.1 shows the code that copies data from the host to the GPU.\nWriting in the CUDA GPU programming language will be covered in chapter 10, but\nthe basic operation is clear from the function names. In listing 9.1, we take in the size\nof a flat 1-D array as input that we want to copy from CPU (host) to the GPU (device). \nNOTE This code is available in the PCI_Bandwidth_Benchmark subdirectory\nat https:/ /github.com/EssentialsofParallelComputing/Chapter9 . Example: Estimating the theoretical bandwidth\nWe have identified that we have a Gen3 PCIe system with 16 lanes. Gen3 systems\nhave a maximum transfer rate of 8.0 GT/s and an overhead factor of 0.985. With 16\nlanes, the theoretical bandwidth is 15.75 GB/s as shown here.\nTheoretical Bandwidth (GB/s)\n                    = 16 lanes × 8.0 GT/s × 0.985 (Gb/GT) × byte/8 bits\n                    = 15.75 GB/s\n330 CHAPTER  9GPU architectures and concepts\nPCI_Bandwidth_Benchmark.c\n35 void Host_to_Device_Pinned( int N, double *copy_time )\n36 {\n37    float *x_host, *x_device;\n38    struct timespec tstart;\n39 \n40    cudaError_t status = cudaMallocHost((void**)&x_host, N*sizeof(float));\n41    if (status != cudaSuccess)\n42          printf(""Error allocating pinned host memory\n"");\n43    cudaMalloc((void **)&x_device, N*sizeof(float));    \n44    \n45    cpu_timer_start(&tstart);\n46    for(int i = 1; i <= 1000; i++ ){\n47       cudaMemcpy(x_device, x_host, N*sizeof(float), \n         cudaMemcpyHostToDevice);                   \n48    }\n49    cudaDeviceSynchronize();     \n50    \n51    *copy_time = cpu_timer_stop(tstart)/1000.0;\n52    \n53    cudaFreeHost( x_host );   \n54    cudaFree( x_device );     \n55 }\nIn listing 9.1, the first step is to allocate memory for both the host and device copy.\nThe routine on line 40 in this listing uses cudaMallocHost  to allocate pinned memory\non the host for faster data transfer. For the routine that uses regular pageable mem-\nory, the standard malloc  and free  calls are used. The cudaMemcpy  routine transfers\nthe data from the CPU host to the GPU. The cudaDeviceSynchronize  call waits until\nthe copy is complete. Before the loop, where we repeat the host to device copy, we\ncapture the start time. We then execute the host-to-device copy 1,000 times and cap-\nture the current time again. The average time for copying from host to device is then\ncalculated by dividing by 1,000. To keep things neat, we free the space held by the host\nand device arrays. \n With the knowledge of the time it takes to transfer an array of size N from host to\ndevice, we can now call this routine multiple times, changing N each time. However,\nwe’re more interested in estimating the achieved bandwidth. \n Recall that the bandwidth is the number of transmitted bytes per unit time. At the\nmoment, we know the number of array elements and the time it takes to copy the\narray between the CPU and GPU. The number of bytes transmitted depends on the\ntype of data stored in the array. For example, in an array of size N containing floats (4\nbytes), the amount of data copied between CPU and GPU is 4 N. If 4 N bytes are trans-\nferred in time T, the achieved bandwidth is\nB = 4 N/TListing 9.1 Copying data from CPU host to GPU device\nAllocates pinned host\nmemory for an array\non the CPU\nAllocates \nmemory for an \narray on the GPU\nCopies memory \nto the GPU\nSynchronizes the GPU so \nthat the work completes\nFrees the arrays\n331 The PCI bus: CPU to GPU data transfer overhead\nThis allows us to build a dataset showing the achieved bandwidth as a function of N.\nThe subroutine in the following listing requires that the maximum array size is speci-\nfied and then returns the bandwidth measured for each experiment.\nPCI_Bandwidth_Benchmark.c\n81 void H2D_Pinned_Experiments(double **bandwidth, int n_experiments,\n         int max_array_size){\n82    long long array_size;\n83    double copy_time;\n84 \n85    for(int j=0; j<n_experiments; j++){     \n86       array_size = 1;\n87       for(int i=0; i<max_array_size; i++ ){      \n88 \n89          Host_to_Device_Pinned( array_size, &copy_time );   \n90 \n91          double byte_size=4.0*array_size;                               \n92          bandwidth[j][i] = byte_size/(copy_time*1024.0*1024.0*1024.0);  \n93 \n94          array_size = array_size*2;              \n95       }\n96    }\n97 }\nHere, we loop over array sizes, and for each array size, we obtain the average host-\nto-device copy time. The bandwidth is then calculated by the number of bytes cop-\nied divided by the time it takes to copy. The array contains floats, which have four\nbytes for each array element. Now, let’s walk through an example that shows how\nyou can use the micro-benchmark application to characterize the performance of\nyour PCI Bus.\nPERFORMANCE  OF A GEN3 X16 ON A LAPTOP\nWe ran the PCI bandwidth benchmark application on a GPU accelerated laptop. On\nthis system, the lspci  command shows that it is equipped with a Gen3 x16 PCI bus:\n$ sudo lspci -vvv | grep -E 'PCI|LnkCap'\n00:01.0 PCI bridge: Intel Corporation Sky Lake PCIe Controller (x16) \n                    (rev 07)\n            LnkCap: Port #2, Speed 8GT/s, Width x16, ASPM L0s L1, \n                    Exit Latency L0s\nFor this system, the theoretical peak bandwidth of the PCI Bus is 15.8 GB/s. Figure 9.8\nshows a plot of the achieved bandwidth in our micro-benchmark application (curved\nlines) compared to the theoretical peak bandwidth (horizontal dashed lines). The\nshaded region around the achieved bandwidth indicates the +/– 1 standard deviation\nin bandwidth.\n First, notice that when small chunks of data are sent across the PCI bus, the\nachieved bandwidth is low. Beyond array sizes of 107 bytes, the achieved bandwidthListing 9.2 Calling a CPU to GPU memory transfer for different array sizes\nRepeats the \nexperiments \na few times\nDoubles\nthe array\nsize with\neach\niterationCalls CPU to GPU \nmemory test and \ntiming\nCalculates\nbandwidth",7913
125-9.5.2 A higher performance alternative to the PCI bus.pdf,125-9.5.2 A higher performance alternative to the PCI bus,"332 CHAPTER  9GPU architectures and concepts\napproaches a maximum around 11.6 GB/s. Note also that the bandwidth with pinned\nmemory is much higher than pageable memory, and pageable memory has a much\nwider variation of performance results for each memory size. It is helpful to know\nwhat pinned and pageable memory are to understand the reason for this difference.\nPinned memory —Memory that cannot be paged from RAM and, thus, can be\ndirectly sent to the GPU without first making a copy\nPageable memory —Standard memory allocations that can be paged out to disk\nAllocating pinned memory reduces the memory available to other processes because\nthe OS kernel can no longer make the memory page out to disk so other processes\ncan use it. Pinned memory is allocated from the standard DRAM memory for the pro-\ncessor. The allocation process takes a little bit longer than a regular allocation. When\nusing pageable memory, it must be copied into a pinned memory location before it\ncan be sent. The pinned memory prevents it from being paged out to disk while the\nmemory is being transferred. The example in this section shows that larger data trans-\nfers between the CPU and GPU can result in higher achieved bandwidth. Further, on\nthis system, the maximum achieved bandwidth only reaches about 72% of the theoret-\nical peak performance.\n9.5 Multi-GPU platforms and MPI\nNow that we’ve introduced the basic components of a GPU-accelerated platform, we’ll\ndiscuss more exotic configurations that you might encounter. These exotic configura-\ntions come from the introduction of multiple GPUs. Some platforms offer multiple\nGPUs per node, connected to one or more CPUs. Others offer connections to multi-\nple compute nodes over network hardware.\n On the types of multi-GPU platforms (figure 9.9), it is usually necessary to use an\nMPI+GPU approach to parallelism. For data parallelism, each MPI rank is assigned to\none of the GPUs. Let’s look at a couple of possibilities:Array size (B)Theoretical and benchmark bandwidth\nBandwidth (GB/s)\n02468101214\n102 103 104 105 106 107 108Pinned benchmark\nPageable benchmark\nGen2 theoretical peak\nGen1 theoretical peakGen3 theoretical peak\nFigure 9.8 A theoretical peak \nbandwidth (horizontal lines) and an \nempirically measured bandwidth from \nthe micro-benchmark application are \nshown for a Gen3 x16 PCIe system. \nThe figure also shows results for both \npinned and pageable memory.\n333 Multi-GPU platforms and MPI\nSingle MPI rank drives each GPU\nMultiple MPI ranks multiplex their work on a GPU\nSome of the early GPU software and hardware did not handle multiplexing efficiently,\nresulting in poor performance. With many of the performance problems fixed in the\nlatest software, it is becoming increasingly attractive to multiplex MPI ranks onto the\nGPUs.\n9.5.1 Optimizing the data movement between GPUs across \nthe network\nTo use multiple GPUs, we have to send data from one GPU to another. Before we can\ndiscuss the optimization, we need to describe the standard data transfer process.\n1Copy the data from the GPU to the host processor\naMove the data across the PCI bus to the processor\nbStore the data in CPU DRAM memory\n2Send the data in an MPI message to another processor\naStage the data from CPU memory to the processor\nbMove the data across the PCI bus to the network interface card (NIC)\ncStore the data from the processor to CPU memory\n3Copy the data from the second processor to the second GPU\naLoad the data from CPU memory to the processor\nbSend the data across the PCI bus to the GPU\nAs figure 9.10 shows, this is a lot of data movement and will be a major limitation to\napplication performance.\n In an NVIDIA GPUDirect®, CUDA adds the capability to send the data in a mes-\nsage. AMD has a similar capability called DirectGMA for GPU-to-GPU in OpenCL. The\npointer to the data still has to be transferred, but the message itself gets sent over the PCI\nbus directly from one GPU to another GPU, thereby reducing the memory movement.Processor\nCPUGPU\nNetworkProcessor ProcessorGPU\nProcessor Processor\nCPUGPU\nProcessor ProcessorGPU\nProcessor\nFigure 9.9 Here we illustrate a multi-GPU platform. A single compute node can have multiple GPUs and \nmultiple processors. There can also be multiple nodes connected across a network.",4345
126-9.6.1 Reducing time-to-solution.pdf,126-9.6.1 Reducing time-to-solution,"334 CHAPTER  9GPU architectures and concepts\n9.5.2 A higher performance alternative to the PCI bus\nThere is little argument that the PCI bus is a major limitation for compute nodes with\nmulti-GPUs. While this is mostly a concern for large applications, it also impacts heavy\nworkloads such as in machine learning on smaller clusters. NVIDIA introduced\nNVLink® to replace GPU-to-GPU and GPU-to-CPU connections with their Volta line\nof P100 and V100 GPUs. With NVLink 2.0, the data transfer rates can reach 300\nGB/sec. The new GPUs and CPUs from AMD incorporate Infinity Fabric to speed\nup data transfers. And Intel has been accelerating data transfers between CPUs and\nmemory for some years.\n9.6 Potential benefits of GPU-accelerated platforms\nWhen is porting to GPUs worth it? At this point you’ve seen the theoretical peak per-\nformance for modern GPUs and how this compares to CPUs. Compare the roofline\nplot for the GPU in figure 9.5 to the CPU roofline plot in section 3.2.4 for both the\nfloating-point calculation and the memory bandwidth limits. In practice, manySystem memoryNetwork\nGPU\nmemoryPCI bus\nGPU\nmemorySystem memory\nGPU\nmemoryPCI bus\nGPU\nmemoryProcessor Processor\nGPU GPUProcessor Processor\nGPU GPU\nSystem memory\nGPUNetwork\nGPU\nmemoryProcessor\nPCI bus\nGPU\nGPU\nmemorySystem memory\nGPU\nGPU\nmemoryPCI bus\nGPU\nGPU\nmemoryProcessor Processor Processor Figure 9.10 On the top is \nthe standard data movement \nfor sending data from a GPU \nto other GPUs. On the bottom, \nthe data movement bypasses \nthe CPU when moving data \nfrom one GPU to another.\n335 Potential benefits of GPU-accelerated platforms\napplications do not reach these peak performance values. However, with the ceilings\nraised for GPUs, there is potential to outperform CPU architectures relative to a few\nmetrics. These include the time to execute your application, energy consumption,\ncloud computing costs, and scalability. \n9.6.1 Reducing time-to-solution\nSuppose you have an existing code that runs on CPUs. You’ve spent a lot of time put-\nting OpenMP or MPI into the code so that you can use all of the cores on the CPU.\nYou feel like the code is well tuned, but a friend has told you that you might benefit\nmore by porting your code to GPUs. You have more than 10,000 lines of code in your\napplication, and you know that it will take considerable effort to get your code run-\nning on GPUs. At this point, you’re interested in the prospect of running on GPUs\nbecause you like learning new things and you trust your friend’s insights. Now, you\nhave to make the case to your colleagues and your boss. \n The important measure for your application is to reduce the time-to-solution for\njobs that run days at a stretch. The best way to get across the impact of a reduction in\ntime-to-solution is to look at an example. We’ll use the Cloverleaf application as a\nproxy for this study.\nNow let's get the run time for a couple of possible replacement platforms.Example: Considering an upgrade to your current workhorse system\nHere are the steps to gather the performance of Cloverleaf on the base system. We\nuse an Intel Ivybridge system (E5-2650 v2 @ 2.60GHz) with 16 physical cores. On\naverage your application runs for about 500,000 cycles. You can get an estimate of\nthe run time for a short sample run with these steps:\n1Clone Cloverleaf.git with\ngit clone --recursive git@github.com:UK-MAC/CloverLeaf.git CloverLeaf\n2Then enter these commands\ncd CloverLeaf_MPI\nmake COMPILER=INTEL\nsed -e ‘1,$s/end_step=2955/end_step=500/’ InputDecks/clover_bm64.in\\n         >clover.in\nmpirun -n 16 --bind-to core ./clover_leaf\nThe run time for 500 cycles is 615.1 secs or 1.23 secs per cycle. This gives you a\nrun time of 171 hours or 7 days and 3 hours.\n336 CHAPTER  9GPU architectures and concepts\nNot bad! Less than half the time as before. You are about ready to purchase this sys-\ntem but then think that maybe you should check out those GPUs that you have been\nhearing about.\nIs this typical of the performance gains with GPUs? Performance gains for applica-\ntions span a wide range, but these results are not unusual. Example CPU replacement: Skylake Gold 6152 with 2.10GHz with 36 \nphysical cores\nThe only difference for this run is that we increase the 16 processors to 36 for mpirun\nwith this command:\nmpirun -n 36 --bind-to core ./clover_leaf\nThe run time for the Skylake system is 273.3 secs or 0.55 secs per cycle. This would\ngive a run time for the typical application problem of 76.4 hours or 3 days and 4\nhours. \nExample GPU replacement: V100\nCloverLeaf has a CUDA version that runs on the V100. You measure the performance\nwith the following steps:\n1Clone Cloverleaf.git with\ngit clone --recursive git@github.com:UK-MAC/CloverLeaf.git CloverLeaf\n2Then type\ncd CloverLeaf_CUDA\n3Add the CUDA architecture flags for Volta to the makefile to the current list of\nCODE_GEN architectures:\nCODE_GEN_VOLTA=-arch=sm_70\n4Add the path to CUDA library CUDART, if necessary:\nmake COMPILER=GNU NV_ARCH=VOLTA\nsed -e ‘1,$s/end_step=2955/end_step=500/’ clover_bm64.in >clover.in\n./clover_leaf\nWhoa! The run is so fast you don’t even have time to get a cup of coffee. But there\nit is: 59.3 secs! This is 0.12 secs per cycle or, for the full test problem size, a sweet\n16.5 hours. Compared to the Skylake system, this is 4.6 times faster, and 10.4\ntimes faster than the original Ivy Bridge system. Just imagine how much more work\nyou could get done with an overnight turnaround instead of days!",5560
127-9.6.2 Reducing energy use with GPUs.pdf,127-9.6.2 Reducing energy use with GPUs,"337 Potential benefits of GPU-accelerated platforms\n9.6.2 Reducing energy use with GPUs\nEnergy costs are becoming increasingly important for parallel applications. Where\nonce the energy consumption of computers was not a concern, now the energy costs\nof running the computers, storage disks, and cooling system are fast approaching the\nsame levels as the hardware purchase costs over the lifetime of the computing system. \n In the race to Exascale computing, one of the biggest challenges is keeping the\npower requirements for the Exascale system to around 20 MW. For comparison, this\nis about enough power to supply 13,000 homes. There simply isn’t enough installed\npower in data centers to go much beyond that. At the other end of the spectrum,\nsmart phones, tablets, and laptops run on batteries with a limited amount of avail-\nable energy (between charges). On these devices, it can be beneficial to focus on\nreducing energy costs for a computation to stretch out the battery life. Fortunately,\naggressive efforts to reduce energy usage have kept the rate of an increase in power\ndemands reasonable.\n Accurately calculating the energy costs of an application is challenging without\ndirect measurements of power usage. However, you can get a higher bound on the\ncost by multiplying the manufacturer’s thermal design power (TDP) by the run time\nof the application and the number of processors used. TDP is the rate at which energy\nis expended under typical operational loads. The energy consumption for your appli-\ncation can be estimated using the formula\nEnergy = ( N Processors) × ( R Watts/Processor) × ( T hours)\nwhere Energy  is the energy consumption, N is the number of processors, R is the TDP,\nand T is the application’s run time. Let’s compare a GPU-based system to a roughly\nequivalent CPU system (table 9.6). We’ll assume our application is memory bound, so\nwe’ll calculate the costs and energy consumption for a 10 TB/sec system. \nTo calculate the energy costs for one day, we take the specifications from table 9.6 and\ncalculate the nominal energy costs.\n Table 9.6 Designing a 10 TB/s bandwidth GPU and CPU system.\nNVIDIA V100 Intel CPU Skylake Gold 6152\nNumber 12 GPUs 45 processors (CPUs)\nBandwidth 12 × 850 GB/s = 10.2 TB/s 45 × 224 GB/s = 10.1 TB/s \nCost 12 × $11,000 = $132,000 45 × $3,800 = $171,000\nPower 300 watt per GPU 140 watt per CPU\nEnergy for 1 day 86.4 kWhrs 151.2 kWhrs\n338 CHAPTER  9GPU architectures and concepts\nIn general, GPUs have a higher TDP than CPUs (300 watts vs. 140 watts from table 9.6)\nso they consume energy at a higher rate. But GPUs can potentially reduce run time or\nrequire only a few to run your calculation. The same formula can be used as before,\nwhere N is now seen as the number of GPUs.\nNow we can see there is great potential for a GPU system, but the nominal values that\nwe used might be quite a ways from reality. We could further refine this estimate by\ngetting measured performance and energy draws for our algorithm. \n Achieving a reduction in energy cost through GPU accelerator devices requires\nthat the application expose sufficient parallelism and that the device’s resources are\nefficiently utilized. In the hypothetical example, we were able to reduce the energy\nusage in half when running on 12 GPUs for the same amount of time it takes to exe-\ncute on 45 fully subscribed CPU processors. The formula for energy consumption\nalso suggests other strategies for reducing energy costs. We’ll discuss these strategies\nin a bit, but it’s important to note that, in general, a GPU consumes more energy\nthan a CPU per unit time. We’ll start by examining the energy consumption between\na single CPU processor and a single GPU.\n Listing 9.3 shows how to plot the power and utilization for the V100 GPU.\n Example: TDP for Intel’s 22 core Xeon Gold 6152 Processor\nIntel’s 22 core Xeon Gold 6152 Processor has a TDP of 140 W. Suppose that your\napplication uses 15 of these processors for 24 hours to run to completion. The esti-\nmated energy usage for your application is \nEnergy = (45 Processors) × (140 W/Processors) × (24 hrs) = 151.2 kWhrs\nThis is a significant amount of energy! Your energy consumption for this calculation\nis enough to power seven homes for the same 24 hours.\nExample: TDP for a multi-GPU platform\nSuppose that you’ve ported your application to a multi-GPU platform. You can now\nrun your application on four NVIDIA Tesla V100 GPUs in 24 hrs. NVIDIA’s Tesla\nV100 GPU has a maximum TDP of 300 W. The estimated energy usage for your\napplication is\nEnergy = (12 GPUs) × (300 W/GPUs) × (24 hrs) = 86.4 kWhrs\nIn this example, the GPU accelerated application runs at a much lower energy cost\ncompared to the CPU-only version. Note that in this case, even though the time to\nsolution remains the same, the energy expense is cut by about 40%! We also see a\n20% reduction in initial hardware costs, but we haven’t accounted for the host CPU\nexpense for the GPUs. \n339 Potential benefits of GPU-accelerated platforms\npower_plot.py\n 1 import matplotlib.pyplot as plt\n 2 import numpy as np\n 3 import re\n 4 from scipy.integrate import simps\n 5 \n 6 fig, ax1 = plt.subplots()\n 7 Example: Monitoring GPU power consumption over application lifetime\nLet’s go back to the CloverLeaf problem that we ran earlier on the V100 GPU. We used the\nnvidia-smi (NVIDIA System Management Interface) tool to collect performance metrics for the\nrun, including power and GPU utilization. To do this, we ran the following command before run-\nning our application:\nnvidia-smi dmon -i 0 --select pumct -c 65 --options DT\ \n  --filename gpu_monitoring.log &\nThe following table shows the options for the nvidia-smi command.\nThe shortened data output to the file looks like\nTime      pwr gtemp mtemp   sm  mem enc dec    fb bar1 mclk pclk rxpci txpci\nHH:MM:SS    W     C     C    %    %   %   %    MB   MB  MHz  MHz  MB/s  MB/s\n21:36:47   64    43    41   24   28   0   0     0    0  877 1530     0     0\n21:36:48  176    44    44   96  100   0   0 11181    0  877 1530     0     0\n21:36:49  174    45    45  100  100   0   0 11181    0  877 1530     0     0\n... \nListing 9.3 Plotting the power and utilization data from nvidia-smidmon Collects monitoring data.\n-i 0 Queries GPU device 0.\n--select pumct Selects power [p], utilization [u], memory usage [m], clocks [c], PCI throughput \n[t]. You can also use -s as shorthand for --select .\n-c 65 Collects 65 samples. Default time is 1 s. \n-d <#> Changes the sampling interval.\n--options DT Prepends monitoring data with date in YYYMMDD format and time in HH:MM::SS \nformat, respectively.\n--filename \n<name>Writes output to the specified filename.\n& Puts the job in the background so you can run your application.\n340 CHAPTER  9GPU architectures and concepts\n 8 gpu_power = []\n 9 gpu_time = []\n10 sm_utilization = []\n11 \n12 # Collect the data from the file, ignore empty lines\n13 data = open('gpu_monitoring.log', 'r')\n14 \n15 count = 0\n16 energy = 0.0\n17 nominal_energy = 0.0\n18 \n19 for line in data:\n20     if re.match('^ 2019',line):\n21         line = line.rstrip(""\n"")\n22         dummy, dummy, dummy, gpu_power_in, dummy, dummy,  \nsm_utilization_in, dummy,\n             dummy, dummy, dummy, dummy, dummy, dummy, dummy, dummy = \nline.split()\n23         if (float(sm_utilization_in) > 80):\n24           gpu_power.append(float(gpu_power_in))\n25           sm_utilization.append(float(sm_utilization_in))\n26           gpu_time.append(count)\n27           count = count + 1\n28           energy = energy + float(gpu_power_in)*1.0      \n29           nominal_energy = nominal_energy + float(300.0)*1.0     \n30 \n31 print(energy, ""watts-secs"", simps(gpu_power, gpu_time))   \n32 print(nominal_energy, ""watts-secs"", ""  ratio \n"",energy/nominal_energy*100.0)                        \n33 \n34 ax1.plot(gpu_time, gpu_power, ""o"", linestyle='-', color='red')\n35 ax1.fill_between(gpu_time, gpu_power, color='orange')\n36 ax1.set_xlabel('Time (secs)',fontsize=16)\n37 ax1.set_ylabel('Power Consumption (watts)',fontsize=16, color='red')\n38 #ax1.set_title('GPU Power Consumption from nvidia-smi')\n39 \n40 ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n41 \n42 ax2.plot(gpu_time, sm_utilization, ""o"", linestyle='-', color='green')\n43 ax2.set_ylabel('GPU Utilization (%)',fontsize=16, color='green')\n44 \n45 fig.tight_layout()\n56 plt.savefig(""power.pdf"")\n57 plt.savefig(""power.svg"")\n58 plt.savefig(""power.png"", dpi=600)\n59 \n60 plt.show()\nFigure 9.11 shows the resulting plot. At the same time, we integrate the area under the\ncurve to get the energy usage. Note that even with the utilization at 100%, the power\nrate is only about 61% of the nominal GPU power specification. At idle, the GPU\npower consumption is around 20% of the nominal amount. This shows that the real\npower usage rate for GPUs is significantly lower than estimates based on nominalIntegrates power \ntimes time to get \nenergy in watts/s\nGets the energy \nusage based on \nnominal power \nspecification\nPrints the calculated\nenergy and uses the\nsimps integration\nfunction from scipyCalculates\nthe actual\nvs. nominal\nenergy\nusage\n341 Potential benefits of GPU-accelerated platforms\nspecifications. CPUs also are lower than the nominal amount, but probably not by as\ngreat a percentage of their nominal rate.\nWHEN WILL MULTI -GPU PLATFORMS  SAVE YOU ENERGY ?\nIn general, parallel efficiency drops off as you add more CPUs or GPUs (remember\nAmdahl’s law from section 1.2?), and the cost for a computational job goes up. Some-\ntimes, there are fixed costs (such as storage) associated with the overall run time of a\njob that are reduced if the job is finished sooner and the data can be transferred or\ndeleted. The usual situation, however, is that you have a suite of jobs to run with a\nchoice of how many processors for each job. The following example highlights the\ntradeoffs in this situation.\nExample: Suite of parallel jobs to run\nYou have 100 jobs to run, roughly of the same type and length. You can run the jobs\non either 20 processors or 40 processors. At 20 processors, the jobs take 10 hours\neach. The parallel efficiency in this processor range is about 80%. The cloud cluster\nyou have access to has 200 processors. Let’s look at two scenarios.\nCase 1. Running 10 jobs at a time with 20 processors gives us this solution:\nTotal Suite Run Time = 10 hrs × 100/10 = 100 hrs\nCase 2. Run 5 jobs at a time with 40 processors\nIncreasing the number of processors to 40 drops the run time to 5 hrs if parallel effi-\nciency is perfect. But it is only 80% efficient, so the run time is 6.25 hrs. How did we0 10 20 30\nTime (secs)GPU utilization (%)\nPower consumption (watts)100.0\n200\n100\n50\n015099.5\n99.0\n98.5\n98.0\n97.5\n97.0\n96.5\n96.0\n40 50Figure 9.11 Power consumption \nfor the CloverLeaf problem running \non the V100. We integrate under \nthe power curve to get 10.4 kJ for a \nrun that lasted about 60 seconds. \nThe rate of power consumption is \nabout 61% of the nominal power \nspecification for the V100 GPU.",11177
128-9.7 When to use GPUs.pdf,128-9.7 When to use GPUs,"342 CHAPTER  9GPU architectures and concepts\nThis example shows that if we are optimizing the run time for a large suite of jobs, it is\noften better to use less parallelism. In contrast, if we are more concerned with the\nturnaround time for a single job, more processors will be better.\n9.6.3 Reduction in cloud computing costs with GPUs\nCloud computing services from Google and Amazon let you match your workloads to\na wide range of compute server types and demands.\nIf your application is memory bound, you can use a GPU that has a lower flops-\nto-loads ratio at a lower cost.\nIf you are more concerned with turnaround time, you can add more GPUs or\nCPUs.\nIf your deadlines are less serious, you can use preemptible resources at a consid-\nerable reduction in cost.\nAs the cost of computing is more visible with cloud computing services, optimizing\nyour application’s performance becomes a higher priority. Cloud computing has the\nadvantage of giving you access to a wider variety of hardware than you can have on-site\nand more options to match the hardware to the workload.(continued)\nget this number? Assume that 20 processors is the base case. For doubling the num-\nber of processors, the parallel efficiency formula is\nPefficiency  = S/Pmult = 80%\nS is the speedup for the problem. The processor multiplier, Pmult, is a factor 2. Solving\nfor the speedup, we get\nS = 0.8 × Pmult = 0.8 × 2 = 1.6\nNow we use the speedup equation to calculate the new time, TN:\nS = Tbase/Tnew\nWe use Tbase instead of Tserial for this form of the speedup equation. Parallel effi-\nciency often decreases as we add processors, so we want the relationship at this\npoint on the efficiency curve. Solving for Tnew, we get\nTnew = Tbase/S = 10/1.6 = 6.25 hrs\nWith this run time for 40 processors, we can now calculate the total suite run time:\nTotal Suite Time = 6.25 hrs × 100/5 = 125 hrs\nIn summary, the scenario used in case 1 is much faster to accomplish the same\namount of work.",1994
129-9.8 Further explorations.pdf,129-9.8 Further explorations,,0
130-9.8.2 Exercises.pdf,130-9.8.2 Exercises,"343 Further explorations\n9.7 When to use GPUs\nGPUs are not general-purpose processors. They are most appropriate when the com-\nputation workload is similar to a graphics workload—lots of operations that are identi-\ncal. There are some areas where GPUs still do not perform well, although with each\niteration of the GPU hardware and software, some of these are addressed.\nLack of parallelism —To paraphrase Spiderman, “ With great power comes great need\nfor parallelism. ” If you don’t have the parallelism, GPUs can’t do a lot for you.\nThis is the first law of GPGPU programming.\nIrregular memory access —CPUs also struggle with this. The massive parallelism of\nGPUs brings no benefit to this situation. This is the second law of GPGPU pro-\ngramming.\nThread divergence —Threads on GPUs all execute on each and every branch. This\nis a characteristic of SIMD and SIMT architectures (see section 1.4). Small\namounts of short branching are fine, but wildly different branch paths do poorly.\nDynamic memory requirements —Memory allocation is done on the CPU, which\nseverely limits algorithms that require memory sizes determined on the fly.\nRecursive algorithms —GPUs have limited stack memory resources, and suppliers\noften state that recursion is not supported. However, a limited amount of recur-\nsion has been demonstrated to work in the mesh-to-mesh remapping algo-\nrithms in section 5.5.2.\n9.8 Further explorations\nGPU architectures continue to evolve with each iteration of hardware design. We sug-\ngest that you continue to track the latest developments and innovations. At the outset,\nGPU architectures were first and foremost for graphics performance. But the market\nhas broadened into machine learning and computation as well.\n9.8.1 Additional reading\nFor a much more detailed discussion of STREAM Benchmark performance and how\nit varies across parallel programming languages, we refer you to the following paper:\nT. Deakin, J. Price, et al., “Benchmarking the achievable memory bandwidth of\nmany-core processors across diverse parallel programming models,” GPU-STREAM ,\nv2.0 (2016). Paper presented at Performance Portable Programming models for\nManycore or Accelerators (P^3MA) Workshop at ISC High Performance, Frank-\nfurt, Germany.\nA good resource on the roofline model for GPUs can be found at Lawrence Berkeley\nLab. A good starting point is\nCharlene Yang and Samuel Williams, “Performance Analysis of GPU-Accelerated\nApplications using the Roofline Model,” GPU Technology Conference (2019) avail-\nable at https:/ /crd.lbl.gov/assets/Uploads/GTC19-Roofline.pdf .",2616
131-Summary.pdf,131-Summary,"344 CHAPTER  9GPU architectures and concepts\nIn this chapter, we presented a simplified view of the mixbench performance model\nby assuming simple application performance requirements. The following paper\npresents a more thorough procedure to account for the complications of real\napplications:\nElias Konstantinidis and Yiannis Cotronis, “A quantitative roofline model for GPU\nkernel performance estimation using micro-benchmarks and hardware metric pro-\nfiling.” Journal of Parallel and Distributed Computing  107 (2017): 37–56.\n9.8.2 Exercises\n1Table 9.7 shows the achievable performance for a 1 flop/load application. Look\nup the current prices for the GPUs available on the market and fill in the last\ntwo columns to get the flop per dollar for each GPU. Which looks like the best\nvalue? If turnaround time for your application run time is the most important\ncriterion, which GPU would be best to purchase?\n2Measure the stream bandwidth of your GPU or another selected GPU. How\ndoes it compare to the ones presented in the chapter?\n3Use the likwid performance tool to get the CPU power requirements for the\nCloverLeaf application on a system where you have access to the power hard-\nware counters.\nSummary\nThe CPU-GPU system can provide a powerful boost for many parallel applica-\ntions. It should be considered for any application with a lot of parallel work.\nThe GPU component of the system is in reality a general-purpose parallel accel-\nerator. This means that it should be given the parallel part of the work.\nData transfer over the PCI bus and memory bandwidth are the most common\nperformance bottlenecks on CPU-GPU systems. Managing the data transfer and\nmemory use is important for good performance.Table 9.7 Achievable performance for a 1 flop/load application with various GPUs\nGPUAchievable \nPerformance Gflops/secPrice Flops/$\nV100 108.23\nVega 20 91.38\nP100 74.69\nGeForce GTX1080Ti 44.58\nQuadro K6000 31.25\nTesla S2050 18.50\n345 Summary\nYou’ll find a wide range of GPUs available for different workloads. Selecting the\nmost suitable model will give the best price to performance ratio.\nGPUs can reduce time-to-solution and energy costs. This can be a prime motiva-\ntor in porting an application to GPUs.",2262
132-10 GPU programming model.pdf,132-10 GPU programming model,"346GPU programming model\nIn this chapter, we will develop an abstract model of how work is performed on\nGPUs. This programming model fits a variety of GPU devices from different vendors\nand across the models from each vendor. It is also a simpler model than what occurs\non the real hardware, capturing just the essential aspects required to develop an\napplication. Fortunately, various GPUs have a lot of similarities in structure. This is a\nnatural result of the demands of high-performance graphics applications.\n The choice of data structures and algorithms has a long-range impact on the\nperformance and ease of programming for the GPU. With a good mental model of\nthe GPU, you can plan how data structures and algorithms map to the parallelism\nof the GPU. Especially for GPUs, our primary job as application developers is toThis chapter covers\nDeveloping a general GPU programming model\nUnderstanding how it maps to different vendors’ \nhardware\nLearning what details of the programming model \ninfluence performance\nMapping the programming model to different GPU \nprogramming languages\n347\nexpose as much parallelism as we can. With thousands of threads to harness, we need\nto fundamentally change the work so that there are a lot of small tasks to distribute\nacross the threads. In a GPU language, as in any other parallel programming lan-\nguage, there are several components that must exist. These are a way to\nExpress the computational loops in a parallel form for the GPU (see section 10.2)\nMove data between the host CPU and the GPU compute device (see section 10.2.4)\nCoordinate between threads that are needed for a reduction (see section 10.4)\nLook for how these three components are accomplished in each GPU programming\nlanguage. In some languages, you directly control some aspects, and in others, you\nrely on the compiler or template programming to implement the needed operations.\nWhile the operation of a GPU might seem mysterious, these operations are not all\nthat different from what is necessary on a CPU for parallel code. We have to write\nloops that are safe for fine-grained parallelism, sometimes called do concurrent  for\nFortran or forall  or foreach  in C/C++. We have to think about data movement\nbetween nodes, processes, and the processor. We also have to have special mecha-\nnisms for reductions.\n For native GPU computation languages like CUDA and OpenCL, the program-\nming model is exposed as part of the language. These GPU languages are covered in\nchapter 12. In that chapter, you’ll explicitly manage many aspects of parallelization\nfor the GPU in your program. But with our programming model, you will be better\nprepared to make important programming decisions for better performance and scal-\ning across a wide range of GPU hardware.\n If you are using a higher-level programming language, such as the pragma-based\nGPU languages covered in chapter 11, do you really need to understand all the details\nof the GPU programming model? Even with pragmas, it is still helpful to understand\nhow the work gets distributed. When you use a pragma, you are trying to steer the\ncompiler and library to do the right thing. In some ways, this is harder than writing\nthe program directly.\n The goal of this chapter is to help you develop your application design for the\nGPU. This is mostly independent of the programming language for the GPU. There\nare questions you should answer up front. How will you organize your work and what\nkind of performance can be expected? Or the more basic question of whether your\napplication should even be ported to the GPU or would it be better off staying with\nthe CPU? GPUs, with their promise of an order-of-magnitude performance gains and\nlower energy use, are a compelling platform. But these are not a panacea for every\napplication and use case. Let’s dive into the details of the GPU’s programming model\nand see what it can do for you.\nNOTE We encourage you to follow along with the examples for this chapter\nat https:/ /github.com/EssentialsofParallelComputing/Chapter10 .",4100
133-10.1 GPU programming abstractions A common framework.pdf,133-10.1 GPU programming abstractions A common framework,,0
134-10.1.2 Inability to coordinate among tasks.pdf,134-10.1.2 Inability to coordinate among tasks,"348 CHAPTER  10 GPU programming model\n10.1 GPU programming abstractions: A common framework\nThe GPU programming abstractions are possible for a reason. The basic characteris-\ntics, which we explore in more detail in a bit, include the following. Then we’ll take a\nquick look at some basic terminology for GPU parallelism.\nGraphics operations have massive parallelism \nOperations cannot be coordinated among tasks\n10.1.1 Massive parallelism \nAbstractions are based on what is necessary for high-performance graphics with GPUs.\nGPU workflows have some special characteristics that help to drive the commonality\nin the GPU-processing techniques. For a high frame rate and high-quality graphics,\nthere are lots of pixels, triangles, and polygons to process and display. \n Because of the large amounts of data, GPUs have massive parallelism. The opera-\ntions on the data are generally identical, so GPUs use similar techniques to apply a sin-\ngle instruction to multiple data items to gain another level of efficiency. Figure 10.1\nshows the common programming abstractions across various vendors and GPU mod-\nels. These can be summarized as three or four basic techniques.\nWe start with the computational domain and iteratively break up the work with the fol-\nlowing components. We’ll discuss each of these subdivisions of work in sections 10.1.4\nthrough 10.1.8:\nData decomposition\nChunk-sized work for processing with some shared, local memory\nOperating on multiple data items with a single instruction\nVectorization (on some GPUs)\nOne thing to note from these GPU parallel abstractions is that there are fundamen-\ntally three, or maybe four, different levels of parallelization that you can apply to a\ncomputational loop. In the original graphics use case, there is not much of a need to\ngo beyond two or three dimensions and the corresponding number of parallelization\nlevels. If your algorithm has more dimensions or levels, you must combine some com-\nputational loops to fully parallelize your problem.GY\nGXComputational domain\nData decompositionWork group\nshared resources\nSubgroupOne instruction\n+, –, x, /\n32 Work items (data)SIMD/VectorSome\nGPUs\nFigure 10.1 Our mental model for GPU parallelization contains the common programming abstractions across \nmost GPU hardware.",2318
135-10.1.4 Data decomposition into independent units of work An NDRange or grid.pdf,135-10.1.4 Data decomposition into independent units of work An NDRange or grid,"349 GPU programming abstractions: A common framework\n10.1.2 Inability to coordinate among tasks\nGraphics workloads do not require much coordination within the operations. But as\nwe will see in later sections, there are algorithms such as reductions that require coor-\ndination. We will have to develop complicated schemes to handle these situations.\n10.1.3 Terminology for GPU parallelism\nThe terminology for components of the GPU parallelism varies across vendors, add-\ning a degree of confusion when reading programming documentation or articles. To\nhelp with cross-referencing the use of various terms, we summarize the official terms\nfrom each vendor in table 10.1.\nOpenCL is the open standard for GPU programming, so we use it as the base termi-\nnology. OpenCL runs on all of the GPU hardware and many other devices such as\nCPUs and even more exotic hardware such as field-programmable gate arrays\n(FPGAs) and other embedded devices. CUDA, the NVIDIA proprietary language for\ntheir GPUs, is the most widely used language for GPU computation and, thus, used in\na great fraction of the documentation on programming GPUs. HIP (Heterogeneous-\nComputing Interface for Portability) is a portable derivative of CUDA developed by\nAMD for their GPUs. It uses similar terminology as CUDA. The native AMD Heteroge-\nneous Compute (HC) Compiler and the C++ AMP language from Microsoft use a lot\nof the same terms. (C++ AMP is in maintenance mode and not under active develop-\nment as of this writing.) When trying to get portable performance, it’s also important\nto consider the corresponding features and terms for the CPU as shown in the last col-\numn in table 10.1.Table 10.1 Programming abstractions and associated terminology for GPUs\nOpenCL CUDA HIPAMD GPU \n(HC compiler) C++ AMP CPU\nNDRange \n(N-dimensional \nrange)grid grid extent extent Standard loop bounds \nor index sets with loop \nblocking\nWork Group block or \nthread blockblock tile tile loop block\nSubgroup or \nWavefrontwarp warp wavefront N/A SIMD length\nWork Item thread thread thread thread thread\n350 CHAPTER  10 GPU programming model\n10.1.4 Data decomposition into independent units of work: \nAn NDRange or grid\nThe technique of data decomposition is at the heart of how GPUs obtain perfor-\nmance. GPUs break up the problem into many smaller blocks of data. Then they\nbreak it up again, and again.\n GPUs must draw a lot of triangles and polygons to generate high frame rates.\nThese operations are completely independent from each other. For this reason, the\ntop-level data decomposition for computational work on a GPU also generates inde-\npendent and asynchronous work. \n With lots of work to do, GPUs hide latency (stalls for memory loads) by switching\nto another work group that is ready to compute. Figure 10.2 shows a case where only\nfour subgroups (warps or wavefronts) can be scheduled due to resource limitations.\nWhen the subgroups hit a memory read and stall, execution switches to other sub-\ngroups. The execution switch, also called a context switch, is hiding latency with com-\nputation rather than with a deep cache hierarchy. If you only have a single instruction\nstream on a single piece of data, a GPU will be slow because it has no way to hide the latency. But\nif you have lots of data to operate on, it’s incredibly fast.\nTable 10.2 shows the device limitations for the current NVIDIA and AMD schedulers.\nFor these devices, we want a high number of candidate work groups and subgroups to\nkeep the processing elements busy. \n Data movement and, in particular, moving data up and down the cache hierarchy,\nis a substantial part of the energy cost for a processor. Therefore, the reduction in theWork group 0 Work group 1 Work group 2 Work group 3Compute unit\nSubgroup 0 Subgroup 1 Subgroup 2 Subgroup 3 Subgroup 4 Subgroup 5 Subgroup 6 Subgroup 7\nSubgroup 0 Subgroup 1 Subgroup 2 Subgroup 3 Subgroup 4 Subgroup 5 Subgroup 6 Subgroup 7\nSubgroup 0 Subgroup 1 Subgroup 2 Subgroup 3 Subgroup 4 Subgroup 5 Subgroup 6 Subgroup 7\nSubgroup 0 Subgroup 1 Subgroup 2 Subgroup 3 Subgroup 4 Subgroup 5 Subgroup 6 Subgroup 7Cycle 0\nCycle 1\nCycle 2\nCycle 3Stalled for memory read\nStalled for memory read\nSelectedSync work group\nInactive or stalledLegend\nFigure 10.2 The GPU subgroup (warp) scheduler switches to other subgroups to cover memory reads and \ninstruction stalls. Multiple work groups allow work to be done even when a work group is being synchronized. \n351 GPU programming abstractions: A common framework\nneed for a deep cache hierarchy has some significant benefits. There is a large reduc-\ntion in the energy usage. Also, a lot of precious silicon space is freed on the processor.\nThis space can then be filled with more arithmetic logic units (ALUs). \n We show the data decomposition operation in figure 10.3, where a 2D computa-\ntional domain is split into smaller 2D blocks of data. In OpenCL, this is called an\nNDRange , short for N-dimensional range (the CUDA term, a grid, is a little more palat-\nable). The NDRange in this case is a 3×3 set of tiles of size 8×8. The data decomposi-\ntion process breaks up the global computational domain, Gy by Gx, into smaller blocks\nor tiles of size Ty by Tx.\nLet’s work through an example to see what this step accomplishes. Table 10.2 GPU subgroup (warp or wavefront) scheduler limitations\nNVIDIA Volta \nand Ampere AMD MI50\nActive number of subgroups per compute unit 64 40\nActive number of work groups per compute unit 32 40\nSelected subgroups for execution per compute unit 4 4\nSubgroup (warp or wavefront) size 32 64\nExample: Data decomposition of a 1024×1024 2D computational domain\nIf we set a tile size of 16x8, the data decomposition will be as follows:\nNTx = Gx/Tx = 1024/16 = 64\nNTy = Gy/Ty = 1024/8 = 128\nNT = 64 × 128 = 8,192 tiles\nThis first level of the data decomposition begins to break up the dataset into a\nlarge number of smaller blocks or tiles. The GPUs make some assumptions of theNDRange or grid\nWork group\nGY\nGXTY\nTXFigure 10.3 Breaking up \nthe computational domain into \nsmall, independent work units\n352 CHAPTER  10 GPU programming model\nTable 10.3 shows examples of how this data decomposition might occur for 1D-, 2D-,\nand 3D-computational domains. The fastest changing tile dimension, Tx, should be a\nmultiple of the cache line length, memory bus width, or subgroup (wavefront or\nwarp) size for best performance. The number of tiles, NT, overall and in each dimen-\nsion, results in a lot of work groups (tiles) to distribute across the GPU compute engines\nand processing elements. \nFor algorithms that need neighbor information, the optimum tile size for memory\naccesses needs to be balanced against getting the minimum surface area for the tile\n(figure 10.4). Neighbor data must be loaded more than once for adjacent tiles, which\nmakes this an important consideration.(continued)\ncharacteristics of the work groups created by this data decomposition. These assump-\ntions are that the work groups\nAre completely independent and asynchronous\nHave access to global and constant memory\nEach work group that is created is an independent unit of work. This means that each\nof these work groups can be operated on in any order, providing a level of parallelism\nto spread out across the many compute units on a GPU. These are the same proper-\nties that we attributed to fine-grained parallelism in section 7.3.6. The same changes\nto loops for independent iterations for OpenMP and vectorization also enable GPU\nparallelism.\nTable 10.3 Data decomposition of the computational domain into tiles or blocks\n1D Small 2D Large 2D 3D\nGlobal size 1,048,576 1024 × 1024 1024 × 1024 128 × 128 × 128\nTz × Ty × Tx 128 8 × 8 8 × 16 4 × 4 × 8\nTile size 128 64 128 128\nNTz × NTy × NTx 8,192 128 × 128 128 × 64 32 × 32 × 16\nNT (number of work groups) 8,192 16,384 8,192 16,384\nWork group 1Work group 2\nWG 1WG 2\n32\n16Figure 10.4 Each work group needs to load neighbor data from \nthe dashed rectangle, resulting in duplicate loads in the shaded \nregions where more duplicate loads will be needed for the case \non the left. This must be balanced against optimum contiguous \ndata loads in the x-direction.",8265
136-10.1.5 Work groups provide a right-sized chunk of work.pdf,136-10.1.5 Work groups provide a right-sized chunk of work,,0
137-10.1.7 Work item The basic unit of operation.pdf,137-10.1.7 Work item The basic unit of operation,"353 GPU programming abstractions: A common framework\n10.1.5 Work groups provide a right-sized chunk of work\nThe work group spreads out the work across the threads on a compute unit. Each\nGPU model has a maximum size specified for the hardware. OpenCL reports this as\nCL_DEVICE_MAX_WORK_GROUP_SIZE  in its device query. PGI reports it as Maximum\nThreads  per Block  in the output from its pgaccelinfo  command (see figure 11.3).\nThe maximum size for a work group is usually between 256 and 1,024. This is just the\nmaximum. For computation, work group sizes are typically much smaller, so that\nthere are more memory resources per work item or thread.\n The work group is subdivided into subgroups or warps (figure 10.5). A subgroup is\nthe set of threads that execute in lockstep. For NVIDIA, the warp size is 32 threads.\nFor AMD it is called a wavefront, and the size is usually 64 work items. The work group\nsize must be a multiple of the subgroup size.\nThe typical characteristics of work groups on GPUs are that they\nCycle through processing each subgroup\nHave local memory (shared memory) and other resources shared within the\ngroup\nCan synchronize within a work group or a subgroup\nLocal memory provides fast access and can be used as a sort of programmable cache\nor scratchpad memory. If the same data is needed by more than one thread in a work\ngroup, performance can generally be improved by loading it into the local memory at\nthe start of the kernel.\n10.1.6 Subgroups, warps, or wavefronts execute in lockstep\nTo further optimize the graphics operations, GPUs recognize that the same opera-\ntions can be performed on many data elements. GPUs are therefore optimized by\nworking on sets of data with a single instruction rather than with separate instructions\nfor each. This reduces the number of instructions that need to be handled. This tech-\nnique on the CPU is called single instruction, multiple data  (SIMD). All GPUs emulateWork group\nWork group\nSubgroup32 32 32 32Each subgroup (warp) is 32 work items (threads).\nThis work group size is 28, but can be up to ,024. 11\nWork item\nWork item\nFigure 10.5 A multi-dimensional work group is linearized onto a 1D strip \nwhere it is broken up into subgroups of 32 or 64 work items. For performance \nreasons, work groups should be multiples of the subgroup size.",2353
138-10.2.4 How to address memory resources in your GPU programming model.pdf,138-10.2.4 How to address memory resources in your GPU programming model,"354 CHAPTER  10 GPU programming model\nthis with a group of threads where it is called single instruction, multi-thread  (SIMT). See\nsection 1.4 for the original discussion of SIMD and SIMT.\n Because SIMT simulates SIMD operations, it is not necessarily constrained the\nsame way as are SIMD operations by the underlying vector hardware. Current SIMT\noperations are executed in lockstep, with every thread in the subgroup executing all\npaths through branching if any one thread must go through a branch (figure 10.6).\nThis is similar to how a SIMD operation is done with a mask. But because the SIMT\noperation is emulated, this could be relaxed with more flexibility in the instruction\npipeline, where more than one instruction could be supported.\nSmall sections of conditionals for GPUs do not have a significant impact on overall\nperformance. But if some threads take thousands of cycles longer than others, there’s\na serious issue. If threads are grouped such that all the long branches are in the same\nsubgroup (wavefront), there will be little or no thread divergence.\n10.1.7 Work item: The basic unit of operation\nThe basic unit of operation is called a work item in OpenCL. This work item can be\nmapped to a thread or to a processing core, depending on the hardware implementa-\ntion. In CUDA, it is simply called a thread because that is how it is mapped in NVIDIA\nGPUs. Calling it a thread is mixing the programming model with how it is imple-\nmented in the hardware, but it is a little clearer to the programmer.\n A work item can invoke another level of parallelism on GPUs with vector hardware\nunits as figure 10.7 shows. This model of operation also maps to the CPU where a\nthread can execute a vector operation.\n10.1.8 SIMD or vector hardware\nSome GPUs also have vector hardware units and can do SIMD (vector) operations in\naddition to SIMT operations. In the graphics world, the vector units process spatial\nor color models. The use in scientific computation is more complicated and notif (i > 0) {\nx = 0.0;\n} else {\nx = 1.0;\n}CPU\nThreads\nThread 0 Thread 1CPU\nSIMD\nLanes 0–3 Lanes 4–7GPU\nSIMT\nThreads 0–15 Threads 16–31\nFigure 10.6 The shaded rectangles show the executed statements by threads and lanes. \nSIMD and SIMT operations execute all the statements in lockstep with masks for those that \nare false. Large blocks of conditionals can cause branch divergence problems for GPUs.\n355 The code structure for the GPU programming model\nnecessarily portable between GPU hardware. The vector operation is done per work\nitem, increasing the resource utilization for the kernel. But often there are additional\nvector registers to compensate for the additional work. Effective utilization of the vec-\ntor units can provide a significant boost to performance when done well.\n Vector operations are exposed in the OpenCL language and AMD languages.\nBecause the CUDA hardware does not have vector units, the same level of support is\nnot present in CUDA languages. Still, OpenCL code with vector operations will run\non CUDA hardware, so it can be emulated in the CUDA hardware.\n10.2 The code structure for the GPU programming model\nNow we can begin to look at the code structure for the GPU that incorporates the pro-\ngramming model. For convenience and generality, we call the CPU the host and we\nuse the term device  to refer to the GPU.\n The GPU programming model splits the loop body from the array range or index\nset that is applied to the function. The loop body creates the GPU kernel. The index\nset and arguments will be used on the host to make the kernel call. Figure 10.8 shows\nthe transformation from a standard loop to the body of the GPU kernel. This example\nuses OpenCL syntax. But the CUDA kernel is similar, replacing the get_global_id\ncall with \ngid = blockIdx.x *blockDim.x + threadIdx.x\nIn the next four sections, we look separately at how the loop body becomes the paral-\nlel kernel and how to tie it back to the index set on the host. Let’s break this down\ninto four steps:\n1Extract the parallel kernel\n2Map from the local data tile to global dataWork group\nSubgroup32 32 32 32\nSIMD/VectorWork itemFigure 10.7 Each work item on an AMD or \nIntel GPU may be able to do a SIMD or Vector \noperation. This maps well over to the vector \nunit on a CPU as well.\nCPU loop GPU kernel\nPrevents access\nout-of-boundsIndex set // stream triad loop\nfo (int i=0; i<STREAM_ARRAY_SIZE; i++){ r\nc[i] = a[i] + scalar*b[i];\n}si e_t zgi = dg t_global_id e (0);\nif (gid >= STREAM_ARRAY_SIZE) return;\nc[ id] = a[gid] + scalar*b[gid]; g Loop body\nFigure 10.8 Correspondence between standard loop and the GPU kernel code structure\n356 CHAPTER  10 GPU programming model\n3Calculate data decomposition on the host into blocks of data\n4Allocate any required memory\n10.2.1 “Me” programming: The concept of a parallel kernel\nGPU programming is the perfect language for the “Me” generation. In the kernel,\neverything is relative to yourself. Take for example\nc[i] = a[i] + scalar*b[i];\nIn this expression, there is no information about the extent of the loop. This could be\na loop where i, the global i index, covers a range from 0 to 1,000 or just the single\nvalue 22. Each data item knows what needs to be done to itself and itself only. This is\ntruly a “Me” programming model, where I care only about myself. What is so powerful\nabout this is that the operations on each data element become completely indepen-\ndent. Let’s look at the more complicated example of the stencil operator. Although\nwe have two indices, both i and j, and some of the references are to adjacent data val-\nues, this line of code is still fully defined once we determine the values of i and j.\nxnew[j][i] = (x[j][i] + x[j][i–1] + x[j][i+1] + x[j–1][i] + x[j+1][i])/5.0;\nThe separation of the loop body and the index set can be done in C++ with either\nfunctors or lambda expressions. In C++, lambda expressions have been around since\nthe C++ 11 standard. Lambdas are used as a way for compilers to provide portability\nfor single-source code to either CPUs or GPUs. Listing 10.1 shows the C++ lambda.\nDEFINITION Lambda expressions  are unnamed, local functions that can be\nassigned to a variable and used locally or passed to a routine.\nlambda.cc\n 1 int main() {\n 2     const int N = 100;\n 3     double a[N], b[N], c[N];\n 4     double scalar = 0.5;\n 5 \n 6     // c, a, and b are all valid scope pointers on the device or host\n 7 \n 8     // We assign the loop body to the example_lambda variable\n 9     auto example_lambda = [&] (int i) {      \n10         c[i] = a[i] + scalar * b[i];      \n11     };\n12 \n13     for (int i = 0; i < N; i++)   \n14     {\n15         example_lambda(i);    \n16     }\n17 }Listing 10.1 C++ lambda for the stream triad\nLambda \nvariable\nLambda \nbodyArguments or \nindex set for \nlambda\nInvokes \nlambda\n357 The code structure for the GPU programming model\nThe lambda expression is composed of four main components:\nLambda body —The function to be executed by the call. In this case, the body is \nc[i] = a[i] + scalar * b[i];.\nArguments —The argument (int  i) used in the later call to the lambda expression.\nCapture closure —The list of variables in the function body that are defined exter-\nnally and how these are passed to the routine, specified by [&] in listing 10.1. The\n& indicates that the variable is referred to by reference and an = sign says to copy it\nby value. A single & sets the default to variables by reference. We can more fully\nspecify the variables with the capture specification of [&c,  &a, &b, &scalar] .\nInvocation —The for loop in lines 13 to 16 in listing 10.1 invokes the lambda\nover the specified array values.\nLambda expressions form the basis for more naturally generating code for GPUs in\nemerging C++ languages like SYCL, Kokkos, and Raja. We will briefly cover SYCL in chap-\nter 12 as a higher-level C++ language (originally built on top of OpenCL). Kokkos\nfrom Sandia National Laboratories (SNL) and Raja, originating at Lawrence Liver-\nmore National Laboratory (LLNL), are two higher-level languages developed to sim-\nplify the writing of portable scientific applications for the broad array of today’s\ncomputing hardware. We’ll introduce Kokkos and Raja in chapter 12 as well.\n10.2.2 Thread indices: Mapping the local tile to the global world\nThe key to how the kernel can compose its local operation is that, as a product of the\ndata decomposition, we provide each work group with some information about where it\nis in the local and global domains. In OpenCL, you can get the following information:\nDimension —Gets the number of dimensions, either 1D, 2D, or 3D, for this ker-\nnel from the kernel invocation\nGlobal information —Global index in each dimension, which corresponds to a\nlocal work unit, or the global size in each dimension, which is the size of the\nglobal computational domain in each dimension\nLocal (tile) information —The local size in each dimension, which corresponds to\nthe tile size in this dimension, or the local index in each dimension, which cor-\nresponds to the tile index in this dimension\nGroup information —The number of groups in each dimension, which corre-\nsponds to the number of groups in this dimension, or the group index in each\ndimension, which corresponds to the group index in this dimension\nSimilar information is available in CUDA, but the global index must be calculated\nfrom the local thread index plus the block (tile) information:\ngid = blockIdx.x *blockDim.x + threadIdx.x; \nFigure 10.9 presents the indexing for the work group (block or tile) for OpenCL and\nCUDA. The function call for OpenCL is first, followed by the variable defined by\n358 CHAPTER  10 GPU programming model\nCUDA. All of this indexing support is automatically done for you by the data decom-\nposition for the GPU, greatly simplifying the handling of the mapping from the global\nspace to the tile.\n10.2.3 Index sets \nThe size of the indices for each work group should be identical. This is done by pad-\nding the global computational domain out to a multiple of the local work group size.\nWe can do this with some integer arithmetic to get one extra work group and a pad-\nded global work size. The following example shows an approach using basic integer\noperations and then a second with the C ceil  intrinsic function.\nglobal_work_size x = ((global_size x + local_work_size x – 1)/\n                      local_work_size x) * local_work_size x\nExample: Calculating work group sizes\nThis code uses the arguments for the kernel invocation to get uniform work group\nsizes. \nint global_size x = 1000;\nint local_work_size x = 128;\nint global_work_size x = ((global_size x + local_work_size x - 1)/\n                          local_work_size x) * local_work_size x = 1024;NDRange or grid\nWork group\nget_group_id(0) blockIdx.x or01 2\nget_num_groups(0) gridDim.x = 3 orget_local_size(0)\norblockDim.x = 8 get_group_id(1) blockIdx.yor\n012\nget_local_id(0) threadIdx.x or01 34567 2\n16 17 19 20 21 22 23 18\nget_global_id(0) or\nblockIdx.x*blockDim.x + threadIdx.xGY TY\nGXTX\nFigure 10.9 Mapping of the index of individual work item to global index space. The OpenCL call is given first, \nfollowed by the variable defined in CUDA.\n359 The code structure for the GPU programming model\nNOTE Avoiding out-of-bound reads and writes is important in GPU kernels\nbecause they lead to random kernel crashes with no error message or output.\n10.2.4 How to address memory resources in your GPU \nprogramming model\nMemory is still the most important concern impacting your application program-\nming plan. Fortunately, there is a lot of memory on today’s GPUs. Both the NVIDIA\nV100 and AMD Radeon Instinct MI50 GPUs support 32 GB of RAM. Compared to\nwell-provisioned HPC CPU nodes with 128 GB of memory, a GPU compute node with\n4–6 GPUs has the same memory. There is as much memory on GPU compute nodes as\non CPUs. Therefore, we can use the same memory allocation strategy as we have for\nthe CPU and not have to transfer data back and forth due to limited GPU memory. \n Memory allocation for the GPU has to be done on the CPU. Often, memory is allo-\ncated for both the CPU and the GPU at the same time and then data is transferred\nbetween them. But if possible, you should allocate memory only for the GPU. This\navoids expensive memory transfers back and forth from the CPU and frees up mem-\nory on the CPU. Algorithms that use a dynamic memory allocation present a problem\nfor the GPU and need to be converted to a static memory algorithm, with the memory\nsize known ahead of time. The lates t GPUs do a good job of coalescing  irregular or\nshuffled memory accesses into single, coherent cache-line loads when possible.\nDEFINITION Coalesced memory loads  are the combination of separate memory\nloads from groups of threads into a single cache-line load.\nOn the GPU, the memory coalescing is done at the hardware level in the memory con-\ntroller. The performance gains from these coalesced loads are substantial. But also\nimportant is that a lot of the optimizations from earlier GPU programming guides are\nno longer necessary, significantly reducing the GPU programming effort.\n You can get some additional speedup from using local (shared) memory for data\nthat is used more than once. This optimization used to be important for performance,\nbut the better cache on GPUs is making the speedup less significant. There are a cou-\nple of strategies on how to use the local memory, depending on whether you can\npredict the size of the local memory required. Figure 10.10 shows the regular gridint number_work_groups x = (global_size x + local_work_size x - 1)/\n                           local_work_size x = 8\n// or, alternatively\nint global_work_size x = ceil(global_size x/local_work_size x) * \n                             local_work_size x = 1024;\nint number_work_groups x = ceil(global_size x/local_work_size x) = 8;\nTo avoid reading past the end of the array, we should test the global index in each\nkernel and skip the read if it is past the end of the array with something like\nif (gid x > global_size x) return;\n360 CHAPTER  10 GPU programming model\napproach on the left and the irregular grid for unstructured and adaptive mesh refine-\nment on the right. The regular grid has four abutting tiles with overlapping halo\nregions. The adaptive mesh refinement shows only four cells; a typical GPU applica-\ntion would load 128 or 256 cells and then bring in the required neighbor cells around\nthe periphery.\nThe processes for the two cases are \nThreads need the same memory loads as adjacent threads.  A good example of this is\nthe stencil operation we use throughout the book. Thread i needs the i–1 and\ni+1 values, which means that multiple threads will need the same values. The\nbest approach for this situation is to do cooperative memory loads. Copying the\nmemory values from global memory to local (shared) memory results in a sig-\nnificant speedup.\nAn irregular mesh has an unpredictable number of neighbors, making it difficult to load\ninto local memory.  One way to handle this is to copy the part of the mesh to be\ncomputed into local memory. Then load the neighbor data into registers for\neach thread. \nThese are not the only ways to utilize the memory resources on the GPU. It is import-\nant to think through the issues with regard to the limited resources and the potential\nperformance benefits for your particular application.1684\n12Step : Threads work 1\ntogether to load values\nfrom global memory to\nthe 8x 6 tile, including 1\nthe neighbor region\nStep 2: Synchronize\nthreads\nStep 3: Do the stencil\ncalculation for the 4x 2 1\ninner region using local\nmemory and store to\nglobal memoryStep : Threads work 1\ntogether to load values\nfrom global memory to\nthe shaded region in local\nmemory\nStep 3: Do the stencil\ncalculation using local\nmemory where possible\nand loading into registers\nfor restStep 2: Synchronize\nthreadsCooperative memory loads Irregular memory loads\nFigure 10.10 For stencils on regular grids, load all the data into local memory and then use local \nmemory for the computation. The inner solid rectangle is the computational tile. The outer dashed \nrectangle encloses the neighboring data needed for the calculation. You can use cooperative loads to \nload the data in the outer rectangle into the local memory for each work group. Because irregular grids \nhave an unpredictable size, load only the computed region into local memory and use registers for \neach thread for the rest.",16715
139-10.3 Optimizing GPU resource usage.pdf,139-10.3 Optimizing GPU resource usage,,0
140-10.3.2 Occupancy Making more work available for work group scheduling.pdf,140-10.3.2 Occupancy Making more work available for work group scheduling,"361 Optimizing GPU resource usage\n10.3 Optimizing GPU resource usage\nThe key to good GPU programming is to manage the limited resources available for\nexecuting kernels. Let’s look at a few of the more important resource limitations in\ntable 10.4. Exceeding the available resources can lead to significant decreases in perfor-\nmance. The NVIDIA compute capability 7.0 is for the V100 chip. The newer Ampere\nA100 chip uses a compute capability of 8.0 with nearly identical resource limits.\nThe most important control available to the GPU programmer is the work group size.\nAt first, it would seem that using the maximum number of threads per work group\nwould be desirable. But for computational kernels, the complexity of computational\nkernels in comparison to graphics kernels means that there are a lot of demands on\ncompute resources. This is known colloquially as memory pressure  or register pressure .\nReducing the work group size gives each work group more resources to work with. It\nalso gives more work groups for context switching, which we discussed in section\n10.1.1. The key to getting good GPU performance is finding the right balance of work\ngroup size and resources.\nDEFINITION Memory pressure  is the effect of the computational kernel resource\nneeds on the performance of GPU kernels. Register pressure  is a similar term,\nreferring to demands on registers in the kernel.\nA full analysis of the resource requirements of a particular kernel and the resources\navailable on the GPU requires an involved analysis. We’ll give examples of a couple of\nthese types of deep dives. In the next two sections, we look at \nHow many registers a kernel uses\nHow busy the multi-processors are kept, which is called occupancy\n10.3.1 How many registers does my kernel use?\nYou can find out how many registers your code uses by adding the -Xptxas=""-v""  flag\nto the nvcc  compile command. In OpenCL for NVIDIA GPUs, use the -cl-nv-verbose\nflag for the OpenCL compile line to get a similar output.Table 10.4 Some resource limitations on current GPUs\nResource limitNVIDIA \ncompute capability 7.0 AMD Vega 20 (MI50)\nMaximum threads per work group 1024 256\nMaximum threads per compute unit 2048\nMaximum work groups per compute unit 32 16\nLocal memory per compute unit 96 KB 64 KB\nRegister file size per compute unit 64K 256 KB vector\nMaximum 32-bit registers per thread 255\n362 CHAPTER  10 GPU programming model\n10.3.2 Occupancy: Making more work available for \nwork group scheduling \nWe have discussed the importance of latency and context switching for good perfor-\nmance on the GPU. The benefit in “right-sized” work groups is that more work groups\ncan be in flight at one time. For the GPU, this is important because when progress on\na work group stalls due to memory latency, it needs to have other work groups that it\ncan execute to hide the latency. To set the proper work group size, we need a measure\nof some sort. On GPUs, the measure used for analyzing work groups is called occu-\npancy . Occupancy is a measure of how busy the compute units are during the calcula-\ntion. The measure is complicated because it is dependent on a lot of factors, such as\nthe memory required and the registers used. The precise definition is\nOccupancy = \nNumber of Active Threads/Maximum Number of Threads Per Compute Unit\nBecause the number of threads per subgroup is fixed, an equivalent definition is\nbased on subgroups, also known as wavefronts or warps:\nOccupancy = \nNumber of Active Subgroups/Maximum Number of Subgroups Per Compute Unit\nThe number of active subgroups or threads is determined by the work group or\nthread resource that is exhausted first. Often this is the number of registers or local\nmemory that is needed by a work group, preventing another work group from start-\ning. We need a tool such as the CUDA Occupancy Calculator (presented in the follow-\ning example) to do this well. NVIDIA programming guides focus a lot of attention on\nmaximizing occupancy. While important, there just need to be enough work groups\nto switch between to hide latency and stalls.Example: Getting the register usage for your kernel on an NVIDIA GPU\nFirst, we build BabelStream with the extra compiler flags:\ngit clone git@github.com:UoB-HPC/BabelStream.git\ncd BabelStream\nexport EXTRA_FLAGS='-Xptxas=""-v""'\nmake -f CUDA.make\nThe output from the NVIDIA compiler shows the register usage for the stream triad:\nptxas info    : Used 14 registers, 4096 bytes smem, 380 bytes cmem[0]\nptxas info    : Compiling entry function '_Z12triad_kernelIfEvPT_PKS0_S3_'\n                for 'sm_70'\nptxas info    : Function properties for _Z12triad_kernelIfEvPT_PKS0_S3_\n    0 bytes stack frame, 0 bytes spill stores, 0 bytes spill loads\nIn this simple kernel, we use 14 registers out of the 255 available on the NVIDIA GPU.\n363 Optimizing GPU resource usage\nExample: CUDA Occupancy Calculator\n1Download CUDA Occupancy Calculator spreadsheet from\nhttps:/ /docs.nvidia.com/cuda/cuda-occupancy-calculator/index.html\n2Enter the register count output from NVCC compiler (section 10.3.1) and work\ngroup size (1,024).\nThe following figure shows the results for the Occupancy Calculator. There are also\nplots on the spreadsheet for varying block size, register count, and local memory\nusage (not shown).\nOutput from CUDA Occupancy Calculator for the stream triad showing the resource usage for the \nkernel. The third block from the top shows the occupancy measures.1.) Select compute capability (click):\n1.b) Select shared memory size conﬁg (bytes)\n(Don’t edit anything below this line)2.) Enter your resource usage:\nThreads per block\nRegisters per thread\nShared memory per block (bytes)\nThreads per warp\nMax warps per multiprocessor\nMax thread blocks per multiprocessor\nMax threads per multiprocessor\nMaximum thread block size\nRegisters per multiprocessor\nMax registers per thread block\nMax registers per thread\nShared memory per multiprocessor (bytes)\nMax shared memory per block\nRegister allocation unit size\nRegister allocation granularity\nShared memory allocation unit size\nWarp allocation granularity3.) GPU occupancy data is displayed here and in the graphs:\nActive threads per multiprocessor\nActive warps per multiprocessor\nActive thread blocks per multiprocessor\nOccupancy of each multiprocessor2048\n64\n2\n100%(Help)\n(Help)\n(Help)7.0\n32768CUDA occupancy calculator\n1024\n14\n4096\nPhysical limits for GPU compute capability:\nWarps                (Threads per block/Threads per warp)\nRegisters           (Warp limit per SM due to per-warp reg count)\nShared memory (bytes)\nNote: SM is an abbreviation for (streaming) multiprocessor\nNote: Occupancy limiter is shown in orange32\n32\n4096Allocated Resources Per block\n64\n128\n327682\n4\n8\nPhysical max warps/SM = 64\nOccupancy = 64/64 = 100%Limit per SM= Allocatable\nBlocks per SM7.0\n32\n64\n32\n2048\n1024\n65536\n65536\n255\n32768\n32768\n256\nwarp\n256\n4\nLimited by max warps or max blocks per multiprocessor\nLimited by registers per multiprocessor\nLimited by shared memory per multiprocessor2\n4\n6Maximum thread blocks per multiprocessor Blocks/SM\n32 64* Warps/Block = Warps/SMJust follow steps 1, 2, and 3 below! (or click here for help)",7287
141-10.6.2 Case 2 Unstructured mesh application.pdf,141-10.6.2 Case 2 Unstructured mesh application,"364 CHAPTER  10 GPU programming model\n10.4 Reduction pattern requires synchronization across \nwork groups\nUp to now, the computational loops we have looked at over cells, particles, points, and\nother computational elements could be handled by the approach in figure 10.8,\nwhere the for loops are stripped from the computational body to create a GPU ker-\nnel. Making this transformation is quick and easy and can be applied to the vast major-\nity of loops in a scientific application. But there are other situations where the code\nconversion to the GPU is exceedingly difficult. We’ll look at algorithms that require a\nmore sophisticated approach. Take for example, the single line of Fortran code using\narray syntax:\nxmax = sum(x(:))\nIt looks so simple in Fortran, but it’s far more complicated on the GPU. The source of\nthe difficulty is that we cannot do cooperative work or comparisons across work\ngroups. The only way to accomplish this is to exit the kernel. Figure 10.11 illustrates\nthe general strategy that deals with this situation.\nFor ease of illustration, figure 10.11 shows an array 32 elements long. The typical array\nfor this method would be hundreds of thousands or even millions of elements long so\nthat it is much larger than the size of a work group. In the first step, we find the sum of\neach work group and store it in a scratch array the length of the number of work\ngroups or blocks. The first pass reduces the size of the array by the size of our work\ngroup, which could be 512 or 1,024. At this point, we cannot communicate between\nwork groups, so we exit the kernel and start a new kernel with just one work group.\nThe remaining data might be greater than the work group size of 512 or 1,024, so we\nloop through the scratch array, summing up the values into each work item. We can\ncommunicate between the work items in the work group, so we can do a reduction to\na single global value, summing along the way.1. Calculate the sum for every\nwork group and store into a\nscratch array (xblock) the size\nof global_size/work_group_size.\n2a. Use a single work group and loop\nthrough array to ﬁnd the sum per\nwork item.\n2b. Do a reduction within the work\ngroup to get the global sum.Work group\nsize is 4 for\nillustrative\npurposes.xblock(0:# workgroups-1)=sumwork group\nFigure 10.11 The reduction pattern on the GPU requires two kernels to synchronize multiple work \ngroups. We exit the first kernel, represented by the rectangle, and then start another one the size \nof a single work group to allow thread cooperation for the final pass.\n365 Asynchronous computing through queues (streams)\n Complicated! The code to perform this operation on the GPU takes dozens of\nlines of code and two kernels to do the same operation that we can do in one line for\nthe CPU. We’ll see more of the actual code for a reduction in chapter 12 when we\ncover CUDA and OpenCL programming. The performance that is obtained on the\nGPU is faster than the CPU, but it takes a lot of programming work. And we’ll start to\nsee that one of the characteristics of GPUs is that synchronization and comparisons\nare hard to do.\n10.5 Asynchronous computing through queues (streams)\nWe are going to see how we can more fully utilize a GPU by overlapping data transfer\nand computation. Two data transfers can occur at the same time as a computation on\na GPU.\n The basic nature of work on GPUs is asynchronous. Work is queued up on the\nGPU and, usually, only gets executed when a result or synchronization is requested.\nFigure 10.12 shows a typical set of commands sent to a GPU for a computation.\nWe can also schedule work in multiple queues that are independent and asynchro-\nnous. The use of multiple queues as illustrated in figure 10.13 exposes the potential\nfor overlapping data transfer and computation. Most of the GPU languages support\nsome form of asynchronous work queues. In OpenCL the commands are queued, and\nin CUDA, the operations are placed in streams. While the potential for parallelism is\ncreated, whether it actually happens is dependent on the hardware capabilities and\ncoding details.Copy image from host to device\nInvoke kernel to compute\nmodiﬁed image\nCopy new image from device\nto host and wait for completionCommand 0:\nCommand 1:\nCommand 2:Figure 10.12 Work scheduled on a GPU in the default \nqueue only gets completed when the wait for completion \nis requested. We scheduled the copy of a graphical image \n(a picture) to be copied to the GPU. Then we scheduled a \nmathematical operation on the data to modify it. We also \nscheduled a third operation to bring it back. None of these \noperations has to start until we demand the wait for \ncompletion.\nQueue 3\nCopy image from host to device\nInvoke kernel to compute\nmodiﬁed image\nCopy new image from device\nto host and wait for completionQueue 2\nCopy image from host to device\nInvoke kernel to compute\nmodiﬁed image\nCopy new image from device\nto host and wait for completionQueue 1\nCopy image from host to device\nInvoke kernel to compute\nmodiﬁed image\nCopy new image from device\nto host and wait for completion\nFigure 10.13 Staging work for three images in parallel queues\n366 CHAPTER  10 GPU programming model\nIf we have a GPU capable of simultaneously performing these operations, \nCopying data from host to device\nKernel computation(s)\nCopying data from device to host\nthen the work that is set up in three separate queues in figure 10.13 can overlap com-\nputation and communication as figure 10.14 shows.\n10.6 Developing a plan to parallelize an application \nfor GPUs\nNow we’ll move on to using our understanding of the GPU programming model to\ndevelop a strategy for parallelization of our application. We’ll use a couple of applica-\ntion examples to demonstrate the process.\n Queue 1 Queue 2 Queue 3\nTime (ms)Copy image from host to device\n10 ms\nInvoke kernel to compute\nmodiﬁed image\n5 ms\nCopy new image from device\nto host and wait for completion\n10 ms10\n20\n30\n40Copy image from host to device\n10 ms\nInvoke kernel to compute\nmodiﬁed image\n5 ms\nCopy new image from device\nto host and wait for completion\n10 msCopy image from host to device\n10 ms\nInvoke kernel to compute\nmodiﬁed image\n5 ms\nCopy new image from device\nto host and wait for completion\n10 ms\nFigure 10.14 Overlapping computation and data transfers reduce the time for three images from 75 ms to \n45 ms. This is possible because the GPU can do a computation, a data transfer from the host to the device, \nand another one from the device to the host simultaneously.\n367 Developing a plan to parallelize an application for GPUs\n10.6.1 Case 1: 3D atmospheric simulation \nYour application is an atmospheric simulation ranging from 1024x1024x1024 to\n8192x8192x8192 in size with x as the vertical dimension, y as the horizontal, and z as\nthe depth. Let’s look at the options you might consider:\nOption 1: Distribute data in a 1D fashion across the z-dimension (depth).\nFor GPUs, we need tens of thousands of work groups for effective parallelism.\nFrom the GPU specification (table 9.3), we have 60–80 compute units of 32\ndouble-precision arithmetic units for about 2,000 simultaneous arithmetic path-\nways. In addition, we need more work groups for latency hiding via context\nswitching. Distributing data across the z-dimension gets us 1,024 to 8,192 work\ngroups, which is low for a GPU parallelism.\nLet’s look at the resources needed for each work group. The minimum\ndimensions would be a 1024x1024 plane, plus any required neighbor data in\nghost cells. We’ll assume one ghost cell in both directions. We would therefore\nneed 1024 × 1024 × 3 × 8 bytes or 24 MiB of local data. Looking at table 10.4,\nGPUs have 64–96 KiB of local data, so we would not be able to preload data into\nlocal memory for faster processing.\nOption 2: Distribute data in 2D vertical columns across y- and z-dimensions.\nDistributing across two dimensions would give us over a million potential work\ngroups, so we would have enough independent work groups for the GPU. For\neach work group, we would have 1,024 to 8,192 cells. We have our own cell plus\n4 neighbors for 1024 × 5 × 8 = 40 KiB minimum of required local memory. For\nlarger problems and with more than one variable per cell, we would not have\nenough local memory.\nOption 3: Distribute data in 3D cubes across x-, y-, and z-dimensions.\nUsing the template from table 10.3, for each work group, let’s try using a 4x4x8\ncell tile. With neighbors, this is 6 × 6 × 10 × 8 bytes for 2.8 KiB minimum of\nrequired local memory. We could have more variables per cell and can experi-\nment with making the tile size a little larger.\nTotal memory requirements for the 1024x1024x1024 cell tile × 8 bytes is 8\nGiB. This is a large problem. GPUs have as much as 32 GiB of RAM, so the prob-\nlem would possibly fit on one GPU. Larger size problems would require poten-\ntially up to 512 GPUs. So we should plan for distributed memory parallelism\nusing MPI as well.\nLet’s compare this to the CPU where these design decisions would have different out-\ncomes. We might have work to spread across 44 processes, each with fewer resource\nrestrictions. While the 3D approach could work, the 1D and 2D will also be feasible.\nNow let’s contrast that to an unstructured mesh where the data is all contained in 1D\narrays.",9422
142-10.7.2 Exercises.pdf,142-10.7.2 Exercises,"368 CHAPTER  10 GPU programming model\n10.6.2 Case 2: Unstructured mesh application\nIn this case, your application is a 3D unstructured mesh using tetrahedral or polygo-\nnal cells that range from 1 to 10 million cells. But the data is a 1D list of polygons with\ndata such as x, y, and z that contains the spatial location. In this case, there’s only one\noption: 1D data distribution.\n Because the data is unstructured and contained in 1D arrays, the choices are sim-\npler. We distribute the data in 1D with a tile size of 128. This gives us from 8,000 to\n80,000 work groups, providing plenty of work for the GPU to switch between and hide\nlatency. The memory requirements are 128 × 8 byte double-precision value = 1 KB,\nallowing space for multiple data values per cell. \n We will also need space for some integer mapping and neighbor arrays to provide\nthe connectivity between the cells. Neighbor data is loaded into registers for each\nthread so that we don’t have to worry about the impact on local memory and possibly\nblowing past the memory limit. The largest size mesh at 10 million cells requires 80\nMB, plus space for face, neighbor, and mapping arrays. These connectivity arrays can\nincrease the memory usage significantly, but there should be plenty of memory on a\nsingle GPU to run computations on even the largest size meshes.\n For best results, we will need to provide some locality for the unstructured data by\nusing a data-partitioning library or by using a space-filling curve that keeps cells close\nto each other in the array that are close to each other spatially.\n10.7 Further explorations\nWhile the basic contours of the GPU programming model have stabilized, there are\nstill a lot of changes occurring. In particular, the resources available for the kernels\nhave slowly increased as the target uses broaden from 2D to 3D graphics and physics\nsimulations for more realistic games. Markets such as scientific computing and\nmachine learning are also becoming more important. For both these markets, custom\nGPU hardware has been developed: double precision for scientific computing and\ntensor cores for machine learning.\n In our presentation, we’ve mostly discussed discrete GPUs. But there are also inte-\ngrated GPUs as first discussed in section 9.1.1. The Accelerated Processing Unit\n(APU) is an AMD product offering. Both AMD’s APU and Intel’s integrated GPUs\noffer some advantages in reducing the memory transfer costs because these are no\nlonger on the PCI bus. This is offset by the reduction in the silicon area for GPU tran-\nsistors and a lower power envelope. Still, this capability has been underappreciated\nsince it appeared. The primary development focus has been on the big discrete GPUs\nthat are in the top-end HPC systems. But the same GPU programming languages and\ntools work equally as well with integrated GPUs. The critical limitation on developing\nnew, accelerated applications is the widespread knowledge on how to program and\nexploit these devices.\n \n369 Further explorations\n Other mass-market devices such as Android tablets and cell phones have program-\nmable GPUs with the OpenCL language. Some resources for these include\nDownload OpenCL-Z and OpenCL-X benchmark applications from Google\nPlay to see if your device supports OpenCL. Drivers may also be available from\nhardware vendors.\nCompubench ( https:/ /compubench.com ) has performance results for some\nmobile devices that use OpenCL or CUDA.\nIntel has a nice site on programming with OpenCL for Android at https:/ /soft\nware.intel.com/en-us/android/articles/opencl-basic-sample-for-android-os .\nIn recent years, GPU hardware and software have added support for other types of pro-\ngramming models, such as task-based approaches (see figure 1.25) and graph algo-\nrithms. These alternative programming models have long been an interest in parallel\nprogramming, but have struggled with efficiency and scale. There are critical applica-\ntions, such as sparse matrix solvers, that cannot easily be implemented without further\nadvances in these areas. But the fundamental question is whether enough parallelism\ncan be exposed (revealed to the hardware) to utilize the massive parallel architecture\nof the GPUs. Only time will tell.\n10.7.1 Additional reading\nNVIDIA has long supported research into GPU programming. The CUDA C program-\nming and best practices guides (available at https:/ /docs.nvidia.com/cuda ) are worth\nreading. Other resources include\nThe GPU Gems series ( https:/ /developer.nvidia.com/gpugems ) is an older set\nof papers that still contains a lot of relevant materials. \nAMD also has a lot of GPU programming materials at their GPUOpen site at\nhttps:/ /gpuopen.com/compute-product/rocm/  \nand at the ROCm site \nhttps:/ /rocm.github.io/documentation.html  \nAMD provides one of the better tables comparing terminology of different GPU\nprogramming languages available at https:/ /rocm.github.io/languages.html .\nDespite having about 65% of the GPU market (mostly integrated GPUs), Intel® is just\nbeginning to be a serious player in GPU computation. They have announced a new\ndiscrete graphics board and will be the GPU vendor for the Aurora system at Argonne\nNational Laboratory (to be delivered in 2022). The Aurora system is the first exascale\nsystem ever produced and has 6x the performance of the current top system in the\nworld. The GPU is based on the Intel® Iris® Xe architecture, code named “Ponte Vec-\nchio.” With much fanfare, Intel has released its oneAPI programming initiative. The",5587
143-11 Directive-based GPU programming.pdf,143-11 Directive-based GPU programming,"370 CHAPTER  10 GPU programming model\noneAPI toolkit comes with the Intel GPU driver, compilers, and tools. Go to https:/ /\nsoftware.intel.com/oneapi  for more information and downloads.\n10.7.2 Exercises\n1You have an image classification application that will take 5 ms to transfer each\nfile to the GPU, 5 ms to process, and 5 ms to bring back. On the CPU, the pro-\ncessing takes 100 ms per image. There are one million images to process. You\nhave 16 processing cores on the CPU. Would a GPU system do the work faster?\n2The transfer time for the GPU in problem 1 is based on a third generation PCI\nbus. If you can get a Gen4 PCI bus, how does that change the design? A Gen5\nPCI bus? For image classification, you shouldn’t need to bring back a modified\nimage. How does that change the calculation? \n3For your discrete GPU (or NVIDIA GeForce GTX 1060, if none), what size 3D\napplication could you run? Assume 4 double-precision variables per cell and a\nusage limit of half the GPU memory so you have room for temporary arrays.\nHow does this change if you use single precision?\nSummary\nParallelism on the GPU needs to be in the thousands of independent work\nitems because there are thousands of independent arithmetic units. The CPU\nonly needs parallelism in the tens of independent work items to distribute work\nacross the processing cores. Thus, for the GPU, it is important to expose more\nparallelism in our applications to keep the processing units busy.\nDifferent GPU vendors have similar programming models driven by the needs\nof high-frame-rate graphics. Because of this, a general approach can be devel-\noped that is applicable across many different GPUs.\nThe GPU programming model is particularly well suited for data parallelism\nwith large sets of computational data but can be difficult for some tasks with a\nlot of coordination, such as reductions. The result is that many highly parallel\nloops port easily, but there are some that take a lot of effort.\nThe separation of a computational loop into a loop body and the loop con-\ntrol, or index set, is a powerful concept for GPU programming. The loop body\nbecomes the GPU kernel, and the CPU does the memory allocation, and invokes\nthe kernel.\nAsynchronous work queues can overlap communication and computation. This\ncan help to improve the utilization rate of the GPU.\n371Directive-based GPU\nprogramming\nThere has been a scramble to establish standards for directive-based languages for\nprogramming for GPUs. The pre-eminent directive-based language, OpenMP,\nreleased in 1997, was the natural candidate to look to as an easier way to program\nGPUs. At that time, OpenMP was playing catchup and mainly focused on new CPU\ncapabilities. To address GPU accessibility, in 2011, a small group of compiler ven-\ndors, (Cray, PGI and CAPS) along with NVIDIA as the GPU vendor, joined to\nrelease the OpenACC standard, providing a simpler pathway to GPU program-\nming. Similar to what you saw in chapter 7 for OpenMP, OpenACC also uses prag-\nmas. In this case, OpenACC pragmas direct the compiler to generate GPU code. A\ncouple of years later, the OpenMP Architecture Review Board (ARB) added their\nown pragma support for GPUs to the OpenMP standard.This chapter covers\nSelecting the best directive-based language for \nyour GPU\nUsing directives or pragmas to port your code \nto GPUs or other accelerator devices\nOptimizing the performance of your GPU \napplication\n372 CHAPTER  11 Directive-based GPU programming\n We’ll work through some basic examples in OpenACC and OpenMP to give you an\nidea of how they work. We suggest that you try out the examples on your target system\nto see what compilers are available and their current status. \nNOTE As always, we encourage you to follow along with the examples for this\nchapter at https:/ /github.com/EssentialsofParallelComputing/Chapter11 .\nMany programmers find themselves “on the fence” in regard to which directive-based\nlanguage—OpenACC or OpenMP—they should use. Often, the choice is clear once\nyou find out what is available on your system of choice. Keep in mind that the biggest\nhurdle to overcome is simply to start. If you later decide to switch GPU languages, the\npreliminary work will still prove valuable as the core concepts transcend the language.\nWe hope that by seeing how little effort is required to generate GPU code using prag-\nmas and directives, you will be encouraged to try it on some of your code. You may\neven experience a modest speedup with just a little effort.\nThe history of OpenMP and OpenACC\nThe development of OpenMP and OpenACC standards is mostly a friendly competi-\ntion; some members of the OpenACC committee are also on the OpenMP committee.\nImplementations are still emerging, led by efforts at Lawrence Livermore National\nLaboratory, IBM, and GCC. Attempts have been made to merge the two approaches,\nbut they continue to co-exist and will probably do so for the foreseeable future. \nOpenMP is gaining steam and is believed to be the stronger long-term path, but for\nnow, OpenACC has the more mature implementations and broader support by com-\npilers. The following figure shows the full history of the standard’s releases. Note that\nversion 4.0 of the OpenMP standard is the first one to support GPU and accelerators.\nOpenACC\n• Version 1.0 Nov 2011\n• Version 2.0 Jun 2013\n• Version 2.5 Oct 2015\n• Version 2.6 Nov 2017\n• Version 2.7 Nov 2018\nOpenMP with GPU support\n• Version 4.0 July 2013\n• Version 4.5 Nov 2015\n• Version 5.0 Nov 2018\n• Version 5.1 Nov 2020 Release dates of GPU \npragma-based languages",5636
144-11.2.1 Compiling OpenACC code.pdf,144-11.2.1 Compiling OpenACC code,"373 Process to apply directives and pragmas for a GPU implementation\n11.1 Process to apply directives and pragmas \nfor a GPU implementation\nDirective or pragma-based annotations to C, C++, or Fortran applications provide one\nof the more attractive pathways to access the compute power of GPUs. Much like the\nOpenMP threading model covered in chapter 7, you can add just a few lines to your\napplication and the compiler generates code that can run on the GPU or the CPU. As\nfirst covered in chapters 6 and 7, pragmas are preprocessor statements in C and C++\nthat give the compiler special instructions. These take the form\n#pragma acc <directive> [clause]\n#pragma omp <directive> [clause]\nDirectives in the form of special comments provide the corresponding capability for\nFortran code. The directives start with the comment character, followed by either the\nacc or omp keyword to identify these as directives for OpenACC and OpenMP, respec-\ntively.\n!$acc <directive> [clause]\n!$omp <directive> [clause]\nThe same general steps are used for implementing OpenACC and OpenMP in appli-\ncations. Figure 11.1 shows these steps and we’ll detail them in the following sections.\nCPU GPU\nOriginal\nOﬄoad work\nto GPU\nManage data\nmovement\nto GPU\nOptimize\nGPU kernelsa[1000]\nfor door loop (128 threads)\nfree afor door loop (128 threads)\na[1000]\nfor door loop (128 threads)\nfor door loop (128 threads)\nfree aa[1000]\nfor door loop\nfor do or loop\nfree a\na[1000]\nfor door loop ( 256 threads )\nfor door loop ( 64 threads )\nfree a#work pragma\n#work pragma\n#work pragma\n#work pragma#data pragma{\n}\n#data pragma{\n#work pragma\n#work pragma\n}Steps\n1.\n2.\n3.Figure 11.1 Steps to \nimplement a GPU port with the \npragma-based languages. \nOffloading the work to a GPU \ncauses data transfers that slow \ndown the application until the \ndata movement is reduced.\n374 CHAPTER  11 Directive-based GPU programming\nWe summarize the three steps that we will use to convert a code to run on the GPU\nwith either OpenACC or OpenMP as follows:\n1Move the computationally intensive work to the GPU. This forces data transfers\nbetween the CPU and GPU that will slow down the code, but the work has to be\nmoved first.\n2Reduce the data movement between the CPU and GPU. Move allocations to the\nGPU if the data is only used there.\n3Tune the size of the workgroup, number of workgroups, and other kernel param-\neters to improve kernel performance.\nAt this point, you will have an application running much faster on the GPU. Further\noptimizations are possible to improve performance, although these tend to be more\nspecific for each application.\n11.2 OpenACC: The easiest way to run on your GPU\nWe’ll start with getting a simple application running with OpenACC. We do this to\nshow the basic details of getting things working. Then we’ll work on how to optimize\nthe application once it is running. As might be expected with a pragma-based approach,\nthere is a large payoff for a small effort. But first, you have to work through the initial\nslowdown of the code. Don’t despair! It is normal to encounter an initial slowdown on\nyour journey to faster computations on a GPU.\n Often the most difficult step is getting a working OpenACC compiler toolchain.\nSeveral solid OpenACC compilers are available. The most notable of the available\ncompilers are listed as follows:1\nPGI—This is a commercial compiler, but note that PGI has a community edi-\ntion for a free download.\nGCC —Versions 7 and 8 implement most of the OpenACC 2.0a specification.\nVersion 9 implements most of the OpenACC 2.5 specification. The OpenACC\ndevelopment branch in GCC is working on OpenACC 2.6, featuring further\nimprovements and optimizations.\nCray—Another commercial compiler; it is only available on Cray systems. Cray\nhas announced that they will no longer support OpenACC in their new LLVM-\nbased C/C++ compiler as of version 9.0. A “classic” version of the compiler that\nsupports OpenACC continues to be available.\nFor these examples, we’ll use the PGI compiler (version 19.7) and CUDA (version 10.1).\nThe PGI compiler is the most mature option among the more readily available compil-\ners. The GCC compiler is another option but be sure to use the most recent version\navailable. The Cray compiler is a great option if you have access to their system. \n1One of the original OpenACC compilers, CAPS, went out of business in 2016 and is no longer available.\n375 OpenACC: The easiest way to run on your GPU\nNOTE What if you don’t have a suitable GPU? You can still try the examples\nby running the code on your CPU with the OpenACC generated kernels. Per-\nformance will be different, but the basic code should be the same.\nWith the PGI compiler, you can first get information on your system with the pgac-\ncelinfo  command. It also lets you know if your system and environment are in work-\ning order. After running the command, the output should look something like what is\nshown in figure 11.2.\n11.2.1 Compiling OpenACC code\nListing 11.1 shows some excerpts from OpenACC makefiles. CMake provides the Find-\nOpenACC.cmake module called in line 18 in the listing. The full CMakeLists.txt file is\nincluded in the supplemental source code for the chapter in the OpenACC/StreamTriad\nFigure 11.2 Output from the pgaccelinfo  command shows the type of GPU and its characteristics.\n376 CHAPTER  11 Directive-based GPU programming\ndirectory at https:/ /github.com/EssentialsofParallelComputing/Chapter11 . We set some\nflags for compiler feedback and for the compiler to be less conservative about poten-\ntial aliasing. Both a CMake file and a simple makefile are provided in the subdirectory.\nOpenACC/StreamTriad/CMakeLists.txt\n 8 if (NOT CMAKE_OPENACC_VERBOSE)\n 9     set(CMAKE_OPENACC_VERBOSE true)\n10 endif (NOT CMAKE_OPENACC_VERBOSE)\n11 \n12 if (CMAKE_C_COMPILER_ID MATCHES ""PGI"")\n13     set(CMAKE_C_FLAGS ""${CMAKE_C_FLAGS} -alias=ansi"")\n14 elseif (CMAKE_C_COMPILER_ID MATCHES ""GNU"")\n15     set(CMAKE_C_FLAGS ""${CMAKE_C_FLAGS} -fstrict-aliasing"")\n16 endif (CMAKE_C_COMPILER_ID MATCHES ""PGI"")\n17 \n18 find_package(OpenACC)   \n19\n20 if (CMAKE_C_COMPILER_ID MATCHES ""PGI"")\n21     set(OpenACC_C_VERBOSE ""${OpenACC_C_VERBOSE} -Minfo=accel"")\n22 elseif (CMAKE_C_COMPILER_ID MATCHES ""GNU"")\n23    set(OpenACC_C_VERBOSE \n          ""${OpenACC_C_VERBOSE} -fopt-info-optimized-omp"")\n24 endif (CMAKE_C_COMPILER_ID MATCHES ""PGI"")\n25 \n26 if (CMAKE_OPENACC_VERBOSE)                      \n27   set(OpenACC_C_FLAGS \n       ""${OpenACC_C_FLAGS} ${OpenACC_C_VERBOSE}"")  \n28 endif (CMAKE_OPENACC_VERBOSE)                   \n29\n    < ... skipping first target ... > \n33 # Adds build target of stream_triad with source code files\n34 add_executable(StreamTriad_par1 StreamTriad_par1.c timer.c timer.h)\n35 set_source_files_properties(StreamTriad_par1.c PROPERTIES COMPILE_FLAGS\n     ""${OpenACC_C_FLAGS}"")   \n36 set_target_properties(StreamTriad_par1 PROPERTIES LINK_FLAGS\n     ""${OpenACC_C_FLAGS}"")   \nThe simple makefiles can also be used for building the example codes by copying or\nlinking these over to a Makefile by using either of these commands: \nln -s Makefile.simple.pgi Makefile\ncp Makefile.simple.pgi Makefile\nFrom the makefiles for the PGI and GCC compilers, we show the suggested flags for\nOpenACC:\nMakefile.simple.pgi\n 6 CFLAGS:= -g -O3 -c99 -alias=ansi -Mpreprocess -acc -Mcuda -Minfo=accel\n 7 Listing 11.1 Excerpts from OpenACC makefiles\nCMake module sets compiler \nflags for OpenACC\nAdds compiler feedback \nfor accelerator \ndirectives\nAdds OpenACC flags to compile and link stream triad source",7661
145-11.2.2 Parallel compute regions in OpenACC for accelerating computations.pdf,145-11.2.2 Parallel compute regions in OpenACC for accelerating computations,"377 OpenACC: The easiest way to run on your GPU\n 8 %.o: %.c\n 9   ${CC} ${CFLAGS} -c $^\n10 \n11 StreamTriad: StreamTriad.o timer.o\n12   ${CC} ${CFLAGS} $^ -o StreamTriad\nMakefile.simple.gcc\n 6 CFLAGS:= -g -O3 -std=gnu99 -fstrict-aliasing -fopenacc \\n                              -fopt-info-optimized-omp\n 7 \n 8 %.o: %.c\n 9   ${CC} ${CFLAGS} -c $^\n10 \n11 StreamTriad: StreamTriad.o timer.o\n12   ${CC} ${CFLAGS} $^ -o StreamTriad\nFor PGI, the flags to enable OpenACC compilation for GCC are -acc  -Mcuda . The\nMinfo=accel  flag tells the compiler to provide feedback on accelerator directives. We\nalso include the -alias=ansi  flag to tell the compiler to be less concerned about\npointer aliasing so that it can more freely generate parallel kernels. It is still a good\nidea to include the restrict  attribute on arguments in your source code to tell the\ncompiler that variables do not point to overlapping regions of memory. We also\ninclude a flag in both makefiles to set the C 1999 standard so that we can define loop\nindex variables in a loop for clearer scoping. The -fopenacc  flag turns on the parsing\nof the OpenACC directives for GCC. The -fopt-info-optimized-omp  flag tells the\ncompiler to provide feedback for code generation for the accelerator. \n For the Cray compiler, OpenACC is on by default. You can use the compiler option\n-hnoacc  if you need to turn it off. And the OpenACC compilers must define the\n_OPENACC  macro. The macro is particularly important because OpenACC is still in the\nprocess of being implemented by many compilers. You can use it to tell what version\nof OpenACC your compiler supports and to implement conditional compilations for\nnewer features by comparing against the compiler macro _OPENACC  == yyyymm , where\nthe version dates are\nVersion 1.0: 201111\nVersion 2.0: 201306\nVersion 2.5: 201510\nVersion 2.6: 201711\nVersion 2.7: 201811\nVersion 3.0: 201911\n11.2.2 Parallel compute regions in OpenACC for accelerating \ncomputations\nThere are two different options for declaring an accelerated block of code for computa-\ntions. The first is the kernels  pragma that gives the compiler freedom to auto-parallelize\nthe code block. This code block can include larger sections of code with several loops.\n378 CHAPTER  11 Directive-based GPU programming\nThe second is the parallel  loop  pragma that tells the compiler to generate code for\nthe GPU or other accelerator device. We’ll go over examples of each approach.\nUSING THE KERNELS  PRAGMA  TO GET AUTO-PARALLELIZATION  FROM THE COMPILER\nThe kernels  pragma allows auto-parallelization of a code block by the compiler. It is\noften used first to get feedback from the compiler on a section of code. We’ll cover\nthe formal syntax for the kernels  pragma, including its optional clauses. Then we’ll\nlook at the stream triad example we used in all of our programming chapters and\napply the kernels  pragma. First, we’ll list the specification for the kernels  pragma\nfrom the OpenACC 2.6 standard:\n#pragma acc kernels [ data clause | kernel optimization | async clause |\n                      conditional ]\n where\n   data clauses - [ copy | copyin | copyout | create | no_create |\n                    present | deviceptr | attach | default(none|present) ]\n   kernel optimization - [ num_gangs | num_workers | vector_length |\n                           device_type | self ]\n   async clauses - [ async | wait ]\n   conditional - [ if ]\nWe’ll discuss the data clauses in more detail in section 11.2.3, although you can also\nuse the data clauses in the kernel  pragma if these only apply to a single loop. We’ll\ncover the kernel optimizations in section 11.2.4. And we’ll briefly mention the async\nand conditional clauses in section 11.2.5.\n We first start by specifying where we want the work to be parallelized by adding\n#pragma  acc kernels  around the targeted blocks of code. The kernels  pragma\napplies to the code block following the directive or for the code in the next listing, the\nfor loop.\nOpenACC/StreamTriad/StreamTriad_kern1.c\n 1 #include <stdio.h>\n 2 #include <stdlib.h>\n 3 #include ""timer.h""\n 4 \n 5 int main(int argc, char *argv[]){\n 6 \n 7    int nsize = 20000000, ntimes=16;\n 8    double* a = malloc(nsize * sizeof(double));\n 9    double* b = malloc(nsize * sizeof(double));\n10    double* c = malloc(nsize * sizeof(double));\n11 \n12    struct timespec tstart;\n13    // initializing data and arrays\n14    double scalar = 3.0, time_sum = 0.0;\n15 #pragma acc kernels                       Listing 11.2 Adding the kernels  pragma\nInserts OpenACC \nkernels pragma\n379 OpenACC: The easiest way to run on your GPU\n16    for (int i=0; i<nsize; i++) {      \n17       a[i] = 1.0;                     \n18       b[i] = 2.0;                     \n19    }                                  \n20       \n21    for (int k=0; k<ntimes; k++){\n22       cpu_timer_start(&tstart);\n23       // stream triad loop \n24 #pragma acc kernels             \n25       for (int i=0; i<nsize; i++){    \n26          c[i] = a[i] + scalar*b[i];   \n27       }                               \n28       time_sum += cpu_timer_stop(tstart);\n29    }   \n30 \n31    printf(""Average runtime for stream triad loop is %lf msecs\n"",\n             time_sum/ntimes);\n32 \n33    free(a);\n34    free(b);\n35    free(c);\n36 \n37    return(0);\n38 }\nThe following output shows the feedback from the PGI compiler:\nmain:\n     15, Generating implicit copyout(b[:20000000],a[:20000000]) \n         [if not already present]\n     16, Loop is parallelizable\n         Generating Tesla code\n         16, #pragma acc loop gang, vector(128) \n             /* blockIdx.x threadIdx.x */\n     16, Complex loop carried dependence of a-> prevents parallelization\n         Loop carried dependence of b-> prevents parallelization\n     24, Generating implicit copyout(c[:20000000]) [if not already present]\n         Generating implicit copyin(b[:20000000],a[:20000000]) \n         [if not already present]\n     25, Complex loop carried dependence of a->,b-> prevents\n         parallelization\n         Loop carried dependence of c-> prevents parallelization\n         Loop carried backward dependence of c-> prevents vectorization\n         Accelerator serial kernel generated\n         Generating Tesla code\n         25, #pragma acc loop seq\n     25, Complex loop carried dependence of b-> prevents parallelization\n         Loop carried backward dependence of c-> prevents vectorization\nWhat isn’t clear in this listing is that OpenACC treats each for loop as if it has a\n#pragma  acc loop  auto  in front of it. We have left the decision to the compiler to\ndecide whether it could parallelize the loop. The output in bold indicates that theCode block for \nkernels pragma Inserts\nOpenACC\nkernels\npragma\n380 CHAPTER  11 Directive-based GPU programming\ncompiler doesn’t think it can. The compiler is telling us it needs help. The simplest fix\nis to add a restrict  attribute to lines 8-10 in listing 11.2.\n 8    double* restrict a = malloc(nsize * sizeof(double));\n 9    double* restrict b = malloc(nsize * sizeof(double));\n10    double* restrict c = malloc(nsize * sizeof(double));\nOur second choice for a fix to help the compiler is to change the directive to tell the\ncompiler it is Ok to generate parallel GPU code. The problem is the default loop\ndirective ( loop  auto ), which we mentioned earlier. Here is the specification from the\nOpenACC 2.6 standard:\n#pragma acc loop [ auto | independent | seq | collapse | gang | worker | \n                   vector | tile | device_type | private | reduction ]\nWe cover many of these clauses in later sections. For now, we’ll focus on the first three:\nauto , independent , and seq. \nauto  lets the compiler do the analysis. \nseq, short for sequential, says to generate a sequential version.\nindependent  asserts that the loop can and should be parallelized.\nChanging the clause from auto  to independent  tells the compiler to parallelize the\nloop:\n15 #pragma acc kernels loop independent\n   <Skipping unchanged code>\n24 #pragma acc kernels loop independent\nNote that we have combined the two constructs in these directives. You can combine\nvalid individual clauses into a single directive, if you like. Now the output shows that\nthe loop is parallelized: \nmain:\n     15, Generating implicit copyout(a[:20000000],b[:20000000]) \n         [if not already present]\n     16, Loop is parallelizable\n         Generating Tesla code\n         16, #pragma acc loop gang, vector(128) \n             /* blockIdx.x threadIdx.x */\n     24, Generating implicit copyout(c[:20000000]) [if not already present]\n         Generating implicit copyin(b[:20000000],a[:20000000]) \n         [if not already present]\n     25, Loop is parallelizable\n         Generating Tesla code\n         25, #pragma acc loop gang, vector(128) \n             /* blockIdx.x threadIdx.x */\nThe important thing to note in this output is the feedback about data transfers (in\nbold). We’ll discuss how to address this feedback in section 11.2.3.\n381 OpenACC: The easiest way to run on your GPU\nTRY THE PARALLEL  LOOP PRAGMA  FOR MORE CONTROL  OVER PARALLELIZATION\nNext we’ll cover how to use the parallel  loop  pragma. This is the technique we rec-\nommend that you use in your application. It is more consistent with the form used in\nother parallel languages such as OpenMP. It also generates more consistent and porta-\nble performance across compilers. Not all compilers can be counted on to perform an\nadequate job of analysis required by the kernels  directive.\n The parallel  loop  pragma is actually two separate directives. The first is the\nparallel  directive that opens a parallel region. The second is the loop  pragma that dis-\ntributes the work across the parallel work elements. We’ll look at the parallel  pragma\nfirst. The parallel  pragma takes the same clauses as the kernel  directive. In the fol-\nlowing example, we bolded the additional clauses for the kernel  directive:\n#pragma acc parallel [ clause ]\n   data clauses - [ reduction  | private | firstprivate  | copy | \n                     copyin | copyout | create | no_create | present |\n                     deviceptr | attach | default(none|present) ]\n   kernel optimization - [ num_gangs | num_workers | \n                            vector_length | device_type | self ]\n   async clauses - [ async | wait ]\n   conditional - [ if ]\nThe clauses for the loop  construct were mentioned earlier in the kernels section. The\nimportant thing to note is that the default for the loop  construct in a parallel region\nis independent  rather than auto . Again, as in the kernels  directive, the combined\nparallel  loop  construct can take any clause that the individual directives can. With\nthis explanation of the parallel  loop  construct, we move on to how it is added to the\nstream triad example as shown in the following listing.\nOpenACC/StreamTriad/StreamTriad_par1.c\n12    struct timespec tstart;\n13    // initializing data and arrays\n14    double scalar = 3.0, time_sum = 0.0;\n15 #pragma acc parallel loop          \n16    for (int i=0; i<nsize; i++) {\n17       a[i] = 1.0;\n18       b[i] = 2.0;\n19    }\n20 \n21    for (int k=0; k<ntimes; k++){\n22       cpu_timer_start(&tstart);\n23       // stream triad loop\n24 #pragma acc parallel loop          \n25       for (int i=0; i<nsize; i++){\n26          c[i] = a[i] + scalar*b[i];\n27       }\n28       time_sum += cpu_timer_stop(tstart);\n29    }Listing 11.3 Adding a parallel  loop  pragma\nInserts the \nparallel loop \ncombined \nconstruct\n382 CHAPTER  11 Directive-based GPU programming\nThe output from the PGI compiler is\nmain:\n     15, Generating Tesla code\n         16, #pragma acc loop gang, vector(128) \n             /* blockIdx.x threadIdx.x */\n     15, Generating implicit copyout(a[:20000000],b[:20000000]) \n         [if not already present]\n     24, Generating Tesla code\n         25, #pragma acc loop gang, vector(128) \n             /* blockIdx.x threadIdx.x */\n     24, Generating implicit copyout(c[:20000000]) [if not already present]\n         Generating implicit copyin(b[:20000000],a[:20000000]) \n         [if not already present]\nEven without the restrict  attribute, the loop is parallelized because the default for\nthe loop  directive is the independent  clause. This is different than the default for the\nkernels  directive that we saw previously. Still, we recommend that you use the restrict\nattribute in your code to help the compiler generate the best code.\n The output is similar to that from the previous kernels  directive. At this point, the\nperformance of the code will likely have slowed down due to the data movement we\nhave shown in bold in this compiler output. Not to worry; we will speed it back up in\nthe next step.\n Before we move on to addressing the data movement, we’ll take a quick look at\nreductions and the serial  construct. Listing 11.4 shows the mass sum example first\nintroduced in section 6.3.3. The mass sum is a simple reduction operation. Instead of\nthe OpenMP SIMD vectorization pragma, we placed an OpenACC parallel  loop\npragma with the reduction  clause before the loop. The syntax of the reduction is\nfamiliar because it is the same as was used by the threaded OpenMP standard.\nOpenACC/mass_sum/mass_sum.c\n 1 #include ""mass_sum.h""\n 2 #define REAL_CELL 1\n 3 \n 4 double mass_sum(int ncells, int* restrict celltype,\n 5                 double* restrict H, double* restrict dx,\n                   double* restrict dy){\n 6    double summer = 0.0;\n 7 #pragma acc parallel loop reduction(+:summer)    \n 8    for (int ic=0; ic<ncells ; ic++) {\n 9       if (celltype[ic] == REAL_CELL) {\n10          summer += H[ic]*dx[ic]*dy[ic];\n11       }\n12    }\n13    return(summer);\n14 }Listing 11.4 Adding a reduction  clause\nAdds a reduction \nclause to a parallel \nloop construct",14007
146-11.2.3 Using directives to reduce data movement between the CPU and the GPU.pdf,146-11.2.3 Using directives to reduce data movement between the CPU and the GPU,"383 OpenACC: The easiest way to run on your GPU\nThere are other operators that you can use in a reduction  clause. These include *,\nmax, min, &, |, &&, and ||. For OpenACC versions up to 2.6, the variable or list of vari-\nables separated by commas are limited to scalars and not arrays. But OpenACC ver-\nsion 2.7 lets you use arrays and composite variables in the reduction clause.\n The last construct we’ll cover in this section is the one for serial work. Some loops\ncannot be done in parallel. Rather than exit the parallel region, we stay within it and tell\nthe compiler to just do this one part in serial. This is done with the serial  directive:\n#pragma acc serial\nBlocks of this code with the serial  directive are executed by one gang of one worker\nwith a vector length of one. Now, let’s turn our attention to addressing the data move-\nment feedback.\n11.2.3 Using directives to reduce data movement between \nthe CPU and the GPU\nThis section returns to a theme we have seen throughout this book. Data movement is\nmore important than flops. Although we have sped up the computations by moving\nthese to the GPU, the overall run time has slowed because of the cost of data move-\nment. Addressing the excessive data movement will start yielding an overall speedup.\nTo do this, we add the data  construct to our code. In the OpenACC standard, v2.6,\nthe specification for the data  construct is as follows:\n#pragma acc data [ copy | copyin | copyout | create | no_create | present | \n                   deviceptr | attach | default(none|present) ]\nYou will also see references to clauses like present_or_copy  or the shorthand pcopy\nthat check for the presence of the data before making the copy. These are no longer\nnecessary, though they are retained for backward compatibility. The standard\nclauses have incorporated this behavior beginning with version 2.5 of the OpenACC\nstandard.\n Many of the data  clauses take an argument that lists the data to be copied or other-\nwise manipulated. The range specification for the array needs to be given to the com-\npiler. An example of this is\n#pragma acc data copy(x[0:nsize])\nThe range specification is subtly different for C/C++ and Fortran. In C/C++, the first\nargument in the specification is the start index, and the second is the length. In For-\ntran, the first argument is the start index, and the second argument is the end index.\n There are two varieties of data regions. The first is the structured data region from\nthe original OpenACC version 1.0 standard. The second, a dynamic data region, was\nintroduced in version 2.0 of OpenACC. We’ll look at the structured data region first.\n384 CHAPTER  11 Directive-based GPU programming\nSTRUCTURED  DATA REGION  FOR SIMPLE  BLOCKS  OF CODE\nThe structured data region is delimited by a code block. This can be a natural code\nblock formed by a loop or a region of code contained within a set of curly braces. In\nFortran, the region is marked with a starting directive and ends with an ending direc-\ntive. Listing 11.5 shows an example of a structured data region that starts with the\ndirective on line 16 and is delimited by the opening brace on line 17 and the ending\nbrace on line 37. We have included a comment on the ending brace in the code to\nhelp identify the block of code that the brace ends.\nOpenACC/StreamTriad/StreamTriad_par2.c\n16 #pragma acc data create(a[0:nsize],\             \n                           b[0:nsize],c[0:nsize])   \n17    {                                           \n18\n19 #pragma acc parallel loop present(a[0:nsize],\    \n                                     b[0:nsize])     \n20       for (int i=0; i<nsize; i++) {\n21          a[i] = 1.0;\n22          b[i] = 2.0;\n23       }\n24 \n25       for (int k=0; k<ntimes; k++){\n26          cpu_timer_start(&tstart);\n27          // stream triad loop\n28 #pragma acc parallel loop present(a[0:nsize],\    \n                          b[0:nsize],c[0:nsize])     \n29          for (int i=0; i<nsize; i++){\n30             c[i] = a[i] + scalar*b[i];\n31          }\n32          time_sum += cpu_timer_stop(tstart);\n33       }\n34 \n35       printf(""Average runtime for stream triad loop is %lf msecs\n"",\n                time_sum/ntimes);\n36 \n37    } //#pragma end acc data block(a[0:nsize],b[0:nsize],c[0:nsize])   \nThe structured data region specifies that the three arrays are to be created at the start\nof the data region. These will be destroyed at the end of the data region. The two par-\nallel loops use the present  clause to avoid data copies for the compute regions.\nDYNAMIC  DATA REGION  FOR A MORE FLEXIBLE  DATA SCOPING\nThe structured data region, originally used by OpenACC, where memory is allocated\nand then there are some loops, does not work with more complicated programs. In par-\nticular, memory allocations in object-oriented code occur when an object is created.\nHow do you put a data region around something with this kind of program structure? \n To address this problem, OpenACC v2.0 added dynamic (also called unstructured)\ndata regions. This dynamic data region construct was specifically created for moreListing 11.5 Structured data block pragma\nThe data directive defines \nthe structured data region.\nStarts\nthe data\nregion\nThe present \ndirective tells \nthe compiler \nthat a copy is \nnot needed.\nClosing brace\nmarks the end of\nthe data region\n385 OpenACC: The easiest way to run on your GPU\ncomplex data management scenarios, such as constructors and destructors in C++.\nRather than using scoping braces to define the data region, the pragma has an enter\nand an exit  clause:\n#pragma acc enter data\n#pragma acc exit data\nFor the exit  data  directive, there is an additional delete  clause that we can use. This\nuse of the enter/exit  data  directive is best done where allocations and deallocations\noccur. The enter  data  directive should be placed just after an allocation, and the\nexit  data  directive should be inserted just before the deallocation. This more natu-\nrally follows the existing data scope of variables in an application. Once you want\nhigher performance than what can be achieved from the loop-level strategy, these\ndynamic data regions become important. With the larger scope of the dynamic data\nregions, there is a need for an additional directive to update data:\n#pragma acc update [self(x) | device(x)]\nThe device  argument specifies that the data on the device is to be updated. The self\nargument says to update the local data, which is usually the host version of the data.\n Let’s look at an example using a dynamic data  pragma in listing 11.6. The enter\ndata  directive is placed after the allocation at line 12. The exit  data  directive at line\n35 is inserted before the deallocations. We suggest using dynamic data regions in pref-\nerence to structured data regions in almost all but the simplest code.\nOpenACC/StreamTriad/StreamTriad_par3.c\n 8    double* restrict a = malloc(nsize * sizeof(double));\n 9    double* restrict b = malloc(nsize * sizeof(double));\n10    double* restrict c = malloc(nsize * sizeof(double));\n11 \n12 #pragma acc enter data create(a[0:nsize],\    \n                      b[0:nsize],c[0:nsize])     \n13 \n14    struct timespec tstart;\n15    // initializing data and arrays\n16    double scalar = 3.0, time_sum = 0.0;\n17 #pragma acc parallel loop present(a[0:nsize],b[0:nsize])\n18    for (int i=0; i<nsize; i++) {\n19       a[i] = 1.0;\n20       b[i] = 2.0;\n21    }\n22 \n23    for (int k=0; k<ntimes; k++){\n24       cpu_timer_start(&tstart);\n25       // stream triad loop\n26 #pragma acc parallel loop present(a[0:nsize],b[0:nsize],c[0:nsize])\n27       for (int i=0; i<nsize; i++){\n28          c[i] = a[i] + scalar*b[i];Listing 11.6 Creating dynamic data regions\nStarts the dynamic data region \nafter memory allocation\n386 CHAPTER  11 Directive-based GPU programming\n29       }\n30       time_sum += cpu_timer_stop(tstart);\n31    }\n32 \n33    printf(""Average runtime for stream triad loop is %lf msecs\n"",\n             time_sum/ntimes);\n34 \n35 #pragma acc exit data delete(a[0:nsize],\.   \n                     b[0:nsize],c[0:nsize])     \n36 \n37    free(a);\n38    free(b);\n39    free(c);\nIf you paid close attention to the previous listing, you will have noticed that the arrays\na, b, and c are allocated on both the host and the device, but are only used on the\ndevice. In listing 11.7, we show one way to fix this by using the acc_malloc  routine\nand then putting the deviceptr  clause on the compute regions.\nOpenACC/StreamTriad/StreamTriad_par4.c\n 1 #include <stdio.h>\n 2 #include <openacc.h>\n 3 #include ""timer.h""\n 4 \n 5 int main(int argc, char *argv[]){\n 6 \n 7    int nsize = 20000000, ntimes=16;\n 8    double* restrict a_d =                   \n         acc_malloc(nsize * sizeof(double));   \n 9    double* restrict b_d =                   \n         acc_malloc(nsize * sizeof(double));   \n10    double* restrict c_d =                   \n         acc_malloc(nsize * sizeof(double));   \n11 \n12    struct timespec tstart;\n13    // initializing data and arrays\n14    const double scalar = 3.0;\n15    double time_sum = 0.0;\n16 #pragma acc parallel loop deviceptr(a_d, b_d)    \n17    for (int i=0; i<nsize; i++) {\n18       a_d[i] = 1.0;\n19       b_d[i] = 2.0;\n20    }\n21 \n22    for (int k=0; k<ntimes; k++){\n23       cpu_timer_start(&tstart);\n24       // stream triad loop\n25 #pragma acc parallel loop deviceptr(a_d, b_d,    \n                                       c_d)         \n26       for (int i=0; i<nsize; i++){\n27          c_d[i] = a_d[i] + scalar*b_d[i];\n28       }Listing 11.7 Allocating data only on the deviceEnds the dynamic data region \nbefore memory deallocation\nAllocates memory \non the device. _d \nindicates a device \npointer.\nThe deviceptr \nclause tells \nthe compiler \nthat memory \nis already on \nthe device.",9957
147-11.2.4 Optimizing the GPU kernels.pdf,147-11.2.4 Optimizing the GPU kernels,"387 OpenACC: The easiest way to run on your GPU\n29       time_sum += cpu_timer_stop(tstart);\n30    }\n31 \n32    printf(""Average runtime for stream triad loop is %lf msecs\n"",\n             time_sum/ntimes);\n33 \n34    acc_free(a_d);    \n35    acc_free(b_d);    \n36    acc_free(c_d);    \n37 \n38    return(0);\n39 }\nThe output from the PGI compiler is now much shorter as shown here:\n16 Generating Tesla code\n17 #pragma acc loop gang, vector(128) /* blockIdx.x threadIdx.x */\n25 Generating Tesla code\n26 #pragma acc loop gang, vector(128) /* blockIdx.x threadIdx.x */\nThe data movement is eliminated and memory requirements on the host reduced. We\nstill have some output giving feedback on the generated kernel that we will look at in\nsection 11.2.4. This example (listing 11.7) works for 1D arrays. For 2D arrays, the\ndeviceptr  clause does not take a descriptor argument, so the kernel has to be\nchanged to do its own 2D indexing in a 1D array.\n When referencing data regions, you have available a rich set of data directives and\ndata movement clauses that you can use to reduce unnecessary data movement. Still,\nthere are more clauses and OpenACC functions that we have not covered that can be\nuseful in specialized situations.\n11.2.4 Optimizing the GPU kernels\nGenerally, you will have greater impact getting more kernels running on the GPU and\nreducing the data movement than optimizing the GPU kernels themselves. The\nOpenACC compiler does a good job at producing the kernels, and the potential gains\nfrom further optimizations will be small. Occasionally, you can help the compiler to\nimprove the performance of key kernels enough for that to be worth some effort. \n In this section, we’ll go over the general strategies for these optimizations. First we’ll\ngo over the terminology used in the OpenACC standard. As figure 11.3 shows, OpenACC\ndefines abstract levels of parallelism that apply over multiple hardware devices.\n OpenACC defines these levels of parallelism:\nGang —An independent work block that shares resources. A gang can also syn-\nchronize within the group but not across the groups. For GPUs, gangs can be\nmapped to CUDA thread blocks or OpenCL work groups.\nWorkers —A warp in CUDA or work items within a work group in OpenCL.\nVector —A SIMD vector on the CPU and a SIMT work group or warp on the GPU\nwith contiguous memory references.Deallocates memory \non the device\n388 CHAPTER  11 Directive-based GPU programming\nSome examples of setting the level of a particular loop directive follow: \n#pragma acc parallel loop vector\n#pragma acc parallel loop gang\n#pragma acc parallel loop gang vector\nThe outer loop must be a gang  loop, and the inner loop should be a vector  loop. A\nworker  loop can appear in between. A sequential ( seq) loop can appear at any level.\n For most current GPUs, the vector length should be set to multiples of 32, so it is\nan integer multiple of the warp size. It should be no larger than the maximum threads\nper block, which is commonly around 1,024 on current GPUs (see the output from\nthe pgaccelinfo  command in figure 11.2). For the examples here, the PGI compiler\nsets the vector length to a reasonable value of 128. The value can be changed for a\nloop with the vector_length(x)  directive.\n In what scenario should you change the vector_length  setting? If the inner loop\no f  c o n t i g u o u s  d a t a  i s  l e s s  t h a n  1 2 8 ,  p a r t  o f  t h e  v e c t o r  w i l l  g o  u n u s e d .  I n  t h i s  c a s e ,\nreducing this value can be helpful. Another option would be to collapse a couple of\nthe inner loops to get a longer vector as we will discuss shortly.\n You can modify the worker  setting with the num_workers  clause. For the examples\nin this chapter, however, it is not used. Even so, it can be useful to increase it when\nshortening the vector length or for an additional level of parallelization. If your code\nneeds to synchronize within the parallel work group, you should use the worker level,\nbut OpenACC does not provide a user with a synchronization directive. The worker\nlevel also shares resources such as cache and local memory. \n The rest of the parallelization is done with gangs, which are the asynchronous par-\nallel level. Lots of gangs are important on GPUs to hide latency and for high occu-\npancy. Generally, the compiler sets this to a large number, so there is no need for theGangs\nVectorWorkersGangVectorWorkersGang\nVectorWorkersGang\nFigure 11.3 The hierarchy of the levels in \nOpenACC: gangs, workers, and vectors\n389 OpenACC: The easiest way to run on your GPU\nuser to override it. There is a num_gangs  clause available in the remote chance you\nmay need to do this.\n Many of these settings will only be appropriate for a particular piece of hardware.\nThe device_type(type)  before a clause restricts it to the specified device type. The\ndevice  type  setting stays active until the next device  type  clause is encountered. For\nexample\n1 #pragma acc parallel loop gang \\n2     device_type(acc_device_nvidia) vector_length(256) \\n3     device_type(acc_device_radeon) vector_length(64)\n4 for (int j=0; j<jmax; j++){\n5       #pragma acc loop vector\n6       for (int i=0; i<imax; i++){\n7           <work>\n8       }\n9 }\nFor a list of valid device types, look at the openacc.h header file for PGI v19.7. Note\nthat there is no acc_device_radeon  in the lines from the openacc.h header file previ-\nously shown, so the PGI compiler does not support the AMD Radeon™ device. This\nmeans we need a C preprocessor ifdef  around line 3 in the previous sample code to\nkeep the PGI compiler from complaining.\nExcerpt from openacc.h file for PGI\n27 typedef enum{\n28     acc_device_none          = 0,\n29     acc_device_default       = 1,\n30     acc_device_host          = 2,\n31     acc_device_not_host      = 3,\n32     acc_device_nvidia        = 4,\n33     acc_device_pgi_opencl    = 7,\n34     acc_device_nvidia_opencl = 8,\n35     acc_device_opencl        = 9,\n36     acc_device_current       = 10\n37     } acc_device_t;\nThe syntax for the kernels  directive is slightly different, with the parallel type applied\nto each loop  directive individually and taking the int argument directly:\n#pragma acc kernels loop gang\nfor (int j=0; j<jmax; j++){\n      #pragma acc loop vector(64)\n      for (int i=0; i<imax; i++){\n          <work>\n      }\n}\nLoops can be combined with the collapse(n)  clause. This is especially useful if there\nare two small inner loops contiguously striding through data. Combining these allows\nyou to use a longer vector length. The loops must be tightly nested.\n390 CHAPTER  11 Directive-based GPU programming\nDEFINITION Two or more loops that have no extra statements between the\nfor or do statements or between the end of the loops are tightly-nested loops .\nAn example of combining two loops in order to use a long vector is\n#pragma acc parallel loop collapse(2) vector(32)\nfor (int j=0; j<8; j++){\n      for (int i=0; i<4; i++){\n          <work>\n      }\n}\nOpenACC v2.0 added a tile  clause that you can use for optimization. You can either\nspecify the tile size or use asterisks to let the compiler choose:\n#pragma acc parallel loop tile(*,*)\nfor (int j=0; j<jmax; j++){\n      for (int i=0; i<imax; i++){\n          <work>\n      }\n}\nNow it is time to try out the various kernel optimizations. The stream triad example\ndid not show any real benefits from our optimization attempts, so we will work with\nthe stencil example used in many of the previous chapters. \n The associated code for the stencil example for this chapter goes through the\nsame first two steps of moving the computational loops to the GPU and then reducing\nthe data movement. The stencil code also requires one additional change. On the\nCPU, we swap pointers at the end of the loop. On the GPU, in lines 45–50, we have to\ncopy the new data back to the original array. The following listing takes up the stencil\ncode example with these steps completed.\nOpenACC/Stencil/Stencil_par3.c\n17 #pragma acc enter data create( \                 \n         x[0:jmax][0:imax], xnew[0:jmax][0:imax])   \n18 \n19 #pragma acc parallel loop present( \             \n         x[0:jmax][0:imax], xnew[0:jmax][0:imax])   \n20    for (int j = 0; j < jmax; j++){\n21       for (int i = 0; i < imax; i++){\n22          xnew[j][i] = 0.0;\n23          x[j][i]    = 5.0;\n24       }\n25    }\n26 \n27 #pragma acc parallel loop present( \             \n         x[0:jmax][0:imax], xnew[0:jmax][0:imax])   \n28    for (int j = jmax/2 - 5; j < jmax/2 + 5; j++){\n29       for (int i = imax/2 - 5; i < imax/2 -1; i++){Listing 11.8 Stencil example with compute loops on the GPU and data motion optimized\nDynamic data \nregion directives\nCompute \nregion \ndirectives\n391 OpenACC: The easiest way to run on your GPU\n30          x[j][i] = 400.0;\n31       }\n32    }\n33 \n34    for (int iter = 0; iter < niter; iter+=nburst){\n35 \n36       for (int ib = 0; ib < nburst; ib++){\n37          cpu_timer_start(&tstart_cpu);\n38 #pragma acc parallel loop present( \               \n         x[0:jmax][0:imax], xnew[0:jmax][0:imax])     \n39          for (int j = 1; j < jmax-1; j++){\n40             for (int i = 1; i < imax-1; i++){\n41                xnew[j][i]=(x[j][i]+x[j][i-1]+x[j][i+1]+\n                                      x[j-1][i]+x[j+1][i])/5.0;\n42             }\n43          }\n44 \n45 #pragma acc parallel loop present( \               \n         x[0:jmax][0:imax], xnew[0:jmax][0:imax])     \n46          for (int j = 0; j < jmax; j++){\n47             for (int i = 0; i < imax; i++){\n48                x[j][i] = xnew[j][i];\n49             }\n50          }\n51          cpu_time += cpu_timer_stop(tstart_cpu);\n52       }\n53 \n54       printf(""Iter %d\n"",iter+nburst);\n55    }\n56 \n57 #pragma acc exit data delete( \                  \n         x[0:jmax][0:imax], xnew[0:jmax][0:imax])   \nFirst, note that we are using the dynamic data region directives, so there are no braces\nwrapping the data region as we would see with the structured data region. The\ndynamic region begins the data region when it encounters the enter  directive and\nends when it reaches an exit  directive, no matter what path occurs between the two\ndirectives. In this case, it is a straight line of execution from the enter  to the exit\ndirective. We’ll add the collapse  clause to the parallel loop to reduce the overhead\nfor the two loops. The following listing shows this change.\nOpenACC/Stencil/Stencil_par4.c\n36       for (int ib = 0; ib < nburst; ib++){\n37          cpu_timer_start(&tstart_cpu);\n38 #pragma acc parallel loop collapse(2)\      \n39      present(x[0:jmax][0:imax], xnew[0:jmax][0:imax])\n40          for (int j = 1; j < jmax-1; j++){\n41             for (int i = 1; i < imax-1; i++){\n42                xnew[j][i]=(x[j][i]+x[j][i-1]+x[j][i+1]+\n                                      x[j-1][i]+x[j+1][i])/5.0;\n43             }Listing 11.9 Stencil example with a collapse  clauseCompute \nregion \ndirectives\nDynamic data \nregion directives\nAdds the collapse clause to \nthe parallel loop directive\n392 CHAPTER  11 Directive-based GPU programming\n44          }\n45 #pragma acc parallel loop collapse(2)\\n46      present(x[0:jmax][0:imax], xnew[0:jmax][0:imax])\n47          for (int j = 0; j < jmax; j++){\n48             for (int i = 0; i < imax; i++){\n49                x[j][i] = xnew[j][i];\n50             }\n51          }\n52          cpu_time += cpu_timer_stop(tstart_cpu);\n53 \n54       }\nWe can also try using the tile  clause. We start out by letting the compiler determine\nthe tile size as shown in lines 41 and 48 in the following listing.\nOpenACC/Stencil/Stencil_par5.c\n39       for (int ib = 0; ib < nburst; ib++){\n40          cpu_timer_start(&tstart_cpu);\n41 #pragma acc parallel loop tile(*,*) \       \n42     present(x[0:jmax][0:imax], xnew[0:jmax][0:imax])\n43          for (int j = 1; j < jmax-1; j++){\n44             for (int i = 1; i < imax-1; i++){\n45                xnew[j][i]=(x[j][i]+x[j][i-1]+x[j][i+1]+\n                                      x[j-1][i]+x[j+1][i])/5.0;\n46             }\n47          }\n48 #pragma acc parallel loop tile(*,*) \\n49     present(x[0:jmax][0:imax], xnew[0:jmax][0:imax])\n50          for (int j = 0; j < jmax; j++){\n51             for (int i = 0; i < imax; i++){\n52                x[j][i] = xnew[j][i];\n53             }\n54          }\n55          cpu_time += cpu_timer_stop(tstart_cpu);\n56 \n57       }\nThe change in the run times from these optimizations is small relative to the improve-\nment seen from the initial OpenACC implementation. Table 11.1 shows the results for\nthe NVIDIA V100 GPU with the PGI compiler v19.7.Listing 11.10 Stencil example with a tile  clause\nTable 11.1 Run times for the OpenACC stencil kernel optimizations\nOpenACC stencil kernel run time (secs)\nSerial CPU code 5.237\nAdding compute and data regions 0.818\nAdding collapse(2)  clause 0.802\nAdding tile(*,*)  clause 0.806Adds the tile clause to the \nparallel loop directive",13155
148-11.2.5 Summary of performance results for the stream triad.pdf,148-11.2.5 Summary of performance results for the stream triad,,0
149-11.2.6 Advanced OpenACC techniques.pdf,149-11.2.6 Advanced OpenACC techniques,"393 OpenACC: The easiest way to run on your GPU\nWe tried changing the vector length to 64 or 256 and different tile sizes, but didn’t see\nany improvement in the run times. More complex code can find more benefit from\nkernel optimizations, but note that any specialization of parameters such as vector\nlength impacts portability by compilers for different architectures.\n Another target for optimization is to implement a pointer swap at the end of the\nloop. The pointer swap is used in the original CPU code as a fast way to get data\nback to the original array. The copy of the data back to the original array doubles the\nrun time on the GPU. The difficulty in pragma-based languages is that the pointer\nswap in a parallel region requires swapping both the host and the device pointers at\nthe same time. \n11.2.5 Summary of performance results for the stream triad\nThe run-time performance during the conversion to the GPU shows the typical pat-\ntern. Moving the computational kernels over to the GPU results in a slow down by\nabout a factor of 3 as shown by the kernel 2 and parallel 1 implementations in table\n11.2. In the kernel 1 case, the computational loop fails to parallelize. Running sequen-\ntially on the GPU, it was even slower. Once the data movement was reduced in kernel\n3 and parallel 2–4, the run times showed a 67x speedup. The particular type of data\nregion didn’t matter so much for performance, but might be important to enable\nports of additional loops in more complex codes.\n11.2.6 Advanced OpenACC techniques\nMany other features in OpenACC are available to handle more complex code. We’ll\ncover these briefly so you know what capabilities are available.Table 11.2 Run times from OpenACC stream triad kernel optimizations\nOpenACC stream triad kernel run time (ms)\nSerial CPU code 39.6\nKernel 1. Fails to parallelize loop 1771\nKernel 2. Adds compute region 118.5\nKernel 3. Adds dynamic data region 0.590\nParallel 1. Adds compute region 118.8\nParallel 2. Adds structured data region 0.589\nParallel 3. Adds dynamic data region 0.590\nParallel 4. Allocates data only on device 0.586\n394 CHAPTER  11 Directive-based GPU programming\nHANDLING  FUNCTIONS  WITH THE OPENACC ROUTINE  DIRECTIVE\nOpenACC v1.0 required functions for use in kernels to be inlined. Version 2.0 added\nthe routine  directive with two different versions to make calling routines simpler. The\ntwo versions are\n#pragma acc routine [gang | worker | vector | seq | bind | no_host | \n                     device_type]\n#pragma acc routine(name)  [gang | worker | vector | seq | bind | no_host | \n                            device_type]\nIn C and C++, the routine  directive should appear immediately before a function pro-\ntotype or definition. The named version can appear anywhere before the function is\ndefined or used. The Fortran version should include the !#acc  routine  directive\nwithin the function body itself or in the interface body.\nAVOIDING  RACE CONDITIONS  WITH OPENACC ATOMICS\nMany threaded routines have a shared variable that has to be updated by multiple\nthreads. This programming construct is both a common performance bottleneck and\na potential race condition. To handle this situation, OpenACC v2 provides atomics to\nallow only one thread to access a storage location at a time. The syntax and valid\nclauses for the atomic directive are\n#pragma acc atomic [read | write | update | capture]\nIf you don’t specify a clause, the default is an update . An example of the use of the\natomic  clause is\n#pragma acc atomic\ncnt++;\nASYNCHRONOUS  OPERATIONS  IN OPENACC\nOverlapping OpenACC operations can help improve performance. The proper term\nfor overlapping operations is asynchronous . OpenACC provides these asynchronous\noperations with the async  and wait  clauses and directives. The async  clause is added\nto a work or data directive with an optional integer argument:\n#pragma acc parallel loop async([<integer>])\nThe wait  can be either a directive or a clause added to a work or data directive. The\nfollowing pseudo-code in listing 11.11 shows how you can use this to launch the calcu-\nlations on the x-faces and y-faces of a computational mesh and then wait for the\nresults to update the cell values for the next iteration.\nfor (int n = 0; n < ntimes; ) {\n   #pragma acc parallel loop async\n      <x face pass>Listing 11.11 Async wait  example in OpenACC\n395 OpenACC: The easiest way to run on your GPU\n   #pragma acc parallel loop async\n      <y face pass>\n   #pragma acc wait\n   #pragma acc parallel loop\n      <Update cell values from face fluxes>\n}\nUNIFIED  MEMORY  TO AVOID  MANAGING  DATA MOVEMENT\nAlthough unified memory is not currently part of the OpenACC standard, there are\nexperimental developments with having the system manage memory movement. Such\nan experimental implementation of unified memory is available in CUDA and the\nPGI OpenACC compiler. Using the -ta=tesla:managed  flag with the PGI compiler\nand recent NVIDIA GPUs, you can try out their unified memory implementation.\nWhile the coding is simplified, the performance impacts are still not known and will\nchange as the compilers mature.\nINTEROPERABILITY  WITH CUDA LIBRARIES  OR KERNELS\nOpenACC provides several directives and functions to make it possible to interoper-\nate with CUDA libraries. In calling libraries, it is necessary to tell the compiler to use\nthe device pointers instead of host data. The host_data  directive can be used for\nthis purpose:\n#pragma acc host_data use_device(x, y)\ncublasDaxpy(n, 2.0, x, 1, y, 1);\nWe showed a similar example when we allocated memory using acc_malloc  in listing\n11.7. With acc_malloc  or cudaMalloc , the pointer returned is already on the device.\nFor this case, we used the deviceptr  clause to pass the pointer to the data region. \n One of the most common mistakes in programming GPUs in any language is con-\nfusing a device pointer and a host pointer. Try finding 86 Pike Place, San Francisco,\nwhen it is really 86 Pike Place, Seattle. The device pointer points to a different physical\nblock of memory on the GPU hardware. \n Figure 11.4 shows the three different operations we have covered to help you under-\nstand the differences. In the first case, the malloc  routine returns a host pointer. The\nHost Device\nx_devx_host\nx_dev\nx_dev(dev *)x_host\nx_dev x_dev\nx_dev\nhost ptr host ptr dev ptr dev ptracc_malloc, cudaMalloc\ndeviceptr(x_dev)malloc\npresent(x_host)Operation in host code\nhost_data use_device(x_dev)\ndev_function(x_dev)\nFigure 11.4 Is it a device pointer or a host pointer? One points to the GPU memory \nand the other to the CPU memory, respectively. OpenACC keeps a map between arrays \nin the two address spaces and provides routines for retrieving each.",6812
150-11.3.2 Generating parallel work on the GPU with OpenMP.pdf,150-11.3.2 Generating parallel work on the GPU with OpenMP,"396 CHAPTER  11 Directive-based GPU programming\npresent  clause converts this to a device pointer for the device kernel. In the second\ncase, where we allocate memory on the device with acc_malloc  or cudaMalloc , we\nare given a device pointer. We use the deviceptr  clause to send it to the GPU without\nany changes. In the last case, we don’t have a pointer on the host at all. We have to use\nthe host_data  use_device(var)  directive to retrieve the device pointer to the host.\nThis is done so that we have a pointer to send back to the device in the argument list\nfor the device function.\n It is good practice to append a _h or _d to pointers to clarify their valid context. In\nour examples, all pointers and arrays are assumed to be on the host except for those\nending with _d, which is for any device pointer.\nMANAGING  MULTIPLE  DEVICES  IN OPENACC\nMany current HPC systems already have multiple GPUs. We can also foresee that we\nwill get nodes with different accelerators. The ability to manage which device we are\nusing becomes more and more important. OpenACC gives us this capability through\nthe following functions:\nint acc_get_num_devices(acc_device_t)\nacc_set_device_type()  / acc_get_device_type()\nacc_set_device_num()  / acc_get_device_num()\nWe have now covered as much of OpenACC as we can in a dozen pages. The skills we’ve\nshown you are enough to get you started on an implementation. There is a lot more\nfunctionality available in the OpenACC standard, but much of it is for more complex sit-\nuations or low-level interfaces that are not necessary for entry-level applications.\n11.3 OpenMP: The heavyweight champ enters \nthe world of accelerators\nThe OpenMP accelerator capability is an exciting addition to the traditional thread-\ning model. In this section, we show you how to get started with these directives. We’ll\nuse the same examples as we did for the OpenACC section 11.2. By the end of this sec-\ntion, you should have some idea of how the two similar languages compare and which\nmight be the better choice for your application.\n Where do OpenMP’s accelerator directives stand in comparison to OpenACC?\nThe OpenMP implementations are notably less mature at this point, though rapidly\nimproving. The currently available implementations for GPUs are as follows:\nCray was first with an OpenMP implementation targeting NVIDIA GPUs in\n2015. Cray now supports OpenMP v4.5. \nIBM fully supports OpenMP v4.5 on Power 9 processor and NVIDIA GPUs.\nClang v7.0+ supports OpenMP v4.5 offloads to NVIDIA GPUs.\nGCC v6+ can offload to AMD GPUs; v7+ can offload to NVIDIA GPUs.\nThe two most mature implementations, Cray and IBM, are available only on their respec-\ntive systems. Unfortunately, not everyone has access to systems from these vendors, but\n397 OpenMP: The heavyweight champ enters the world of accelerators\nthere are more widely available compilers. Two of these compilers, Clang and GCC,\nare in the throes of development with marginal versions available now. Look out for\nnew developments with these compilers. The examples in this section use the IBM®\nXL 16 compiler and CUDA v10.\n11.3.1 Compiling OpenMP code\nWe start with how to set up a build environment and compile an OpenMP code. CMake\nhas an OpenMP module, but it does not have explicit support for the OpenMP acceler-\nator directives. We include an OpenMPAccel module that calls the regular OpenMP\nmodule and adds the flags needed for the accelerator. It also checks the OpenMP ver-\nsion that is supported, and if it is not v4.0 or newer, it generates an error. This CMake\nmodule is included with the source code for the chapter. \n Listing 11.12 shows excerpts from the main CMakeLists.txt file in this chapter.\nFeedback from most of the OpenMP compilers is weak right now, so setting the\n-DCMAKE_OPENMPACCEL  flag for CMake will only have minimal benefit. We’ll leverage\nother tools in these examples to fill in the gap.\nOpenMP/StreamTriad/CMakeLists.txt\n10 if (NOT CMAKE_OPENMPACCEL_VERBOSE)\n11     set(CMAKE_OPENMPACCEL_VERBOSE true)\n12 endif (NOT CMAKE_OPENMPACCEL_VERBOSE)\n13 \n14 if (CMAKE_C_COMPILER_ID MATCHES ""GNU"")\n15     set(CMAKE_C_FLAGS ""${CMAKE_C_FLAGS} -fstrict-aliasing"")\n16 elseif (CMAKE_C_COMPILER_ID MATCHES ""Clang"")\n17     set(CMAKE_C_FLAGS ""${CMAKE_C_FLAGS} -fstrict-aliasing"")\n18 elseif (CMAKE_C_COMPILER_ID MATCHES ""XL"")\n19     set(CMAKE_C_FLAGS ""${CMAKE_C_FLAGS} -qalias=ansi"")\n20 elseif (CMAKE_C_COMPILER_ID MATCHES ""Cray"")\n21     set(CMAKE_C_FLAGS ""${CMAKE_C_FLAGS} -h restrict=a"")\n22 endif (CMAKE_C_COMPILER_ID MATCHES ""GNU"")\n23 \n24 find_package(OpenMPAccel)       \n25 \n26 if (CMAKE_C_COMPILER_ID MATCHES ""XL"")\n27     set(OpenMPAccel_C_FLAGS                     \n         ""${OpenMPAccel_C_FLAGS} -qreport"")        \n28 elseif (CMAKE_C_COMPILER_ID MATCHES ""GNU"")\n29     set(OpenMPAccel_C_FLAGS \n         ""${OpenMPAccel_C_FLAGS} -fopt-info-omp"")  \n30 endif (CMAKE_C_COMPILER_ID MATCHES ""XL"")\n31 \n32 if (CMAKE_OPENMPACCEL_VERBOSE)\n33     set(OpenACC_C_FLAGS ""${OpenACC_C_FLAGS} ${OpenACC_C_VERBOSE}"")\n34 endif (CMAKE_OPENMPACCEL_VERBOSE)\n35 \n36 # Adds build target of stream_triad_par1 with source code files\n37 add_executable(StreamTriad_par1 StreamTriad_par1.c timer.c timer.h)Listing 11.12 Excerpts from an OpenMPaccel makefile\nCMake module sets compiler flags \nfor OpenMP accelerator devices.\nAdds compiler feedback \nfor accelerator \ndirectives\n398 CHAPTER  11 Directive-based GPU programming\n38 set_target_properties(StreamTriad_par1 PROPERTIES\n                         COMPILE_FLAGS ${OpenMPAccel_C_FLAGS})   \n39 set_target_properties(StreamTriad_par1 PROPERTIES\n                         LINK_FLAGS ""${OpenMPAccel_C_FLAGS}"")    \nThe simple makefile can also be used for building the example codes by copying or\nlinking these over to Makefile  with either of the following: \nln -s Makefile.simple.xl Makefile\ncp Makefile.simple.xl Makefile\nThe following code snippet shows the suggested flags for the OpenMP accelerator\ndirectives in the simple makefiles for the IBM XL and GCC compilers:\nMakefile.simple.xl\n 6 CFLAGS:=-qthreaded -g -O3 -std=gnu99 -qalias=ansi -qhot -qsmp=omp \\n           -qoffload -qreport\n 7 \n 8 %.o: %.c\n 9   ${CC} ${CFLAGS} -c $^\n10 \n11 StreamTriad: StreamTriad.o timer.o\n12   ${CC} ${CFLAGS} $^ -o StreamTriad\nMakefile.simple.gcc\n 6 CFLAGS:= -g -O3 -std=gnu99 -fstrict-aliasing \\n 7          -fopenmp -foffload=nvptx-none -foffload=-lm -fopt-info-omp\n 8 \n 9 %.o: %.c\n10   ${CC} ${CFLAGS} -c $^\n11 \n12 StreamTriad: StreamTriad.o timer.o\n13   ${CC} ${CFLAGS} $^ -o StreamTriad\n11.3.2 Generating parallel work on the GPU with OpenMP\nN o w  w e  n e e d  t o  g e n e r a t e  p a r a l l e l  w o r k  o n  t h e  G P U .  T h e  O p e n M P  d e v i c e  p a r a l l e l\nabstractions are more complicated than we saw with OpenACC. But this can also pro-\nvide more flexibility in scheduling work in the future. For now, you should preface\neach loop with this directive:\n#pragma omp target teams distribute parallel for simd\nThis is a long, confusing directive. Let’s go over each of the parts as illustrated in fig-\nure 11.5. The first three clauses specify hardware resources: \ntarget  gets onto the device\nteams  creates a league of teams\ndistribute  spreads work out to teamsAdds OpenMP accelerator flags for\ncompiling and linking of stream triad\n399 OpenMP: The heavyweight champ enters the world of accelerators\nThe remaining three are the parallel work clauses. All three clauses are necessary for\nportability. This is because the implementations by compilers spread out the work in\ndifferent manners.\nparallel  replicates work on each thread\nfor spreads work out within each team\nsimd  spreads work out to threads (GCC)\nFor kernels with three nested loops, one way you can spread out the work is with the\nfollowing:\nk loop: #pragma omp target teams distribute\nj loop: #pragma omp parallel for\ni loop: #pragma omp simd\nEach OpenMP compiler can spread out the work differently, thus requiring some vari-\nants of this scheme. The simd  loop should be the inner loop across contiguous mem-\nory locations. Some simplification of this complexity is being introduced with the\nloop  clause in OpenMP v5.0 as we will present in section 11.3.5. You can also add\nclauses to this directive:\nprivate, firstprivate, lastprivate, shared, reduction, collapse, \n         dist_schedule\nMany of these clauses are familiar from OpenACC and behave the same way. One of\nthe major differences from OpenACC is the default way that data is handled when\nentering a parallel work region. OpenACC compilers generally move all necessary\narrays to the device. For OpenMP, there are two possibilities:\nScalars and statically allocated arrays are moved onto the device by default\nbefore execution.\nData allocated on the heap needs to be explicitly copied to and from the device.target\nparallel for simdCPU GPUT teams distribute\nGPU WG WG WG WG GPU WG WG WG WG GPU WG\nwarpWG\nwarpWG\nwarpWG\nwarpEnabling more hardware resources\nSpreading out workSingle work\ngroup, single\nthreadMultiple work groups,\nsingle threadMultiple work groups,\nmultiple main threads\nSpreading work out\nto threads in work groupT TTTT T1T2T3T4 T4 T3 T2 T1\nFigure 11.5 The target , teams , and distribute  directives enable more hardware resources. The \nparallel  for simd  directive spreads out the work within each workgroup.\n400 CHAPTER  11 Directive-based GPU programming\nLet’s look at a simple example of adding a parallel work directive in listing 11.13. We\nuse statically allocated arrays that behave as if they are allocated on the stack;\nalthough, because of the large size, the actual memory might be allocated by the com-\npiler on the heap.\nOpenMP/StreamTriad/StreamTriad_par1.c\n 6 int main(int argc, char *argv[]){\n 7 \n 8    int nsize = 20000000, ntimes=16;\n 9    double a[nsize];                  \n10    double b[nsize];                  \n11    double c[nsize];                  \n12 \n13    struct timespec tstart;\n14    // initializing data and arrays\n15    double scalar = 3.0, time_sum = 0.0;\n16 #pragma omp target teams distribute parallel for simd\n17    for (int i=0; i<nsize; i++) {\n18       a[i] = 1.0;\n19       b[i] = 2.0;\n20    }\n21 \n22    for (int k=0; k<ntimes; k++){\n23       cpu_timer_start(&tstart);\n24       // stream triad loop \n25 #pragma omp target teams distribute parallel for simd\n26       for (int i=0; i<nsize; i++){\n27          c[i] = a[i] + scalar*b[i];\n28       }\n29       time_sum += cpu_timer_stop(tstart);\n30    }\n31 \n32    printf(""Average runtime for stream triad loop is %lf secs\n"",\n             time_sum/ntimes);\nThe feedback from the IBM XL compiler shows that the two kernels are offloaded to\nthe GPU, but no other information is proffered. GCC gives no feedback at all. The\nIBM XL output is\n"""" 1586-672 (I) GPU OpenMP Runtime elided for offloaded kernel\n                '__xl_main_l15_OL_1'\n"""" 1586-672 (I) GPU OpenMP Runtime elided for offloaded kernel\n                '__xl_main_l23_OL_2'\nTo get some information on what the IBM XL compiler has done, we’ll use the NVIDIA\nprofiler:\nnvprof ./StreamTriad_par1Listing 11.13 Adding OpenMP pragmas to parallelize work on the GPU\nAllocating \nstatic arrays \non the host\n401 OpenMP: The heavyweight champ enters the world of accelerators\nThe first part of the output is\n==141409== Profiling application: ./StreamTriad_par1\n==141409== Profiling result:\nTime(%)  Time   Calls     Avg       Min       Max    Name\n64.11% 554.30ms  7652  72.439us  1.2160us  79.039us  [CUDA memcpy DtoH]\n34.79% 300.82ms  7650  39.323us  23.392us  48.767us  [CUDA memcpy HtoD]\n 1.06% 9.1479ms    16  571.75us  571.39us  572.32us  __xl_main_l23_OL_2\n 0.04% 363.07us     1  363.07us  363.07us  363.07us  __xl_main_l15_OL_1\nFrom this output, we now know that there is a memory copy from the host to the device\n(HtoD  in the output) and then back from the device to the host ( DtoH  in the output).\nThe nvprof  output from GCC is similar but without line numbers. More detail about\nthe order in which the operations occur can be obtained with the following:\nnvprof --print-gpu-trace ./StreamTriad_par1\nMost programs are not written with statically allocated arrays. Let’s take a look at a\nmore commonly found case where the arrays are dynamically allocated as the follow-\ning listing shows.\nOpenMP/StreamTriad/StreamTriad_par2.c\n 9    double* restrict a = \n         malloc(nsize * sizeof(double));    \n10    double* restrict b = \n         malloc(nsize * sizeof(double));    \n11    double* restrict c = \n         malloc(nsize * sizeof(double));    \n12 \n13    struct timespec tstart;\n14    // initializing data and arrays\n15    double scalar = 3.0, time_sum = 0.0;\n16 #pragma omp target teams distribute \      \n               parallel for simd \            \n17             map(a[0:nsize], b[0:nsize], \n                   c[0:nsize])                \n18    for (int i=0; i<nsize; i++) {\n19       a[i] = 1.0;\n20       b[i] = 2.0;\n21    }\n22 \n23    for (int k=0; k<ntimes; k++){\n24       cpu_timer_start(&tstart);\n25       // stream triad loop \n26 #pragma omp target teams distribute \      \n               parallel for simd \            \n27             map(a[0:nsize], b[0:nsize], \n                   c[0:nsize])                \n28       for (int i=0; i<nsize; i++){\n29          c[i] = a[i] + scalar*b[i];Listing 11.14 Parallel work directive with arrays dynamically allocated\nDynamically \nallocated \nmemory\nParallel work \ndirective for heap \nallocated memory",13679
151-11.3.3 Creating data regions to control data movement to the GPU with OpenMP.pdf,151-11.3.3 Creating data regions to control data movement to the GPU with OpenMP,"402 CHAPTER  11 Directive-based GPU programming\n30       }\n31       time_sum += cpu_timer_stop(tstart);\n32    }\n33 \n34    printf(""Average runtime for stream triad loop is %lf secs\n"",\n             time_sum/ntimes);\n35 \n36    free(a);\n37    free(b);\n38    free(c);\nNote that lines 16 and 26 have added the map clause. If you try the directive without\nthis clause, although it compiles fine with the IBM XLC compiler, at run time you’ll\nget this message:\n1587-164 Encountered a zero-length array section that points to memory \nstarting at address 0x200020000010. Because this memory is not currently \nmapped on the target device 0, a NULL pointer will be passed to the \ndevice.\n1587-175 The underlying GPU runtime reported the following error ""an illegal \nmemory access was encountered"". \n1587-163 Error encountered while attempting to execute on the target device \n0.  The program will stop.\nThe GCC compiler, however, both compiles and runs fine without the map directive.\nThus, the GCC compiler moves heap allocated memory over to the device while\nIBM XLC does not. For portability, we should include the map clause in our applica-\ntion code.\n OpenMP also has the reduction  clause for the parallel work-region directives. The\nsyntax is similar to that for the threaded OpenMP work directives and OpenACC. An\nexample of the directive is as follows:\n#pragma omp teams distribute parallel for simd reduction(+:sum)\n11.3.3 Creating data regions to control data movement \nto the GPU with OpenMP\nNow that we have the work moved over to the GPU, we can add data regions to man-\nage the data movement to and from the GPU. The data movement directives in\nOpenMP are similar to those in OpenACC with both a structured and dynamic ver-\nsion. The form of the directive is\n#pragma omp target data [ map() | use_device_ptr() ]\nThe work directives are wrapped in the structured data region as listing 11.15 shows.\nThe data is copied over to the GPU, if not already there. The data is then maintained\nthere until the end of the block (at line 35) and copied back. This greatly reduces the\ndata transfers for every parallel work loop and should result in a net speedup in the\noverall application run time.\n403 OpenMP: The heavyweight champ enters the world of accelerators\nOpenMP/StreamTriad/StreamTriad_par3.c\n17 #pragma omp target data map(to:a[0:nsize], \    \n                      b[0:nsize], c[0:nsize])      \n18    {                                            \n19 #pragma omp target teams distribute \    \n               parallel for simd            \n20       for (int i=0; i<nsize; i++) {\n21          a[i] = 1.0;\n22          b[i] = 2.0;\n23       }\n24 \n25       for (int k=0; k<ntimes; k++){\n26          cpu_timer_start(&tstart);\n27          // stream triad loop \n28 #pragma omp target teams distribute \    \n               parallel for simd            \n29          for (int i=0; i<nsize; i++){\n30             c[i] = a[i] + scalar*b[i];\n31          }\n32          time_sum += cpu_timer_stop(tstart);\n33       }\n34 \n35    }                                            \nStructured data regions cannot handle more general-programming patterns. Both\nOpenACC and OpenMP (version 4.5) added dynamic data regions, often referred\nto as unstructured  data regions. The form for the directive has enter  and exit  clauses\nwith a map modifier to specify the data transfer operation (such as the defaults to\nand from ):\n#pragma omp target enter data map([alloc | to]:array[[start]:[length]])\n#pragma omp target exit data map([from | release | delete]: \n                                  array[[start]:[length]])\nIn listing 11.16, we convert the omp target  data  directive to omp target  enter  data\ndirective (line 13). The scope of the data on the GPU concludes when it encounters an\nomp target  exit  data  directive (line 36). The effect of these directives is the same as the\nstructured data region in listing 11.15. But the dynamic data region can be used in more\ncomplex data management scenarios like constructors and destructors in C++.\nOpenMP/StreamTriad/StreamTriad_par4.c\n13 #pragma omp target enter data \                 \n      map(to:a[0:nsize], b[0:nsize], c[0:nsize])   \n14 \n15    struct timespec tstart;\n16    // initializing data and arrays\n17    double scalar = 3.0, time_sum = 0.0;Listing 11.15 Adding OpenMP pragmas to create a structured data region on the GPU\nListing 11.16 Using a dynamic OpenMP data regionStructured \ndata region \ndirectiveWork\nregion\ndirective\nStarts dynamic data \nregion directive\n404 CHAPTER  11 Directive-based GPU programming\n18 #pragma omp target teams distribute \     \n               parallel for simd             \n19    for (int i=0; i<nsize; i++) {\n20       a[i] = 1.0;\n21       b[i] = 2.0;\n22    }\n23 \n24    for (int k=0; k<ntimes; k++){\n25       cpu_timer_start(&tstart);\n26       // stream triad loop \n27 #pragma omp target teams distribute \     \n               parallel for simd             \n28       for (int i=0; i<nsize; i++){\n29          c[i] = a[i] + scalar*b[i];\n30       }\n31       time_sum += cpu_timer_stop(tstart);\n32    }\n33 \n34    printf(""Average runtime for stream triad loop is %lf msecs\n"",\n             time_sum/ntimes);\n35 \n36 #pragma omp target exit data \                   \n     map(from:a[0:nsize], b[0:nsize], c[0:nsize])   \nWe can further optimize the data transfers by allocating on the device and deleting\nthe arrays on exit from the data region, thereby eliminating another data transfer.\nWhen transfers are needed to move data back and forth from the CPU and the GPU,\nyou can use the omp target  update  directive. The syntax for the directive is \n#pragma omp target update [to | from] (array[start:length])\nWe should also recognize that in this example the CPU never uses the array memory.\nFor memory that only exists on the GPU, we can allocate it there and then tell the paral-\nlel work regions that it is already there. There are a couple of ways we can do this. One is\nto use an OpenMP function call to allocate and free memory on the device. These\ncalls look like the following and require the inclusion of the OpenMP header file:\n#include <omp.h>\ndouble *a = omp_target_alloc(nsize*sizeof(double), omp_get_default_device());\nomp_target_free(a, omp_get_default_device());\nWe could also use the CUDA memory allocation routines. We need to include the\nCUDA run-time header file to use these routines:\n#include <cuda_runtime.h>\ncudaMalloc((void *)&a,nsize*sizeof(double));\ncudaFree(a);\nOn the parallel work directives, we then need to add another clause to pass the device\npointers to the kernels on the device:\n#pragma omp target teams distribute parallel for is_device_ptr(a)Work \nregion \ndirective\nEnds dynamic data \nregion directive\n405 OpenMP: The heavyweight champ enters the world of accelerators\nPutting this all together, we end up with the changes to the code shown in the follow-\ning listing. \nOpenMP/StreamTriad/StreamTriad_par6.c\n11    double *a = omp_target_alloc(nsize*sizeof(double),\n                  omp_get_default_device());\n12    double *b = omp_target_alloc(nsize*sizeof(double),\n                  omp_get_default_device());\n13    double *c = omp_target_alloc (nsize*sizeof(double),\n                  omp_get_default_device());\n14 \n15    struct timespec tstart;\n16    // initializing data and arrays\n17    double scalar = 3.0, time_sum = 0.0;\n18 #pragma omp target teams distribute \n               parallel for simd is_device_ptr(a, b, c)\n19    for (int i=0; i<nsize; i++) {\n20       a[i] = 1.0;\n21       b[i] = 2.0;\n22    }\n23 \n24    for (int k=0; k<ntimes; k++){\n25       cpu_timer_start(&tstart);\n26       // stream triad loop \n27 #pragma omp target teams distribute \\n               parallel for simd is_device_ptr(a, b, c)\n28       for (int i=0; i<nsize; i++){\n29          c[i] = a[i] + scalar*b[i];\n30       }\n31       time_sum += cpu_timer_stop(tstart);\n32    }\n33 \n34    printf(""Average runtime for stream triad loop is %lf msecs\n"",\n             time_sum/ntimes);\n35 \n36    omp_target_free(a, omp_get_default_device());\n37    omp_target_free(b, omp_get_default_device());\n38    omp_target_free (c, omp_get_default_device());\nOpenMP has another way to allocate data on the device. This method uses the omp\ndeclare  target  directive as shown in listing 11.18. We first declare the pointers to the\narray on lines 10-12 and then allocate these on the device with the following block of\ncode (lines 14–19). A similar block is used on lines 42-47 for freeing the data on the\ndevice.\nOpenMP/StreamTriad/StreamTriad_par8.c\n10 #pragma omp declare target          \n11    double *a, *b, *c;               \n12 #pragma omp end declare target      Listing 11.17 Creating arrays only on the GPU\nListing 11.18 Using omp declare  to create arrays only on the GPU\nDeclares target \ncreates a pointer \non the device",8984
152-11.3.4 Optimizing OpenMP for GPUs.pdf,152-11.3.4 Optimizing OpenMP for GPUs,"406 CHAPTER  11 Directive-based GPU programming\n13 \n14 #pragma omp target                         \n15    {                                       \n16        a = malloc(nsize* sizeof(double);   \n17        b = malloc(nsize* sizeof(double);   \n18        c = malloc(nsize* sizeof(double);   \n19    }                                       \n     < unchanged code>\n42 #pragma omp target    \n43    {                  \n44        free(a);       \n45        free(b);       \n46        free(c);       \n47    }                  \nAs we have seen, there are a lot of different options for data management for the\nGPU. We now have covered the most common data region directives and clauses in\nOpenMP. Recent additions to the OpenMP standard handle more complicated data\nstructures and data transfers.\n11.3.4 Optimizing OpenMP for GPUs\nLet’s switch to a stencil example for the kernel optimization like we did for OpenACC.\nThere are a few things you can try for speeding up individual kernels, but for the most\npart, it is best to let the compiler do the optimization for portability reasons. The core\npart of the stencil kernel with the OpenMP data and work regions in the following list-\ning is the starting point for the optimization work.\nOpenMP/Stencil/Stencil_par2.c\n15    double** restrict x    = malloc2D(jmax, imax);\n16    double** restrict xnew = malloc2D(jmax, imax);\n17 \n18 #pragma omp target enter data \    \n      map(to:x[0:jmax][0:imax], \     \n             xnew[0:jmax][0:imax])    \n19 \n20 #pragma omp target teams                     \n21    {                                         \n22 #pragma omp distribute parallel for simd     \n23       for (int j = 0; j < jmax; j++){\n24          for (int i = 0; i < imax; i++){\n25             xnew[j][i] = 0.0;\n26             x[j][i]    = 5.0;\n27          }\n28       }\n29 \n30 #pragma omp distribute parallel for simd     \n31       for (int j = jmax/2 - 5; j < jmax/2 + 5; j++){\n32          for (int i = imax/2 - 5; i < imax/2 -1; i++){\n33             x[j][i] = 400.0;\n34          }Listing 11.19 Initial OpenMP version of stencilAllocates \ndata on the \ndevice\nFrees \ndevice \ndata\nOpenMP \ndata region\nParallel \nwork \ndirective\n407 OpenMP: The heavyweight champ enters the world of accelerators\n35       }\n36    } // omp target teams                         \n37 \n38    for (int iter = 0; iter < niter; iter+=nburst){\n39 \n40       for (int ib = 0; ib < nburst; ib++){\n41          cpu_timer_start(&tstart_cpu);\n42 #pragma omp target teams distribute \           \n               parallel for simd                   \n43          for (int j = 1; j < jmax-1; j++){       \n44             for (int i = 1; i < imax-1; i++){    \n45                xnew[j][i]=(x[j][i]+              \n                     x[j][i-1]+x[j][i+1]+           \n                     x[j-1][i]+x[j+1][i])/5.0;      \n46             }                                    \n47          }                                       \n48\n49 #pragma omp target teams distribute \           \n               parallel for simd                   \n50          for (int j = 0; j < jmax; j++){      \n51             for (int i = 0; i < imax; i++){   \n52                x[j][i] = xnew[j][i];          \n53             }                                 \n54          }                                    \n55          cpu_time += cpu_timer_stop(tstart_cpu);\n56 \n57       }\n58 \n59       printf(""Iter %d\n"",iter+nburst);\n60    }\n61 \n62 #pragma omp target exit data \     \n      map(from:x[0:jmax][0:imax], \   \n               xnew[0:jmax][0:imax])  \n63 \n64    free(x);\n65    free(xnew);\nSimply adding a single work directive for the 2D loop and the data construct is not\nenough to get the work efficiently generated for the GPU for version 16 of the IBM\nXL compiler. The run time is nearly twice as long as the serial version (see table 11.4\nat the end of this section). You can use nvprof to find where the time is being spent.\nHere’s the output:\n==11376== Profiling application: ./Stencil_par2\n==11376== Profiling result:\n Time(%)   Time  Calls    Avg       Min       Max     Name\n 51.63%  9.73622s 1000 9.7362ms  9.6602ms  15.378ms  __xl_main_l42_OL_3\n 48.26%  9.10010s 1000 9.1001ms  9.0323ms  13.588ms  __xl_main_l41_OL_2\n  0.11%  20.439ms    1 20.439ms  20.439ms  20.439ms  __xl_main_l18_OL_1\n  0.00%  7.2960us    5 1.4590us  1.2160us  2.1440us  [CUDA memcpy DtoH]\n  0.00%  5.3760us    2 2.6880us  2.5600us  2.8160us  [CUDA memcpy HtoD]\n   < more output>Parallel\nwork\ndirective\nStencil \nkernel\nReplaces swap \nwith copy from \nnew back to \noriginal\nOpenMP \ndata region\n408 CHAPTER  11 Directive-based GPU programming\nThe first line shows that the third kernel is taking up more than 50% of the run time.\nThe copy back to the original array is taking an additional 48% of the run time. It’s\nthe kernel code and not the data transfer that is causing the problem! To correct this,\nthe first thing to try is to collapse the two nested loops into a single parallel construct.\nThe changes for this include adding the collapse  clause along with the number of\nloops to collapse on the work directives. This is shown on lines 22, 30, 42, and 49 in\nthe next listing.\nOpenMP/Stencil/Stencil_par3.c\n20 #pragma omp target teams\n21    {\n22 #pragma omp distribute parallel \                \n               for simd collapse(2)                 \n23       for (int j = 0; j < jmax; j++){\n24          for (int i = 0; i < imax; i++){\n25             xnew[j][i] = 0.0;\n26             x[j][i]    = 5.0;\n27          }\n28       }\n29 \n30 #pragma omp distribute parallel \                \n               for simd collapse(2)                 \n31       for (int j = jmax/2 - 5; j < jmax/2 + 5; j++){\n32          for (int i = imax/2 - 5; i < imax/2 -1; i++){\n33             x[j][i] = 400.0;\n34          }\n35       }\n36    }\n37 \n38    for (int iter = 0; iter < niter; iter+=nburst){\n39 \n40       for (int ib = 0; ib < nburst; ib++){\n41          cpu_timer_start(&tstart_cpu);\n42 #pragma omp target teams distribute \            \n               parallel for simd collapse(2)        \n43          for (int j = 1; j < jmax-1; j++){\n44             for (int i = 1; i < imax-1; i++){\n45                xnew[j][i]=(x[j][i]+x[j][i-1]+x[j][i+1]+\n                                      x[j-1][i]+x[j+1][i])/5.0;\n46             }\n47          }\n48\n49 #pragma omp target teams distribute \            \n               parallel for simd collapse(2)        \n50          for (int j = 0; j < jmax; j++){\n51             for (int i = 0; i < imax; i++){\n52                x[j][i] = xnew[j][i];\n53             }\n54          }\n55          cpu_time += cpu_timer_stop(tstart_cpu);\n56 Listing 11.20 Using collapse  for optimization\nAdds\ncollapse\nclause\n409 OpenMP: The heavyweight champ enters the world of accelerators\n57       }\n58 \n59       printf(""Iter %d\n"",iter+nburst);\n60    }\nThe run time is now faster than the CPU (see table 11.3), though not as fast as the ver-\nsion generated by the PGI OpenACC compiler (table 11.1). We expect that as the IBM\nXL compiler improves, this should get better. Let’s try another approach of splitting\nthe parallel work directives across the two loops as shown in the following listing.\nOpenMP/Stencil/Stencil_par4.c\n20 #pragma omp target teams\n21    {\n22 #pragma omp distribute                         \n23       for (int j = 0; j < jmax; j++){\n24 #pragma omp parallel for simd                  \n25          for (int i = 0; i < imax; i++){\n26             xnew[j][i] = 0.0;\n27             x[j][i]    = 5.0;\n28          }\n29       }\n30 \n31 #pragma omp distribute                         \n32       for (int j = jmax/2 - 5; j < jmax/2 + 5; j++){\n33 #pragma omp parallel for simd                  \n34          for (int i = imax/2 - 5; i < imax/2 -1; i++){\n35             x[j][i] = 400.0;\n36          }\n37       }\n38    }\n39 \n40    for (int iter = 0; iter < niter; iter+=nburst){\n41 \n42       for (int ib = 0; ib < nburst; ib++){\n43          cpu_timer_start(&tstart_cpu);\n44 #pragma omp target teams distribute            \n45          for (int j = 1; j < jmax-1; j++){\n46 #pragma omp parallel for simd                  \n47             for (int i = 1; i < imax-1; i++){\n48                xnew[j][i]=(x[j][i]+x[j][i-1]+x[j][i+1]+\n                                      x[j-1][i]+x[j+1][i])/5.0;\n49             }\n50          }\n51\n52 #pragma omp target teams distribute            \n53          for (int j = 0; j < jmax; j++){\n54 #pragma omp parallel for simd                  \n55             for (int i = 0; i < imax; i++){\n56                x[j][i] = xnew[j][i];\n57             }\n58          }\n59          cpu_time += cpu_timer_stop(tstart_cpu);Listing 11.21 Splitting work directives for optimization\nSplits\nwork\nover\ntwo\nloop\nlevels\n410 CHAPTER  11 Directive-based GPU programming\n60 \n61       }\n62 \n63       printf(""Iter %d\n"",iter+nburst);\n64    }\nThe timing from the IBM XL compiler for the split parallel work directives is similar\nto the collapse  clause. Table 11.3 shows the results of our experiments with kernel\noptimizations.\nWe also look at the run time results for the stream triad example from the IBM XL\ncompiler v16 on a Power 9 processor with an NVIDIA V100 GPU in table 11.4. The\nperformance on the CPU is different because, in one case, we used an Intel Skylake\nprocessor and, in this case, we are using a Power 9 processor. But it is encouraging to\nsee that the performance of the stream kernel with OpenMP on the V100 GPU is\nessentially the same as that for the PGI OpenACC compiler in table 11.2.\nThe performance of OpenMP with the IBM XL compiler is good on a simple 1D test\nproblem but could be improved for the 2D stencil case. The focus thus far has been\non correctly implementing the OpenMP standard for device offloading. We expect\nthat performance will improve with each compiler release and with more compiler\nvendors offering OpenMP device offloading support.Table 11.3 Run times from OpenMP stencil kernel optimizations\nOpen MP stencil kernel run time (secs)\nSerial CPU code 5.497\nAdding work directive 19.01\nAdding compute and data regions 18.97\nAdding collapse(2)  clause 3.035\nSplitting parallel directives 2.50\nTable 11.4 Run times from OpenMP stream triad kernel optimizations\nOpenMP stream triad kernel run time (ms)\nSerial CPU code 15.9\nParallel 1. Compute region added 85.7\nParallel 3. Structured data region added 0.585\nParallel 4. Dynamic data region added 0.584\nParallel 8. Allocating data only on device 0.584",10661
153-11.3.5 Advanced OpenMP for GPUs.pdf,153-11.3.5 Advanced OpenMP for GPUs,"411 OpenMP: The heavyweight champ enters the world of accelerators\n11.3.5 Advanced OpenMP for GPUs\nOpenMP has many additional advanced capabilities. OpenMP is also changing based\non the experience with the early implementations on GPUs and as hardware contin-\nues to evolve. We’ll cover just a few of the advanced directives and clauses that are\nimportant for\nFine-tuning kernels\nHandling various important programming constructs (functions, scans, and\nshared access to variables)\nAsynchronous operations that overlap data movement and computation\nControlling memory placement\nHandling complex data structures\nSimplifying work directives\nCONTROLLING  THE GPU KERNEL  PARAMETERS  IMPLEMENTED  BY THE OPENMP COMPILER\nWe start by looking at clauses that can be used to fine-tune kernel performance. We\ncan add these clauses to directives to modify the kernels that the compiler generates\nfor the GPU: \nnum_teams  defines the number of teams generated by the teams  directive.\nthread_limit  adds the number of threads used by each team.\nschedule  or schedule(static,1)  specifies that the work items are distributed\nin a round-robin manner rather than in a block. This can help with memory\nload coalescing on the GPU.\nsimdlen  specifies the vector length or threads for the workgroup.\nThese clauses can be useful in special situations, but in general, it is better to leave the\nparameters for the compiler to optimize.\nDECLARING  AN OPENMP DEVICE  FUNCTION\nWhen we call a function within a parallel region on the device, we need a way to tell\nthe compiler it should also be on the device. This is done by adding a declare  target\ndirective to the function. The syntax is similar to that for variable declarations. Here is\nan example:\n#pragma omp declare target\nint my_compute(<args>){\n   <work>\n}\nNEW SCAN REDUCTION  TYPE\nWe discussed the importance of the scan algorithm in section 5.6, where we also saw\nthe complexity of implementing this algorithm on the GPU. This is a ubiquitous oper-\nation in parallel computing and complicated to write, so the addition of this type is\nhelpful. The scan  type will be available in version 5.0 of OpenMP. \n412 CHAPTER  11 Directive-based GPU programming\nint run_sum = 0;\n#pragma omp parallel for simd reduction(inscan,+: run_sum)\nfor (int i = 0; i < n; ++i) {\n   run_sum += ncells[i];\n   #pragma omp scan exclusive(run_sum)\n   cell_start[i] = run_sum;\n   #pragma omp scan inclusive(run_sum)\n   cell_end[i] = run_sum;\n} \nPREVENTING  RACE CONDITIONS  WITH OPENMP A TOMIC\nIt is normal in an algorithm that several threads access a common variable. It is often a\nbottleneck in the performance of routines. Atomics have provided this functionality\nin various compilers and thread implementations. OpenMP also provides an atomic\ndirective. An example of the use of the directive is\n#pragma omp atomic\n   i++;\nOPENMP’ S VERSION  OF ASYNCHRONOUS  OPERATIONS\nIn section 10.5, we discussed the value of overlapping data transfer and computa-\ntion through asynchronous operations. OpenMP also provides its version of these\noperations.\n You create asynchronous device operations using the nowait  clause on either a\ndata or work directive. You can then use a depend  clause to specify that a new opera-\ntion cannot start until the previous operation is complete. These operations can be\nchained to form a sequence of operations. We can use a simple taskwait  directive to\nwait for completion of all tasks:\n#pragma omp taskwait\nACCESSING  SPECIAL  MEMORY  SPACES\nMemory bandwidth is often one of the most important performance limits. With\npragma-based languages, it has not always been possible to control the placement of\nmemory and the resulting memory bandwidth. The addition of features to give the\nprogrammer more control over this has been one of the more eagerly anticipated\nadditions to OpenMP. With OpenMP 5.0, you will be able to target special memory\nspaces such as shared memory and high-bandwidth memory. The capability is through\na new allocator  clause modifier. The allocate  clause takes an optional modifier\nas follows:\nallocate([allocator:] list)\nYou can use the following pair of functions to directly allocate and free memory:\nomp_alloc(size_t size, omp_allocator_t *allocator)\nomp_free(void *ptr, const omp_allocator_t *allocator)\n413 OpenMP: The heavyweight champ enters the world of accelerators\nThe OpenMP 5.0 standard specifies some predefined memory spaces for allocators as\nthis table shows.\nA set of functions is available to define new memory allocators. The two main rou-\ntines are\nomp_init_allocator\nomp_destroy_allocator\nThese allocators take one of the predefined space arguments and allocator traits such\nas whether it should be pinned, aligned, private, nearby, or many others. Implementa-\ntions of this capability are still under development. This functionality will be of increas-\ning importance with new architectures, where there are special memory types with\ndifferent latency and bandwidth performance characteristics.\nDEEP COPY SUPPORT  FOR TRANSFERRING  COMPLEX  DATA STRUCTURES\nOpenMP 5.0 also adds a declare  mapper  construct that can do deep copies. Deep copies\nnot only duplicate a data structure with pointers but also the data referred to by\nthe pointers. Programs with complex data structures and classes have struggled\nwith the difficulty of porting to GPUs. The ability to do deep copies greatly simplifies\nthese implementations.\nSIMPLIFYING  WORK  DISTRIBUTION  WITH THE NEW LOOP DIRECTIVE\nThe OpenMP 5.0 standard introduces more flexible work directives. One of these is\nthe loop  directive that is simpler and closer to the functionality in OpenACC. The\nloop  directive takes the place of distribute  parallel  for simd . With the loop  direc-\ntive, you are telling the compiler that the loop iterations can be executed concur-\nrently, but you leave the actual implementation to the compiler. The following listing\nshows an example of using this directive in the stencil kernel. \n47 #pragma omp target teams     \n48 #pragma omp loop                        \n49          for (int j = 1; j < jmax-1; j++){\n50 #pragma omp loop                       Memory space Memory type description\nomp_default_mem_alloc/omp_default_mem_space Default system storage space\nomp_large_cap_mem_alloc/omp_large_cap_mem_space Large-capacity storage space\nomp_const_mem_alloc/omp_const_mem_space Storage for constant, unchanging data\nomp_high_bw_mem_alloc/omp_high_bw_mem_space High bandwidth memory\nomp_low_lat_mem_alloc/omp_low_lat_mem_space Storage with low latency\nListing 11.22 Using the new loop  directive in OpenMP 5.0\nLaunches work on the \nGPU with multiple teams\nThe loop parallelized \nas independent work",6799
154-11.4 Further explorations.pdf,154-11.4 Further explorations,,0
155-11.4.1 Additional reading.pdf,155-11.4.1 Additional reading,"414 CHAPTER  11 Directive-based GPU programming\n51             for (int i = 1; i < imax-1; i++){\n52                xnew[j][i]=(x[j][i]+x[j][i-1]+x[j][i+1]+\n                                      x[j-1][i]+x[j+1][i])/5.0;\n53             }\n54          }\nThe loop  clause is really a loop independent  or concurrent  clause that tells the com-\npiler that iterations of the loop have no dependencies. The loop  clause gives the com-\npiler information or a descriptive clause rather than telling the compiler what to do,\nwhich is a prescriptive clause. Most compilers have not implemented this new feature,\nso we continue to work with the prescriptive clauses in the earlier examples in this\nchapter. If you’re not familiar with these concepts, here’s a definition of each:\nPrescriptive directives and clauses —Directives from the programmer that tell the\ncompiler specifically what to do.\nDescriptive directives and clauses —Directives that give the compiler information\nabout the following loop construct; also gives the compiler some freedom to\ngenerate the most efficient implementation.\nOpenMP has traditionally used prescriptive clauses in its specifications. This reduces\nthe variation between implementations and improves portability. But in the case of\nGPUs, it has led to the long, complex directives with subtle differentiation on whether\nsynchronization is possible between threads and other hardware-specific features. \n The descriptive approach is closer to the OpenACC philosophy and is not so bur-\ndened with the details of the hardware. This gives the compiler both the freedom and\nresponsibility of how to properly and effectively generate code for the targeted hard-\nware. Note that this is not only a significant shift for OpenMP, but an important one. If\nOpenMP continues to try and go down the path of prescriptive directives, as hardware\ncomplexity continues to grow, the OpenMP language will grow too complicated and\nthe portability of codes will be reduced.\n11.4 Further explorations\nBoth OpenACC and OpenMP are large languages with many directives, clauses, modi-\nfiers, and functions. Beyond the core functionality of these languages, there are few\nexamples and sparse documentation. Indeed, many of the lesser used parts may not\nwork in all compilers. You should test new functionality in a small example before\nadding it to a large application. To learn more about these languages, refer to the\nadditional reading materials that follow. Also, be sure and get some hands-on experi-\nence with the exercises in section 11.4.2.\n11.4.1 Additional reading\nBecause the OpenACC and OpenMP languages are still evolving, the best sources for\nadditional materials are at the respective websites: https:/ /openacc.org  and https:/ /\nopenmp.org . Each site lists additional resources, including tutorials and presentations\nat leading HPC conferences.\n415 Further explorations\nOPENACC RESOURCES  AND REFERENCES\nOpenACC has been out a little longer than OpenMP and has more books and docu-\nmentation. The starting place for the language is the OpenACC standard. At 150\npages, version 3.0 of the standard is very readable and relevant to the end user. It can\nbe found on the openacc.org website. The following URL provides a link to The\nOpenACC Application Programming Interface , v3.0 (November, 2018): \nhttps:/ /www.openacc.org/sites/default/files/inline-images/Specification/\nOpenACC.3.0.pdf.\nThe OpenACC site also has a document on programming and best practices. It is not\nlinked to a particular version of the standard, but has not been updated since 2015.\nYou’ll find OpenACC-standard.org’s OpenACC Programming and Best Practices Guide\n(June, 2015) here: \nhttps:/ /www.openacc.org/sites/default/files/inline-files/OpenACC_Programming_\nGuide_0.pdf .\nThe leading book for OpenACC is \nSunita Chandrasekaran and Guido Juckeland, OpenACC for Programmers: Concepts\nand Strategies  (Addison-Wesley Professional, 2017).\nOPENMP RESOURCES  AND REFERENCES\nMost of the books and guides to OpenMP predate device offloading capabilities, but\nthe language specification thoroughly describes the OpenMP device offloading direc-\ntives. At over 600 pages, it is more of a reference than a user’s guide. Still, it is the go-\nto document for details on the features of the language.2\nOpenMP Architecture Review Board, OpenMP Application Programming Interface ,\nVol. 5.0 (November, 2018) at https:/ /www.openmp.org/wp-content/uploads/Open\nMP-API-Specification-5.0.pdf .\nA companion to the specification is the example guide. This guide gives short exam-\nples of how each feature should work, but not complete application-level cases:\nOpenMP Architecture Review Board, OpenMP Application Programming Interface: Exam-\nples, Vol. 5.0 (November, 2019) at https:/ /www.openmp.org/wp-content/uploads/\nopenmp-examples-5.0.0.pdf .\nWith OpenMP still seeing significant changes and compilers still working on imple-\nmenting v5.0 features, it is not surprising that there are few books that discuss the\ndevice offloading features. Ruud van der Pas and others recently completed a book\nthat covers the new features of OpenMP up through v4.5.\n2To get a relative comparison for the complexity of these pragma-based languages, the final draft of the C18\nstandard for the C language is just over 500 pages.",5362
156-11.4.2 Exercises.pdf,156-11.4.2 Exercises,,0
157-12 GPU languages Getting down to basics.pdf,157-12 GPU languages Getting down to basics,"416 CHAPTER  11 Directive-based GPU programming\nRuud Van der Pas, Eric Stotzer, and Christian Terboven, Using OpenMP—The Next\nStep: Affinity, Accelerators, Tasking, and SIMD  (MIT Press, 2017).\n11.4.2 Exercises\n1Find what compilers are available for your local GPU system. Are both OpenACC\nand OpenMP compilers available? If not, do you have access to any systems that\nwould allow you to try out these pragma-based languages?\n2Run the stream triad examples from the OpenACC/StreamTriad and/or the\nOpenMP/StreamTriad directories on your local GPU development system. You’ll\nfind these directories at https:/ /github.com/EssentialsofParallelComputing/\nChapter11 .\n3Compare your results from exercise 2 to the BabelStream results at https:/ /\nuob-hpc.github.io/BabelStream/results/ . For the stream triad, the bytes moved\nare 3 * nsize  * sizeof(datatype) .\n4Modify the OpenMP data region mapping in listing 11.16 to reflect the actual\nuse of the arrays in the kernels.\n5Implement the mass sum example from listing 11.4 in OpenMP.\n6For x and y arrays of size 20,000,000, find the maximum radius for the arrays\nusing both OpenMP and OpenACC. Initialize the arrays with double-precision\nvalues that linearly increase from 1.0 to 2.0e7 for the x array and decrease from\n2.0e7 to 1.0 for the y array.\nSummary\nPragma-based languages are the easiest way to port to the GPU. Using these\ngives you the quickest result with the least effort.\nThe porting process is to move work to the GPU and then manage the data\nmovement. This gets as much work as possible on the GPU while minimizing\nexpensive data movement.\nThe kernel optimization comes last and should mostly be left to the compiler.\nThis produces the most portable and future-proof code.\nTrack the latest developments of the pragma-based language and compilers. These\ncompilers are still under rapid development and should continue improving.\n417GPU languages:\nGetting down to basics\nThis chapter covers lower-level languages for GPUs. We call these native languages\nbecause they directly reflect features of the target GPU hardware. We cover two of\nthese languages, CUDA and OpenCL, that are widely used. We also cover HIP, a\nnew variant for AMD GPUs. In contrast to the pragma-based implementation, these\nGPU languages have a smaller reliance on the compiler. You should use these lan-\nguages for more fine-tuned control of your program’s performance. How are these\nlanguages different than those presented in chapter 11? Our distinction is that\nthese languages have grown up from the characteristics of the GPU and CPU hard-\nware, while the OpenACC and OpenMP languages started with high-level abstrac-\ntions and rely on a compiler to map those to different hardware.\n The set of native GPU languages, CUDA, OpenCL, and HIP, requires a separate\nsource to be created for the GPU kernel. The separate source code is often similarThis chapter covers\nUnderstanding the current landscape of native \nGPU languages\nCreating simple GPU programs in each language\nTackling more complex multi-kernel operations\nPorting between various GPU languages\n418 CHAPTER  12 GPU languages: Getting down to basics\nto the CPU code. The challenges of having two different sources to maintain is a\nmajor difficulty. If the native GPU language only supports one type of hardware, then\nthere can be even more source variants to maintain if you want to run on more than\none vendor’s GPU. Some applications have implemented their algorithms in multiple\nGPU languages and CPU languages. Thus, you can understand the critical need for\nmore portable GPU programming languages. \n Thankfully, portability is getting more attention with some of the newer GPU lan-\nguages. OpenCL was the first open-standard language to run on a variety of GPU\nhardware and even CPUs. After an initial splash, OpenCL has not gotten as wide-\nspread an acceptance as originally hoped for. Another language, HIP, is designed by\nAMD as a more portable version of CUDA, which generates code for AMD’s GPUs. As\npart of AMD’s portability initiative, support for GPUs from other vendors is included.\n The difference between these native languages and higher-level languages is\nblurring as new languages are introduced. The SYCL language, originally a C++\nlayer on top of OpenCL, is typical of these newer, more portable languages. Along\nwith the Kokkos and RAJA languages, SYCL supports a single source for both CPU\nand GPU. We’ll touch on these languages at the end of the chapter. Figure 12.1 shows\nthe current picture of the interoperability for the GPU languages that we cover in\nthis chapter.\nThe focus on language interoperability is gaining traction as more diversity of GPUs\nappears in the largest HPC installations. The top Department of Energy HPC systems,\nSierra and Summit, are provisioned with NVIDIA GPUs. In 2021, Argonne’s Aurora\nsystem with Intel GPUs and Oak Ridge’s Frontier system with AMD GPUs will be\nadded to the list of Department of Energy HPC systems. With the introduction of the\nAurora system, SYCL has emerged from near obscurity to become a major player withNVIDIA\nGPUsAMD\nGPUsIntel\nGPUsIntel\nCPUsCUDA OpenCLHIP\nRequires PCI 3SYCL\nFigure 12.1 The interoperability map for the GPU languages shows \nan increasingly complex situation. Four GPU languages are shown \nat the top with the various hardware devices at the bottom. The \narrows show the code generation pathways from the languages to \nthe hardware. The dashed lines are for hardware that is still in \ndevelopment.",5580
158-12.2.1 Writing and building your first CUDA application.pdf,158-12.2.1 Writing and building your first CUDA application,"419 Features of a native GPU programming language\nmultiple implementations. SYCL was originally developed to provide a more natural\nC++ layer on top of OpenCL. The reason for the sudden emergence of SYCL was its\nadoption by Intel as part of the OneAPI programming model for Intel GPUs on the\nAurora system. Because of SYCL’s new-found importance, we cover SYCL in section 12.4.\nA similar growth in interest in other languages and libraries that provide portability\nacross the GPU landscape is also prevalent.\n We end the chapter with a brief look at a couple of these performance portability\nsystems, Kokkos and RAJA, that were created to ease the difficulty of running on a\nwide range of hardware, from CPUs to GPUs. These work at a slightly higher level of\nabstraction, but promise a single source that will run everywhere. Their development\nhas resulted from a major Department of Energy effort to support the porting of large\nscientific applications to newer hardware. The aim of RAJA and Kokkos is a one-time\nrewrite to create a single-source code base that is portable and maintainable through\na time of great change in hardware design.\n Last, we want to provide guidance on how to approach this chapter. We cover a lot\nof different languages in a short space. The proliferation of languages reflects the lack\nof cooperation among language developers at this point in time, as developers chase\ntheir immediate goals and hardware concerns. Rather than treat these languages as\ndifferent languages, think of them as slightly different dialects of one or two lan-\nguages. We recommend that you seek to learn a couple of these languages and appre-\nciate the differences and similarities with the others. We will be comparing and\ncontrasting the languages to help you see that they are not all that different once you\nget over the particular syntax of each and their quirks. We do expect that the lan-\nguages will merge to a more common form because the current situation is not sus-\ntainable. We already see the beginnings of that with the push for more language\nportability driven by the needs of large applications.\n12.1 Features of a native GPU programming language\nA GPU programming language must have several basic features. It is helpful to under-\nstand what these features are so that you can recognize these in each GPU language.\nWe summarize the necessary GPU language features here.\nDetecting the accelerator device —The language must provide a detection of the\naccelerator devices and a way to choose between those devices. Some languages\ngive more control over the selection of devices than others. Even for a language\nsuch as CUDA, which just looks for an NVIDIA GPU, there must be a way to\nhandle multiple GPUs on a node.\nSupport for writing device kernels —The language must provide a way to generate\nthe low-level instructions for GPUs or other accelerators. GPUs provide nearly\nidentical basic operations as CPUs, so the kernel language should not be dra-\nmatically different. Rather than invent a new language, the most straightfor-\nward way is to leverage current programming languages and compilers to\ngenerate the new instruction set. GPU languages have done this by adopting a\n420 CHAPTER  12 GPU languages: Getting down to basics\nparticular version of the C or C++ language as a basis for their system. CUDA\noriginally was based on the C programming language but now is based on C++\nand has some support for the Standard Template Library (STL). OpenCL is\nbased on the C99 standard and has released a new specification with C++ support.\nThe language design also needs to address whether to have the host and\ndesign source code in the same file or in different files. Either way, the compiler\nmust distinguish between the host and design sources and must provide a way\nto generate the instruction set for the different hardware. The compiler must\neven decide when  to generate the instruction set. For example, OpenCL waits\nfor the device to be selected and then generates the instruction set with a just-\nin-time (JIT) compiler approach.\nMechanism to call device kernels from the host —Ok, now we have the device code,\nbut we also have to have a way of calling the code from the host. The syntax for\nperforming this operation varies the most across the various languages. But the\nmechanism is only slightly more complicated than a standard subroutine call.\nMemory handling —The language must have support for memory allocations,\ndeallocations, and moving data back and forth from the host to the device. The\nmost straightforward way for this is to have a subroutine call for each of these\noperations. But another way is through the compiler detecting when to move\nthe data and doing it for you behind the scenes. As this is such a major part of\nGPU programming, innovation continues to occur on the hardware and soft-\nware side for this functionality.\nSynchronization —A mechanism must be provided to specify the synchronization\nrequirements between the CPU and the GPU. Synchronization operations must\nalso be provided within kernels.\nStreams —A complete GPU language allows the scheduling of asynchronous\nstreams of operations along with the explicit dependencies between the kernels\nand the memory transfer operations.\nThis is not such a scary list. For the most part, native GPU languages do not look so\ndifferent than current CPU code. Also recognizing these commonalities among native\nGPU language functionality helps you to become comfortable moving from one lan-\nguage to another.\n12.2 CUDA and HIP GPU languages: \nThe low-level performance option\nWe will begin with a look at two of the low level GPU languages, CUDA and HIP.\nThese are two of the most common languages for programming GPUs. \n Compute Unified Device Architecture (CUDA) is a proprietary language from\nNVIDIA that only runs on their GPUs. First released in 2008, it is currently the domi-\nnant native programming language for GPUs. With a decade of development, CUDA\nhas a rich set of features and performance enhancements. The CUDA language closely\n421 CUDA and HIP GPU languages: The low-level performance option\nreflects the architecture of the NVIDIA GPU. It does not purport to be a general\naccelerator language. Still, the concepts of most accelerators are similar enough for\nthe CUDA language design to be applicable.\n The AMD (formerly ATI) GPUs have had a series of short-lived programming lan-\nguages. These have finally settled on a CUDA look-a-like that can be generated by\n“HIPifying” CUDA code with their HIP compiler. This is part of the ROCm suite of\ntools that provide extensive portability between GPU languages, including the OpenCL\nlanguage for GPUs (and CPUs) discussed in section 12.3.\n12.2.1 Writing and building your first CUDA application\nWe’ll start with how to build and compile a simple CUDA application that runs on a\nGPU. We’ll use the stream triad example we have used throughout the book that\nimplements a loop for this calculation: C = A + scalar  * B. The CUDA compiler splits\nthe regular C++ code to pass to the underlying C++ compiler. It then compiles the\nremaining CUDA code. Code from these two paths is linked together into a single\nexecutable. \n To follow along with this example, you might first need to install the CUDA soft-\nware.1 Each release of CUDA works with a limited range of compiler versions. As of\nCUDA v10.2, GCC compilers up through v8 are supported. If you are working with\nmultiple parallel languages and packages, this constantly-battling-the-compiler-version\nissue is perhaps one of the most frustrating things about CUDA. But on a positive\nnote, you can use much of your regular toolchain and build systems with just the ver-\nsion constraints and a few special additions. \n We’ll show three different approaches starting with a simple makefile and then\na couple of different ways of using CMake. We encourage you to follow along with\nthe examples for this chapter at https:/ /github.com/EssentialsofParallelComputing/\nChapter12 .\n You can select this simple makefile for CUDA by copying or linking it to Makefile,\nthe default filename for make. The following listing shows the makefile itself. \n1To link to the file, type ln -s Makefile.simple  Makefile\n2Build the application with make\n3Run the application with ./StreamTriad\nCUDA/StreamTriad/Makefile.simple\n 1 all: StreamTriad\n 2 \n 3 NVCC = nvcc          \n 4 #NVCC_FLAGS = -arch=sm_30     \n 5 #CUDA_LIB = <path>            \n 6 CUDA_LIB=`which nvcc | sed -e 's!/bin/nvcc!!'`/lib\n1See the CUDA installation guide for details ( https:/ /docs.nvidia.com/cuda/cuda-installation-guide-linux/ ).Listing 12.1 A simple CUDA makefile\nSpecifies NVIDIA \nCUDA compiler You may need to set \nlibrary path and GPU \narchitecture type here.\n422 CHAPTER  12 GPU languages: Getting down to basics\n 7 CUDA_LIB64=`which nvcc | sed -e 's!/bin/nvcc!!'`/lib64\n 8 \n 9 %.o : %.cu                              \n10   ${NVCC} ${NVCC_FLAGS} -c $< -o $@     \n11 \n12 StreamTriad: StreamTriad.o timer.o\n13   ${CXX} -o $@ $^ -L${CUDA_LIB} -lcudart     \n14 \n15 clean:\n16   rm -rf StreamTriad *.o\nThe key addition is a pattern rule on lines 9–10, which converts a file with a .cu suffix\ninto an object file. We use the NVIDIA NVCC compiler for this operation. We then\nneed to add the CUDA runtime library, CUDART, to the link line. You can use lines 4\nand 5 to specify a particular NVIDIA GPU architecture and a special path to the\nCUDA libraries. \nDEFINITION A pattern rule  is a specification to the make utility that provides a\ngeneral rule on how to convert any file with one suffix pattern to a file with\nanother suffix pattern.\nCUDA has extensive support in the CMake build system. Next, we cover both the old-\nstyle support and the new modern CMake approach that’s recently emerged. We show\nthe old-style method in listing 12.2. It has the advantage of more portability for sys-\ntems with older CMake versions and the automatic detection of the NVIDIA GPU\narchitecture. This latter feature of detecting the hardware device is such a conve-\nnience that the old-style CMake is the recommended approach at present. To use this\nbuild system, link the CMakeLists_old.txt to CMakeLists.txt:\nln -s CMakeLists_old.txt CMakeLists.txt\nmkdir build && cd build\ncmake ..\nmake\nCUDA/StreamTriad/CMakeLists_old.txt\n 1 cmake_minimum_required (VERSION 2.8)     \n 2 project (StreamTriad)\n 3 \n 4 find_package(CUDA REQUIRED)     \n 5 \n 6 set (CMAKE_CXX_STANDARD 11)\n 7 set (CMAKE_CUDA_STANDARD 11)\n 8 \n 9 # sets CMAKE_{C,CXX}_FLAGS from CUDA compile flags. \n   # Includes DEBUG and RELEASE\n10 set (CUDA_PROPAGATE_HOST_FLAGS ON) # default is on\n11 set (CUDA_SEPARABLE_COMPILATION ON)     \n12 \n13 if (CMAKE_VERSION VERSION_GREATER ""3.9.0"")Listing 12.2 Old style CUDA CMake fileImplicit rule to compile \nCUDA source files\nLink line \nfor CUDA \napplications\nYou need a minimum of \nCMake v2.8 for CUDA support.\nTraditional CMake module \nsets compiler flags.\nSet to “on” for calling \nfunctions in other compile \nunits (default off)\n423 CUDA and HIP GPU languages: The low-level performance option\n14    cuda_select_nvcc_arch_flags(ARCH_FLAGS)    \n15 endif()\n16 \n17 set (CUDA_NVCC_FLAGS ${CUDA_NVCC_FLAGS}    \n        -O3 ${ARCH_FLAGS})                    \n18 \n19 # Adds build target of StreamTriad with source code files\n20 cuda_add_executable(StreamTriad       \n      StreamTriad.cu timer.c timer.h)    \n21 \n22 if (APPLE)\n23    set_property(TARGET StreamTriad PROPERTY BUILD_RPATH\n        ${CMAKE_CUDA_IMPLICIT_LINK_DIRECTORIES})\n24 endif (APPLE)\n25\n26 # Cleanup\n27 add_custom_target(distclean COMMAND rm -rf CMakeCache.txt CMakeFiles\n28                   Makefile cmake_install.cmake \n                     StreamTriad.dSYM ipo_out.optrpt)\n29 \n30 # Adds a make clean_cuda_depends target\n   #    -- invoke with ""make clean_cuda_depends""\n31 CUDA_BUILD_CLEAN_TARGET()\nMuch of the CMake build system is standard. The separable compilation attribute on\nline 11 is suggested for a more robust build system for general development. You can\nthen turn it off at a later stage to save a few registers in the CUDA kernels to get a\nsmall optimization in the generated code. The CUDA defaults are for performance,\nnot for a more general, robust build. The automatic detection of the NVIDIA GPU\narchitecture on line 14 is a significant convenience that keeps you from having to\nmanually modify your makefile.\n With version 3.0, CMake is undergoing a fairly major revision to its structure to\nwhat they call “modern” CMake. The key attributes of this style are a more integrated\nsystem and a per target application of attributes. Nowhere is it more apparent than in\nits support of CUDA. Let’s take a look at the listing 12.3 to see how to use it. To use\nthis build system for the modern, new style CMake support for CUDA, link the\nCMakeLists_new.txt to CMakeLists.txt: \nln -s CMakeLists_new.txt CMakeLists.txt\nmkdir build && cd build\ncmake ..\nmake\nCUDA/StreamTriad/CMakeLists_new.txt\n  1 cmake_minimum_required (VERSION 3.8)     \n 2 project (StreamTriad)\n 3 \n 4 enable_language(CXX CUDA)    \n 5 Listing 12.3 New style (modern) CUDA CMake fileDetects and sets proper \narchitecture flag for \ncurrent NVIDIA GPU\nSets the compiler flags for \nthe NVIDIA compiler\nSets the proper build \nand link flags for a \nCUDA executable\nRequires \nCMake v3.8\nEnabes CUDA as \nthe language\n424 CHAPTER  12 GPU languages: Getting down to basics\n 6 set (CMAKE_CXX_STANDARD 11)\n 7 set (CMAKE_CUDA_STANDARD 11)\n 8\n 9 #set (ARCH_FLAGS -arch=sm_30)     \n10 set (CMAKE_CUDA_FLAGS ${CMAKE_CUDA_FLAGS};    \n        ""-O3 ${ARCH_FLAGS}"")                     \n11 \n12 # Adds build target of StreamTriad with source code files\n13 add_executable(StreamTriad StreamTriad.cu timer.c timer.h)\n14 \n15 set_target_properties(StreamTriad PROPERTIES    \n      CUDA_SEPARABLE_COMPILATION ON)               \n16 \n17 if (APPLE)\n18     set_property(TARGET StreamTriad PROPERTY BUILD_RPATH\n          ${CMAKE_CUDA_IMPLICIT_LINK_DIRECTORIES})\n19 endif(APPLE)\n20 \n21 # Cleanup\n22 add_custom_target(distclean COMMAND rm -rf CMakeCache.txt CMakeFiles\n23                   Makefile cmake_install.cmake \n                     StreamTriad.dSYM ipo_out.optrpt)\nThe first thing to note with this modern CMake approach is how much simpler it is\nthan the old style. The key is the enabling of the CUDA as the language in line 4.\nFrom then on, little additional work needs to be done. \n We can set the flags to compile for a specific GPU architecture as shown in lines 9–10.\nHowever, we don’t have an automatic way to detect the architecture yet with the mod-\nern CMake style. Without an architecture flag, the compiler generates code and opti-\nmizes for the sm_30 GPU device. The sm_30 generated code runs on any device from\nKepler K40 or newer, but it will not be optimized for the latest architectures. You can\nalso specify multiple architectures in one compiler. Compiles will be slower, and the\ngenerated executable will be larger. \n We can also set the separable compilation attribute for CUDA, but in a different\nsyntax in which it applies to the specific target. The optimization flag on line 10, -O3,\nis only sent to the host compiler for the regular C++ code. The default optimization\nlevel for CUDA code is -O3 and seldom needs to be modified.\n Overall, the process of building a CUDA program is easy and getting easier.\nExpect changes to the build to continue, however. Clang is adding native support\nfor compiling CUDA code to give you another option besides the NVIDIA compiler.\nNow let’s move on to the source code. We’ll begin with the kernel for the GPU in\nthe following listing. \nCUDA/StreamTriad/StreamTriad.cu\n 2 __global__ void StreamTriad(\n 3                const int n,\n 4                const double scalar,Listing 12.4 CUDA version of stream triad: The kernelManually sets the \nCUDA architecture\nSets the compile \nflags for CUDA\nSets the separable \ncompilation flag\n425 CUDA and HIP GPU languages: The low-level performance option\n 5                const double *a,\n 6                const double *b,\n 7                      double *c)\n 8 {\n 9    int i = blockIdx.x*blockDim.x+threadIdx.x;    \n10 \n11    // Protect from going out-of-bounds\n12    if (i >= n) return;               \n13 \n14    c[i] = a[i] + scalar*b[i];    \n15 }\nAs is typical with GPU kernels, we strip the for loop from the computational block.\nThis leaves the loop body on line 14. We need to add the conditional at line 12 to pre-\nvent accessing out-of-bounds data. Without this protection, kernels can randomly\ncrash without a message. And then, in line 9, we get the global index from the block\nand thread variables set by the CUDA run time. Adding the __global__  attribute to\nthe subroutine tells the compiler that this is a GPU kernel that will be called from the\nhost. Meanwhile on the host side, we have to set up the memory and make the kernel\ncall. The following listing shows this process.\nCUDA/StreamTriad/StreamTriad.cu\n31    // allocate host memory and initialize\n32    double *a = (double *)malloc(                   \n                  stream_array_size*sizeof(double));  \n33    double *b = (double *)malloc(                   \n                  stream_array_size*sizeof(double));  \n34    double *c = (double *)malloc(                   \n                  stream_array_size*sizeof(double));  \n35 \n36    for (int i=0; i<stream_array_size; i++) {\n37       a[i] = 1.0;                          \n38       b[i] = 2.0;                          \n39    }\n40 \n41    // allocate device memory. suffix of _d indicates a device pointer\n42    double *a_d, *b_d, *c_d;\n43    cudaMalloc(&a_d, stream_array_size*    \n                       sizeof(double));      \n44    cudaMalloc(&b_d, stream_array_size*    \n                       sizeof(double));      \n45    cudaMalloc(&c_d, stream_array_size*    \n                       sizeof(double));      \n46 \n47    // setting block size and padding total grid size\n      //    to get even block sizes\n48    int blocksize = 512;                      \n49    int gridsize =                            \n         (stream_array_size + blocksize - 1)/   \n          blocksize;                            Listing 12.5 CUDA version of stream triad: Set up and tear downGets cell index\nProtects from going \nout-of-bounds\nstream triad body\nAllocates host \nmemory\nInitializes arrays\nAllocates \ndevice memory\nSets block size and \ncalculates number \nof blocks\n426 CHAPTER  12 GPU languages: Getting down to basics\n50\n       < ... timing loop ... code shown below in listing 12.6 > \n78    printf(""Average runtime is %lf msecs data transfer is %lf msecs\n"",\n79            tkernel_sum/NTIMES, (ttotal_sum - tkernel_sum)/NTIMES);\n80 \n81    cudaFree(a_d);    \n82    cudaFree(b_d);    \n83    cudaFree(c_d);    \n84 \n85    free(a);    \n86    free(b);    \n87    free(c);    \n88 }\nFirst, we allocate memory on the host and initialize it on lines 31–39. We also need a\ncorresponding memory space on the GPU to hold the arrays while the GPU is operat-\ning on those. For that, we use the cudaMalloc  routine on lines 43–45. Now we come to\nsome interesting lines (from 47–49) that are needed solely for the GPU. The block\nsize is the size of the workgroup on the GPU. This is known by the tile size, block size,\nor workgroup size, depending on the GPU programming language being used (see\ntable 10.1). The next line that calculates the grid size is characteristic of GPU code.\nWe won’t always have an array size that is an even integer multiple of the block size.\nSo, we need to have an integer that is equal to or greater than the fractional number\nof blocks. Let’s work through an example to understand what is being done.\nExample: Calculating block size for the GPU\nOn line 3 in the following listing, we calculate the fractional number of blocks. For this\nexample with an array size of 1,000, it is 1.95 blocks. Rather than truncate this to\n1, which is what would happen with the default application of integer arithmetic, we\nneed to round up to 2. If we just calculated array size divided by the block size, we\nwould get integer truncation. So we have to cast each of these to a floating-point\nvalue to get floating-point division. We actually only need to cast one of the values,\nand the C/C++ standard requires the compiler to promote the other items. But in our\nprogramming conventions, a type conversion must be explicitly called for or it is a pro-\ngramming error. Compilers often don’t flag these cases, but they can mask unin-\ntended situations. \nThe C ceil function used on lines 4 and 5 in the listing rounds up to the next integer\nvalue equal to or greater than the floating-point number. We can get the same result\nwith integer arithmetic by adding one less than the block size and then performing\ninteger division with truncation as is done on line 6. We choose to use this version\nbecause the integer form does not require any floating-point operations and should\nbe faster. \n1 int stream_array_size = 1000\n2 int blocksize = 512\n3 float frac_blocks = (float)stream_array_size/(float)blocksize;Frees device \nmemory\nFrees host \nmemory\n427 CUDA and HIP GPU languages: The low-level performance option\nNow all the blocks but the last one have 512 values. The last block will be size 512, but\nwill contain only 488 data items. The out-of-bounds check on line 12 of listing 12.4\nkeeps us from getting in trouble with this partially filled block. The last few lines in\nlisting 12.5 free the device pointers and the host pointers. You must be careful to use\ncudaFree  for the device pointers and the C library function, free , for host pointers.\n All we have left is to copy memory to the GPU, call the GPU kernel, and copy the\nmemory back. We do this in a timing loop (in listing 12.6) that can be executed multi-\nple times to get a better measurement. Sometimes the first call to a GPU will be slower\ndue to initialization costs. We can amortize it by running several iterations. If this is\nnot sufficient, you can also throw away the timing from the first iteration.\nCUDA/StreamTriad/StreamTriad.cu\n51 for (int k=0; k<NTIMES; k++){\n52    cpu_timer_start(&ttotal);\n53    cudaMemcpy(a_d, a, stream_array_size*         \n         sizeof(double), cudaMemcpyHostToDevice);   \n54    cudaMemcpy(b_d, b, stream_array_size*         \n         sizeof(double), cudaMemcpyHostToDevice);   \n55    // cuda memcopy to device returns after buffer available\n56    cudaDeviceSynchronize();        \n57 \n58    cpu_timer_start(&tkernel);\n59    StreamTriad<<<gridsize, blocksize>>>             \n         (stream_array_size, scalar, a_d, b_d, c_d);   \n60    cudaDeviceSynchronize();            \n61    tkernel_sum += cpu_timer_stop(tkernel);\n62 \n63    // cuda memcpy from device to host blocks for completion\n      //   so no need for synchronize\n64    cudaMemcpy(c, c_d, stream_array_size*         \n         sizeof(double), cudaMemcpyDeviceToHost);   \n65    ttotal_sum += cpu_timer_stop(ttotal);\n66    // check results and print errors if found. \n      //    limit to only 10 errors per iteration\n67    for (int i=0, icount=0; i<stream_array_size && icount < 10; i++){>>>frac_blocks = 1.95\n4 int nblocks = ceil(frac_blocks);\n>>> nblocks = 2\nor \n5 int nblocks = ceil((float)stream_array_size/(float)blocksize);\nor \n6 int nblocks = (stream_array_size + blocksize - 1)/blocksize;\nListing 12.6 CUDA version of stream triad: Kernel call and timing loop\nCopies array \ndata from host \nto device\nSynchronizes to get accurate \ntiming for kernel only\nLaunches \nStreamTriad kernel\nForces completion \nto get timing\nCopies array data back \nfrom device to host\n428 CHAPTER  12 GPU languages: Getting down to basics\n68       if (c[i] != 1.0 + 3.0*2.0) {\n69          printf(""Error with result c[%d]=%lf on iter %d\n"",i,c[i],k);\n70          icount++;\n71       } // if not correct, print error\n72    } // result checking loop\n73 } // timing for loop\nThe pattern in the timing loop is composed of the following steps:\n1Copy data to the GPU (lines 53–54)\n2Call the GPU kernel to operate on the arrays (line 59)\n3Copy the data back (line 64)\nWe add some synchronization and timer calls to get an accurate measurement of the\nGPU kernel. At the end of the loop, we then put in a check for the correctness of\nthe result. Once this goes into production, we can remove the timing, synchroniza-\ntion, and the error check. The call to the GPU kernel can easily be spotted by the tri-\nple chevrons, or angle brackets. If we ignore the chevrons and the variables contained\nwithin these, the line has a typical C subroutine call syntax:\nStreamTriad(stream_array_size, scalar, a_d, b_d, c_d);\nThe values within the parentheses are the arguments to be passed to the GPU kernel.\nFor example\n<<<gridsize, blocksize>>>\nSo what are the arguments contained within the chevrons? These are the arguments\nto the CUDA compiler on how to break up the problem into blocks for the GPU. Ear-\nlier, on lines 48 to 49 of listing 12.2, we set the block size and calculated the number of\nblocks, or grid size, to contain all the data in the array. The arguments in this case are\n1D. We can also have 2D or 3D arrays by declaring and setting these arguments as fol-\nlows for an NxN matrix.\ndim3 blocksize(16,16); dim3 blocksize(8,8,8);\ndim3 gridsize( (N + blocksize.x - 1)/blocksize.x, \n               (N + blocksize.y - 1)/blocksize.y );\nWe can speed up the memory transfers by eliminating a data copy. This is possible\nthrough a deeper understanding of how the operating system functions. Memory that\nis transferred over the network must be in a fixed location that cannot be moved\nduring the operation. Normal memory allocations are placed into pageable memory , or\nmemory that can be moved on demand. The memory transfer must first move the\ndata into pinned memory , or memory that cannot be moved. We first saw the use of\npinned memory in section 9.4.2 when benchmarking memory movement over the\nPCI bus. We can eliminate a memory copy by allocating our arrays in pinned memory",26393
159-12.2.2 A reduction kernel in CUDA Life gets complicated.pdf,159-12.2.2 A reduction kernel in CUDA Life gets complicated,"429 CUDA and HIP GPU languages: The low-level performance option\nrather than pageable memory. Figure 9.8 shows the difference in performance that we\nmight obtain. Now, how do we make this happen? \n CUDA gives us a function call, cudaHostMalloc , that does this for us. It is a\nstraight-up replacement for the regular system malloc  routines, with a slight change\nin arguments, where the pointer is returned as an argument as shown:\ndouble *x_host = (double *)malloc(stream_array_size*sizeof(double));\ncudaMallocHost((void**)&x_host, stream_array_size*sizeof(double));\nIs there a downside to using pinned memory? Well, if you do use a lot of pinned mem-\nory, there is no place to swap in another application. Swapping out the memory for\none application and bringing in another is a huge convenience for users. This process\nis called memory paging.\nDEFINITION Memory paging  in multi-user, multi-application operating systems\nis the process of moving memory pages temporarily out to disk so that\nanother process can take place. \nMemory paging is an important advance in operating systems to make it seem like you\nhave more memory than you really do. For example, it allows you to temporarily start\nup Excel while working on Word and not have to close down your original applica-\ntion. It does this by writing your data out to disk and then reading it back when you\nreturn to Word. But this operation is expensive, so in high performance computing,\nwe avoid memory paging because of the severe performance penalty that it incurs.\nSome heterogeneous computing systems with both a CPU and a GPU are implement-\ning unified memory.\nDEFINITION Unified memory  is memory that has the appearance of being a sin-\ngle address space for both the CPU and the GPU.\nBy now, you have seen that the handling of separate memory spaces on the CPU and the\nGPU introduces much of the complexity of writing GPU code. With unified memory,\nthe GPU runtime system handles this for you. There may still be two separate arrays, but\nthe data is moved automatically. On integrated GPUs, there is the possibility that mem-\nory does not have to be moved at all. Still, it is advisable to write your programs with\nexplicit memory copies so that your programs are portable to systems without unified\nmemory. The memory copy is skipped if it is not needed on the architecture.\n12.2.2 A reduction kernel in CUDA: Life gets complicated\nWhen we need cooperation among GPU threads, things get complicated with lower-\nlevel, native GPU languages. We’ll look at a simple summation example to see how we\ncan deal with this. The example requires two separate CUDA kernels and is shown in\nlistings 12.7–12.10. The following listing shows the first pass, where we sum up the val-\nues within a thread block and store the result back out to the reduction scratch array,\nredscratch .\n430 CHAPTER  12 GPU languages: Getting down to basics\nCUDA/SumReduction/SumReduction.cu (four parts)\n23 __global__ void reduce_sum_stage1of2(\n24                  const int      isize,      // 0  Total number of cells.\n25                        double  *array,      // 1\n26                        double  *blocksum,   // 2\n27                        double  *redscratch) // 3\n28 {\n29     extern __shared__ double spad[];          \n30     const unsigned int giX  = blockIdx.x*blockDim.x+threadIdx.x;\n31     const unsigned int tiX  = threadIdx.x;\n32 \n33     const unsigned int group_id = blockIdx.x;\n34 \n35     spad[tiX] = 0.0;             \n36     if (giX < isize) {           \n37       spad[tiX] = array[giX];    \n38     }                            \n39 \n40     __syncthreads();     \n41 \n42     reduction_sum_within_block(spad);       \n43 \n44     //  Write the local value back to an array \n       //     the size of the number of groups\n45     if (tiX == 0){                       \n46       redscratch[group_id] = spad[0];    \n47       (*blocksum) = spad[0];\n48     }\n49 }\nWe start out the first pass by having all of the threads store their data into a scratchpad\narray in CUDA shared memory (lines 35–38). All the threads in the block can access\nthis shared memory. Shared memory can be accessed in one or two processor cycles\ninstead of the hundreds required for main GPU memory. You can think of shared\nmemory as a programmable cache or as scratchpad memory. To make sure all the\nthreads have completed the store, we use a synchronization call on line 40. \n Because the reduction sum within the block is going to be used in both reduction\npasses, we put the code in a device subroutine and call it on line 42. A device subroutine\nis a subroutine that is to be called from another device subroutine rather than from\nthe host. After the subroutine, the resulting sum is stored back out into a smaller\nscratch array that we read in during the second phase. We also store the result on line\n47 in case the second pass can be skipped. Because we cannot access the values in\nother thread blocks, we have to complete the operation in a second pass in another\nkernel. In this first pass, we have reduced the length of the data by our block size.\n Let’s move on to look at the common device code that we mentioned in the first pass.\nWe will need a sum reduction for the CUDA thread block in both passes, so we write it as\na general device routine. The code shown in the following listing can also be easily modi-\nfied for other reduction operators and only needs small changes for HIP and OpenCL.Listing 12.7 First pass of a sum reduction operation\nScratchpad array in \nCUDA shared memory\nLoads memory \ninto scratchpad \narray\nSynchronizes threads before \nusing scratchpad data\nSets reduction \nwithin thread block\nOne thread stores \nresult for block.\n431 CUDA and HIP GPU languages: The low-level performance option\nCUDA/SumReduction/SumReduction.cu (four parts)\n 1 #define MIN_REDUCE_SYNC_SIZE warpSize     \n 2\n 3 __device__ void reduction_sum_within_block(double  *spad)\n 4 {  \n 5    const unsigned int tiX  = threadIdx.x;\n 6    const unsigned int ntX  = blockDim.x;\n 7    \n 8    for (int offset = ntX >> 1; offset > MIN_REDUCE_SYNC_SIZE; \n           offset >>= 1) {\n 9       if (tiX < offset) {          \n10          spad[tiX] = spad[tiX] + spad[tiX+offset];\n11       }\n12       __syncthreads();       \n13    }\n14    if (tiX < MIN_REDUCE_SYNC_SIZE) {\n15       for (int offset = MIN_REDUCE_SYNC_SIZE; offset > 1; offset >>= 1) {\n16          spad[tiX] = spad[tiX] + spad[tiX+offset];\n17          __syncthreads();    \n18       }\n19       spad[tiX] = spad[tiX] + spad[tiX+1];\n20    }\n21 }\nThe common device routine that will be called from both passes is defined on line 3.\nIt does a sum reduction within the thread block. The __device__  attribute before the\nroutine indicates that it will be called from a GPU kernel. The basic concept of the\nroutine is a pair-wise reduction tree in O(log n) operations as figure 12.2 shows. TheListing 12.8 Common sum reduction device kernel\nCUDA defines \nwarpSize to be 32\nOnly use threads \nneeded when greater \nthan the warp size\nSynchronizes between every level of the pass\n012Thread Warp\n34567891011121314151617181920212223242526272829 3130\nFigure 12.2 Pair-wise reduction tree for a warp that sums up values in log n steps.\n432 CHAPTER  12 GPU languages: Getting down to basics\nbasic reduction tree from the figure is represented by the code on lines 15–18. We imple-\nment some minor modifications when the working set is larger than the warp size on\nlines 8–13 and for the final pass level on line 19 to avoid unnecessary synchronization.\n The same pair-wise reduction concept is used for the full-thread block that can be\nup to 1,024 on most GPU devices, though 128 to 256 is more commonly used. But\nwhat do you do if your array size is greater than 1,024? We add a second pass that uses\njust a single thread block as the following listing shows.\nCUDA/SumReduction/SumReduction.cu (four parts)\n51 __global__ void reduce_sum_stage2of2(\n52                  const int    isize,\n53                        double *total_sum,\n54                        double *redscratch)\n55 {  \n56    extern __shared__ double spad[];\n57    const unsigned int tiX  = threadIdx.x;\n58    const unsigned int ntX  = blockDim.x;\n59    \n60    int giX = tiX;\n61    \n62    spad[tiX] = 0.0;\n63    \n64    // load the sum from reduction scratch, redscratch\n65    if (tiX < isize) spad[tiX] = redscratch[giX];   \n66    \n67    for (giX += ntX; giX < isize; giX += ntX) {   \n68       spad[tiX] += redscratch[giX];              \n69    }                                             \n70    \n71    __syncthreads();     \n72    \n73    reduction_sum_within_block(spad);    \n74    \n75    if (tiX == 0) {\n76      (*total_sum) = spad[0];    \n77    }\n78 }\nTo avoid more than two kernels for larger arrays, we use one thread block and loop\non lines 67–69 to read and sum any additional data into the shared scratchpad. We\nuse a single thread block because we can synchronize within it, avoiding the need for\nanother kernel call. If we are using thread block sizes of 128 and have a one million\nelement array, the loop will sum in about 60 values into each location in shared mem-\nory (1000000/1282). The array size is reduced by 128 in the first pass and then we\nsum into a scratchpad that is size 128, giving us the division by 128 squared. If we use\nlarger block sizes, such as 1,024, we could reduce the loop from 60 iterations to a sin-\ngle read. Now we just call the same common thread block reduction that we usedListing 12.9 Second pass for reduction operation\nLoads values into \nscratchpad array\nLoops by thread \nblock-size increments \nto get all the dataSynchronizes when \nscratchpad array is filled\nCalls our common block \nreduction routine\nOne thread sets the \ntotal sum for return.\n433 CUDA and HIP GPU languages: The low-level performance option\nbefore. The result will be the first value in the scratchpad array. The last part of this is\nto set up and call these two kernels from the host. We’ll see how this is done in the\nfollowing listing.\nCUDA/SumReduction/SumReduction.cu (four parts)\n100 size_t blocksize = 128;                           \n101 size_t blocksizebytes = blocksize*                \n                            sizeof(double);           \n102 size_t global_work_size = ((nsize + blocksize - 1) /blocksize) *\n                              blocksize;\n103 size_t gridsize = global_work_size/blocksize;     \n104 \n105 double *dev_x, *dev_total_sum, *dev_redscratch;\n106 cudaMalloc(&dev_x, nsize*sizeof(double));       \n107 cudaMalloc(&dev_total_sum, 1*sizeof(double));   \n108 cudaMalloc(&dev_redscratch,                     \n               gridsize*sizeof(double));            \n109 \n110 cudaMemcpy(dev_x, x, nsize*sizeof(double),    \n               cudaMemcpyHostToDevice);           \n111 \n112 reduce_sum_stage1of2                              \n          <<<gridsize, blocksize, blocksizebytes>>>   \n          (nsize, dev_x, dev_total_sum,               \n           dev_redscratch);                           \n113 \n114 if (gridsize > 1) {\n115    reduce_sum_stage2of2                         \n          <<<1, blocksize, blocksizebytes>>>        \n          (nsize, dev_total_sum, dev_redscratch);   \n116 }\n117 \n118 double total_sum;\n119 cudaMemcpy(&total_sum, dev_total_sum, 1*sizeof(double),\n               cudaMemcpyDeviceToHost);\n120 printf(""Result -- total sum %lf \n"",total_sum);\n121 \n122 cudaFree(dev_redscratch);\n123 cudaFree(dev_total_sum);\n124 cudaFree(dev_x);\nThe host code first calculates the sizes for the kernel calls on lines 100–103. We then\nhave to allocate the memory for the device arrays. For this operation, we need a\nscratch array where we can store the sums for each block from the first kernel. We\nallocate it on line 108 to be the grid size because that is the number of blocks that\nwe have. We also need a shared memory scratchpad array that is the size of the block\nsize. We calculate this size on line 101 and pass it into the kernel on lines 112 and\n115 as the third parameter to the chevron operator. The third parameter is an\noptional parameter; this is the first time that we have seen it used. Take a look backListing 12.10 Host code for CUDA reduction\nCalculates\nthe block\nand grid\nsizes for\nthe CUDA\nkernels\nAllocates \ndevice memory \nfor the kernel\nCopies the array \nto the GPU device\nCalls the first pass \nof the reduction \nkernel\nIf needed, calls \nthe second pass\n434 CHAPTER  12 GPU languages: Getting down to basics\nat listing 12.9 (line 56) and listing 12.7 (line 29) to see where the corresponding code\nfor the scratchpad is handled on the GPU device.\n Trying to follow all the convoluted loops can be difficult. So we have created a ver-\nsion of the code that does the same loops on the CPU and prints its values as it goes\nalong. It is in the CUDA/SumReductionRevealed directory at \nhttps:/ /github.com/EssentialsofParallelComputing/Chapter12  \nWe don’t have room to show all the code here, but you might find it useful to explore\nand print the values as it executes. We show an edited version of the output in the fol-\nlowing example. \nExample: CUDA/SumReductionRevealed\nCalling first pass with gridsize 2 blocksize 128 blocksizebytes 1024\nSYNCTHREADS after all values are in shared memory block\nData count is 200\n ====== ITREE_LEVEL 1 offset 64 ntX is 128 MIN_REDUCE_SYNC_SIZE 32 ====\nData count is reduced to 128\nSync threads when larger than warp\n ====== ITREE_LEVEL 2 offset 32 ntX is 128 MIN_REDUCE_SYNC_SIZE 32 ====\nSync threads when smaller than warp\nData count is reduced to 64\n ====== ITREE_LEVEL 3 offset 16 ntX is 128 MIN_REDUCE_SYNC_SIZE 32 ====\nSync threads when smaller than warp\nData count is reduced to 32\n ====== ITREE_LEVEL 4 offset 8 ntX is 128 MIN_REDUCE_SYNC_SIZE 32 ====\nSync threads when smaller than warp\nData count is reduced to 16\n ====== ITREE_LEVEL 5 offset 4 ntX is 128 MIN_REDUCE_SYNC_SIZE 32 ====\nSync threads when smaller than warp\nData count is reduced to 8\n ====== ITREE_LEVEL 6 offset 2 ntX is 128 MIN_REDUCE_SYNC_SIZE 32 ====\nSync threads when smaller than warp\nData count is reduced to 4\n ====== ITREE_LEVEL 7 offset 1 ntX is 128 MIN_REDUCE_SYNC_SIZE 32 ====\nData count is reduced to 2\nFinished reduction sum within thread block\nEnd of first pass\nSynchronization in second pass after loading data\nData count is reduced to 2\n ====== ITREE_LEVEL 8 offset 1 ntX is 128 MIN_REDUCE_SYNC_SIZE 32 ====\nData count is reduced to 1\nFinished reduction sum within thread block",14685
160-12.2.3 Hipifying the CUDA code.pdf,160-12.2.3 Hipifying the CUDA code,"435 CUDA and HIP GPU languages: The low-level performance option\nWe have shown this thread block reduction as a general introduction to kernels that\nrequire thread cooperation. You can see how complicated this is, especially compared\nto the single line needed for the intrinsic call in Fortran. In the process, we also\ngained a lot of speedup over the CPU and kept the data on the GPU for this opera-\ntion. This algorithm can be further optimized, but you can also consider using some\nlibrary services such as CUDA UnBound (CUB), Thrust, or other GPU libraries.\n12.2.3 Hipifying the CUDA code\nCUDA code only runs on NVIDIA GPUs. But AMD has implemented a similar GPU\nlanguage and named it the Heterogeneous Interface for Portability (HIP). It is part of\nthe Radeon Open Compute platform (ROCm) suite of tools from AMD. If you program\nin the HIP language, you can call the hipcc compiler that uses NVCC on NVIDIA plat-\nforms and HCC on AMD GPUs. \n To try these examples, you may need to install the ROCm suite of software and\ntools. The install process frequently changes, so check for the latest instructions.\nThere are some instructions that accompany the examples as well.Synchronization in second pass after reduction sum\nResult -- total sum 19900 \nThis example is for an array that is 200 integers long with each element initialized to\nits index value. We suggest that you follow along with the source code and figure 12.1\nto understand what is happening. The start and end of the first pass and the second\npass are printed. We can see the data count being reduced by a factor of two until\nthere are only two left at the end of the first pass. The second pass quickly reduces\nthis to a single value containing the summation.\nExample: Simple makefile for HIPifying a CUDA code\nThere are two versions of the makefile. One uses hipify-perl  and the other uses\nhipify-clang . The hipify-perl  is a simple Perl script. For more syntax-aware\ntranslation, you can try the hipify-clang . In either case, for more complex pro-\ngrams, you might need to manually complete the last modifications. We’ll use the\nPerl version, so lets’s start by linking Makefile.perl, shown in the following listing, to\nMakefile:\nln -s Makefile.perl Makefile\nmake\nHIP/StreamTriad/Makefile.perl\n 1 all: StreamTriad\n 2 \n 3 CXX = hipcc   A simple makefile for HIP\nSets the C++ \ncompiler to hipcc\n436 CHAPTER  12 GPU languages: Getting down to basics\nThere is also good support for HIP in CMake, and HIP support has been available\nsince version 2.8.3 of CMake. A typical CMakeLists file for HIP is shown in the follow-\ning listing.\nHIP/StreamTriad/CMakeLists.txt\n 1 cmake_minimum_required (VERSION 2.8.3)   \n 2 project (StreamTriad)\n 3 \n 6 if(NOT DEFINED HIP_PATH)      \n 7     if(NOT DEFINED ENV{HIP_PATH})\n 8         set(HIP_PATH ""/opt/rocm/hip"" CACHE PATH ""Path to HIP install"")\n 9     else()\n10         set(HIP_PATH $ENV{HIP_PATH} CACHE PATH ""Path to HIP install"")\n11     endif()\n12 endif()\n13 set(CMAKE_MODULE_PATH ""${HIP_PATH}/cmake"" ${CMAKE_MODULE_PATH})\n14 \n15 find_package(HIP REQUIRED)     \n16 if(HIP_FOUND)\n17     message(STATUS ""Found HIP: "" ${HIP_VERSION})\n20 endif()\n21 \n22 set(CMAKE_CXX_COMPILER ${HIP_HIPCC_EXECUTABLE})     \n23 set(MY_HIPCC_OPTIONS )\n24 set(MY_HCC_OPTIONS )\n25 set(MY_NVCC_OPTIONS )\n26 \n27 # Adds build target of StreamTriad with source code files\n28 HIP_ADD_EXECUTABLE(StreamTriad StreamTriad.cc     \n                      timer.c timer.h)               \n29 target_include_directories(StreamTriad PRIVATE ${HIP_PATH}/include)\n30 target_link_directories(StreamTriad PRIVATE ${HIP_PATH}/lib)\n31 target_link_libraries(StreamTriad hip_hcc)(continued)\n 4 \n 5 %.cc : %.cu             \n 6   hipify-perl $^ > $@   \n 7 \n 8 StreamTriad: StreamTriad.o timer.o\n 9   ${CXX} -o $@ $^\n10 \n11 clean:\n12   rm -rf StreamTriad *.o StreamTriad.cc\nThe only real addition to the standard makefile is changing the compiler to hipcc and\nadding a pattern rule for converting the CUDA source code into HIP source code. We\ncould just do the code conversion by manually invoking the hipify-perl script and then\nuse the HIP version for both CUDA and AMD GPUs.\nListing 12.11 Building A HIP program with CMakeConverts the CUDA \ncode to HIP code\nMinimum version of \nCMake for HIP is 2.8.3\nSets a path to the \nHIP installation\nFinds HIP using \nthe path\nSets the C++ \ncompiler to hipcc\nAdds the executable, \nincludes, and \nlibraries\n437 CUDA and HIP GPU languages: The low-level performance option\n32 \n33 # Cleanup\n34 add_custom_target(distclean COMMAND rm -rf CMakeCache.txt CMakeFiles *.o\n35     Makefile cmake_install.cmake StreamTriad.dSYM ipo_out.optrpt)\nIn the listing, we first try to set different path options for where the HIP install might\nbe located and then call find_package  for HIP on line 15. We then set the C++ com-\npiler to hipcc  on line 22. The HIP_ADD_EXECUTABLE  command adds the build of our\nexecutable, and we round out the listing with settings for the HIP header files and\nlibraries (lines 28–31). Now let’s turn our attention to the HIP source in listing 12.12.\nWe highlight the changes from the CUDA version of the source code given in listings\n12.5–12.6.\nHIP/StreamTriad/StreamTriad.c\n 1 #include ""hip/hip_runtime.h""     \n       < . . . skipping . . . >\n36    // allocate device memory. suffix of _d indicates a device pointer\n37    double *a_d, *b_d, *c_d;\n38    hipMalloc(&a_d, stream_array_size*    \n                sizeof(double));            \n39    hipMalloc(&b_d, stream_array_size*    \n                sizeof(double));            \n40    hipMalloc(&c_d, stream_array_size*    \n                sizeof(double));            \n       < . . . skipping . . . >\n46    for (int k=0; k<NTIMES; k++){\n47       cpu_timer_start(&ttotal);\n48       // copying array data from host to device\n49       hipMemcpy(a_d, a, stream_array_size*\n           sizeof(double), hipMemcpyHostToDevice);   \n50       hipMemcpy(b_d, b, stream_array_size*\n           sizeof(double), hipMemcpyHostToDevice);   \n51       // cuda memcopy to device returns after buffer available, \n52       // so synchronize to get accurate timing for kernel only\n53       hipDeviceSynchronize();     \n54 \n55       cpu_timer_start(&tkernel);\n56       // launch stream triad kernel\n57       hipLaunchKernelGGL(StreamTriad,             \n           dim3(gridsize), dim3(blocksize), 0, 0,    \n           stream_array_size, scalar, a_d, b_d,      \n                                             c_d);   \n58       // need to force completion to get timing\n59       hipDeviceSynchronize();                 \n60       tkernel_sum += cpu_timer_stop(tkernel);\n61 \n62       // cuda memcpy from device to host blocks for completion\n         // so no need for synchronize\n63       hipMemcpy(c, c_d, stream_array_size*        \n           sizeof(double), hipMemcpyDeviceToHost);   \n       < . . . skipping . . . >Listing 12.12 The HIP differences for the stream triad\nWe need to include the \nHIP run-time header.\ncudaMalloc \nbecomes \nhipMalloc.\ncuda-\nMemcpy\nbecomes\nhipMemcpy.cudaDeviceSynchronize\nbecomes hipDevice-\nSynchronize.\nhipLaunchKernel is \na more traditional \nsyntax than the CUDA \nkernel launch.",7302
161-12.3.1 Writing and building your first OpenCL application.pdf,161-12.3.1 Writing and building your first OpenCL application,"438 CHAPTER  12 GPU languages: Getting down to basics\n72    }\n       < . . . skipping . . . >\n75 \n76    hipFree(a_d);     \n77    hipFree(b_d);     \n78    hipFree(c_d);     \nTo convert from CUDA source to HIP source, we replace all occurrences of cuda  in\nthe source with hip. The only more significant change is to the kernel launch call,\nwhere HIP uses a more traditional syntax than the triple chevron used in CUDA.\nOddly enough, the greatest changes are to use the correct terminology in the variable\nnaming for the two languages.\n12.3 OpenCL for a portable open source GPU language\nWith the overwhelming need for portable GPU code, a new GPU programming lan-\nguage, OpenCL, emerged in 2008. OpenCL is an open standard GPU language that\ncan run on both NVIDIA and AMD/ATI graphic cards, as well as many other hard-\nware devices. The OpenCL standard effort was led by Apple with many other organiza-\ntions involved. One of the nice things about OpenCL is that you can use virtually any\nC or even C++ compiler for the host code. For the GPU device code, OpenCL initially\nwas based on a subset of C99. Recently, the 2.1 and 2.2 versions of OpenCL added C++\n14 support, but implementations are still not available. \n The OpenCL release took off with a lot of initial excitement. Finally, here was a\nway to write portable GPU code. For example, GIMP announced that it would support\nOpenCL as a way for GPU acceleration to be made available on many hardware plat-\nforms. The reality has been less compelling. Many feel that OpenCL is too low-level\nand verbose for widespread acceptance. It may even be that its eventual role is as the\nlow-level portability layer for higher level languages. But its value as a portable lan-\nguage across a diverse set of hardware devices has been demonstrated by its accep-\ntance within the embedded device community for field-programmable gate arrays\n(FPGAs). One of the reasons OpenCL is thought to be verbose is that the device selec-\ntion is more complicated (and powerful). You have to detect and select the device you\nwill run on. This can amount to a hundred lines of code just to get started. \n Nearly everyone who uses OpenCL writes a library to handle the low-level con-\ncerns. We are no exception. Our library is called EZCL. Nearly every OpenCL call is\nwrapped with at least a light layer to handle the error conditions. Device detection,\ncompiling code, and error handling consume a lot of lines of code. \n We’ll use an abbreviated version of our EZCL library, called EZCL_Lite, in our\nexamples so that you can see the actual OpenCL calls. The EZCL_Lite routines are\nused to select the device and set it up for the application, then compile the device\ncode and handle the errors. The code for these operations is too long to show here,\nso look at the examples in the OpenCL directory at https:/ /github.com/Essentialsof\nParallelComputing/Chapter12 . The full EZCL library is also available in the directory.hipFree replaces \ncudaFree.\n439 OpenCL for a portable open source GPU language\nThe EZCL routines give detailed errors with calls and on which line in the source\ncode that it occurs.\n Before you start out trying the OpenCL code, check to see if you have the proper\nsetup and devices. For this, you can use the clinfo  command. \n12.3.1 Writing and building your first OpenCL application\nThe changes to a standard makefile to incorporate OpenCL are not too complicated.\nThe typical changes are shown in listing 12.13. To use the simple makefile for\nOpenCL, type\nln -s Makefile.simple Makefile\nThen build the application with make  and run the application with ./StreamTriad.\nOpenCL/StreamTriad/Makefile.simple\n 1 all: StreamTriad\n 2 \n 3 #CFLAGS = -DDEVICE_DETECT_DEBUG=1     \n 4 #OPENCL_LIB = -L<path>\n 5 \n 6 %.inc : %.cl                   \n 7   ./embed_source.pl $^ > $@    \n 8 \n 9 StreamTriad.o: StreamTriad.c StreamTriad_kernel.inc\n10 Example: Getting information about the OpenCL installation\nRun the OpenCL info command:\nclinfo\nIf you get the following output, OpenCL is not set up or you do not have an appropriate\nOpenCL device:\nNumber of platforms    0\nIf you don’t have the clinfo  command, try installing it with the appropriate command\nfor your system. For Ubuntu, it is\nsudo apt install clinfo\nThe examples that go along with the chapter include some brief hints for installation\nof OpenCL, but check for the latest information for your system. OpenCL has an\nextension that provides a detailed model for how each device should set up its driver\nin its Installable Client Driver (ICD) specification. This permits multiple OpenCL plat-\nforms and drivers to be available for an application. \nListing 12.13 OpenCL simple makefile\nTurns on device \ndetection verbosity\nPattern rule embeds \nthe OpenCL source\n440 CHAPTER  12 GPU languages: Getting down to basics\n11 StreamTriad: StreamTriad.o timer.o ezclsmall.o\n12   ${CC} -o $@ $^ ${OPENCL_LIB} -lOpenCL\n13 \n14 clean:\n15   rm -rf StreamTriad *.o StreamTriad_kernel.inc\nThe makefile includes a way to set the DEVICE_DETECT_DEBUG  flag to print out detailed\ninformation on the GPU devices available. This flag turns on more verbosity in the\nezcl_lite.c source code. It can be helpful for fixing problems with device detection or\ngetting the wrong device. There is also the addition of a pattern rule on line 6 that\nembeds the OpenCL source into the program for use at run time. This Perl script con-\nverts the source into a comment and as a dependency on line 9. It will be included in\nthe StreamTriad.c file with an include  statement. \n The embed_source.pl utility is one that we developed to link the OpenCL source\ndirectly into the executable. (See the chapter examples for the source to this utility.)\nThe common way for OpenCL code to function is to have a separate source file that\nmust be located at run time, which is then compiled once the device is known. Using a\nseparate file creates problems with it not being able to be found or getting the wrong\nversion of the file. We strongly recommend embedding the source into the executable\nto avoid these problems. We can also use CMake support for OpenCL in our build sys-\ntem as the following listing shows.\nOpenCL/StreamTriad/CMakeLists.txt\n 1 cmake_minimum_required (VERSION 3.1)    \n 2 project (StreamTriad)\n 3 \n 4 if (DEVICE_DETECT_DEBUG)                     \n 5    add_definitions(-DDEVICE_DETECT_DEBUG=1)  \n 6 endif (DEVICE_DETECT_DEBUG)                  \n 7 \n 8 find_package(OpenCL REQUIRED)          \n 9 set(HAVE_CL_DOUBLE ON CACHE BOOL    \n       ""Have OpenCL Double"")           \n10 set(NO_CL_DOUBLE OFF)               \n11 include_directories(${OpenCL_INCLUDE_DIRS})\n12 \n13 # Adds build target of StreamTriad with source code files\n14 add_executable(StreamTriad StreamTriad.c ezclsmall.c ezclsmall.h\n                  timer.c timer.h)\n15 target_link_libraries(StreamTriad ${OpenCL_LIBRARIES})\n16 add_dependencies(StreamTriad StreamTriad_kernel_source)\n17 \n18 ########### embed source target ##############                            \n19 add_custom_command(OUTPUT                                                 \n ${CMAKE_CURRENT_BINARY_DIR}/StreamTriad_kernel.inc                          \n20   COMMAND ${CMAKE_SOURCE_DIR}/embed_source.pl                             \n     ${CMAKE_SOURCE_DIR}/StreamTriad_kernel.cl                               \n     > StreamTriad_kernel.inc                                                \n21         DEPENDS StreamTriad_kernel.cl ${CMAKE_SOURCE_DIR}/embed_source.pl)Listing 12.14 OpenCL CMake file\nCMake\nadded\nOpenCL\nsupport\nwith\nversion\n3.1.Turns on device \ndetection verbosity\nFlags set CL_DOUBLE \nsupport\nCustom command\nembeds OpenCL\nsource into\nexecutable\n441 OpenCL for a portable open source GPU language\n22 add_custom_target(\n          StreamTriad_kernel_source ALL DEPENDS     \n     ${CMAKE_CURRENT_BINARY_DIR}/\n          StreamTriad_kernel.inc)                   \n23 \n24 # Cleanup\n25 add_custom_target(distclean COMMAND rm -rf CMakeCache.txt CMakeFiles\n26                   Makefile cmake_install.cmake StreamTriad.dSYM\n                     ipo_out.optrpt)\n27 \n28 SET_DIRECTORY_PROPERTIES(PROPERTIES ADDITIONAL_MAKE_CLEAN_FILES\n                            ""StreamTriad_kernel.inc"")\nOpenCL support in CMake was added at version 3.1. We added this version require-\nment at the top of the CMakelists.txt file on line 1. There are a few other special\nthings to note. For this example, we used the -DDEVICE_DETECT_DEBUG=1  option to the\nCMake command to turn on the verbosity for the device detection. Also, we included\na way to turn on and off support for OpenCL double precision. We used this in the\nEZCL_Lite code to set the just-in-time (JIT) compile flag for the OpenCL device code.\nLast, we added a custom command in lines 19–22 for embedding the OpenCL device\nsource into the executable. The source code for the OpenCL kernel is in a separate\nfile called StreamTriad_kernel.cl as shown in the following listing. \nOpenCL/StreamTriad/StreamTriad_kernel.cl\n 1 // OpenCL kernel version of stream triad\n 2 __kernel void StreamTriad(       \n 3                const int n,\n 4                const double scalar,\n 5       __global const double *a,\n 6       __global const double *b,\n 7       __global       double *c)\n 8 {\n 9    int i = get_global_id(0);     \n10 \n11    // Protect from going out-of-bounds\n12    if (i >= n) return;\n13 \n14    c[i] = a[i] + scalar*b[i];\n15 }\nCompare this kernel code to the kernel code for CUDA in listing 12.4. The OpenCL\ncode is nearly identical except that __kernel  replaces __global__  on the subroutine\ndeclaration, the __global  attribute is added to the pointer arguments, and there’s a\ndifferent way of getting the thread index. Also, the CUDA kernel code is in the same\n.cu file as the source for the host, while the OpenCL code is in a separate .cl file. We\ncould have separated out the CUDA code into its own .cu file and put the host code in\na standard C++ source file. This would be similar to the structure we use for our\nOpenCL application. Listing 12.15 OpenCL kernelCustom command \nembeds OpenCL source \ninto executable\n__kernel attribute \nindicates this is called \nfrom the host.\nGets the \nthread index\n442 CHAPTER  12 GPU languages: Getting down to basics\nNOTE So many of the differences between the kernel codes for CUDA and\nOpenCL are superficial.\nSo how different is the OpenCL host-side code from the CUDA version? Let’s take a\nlook at the OpenCl version in listing 12.16 and compare it to the code in listing 12.5.\nThere are two versions of the OpenCL stream triad: StreamTriad_simple.c without\nerror checking and StreamTriad.c with error checking. The error checking adds a lot\nof lines of code that initially just get in the way of understanding what is going on.\nOpenCL/StreamTriad/StreamTriad_simple.c\n 5 #include ""StreamTriad_kernel.inc""\n 6 #ifdef __APPLE_CC__             \n 7 #include <OpenCL/OpenCL.h>      \n 8 #else                           \n 9 #include <CL/cl.h>              \n10 #endif                          \n11 #include ""ezcl_lite.h""        \n     < . . . skipping code . . . >\n32    cl_command_queue command_queue;\n33    cl_context context;\n34    iret = ezcl_devtype_init(                   \n             CL_DEVICE_TYPE_GPU, &command_queue,  \n             &context);                           \n35    const char *defines = NULL;\n36    cl_program program  =                     \n         ezcl_create_program_wsource(context,   \n         defines, StreamTriad_kernel_source);   \n37    cl_kernel kernel_StreamTriad =               \n         clCreateKernel(program, ""StreamTriad"",    \n         &iret);                                   \n38 \n39    // allocate device memory. suffix of _d indicates a device pointer\n40    size_t nsize = stream_array_size*sizeof(double);\n41    cl_mem a_d = clCreateBuffer(context,          \n         CL_MEM_READ_WRITE, nsize, NULL, &iret);    \n42    cl_mem b_d = clCreateBuffer(context,          \n         CL_MEM_READ_WRITE, nsize, NULL, &iret);    \n43    cl_mem c_d = clCreateBuffer(context,          \n         CL_MEM_READ_WRITE, nsize, NULL, &iret);    \n44 \n45    // setting work group size and padding\n      //    to get even number of workgroups\n46    size_t local_work_size = 512;                                         \n47    size_t global_work_size = ( (stream_array_size + local_work_size - 1) \n           /local_work_size ) * local_work_size;                            \n     < . . . skipping code . . . >\n74    clReleaseMemObject(a_d);                      \n75    clReleaseMemObject(b_d);                      \n76    clReleaseMemObject(c_d);                      \n77 Listing 12.16 OpenCL version of stream triad: Set up and tear down\nApple has \nto be \ndifferent.\nOur EZCL_Lite \nsupport library\nGets the GPU \ndevice\nCreates the program \nfrom the source\nCompiles the \nStreamTriad kernel \nin the source\nHandles\narray\nmemory\nWork group size\ncalculation is\nsimilar to CUDA.\n443 OpenCL for a portable open source GPU language\n78    clReleaseKernel(kernel_StreamTriad);    \n79    clReleaseCommandQueue(command_queue);   \n80    clReleaseContext(context);              \n81    clReleaseProgram(program);              \nAt the start of the program, we encounter some real differences at lines 34–37, where\nwe have to find our GPU device and compile our device code. This is done for us\nbehind the scenes in CUDA. Two of the lines of OpenCL code call our EZCL_Lite\nroutines to detect the device and to create the program object. We made these calls\nbecause the amount of code required for these functions is too long to show here.\nThe source for these routines is hundreds of lines long, though much of it is error\nchecking. \nNOTE The source is available with the chapter examples in the OpenCL/\nStreamTriad directory at https:/ /github.com/EssentialsofParallelComputing/\nChapter12 . Some of the error checking code has been left out of the short\nversion, StreamTriad_simple.c, but it is in the long version of the code in the\nfile StreamTriad.c. \nThe rest of the set up and tear down code follows the same pattern that we saw in the\nCUDA code, with a little more cleanup required, again related to the device and pro-\ngram source handling. Now, how does the section of code that calls the OpenCL ker-\nnel in the timing loop in listing 12.16 compare to the CUDA code from listing 12.6?\nOpenCL/StreamTriad/StreamTriad_simple.c\n49    for (int k=0; k<NTIMES; k++){\n50       cpu_timer_start(&ttotal);\n51       // copying array data from host to device\n52       iret=clEnqueueWriteBuffer(command_queue,    \n             a_d, CL_FALSE, 0, nsize, &a[0],         \n             0, NULL, NULL);                         \n53       iret=clEnqueueWriteBuffer(command_queue,    \n             b_d, CL_TRUE, 0, nsize, &b[0],          \n             0, NULL, NULL);                         \n54 \n55       cpu_timer_start(&tkernel);\n56       // set stream triad kernel arguments\n57       iret=clSetKernelArg(kernel_StreamTriad,    \n             0, sizeof(cl_int),                     \n             (void *)&stream_array_size);           \n58       iret=clSetKernelArg(kernel_StreamTriad,    \n             1, sizeof(cl_double),                  \n             (void *)&scalar);                      \n59       iret=clSetKernelArg(kernel_StreamTriad,    \n             2, sizeof(cl_mem), (void *)&a_d);      \n60       iret=clSetKernelArg(kernel_StreamTriad,    \n             3, sizeof(cl_mem), (void *)&b_d);      \n61       iret=clSetKernelArg(kernel_StreamTriad,    \n             4, sizeof(cl_mem), (void *)&c_d);      Listing 12.17 OpenCL version of stream triad: Kernel call and timing loopCleans up kernel \nand device-related \nobjects\nMemory \nmovement \ncalls\nSets kernel \narguments\n444 CHAPTER  12 GPU languages: Getting down to basics\n62       // call stream triad kernel\n63       clEnqueueNDRangeKernel(command_queue,       \n             kernel_StreamTriad, 1, NULL,            \n             &global_work_size, &local_work_size,    \n             0, NULL, NULL);                         \n64       // need to force completion to get timing\n65       clEnqueueBarrier(command_queue);\n66       tkernel_sum += cpu_timer_stop(tkernel);\n67 \n68       iret=clEnqueueReadBuffer(command_queue,    \n             c_d, CL_TRUE, 0, nsize, c,             \n             0, NULL, NULL);                        \n69       ttotal_sum += cpu_timer_stop(ttotal);\n70    }\nWhat is happening on lines 57–61? OpenCL requires a separate call for every kernel\nargument. If we check the return code from each, it is even more lines. This is a lot\nmore verbose than the single line 53 in listing 12.6 in the CUDA version. But there is a\ndirect correspondence between the two versions. OpenCL is just more verbose in\ndescribing the operations to pass the arguments. Except for the device detection and\nprogram compilation, the programs are similar in their operations. The biggest differ-\nence is the syntax used in the two languages. \n In listing 12.18, we show a rough call sequence for the device detection and the\ncreate program calls. What makes these routines long is the error checking and the han-\ndling required for special cases. For these two functions, it is important to have good\nerror handling. We need the compiler report for an error in our source code or if it\ngot the wrong GPU device.\nOpenCL/StreamTriad/ezcl_lite.c\n/* init and finish routine */\ncl_int ezcl_devtype_init(cl_device_type device_type, \n   cl_command_queue *command_queue, cl_context *context);\nclGetPlatformIDs -- first to get number of platforms and allocate\nclGetPlatformIDs -- now get platforms\nLoop on number of platforms and\n   clGetDeviceIDs -- once to get number of devices and allocate\n   clGetDeviceIDs -- get devices\n   check for double precision support -- clGetDeviceInfo\nEnd loop\nclCreateContext\nclCreateCommandQueue\n/* kernel and program routines */\ncl_program ezcl_create_program_wsource(cl_context context,\n    const char *defines, const char *source);\n       clCreateProgramWithSource\n       set a compile string (hardware specific options)\n       clBuildProgram\n       Check for error, if foundListing 12.18 OpenCL support library ezcl_liteCalls the \nkernel\nSynchronization \nbarrier",18499
162-12.3.2 Reductions in OpenCL.pdf,162-12.3.2 Reductions in OpenCL,"445 OpenCL for a portable open source GPU language\n          clGetProgramBuildInfo\n          and printout compile report\n       End error handling \nWe conclude this presentation on OpenCL with a nod to the many language inter-\nfaces that have been created for it. There are a C++, Python, Perl, and Java versions. In\neach of these languages, a higher-level interface has been created that hides some of\nthe details in the C version of OpenCL. And, we highly recommend the use of our\nEZCL library or one of the many other middleware libraries for OpenCL. \n There has been an unofficial C++ version available since OpenCL v1.2. The\nimplementation is just a thin layer on top of the C version of OpenCL. Despite fail-\nure to get approval by the standards committee, it is completely usable by develop-\ners. It is available at https:/ /github.com/KhronosGroup/OpenCL-CLHPP . The formal\napproval of C++ in OpenCL has only recently occurred, but we are still waiting on\nimplementations.\n12.3.2 Reductions in OpenCL\nThe sum reduction in OpenCL is similar to that in CUDA. Rather than step through\nthe code, we’ll just look at the differences in the kernel source. Shown first in figure 12.3\nis the side-by-side difference of the sum_within_block , the common routine by both\nkernels.\nThe difference in this device kernel called by another kernel begins with the attri-\nbutes on the declaration. CUDA requires a __device__  attribute on the declaration,\nwhile OpenCL does not. For the arguments, passing in the scratchpad array requires a\n__local  attribute that CUDA does not need. The next difference is the syntax for get-\nting the local thread index and block (tile) size (figure 12.3 on lines 5 and 6). The syn-\nchronization calls are also different. At the top of the routine, a warp size is defined by\na macro to help with portability between NVIDIA and AMD GPUs. CUDA defines this\nas a warp-size variable. For OpenCL, it is passed in with a compiler define. We also\nOpenCL CUDA\nFigure 12.3 Comparison of OpenCL and CUDA reduction kernels: sum_within_block\n446 CHAPTER  12 GPU languages: Getting down to basics\nchange the terminology from block to tile in the actual code to stay consistent with\neach language’s terminology. \n The next routine is the first of two kernel passes, called stage1of2, in figure 12.4.\nThis kernel is called from the host. The __global__  attribute for CUDA becomes\n__kernel  for OpenCL. We also have to add the __global  attribute to the pointer\narguments for OpenCL.\nThe next difference is an important one to take note of. In CUDA, we declare the\nscratchpad in shared memory as an extern  __shared__  variable in the body of the\nkernel. On the host side, the size of this shared memory space is given as a number of\nbytes in the optional third argument in the triple chevron brackets. OpenCL does this\ndifferently. It is passed as the last argument in the argument list with the __local  attri-\nbute. On the host side, the memory is specified in the set argument call for the fourth\nkernel argument:\nclSetKernelArg(reduce_sum_1of2, 4, \n               local_work_size*sizeof(cl_double), NULL);\nThe size is the third argument in the call. The rest of the changes are in the syntax to\nset the thread parameters and the synchronization call. The last part of the compari-\nson is the second pass of the sum reduction kernel in figure 12.5.\n We’ve already seen all of the change patterns in the second kernel. We still have\nthe differences in the declaration of the kernel and the arguments. The local scratch\narray also has the same differences as the kernel for the first pass. The thread parame-\nters and the synchronization also have the same expected differences.\n Looking back at the three comparisons in figures 12.3–5, it is what we didn’t have\nto note that becomes apparent. The bodies of the kernels are essentially the same.\nOpenCL CUDA\nFigure 12.4 Comparison for the first of two kernel passes for the OpenCL and CUDA reduction kernels\n447 OpenCL for a portable open source GPU language\nThe only difference is the syntax for the synchronization call. The host side code for\nthe sum reduction in OpenCL is shown in the following listing.\nOpenCL/SumReduction/SumReduction.c\n20 cl_context context;\n21 cl_command_queue command_queue;\n22 ezcl_devtype_init(CL_DEVICE_TYPE_GPU, &command_queue, &context);\n23 \n24 const char *defines = NULL;\n25 cl_program program = ezcl_create_program_wsource(context, defines,\n      SumReduction_kernel_source);\n26 cl_kernel reduce_sum_1of2=clCreateKernel(         \n      program, ""reduce_sum_stage1of2_cl"", &iret);    \n27 cl_kernel reduce_sum_2of2=clCreateKernel(         \n      program, ""reduce_sum_stage2of2_cl"", &iret);    \n28 \n29 struct timespec tstart_cpu;\n30 cpu_timer_start(&tstart_cpu);\n31 \n32 size_t local_work_size = 128;\n33 size_t global_work_size = ((nsize + local_work_size - 1) \n     /local_work_size) * local_work_size;\n34 size_t nblocks     = global_work_size/local_work_size;\n35 \n36 cl_mem dev_x = clCreateBuffer(context, CL_MEM_READ_WRITE, \n      nsize*sizeof(double), NULL, &iret);\n37 cl_mem dev_total_sum = clCreateBuffer(context, CL_MEM_READ_WRITE, \n      1*sizeof(double), NULL, &iret);\n38 cl_mem dev_redscratch = clCreateBuffer(context, CL_MEM_READ_WRITE,\n      nblocks*sizeof(double), NULL, &iret);\n39 \n40 clEnqueueWriteBuffer(command_queue, dev_x, CL_TRUE, 0, \n      nsize*sizeof(cl_double), &x[0], 0, NULL, NULL);Listing 12.19 Host code for the OpenCL sum reduction\nOpenCL CUDA\nFigure 12.5 Comparison of the second pass for the reduction sum\nTwo kernels to \ncreate from a \nsingle source\n448 CHAPTER  12 GPU languages: Getting down to basics\n41 \n42 clSetKernelArg(reduce_sum_1of2, 0,              \n      sizeof(cl_int), (void *)&nsize);             \n43 clSetKernelArg(reduce_sum_1of2, 1,              \n      sizeof(cl_mem), (void *)&dev_x);             \n44 clSetKernelArg(reduce_sum_1of2, 2,              \n      sizeof(cl_mem), (void *)&dev_total_sum);     \n45 clSetKernelArg(reduce_sum_1of2, 3,              \n      sizeof(cl_mem), (void *)&dev_redscratch);    \n46 clSetKernelArg(reduce_sum_1of2, 4,              \n      local_work_size*sizeof(cl_double), NULL);    \n47    \n48 clEnqueueNDRangeKernel(command_queue,           \n      reduce_sum_1of2, 1, NULL, &global_work_size, \n      &local_work_size, 0, NULL, NULL);            \n49    \n50 if (nblocks > 1) {         \n51    clSetKernelArg(reduce_sum_2of2, 0,               \n         sizeof(cl_int), (void *)&nblocks);            \n52    clSetKernelArg(reduce_sum_2of2, 1,               \n         sizeof(cl_mem), (void *)&dev_total_sum);      \n53    clSetKernelArg(reduce_sum_2of2, 2,               \n         sizeof(cl_mem), (void *)&dev_redscratch);     \n54    clSetKernelArg(reduce_sum_2of2, 3,               \n         local_work_size*sizeof(cl_double), NULL);     \n55       \n56    clEnqueueNDRangeKernel(command_queue,            \n         reduce_sum_2of2, 1, NULL, &local_work_size,   \n         &local_work_size, 0, NULL, NULL);             \n57 }  \n58    \n59 double total_sum;\n60    \n61 iret=clEnqueueReadBuffer(command_queue, dev_total_sum, CL_TRUE, 0,\n      1*sizeof(cl_double), &total_sum, 0, NULL, NULL);\n62    \n63 printf(""Result -- total sum %lf \n"",total_sum);\n64    \n65 clReleaseMemObject(dev_x);\n66 clReleaseMemObject(dev_redscratch);\n67 clReleaseMemObject(dev_total_sum);\n68 \n69 clReleaseKernel(reduce_sum_1of2);\n70 clReleaseKernel(reduce_sum_2of2);\n71 clReleaseCommandQueue(command_queue);\n72 clReleaseContext(context);\n73 clReleaseProgram(program);\nThe call to the first kernel pass creates a local scratchpad array on line 46. The inter-\nmediate results are stored back into the redscratch  array created on line 38. If there\nis more than one block, a second pass is needed. The redscratch  array is passed back\nin to complete the reduction. Note that the kernel parameters in arguments 5 and 6Calls first \nreduction \npass\nIf second pass needed …\n… calls second \nreduction pass",8078
163-12.4 SYCL An experimental C implementation goes mainstream.pdf,163-12.4 SYCL An experimental C implementation goes mainstream,"449 SYCL: An experimental C++ implementation goes mainstream\nare set to local_work_size  or a single work group. This is so a synchronization can be\ndone across all the remaining data and another pass will not be needed.\n12.4 SYCL: An experimental C++ implementation \ngoes mainstream\nSYCL started out in 2014 as an experimental C++ implementation on top of OpenCL.\nThe goal of the developers creating SYCL is a more natural extension of the C++ lan-\nguage than the add-on feeling of OpenCL with the C language. It is being developed\nas a cross-platform abstraction layer that leverages the portability and efficiency of\nOpenCL. Its experimental language focus changed suddenly when Intel chose it as\none of their major language pathways for the announced Department of Energy\nAurora HPC system. The Aurora system will use the new Intel discrete GPUs that are\nunder development. Intel has proposed some additions to the SYCL standard that\nthey have prototyped in their Data Parallel C++ (DPCPP) compiler in their oneAPI\nopen programming system.\n You can get introduced to SYCL in several ways. Some of these even avoid having to\ninstall the software or having the right hardware. You might first try out the following\ncloud-based systems:\nInteractive SYCL provides a tutorial on the tech.io website at https:/ /tech.io/\nplaygrounds/48226/introduction-to-sycl/introduction-to-sycl-2 . \nIntel provides a cloud version of oneAPI and DPCPP at https:/ /software.intel\n.com/en-us/oneapi . You must register to use. \nYou can also download and install versions of SYCL from these sites:\nThe ComputeCPP community edition at https:/ /developer.codeplay.com\n/products/computecpp/ce/home/ . You must register to download.\nThe Intel DPCPP compiler at https:/ /github.com/intel/llvm/blob/sycl/sycl/\ndoc/GetStartedGuide.md\nIntel also provides Docker file setup instructions at https:/ /github.com/intel/\noneapi-containers/blob/master/images/docker/basekit-devel-ubuntu18.04/\nDockerfile\nWe’ll work with Intel’s DPCPP version of SYCL. There are instructions to set up a\nVirtualBox installation of oneAPI with the examples that accompany this chapter in the\nREADME.virtualbox at https:/ /github.com/EssentialsofParallelComputing/Chapter12 .\nYou should be able to run VirtualBox on nearly any operating system. Let’s start off\nwith a simple makefile for the DPCPP compiler as the following listing shows.\nDPCPP/StreamTriad/Makefile\n 1 CXX = dpcpp               \n 2 CXXFLAGS = -std=c++17 -fsycl -O3    \n 3 \n 4 all: StreamTriadListing 12.20 Simple makefile for DPCPP version of SYCL\nSpecifies dpcpp as \nthe C++ compiler\nAdds the SYCL option \nto the C++ flags\n450 CHAPTER  12 GPU languages: Getting down to basics\n 5 \n 6 StreamTriad: StreamTriad.o timer.o\n 7   $(CXX) $(CXXFLAGS) $^ -o $@\n 8 \n 9 clean:\n10   -rm -f StreamTriad.o StreamTriad\nSetting the C++ compiler to the Intel dpcpp compiler takes care of the paths, libraries,\nand include files. The only other requirement is to set some flags for the C++ com-\npiler. The following listing shows the SYCL source for our example.\nDPCPP/StreamTriad/StreamTriad.cc\n 1 #include <chrono>\n 2 #include ""CL/sycl.hpp""      \n 3 \n 4 namespace Sycl = cl::sycl;    \n 5 using namespace std;\n 6 \n 7 int main(int argc, char * argv[])\n 8 {\n 9     chrono::high_resolution_clock::time_point t1, t2;\n10 \n11     size_t nsize = 10000;\n12     cout << ""StreamTriad with "" << nsize << "" elements"" << endl;\n13 \n14     // host data\n15     vector<double> a(nsize,1.0);    \n16     vector<double> b(nsize,2.0);    \n17     vector<double> c(nsize,-1.0);   \n18 \n19     t1 = chrono::high_resolution_clock::now();\n20 \n21     Sycl::queue Queue(Sycl::cpu_selector{});    \n22 \n23     const double scalar = 3.0;\n24 \n25     Sycl::buffer<double,1> dev_a { a.data(),   \n          Sycl::range<1>(a.size()) };             \n26     Sycl::buffer<double,1> dev_b { b.data(),   \n          Sycl::range<1>(b.size()) };             \n27     Sycl::buffer<double,1> dev_c { c.data(),   \n          Sycl::range<1>(c.size()) };             \n28 \n29     Queue.submit([&](Sycl::handler& \n             CommandGroup) {             \n30 \n31        auto a = dev_a.get_access<Sycl::        \n             access::mode::read>(CommandGroup);   \n32        auto b = dev_b.get_access<Sycl::        \n             access::mode::read>(CommandGroup);   \n33        auto c = dev_c.get_access<Sycl::        \n             access::mode::write>(CommandGroup);  Listing 12.21 Stream triad example for DPCPP version of SYCL\nIncludes the SYCL \nheader file\nUses the SYCL \nnamespace\nInitializes the host \nside vectors to \nconstants\nSets up the \ndevice for CPU\nAllocates the \ndevice buffer \nand sets to the \nhost buffer\nLambda for queue \nsubmission\nGets access \nto device \narrays\n451 SYCL: An experimental C++ implementation goes mainstream\n34 \n35        CommandGroup.parallel_for<class           \n              StreamTriad>(Sycl::range<1>{nsize},   \n              [=] (Sycl::id<1> it){                 \n36            c[it] = a[it] + scalar * b[it];\n37        });\n38     });\n39     Queue.wait();    \n40 \n41     t2 = chrono::high_resolution_clock::now();\n42 \n       double time1 = chrono::duration_cast<\n                      chrono::duration<double> >(t2 - t1).count();\n43     cout << ""Runtime is  "" << time1*1000.0 << "" msecs "" << endl;\n44 }\nThe first Sycl  function selects a device and creates a queue to work on it. We ask for a\nCPU, though this code would also work for GPUs with unified memory.\nSycl::queue Queue(sycl::cpu_selector{});\nWe select a CPU for maximum portability so that the code runs on most systems. To\nmake this code work on GPUs without unified memory, we would need to add explicit\ncopies of data from one memory space to another. The default selector preferentially\nfinds a GPU, but falls back to a CPU. If we want to only select a GPU or CPU, we could\nalso specify other selectors such as\nSycl::queue Queue(sycl::default_selector{}); // uses the default device\nSycl::queue Queue(sycl::gpu_selector{});     // finds a GPU device\nSycl::queue Queue(sycl::cpu_selector{});     // finds a CPU device\nSycl::queue Queue(sycl::host_selector{});    // runs on the host (CPU)\nThe last option means that it will run on the host as if there were no SYCL or OpenCL\ncode. The setup of the device and queue is far simpler than what we did in OpenCL.\nNow we need to set up device buffers with the SYCL buffer:\nSycl::buffer<double,1> dev_a { a.data(), Sycl::range<1>(a.size()) };\nThe first argument to the buffer is a data type, and the second is the dimensionality of\nthe data. Then we give it the variable name, dev_a . The first argument to the variable is\nthe host data array to use for initializing the device array, and the second is the index set\nto use. In this case, we specify a 1D range from 0 to the size of the a variable. On line 29,\nwe encounter the first lambda to create a command group handler for the queue:\nQueue.submit([&](Sycl::handler& CommandGroup)\nWe introduced lambdas in section 10.2.1. The lambda capture clause, [&], specifies\ncapturing outside variables used in the routine by reference. For this lambda, theLambda for \nparallel for \nkernel\nWaits for \ncompletion",7303
164-12.5 Higher-level languages for performance portability.pdf,164-12.5 Higher-level languages for performance portability,,0
165-12.5.1 Kokkos A performance portability ecosystem.pdf,165-12.5.1 Kokkos A performance portability ecosystem,"452 CHAPTER  12 GPU languages: Getting down to basics\ncapture gets nsize , scalar , dev_a , dev_b , and dev_c  for use in the lambda. We could\nspecify it with just the single capture setting of by reference, [&], or with the following\nform, where we specify each variable that will be captured. Good programming prac-\ntice would prefer the latter, but the lists can get long.\nQueue.submit([&nsize, &scalar, &dev_a, &dev_b, &dev_c]\n    (Sycl::handler& CommandGroup)\nIn the body of the lambda, we get access to the device arrays and rename them for use\nwithin the device routine. This is equivalent to a list of arguments for the command\ngroup handler. We then create the first task for the command group, a parallel_for .\nThe parallel_for  also is defined with a lambda. \nCommandGroup.parallel_for<class StreamTriad>(Sycl::range<1>{nsize},[=] \n                                            (Sycl::id<1> it)\nThe name of the lambda is StreamTriad . We then tell it that we will operate over a 1D\nrange that goes from 0 to nsize. The capture clause, [=], captures the a, b, and c vari-\nables by value. Determining whether to capture by reference or value is tricky. But if\nthe code gets pushed to the GPU, the original reference may be out of scope and no\nlonger valid. We last create a 1D index variable, it, to iterate over the range.\n12.5 Higher-level languages for performance portability\nBy now, you are seeing that the differences between CPU and GPU kernels are not\nall that big. So why not generate each of them using C++ polymorphism and tem-\nplates? Well, that is exactly what a couple of libraries developed by Department of\nEnergy research laboratories have done. These projects were started to tackle the\nporting of many of their codes to new hardware architectures. The Kokkos system\nwas created by Sandia National Laboratories and has gained a wide following. Law-\nrence Livermore National Laboratory has a similar project by the name of RAJA.\nBoth of these projects have already succeeded in their goal of a single-source, multi-\nplatform capability. \n These two languages have similarities in a lot of respects to the SYCL language\nthat you saw in section 12.4. Indeed, they have borrowed concepts from each other\nas they strive for performance portability. Each of them provides libraries that are\nfairly light layers on top of lower-level parallel programming languages. We’ll take a\nshort look at each of them.\n12.5.1 Kokkos: A performance portability ecosystem\nKokkos is a well-designed abstraction layer for languages such as OpenMP and CUDA.\nIt has been in development since 2011. Kokkos has the following named execution\nspaces. These are enabled in the Kokkos build with the corresponding flag to CMake\n(or the option to build with Spack). Some of these are better developed than others.\n453 Higher-level languages for performance portability\nKokkos/StreamTriad/CMakeLists.txt\n1 cmake_minimum_required (VERSION 3.10)\n2 project (StreamTriad)\n3 \n4 find_package(Kokkos REQUIRED)     \n5 \n6 add_executable(StreamTriad StreamTriad.cc)\n7 target_link_libraries(StreamTriad Kokkos::kokkos)     \nAdding the CUDA option to the Kokkos build generates a version that runs on\nNVIDIA GPUs. There are many other platforms and languages that Kokkos can han-\ndle and more are being developed all the time.Kokkos execution spaces CMake/Spack-enabled flags\nKokkos::Serial -DKokkos_ENABLE_SERIAL=On  (default is on)\nKokkos::Threads -DKokkos_ENABLE_PTHREAD=On\nKokkos::OpenMP -DKokkos_ENABLE_OPENMP=On\nKokkos::Cuda -DKokkos_ENABLE_CUDA=On\nKokkos::HPX -DKokkos_ENABLE_HPX=On\nKokkos::ROCm -DKokkos_ENABLE_ROCm=On\nExercise: Stream triad in Kokkos\nFor this exercise, we built Kokkos with the OpenMP backend and then built and ran\nthe stream triad example. To start:\ngit clone https://github.com/kokkos/kokkos\nmkdir build && cd build\ncmake ../kokkos -DKokkos_ENABLE_OPENMP=On\nThen go to the stream triad source directory for Kokkos and do an out-of-tree build\nwith CMake:\nmkdir build && cd build\nexport Kokkos_DIR=${HOME}/Kokkos/lib/cmake/Kokkos\ncmake ..\nmake\nexport OMP_PROC_BIND=true\nexport OMP_PLACES=threads\nThe Kokkos build with CMake has been streamlined so that it is easy as the following\nlisting. The Kokkos_DIR  variable needs to be set to the location of the CMake con-\nfiguration file for Kokkos. \nListing 12.22 Kokkos CMake file\nFinds Kokkos and \nsets the flags\nAdds dependencies \nand flags to build\n454 CHAPTER  12 GPU languages: Getting down to basics\n The Kokkos stream triad example in listing 12.23 has some similarities to SYCL\nin that it uses C++ lambdas to encapsulate functions for either the CPU or GPU.\nKokkos also supports functors for this mechanism, but lambdas are less verbose to\nuse in practice.\nKokkos/StreamTriad/StreamTriad.cc\n 1 #include <Kokkos_Core.hpp>     \n 2 \n 3 using namespace std;\n 4 \n 5 int main (int argc, char *argv[])\n 6 {\n 7    Kokkos::initialize(argc, argv);{     \n 8 \n 9       Kokkos::Timer timer;\n10       double time1;\n11 \n12       double scalar = 3.0;\n13       size_t nsize = 1000000;\n14       Kokkos::View<double *> a( ""a"", nsize);    \n15       Kokkos::View<double *> b( ""b"", nsize);    \n16       Kokkos::View<double *> c( ""c"", nsize);    \n17       \n18       cout << ""StreamTriad with "" << nsize << "" elements"" << endl;\n19 \n20       Kokkos::parallel_for(nsize,          \n               KOKKOS_LAMBDA (int i) {        \n21          a[i] = 1.0;                       \n22       });                                  \n23       Kokkos::parallel_for(nsize,          \n               KOKKOS_LAMBDA (int i) {        \n24          b[i] = 2.0;                       \n25       });                                  \n26 \n27       timer.reset();\n28 \n29       Kokkos::parallel_for(nsize,          \n               KOKKOS_LAMBDA (const int i) {  \n30          c[i] = a[i] + scalar * b[i];      \n31       });                                  \n32 \n33       time1 = timer.seconds();\n34 \n35       icount = 0;\n36       for (int i=0; i<nsize && icount < 10; i++){\n37          if (c[i] != 1.0 + 3.0*2.0) {\n38             cout << ""Error with result c["" << i << ""]="" << c[i] << endl;\n39             icount++;\n40          }\n41       }\n42 Listing 12.23 Kokkos stream triad example\nIncludes the \nappropriate \nKokkos header\nInitializes \nKokkos\nDeclares arrays \nwith Kokkos::View\nKokkos \nparallel_for \nlambdas for \nCPU or GPU",6472
166-12.5.2 RAJA for a more adaptable performance portability layer.pdf,166-12.5.2 RAJA for a more adaptable performance portability layer,"455 Higher-level languages for performance portability\n43       if (icount == 0) \n            cout << ""Program completed without error."" << endl;\n44       cout << ""Runtime is  "" << time1*1000.0 << "" msecs "" << endl;\n45 \n46    }\n47    Kokkos::finalize();   \n48    return 0;\n49 }\nThe Kokkos program starts with Kokkos::initialize  and Kokkos::finalize . These\ncommands start up those things that are needed for the execution space, such as\nthreads. Kokkos is unique in that it encapsulates flexible multi-dimensional array\nallocations as data views that can be switched depending on the target architecture.\nIn other words, you can use a different data order for CPU versus GPU. We use\nKokkos::View  on lines 14–16, though this is only for 1D arrays. The real value comes\nwith multidimensional arrays. The general syntax for Kokkos::View  is\nView < double *** , Layout , MemorySpace > name (...);\nMemory spaces are an option for the template, but have a default appropriate for the\nexecution space. Some memory spaces are\nHostSpace\nCudaSpace\nCudaUVMSpace\nThe layout can be specified, although it has a default appropriate for the memory space:\nFor LayoutLeft , the leftmost index is stride 1 (default for CudaSpace )\nFor LayoutRight , the rightmost index is stride 1 (default for HostSpace )\nThe kernels are specified using a lambda syntax on one of three data parallel patterns:\nparallel_for\nparallel_reduce\nparallel_scan\nOn lines 20, 23, and 29 in listing 12.23, we used the parallel_for  pattern. The\nKOKKOS_LAMBDA  macro replaces the [=] or [&] capture syntax. Kokkos takes care of\nspecifying this for you and does it in a much more readable form.\n12.5.2 RAJA for a more adaptable performance portability layer\nThe RAJA performance portability layer has the goal of achieving portability with a\nminimum of disruptions to existing Lawrence Livermore National Laboratory codes.\nIn many ways, it is simpler and easier to adopt than other comparable systems. RAJA\ncan be built with support for the following:\n-DENABLE_OPENMP=On  (default on)\n-DENABLE_TARGET_OPENMP=On  (default Off)Finalizes \nKokkos\n456 CHAPTER  12 GPU languages: Getting down to basics\n-DENABLE_CUDA=On  (default Off)\n-DENABLE_TBB=On  (default Off)\nRAJA also has good support for CMake as the following listing shows.\nRaja/StreamTriad/CMakeLists.txt\n 1 cmake_minimum_required (VERSION 3.0)\n 2 project (StreamTriad)\n 3\n 4 find_package(Raja REQUIRED)\n 5 find_package(OpenMP REQUIRED)\n 6 \n 7 add_executable(StreamTriad StreamTriad.cc)\n 8 target_link_libraries(StreamTriad PUBLIC RAJA)\n 9 set_target_properties(StreamTriad PROPERTIES\n                         COMPILE_FLAGS ${OpenMP_CXX_FLAGS})\n10 set_target_properties(StreamTriad PROPERTIES \n                         LINK_FLAGS ""${OpenMP_CXX_FLAGS}"")\nThe RAJA version of the stream triad takes only a few changes as the following list-\ning shows. RAJA also heavily leverages lambdas to provide their portability to CPUs\nand GPUs.\nRaja/StreamTriad/StreamTriad.cc\n 1 #include <chrono>\n 2 #include ""RAJA/RAJA.hpp""    \n 3 \n 4 using namespace std;\n 5 \n 6 int main(int RAJA_UNUSED_ARG(argc), char **RAJA_UNUSED_ARG(argv[]))\n 7 {\n 8    chrono::high_resolution_clock::time_point t1, t2;\n 9    cout << ""Running Raja Stream Triad\n"";\n10 \n11    const int nsize = 1000000;\n12 \n13 // Allocate and initialize vector data.\n14    double scalar = 3.0;\n15    double* a = new double[nsize];\n16    double* b = new double[nsize];\n17    double* c = new double[nsize];\n18 \n19    for (int i = 0; i < nsize; i++) {\n20      a[i] = 1.0;\n21      b[i] = 2.0;\n22    }\n23 \n24    t1 = chrono::high_resolution_clock::now();\n25 Listing 12.24 Raja CMake file\nListing 12.25 Raja stream triad example\nIncludes Raja \nheaders",3799
167-12.6 Further explorations.pdf,167-12.6 Further explorations,,0
168-13 GPU profiling and tools.pdf,168-13 GPU profiling and tools,"457 Further explorations\n26    RAJA::forall<RAJA::omp_parallel_for_exec>(   \n        RAJA::RangeSegment(0,nsize),[=](int i){    \n27      c[i] = a[i] + scalar * b[i];               \n28    });                                          \n29 \n30    t2 = chrono::high_resolution_clock::now();\n31 \n    < ... error checking ... >\n42    double time1 = chrono::duration_cast<\n                     chrono::duration<double> >(t2 - t1).count();\n43    cout << ""Runtime is  "" << time1*1000.0 << "" msecs "" << endl;\n44 }\nThe required changes for RAJA are to include the RAJA header file on line 2 and to\nchange the computation loop to a Raja::forall . You can see that the RAJA develop-\ners provide a low-entry threshold to gaining performance portability. To run the RAJA\ntest, we included a script that builds and installs RAJA as the following listing shows.\nThe script then goes on to build the stream triad code with RAJA and runs it.\nRaja/StreamTriad/Setup_Raja.sh\n1 #!/bin/sh\n2 export INSTALL_DIR=`pwd`/build/Raja\n3 export Raja_DIR=${INSTALL_DIR}/share/raja/cmake    \n4 \n5 mkdir -p build/Raja_tmp && cd build/Raja_tmp\n6 cmake ../../Raja_build -DCMAKE_INSTALL_PREFIX=${INSTALL_DIR}\n7 make -j 8 install && cd .. && rm -rf Raja_tmp\n8 \n9 cmake .. && make && ./StreamTriad     \nWe covered a lot of different programming languages in this chapter. Think of these\nas dialects of a common language rather than completely different ones. \n12.6 Further explorations\nWe have only begun to scratch the surface with all of these native GPU languages and\nperformance portability systems. Even with the initial functionality shown, you can\nbegin to implement some real application codes. If you’re serious about using any of\nthese in your applications, we strongly recommend availing yourself of the many addi-\ntional resources for the language of your choice.\n12.6.1 Additional reading\nAs the dominant GPU language for many years, there are many materials on CUDA\nprogramming. Perhaps the first place to go is the NVIDIA Developer’s website at\nhttps:/ /developer.nvidia.com/cuda-zone . There you’ll find extensive guides on install-\ning and using CUDA.Listing 12.26 Integrated build and run script for Raja stream triadRaja forall \nusing C++ \nlambda\nRaja_DIR points to \nRaja CMake tool.\nBuilds the stream triad \ncode and runs it\n458 CHAPTER  12 GPU languages: Getting down to basics\nThe book by Kirk and Hwu has been one of the go-to references on NVIDIA\nGPU programming:\nDavid B. Kirk and W. Hwu Wen-Mei, Programming massively parallel processors: a\nhands-on approach  (Morgan Kaufmann, 2016).\nAMD ( https:/ /rocm.github.io ) has created a website that covers all aspects of\ntheir ROCm ecosystem.\nIf you want to really learn more about OpenCL, we highly recommend the book\nby Matthew Scarpino:\nMatthew Scarpino, OpenCL in action: how to accelerate graphics and computations\n(Manning, 2011).\nA good source of additional information on OpenCL is https:/ /www.iwocl.org ,\nsponsored by the International Workshop on OpenCL (IWOCL). They also\nhost an international conference annually. SYCLcon is also hosted through the\nsame site.\nKhronos is the open standards body for OpenCL, SYCL, and related software.\nThey host the language specifications, forums, and resource lists:\nKhronos Group, https:/ /www.khronos.org/opencl/  and https:/ /www.khronos\n.org/sycl/.\nFor documentation and training materials on Kokkos, see their GitHub reposi-\ntory. Besides downloading the Kokkos software, you’ll also find a companion\nrepository ( https:/ /github.com/kokkos/kokkos-tutorials ) for the tutorials they\ngive around the country.\nThe RAJA team ( https:/ /raja.readthedocs.io ) has extensive documentation at\ntheir website.\n12.6.2 Exercises\n1Change the host memory allocation in the CUDA stream triad example to use\npinned memory (listings 12.1–12.6). Did you get a performance improvement?\n2For the sum reduction example, try an array size of 18,000 elements all initial-\nized to their index value. Run the CUDA code and then the version in Sum-\nReductionRevealed. You may want to adjust the amount of information printed.\n3Convert the CUDA reduction example to HIP by hipifying it.\n4For the SYCL example in listing 12.20, initialize the a and b arrays on the GPU\ndevice.\n5Convert the two initialization loops in the RAJA example in listing 12.24 to the\nRaja:forall  syntax. Try running the example with CUDA.\n459 Summary\nSummary\nUse straightforward modifications from the original CPU code for most ker-\nnels. This makes the writing of kernels simpler and easier to maintain.\nCareful design of cooperation and comparison in GPU kernels can yield good\nperformance. The key to approaching these operations is breaking down the\nalgorithm into steps and understanding the performance properties of the GPU.\nThink about portability from the start. You will avoid having to create more\ncode versions every time you want to run your application on another hardware\nplatform.\nConsider the single-source performance portability languages. If you need to\nrun on a variety of hardware, these can be worth the initial difficulty in code\ndevelopment.",5205
169-13.1 An overview of profiling tools.pdf,169-13.1 An overview of profiling tools,"460GPU profiling and tools\nIn this chapter, we will cover the tools and the different workflows that you can use\nto accelerate your application development. We’ll show you how profiling tools for\nthe GPU can be helpful. In addition, we’ll discuss how to deal with the challenges\nof using profiling tools when working on a remote HPC cluster. Because the profil-\ning tools continue to change and improve, we’ll focus on the methodology rather\nthan the details of any one tool. The main takeaway of this chapter will be under-\nstanding how to create a productive workflow when using the powerful GPU profil-\ning tools.\n13.1 An overview of profiling tools\nProfiling tools allow for quicker optimization, improving hardware utilization, and\na better understanding of the application performance and hotspots. We’ll discuss\nhow profiling tools expose bottlenecks and assist you in attaining better hardwareThis chapter covers\nAvailable profiling tools for the GPU\nA sample workflow for these tools\nHow to use the output from the GPU \nprofiling tools\n461 An overview of profiling tools\nusage. The following bulleted list highlights the commonly used tools in GPU profil-\ning. We specifically show the NVIDIA tools for use with their GPUs because these tools\nhave been around the longest. If you have a different vendor’s GPU on your system,\nsubstitute their tools in the workflow. Don’t forget about the standard Unix profiling\ntools such as gprof that we’ll use later in section 13.4.2. \n We encourage you to follow along with the examples for this chapter. The accompa-\nnying source code is at http:/ /github.com/EssentialsOfParallelComputing/Chapter13 ,\nwhich shows examples of installing the software packages for tools from different\nhardware vendors. There are detailed lists of all the software that can be installed\nfor each vendor. You will probably want to install the tools for the corresponding\nhardware. \nNOTE While a tool for another vendor might partially run on your system, its\nfull functionality will be crippled.\nNVIDIA nvidia-smi —When trying to get a quick system profile from the com-\nmand line, you can use nvidia-smi. As shown and explained in section 9.6.2,\nNVIDIA SMI (System Management Interface) allows for monitoring and col-\nlecting power and temperature during an application run. NVIDIA SMI gives\nyou hardware information along with many other system metrics. The link to\nthe SMI guide and options are in the “Further Explorations” section later in\nthis chapter. \nNVIDIA nvprof —This NVIDIA Visual Profiler command-line tool collects and\nreports data on GPU performance. The data can also be imported into a visual\nprofiling tool such as the NVIDIA Visual Profiler NVVP or other formats for\napplication performance analysis. It shows performance metrics such as hard-\nware-to-device copies, kernel usage, memory utilization, and many other metrics. \nNVIDIA NVVP —This NVIDIA Visual Profiler tool provides a visual representa-\ntion of the application kernel performance. NVVP provides a GUI and guided\nanalysis. It queries the same data the nvprof does, but represents the data to the\nuser in a visual way, offering a quick timeline feature not as readily available\non nvprof.\nNVIDIA® Nsight™ —NSight is an updated version of NVVP that provides for a\nvisual representation of CPU and GPU usage and application performance. Even-\ntually, it may replace NVVP.\nNVIDIA PGPROF —The PGPROF utility originated with the Portland Group com-\npiler. When the Portland Group was acquired by NVIDIA for their Fortran\ncompiler, they merged Portland’s profiler, PGPROF, with the NVIDIA tools.\nCodeXL (originally AMD CodeXL) —This GPUOpen profiler, debugger, and pro-\ngramming development workbench was originally developed by AMD. See the\nlink to the CodeXL website in the “Additional Reading” section later in this\nchapter.",3900
170-13.3 Example problem Shallow water simulation.pdf,170-13.3 Example problem Shallow water simulation,"462 CHAPTER  13 GPU profiling and tools\n13.2 How to select a good workflow\nBefore beginning any complicated task, you must select the appropriate workflow. You\nmight either be onsite with excellent connectivity, offsite with a slow home network, or\nsomewhere in between. Each case requires a different workflow. In this section, we’ll\ndiscuss four potential and efficient workflows for these different scenarios.\n Figure 13.1 provides a visual representation of the four different workflows. Acces-\nsibility and connection speed are the determining factors in deciding which method\nyou end up using. You can either run the tools with a graphics interface directly on\nthe system, remotely with a client-server mode, or just avoid the problem by using\ncommand-line tools. \nWhen using profiling tools from a remote server, there is often a heavy delay in visual-\nization and graphics interface response. Client-server mode separates the graphics\ninterface so that it runs locally on your system. It then communicates with the server\nat the remote site to run the commands. This helps keep the interactive response of\nthe graphical tool interface. For example, profiling tools such as NVVP can have a\nhigh latency when used on a remote server. Waiting minutes after every mouse click is\nnot a very productive situation. Fortunately, the NVIDIA tools and many of the other\ntools give you several options to work around this problem. We go into greater detail\non the different workflows in the following discussion.\nMethod 1: Run directly on the system —When your network connection for your\ngraphics application is fast, this is the preferred method because the storage\nrequirements are pretty large. If you have a fast connection for graphics display,\nit is the most efficient way to work. But if your display network connection is\nslow, the response time for the graphics window is painful, and you will want toDiskGPU\nclusterGPU\nclusterGPU\ncluster\nDisk Disk\nFirewall\nManual\ncopy\nGPU\nMethod : 1\nRun directlyMethod 2:\nRemote serverMethod 3:\nProﬁle downloadMethod 4:\nDevelop locallyDisk Disk\nFigure 13.1 There are several different methods of using the profiling tools that give you \nalternatives for your application development situation.\n463 Example problem: Shallow water simulation\nuse one of the remote options. VNC, X2Go, and NoMachine can compress the\ngraphics output and send it instead, sometimes making slower connections\nworkable.\nMethod 2: Remote server —This method runs the application with a command-line\ntool on the GPU system, then the files are transferred automatically to your\nlocal system. Firewalls, batch operations of the HPC system, and other network\ncomplications can make this method difficult or impossible to set up. \nMethod 3: Profile file download —This method runs nvprof on an HPC site and\ndownloads the files to your local computer. In this method, you manually\ntransfer files to your local computer using secure copy (scp) or some other\nutility and then work on your local machine. When trying to profile multiple\napplications, it can be easier to take the raw data in a csv format and combine\nit into a single dataframe. Though this method may no longer be usable by the\nconventional profiling tools, you can do your own detailed analysis on the server\nor locally. \nMethod 4: Develop locally —One of the great things about today’s HPC hardware is\nthat you often have similar hardware that you can use to develop an application\nlocally. You might have a GPU from the same vendor but not as powerful as the\nGPU in the HPC system. You can optimize your application with the expectation\nthat everything will be faster on the big system. You might also be able to develop\nyour code on the CPU with some of the languages where debugging is easier.\nThe important thing to realize is that even if you are not on a fast connection to a\ncomputing site, you have some options when using development tools. Whichever\nmethod you use to do your porting and performance analysis, you should ensure that\nthe versions of the software you use match. This is particularly important for CUDA\nand the NVIDIA nvprof and NVVP tools.\n13.3 Example problem: Shallow water simulation\nIn this section, we’ll work with a realistic example to show the code porting process\nand the use of some available tools. We’ll use the problem from figure 1.9, where a\nvolcanic eruption or earthquake might cause a tsunami to propagate outward. Tsuna-\nmis can travel thousands of miles across oceans with just a few feet of height, but when\nthese reach the shore, they can be hundreds of meters high. These types of simula-\ntions are usually done after the event because of the time required to set up and run\nthe problem. We’d prefer to simulate it in real time so that we can provide warnings to\nthose who might be affected. Speeding up the simulation by running it on a GPU\nmight provide this capability.\n We’ll first walk through the physics that occurs in this scenario then translate that\ninto equations to numerically simulate the problem. The specific scenario we want to\nrepresent is the breaking off of a large mass of an island or other land mass, which\nfalls into the ocean as figure 13.2 illustrates. This event actually happened with Anak\nKrakatau (“Child of Krakatau”) in December, 2018.\n464 CHAPTER  13 GPU profiling and tools\nFor the December event, the landslide volume on the west flank of the Krakatau\nisland was about 0.2 cubic km. This was smaller than earlier risk projections estimated.\nAdditionally, wave heights were estimated to be over 100 meters. With the short dis-\ntance from the source to the shore, there was little warning for those in the area, and\nwith over 400 deaths, the event garnered world-wide news coverage. \n Scientists performed many simulations prior to the event and even more after-\nward. You can view some of the visualizations and an analysis of the event at http:/ /\nmng.bz/4Mqw . How were the simulations done? The basic physics required is only a\nsmall step in complexity from the stencil calculations that we have looked at through-\nout this book. A full-fledged simulation code might have a lot more sophisticated bells\nand whistles, but we can go a long way with simple physics. So let’s take a look at the\nrequired physics behind the simulations.\n The mathematical equations for the tsunami are relatively simple. These are con-\nservation of mass and conservation of momentum. The latter is basically Newton’s first\nlaw of motion: “An object at rest stays at rest and an object in motion stays in motion.”\nThe momentum equation uses the second law of motion, “Force is equal to the\nchange in momentum.” For the conservation of the mass equation, we basically have\nthat the change in mass for a computational cell over a small increment in time is\nequal to the sum of the mass crossing the cell boundaries as shown here:\nGraphic courtesy of Cristian Gomez\nFigure 13.2 The tsunami wave that occurred at Anak Krakatau on December 22, 2018, was \ncaused by a sediment slide from the volcanic island.\n465 Example problem: Shallow water simulation\n \n    (Conservation of mass)\nWhere  is the change in mass relative to time, and  and  are the mass fluxes\n(velocity * mass) across the x- and y-faces. Further, because water is incompressible,\nthe density of water can be treated as constant. The mass of a cell is the volume * den-\nsity. If we have cells that are all 1 meter × 1 meter, the volume is height × 1 meter × 1\nmeter. Putting this all together, everything is constant except for height, so we can\nreplace mass with the height variable:\nMass = Volume · Density = Height · 1 Meter · 1 Meter · Density = Constant · Height\nAlso using u = vx and v = vy, we now get the standard form of the conservation law for\nthe shallow water equations:\n    (Conservation of mass)\nThe conservation of momentum is similar but with momentum ( mom) replacing the\nmass or height. We only show the x terms to fit the equation on the page like this: \n    (Conservation of x-momentum)\nThe additional term of 1/2 gh2 i s  d u e  t o  t h e  w o r k  d o n e  o n  t h e  s y s t e m  b y  g r a v i t y .\nAccording to Newton’s second law, the external force creates additional momentum\n(F = ma). We’ll look at how this term comes about with and without calculus. First, the\nacceleration in this case is gravity, and it causes a force acting on the column of water\nas figure 13.3 shows. Each additional meter of water height creates what is known as\nhydrostatic pressure , resulting in a higher pressure along the whole column of water.\nWith calculus, we would integrate the pressure along the column to get the momen-\ntum created. This integration over the elevation ( z) from 0 to the wave height ( h)\nwould be\n    (Integrate force over depth, z)M\nt--------vxM\nx----------------- -vyM\ny----------------- + + 0=\nM\nt--------vxM\nx----------vyM\nx--------- -\nh\nt----- -hu\nx--------------hv\ny------------- + + 0=\nmomx\nt---------------------\nx-----vxmomx  \nx-----1\n2-- -gh2\n++ 0=\npg z z d\n0h\n1\n2-- -gz2\n0h1\n2-- -gh2== =\nGravity\nforce\nFigure 13.3 The force of gravity on the column \nof water creates flow and momentum.\n466 CHAPTER  13 GPU profiling and tools\nThere is also a much simpler derivation. In this case, the pressure is a linear func-\ntion (figure 13.4). If we look at the height midpoint then apply the pressure differ-\nence at the height midpoint to the whole column, we can get the same solution.\nWhat we are doing is summing all of the pressure forces under the curve. The math-\nematical terminology for this is to integrate the function or perform a Riemann sum\nwhere you break the area under a curve into columns then add these. But this is all\noverkill. The area under the curve is a triangle, and we can use the area of a triangle\nor A = 1/2 bh.\n    (Using hydrostatic pressure at height midpoint)\nOur resulting set of equations is \n             (Conservation of mass)\n    (Conservation of x-momentum)\n    (Conservation of y-momentum)\nIf you are observant, you will notice cross-terms of the momentum fluxes for the\ny-momentum in the x-momentum equation and x-momentum in the y-momentum\nequation. In the conservation of x-momentum, the third term has x-momentum ( hu)DepthPressure as a function of depth\nPressure\n0200\n175\n150\n125\n100\n75\n50\n25\n0\n20 30 40 50 60Pressure\nFigure 13.4 The hydrostatic \npressure caused by the force \nof gravity is a linear function \nof depth.\npm g hmidpoint hgh\n2-- -1\n2-- -gh2== =\nh\nt----- -hu\nx--------------hv\ny------------- + + 0=\nhu\nt--------------\nx-----hu21\n2-- -gh2+ \ny-----huv ++ 0=\nhv\nt-------------\nx-----hvu\ny-----hv21\n2-- -gh2+++ 0=",10895
171-13.4 A sample of a profiling workflow.pdf,171-13.4 A sample of a profiling workflow,,0
172-13.4.1 Run the shallow water application.pdf,172-13.4.1 Run the shallow water application,"467 A sample of a profiling workflow\nmoving across the y-face with the y-velocity ( v). You can describe this as the advection,\nor flux, of the x-momentum with the velocity in the y-direction across the top and bot-\ntom faces of the computational cell. The flux of the x-momentum ( hu) across the x-\nfaces with the velocity u is in the second term as hu2. \n We also see that the newly created momentum is split across the two momentum equa-\ntions with the new x-momentum in the x-momentum equation and the y-momentum in\nthe y-momentum equation. These equations are then implemented as three stencil\noperations in our shallow water code, where for simplicity, we use   H = h, U = hu, and\nV = hv. Now we have a simple scientific application that we can use for our demonstra-\ntions.\n We have one more implementation detail. We use a numerical method that esti-\nmates the properties such as mass and momentum at the faces of each cell halfway\nthrough the timestep. We then use these estimates to calculate the amount of mass\nand momentum that moves into the cell during the timestep. This gives us a little\nmore accuracy for the numerical solution.\n Congratulations if you have worked your way through this discussion and gained\nsome understanding. Now you have seen how we take the simple laws of physics and\ncreate a scientific application from those. You should always strive to understand the\nunderlying physics and numerical method rather than treat the code as a set of loops.\n13.4 A sample of a profiling workflow\nNext, we reach the profiling step for the shallow water application. For this, we cre-\nated a shallow water application based on the mathematical and physical equations\npresented in section 13.3. In many ways, the code is just three stencil calculations\nfor the mass and two momentum equations. We have worked with a single, simple\nstencil equation since chapter 1, and the example code is included in https:/ /github\n.com/EssentialsofParallelComputing/Chapter13 .\n13.4.1 Run the shallow water application\nIn this section, we show you how to run the shallow water code. We’ll use the code to\nstep through a sample workflow for porting your code to the GPU. First, some notes\nabout the platforms:\nmacOS —NVIDIA warns that CUDA 10.2 may be the last release to support\nmacOS and only supports it up through macOS v10.13. As a result, NVVP is\nonly supported through macOS v10.13. It sort of works with v10.14 but fails\ncompletely on v10.15 (Catalina). We suggest using VirtualBox https:/ /www\n.virtualbox.org  as a free virtual machine to try out the tools on Mac systems. We\nhave also supplied a Docker container for macOS. \nWindows —NVIDIA still supports Microsoft Windows natively, but you can also\nuse VirtualBox or Docker containers on Windows if you prefer. \nLinux —A direct installation on most Linux systems should work.\n468 CHAPTER  13 GPU profiling and tools\nIf you have a GPU on your local system, you can use the local workflow. If not, you\nwill probably be running remotely on a compute cluster and transferring the files\nback for analysis. \n If you want to use the graphics, you will need to install some additional packages.\nOn an Ubuntu system, you can do this with the following commands. The first com-\nmand is for installing OpenGL and freeglut for real-time graphics. The second is for\ninstalling ImageMagick® to handle the graphics file output that we can use for graph-\nics stills. The graphics snapshots can also be converted into movies. The README\n.graphics file in the GitHub directory has more information on the graphics formats\nand the scripts in the examples that accompany this chapter.\nsudo apt-get install libglu1-mesa-dev freeglut3-dev mesa-common-dev -y\nsudo apt install cmake imagemagick libmagickwand-dev\nWe have found that real-time graphics can accelerate code development and debug-\nging, so we included a sample of how to use them in the example code accompanying\nthis chapter. For example, the real-time graphics output uses OpenGL to display the\nheight of the water in the mesh, giving you immediate visual feedback. The real-time\ngraphics code can also be easily extended to respond to keyboard and mouse interac-\ntions within the real-time graphics window.\n This example is coded with OpenACC, so it is best to use the PGI compiler. A lim-\nited subset of the examples works with the GCC compiler due to its still-developing\nsupport of OpenACC. Compiling the example code is straightforward. We just use\nCMake and make.\n1To build the makefile, type\nmkdir build && cd build\ncmake .. \n2To turn on the graphics, type\ncmake -DENABLE_GRAPHICS=1\n3Set the graphics file format with \nexport GRAPHICS_TYPE=JPEG\nmake\n4Then run the serial code with ./ShallowWater .\nIf you cannot get the graphics output to work, the program will run fine without it.\nBut if you get it set up correctly, the real-time graphics output from the code displays a\ngraphics window like that shown in figure 13.5. The graphics are updated every 100\niterations. The figure here shows a smaller mesh than the hard-coded size in the sam-\nple code. The lines represent the computational cells with the wave height higher on\n469 A sample of a profiling workflow\nthe left. The wave travels to the right with the height, decreasing as it moves. The wave\ncrosses the computational domain and reflects off the right face. Then it travels back\nand forth across the mesh. In a real calculation, there would be objects (such as shore-\nlines) in the mesh.\n If you have a system that can run OpenACC, the executables ShallowWater_par1\nthrough ShallowWater_par4 will also be built. You can use these for the profiling exer-\ncises that follow.Frame: 1 Sim cycle: 0 Sim time(s): 0\nFigure 13.5 Real-time graphics output from the shallow water application. The red \nstripes on the left indicate the beginning of the wave, where the landslide enters the \nwater. The wave progresses to the right as it cross the ocean: orange, yellow, green, \nand blue. If you’re reading this in black and white, the left shaded region corresponds \nto the red, and the far right shaded region corresponds to the blue. The lines are the \noutlines of the computational cells.",6251
173-13.4.3 Add OpenACC compute directives to begin the implementation step.pdf,173-13.4.3 Add OpenACC compute directives to begin the implementation step,"470 CHAPTER  13 GPU profiling and tools\n13.4.2 Profile the CPU code to develop a plan of action\nWe described the parallel development cycle back in chapter 2 as\n1Profile\n2Plan\n3Implement\n4Commit\nThe first step is to profile our application. For most applications, we recommend\nusing a high-level profiler such as the Cachegrind tool we introduced in section 3.3.1.\nCachegrind shows the most time-consuming paths through the code and displays the\nresults in an easy-to-interpret visual representation. However, for a simple program\nlike the shallow water application, function-level profilers like Cachegrind are not\neffective. Cachegrind shows that 100% of the time is spent in the main function,\nwhich doesn’t help us much. We need a line-by-line profiler for this particular situa-\ntion. For this purpose, we draw upon the most well-known profiler on Unix systems—\ngprof. Later, when we have code that runs on the GPU, we will use the NVIDIA NVVP\nprofiling tool to get the performance statistics. To get started, we just need a simple\ntool to profile an application running on the CPU.\nExample: Profiling with gprof\n1Edit CMakeLists.txt by adding the -pg flag to the compiler flags (diff output shows\nthe original line in the CMakeLists with a - symbol and the new line with a + symbol):\n-set(CMAKE_C_FLAGS ""${CMAKE_C_FLAGS} -g -O3"")\n+set(CMAKE_C_FLAGS ""${CMAKE_C_FLAGS} -g -O3 -pg"")\n2Edit ShallowWater.c and increase the mesh size:\n-  int      nx = 500, ny = 200;\n+  int      nx = 5000, ny = 2000;\n3Rebuild the ShallowWater executable by typing make.\n4Run the ShallowWater executable by typing ./ShallowWater . You should get an\noutput file called gmon.out.\n5Run the post-processing step by typing gprof  -l -pg ./ShallowWater .\nThe output from gprof shows the loops that take the most time in the shallow water\napplication (see the following figure).\nEach sample counts as 0.0 seconds. 1\n< ... more output ...>%\ntimecumulative\nsecondsself\nsecondsself\ncallstotal\nTs/call Ts/call name\n42.95\n22.44\n22.34\n12.06140.38\n213.71\n286.74\n326.17140.38\n73.33\n73.03\n39.43main (ShallowWater.c: @ 401885) 207\nmain (ShallowWater.c: @ 401730) 190\nmain (ShallowWater.c: @ 401500) 172\nmain (ShallowWater.c: @ 401330) 160The output from gprof. \nThe loop at line 207 \ntakes the most time \nand would make a \ngood starting point for \nporting to the GPU.\n471 A sample of a profiling workflow\n13.4.3 Add OpenACC compute directives to begin \nthe implementation step\nNow that we have profiled the application and developed a plan, the next step in the\nparallel development cycle is to begin the implementation of the plan. In this step, we\nbegin the eagerly awaited modification of the code.\n The implementation starts with porting the code to the GPU by moving the com-\nputation loop. We follow the same procedure used in section 11.2.2 to port the code\nto the GPU. The computations are moved by inserting the acc parallel  loop  pragma\nin front of every loop as shown on line 95 in the following listing. \nOpenACC/ShallowWater/ShallowWater_par1.c\n 95 #pragma acc parallel loop\n 96       for(int j=1;j<=ny;j++){\n 97         H[j][0]=H[j][1];\n 98         U[j][0]=-U[j][1];\n 99         V[j][0]=V[j][1];\n100         H[j][nx+1]=H[j][nx];\n101         U[j][nx+1]=-U[j][nx];\n102         V[j][nx+1]=V[j][nx];\n103       }\nWe also need to replace the pointer swap on line 191 at the end of the loop with a data\ncopy. This is not ideal because it introduces more data movement and is slower than a\npointer swap. That being said, doing a pointer swap in OpenACC is tricky because the\npointers on the host and device have to be switched simultaneously.\nOpenACC/ShallowWater/ShallowWater_par1.c\n  189       // Need to replace swap with copy                           \n  190 #pragma acc parallel loop                          We look up the loops for each of the line numbers in the profiling output in the figure\nand find that these correspond to the following operations:\nShallowWater.c:207  (second pass loop)\nShallowWater.c:190  (y-face pass)\nShallowWater.c:172  (x-face pass)\nShallowWater.c:160  (timestep calculation)\nThis tells us that we should concentrate our initial efforts on the computation of the sec-\nond pass at the end of the main computation loop and work our way towards the top of\nthe loop. There is a tendency to try and do everything all at once, but the safer approach\nis to work loop-by-loop and make sure the result is still correct. By focusing on the most\nexpensive loops first, some performance improvement will be achieved earlier.\nListing 13.1 Adding a loop directive \nListing 13.2 Replacing the pointer swap with a copy\n472 CHAPTER  13 GPU profiling and tools\n  191       for(int j=1;j<=ny;j++){                          \n  192         for(int i=1;i<=nx;i++){\n  193            H[j][i] = Hnew[j][i];\n  194            U[j][i] = Unew[j][i];\n  195            V[j][i] = Vnew[j][i];\n  196         }\n  197       }\nYou will get better feedback of the performance of your application from a visual rep-\nresentation. At each step of the process, we run the NVVP profiling tool to get the\ngraphical output of the performance trace. \nExample: Getting a visual profile of your performance with the NVIDIA \nVisual Profiler (NVVP)\nTo acquire a visual performance timeline, we run the code with: \nnvprof --export-profile ShallowWater_par1_timeline.prof \n./ShallowWater_par1\nUsing the nvprof  command saves a profiling timeline within the running directory:\nnvvp ShallowWater_par1_timeline.prof\nThe nvvp command then imports the profile into the NVIDIA Visual Profiler suite with\nthe graphical output shown in the following figure. You can copy the profile back to\nyour local machine in between the two steps and view it locally if you like. \nWe’ll first look at the visual profile to get a quick color-coded feel for the relative per-\nformance of our memory copies and computational kernels. This is the timeline\nshown at the top of the visual profiler window. At this point, pay particular attention\nto the lines MemCpy (HtoD) and MemCpy (DtoH), where the data transfer from the\nhost to the device and the device to host are displayed. The guided analysis and\nOpenACC details panes that are at the bottom of this window are discussed in sec-\ntion 13.4.5.\nThe NVIDIA NVVP profiler output shows a timeline view of one computational cycle. You can see \nthe device-to-hardware memory copies and vice versa. On the highlighted line, the output also \nshows compute regions.\nHardware to device Device to hardware",6613
174-13.4.5 Guided analysis can give you some suggested improvements.pdf,174-13.4.5 Guided analysis can give you some suggested improvements,"473 A sample of a profiling workflow\nFigure 13.6 shows the ability to zoom into specific kernels to better identify perfor-\nmance metrics within certain compute cycles. Specifically, we zoomed into line 95\nfrom listing 13.1 to show individual memory copies.\n13.4.4 Add data movement directives\nThe next step in porting the code to the GPU is the addition of data movement direc-\ntives. This allows us to further improve the application performance by eliminating\nexpensive memory copies. In this section, we will show you how it’s done.\n The Visual Profiler, NVVP, helps us to see where we need to focus our efforts. Start\nby looking for the large MemCpy time blocks and eliminating these one-by-one. As\nyou remove the data transfer costs, your code will start to show speedups, recovering\nthe performance lost during the application of compute directives in section 13.4.4. \n In listing 13.3, we show an example of the data movement directives that we added.\nAt the start of the data section, we use the acc enter  data  create  directive to start a\ndynamic data region. The data will then exist on the device until we encounter an acc\nexit  data  directive. For each loop, we add the present  clause to tell the compiler the\ndata is already on the device. Refer to the example code for the chapter 13 in the fileIf your network connection doesn’t allow either using the graphical tool directly or\ntransferring the profile data to your computer, you can always fall back to using nvprof\nin its text-based mode. You can get the same information from the text-based output,\nbut there are always some insights that are clearer with the visual representation.\nA memory copy\nthat takes a lot\nof time\nFigure 13.6 With NVIDIA’s NVVP, you can zoom into specific copies in the timeline view. Here, you can see a \nzoomed in version of individual memory copies within each cycle. This allows you to see what lines these are \non to help you easily refer back to the application.\n474 CHAPTER  13 GPU profiling and tools\nOpenACC/ShallowWater/ShallowWater_par2.c for all the changes made to control\nthe data movement.\nOpenACC/ShallowWater/ShallowWater_par2.c\n51 #pragma acc enter data create( \\n52         H[:ny+2][:nx+2],    U[:ny+2][:nx+2],    V[:ny+2][:nx+2], \\n53         Hx[:ny][:nx+1],     Ux[:ny][:nx+1],     Vx[:ny][:nx+1],  \\n54         Hy[:ny+1][:nx],     Uy[:ny+1][:nx],     Vy[:ny+1][:nx],  \\n55         Hnew[:ny+2][:nx+2], Unew[:ny+2][:nx+2], Vnew[:ny+2][:nx+2])\n   <...>\n59   #pragma acc parallel loop present( \\n60         H[:ny+2][:nx+2], U[:ny+2][:nx+2], V[:ny+2][:nx+2])\nApplying the data movement directives from listing 13.3 and rerunning the profiler\ngives us the new performance results in figure 13.7, where you can see the reduction\nof data movement. By reducing the data transfer time, the overall run time of the\napplication is much faster. In a larger application, you should continue looking for\nother data transfer operations that you can then eliminate to speed up the code even\nmore.\n13.4.5 Guided analysis can give you some suggested improvements\nFor further insight, NVVP provides a guided analysis feature (figure 13.8). In this sec-\ntion, we’ll discuss how to use this feature.\n You must judge the suggestions from the guided analysis based on your knowledge\nof your application. In our example, we have few data transfers, so we will not be able\nto get memory copy and compute overlap mentioned in the top suggestion of Low\nMemcpy/Compute Overlap in figure 13.8. This is true of most of the other suggestions.Listing 13.3 Data movement directives\nPrevious data copies are gone\nFigure 13.7 This timeline from NVIDIA’s Visual Profiler NVVP shows four iterations of the computation but now \nwith data movement optimizations. What is interesting in this figure is not so much what you can see, but what \nis not there. The data movement that occurred in the previous figure is sharply reduced or no longer exists.\n475 A sample of a profiling workflow\nFor example, for low kernel concurrency, we only have one kernel, so we can’t have\nconcurrency. Though our application is small and may not need these extra optimiza-\ntions, these are good to note as they can be useful for larger applications.\nAdditionally, figure 13.8 shows low compute utilization for our application run. This is\nnot unusual. This low GPU utilization is more indicative of the huge compute power\navailable on the GPU and how much more it can do. To briefly go back to the perfor-\nmance measurements and analysis of our mixbench performance tool (section 9.3.4),\nwe have a bandwidth-limited kernel so we will, at best, use 1–2% of the GPU’s floating-\npoint capability. In light of this, 0.1% compute utilization isn’t so bad.\n Another feature of the NVVP tool is an OpenACC Details window that gives the\ntimings for each operation. One of the best ways to use this is by acquiring the before\nand after timings as figure 13.9 shows. The side-by-side comparisons give you a con-\ncrete measurement of improvement from the data movement directives.\n With the OpenACC Details window opened, you’ll note that the line numbers\nmove within the profile. If we look at line 166 in the ShallowWater_par1 listing (on\nthe left in figure 13.10), it takes 4.8% of the run time. The breakdown of the opera-\ntions shows that a lot of that time is due to data transfer costs. The corresponding line\nof code in the ShallowWater_par2 listing is 181 (on the right in figure 13.10) and has\nthe addition of the present  data clause. We can see that the time for line 181 is only\n0.81% and that this is largely due to the elimination of the data transfer costs. The\ncompute construct takes about the same time in both cases at 0.16 ms as shown in the\nline labeled acc_compute_construct just below the highlighted line.\n \n \n \nFigure 13.8 NVVP provides a guided analysis section as well. Here, the user can acquire insight for further \noptimizations. Note that the highlighted region shows low compute utilization.",6057
175-13.4.6 The NVIDIA Nsight suite of tools can be a powerful development aid.pdf,175-13.4.6 The NVIDIA Nsight suite of tools can be a powerful development aid,"476 CHAPTER  13 GPU profiling and tools\n13.4.6 The NVIDIA Nsight suite of tools can be a powerful \ndevelopment aid\nNVIDIA is replacing their Visual Profiler tools (NVVP and nvprof) with the Nsight™\ntool suite. The tool suite is anchored by two integrated development environments\n(IDEs):\nFigure 13.9 NVVP’s OpenACC Details window shows information on each OpenACC kernel and the cost of \neach operation. We can see the cost of the data transfer in the left window for version 1 of the code versus \nthe time for the optimized data motion in version 2 on the right.\nFigure 13.10 Side-by-side code comparison showing that line 166 in version 1 of the ShallowWater code is now \nline 181, which has the additional present  clause.\n477 A sample of a profiling workflow\n1Nsight Visual Studio Edition supports CUDA and OpenCL development in the\nMicrosoft Visual Studio IDE.\n2Nsight Eclipse Edition adds the CUDA language to the popular open source\nEclipse IDE.\nFigure 13.11 shows our shallow water application in the Nsight Eclipse Edition devel-\nopment tool.\nThe Nsight suite of tools also has single function components that can be downloaded\nby registered NVIDIA developers. These profilers incorporate the functionality from\nthe NVIDIA Visual Profiler and add additional capabilities. The two components are\nNsight Systems, a system-level performance tool, looks at overall data move-\nment and computation.\nNsight Compute, a performance tool, gives a detailed view of GPU kernel\nperformance.\nFigure 13.11 The NVIDIA Nsight Eclipse Edition application is a code development tool. This window in the tool \nshows the ShallowWater_par1 application.",1668
176-13.5 Dont get lost in the swamp Focus on the important metrics.pdf,176-13.5 Dont get lost in the swamp Focus on the important metrics,"478 CHAPTER  13 GPU profiling and tools\n13.4.7 CodeXL for the AMD GPU ecosystem\nAMD also has code development and performance analysis capabilities in their\nCodeXL suite of tools. As figure 13.12 shows, the application development tool is a\nfull-featured code workbench. CodeXL also includes a profiling component (in the\nProfile menu) that helps with optimizing code for the AMD GPU.\nThese new tools from NVIDIA and AMD are still being rolled out. The availability of\nfull-featured tools, including debuggers and profilers, will be a tremendous boost for\nGPU code development.\nFigure 13.12 The CodeXL development tool supports compiling, running, debugging, and profiling.",682
177-13.5.2 Issue efficiency Are your warps on break too often.pdf,177-13.5.2 Issue efficiency Are your warps on break too often,"479 Don’t get lost in the swamp: Focus on the important metrics\n13.5 Don’t get lost in the swamp: Focus on \nthe important metrics\nAs with many profiling and performance measurement tools, the amount of informa-\ntion is initially overwhelming. You should focus on the most important metrics that\nyou can gather from hardware counters and other measurement tools. In recent pro-\ncessors, the number of hardware counters has steadily grown, giving you insight into\nmany aspects of processor performance that were previously hidden. We suggest the\nfollowing three aspects as the most critical: occupancy, issue efficiency, and memory\nbandwidth. \n13.5.1 Occupancy: Is there enough work?\nThe concept of occupancy is often mentioned as the top concern for GPUs. We first\ndiscussed this measure in section 10.3. For good GPU performance, we need enough\nwork to keep compute units (CUs) busy. In addition, we need alternate work to cover\nstalls when workgroups hit memory-load waits (figure 13.13). As a reminder, CUs in\nOpenCL terminology are called streaming multiprocessors (SMs) in CUDA. The\nactual achieved occupancy is reported by the measurement counters. If you encoun-\nter low occupancy measures, you can modify the workgroup size and resource\nusage in the kernels to try and improve this factor. A higher occupancy is not always\nbetter. The occupancy just needs to be high enough so that there is alternate work\nfor the CUs.\nWork\nCU CU CU CU CU CU CU CUWorkStall\nWork WorkStall\nWork WorkStall\nWork WorkStall\nWork\nWe have eight CUs and only one chunk of work.\nSeven CUs go unused. This is very low occupancy.We break up the work into eight small sets. Now all\nof the CUs have work. But we also show that four\nchunks have a data load stall for reading the data\nfrom the main memory of the GPU.\nInstead of just eight sets of work, we break up the work into sixteen smaller sets so that each CU has two\nchunks of work. Now when a CU has a stall on a chunk of work, it just switches to another chunk of work.CU CU CU CU CU CU CU CU\nWorkStall\nWork WorkStall\nWork WorkStall\nWork WorkStall\nWork WorkStall\nWork WorkStall\nWork WorkStall\nWork WorkStall\nWork\nCU CU CU CU CU CU CU CU\nFigure 13.13 GPUs have a lot of compute units (CUs), also called streaming multiprocessors (SMs). We \nneed to create a lot of work to keep the CUs busy, with enough extra work for handling stalls.",2413
178-13.5.3 Achieved bandwidth It always comes down to bandwidth.pdf,178-13.5.3 Achieved bandwidth It always comes down to bandwidth,,0
179-13.6 Containers and virtual machines provide alternate workflows.pdf,179-13.6 Containers and virtual machines provide alternate workflows,,0
180-13.6.1 Docker containers as a workaround.pdf,180-13.6.1 Docker containers as a workaround,"480 CHAPTER  13 GPU profiling and tools\n13.5.2 Issue efficiency: Are your warps on break too often?\nIssue efficiency  is the measurement of the instructions issued per cycle versus the maxi-\nmum possible per cycle. To be able to issue an instruction, each CU scheduler must\nhave an eligible wavefront, or warp, ready for execution. An eligible wavefront  is an\nactive wavefront that is not stalled. In some sense, it is an important result from having\nhigh enough occupancy so that there are lots of active wavefronts. The instructions\ncan be floating point, integer or memory operations. Poorly written kernels with lots\nof stalls cause low-issue efficiency even if the occupancy is high. There are a variety of\nreasons for kernels to encounter stalls. There are also counters that can identify par-\nticular reasons for stalls. Some of the possibilities are\nMemory dependency —Waiting on a memory load or store\nExecution dependency —Waiting on a previous instruction to complete\nSynchronization —Blocked warp due to a synchronization call\nMemory throttle —Large number of outstanding memory operations\nConstant miss —Miss in the constants cache\nTexture busy —Fully utilized texture hardware\nPipeline busy —Compute resources not available\n13.5.3 Achieved bandwidth: It always comes down to bandwidth\nBandwidth is an important metric to understand because most applications are band-\nwidth limited. The best starting point is to look at the bandwidth measure. There are\nmany memory counters available, allowing you to go as deep as you want. Comparing\nyour achieved bandwidth measurements to the theoretical and measured bandwidth\nperformance for your architecture from sections 9.3.1 through 9.3.3 can give you an\nestimate on how well your application is doing. You can use the memory measure-\nments to determine whether it would be helpful to coalesce memory loads, to store\nvalues in the local memory (scratchpad), or to restructure code to reuse data values.\n13.6 Containers and virtual machines provide \nalternate workflows\nYou are on a flight from somewhere to nowhere and just want to get some of your\nGPU code working. The latest software release doesn’t work on your company-issued\nlaptop. The workaround is to use a container or a virtual machine (VM) to run a dif-\nferent operating system or a different compiler version. \n13.6.1 Docker containers as a workaround\nEach of our chapters has an example Dockerfile and instructions for its use. The\nDockerfile contains the commands to build a basic OS and then to install the neces-\nsary software that it needs.\n \n481 Containers and virtual machines provide alternate workflows\nA Docker container is useful for dealing with software that does not work on your\noperating system. For example, for software that only runs on Linux, you can install a\ncontainer on your Mac or Windows laptop that runs Ubuntu 20.0.4. Using a container\nworks well for text-based, command-line software. \n Containers also limit access to hardware devices such as GPUs. One option is to\nrun the device kernels on the CPU for the GPU languages that have that capability.\nDoing this, we can at least test our software. If that is not enough for our needs, we can\ntackle some additional steps to try and get the graphics and GPU computation work-\ning. We’ll start by looking at getting the graphics working. Running a graphical inter-\nface from a Docker build takes a little more effort. Example: Building a Docker image with the supplied Dockerfile\nThe top-level directory for most chapters has a Dockerfile. In the directory for the\nchapter you are interested in, run the Docker build  command and create a Docker\ncontainer using the -t option to name it. In this example, we build the chapter 2\nDocker container and name it chapter2 : \ndocker build -t essentialsofparallelcomputing/chapter2 .\nNow run the Docker container with the following command:\ndocker run -it --entrypoint /bin/bash \nessentialsofparallelcomputing/chapter2\nAlternatively, use\n./docker_run.sh\nSome chapters have both a text-based and a graphics-based Dockerfile. To enable\nthe text-based file, remove the Dockerfile and link in the text-based version with this\ncommand: \nln -s Dockerfile.Ubuntu20.04 Dockerfile\nExample: Running the Docker image with a GUI on macOS\nMac laptops do not have an X Window client built into their standard software. There-\nfore, you need to install an X Window client on your Mac if you have not done this\nalready. The XQuartz package is an open source version of the original X Window that\nwas included on older versions of macOS. You can install it with the brew package\nmanager like this:\nbrew cask install xQuartz\n482 CHAPTER  13 GPU profiling and tools\nFor chapters that require a GUI for tools or plots, the instructions are a little differ-\nent. We use the Virtual Network Computing (VNC) software to enable the graphics\ncapabilities through a web interface and VNC client viewers. You must use the dock-\ner_run.sh script to start the VNC server, then you need to start a VNC client on your\nlocal system. You can use one of a variety of VNC client packages, or you can open\nthe graphics file through some browsers with the following in your browser toolbar\nsite name: \nhttp:/  /localhost:6080/vnc.html?resize=downscale&autoconnect=1&password=\n  <password>""\nTo test an application with a graphical interface such as NVVP, type nvvp . Or you\nmight want to test the graphics with a simple X Window application such as xclock or\nxterm. We can also try to get access to the GPUs for computation. Access to the GPUs\ncan be obtained by using the --gpus  option or the older --device=/dev/<device\nname> . The option is a relatively new addition to Docker and, currently, is only imple-\nmented for NVIDIA GPUs.\nMost of the chapters have prebuilt Docker containers. You can access the containers\nfor each chapter at https:/ /hub.docker.com/u/essentialsofparallelcomputing . You\ncan retrieve the container for a chapter with the following command:\ndocker run -p 4000:80 -it --entrypoint /bin/bash \nessentialsofparallelcomputing/chapter2(continued)\nNow start XQuartz and look for the XQuartz menu bar at the top of the screen. If you\ndon’t see it, you might need to also start a sample X Window application such as\nxterm by using a right-click on the XQuartz icon. Then \n1Select the XQuartz menu bar and then the Preferences option \n2Go to the Security tab and select Allow Connections from Network Clients \n3Reboot your system to apply the settings. \n4Start XQuartz again\nExample: Accessing GPUs for computational work\nTo access a GPU for computation, add the --gpus  option with an integer argument\nfor the number of GPUs (gpus) to make available or all for all of the GPUs:\ndocker run -it --gpus all --entrypoint /bin/bash chapter13\nFor Intel GPUs, you can try\ndocker run -it --device=/dev/dri --entrypoint /bin/bash chapter13",6963
181-13.6.2 Virtual machines using VirtualBox.pdf,181-13.6.2 Virtual machines using VirtualBox,"483 Containers and virtual machines provide alternate workflows\nThere is also a prebuilt Docker container from NVIDIA that you can use as a starting\npoint for your own Docker images. Visit the site at https:/ /github.com/NVIDIA/\nnvidia-docker  for up-to-date instructions. There is another site at NVIDIA with sub-\nstantial container varieties at https:/ /ngc.nvidia.com/catalog/containers . For ROCm,\nthere are extensive instructions on Docker containers at https:/ /github.com/Rade\nonOpenCompute/ROCm-docker . And Intel has a site for how to set up their oneAPI\nsoftware in containers at https:/ /github.com/intel/oneapi-containers . Some of their\nbase containers are large and require a good internet connection.\n The PGI compiler is important for OpenACC code development and some other\nGPU code development challenges as well. If you need the PGI compiler for your work,\nthe container site for PGI compilers is at https:/ /ngc.nvidia.com/catalog/containers/\nhpc:pgi-compilers . As you can see from the sites mentioned here, there are many\nresources for creating work environments with Docker containers. But this is also a\nrapidly evolving capability.\n13.6.2 Virtual machines using VirtualBox\nUsing a virtual machine (VM) allows the user to create a guest OS within their own\ncomputer. The normal operating system is called a host, and the VM is called the\nguest. You can have more than one VM running as a guest. VMs use a more restrictive\nenvironment for the guest operating system than exists in the container implemen-\ntations. Often, it is easier to set up GUIs in comparison to containers. Unfortunately,\naccess to the GPU for computation is difficult or impossible. You might find VMs\nuseful for GPU languages that have an option supporting computation on the host\nCPU processor. \n Let’s look at the process of setting up an Ubuntu guest operating system in Virtual-\nBox. This example sets up the shallow water example running on the CPU with the\nPGI compiler in VirtualBox with graphics.\nExample: Setting up an Ubuntu guest OS in VirtualBox\nTo set up your system for VirtualBox \n1Download VirtualBox for your system and install\n2Download the Ubuntu desktop and save it to your local disk \n[ubuntu-20.04-desktop-amd64.iso]\n3Download the VBoxGuestAdditions.iso file, which may already be included in the\nVirtualBox download\nNext, we set up the Ubuntu guest system. An automated script, autovirtualbox.sh, is\nincluded in the examples for this chapter to automate setting up the Ubuntu guest sys-\ntem in VirtualBox at https://github.com/EssentialsOfParallelComputing/Chapter13.git .\nMost of the other chapters have similar scripts. To set up the Ubuntu guest system,\nfollow this process:\n484 CHAPTER  13 GPU profiling and tools\nNow we are ready to install Ubuntu. The process is the same as setting up an Ubuntu\nsystem on your desktop.(continued)\n1Start VirtualBox and click New\n2Type in a name (chapter13, for example)\n3Select Linux and then Ubuntu 64-bit\n4Select the amount of memory (8192, for example)\n5Create a virtual hard disk\n6Select VDI VirtualBox Disk Image\n7Select Fixed Size Disk\n8Select 50 GB\nYour new virtual machine should now be added to the list. \nExample: Installing Ubuntu \nTo install Ubuntu, follow these steps:\n1Start the Ubuntu VM by clicking the green start arrow\n2Select the iso file saved earlier by typing to ubuntu-20.04-desktop-amd64.iso\nfrom the options presented\n3Select Install Ubuntu\n4Select your keyboard and click Continue\n5Select Minimal Install, download updates, and install third-party software, then\nclick Continue\n6Select Erase Disk and install Ubuntu and then click Install and select a time zone\n7Type the following into the text boxes: your name (chapter13, for example), your\ncomputer's name (chapter13-virtualbox), username (chapter13), and password\n(chapter13)\n8Select Require My Password to Log In and then select Continue\nTime to get some coffee. When the installation is complete, restart your computer\nand follow these steps:\n1Sign back in\n2Click through What’s New\n3Select the dots at bottom left and start a terminal\n4Edit Sudo Authorized Users configuration file with \nsudo -i\nvisudo\nand add the following in any blank line, %vboxsf  ALL=(ALL)  ALL, then exit",4294
182-13.8 Further explorations.pdf,182-13.8 Further explorations,"485 Cloud options: A flexible and portable capability\nThere are instructions for setting up virtual machines with the examples for each\nchapter. For this chapter, log back in and install the chapter examples:\ngit clone --recursive https:/  /github.com/essentialsofparallelcomputing/\nChapter13.git\ncd Chapter13 && sh -v README.virtualbox\nThe commands in the README.virtualbox file install the software, and build and run\nthe shallow water application. The real time graphics output should also work. You\ncan also try the nvprof utility to profile the shallow water application as well.\n13.7 Cloud options: A flexible and portable capability\nWhen access to a specific GPU is limited (no supercomputer, laptop or desktop GPU, or\nremote server), you can make use of cloud computing.1 Cloud computing refers to serv-\ners provided by large data centers. While most of these services are for more general\nusers, some sites catering towards HPC-style services are beginning to appear. One of\nthese sites is http:/ /mng.bz/Q2YG . The Fluid Numerics Cloud cluster (fluid-slurm-gcp)\nsetup on the Google Cloud Platform (GCP) has the Slurm batch scheduler and MPI.\nNVIDIA GPUs can be scheduled as well. Getting started can be a bit complicated.\nThe Fluid Numerics site has some information to help with that process at http:/ /mng\n.bz/XYwv . \n The advantages of having hardware resources available on demand is often com-\npelling. Google Cloud offers a $300 trial credit that should be more than sufficient for\nexploring the service. There are other cloud providers and add-on services that can\nprovide exactly what you need, or you can customize the environment yourself. IntelYou may need to wait for updates or reboot and sign back in. Once logged back in\ninstall basic build tools with sudo apt install  build-essential  dkms git -y. Then\n1Make the VirtualBox window active and select the Devices pull-down menu from\nthe window’s menus at top of screen \n2Set the Shared Clipboard option to Bidirectional\n3Set the Drag and Drop option to Bidirectional\n4Install the guest additions by selecting the menu option virtualbox-guest-addi-\ntions-iso \n5Remove the optical disk: from the desktop, right-click and eject the device or in\nthe VirtualBox window, select Devices > Optical Disk and remove the disk from\nthe virtual drive\n6Reboot and test by copying and pasting (copy on the Mac is Command-C and\npaste in Ubuntu is Shift-Ctrl-v)\nYour Ubuntu guest system is now ready for downloading and installing software. \n1See the README.cloud file in the examples for this chapter for the latest information on using the cloud.",2642
183-13.8.2 Exercises.pdf,183-13.8.2 Exercises,"486 CHAPTER  13 GPU profiling and tools\nhas set up a cloud service for testing out Intel GPUs so that developers have access to\nboth software and hardware for their oneAPI initiative and their DPCPP compiler that\nprovides a SYCL implementation. You can try it out by going to https:/ /software.intel\n.com/en-us/oneapi  and registering to use it.\n13.8 Further explorations\nIncorporating a workflow and development environment is especially important for\nGPU code development. With the great variety of possible hardware configurations,\nthe examples presented in this chapter will likely require some customization for your\nsituation. Indeed, the configuration and setup of development systems is one of the\nchallenges of GPU computing. You may even find that it is easier to use one of the pre-\nbuilt Docker containers rather than figure out the process to configure and install\nsoftware on your system.\n We also suggest checking the most recent documentation relevant to your needs\nfrom the additional reading suggested in section 13.8.1. The tools and workflows are\nthe fastest changing aspects of GPU programming. While the examples in this chapter\nwill be generally relevant, the details are likely to change. Much of the software is so\nnew that documentation on its use is still being developed.\n13.8.1 Additional reading\nThe NVIDIA installation manual has some information on installing CUDA tools\nusing a package manager at:\nhttps:/ /docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#pack\nage-manager-installation\nNVIDIA has a couple of resources on their profiling tools and the transition from\nNVVP to the Nsight tool suite at the following sites: \nNVIDIA NSight Guide at https:/ /docs.nvidia.com/nsight-compute/Nsight\nCompute/index.html#nvvp-guide\nNVIDIA profiling tool comparison at https:/ /devblogs.nvidia.com/migrat\ning-nvidia-nsight-tools-nvvp-nvprof/\nOther tools include the following:\nCodeXL has been released as open source under the GPUopen initiative. AMD\nhas also removed its AMD brand from the tool to promote cross-platform\ndevelopment. For more information, see https:/ /github.com/GPUOpen-Tools/\nCodeXL .\nNVIDIA has a GPU Cloud with resources such as the PGI compilers in a con-\ntainer at https:/ /ngc.nvidia.com/catalog/containers/hpc:pgi-compilers .\nAMD also has a webpage on setting up virtualization environments and con-\ntainers. The virtualization instructions include a passthrough technique to get",2483
184-Summary.pdf,184-Summary,"487 Summary\naccess to the GPU for computation. You find this information at http:/ /mng\n.bz/MgWW\n13.8.2 Exercises\n1Run nvprof on the stream triad example. You might try the CUDA version from\nchapter 12 or the OpenACC version from chapter 11. What workflow did you\nuse for your hardware resources? If you don’t have access to an NVIDIA GPU,\ncan you use another profiling tool?\n2Generate a trace from nvprof and import it into NVVP. Where is the run time\nspent? What could you do to optimize it?\n3Download a prebuilt Docker container from the appropriate vendor for your sys-\ntem. Start up the container and run one of the examples from chapter 11 or 12.\nSummary\nImproving performance is a high priority for scientific and big data applica-\ntions. Performance tools can help you get the most out of your GPU hardware.\nThere are many profiling tools available for GPU programming. You should try\nout the many new and emerging capabilities that are available.\nWorkflows are essential for efficient GPU code development. Explore what\nworks for you in your environment and the available GPU hardware.\nThere are workarounds through the use of containers, virtual machines, and\ncloud computing to handle incompatibilities, computing needs, and access to\nGPU hardware. These workarounds give access to a large sampling of GPU ven-\ndor hardware that might not otherwise be available.",1399
185-Part 4High performance computing ecosystems.pdf,185-Part 4High performance computing ecosystems,"Part 4\nHigh performance\ncomputing ecosystems\nW ith today’s high performance computing (HPC) systems, it is not enough\nfor you to just learn parallel programming languages. You also need to under-\nstand many aspects of the ecosystem including the following:\n■Placing and scheduling your processes for better performance\n■Requesting and scheduling resources using an HPC batch system\n■Writing and reading data in parallel on parallel file systems\n■Making full use of the tools and resources to analyze performance and\nassist software development\nThese are just some of the important topics that surround the core parallel pro-\ngramming languages; forming a complementary set of capabilities we call the\nHPC ecosystem.\n Our computing systems are exponentially growing in both complexity and\nthe number of cores. Many of the considerations in HPC are also becoming\nimportant for high-end workstations. With so many processor cores, we need to\ncontrol the placement and scheduling of processes within a node, a practice that\nis loosely called process affinity  and done in conjunction with the OS kernel. As\nthe number of cores on processors grows, the tools for controlling process affin-\nity are quickly being developed to help with new concerns about process place-\nment. We’ll cover some of the techniques that are available for assigning process\naffinity in chapter 14.\n490 PART 4High performance computing ecosystems\n Sophisticated resource management systems have become ubiquitous due to the\ngrowth in complexity of computing resources. These “batch systems” form a queue of\nrequests for the resources and allocate these out, according to a priority system called\na fair share algorithm. When you first get on an HPC system, the batch system can be\nconfusing. Without knowing how to use a scheduler, you cannot deploy your applica-\ntions on these large machines. This is why we think it’s essential to go over the basics\nof using the most common batch systems in chapter 15.\n We also don’t just write out files the same way on HPC systems; we write these out\nin parallel to special filesystem hardware that can stripe the file writes across multiple\ndisks simultaneously. For exploiting the power of these parallel filesystems, you need\nto learn about some of the software used for parallel file operations. In chapter 16, we\nshow you how to use MPI-IO and HDF5, which are a couple of the more common par-\nallel file software libraries. With data sets growing ever larger, the potential uses of par-\nallel file software is expanding far outside the traditional HPC applications.\n Chapter 17 covers a broad range of important tools and resources for the HPC\napplication developer. You might find profilers of great value in helping your applica-\ntion performance. There is a wide range of profilers for different use cases and hard-\nware such as GPUs. There are also tools that help with the software development\nprocess. These tools allow you to produce correct, robust applications. Additionally,\nmany application developers can discover specialized approaches for their application\nfrom the wide variety of sample applications.\n The capabilities of the HPC ecosystem are becoming more important as the com-\nplexity and scale of our computing platforms grow. The knowledge of how to use\nthese capabilities has often been neglected. We hope that by covering these often\noverlooked aspects of high-performance computing in these four chapters, you will be\nable to get more productive use from your computing hardware.",3568
186-14.2 Discovering your architecture.pdf,186-14.2 Discovering your architecture,"491Affinity:\nTruce with the kernel\nWe first encountered affinity in section 8.6.2 on the MPI (Message Passing Inter-\nface), where we defined it and briefly showed how to handle it. We repeat the defi-\nnition here and also define process placement.\nAffinity —Assigns a preference for the scheduling of a process, rank or thread\nto a particular hardware component. This is also called pinning  or binding .\nPlacement —Assigns a process or thread to a hardware location.\nWe’ll go into more depth about affinity, placement, and the order of threads or\nranks in this chapter. Concerns about affinity are recent phenomena. In the past,\nwith just a few processor cores per CPU, there wasn’t that much to gain. As the\nnumber of processors grows and the architecture of a compute node gets more\ncomplicated, affinity has become more and more important. Still, the gains are rela-\ntively modest; perhaps the biggest benefit is in reducing the variation in performanceThis chapter covers\nWhy affinity is an important concern for modern \nCPUs\nControlling affinity for your parallel applications\nFine-tuning performance with process placement\n492 CHAPTER  14 Affinity: Truce with the kernel\nfrom run to run and getting better on-node scaling. Occasionally, controlling affinity\ncan avoid truly disastrous scheduling decisions by the kernel with respect to the char-\nacteristics of your application.\n The decision of where to place a process or a thread is handled by the operating\nsystem kernel. Kernel scheduling has a rich history and is key to the development of\nmultitasking, multi-user operating systems. It is due to these capabilities that you can\nfire up a spreadsheet, temporarily switch to a word processor, and then handle an\nimportant email. However, the scheduling algorithms developed for the general user\nare not always suitable for parallel computing. We can launch four processes for a four\nprocessor core system, but the operating system schedules those four processes any\nway it wants. It could place all four processes on the same processor, or it could spread\nthem out across the four processors. Generally the kernel does something reasonable,\nbut it can interrupt one of the parallel processes to perform a system function, caus-\ning all the other processes to idle and wait. \n In chapter 1, figures 1.20 and 1.21, we showed question marks about where the\nprocesses get placed because we have no control over the placement of processors or\nthreads on processors. At least until now. Recent releases of MPI, OpenMP, and batch\nschedulers have started to offer features to control placement and affinity. Although\nthere is a lot of change in the options in some of the interfaces, things seem to be set-\ntling down with recent releases. However, you are advised to check the documentation\nfor the releases that you use for any differences.\n14.1 Why is affinity important?\nUnlike most common desktop applications, parallel processes need to be scheduled\ntogether. This is referred to as gang scheduling. \nDEFINITION Gang scheduling  is a kernel scheduling algorithm that activates a\ngroup of processes at the same time.\nBecause parallel processes generally synchronize periodically during a run, schedul-\ning a single thread that ends up waiting on another process that is not active has no\nbenefit. The kernel scheduling algorithm has no information that a process is depen-\ndent on another’s operation. This is true for MPI, OpenMP threads, and GPU kernels\nas well. The best approach for getting gang scheduling is to only allocate as many pro-\ncesses as there are processors and bind those processes to the processors. We cannot\nforget that the kernel and system processes need somewhere to run. Some advanced\ntechniques reserve a processor just for system processes.\n It is not enough to keep every parallel process active and scheduled. We also need\nto keep processes scheduled on the same Non-Uniform Memory Access (NUMA)\ndomain to minimize memory access costs. With OpenMP, we typically go to a lot of trou-\nble to “first touch” data arrays on the processor where the data is used (see section 7.1.1).\nIf the kernel then moves your process to another NUMA domain, your efforts are all\nfor naught. We saw in section 7.3.1 that the penalty for memory access in the wrong\n493 Discovering your architecture\nNUMA domain can typically be a factor of two or more. It is a top priority for our pro-\ncesses to stay on the same memory domain. \n Typically, a NUMA domain is aligned with the sockets on a node. If we can tell a\nprocess to schedule an affinity on the same socket, we’ll always get the same, optimal\nmemory access time for main memory. The need for NUMA region affinity, however,\nis dependent on your CPU architecture. Personal computing systems often have only\none NUMA region, while large HPC systems often have far more processing cores per\nnode with two CPU sockets and two or more NUMA regions.\n While tying affinity to a NUMA domain optimizes our access time to main memory,\nwe still can have less than optimal performance due to poor cache usage. A process\nfills the L1 and L2 cache with the memory that it needs. But then, if it gets swapped\nout to another processor on the same NUMA domain with a different L1 and L2\ncache, cache performance suffers. The caches then need to be filled again. If you\nreuse data a lot, this causes a performance loss. For MPI, we want to lock processes or\nranks to a processor. But with OpenMP, this causes all the threads to be launched on\nthe same processor because the affinity is inherited by the spawned threads. With\nOpenMP, we want to have affinity for each thread to its processor.\n Some processors also have a new feature called hyperthreads. Hyperthreads add\nanother layer of complexity to the process placement considerations. First we need to\ndefine hyperthreading and what it is. \nDEFINITION Hyperthreading , an Intel technology, makes a single processor\nappear to be two virtual processors to the operating system through sharing\nof hardware resources between two threads.\nHyperthreads share a single physical core and its cache system. Because the cache is\nshared, there isn’t as much penalty for movement between hyperthreads. But it also\nmeans that each virtual core has half the cache as a real physical core if the processes\ndo not have any data in common. For our memory-bound applications, halving the\ncache can be a serious blow. Thus, the effectiveness of these virtual cores is mixed.\nMany HPC systems turn them off because some programs slow down with hyper-\nthreads. Not all hyperthreads are equal either on the hardware or operating system\nlevel, so don’t assume that if you didn’t see a benefit on a previous implementation,\nyou won’t on your current system. If we use hyperthreads, we’ll want the process place-\nment to be close by so that the shared cache benefits both virtual processors.\n14.2 Discovering your architecture\nIn order to leverage affinity for better performance, we need to know the details of our\nhardware architecture. The variety of hardware architectures makes this difficult; Intel\nalone has over a thousand CPU models. In this section, we introduce how to understand\nyour architecture. This is a requirement before you can use affinity to exploit it.\n You can get the best view of your architecture with the lstopo utility. We first saw\nlstopo in section 3.2.1 with the output for a Mac laptop in figure 3.2. The laptop is a\n494 CHAPTER  14 Affinity: Truce with the kernel\nsimple architecture with four physical processing cores which, with hyperthreading\nenabled, appears as eight virtual cores to the operating systems. We can also see in fig-\nure 3.2 that the L1 and L2 caches are private to the physical core, and the L3 cache is\nshared across all of the processors. We also note that there is just one NUMA domain.\nNow let’s take a look at a more complicated CPU. Figure 14.1 shows the architecture\nfor an Intel Skylake Gold CPU.\nThe gray boxes in figure 14.1, each labeled core and containing two light rectangles\nlabled PU for processing unit, are physical cores. Each of these gray boxes has two\nboxes inside that are the virtual processors created by hyperthreads. The L1 and L2\ncaches are private to each physical processor, while the L3 cache is shared across the\nNUMA domain. We also can see that the network and other peripherals at the right of\nthe figure are closer to the first NUMA domain. We can get some information on most\nLinux or Unix systems with the lscpu  command (figure 14.2).\n The output from lscpu  confirms that there are two threads per core and two\nNUMA domains. The processor numbering seems a little odd, but by having the first\n22 processors on the first NUMA node and then skipping to include the next 22 pro-\ncessors on the second node, we leave the hyperthreads to be numbered last. Remem-\nber that the NUMA utilities definition of a node is different than our definition,\nwhere it is a separate, distributed memory system.\n So what is the strategy for affinity and process placement for this architecture?\nWell, it depends on the application. Each application has different scaling and thread-\ning performance needs that must be considered. We will want to watch that we keep\nprocesses in their NUMA domains to get the optimal bandwidth to main memory. \n Machine (383GB total)\nNUMANode P#0 (191GB)\nPackage P#0\nL3 (30MB)\nL2 (1024KB)\nL1d (32KB)\nL1i (32KB)\nCore P#0\nPU P#0\nPU P#44L2 (1024KB)\nL1d (32KB)\nL1i (32KB)\nCore P#1\nPU P#1\nPU P#45L2 (1024KB)\nL1d (32KB)\nL1i (32KB)\nCore P#2\nPU P#2\nPU P#46L2 (1024KB)\nL1d (32KB)\nL1i (32KB)\nCore P#3\nPU P#3\nPU P#47L2 (1024KB)\nL1d (32KB)\nL1i (32KB)\nCore P#4\nPU P#4\nPU P#48L2 (1024KB)\nL1d (32KB)\nL1i (32KB)\nCore P#5\nPU P#5\nPU P#49L2 (1024KB)\nL1d (32KB)\nL1i (32KB)\nCore P#8\nPU P#6\nPU P#50L2 (1024KB)\nL1d (32KB)\nL1i (32KB)\nCore P#9\nPU P#7\nPU P#51L2 (1024KB)\nL1d (32KB)\nL1i (32KB)\nCore P#10\nPU P#8\nPU P#52L2 (1024KB)\nL1d (32KB)\nL1i (32KB)\nCore P#11\nPU P#9\nPU P#53L2 (1024KB)\nL1d (32KB)\nL1i (32KB)\nCore P#12\nPU P#10\nPU P#54L2 (1024KB)\nL1d (32KB)\nL1i (32KB)\nCore P#16\nPU P#11\nPU P#55L2 (1024KB)\nL1d (32KB)\nL1i (32KB)\nCore P#17\nPU P#12\nPU P#56L2 (1024KB)\nL1d (32KB)\nL1i (32KB)\nCore P#18\nPU P#13\nPU P#57L2 (1024KB)\nL1d (32KB)\nL1i (32KB)\nCore P#19\nPU P#14\nPU P#58L2 (1024KB)\nL1d (32KB)\nL1i (32KB)\nCore P#20\nPU P#15\nPU P#59L2 (1024KB)\nL1d (32KB)\nL1i (32KB)\nCore P#21\nPU P#16\nPU P#60L2 (1024KB)\nL1d (32KB)\nL1i (32KB)\nCore P#24\nPU P#17\nPU P#61L2 (1024KB)\nL1d (32KB)\nL1i (32KB)\nCore P#25\nPU P#18\nPU P#62L2 (1024KB)\nL1d (32KB)\nL1i (32KB)\nCore P#26\nPU P#19\nPU P#63L2 (1024KB)\nL1d (32KB)\nL1i (32KB)\nCore P#27\nPU P#20\nPU P#64L2 (1024KB)\nL1d (32KB)\nL1i (32KB)\nCore P#28\nPU P#21\nPU P#65PCI 8086:a1d2\nPCI 8086:a182\nPCI 1a03:2000\ncard0\ncontrolD64PCI 8086:1528\neth0\nPCI 8086:1528\neth1\nPCI 8086:0953PCI 15b3:1013\nib0\nmlx5_0\nNUMANode P#1 (192GB)\nPackage P#1\nL3 (30MB)\nL2 (1024KB)\nL1d (32KB)\nL1i (32KB)\nCore P#0\nPU P#22\nPU P#66L2 (1024KB)\nL1d (32KB)\nL1i (32KB)\nCore P#1\nPU P#23\nPU P#67L2 (1024KB)\nL1d (32KB)\nL1i (32KB)\nCore P#2\nPU P#24\nPU P#68L2 (1024KB)\nL1d (32KB)\nL1i (32KB)\nCore P#3\nPU P#25\nPU P#69L2 (1024KB)\nL1d (32KB)\nL1i (32KB)\nCore P#4\nPU P#26\nPU P#70L2 (1024KB)\nL1d (32KB)\nL1i (32KB)\nCore P#5\nPU P#27\nPU P#71L2 (1024KB)\nL1d (32KB)\nL1i (32KB)\nCore P#8\nPU P#28\nPU P#72L2 (1024KB)\nL1d (32KB)\nL1i (32KB)\nCore P#9\nPU P#29\nPU P#73L2 (1024KB)\nL1d (32KB)\nL1i (32KB)\nCore P#10\nPU P#30\nPU P#74L2 (1024KB)\nL1d (32KB)\nL1i (32KB)\nCore P#11\nPU P#31\nPU P#75L2 (1024KB)\nL1d (32KB)\nL1i (32KB)\nCore P#12\nPU P#32\nPU P#76L2 (1024KB)\nL1d (32KB)\nL1i (32KB)\nCore P#16\nPU P#33\nPU P#77L2 (1024KB)\nL1d (32KB)\nL1i (32KB)\nCore P#17\nPU P#34\nPU P#78L2 (1024KB)\nL1d (32KB)\nL1i (32KB)\nCore P#18\nPU P#35\nPU P#79L2 (1024KB)\nL1d (32KB)\nL1i (32KB)\nCore P#19\nPU P#36\nPU P#80L2 (1024KB)\nL1d (32KB)\nL1i (32KB)\nCore P#20\nPU P#37\nPU P#81L2 (1024KB)\nL1d (32KB)\nL1i (32KB)\nCore P#21\nPU P#38\nPU P#82L2 (1024KB)\nL1d (32KB)\nL1i (32KB)\nCore P#24\nPU P#39\nPU P#83L2 (1024KB)\nL1d (32KB)\nL1i (32KB)\nCore P#25\nPU P#40\nPU P#84L2 (1024KB)\nL1d (32KB)\nL1i (32KB)\nCore P#26\nPU P#41\nPU P#85L2 (1024KB)\nL1d (32KB)\nL1i (32KB)\nCore P#27\nPU P#42\nPU P#86L2 (1024KB)\nL1d (32KB)\nL1i (32KB)\nCore P#28\nPU P#43\nPU P#87\nHost: cn610\nIndexes: physical\nDate: Sat 11 Apr 2020 07:39:20 PM MDT\nFigure 14.1 The Intel Skylake Gold architecture with two NUMA domains and 88 processing cores reveals \nthe complexity of higher-end compute nodes.",12701
187-14.3 Thread affinity with OpenMP.pdf,187-14.3 Thread affinity with OpenMP,"495 Thread affinity with OpenMP\n14.3 Thread affinity with OpenMP\nThread affinity is vital when optimizing applications with OpenMP. Tying a thread to\nthe location of the memory it uses is important to achieve good memory latency and\nbandwidth. We go to great effort to do  first touch  to get memory placed close to the\nthread as we discussed in section 7.1.1. If the threads are moving around to different\nprocessors, we lose all the benefits we should get from our extra effort.\n With OpenMP v4.0, the affinity controls for OpenMP were expanded to include\nthe close , spread , and primary  keywords, in addition to the existing true  or false\noptions. Also added were three options for the OMP_PLACES environment variable,\nsockets , cores , and threads . In summary, we now have these affinity and placement\ncontrols:\nOMP_PLACES = [sockets|cores|threads]  or an explicit list of places\nOMP_PROC_BIND = [close|spread|primary]  or [true|false]\nOMP_PLACES puts limits on where the threads can be scheduled. There is actually\none option that is not listed: the node . It is the default and allows each thread to be\nscheduled anywhere in the “place.” With more than one thread on the default place\nFigure 14.2 Output from lscpu  command for the Intel Skylake Gold processor.\n496 CHAPTER  14 Affinity: Truce with the kernel\nof the node, the possibility exists that the scheduler will move the threads or have col-\nlisions with two or more threads scheduled for one virtual processor. One sensible\napproach is not to have more threads than the quantity of the specified place. Perhaps\nthe better rule is to specify a place that has a quantity greater than the desired number\nof threads. We’ll show how that works in an example later in this section.\n The OMP_PROC_BIND environment variable has five possible settings, but these\nhave some overlap in meaning. The close , spread , and primary  settings are special-\nized versions of true . \nNOTE We also note that primary  replaces the deprecated master  keyword as\nof the OpenMP v5.1 standard. You may continue to encounter the old usage\nas compilers implement the new standard. \nWith the false  setting, the kernel scheduler is free to move threads around. The true\nsetting tells the kernel not to move the thread once it gets scheduled. But it can be sched-\nuled anywhere within the place constraint and can vary from run to run. The primary\nsetting is a special case that schedules threads on the main processor. The close  setting\nschedules the threads close together and spread  distributes the threads. The choice\nof which of these two settings to use has some subtle implications that you will see in\nthe example for this section.\nNOTE You can also set the placement with a detailed list. This is a more\nadvanced use case that we won’t go over here. The detailed list can give more\nfine-tuned control, but it is less portable to a different CPU type.\nThe OpenMP environment variables set the affinity and placement for the whole pro-\ngram. You can also set the affinity for individual loops through the addition of a clause\non the parallel  directive. The clause has this syntax:\nproc_bind([primary|close|spread])\nThe following example shows these affinity controls in operation on our simple vector\naddition program from section 7.3.1. The affinity-reporting routines can also be\nadded to your code to see the impact there.\nExample: Vector addition with all possible settings of OMP_PLACES  and \nOMP_PROC_BIND\nFor this example, we set every combination of OpenMP affinity and placement envi-\nronment variables. We first modify the vector add from section 7.3.1 to call a routine\nthat reports placement of threads shown in the following listing.\n497 Thread affinity with OpenMP\nOpenMP/vecadd_opt3.c\n 1 #include <stdio.h>\n 2 #include <time.h>\n 3 #include ""timer.h""\n 4 #include ""omp.h""\n 5 #include ""place_report_omp.h""\n 6 \n 7 // large enough to force into main memory\n 8 #define ARRAY_SIZE 80000000\n 9 static double a[ARRAY_SIZE], b[ARRAY_SIZE], c[ARRAY_SIZE];\n10 \n11 void vector_add(double *c, double *a, double *b, int n);\n12 \n13 int main(int argc, char *argv[]){\n14 #ifdef VERBOSE                    \n15    place_report_omp();     \n16 #endif\n17    struct timespec tstart;\n18    double time_sum = 0.0;\n19 #pragma omp parallel\n20    {\n21 #pragma omp for\n22       for (int i=0; i<ARRAY_SIZE; i++) {\n23          a[i] = 1.0;\n24          b[i] = 2.0;\n25       }\n26 \n27 #pragma omp masked\n28       cpu_timer_start(&tstart);\n29       vector_add(c, a, b, ARRAY_SIZE);\n30 #pragma omp masked\n31       time_sum += cpu_timer_stop(tstart);\n32    } // end of omp parallel\n33 \n34    printf(""Runtime is %lf msecs\n"", time_sum);\n35 }\n36 \n37 void vector_add(double *c, double *a, double *b, int n)\n38 {\n39 #pragma omp for\n40    for (int i=0; i < n; i++){\n41       c[i] = a[i] + b[i];\n42    }\n43 }\nThe main work is done in the place_report_omp  subroutine. We use an ifdef\naround the call to easily turn the reporting on and off. So now let’s take a look at the\nreporting routine in the next listing.Modified vecadd_opt3.c for affinity study\nDefine to enable \nreporting\nCall to placement \nreport\n498 CHAPTER  14 Affinity: Truce with the kernel\n(continued)\nOpenMP/place_report_omp.c\n41 void place_report_omp(void)\n42 {\n43    #pragma omp parallel\n44    {\n45       if (omp_get_thread_num() == 0){\n46          printf(""Running with %d thread(s)\n"",   \n                   omp_get_num_threads());          \n47          int bind_policy = omp_get_proc_bind();    \n48          switch (bind_policy)\n49          {\n50             case omp_proc_bind_false:\n51                printf(""  proc_bind is false\n"");\n52                break;\n53             case omp_proc_bind_true:\n54                printf(""  proc_bind is true\n"");\n55                break;\n56             case omp_proc_bind_master:\n57                printf(""  proc_bind is master\n"");\n58                break;\n59             case omp_proc_bind_close:\n60                printf(""  proc_bind is close\n"");\n61                break;\n62             case omp_proc_bind_spread:\n63                printf(""  proc_bind is spread\n"");\n64          }\n65          printf(""  proc_num_places is %d\n"",   \n                   omp_get_num_places());         \n66       }\n67    }\n68 \n69    int socket_global[144];\n70    char clbuf_global[144][7 * CPU_SETSIZE];\n71 \n72    #pragma omp parallel\n73    {\n74       int thread = omp_get_thread_num();\n75       cpu_set_t coremask;\n76       char clbuf[7 * CPU_SETSIZE];\n77       memset(clbuf, 0, sizeof(clbuf));\n78       sched_getaffinity(0, sizeof(coremask),   \n                           &coremask);            \n79       cpuset_to_cstr(&coremask, clbuf);       \n80       strcpy(clbuf_global[thread],clbuf);\n81       socket_global[omp_get_thread_num()] =    \n            omp_get_place_num();                  \n82       #pragma omp barrier\n83       #pragma omp masterReporting place settings in OpenMP\nReports number \nof threads\nQueries \nand reports \nOMP_PROC_BIND \nsetting\nQueries and reports overall \nthread placement restrictions\nGets the affinity \nbit mask\nConverts the bit mask to \nsomething we can print\nGets the actual place \nnumber to print\n499 Thread affinity with OpenMP\nIn the placement reporting routine, we query the OpenMP settings, report those,\nthen show the placement and affinity for each thread. To try it out, compile the code\nwith the verbose setting and run it with 44 threads or whatever number of threads\nmakes sense on your system, and no special environment variable settings. The exam-\nple code is at https:/ /github.com/EssentialsofParallelComputing/Chapter14.git  in the\nOpenMP subdirectory. 84       for (int i=0; i<omp_get_num_threads(); i++){\n85          printf(""Hello from thread %d: (core affinity = %s)""\n               "" OpenMP socket is %d\n"",\n86             i, clbuf_global[i], socket_global[i]);\n87       }\n88    }\n89 }\nThe CPU affinity bit mask needs to be converted to a more understandable format for\nprinting out. The next listing shows that routine.\nOpenMP/place_report_omp.c\n12 static char *cpuset_to_cstr(cpu_set_t *mask, char *str)\n13 { \n14   char *ptr = str;\n15   int i, j, entry_made = 0;\n16   for (i = 0; i < CPU_SETSIZE; i++) {\n17     if (CPU_ISSET(i, mask)) {\n18       int run = 0; \n19       entry_made = 1; \n20       for (j = i + 1; j < CPU_SETSIZE; j++) {\n21         if (CPU_ISSET(j, mask)) run++;\n22         else break;\n23       }\n24       if (!run)\n25         sprintf(ptr, ""%d,"", i);\n26       else if (run == 1) {\n27         sprintf(ptr, ""%d,%d,"", i, i + 1);\n28         i++; \n29       } else {\n30         sprintf(ptr, ""%d-%d,"", i, i + run);\n31         i += run;\n32       }\n33       while (*ptr != 0) ptr++;\n34     }\n35   }\n36   ptr -= entry_made;\n37   *ptr = 0;\n38   return(str);\n39 }Routine to convert CPU bit mask to a C string\n500 CHAPTER  14 Affinity: Truce with the kernel\nLet’s see what happens when we place the threads on hardware cores and set the affin-\nity binding to close .\nexport OMP_PLACES=cores\nexport OMP_PROC_BIND=close\n./vecadd_opt3\nThe output with this affinity and placement settings is shown in Figure 14.3.\n Wow! We can actually control the kernel! The threads are now pinned to the two\nvirtual cores belonging to a single hardware core. The run time of 0.0166 ms is the\nlast number in the output. This run time is a substantial improvement over the\n0.0221 ms in the previous run for a 25% reduction in the computation time. You can\nexperiment with various environment variable settings and see how the threads are\nplaced on the node.\n We are going to automate the exploration of all the settings and how they scale\nwith different numbers of threads. We’ll turn off the verbose option to reduce theExample: Querying the OpenMP settings for the placement reporting routine\nTo query the OpenMP settings, report them, then show the placement and affinity for\neach thread, the steps are\nmkdir build && cd build\ncmake -DCMAKE_VERBOSE=on ..\nmake\nexport OMP_NUM_THREADS=44\n./vecadd_opt3\nRunning this on the Intel Skylake-Gold with GCC 9.3 gives the following output.\nThe core affinity allows the thread to run on any of the 88 virtual cores. Running with 44 thread(s)\nproc_bind is false\nproc_num_places is 0\nHello from thread 0: (core affinity = 0-87) OpenMP socket is -1\nHello from thread 1: (core affinity = 0-87) OpenMP socket is -1\nHello from thread 2: (core affinity = 0-87) OpenMP socket is -1\nHello from thread 3: (core affinity = 0-87) OpenMP socket is -1\nHello from thread 4: (core affinity = 0-87) OpenMP socket is -1\n<... skipping output ...>\nHello from thread 42: (core affinity = 0-87) OpenMP socket is -1\nHello from thread 43: (core affinity = 0-87) OpenMP socket is -1\n0.022119Any processor\nlocation\nThe output shows the affinity and placement report with no environment variables set. \nThe threads are allowed to run on any processor from 0 to 87. \n501 Thread affinity with OpenMP\noutput that we have to deal with. Only the run time will print. Remove the previous\nbuild and rebuild the code as follows:\nmkdir build && cd build\ncmake ..\nmake\nWe then run the script in the following listing to get the performance for all cases. \nOpenMP/run.sh\n 1 #!/bin/sh\n 2 \n 3 calc_avg_stddev()     \n 4 {\n 5    #echo ""Runtime is $1""\n 6    awk '{\n 7      sum = 0.0; sum2 = 0.0       # Initialize to zero\n 8      for (n=1; n <= NF; n++) {   # Process each value on the line\n 9        sum += $n;                # Running sum of values\n10        sum2 += $n * $n           # Running sum of squares\n11      }\n12      print "" Number of trials="" NF "",    avg="" sum/NF "", \\n              std dev="" sqrt((sum2 - (sum*sum)/NF)/NF);\n13        }' <<< $1\n14 }\n15 \n16 conduct_tests()    \n17 {\n18    echo """"\n19    echo -n `printenv |grep OMP_` ${exec_string}\n20    foo=""""\n21    for index in {1..10}    \n22    do\n23       time_result=`${exec_string}`Listing 14.1 Script to automate exploring all settingsRu ning with 44 thread(s) n\np oc_bind is close r\np oc_num_places is 44 r\nHe lo from thread 0: (core affinity = 0,44) OpenMP so ket is 0 l                                                  c\nHe lo from thread 1: (core affinity = 1,45) OpenMP so ket is 1 l                                                  c\nHe lo from thread 2: (core affinity = 2,46) OpenMP so ket is 2 l                                                  c\nHe lo from thread 3: (core affinity = 3,47) OpenMP so ket is 3 l                                                  c\nHe lo from thread 4: (core affinity = 4,48) OpenMP so ket is 4 l                                                  c\n... skipping output ...> <\nHe lo from thread 42: (core affinity = 42,86) OpenM P ocket is 42 l                                                  s\nHe lo from thread 43: (core affinity = 43,87) OpenM P ocket is 43 l                                                  s\n0 016601 .Hardware core\nFigure 14.3 Affinity and placement report for OMP_PLACES=cores  and \nOMP_PROC_BIND=close . Each thread can run on two possible virtual cores. \nThese two processors belong to a single hardware core due to hyperthreading. \nCalculates average and \nstandard deviation\nDoes the test\nRepeats ten times \nto get statistics\n502 CHAPTER  14 Affinity: Truce with the kernel\n24       time_val[$index]=${time_result}\n25       foo=""$foo ${time_result}""\n26    done\n27    calc_avg_stddev ""${foo}""\n28 }\n29 \n30 exec_string=""./vecadd_opt3 ""\n31 \n32 conduct_tests\n33 \n34 THREAD_COUNT=""88 44 22 16 8 4 2 1""\n35 \n36 for my_thread_count in ${THREAD_COUNT}   \n37 do\n38    unset OMP_PLACES\n39    unset OMP_PROC_BIND\n40    export OMP_NUM_THREADS=${my_thread_count}\n41 \n42    conduct_tests\n43 \n44    PLACES_LIST=""threads cores sockets""\n45    BIND_LIST=""true false close spread primary""\n46 \n47    for my_place in ${PLACES_LIST}     \n48    do\n49       for my_bind in ${BIND_LIST}     \n50       do\n51          export OMP_NUM_THREADS=${my_thread_count}\n52          export OMP_PLACES=${my_place}\n53          export OMP_PROC_BIND=${my_bind}\n54 \n55          conduct_tests\n56       done\n57    done\n58 done\nDue to space, we show only a few of the results in figure 14.4. All of the values are the\nspeedup from a single thread with no affinity or placement settings.\n The first thing to note from figure 14.4 in our analysis is that the program is gener-\nally the fastest for all settings with only 44 threads. Overall, hyperthreading does not\nhelp. The exception is the close  setting for threads because until we have more than\n44 threads with this setting, there are no processes on the second socket. With threads\nonly on the first socket, it limits the total memory bandwidth that can be obtained. At\nthe full 88 threads, the close  setting for threads gives the best performance, although\nby only a little bit. The close  setting, in general, shows the same limited memory band-\nwidth effect due to only having threads on the first socket. You can also see that at\nlarger process counts with process binding, the performance is higher than without\nprocess binding. \n \n Loops over number \nof threads\nLoops over place \nsettings\nLoops over \naffinity settings",15377
188-14.4.1 Default process placement with OpenMPI.pdf,188-14.4.1 Default process placement with OpenMPI,"503 Process affinity with MPI\nSome key points to take away from this analysis\nHyperthreading does not help with simple memory-bound kernels, but it also\ndoesn’t hurt.\nFor memory-bandwidth-limited kernels on multiple sockets (NUMA domains),\nget both sockets busy.\nWe don’t show the results for setting OMP_PROC_BIND to primary  because it forces\nall the threads to be on the same processor and slows the program by as much as a fac-\ntor of two. We also don’t show setting OMP_PLACES to sockets  because it has lower\nperformance than those shown. \n14.4 Process affinity with MPI\nThere are also benefits to applying affinity with MPI applications as discussed in sec-\ntion 14.2. It helps to get full memory bandwidth and cache performance by keeping\nthe processes from being migrated to different processor cores by the operating sys-\ntem kernel. We will discuss affinity with OpenMPI because it has the most publicly\navailable tools for affinity and process placement. Other MPI implementations like\nMPICH must be compiled with SLURM support enabled, which isn’t as applicable to\npersonal machines. We will discuss the command-line tools that can be used in more\ngeneral situations in section 14.6. For now, let’s move onward with our exploration of\naffinity in OpenMPI!20\n18\n16\n14\n12\n10\nVecAdd speedup8\n6\n4\n2\n1\n12 4 8 16 22 44 88Ideal scaling\nThreads spread\nCores spread\nThreads close\nCores close\nCores false\nThreads false\nOMP_NUM_PROCS only\nNumber of threads\nFigure 14.4 OpenMP affinity and placement settings of OMP_PROC_BIND=spread  boosts the \nparallel scaling by 50%. The lines are for various numbers of threads for a particular setting and are \nordered roughly from high to low in the legend.",1741
189-14.4.2 Taking control Basic techniques for specifying process placement in OpenMPI.pdf,189-14.4.2 Taking control Basic techniques for specifying process placement in OpenMPI,"504 CHAPTER  14 Affinity: Truce with the kernel\n14.4.1 Default process placement with OpenMPI\nRather than leaving process placement to the kernel scheduler, OpenMPI specifies a\ndefault placement and affinity. The default settings for OpenMPI vary depending on\nthe number of processes. These are\nProcesses <= 2 (bind to core)\nProcesses > 2 (bind to socket)\nProcesses > processors (bind to none)\nSome HPC centers might set other defaults such as always binding to cores. This bind-\ning policy may make sense for most MPI jobs but can cause problems with applications\nusing both OpenMP threading and MPI. The threads will all be bound to a single pro-\ncessor, serializing the threads.\n Recent versions of OpenMPI have extensive support for process placement and\naffinity. Using these tools, you usually get a performance gain. The gain depends upon\nhow the process scheduler in the operating system is optimizing placement. Most sched-\nulers are tuned for general computing, such as word processing and spreadsheets, but\nnot parallel applications. Coaxing the scheduler to “do the right thing” potentially\nyields a benefit of 5–10%, but it can be a lot more.\n14.4.2 Taking control: Basic techniques for specifying process \nplacement in OpenMPI\nFor most use cases, it is sufficient to use simple controls to place processes and to bind\nthese to hardware components. These controls are supplied to the mpirun  command\nas options. Let’s start with looking at distributing processes equally across a multi-\nnode job. It is easiest to demonstrate this with an example.\nExample: Distributing processes equally across multi-node jobs\nWe have an application that we want to run on 32 MPI ranks, but it is a memory-hungry\napplication that needs half a terabyte of memory. A single node doesn’t have enough\nmemory, so how do we manage this? \nIf we look at the system details, each node has two sockets filled with Intel Broadwell\n(E52695) CPUs. Each CPU has 18 hardware cores that, with hyperthreading, gives\nus 36 virtual processors per socket. Each node has 128 GiB of memory.\nFrom the lscpu  command\nNUMA node0 CPU(s):     0-17,36-53\nNUMA node1 CPU(s):     18-35,54-71\nFrom the /proc/meminfo file\nMemTotal:       131728700 kB\n505 Process affinity with MPI\nFor our first run of our application, we simply ask mpirun to launch 32 processes:\nmpirun -n 32 ./MPIAffinity | sort -n -k 4For this example, we use our placement reporting tool for MPI applications. The two\nparts of the code are shown in the following listings.\nMPI/MPIAffinity.c\n 1 #include <mpi.h>\n 2 #include <stdio.h>\n 3 #include ""place_report_mpi.h""\n 4 int main(int argc, char **argv)\n 5 {\n 6    MPI_Init(&argc, &argv);\n 7 \n 8    place_report_mpi();    \n 9 \n10    MPI_Finalize();\n11    return 0;\n12 }\nWe need to insert the call to our placement reporting subroutine after MPI is initial-\nized. You can easily add this to your MPI application as well. Now let’s look at the\nreporting subroutine in the next listing. \nMPI/place_report_mpi.c\n40 void place_report_mpi(void)\n41 {\n42   int rank;\n43   cpu_set_t coremask;\n44   char clbuf[7 * CPU_SETSIZE], hnbuf[64];\n45 \n46   memset(clbuf, 0, sizeof(clbuf));\n47   memset(hnbuf, 0, sizeof(hnbuf));\n48 \n49   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n50 \n51   gethostname(hnbuf, sizeof(hnbuf));      \n52   sched_getaffinity(0, sizeof(coremask),    \n                       &coremask);             \n53   cpuset_to_cstr(&coremask, clbuf);           \n54   printf(""Hello from rank %d, on %s. (core affinity = %s)\n"",\n55           rank, hnbuf, clbuf);\n56 }Main MPI affinity code\nInserts placement reporting \ncall after MPI_Init\nMPI placement reporting tool\nGets our \nnode name\nGets the affinity \nsetting of our process\nSame cpuset_to_cstr routine \nfrom the vector addition \nlisting in section 14.3\n506 CHAPTER  14 Affinity: Truce with the kernel\nWe then have to sort the output by the data in the fourth column because the order of\noutput by processes is random (done by the command sort  -n -k 4). The output for\nthis command with our placement report routine is shown in figure 14.5.\nF r o m  t h e  o u t p u t  i n  f i g u r e  1 4 . 5 ,  w e  s e e  t h a t  a l l  t h e  r a n k s  w e r e  l a u n c h e d  o n  n o d e\ncn328. Referring to the default affinity settings for OpenMPI at the start of this sec-\ntion, for more than two ranks the affinity is set to bind to the socket. The output from\nthe lscpu  command shows our first NUMA region contains the virtual processing\ncores 0–17, 36–53. NUMA regions are usually aligned with each socket. In our output,\nwe see that the core affinity equals 0–17, 36–53, confirming that the affinity was set to\nthe socket.\n Because our real application memory requirements are larger than the 128 GiB on\nthe node, it fails when allocating memory. We thus need to find a way to spread out\nthe processes. For this, we add another option, --npernode  <#> or -N <#>, which tells\nMPI how many ranks to put on each node. We need to have four nodes to get enough\nmemory for our problem, so we want eight processes per node. \nmpirun -n 32 --npernode 8 ./MPIAffinity | sort -n -k 4\nFigure 14.6 shows our placement report.Hello from rank 0, on cn328. (core affinity = 0-17,36-53)\nHello from rank 1, on cn328. (core affinity = 18-35,54-71)\nHello from rank 2, on cn328. (core affinity = 0-17,36-53)\nHello from rank 3, on cn328. (core affinity = 18-35,54-71)\n<... skipping output ...>\nHello from rank 28, on cn328. (core affinity = 0-17,36-53)\nHello from rank 29, on cn328. (core affinity = 18-35,54-71)\nHello from rank 30, on cn328. (core affinity = 0-17,36-53)\nHello from rank 31, on cn328. (core affinity = 18-35,54-71)NUMA region\nFigure 14.5 For mpirun  -n 32, all of our processes are on the cn328 node. The affinity is \nset to the NUMA region (socket).\nHello from rank 0, on cn328. (core affinity = 0-17,36-53)\nHello from rank 1, on cn328. (core affinity = 18-35,54-71)\n< ... skipping output ... >\nHello from rank 8, on cn329. (core affinity = 0-17,36-53)\nHello from rank 9, on cn329. (core affinity = 18-35,54-71)\n< ... skipping output ... >\nHello from rank 16, on cn330. (core affinity = 0-17,36-53)\nHello from rank 17, on cn330. (core affinity = 18-35,54-71)\n< ... skipping output ... >\nHello from rank 24, on cn331. (core affinity = 0-17,36-53)\nHello from rank 25, on cn331. (core affinity = 18-35,54-71)NUMA region\nFigure 14.6 The MPI processes are spread out across the four nodes, cn328 through 331. \nThe affinity is still tied to the NUMA region.\n507 Process affinity with MPI\nFrom the output in figure 14.6, we can see that we are running on four nodes. We\nshould now have enough memory to run our application. Alternatively, we could spec-\nify how many ranks per socket with --npersocket . We have two sockets per node, so\nwe want four ranks per socket, thus:\nmpirun -n 32 --npersocket 4 ./MPIAffinity | sort -n -k 4\nFigure 14.7 shows the output from the placement per socket.\nThe placement report in figure 14.7 shows that the order of the ranks places adjacent\nranks on the same NUMA domain instead of alternating the ranks between NUMA\ndomains. That might be better if ranks are communicating with nearest neighbors.\n So far, we have only worked on the placement of processes. Now let’s try to see\nwhat we can do about the affinity and binding of the MPI processes. For this, we add\nthe --bind-to  [socket  | numa  | core  | hwthread]  option to mpirun:\nmpirun -n 32 --npersocket 4 --bind-to core ./MPIAffinity | sort -n -k 4\nWe can see how this changes the affinity for the processes in the placement report in\nfigure 14.8.Hello from rank 0, on cn328. (core affinity = 0-17,36-53)\nHello from rank 1, on cn328. (core affinity = 0-17,36-53)\nHello from rank 2, on cn328. (core affinity = 0-17,36-53)\nHello from rank 3, on cn328. (core affinity = 0-17,36-53)\nHello from rank 4, on cn328. (core affinity = 18-35,54-71)\nHello from rank 5, on cn328. (core affinity = 18-35,54-71)\nHello from rank 6, on cn328. (core affinity = 18-35,54-71)\nHello from rank 7, on cn328. (core affinity = 18-35,54-71)\nHello from rank 8, on cn329. (core affinity = 0-17,36-53)\nHello from rank 9, on cn329. (core affinity = 0-17,36-53)\n< ... skipping output ... >NUMA region\nFigure 14.7 With the placement set to four processes per socket, the order of the ranks \nchanges. Now the four adjacent ranks are on the same NUMA region.\nHello from rank 0, on cn328. (core affinity = 0,36)\nHello from rank 1, on cn328. (core affinity = 1,37)\nHello from rank 2, on cn328. (core affinity = 2,38)\nHello from rank 3, on cn328. (core affinity = 3,39)\nHello from rank 4, on cn328. (core affinity = 18,54)\nHello from rank 5, on cn328. (core affinity = 19,55)\nHello from rank 6, on cn328. (core affinity = 20,56)\nHello from rank 7, on cn328. (core affinity = 21,57)\nHello from rank 8, on cn329. (core affinity = 0,36)\nHello from rank 9, on cn329. (core affinity = 1,37)\n< ... skipping output ... >Hardware core\nFigure 14.8 The affinity from binding to a core changes the affinity for the processes to \na hardware core. Each hardware core represents two virtual cores because of \nhyperthreading. We get two locations for each process.\n508 CHAPTER  14 Affinity: Truce with the kernel\nThe placement results in figure 14.8 show that the process affinity is now restricted\nmore than it was previously. There are two virtual cores that each process can sched-\nule to run on. These two virtual cores belong to one hardware core, thus showing that\nthe core binding option refers to a hardware core. Only four of the 18 processor cores\non each socket are used. This is what we want so that there is more memory for each\nMPI rank. Let’s try binding the process to the hyperthreads instead of to the core by\nusing the hwthread  option. This should force the scheduler to place processes on one,\nand only one, virtual core.\nmpirun -n 32 --npersocket 4 --bind-to hwthread ./MPIAffinity | sort -n -k 4\nAgain, we use our placement report program to visualize the placement with the out-\nput shown in figure 14.9.\nOur last processor layout finally restricts where each process can run to a single loca-\ntion as shown in figure 14.9. That seems like a good result. But wait. Take a closer\nlook. The first two ranks are placed on the pair of hyperthreads (0 and 36) of a single\nhardware core. This is not a good idea. That means the two ranks are sharing the\ncache and hardware components of that hardware core instead of having their own\nfull complement of resources. \n The mpirun  command in OpenMPI also has a built-in option to report bindings. It\nis convenient for small problems, but the amount of output for nodes with a lot of\nprocessors and MPI ranks is hard to handle. Adding --report-bindings  to the mpi-\nrun command used for figure 14.9 produces the output shown in figure 14.10.\n The visual layout is a little easier to quickly understand, and there is a lot of infor-\nmation packed into the output. Each line indicates a rank in MPI_COMM_WORLD  (MCW) .\nThe symbols between the forward slashes on the right side indicate the binding loca-\ntion for that process. The set of two dots between the forward slash symbols shows that\nthere are two hyperthreads per core. The two sets of brackets delineate the two sock-\nets on the node. Hello from rank 0, on cn328. (core affinity = 0)\nHello from rank 1, on cn328. (core affinity = 36)\nHello from rank 2, on cn328. (core affinity = 1)\nHello from rank 3, on cn328. (core affinity = 37)\nHello from rank 4, on cn328. (core affinity = 18)\nHello from rank 5, on cn328. (core affinity = 54)\nHello from rank 6, on cn328. (core affinity = 19)\nHello from rank 7, on cn328. (core affinity = 55)\nHello from rank 8, on cn329. (core affinity = 0)\nHello from rank 9, on cn329. (core affinity = 36)\n< ... skipping output ... >Processes\non a pair of\nhyperthreads\non a single core\nFigure 14.9 The process placement from the hwthread  option limits where the \nprocesses can run to only one location.",12128
190-14.4.3 Affinity is more than just process binding The full picture.pdf,190-14.4.3 Affinity is more than just process binding The full picture,"509 Process affinity with MPI\nWith the examples we explored in this section, you should be getting an idea of how\nto control placement and affinity. You should also have some tools to check that you\nare getting the placement and process bindings you expect.\n14.4.3 Affinity is more than just process binding: The full picture\nNow we will explore the full picture of affinity for parallel computing. We will use this as\na way of introducing the advanced options offered in OpenMPI for even more control.\n The concept of affinity is born out of how the operating system sees things. At the\nlevel of the operating system, you can set where each process is allowed to run. On\nLinux, this is done through either the taskset  or the numactl  commands. These\ncommands, and similar utilities on other operating systems, emerged as the complex-\nity of the CPU grew so that you could provide more information to the scheduler in\nthe operating system. The directions might be taken as hints or requirements by the\nscheduler. Using these commands, you can pin a server process to a particular proces-\nsor to be closer to a particular hardware component or to gain faster response. This\nfocus on affinity alone is enough when dealing with a single process.\n For parallel programming, there are additional considerations. We have a set of\nprocesses that we need to consider. Lets say we have 16 processors and we are running\na four rank MPI job. Where do we put the ranks? Do we put these across the sockets,\non all the sockets, pack them close together, or spread them out? Do we place certain\nranks next to each other (ranks 1 and 2 together or ranks 1 and 4 together)? To be\nable to answer these questions, we need to address the following:\nMapping (the placement of processes)\nOrder of ranks (which ranks are close together)\nBinding (affinity or tying a process to a location or locations)[cn333:06278] MCW rank 0 bound to socket 0[core 0[hwt 0-1]]: [  /../../../../../../../../../../../../../../../../..][../../../../../../../../../../../../../../../../../..] BB\n[cn333:06278] MCW rank 1 bound to socket 0[core 1[hwt 0-1]]: [../  /../../../../../../../../../../../../../../../..][../../../../../../../../../../../../../../../../../..] BB\n[cn333:06278] MCW rank 2 bound to socket 0[core 2[hwt 0-1]]: [../../  /../../../../../../../../../../../../../../..][../../../../../../../../../../../../../../../../../..] BB\n[cn333:06278] MCW rank 3 bound to socket 0[core 3[hwt 0-1]]: [../../../  /../../../../../../../../../../../../../..][../../../../../../../../../../../../../../../../../..] BB\n[cn333:06278] MCW rank 4 bound to socket 1[core 18[hwt 0-1]]: [../../../../../../../../../../../../../../../../../..][  /../../../../../../../../../../../../../../../../..] BB\n[cn333:06278] MCW rank 5 bound to socket 1[core 19[hwt 0-1]]: [../../../../../../../../../../../../../../../../../..][../  /../../../../../../../../../../../../../../../..] BB\n[cn333:06278] MCW rank 6 bound to socket 1[core 20[hwt 0-1]]: [../../../../../../../../../../../../../../../../../..][../../  /../../../../../../../../../../../../../../..] BB\n[cn333:06278] MCW rank 7 bound to socket 1[core 21[hwt 0-1]]: [../../../../../../../../../../../../../../../../../..][../../../  /../../../../../../../../../../../../../..] BB\n[cn334:37227] MCW rank 8 bound to socket 0[core 0[hwt 0-1]]: [  /../../../../../../../../../../../../../../../../..][../../../../../../../../../../../../../../../../../..] BB\n[cn334:37227] MCW rank 9 bound to socket 0[core 1[hwt 0-1]]: [../  /../../../../../../../../../../../../../../../..][../../../../../../../../../../../../../../../../../..] BB\n[cn334:37227] MCW rank 10 bound to socket 0[core 2[hwt 0-1]]: [../../  /../../../../../../../../../../../../../../..][../../../../../../../../../../../../../../../../../..] BB\n[cn334:37227] MCW rank 11 bound to socket 0[core 3[hwt 0-1]]: [../../../  /../../../../../../../../../../../../../..][../../../../../../../../../../../../../../../../../..] BB\n[cn334:37227] MCW rank 12 bound to socket 1[core 18[hwt 0-1]]: [../../../../../../../../../../../../../../../../../..][  /../../../../../../../../../../../../../../../../..] BB\n[cn334:37227] MCW rank 13 bound to socket 1[core 19[hwt 0-1]]: [../../../../../../../../../../../../../../../../../..][../  /../../../../../../../../../../../../../../../..] BB\n[cn334:37227] MCW rank 14 bound to socket 1[core 20[hwt 0-1]]: [../../../../../../../../../../../../../../../../../..][../../  /../../../../../../../../../../../../../../..] BB\n[cn334:37227] MCW rank 15 bound to socket 1[core 21[hwt 0-1]]: [../../../../../../../../../../../../../../../../../..][../../../  /../../../../../../../../../../../../../..] BB\n[cn335:61077] MCW rank 16 bound to socket 0[core 0[hwt 0-1]]: [  /../../../../../../../../../../../../../../../../..][../../../../../../../../../../../../../../../../../..] BB\n[cn335:61077] MCW rank 17 bound to socket 0[core 1[hwt 0-1]]: [../  /../../../../../../../../../../../../../../../..][../../../../../../../../../../../../../../../../../..] BB\n[cn335:61077] MCW rank 18 bound to socket 0[core 2[hwt 0-1]]: [../../  /../../../../../../../../../../../../../../..][../../../../../../../../../../../../../../../../../..] BB\n[cn335:61077] MCW rank 19 bound to socket 0[core 3[hwt 0-1]]: [../../../  /../../../../../../../../../../../../../..][../../../../../../../../../../../../../../../../../..] BB\n[cn335:61077] MCW rank 20 bound to socket 1[core 18[hwt 0-1]]: [../../../../../../../../../../../../../../../../../..][  /../../../../../../../../../../../../../../../../..] BB\n[cn335:61077] MCW rank 21 bound to socket 1[core 19[hwt 0-1]]: [../../../../../../../../../../../../../../../../../..][../  /../../../../../../../../../../../../../../../..] BB\n[cn335:61077] MCW rank 22 bound to socket 1[core 20[hwt 0-1]]: [../../../../../../../../../../../../../../../../../..][../../  /../../../../../../../../../../../../../../..] BB\n[cn335:61077] MCW rank 23 bound to socket 1[core 21[hwt 0-1]]: [../../../../../../../../../../../../../../../../../..][../../../  /../../../../../../../../../../../../../..] BB\n[cn336:55199] MCW rank 24 bound to socket 0[core 0[hwt 0-1]]: [  /../../../../../../../../../../../../../../../../..][../../../../../../../../../../../../../../../../../..] BB\n[cn336:55199] MCW rank 25 bound to socket 0[core 1[hwt 0-1]]: [../  /../../../../../../../../../../../../../../../..][../../../../../../../../../../../../../../../../../..] BB\n[cn336:55199] MCW rank 26 bound to socket 0[core 2[hwt 0-1]]: [../../  /../../../../../../../../../../../../../../..][../../../../../../../../../../../../../../../../../..] BB\n[cn336:55199] MCW rank 27 bound to socket 0[core 3[hwt 0-1]]: [../../../  /../../../../../../../../../../../../../..][../../../../../../../../../../../../../../../../../..] BB\n[cn336:55199] MCW rank 28 bound to socket 1[core 18[hwt 0-1]]: [../../../../../../../../../../../../../../../../../..][  /../../../../../../../../../../../../../../../../..] BB\n[cn336:55199] MCW rank 29 bound to socket 1[core 19[hwt 0-1]]: [../../../../../../../../../../../../../../../../../..][../  /../../../../../../../../../../../../../../../..] BB\n[cn336:55199] MCW rank 30 bound to socket 1[core 20[hwt 0-1]]: [../../../../../../../../../../../../../../../../../..][../../  /../../../../../../../../../../../../../../..] BB\n[cn336:55199] MCW rank 31 bound to socket 1[core 21[hwt 0-1]]: [../../../../../../../../../../../../../../../../../..][../../../  /../../../../../../../../../../../../../..] BBBinding location\nFigure 14.10 Placement report from the --report-bindings  option to mpirun shows where ranks are bound \nwith the letter B.\n510 CHAPTER  14 Affinity: Truce with the kernel\nWe’ll go over each in turn, along with how OpenMPI allows you to control these things.\nMAPPING  PROCESSES  TO PROCESSORS  OR OTHER  LOCATIONS\nWhen thinking about a parallel application, we have a set of processes and a set of\nprocessors. How do we map the processes to the processors? In the example used\nthroughout section 14.4.2, we wanted to spread the processes over four nodes so that\nevery process has more memory than it would if it were on a single node. The more\ngeneral form for mapping processes in OpenMPI is -mapby  hwresource , where the\nargument hwresource  is any of a large number of hardware components. The most\ncommon include the following:\n--map-by [slot | hwthread | core | socket | numa | node]\nWith the --map-by  option to the mpirun  command, the processes are distributed in a\nround-robin fashion across this hardware resource. The default for the option is\nsocket . Most of these hardware locations are self-explanatory except for slot . Slots\nare the list of possible locations for processes from the environment, the scheduler, or\na host file. This form of the --map-by  option is still limited in its meaning and, there-\nfore, its effect. \n A more general form uses an option called ppr or processes per resource, where n\nis the number of processes. Instead of a round-robin mapping by resource, you can\nspecify a block of processes per hardware resource:\n--map-by ppr:n:hwresource\nOr, more explicitly\n--map-by ppr:n:[slot | hwthread | core | socket | numa | node]\nIn our earlier examples, we used the simpler option of --npernode  8. In this more\ngeneral form, it would be shorthand for\n--map-by ppr:8:node\nIf the level of control from the previous options to mpirun  is not sufficient, you can\nspecify a list of processor numbers to map with the --cpu-list  <logical  processor\nnumbers>  option, where the processor numbers are a list that corresponds to the list\nfrom lstopo  or lscpu . This option also binds the processes to the logical (virtual) pro-\ncessor at the same time.\nORDERING  OF MPI R ANKS\nAnother thing you might want to control is the ordering of your MPI ranks. You may\nwant adjacent MPI ranks to be close to each other in physical processor space if they\ncommunicate a lot with each other. This reduces the cost of the communication\nbetween these ranks. Usually, it is sufficient to control this with the block size of the",10172
191-14.5 Affinity for MPI plus OpenMP.pdf,191-14.5 Affinity for MPI plus OpenMP,"511 Affinity for MPI plus OpenMP\ndistribution during mapping, but you can get additional control with the --rank-by\noption:\n--rank-by ppr:n:[slot | hwthread | core | socket | numa | node]\nAn even more general option is to use a rank file:\n--rankfile <filename>\nWhile you can fine-tune the placement of your MPI ranks with these commands and\nperhaps gain a couple of percent in performance, it is difficult to come up with the\noptimum formula. \nBINDING  PROCESSES  TO HARDWARE  COMPONENTS\nThe last piece to control is affinity itself. Affinity  is the process of binding the process\nto the hardware resource. The option is similar to the previous ones:\n--bind-to [slot | hwthread | core | socket | numa | node]\nThe default setting of core  is sufficient for most MPI applications (without the\n--bind-to  option the default is socket  for greater than two processes as mentioned\nin section 14.4.1). But there are cases where that affinity setting causes problems. \n As we saw in the example for figure 14.8, the affinity is set to the two hyperthreads\non the hardware core. We might want to try --map-to  core  --bind-to  hwthread  to\ndistribute the processes across the cores but bind each process more tightly to a single\nhyperthread. The performance difference from such fine-tuning is probably small.\nThe greater problem comes when we try to implement a hybrid MPI and OpenMP\napplication. It is important to realize that child processes inherit the affinity settings\nof their parent. If we use the options of npersocket  4 --bind-to  core  and then\nlaunch two threads, we have two locations for the threads to run (two hyperthreads\nper core), so we are Ok. If we launch four threads, these will share only two logical\nprocessor locations and performance will be limited.\n We saw earlier in this section that there are a lot of options for controlling process,\nplacement, and affinity. Indeed, there are too many combinations to even fully\nexplore as we did in section 14.3 for OpenMP. In most cases, we should be satisfied\nwith getting reasonable settings that reflect the needs of our applications.\n14.5 Affinity for MPI plus OpenMP\nOur goal in this section is to understand how to set affinity for hybrid MPI and\nOpenMP applications. Getting affinity right for these hybrid situations can be tricky.\nFor this exploration, we’ve created a hybrid stream triad example with MPI and\nOpenMP. We have also modified the placement report used throughout this chapter\nto output information for hybrid MPI and OpenMP applications. The following listing\nshows the modified subroutine, place_report_  mpi_omp.c .\n512 CHAPTER  14 Affinity: Truce with the kernel\nStreamTriad/place_report_mpi_omp.c\n41 void place_report_mpi_omp(void)\n42 {\n43    int rank;\n44    MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n45\n46    int socket_global[144];\n47    char clbuf_global[144][7 * CPU_SETSIZE];\n48 \n49    #pragma omp parallel\n50    {\n51       if (omp_get_thread_num() == 0 && rank == 0){\n52          printf(""Running with %d thread(s)\n"",omp_get_num_threads());\n53          int bind_policy = omp_get_proc_bind();\n54          switch (bind_policy)\n55          {\n56             case omp_proc_bind_false:\n57                printf(""  proc_bind is false\n"");\n58                break;\n59             case omp_proc_bind_true:\n60                printf(""  proc_bind is true\n"");\n61                break;\n62             case omp_proc_bind_master:\n63                printf(""  proc_bind is master\n"");\n64                break;\n65             case omp_proc_bind_close:\n66                printf(""  proc_bind is close\n"");\n67                break;\n68             case omp_proc_bind_spread:\n69                printf(""  proc_bind is spread\n"");\n70          }\n71          printf(""  proc_num_places is %d\n"",omp_get_num_places());\n72       }\n73 \n74       int thread = omp_get_thread_num();\n75       cpu_set_t coremask;\n76       char clbuf[7 * CPU_SETSIZE], hnbuf[64];\n77       memset(clbuf, 0, sizeof(clbuf));\n78       memset(hnbuf, 0, sizeof(hnbuf));\n79       gethostname(hnbuf, sizeof(hnbuf));\n80       sched_getaffinity(0, sizeof(coremask), &coremask);\n81       cpuset_to_cstr(&coremask, clbuf);\n82       strcpy(clbuf_global[thread],clbuf);\n83       socket_global[omp_get_thread_num()] = omp_get_place_num();\n84       #pragma omp barrier\n85       #pragma omp master\n86       for (int i=0; i<omp_get_num_threads(); i++){\n87          printf(""Hello from rank %02d,""       \n                   "" thread %02d, on %s.""        \n88                 "" (core affinity = %2s)""      \n                   "" OpenMP socket is %2d\n"",    \n89                  rank, i, hnbuf,              \n                    clbuf_global[i],             \n                    socket_global[i]);           Listing 14.2 MPI and OpenMP placement reporting tool hybrid stream triad\nMerges OpenMP \nand the MPI \naffinity report\n513 Affinity for MPI plus OpenMP\n90       }\n91    }\n92 }\nWe start this example by compiling the stream triad application. The stream triad\ncode is at https:/ /github.com/EssentialsofParallelComputing/Chapter14  in the Stream-\nTriad directory. Compile the code with\nmkdir build && cd build\n./cmake -DCMAKE_VERBOSE=1 ..\nmake\nWe ran this code on our Skylake Gold processor with 44 hardware processors and two\nhyperthreads each. We placed the two OpenMP threads on the hyperthreads and then\nan MPI rank on each hardware core. The following commands accomplish this layout:\nexport OMP_NUM_THREADS=2\nmpirun -n 44 --map-by socket ./StreamTriad\nThe stream triad code has a call to our placement report from listing 14.2. Figure 14.11\nshows the output.\nAs the output in figure 14.11 shows, we succeeded in getting the ranks distributed\nacross the NUMA domains in a round-robin manner, keeping the two threads\ntogether. This should give us good bandwidth from main memory. The affinity con-\nstraints are only sufficient to keep the processes within the NUMA domain and let the\nscheduler move the processes around as they wish. The scheduler can place thread 0\non any of 44 different virtual processors, including 0–21 or 44–65. The numbering\ncan be confusing; 0 and 44 are two hyperthreads on the same physical core.\n Now let’s try to obtain more affinity constraints. For this, we need to use the form\n–mapby  ppr:N:socket:PE=N . This command gives us the ability to spread out the pro-\ncesses with a specified spacing and specify how many MPI ranks to place on each\nsocket. It is hard to unbundle the complexity of the option. \n Let’s start with the ppr:N:socket  part. We want half of our MPI ranks on each\nsocket. This should be 22 MPI ranks per socket or ppr:22:socket . The last partHello from rank 00, thread 00, on cn618. (core affinity = 0-21,44-65) OpenMP socket is -1\nHello from rank 00, thread 01, on cn618. (core affinity = 0-21,44-65) OpenMP socket is -1\nHello from rank 01, thread 00, on cn618. (core affinity = 22-43,66-87) OpenMP socket is -1\nHello from rank 01, thread 01, on cn618. (core affinity = 22-43,66-87) OpenMP socket is -1\n< ... skipping output ... >NUMA domain\nFigure 14.11 The MPI ranks are placed in a round-robin fashion across the sockets with two slots to \naccommodate the two OpenMP threads. The placement is restricted to a NUMA domain to keep memory \nclose to the threads. The processes are not bound tightly to any particular virtual core, and the scheduler \ncan move these around freely within the NUMA domain.\n514 CHAPTER  14 Affinity: Truce with the kernel\ndetermines how many processors we want between the placement of processes. We\nwant two threads for each MPI rank, so we want two virtual processors in each block.\nThe specification is for hardware cores. It is important to know that each hardware\ncore contains two virtual processors. Therefore, you only need one hardware core\n(PE=1 ). We then pin the threads to a hardware thread. For rank 0, we should get the\nfirst hardware core with the virtual processors 0 and 44. That gives us the following\ncommands:\nexport OMP_NUM_THREADS=2\nexport OMP_PROC_BIND=true\nmpirun -n 44 --map-by ppr:22:socket:PE=1 ./StreamTriad\nWhew! That was complicated. Did we get it right? Well, let’s check the output from the\ncommand as shown in figure 14.12.\nFrom the output in figure 14.12, we have the threads locked down where we want\nthem. We also have the MPI rank pinned to the hardware core. You can verify this by\nunsetting the OMP_PROC_BIND environment variable ( unset  OMP_PROC_BIND ) and\nthe output (figure 14.13) confirms that the rank is bound to two logical processors,\ncomposing a single hardware core.\nWe’ve worked through one case and were able to get the affinity settings the way we\nwanted. But now you want to know if we can run more than two OpenMP threads andHello from rank 00, thread 00, on cn626. (core affinity = 0) OpenMP socket is 0\nHello from rank 00, thread 01, on cn626. (core affinity = 44) OpenMP socket is 1\nHello from rank 01, thread 00, on cn626. (core affinity = 1) OpenMP socket is 0\nHello from rank 01, thread 01, on cn626. (core affinity = 45) OpenMP socket is 1\n< ... skipping output ... >Hyperthreads\nFigure 14.12 The process and thread affinity are now constrained to a logical core, and the two OpenMP \nthreads per rank are located on the hyperthread pairs ( 0 and 44 in the figure). The ranks are packed close in \norder to reduce communication costs for more complicated programs. The MPI ranks are pinned to hardware \ncores and the thread affinity is to the hyperthread.\nHardware core\nHello from rank 00, thread 00, on cn610. (core affinity = 0,44) OpenMP socket is -1\nHello from rank 00, thread 0l, on cn610. (core affinity = 0,44) OpenMP socket is -1\nHello from rank 01, thread 00, on cn610. (core affinity = 1,45) OpenMP socket is -1\nHello from rank 01, thread 0l, on cn610. (core affinity = 1,45) OpenMP socket is -1\n< ... skipping output ... >\nFigure 14.13 Output without OMP_PROC_BIND=true  shows that the MPI ranks are pinned to hardware cores.\n515 Affinity for MPI plus OpenMP\nhow the program performs. Let’s take a look at a set of commands that test any num-\nber of threads that divides into the number of processors evenly. The following listing\nshows the key scripting commands.\nExtracted from StreamTriad/run.sh\n  1 #!/bin/sh\n  2 LOGICAL_PES_AVAILABLE=`lscpu |\                   \n     grep '^CPU(s):' |cut -d':' -f 2`                 \n  3 SOCKETS_AVAILABLE=`lscpu |\                       \n     grep '^Socket(s):' |cut -d':' -f 2`              \n  4 THREADS_PER_CORE=`lscpu |\                        \n     grep '^Thread(s) per core:' |cut -d':' -f 2`     \n  5 POST_PROCESS=""|& grep -e Average -e mpirun |sort -n -k 4""\n  6 THREAD_LIST_FULL=""2 4 11 22 44""\n  7 THREAD_LIST_SHORT=""2 11 22""\n  8 \n  9 unset OMP_PLACES\n 10 unset OMP_CPU_BIND\n 11 unset OMP_NUM_THREADS\n 12 \n     < ... basic tests not shown ... >\n 21 \n 22 export OMP_PROC_BIND=true                     \n     < ... first loop block not shown ... >\n 37 for num_threads in ${THREAD_LIST_FULL}\n 38 do\n 39    export OMP_NUM_THREADS=${num_threads}}     \n 40    \n 41    HW_PES_PER_PROCESS=$((${OMP_NUM_THREADS}/\n                    ${THREADS_PER_CORE}))          \n 42    MPI_RANKS=$((${LOGICAL_PES_AVAILABLE}/ \    \n                    ${OMP_NUM_THREADS}))           \n 43    PES_PER_SOCKET=$((${MPI_RANKS}/\            \n                         ${SOCKETS_AVAILABLE}))    \n 44    \n 45    RUN_STRING=""mpirun -n ${MPI_RANKS} \          \n         --map-by ppr:${PES_PER_SOCKET}:socket:PE=${HW_PES_PER_PROCESS} \\n         ./StreamTriad ${POST_PROCESS}"" \n 46    echo ${RUN_STRING}\n 47    eval ${RUN_STRING}\n 48 done\n      < ... additional loop blocks ... >\nTo make the script portable, we grab the hardware characteristics using the lscpu\ncommand. We then set the desired OpenMP environment parameters. We could set\nOMP_PROC_BIND to true , close , or spread  with the same result for this case, where\nall the slots are filled. Then we calculate the variables needed for the mpirun  com-\nmand and launch the job.Listing 14.3 Setting affinity for hybrid MPI and OpenMP\nGets hardware \ncharacteristics\nSets OMP \nenvironment \nvariables\nCalculates \nneeded values\nFills the mpirun \ncommand",12316
192-14.6 Controlling affinity from the command line.pdf,192-14.6 Controlling affinity from the command line,,0
193-14.6.1 Using hwloc-bind to assign affinity.pdf,193-14.6.1 Using hwloc-bind to assign affinity,"516 CHAPTER  14 Affinity: Truce with the kernel\n In the full stream triad example in listing 14.2, we tested a combination of thread\nsizes and MPI ranks that divide evenly into 88 processes. We followed that with 44 total\nprocesses where we skip the hyperthreads because we didn’t really get any better per-\nformance with them (section 14.3). The performance results are pretty constant over\nthe set of tests. That is because all that is being measured is the bandwidth from main\nmemory. There is little work being done and no MPI communication. The benefits of\nhybrid MPI and OpenMP are limited in this situation. Where we would expect to see\nbenefits is in much larger simulations where substituting an OpenMP thread for a\nMPI rank would\nReduce the MPI buffer memory requirements\nCreate larger domains that consolidate and reduce ghost cell regions\nReduce contention for processors on a node for a single network interface\nAccess vector units and other processor components that are not fully utilized\n14.6 Controlling affinity from the command line\nThere are also general ways to control affinity from the command line. The com-\nmand-line tools can help in situations where your MPI or special parallel application\ndoesn’t have built-in options to control affinity. These tools can also help with general-\npurpose applications by binding these close to important hardware components such\nas graphics cards, network ports, and storage devices. In this section, we cover two\ncommand-line options: the hwloc and likwid suite of tools. These tools are developed\nwith high-performance computing in mind. \n14.6.1 Using hwloc-bind to assign affinity\nThe hwloc project was developed by INRIA, the French National Institute for\nResearch in Computer Science and Automation. A subproject of the OpenMPI proj-\nect, hwloc implements the OpenMPI placement and affinity capabilities that we saw in\nsections 14.4 and 14.5. The hwloc package is also a standalone package with command-\nline tools. Because there are many hwloc tools, as an introduction, we’ll just look at a\ncouple of these. We’ll use hwloc-calc to get a list of hardware cores and hwloc-bind to\nbind these. \n Using hwloc-bind is simple. Just prefix the application with hwloc-bind  and then add\nthe hardware location where you want it to bind. For our application, we’ll use the\nlstopo  command. The lstopo  command is also part of the hwloc tools. Here is our one-\nliner to launch the job on all the hardware cores and bind the processes to the cores:\nfor core in `hwloc-calc --intersect core --sep "" "" all`; do hwloc-bind \\n    core:${core} lstopo --no-io --pid 0 & done\nThe --intersect  core  option only uses hardware cores. The --sep  "" "" says to sepa-\nrate the numbers in the output with spaces instead of commas. The result of this com-\nmand on our usual Skylake Gold processor launches 44 lstopo graphic windows, each\n517 Controlling affinity from the command line\nlooking similar to that in figure 14.14. Each window has the bound locations high-\nlighted in green.\nWe could use a similar command to launch two processes on the first core of each\nsocket. For example\nfor socket in `hwloc-calc --intersect socket \\n    --sep "" "" all`; do hwloc-bind \\n    socket:${socket}.core:0 lstopo --no-io --pid 0 & done\nThe following listing shows how we can build a general-purpose mpirun  command\nwith binding.\nMPI/mpirun_distrib.sh\n 1 #!/bin/sh\n 2 PROC_LIST=$1\n 3 EXEC_NAME=$2\n 4 OUTPUT=""mpirun ""     \n 5 for core in ${PROC_LIST}\n 6 do\n 7     OUTPUT=""$OUTPUT -np 1""\              \n              "" hwloc-bind core:${core}""\   \n              "" ${EXEC_NAME} :""             \n 8 done\n 9 OUTPUT=`echo ${OUTPUT} | sed -e 's/:$/\n/'`     \n10 eval ${OUTPUT}Listing 14.4 Using hwloc-bind to bind processes\nFigure 14.14 The lstopo image shows the bound location in green (shaded core) at the lower left. This shows \nthat process 22 is bound to the 22nd and 66th virtual cores, which are hyperthreads for a single physical core.\nInitializes this \nstring with mpirun\nAppends another \nMPI rank launch \nwith binding\nStrips last colon \nand substitutes a \nnew line",4174
194-14.6.2 Using likwid-pin An affinity tool in the likwid tool suite.pdf,194-14.6.2 Using likwid-pin An affinity tool in the likwid tool suite,"518 CHAPTER  14 Affinity: Truce with the kernel\nNow we can launch our MPI affinity application from section 14.4 on the first core of\neach socket with this command:\n./mpirun_distrib.sh ""1 22"" ./MPIAffinity\nThis mpirun_distrib script builds the following command and executes it:\nmpirun -np 1 hwloc-bind core:1 ./MPIAffinity : -np 1 hwloc-bind core:22\n  ./MPIAffinity\n14.6.2 Using likwid-pin: An affinity tool in the likwid tool suite\nThe likwid-pin tool is one of the many great tools from the likwid (“Like I Knew What\nI’m Doing”) team at the University of Erlangen. We saw our first likwid tool, likwid-\nperfctr in section 3.3.1. The likwid tools in this section are command-line tools to set\naffinity. We’ll look at variants of the tool for OpenMP threads, MPI, and hybrid MPI\nplus OpenMP applications. The basic syntax for selecting processor sets in likwid uses\nthese options:\nDefault (physical numbering)\nN (node-level numbering)\nS (socket-level numbering)\nC (last level cache numbering)\nM (NUMA memory domain numbering)\nTo set the affinity, use this syntax: -c <N,S,C,M>:[n1,n2,n3-n4] . To get a list of the\nnumbering schemes, use the command likwid-pin  -p. Understanding how likwid-\npin works is best gained from examples and experimentation. \nPINNING  OPENMP THREADS  WITH LIKWID -PIN\nThis example shows how to use likwid-pin with OpenMP applications:\nexport OMP_NUM_THREADS=44\nexport OMP_PROC_BIND=spread\nexport OMP_PLACES=threads\n./vecadd_opt3\nTo get this same pinning result with likwid-pin for OpenMP applications, we use the\nsocket (S) option. In the following, we distribute 22 threads on each socket, where\nthe two pin sets are separated and concatenated with the @ symbol:\nlikwid-pin -c S0:0-21@S1:0-21 ./vecadd_opt3\nThe OMP environment variables are not necessary when using likwid-pin and are\nmostly ignored. The number of threads is determined from the pin set lists. For this\ncommand, it is 44. We ran the vecadd example from section 14.3, configured with the\n-DCMAKE_VERBOSE  option to get our placement report as figure 14.15 shows.\n519 Controlling affinity from the command line\nOur placement report shows that the OMP environment variables are not set and that\nOpenMP has not placed and pinned the threads in the OpenMP sockets. And yet,\nwe get the same placement and pinning from the likwid-pin tool with the same per-\nformance results. We have just confirmed that the OMP environment variables are not neces-\nsary with likwid-pin as we claimed in the previous paragraph.  One thing to note is that if\nyou set the OMP_NUM_THREADS environment variable to something other than\nthe number of threads in the pin sets, the likwid tool distributes the threads from the\nOMP_NUM_THREADS  variable across the processors specified in the pin sets. When there\nare more threads than processors, the tool wraps the thread placement around on the\navailable processors.\nPINNING  MPI RANKS  WITH LIKWID -MPIRUN\nThe likwid pinning functionality for MPI applications is included in the likwid-mpirun\ntool. You can use this tool as a substitute for mpirun in most MPI implementations.\nLet’s look at the MPIAffinity example from section 14.4. \nExample: Pinning MPI ranks with likwid-mpirun\nRun the MPIAffinity example on 44 ranks and use the likwid-mpirun  command to\npin the ranks to the hardware cores. By default, likwid-mpirun pins the ranks to the\ncores, so we need to use the likwid-mpirun  command to get what we usually want\nwithout any additional options:\nlikwid-mpirun -n 44 ./MPIAffinity |sort -n -k 4[pthread wrapper]\n[pthread wrapper] MAIN -> 0\n[pthread wrapper] PIN MASK: 0->1 1->2 2->3 3->4 4->5 5->6 6->7 7->8 8->9 9->10\n10->11 11->12 12->13 13->14 14->15 15->16 16->17 17->18 18->19 19->20 20->21\n21->22 22->23 23->24 24->25 25->26 26->27 27->28 28->29 29->30 30->31 31->32\n32->33 33->34 34->35 35->36 36->37 37->38 38->39 39->40 40->41 41->42 42->43\n[pthread wrapper] SKIP MASK: 0x0\nthreadid 47149577160576 -> core 1 - OK\nthreadid 47149581363200 -> core 2 - OK\n> < ... skipped output ...\nthreadid 47152378182656 -> core 42 - OK\nthreadid 47152382389376 -> core 43 - OK\nRunning with 44 thread(s)\nproc_bind is false\nproc_num_places is 0\nHello from thread 0: (                 ) OpenMP socket is -1 core affinity = 0\nHello from thread 1: (core affinity = 1) OpenMP socket is -1\n< ... skipped output ... >\nHello from thread 42: (core affinity = 42) OpenMP socket is -1\nHello from thread 43: (core affinity = 43) OpenMP socket is -1\n0.016692Physical core\nFigure 14.15 The likwid-pin output is at the top of the screen, followed by our placement report output. \nThe output shows that the threads are pinned to the 44 physical cores.",4733
195-14.7.2 Changing your process affinities during run time.pdf,195-14.7.2 Changing your process affinities during run time,"520 CHAPTER  14 Affinity: Truce with the kernel\nFigure 14.16 shows the output from our placement report for this example.\nThat was easy! As figure 14.16 shows, likwid-mpirun pins the ranks to the hardware\ncores. Let’s move on to an example where we have to provide some options to the\ncommand. \n14.7 The future: Setting and changing affinity at run time\nWhat if the user didn’t need to worry about affinity? It is challenging to get users to\nuse the complicated invocations to properly place and pin processes. It might make\nmore sense in many cases to embed the pinning logic into the executable. One way to\ndo this would be to query information about the hardware and set the affinity appro-\npriately. Few applications have yet undertaken this approach, but we expect to see\nmore that do in the future. \n Some applications not only set their affinity at run time but also modify the affinity\nto adapt to changing characteristics during run time! This innovative technique was\ndeveloped by Sam Gutiérrez of Los Alamos National Laboratory in his QUO library.\nPerhaps you have an application that uses all MPI ranks on a node, but it calls a library\nthat uses a combination of MPI ranks and OpenMP threads. The QUO library pro-\nvides a simple interface built on top of hwloc to set proper affinities. It can then push\nthe settings onto a stack, quiesce the processors, and set a new binding policy. We’ll\nlook at examples of initiating process binding within your application and changing it\nduring run time in the following sections.Example: Options for pinning MPI ranks with likwid-mpirun\nWe start with the basic command:\nlikwid-mpirun -n 22  ./MPIAffinity |sort -n -k 4\nThe ranks are distributed across the first 22 hardware cores on socket 0 and none\non socket 1. We showed earlier that you need to distribute the processes across both\nsockets to get the full bandwidth from main memory. Adding the -nperdomain  option\nlets us specify how many sockets per NUMA domain and the S:11 pin set gets the\nright numbers for 11 ranks on the socket. The command now looks like\nlikwid-mpirun -n 22 -nperdomain S:11  ./MPIAffinity |sort -n -k 4WARN: Cannot extract OpenMP vendor from executable or commandline, assuming no OpenMP\nHello from rank 0, on cn630. (                 ) core affinity = 0\nHello from rank 1, on cn630. (core affinity = 1)\n< ... skipping output ... .\nHello from rank 42, on cn630. (core affinity = 42)\nHello from rank 43, on cn630. (core affinity = 43)Processor core\nFigure 14.16 The placement report for likwid-mpirun shows that each rank is pinned to cores in numeric order.\n521 The future: Setting and changing affinity at run time\n14.7.1 Setting affinities in your executable\nSetting your process placement and affinities in your application means that you no\nlonger have to deal with complicated mpirun commands or portability between MPI\nimplementations. Here we use the QUO library to implement this binding to all the\nc o r e s  o n  a  S k y l a k e  G o l d  p r o c e s s o r .  T h e  o p e n  s o u r c e  Q U O  l i b r a r y  i s  a v a i l a b l e  a t\nhttps:/ /github.com/LANL/libquo.git . First, we build the executable in the Quo direc-\ntory and run the application with the number of hardware cores on your system:\nmake autobind\nmpirun -n 44 ./autobind\nThe source code for autobind is shown in listing 14.5. The program has the following\nsteps. Our placement reporting routine is called before and afterward to show the\nprocess bindings.\n1Initialize QUO\n2Set affinities to the hardware cores\n3Distribute the processes and bind these to the cores\n4Return to the initial affinities\nQuo/autobind.c\n31 int main(int argc, char **argv)\n32 {\n33     int ncores, nnoderanks, noderank, rank, nranks;\n34     int work_member = 0, max_members_per_res = 2, nres = 0;\n35     QUO_context qcontext;\n36 \n37     MPI_Init(&argc, &argv);\n38     QUO_create(&qcontext, MPI_COMM_WORLD);      \n39     MPI_Comm_size(MPI_COMM_WORLD, &nranks);\n40     MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n41     QUO_id(qcontext, &noderank);             \n42     QUO_nqids(qcontext, &nnoderanks);        \n43     QUO_ncores(qcontext, &ncores);           \n44 \n45     QUO_obj_type_t tres = QUO_OBJ_NUMANODE;  \n46     QUO_nnumanodes(qcontext, &nres);         \n47     if (nres == 0) {                         \n48         QUO_nsockets(qcontext, &nres);       \n49         tres = QUO_OBJ_SOCKET;               \n50     }                                        \n51 \n52     if ( check_errors(ncores, nnoderanks, noderank, nranks, nres) )\n53         return(-1);\n54 \n55     if (rank == 0)\n56         printf(""\nDefault binding for MPI processes\n\n"");\n57     place_report_mpi();      \n58 Listing 14.5 Using QUO to bind processes from your executable\nInitializes QUO \ncontext\nGets system \ninformation\nReports default bindings\n522 CHAPTER  14 Affinity: Truce with the kernel\n59     SyncIt();\n60     QUO_bind_push(qcontext,                  \n                     QUO_BIND_PUSH_PROVIDED,    \n61                   QUO_OBJ_CORE, noderank);   \n62     SyncIt();\n63 \n64     QUO_auto_distrib(qcontext, tres,        \n                        max_members_per_res,   \n65                      &work_member);         \n66     if (rank == 0)\n67         printf(""\nProcesses should be pinned to the hw cores\n\n"");\n68     place_report_mpi();     \n69 \n70     SyncIt();\n71     QUO_bind_pop(qcontext);    \n72     SyncIt();\n73 \n74     QUO_free(qcontext);\n75     MPI_Finalize();\n76     return(0);\n77 }\nWe need to be careful to synchronize processes as we change the bindings. To ensure\nthat, in the following listing, we use an MPI barrier and a micro sleep call in the SyncIt\nroutine.\nQuo/autobind.c\n23 void SyncIt(void)\n24 {\n25     int rank;\n26     MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n27     MPI_Barrier(MPI_COMM_WORLD);     \n28     usleep(rank * 1000);    \n29 }\nThe output from the autobind application (figure 14.17) clearly shows the bindings\nchanged from sockets to the hardware cores.\n14.7.2 Changing your process affinities during run time\nSuppose we have an application with one part that wants to use all MPI ranks and\nanother part that works best with OpenMP threads. To handle this, we need to switch\nthe affinities during run time. This is the scenario that QUO is designed for! The steps\nfor this include\n1Initialize QUO\n2Set the process bindings to cores for MPI region\n3Expand the bindings to the whole node for the OpenMP region\n4Return to the settings for MPIListing 14.6 SyncIt  subroutineSets new bindings \nto core\nDistributes and \nbinds MPI ranks\nReports new bindings\nPops off the bindings \nand returns to initial \nsettings\nStandard MPI \nbarrier\nAdditional micro sleep\n523 The future: Setting and changing affinity at run time\nLet’s see how this is done with Quo in the following listing.\nQuo/dynaffinity.c\n45 int main(int argc, char **argv)\n46 {\n47     int rank, noderank, nnoderanks;\n48     int work_member = 0, max_members_per_res = 44;\n49     QUO_context qcontext;\n50 \n51     MPI_Init(&argc, &argv);\n52     MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n53     QUO_create(&qcontext, MPI_COMM_WORLD);     \n54 \n55     node_info_report(qcontext, &noderank, &nnoderanks);\n56 \n57     SyncIt();\n58     QUO_bind_push(qcontext,                 \n                     QUO_BIND_PUSH_PROVIDED,   \n59                   QUO_OBJ_CORE, noderank);  \n60     SyncIt();\n61 \n62     QUO_auto_distrib(qcontext, QUO_OBJ_SOCKET,   \n                        max_members_per_res,        \n63                      &work_member);              \n64 \n65     place_report_mpi_quo(qcontext);  \n66 \n67     /* change binding policies to accommodate OMP threads on node 0 */\n68     bool on_rank_0s_node = rank < nnoderanks;\n69     if (on_rank_0s_node) {\n70         if (rank == 0) {\n71             printf(""\nEntering OMP region...\n\n"");\n72             // expands the caller's cpuset\n               //    to all available resources on the node.Listing 14.7 Dynamic affinity demo switching from MPI to OpenMPDefault binding for MPI processes\nHello from process93096, rank 0, on cn630. (core affinity = 0-21,44-65)\nHello from process 93093, rank 1, on cn630. (core affinity = 22-43,66-87)\n< ... skipping output ... >\nHello from process 93162, rank 42, on cn630. (core affinity = 0-21,44-65)\nHello from process 93159, rank 43, on cn630. (core affinity = 22-43,66-87)\nProcesses should be pinned to the hw cores\nHello from process 93096, rank 0, on cn630. (core affinity = 0,44)\nHello from process 93093, rank 1, on cn630. (core affinity = 1,45)\n< ... skipping output ... >\nHello from process 93162, rank 42, on cn630. (core affinity = 42,86)\nHello from process 93159, rank 43, on cn630. (core affinity = 43,87)Socket\nCore\nFigure 14.17 The output from the autobind demo shows cores initially bound to sockets, but \nafterwards, these are bound to hardware cores.\nInitializes \nQUO context\nSets affinities to \nhardware cores\nDistributes and \nbinds MPI ranks\nReports process affinities \nfor all MPI regions\n524 CHAPTER  14 Affinity: Truce with the kernel\n73             QUO_bind_push(qcontext,             \n                             QUO_BIND_PUSH_OBJ,    \n                             QUO_OBJ_SOCKET, -1);  \n74             report_bindings(qcontext, rank);      \n75             /* do the OpenMP calculation */\n76             place_report_mpi_omp();             \n77             /* revert to old binding policy */\n78             QUO_bind_pop(qcontext);               \n79         }\n80         /* QUO_barrier because it's cheaper than\n              MPI_Barrier on a node. */\n81         QUO_barrier(qcontext);\n82     }\n83     SyncIt();\n84 \n85     // Wrap-up\n86     QUO_free(qcontext);\n87     MPI_Finalize();\n88     return(0);\n89 }\nWe can run the dynaffinity application with the number of hardware cores on our sys-\ntem with\nmake dynaffinity\nmpirun -n 44 ./dynaffinity\nWe again use our reporting routines to check the process bindings for the MPI region\nand for OpenMP. Figure 14.18 displays the output.\n The output in figure 14.18 shows that the process bindings changed between the\nMPI and the OpenMP regions, accomplishing a dynamic modification of the affinities\nduring run time.Sets affinity to \nwhole system\nReports CPU masks \nfor OpenMP region\nReports process \naffinities for OpenMP \nregion\nPops off bindings \nand returns to \nMPI bindings\nNodeinfo: nnodes 1 nnoderanks 44 nsockets 2 ncores 44 nhwthreads 88\nHello from process 96779, rank 0, on cn630. (                    ) cbind [0x00001000,0x00000001] core affinity = 0,44\nHello from process 96781, rank 1, on cn630. (core affinity = 1,45) cbind [0x00002000,0x00000002]\n<... skipping output ...>\nHello from process 96851, rank 42, on cn630. (core affinity = 42,86) cbind [0x00400000,0x00000400,0x0]\nHello from process 96849, rank 43, on cn630. (core affinity = 43,87) cbind [0x00800000,0x00000800,0x0]\nEntering OMP region...\nrank O's cpuset: Ox00ffffff,Oxffffffff,Oxffffffff\nRunning with 44 thread(s)\nproc_bind is false\nproc_num_places is 0\nHello from rank 00, thread 00, on cn625. (                    ) OpenMP socket is -1 core affinity = 0-87\nHello from rank 00, thread 01, on cn625. (core affinity = 0-87) OpenMP socket is -1\n<... skipping output ...>\nHello from rank 00, thread 42, on cn625. (core affinity = 0-87) OpenMP socket is -1\nHello from rank 00, thread 43, on cn625. (core affinity = 0-87) OpenMP socket is -1Any processor coreHardware core\nFigure 14.18 For the MPI region, the processes are bound to the hardware cores. When we enter the OpenMP \nregion, the affinities are expanded to the whole node.",11742
196-14.8 Further explorations.pdf,196-14.8 Further explorations,,0
197-15.2 How not to be a nuisance when working on a busy cluster.pdf,197-15.2 How not to be a nuisance when working on a busy cluster,"525 Further explorations\n14.8 Further explorations\nThe handling of process placement and bindings is relatively new. Watch for presenta-\ntions in the MPI and OpenMP communities for additional developments in this area.\nIn the next section, we list some of the most current materials on affinity that we rec-\nommend for additional reading. We’ll follow the additional reading with some exer-\ncises to explore the topic further.\n14.8.1 Additional reading\nThe process placement reporting programs used in this chapter for OpenMP, MPI,\nand MPI plus OpenMP are modified from the xthi.c program used in training for sev-\neral HPC sites. Here are references to papers and presentations that use it to explore\naffinities:\nY. He, B. Cook, et al., “Preparing NERSC users for Cori, a Cray XC40 system\nwith Intel many integrated cores” In Concurrency Computat: Pract Exper. , 2018;\n30:e4291 ( https:/ /doi.org/10.1002/cpe.4291 ).\nArgonne National Laboratory, “Affinity on Theta,” at https:/ /www.alcf.anl.gov/\nsupport-center/theta/affinity-theta .\nNational Energy Research Scientific Computing Center (NERSC), “Process and\nThread Affinity,” at https:/ /docs.nersc.gov/jobs/affinity/ .\nHere’s a good presentation on OpenMP that includes a discussion on affinity and how\nto handle it:\nT. Mattson and H. He, “OpenMP: Beyond the common core,” at http:/ /mng.bz/\naK47 .\nWe only covered part of the options for the mpirun command in OpenMPI. For explor-\ning more capabilities, see the man page for OpenMPI: \nhttps:/ /www.open-mpi.org/doc/v4.0/man1/mpirun.1.php.\nPortable Hardware Locality (hwloc) is a subproject of The Open MPI Project. It is a\nstandalone package that works equally well with either OpenMPI or MPICH and has\nbecome the universal hardware interface for most MPI implementations and many\nother parallel programming software applications. For further information, see the\nfollowing references:\nThe hwloc project main page https:/ /www.open-mpi.org/projects/hwloc/ , where\nyou’ll also find some key presentations.\nB. Goglin, “Understanding and managing hardware affinities with Hardware\nLocality (hwlooc),” High Performance and Embedded Architecture and Compilation\n(HiPEAC, 2013), http:/ /mng.bz/gxYV .\n526 CHAPTER  14 Affinity: Truce with the kernel\nThe “Like I Knew What I’m Doing” (likwid) suite of tools is well regarded for its sim-\nplicity, usability, and good documentation. Here is a good starting point to investigate\nthese tools further:\nUniversity of Erlangen-Nuremberg’s performance monitoring and benchmarking\nsuite, https:/ /github.com/RRZE-HPC/likwid/wiki .\nThis conference presentation about the QUO library gives a more complete overview\nand the philosophy behind it:\nS. Gutiérrez et al., “Accommodating Thread-Level Heterogeneity in Coupled Paral-\nlel Applications,” https:/ /github.com/lanl/libquo/blob/master/docs/slides/gutier\nrez-ipdps17.pdf , 2017 International Parallel and Distributed Processing Symposium\n(IPDPS17).\n14.8.2 Exercises\n1Generate a visual image of a couple of different hardware architectures. Dis-\ncover the hardware characteristics for these devices.\n2For your hardware, run the test suite using the script in listing 14.1. What did\nyou discover about how to best use your system?\n3Change the program used in the vector addition (vecadd_opt3.c) example in\ns e c t i o n  1 4 . 3  t o  i n c l u d e  m o r e  f l o a t i n g - p o i n t  o p e r a t i o n s .  T a k e  t h e  k e r n e l  a n d\nchange the operations in the loop to the Pythagorean formula:\nc[i] = sqrt(a[i]*a[i] + b[i]*b[i]);\nHow do your results and conclusions about the best placement and bindings\nchange? Do you see any benefit from hyperthreads now (if you have those)?\n4For the MPI example in section 14.4, include the vector add kernel and gener-\nate a scaling graph for the kernel. Then replace the kernel with the Pythago-\nrean formula used in exercise 3.\n5Combine the vector add and Pythagorean formula in the following routine\n(either in a single loop or two separate loops) to get more data reuse: \nc[i] = a[i] + b[i];\nd[i] = sqrt(a[i]*a[i] + b[i]*b[i]);\nHow does this change the results of the placement and binding study?\n6Add code to set the placement and affinity within an application from one of\nthe previous exercises.\n527 Summary\nSummary\nThere are tools that show your process placement. These tools can also show\nyou the affinity for your processes.\nUse process placement for your parallel applications. This gives you full main\nmemory bandwidth for your application.\nSelect a good process ordering for your OpenMP threads or MPI ranks. A good\nordering reduces communication costs between processes.\nUse a binding policy for your parallel processes. Binding each process keeps the\nkernel from moving your process and losing the data it has loaded into cache.\nIt is possible to change affinity within your application. This can accommodate\ncode sections that would do better with different process affinities.\n528Batch schedulers:\nBringing order to chaos\nMost high performance computing systems use batch schedulers to schedule the\nrunning of applications. We’ll give you a brief idea why in the first section of this\nchapter. Because schedulers are ubiquitous on high-end systems, you should have\nat least a basic understanding of them to be able to run jobs at high-performance\ncomputing centers and even smaller clusters. We’ll cover the purpose and usage of\nthe batch schedulers. We won’t go into how to set up and manage them (that’s a\nwhole other beast). Set up and management is a topic for system administrators\nand we are just lowly system users.\n What if you don’t have access to a system with a batch scheduler? We don’t rec-\nommend installing a batch scheduler just to try out these examples. Rather, count\nyour blessings and keep the information in this chapter handy for when the needThis chapter covers\nThe role of batch schedulers in high performance \ncomputing\nSubmitting a job to a batch scheduler\nLinking job submissions for long runs or more \ncomplex workflows\n529 The chaos of an unmanaged system\narises. If your demand for computational resources grows and you begin using a\nlarger multi-user cluster, you can come back to this chapter.\n There are many different batch schedulers, and each installation has its own\nunique customizations. We’ll discuss two batch schedulers that are freely available:\nthe Portable Batch System (PBS) and the Simple Linux Utility for Resource Manage-\nment (Slurm). There are variants of each of these, including commercially sup-\nported versions.\n The PBS scheduler originated at NASA in 1991 and was released as open source\nunder the name OpenPBS in 1998. Subsequently, commercial versions, PBS Profes-\nsional by Altair and PBS/TORQUE by Adaptive Computing Enterprises, were forked\noff as separate versions. Freely available versions are still available and in common use\non smaller clusters. Larger high performance computing sites tend to have similar\nversions but with a support contract.\n The Slurm scheduler originated at Lawrence Livermore National Laboratory in\n2002 as a simple resource manager for Linux clusters. It later was spun off into various\nderivative versions such as the SchedMD version.\n Schedulers can also be customized with plugins or add-ins that provide addi-\ntional functionality, support for special workloads, and improved scheduling algo-\nrithms. You’ll also find a number of strictly commercial batch schedulers, but their\nfunctionality is similar to those presented here. The basic concepts of each sched-\nuler implementation are much the same, and often, many details vary from site to\nsite. Portability of batch scripts can still be a bit of a challenge and require some cus-\ntomization for each system.\n15.1 The chaos of an unmanaged system\nYou just got your latest cluster up for your group and the software is running. Soon,\nyou’ll have a dozen of your colleagues logging in and launching jobs. Ka-Boom—you\nhave multiple parallel jobs on compute nodes colliding with each other, slowing these\ndown, and sometimes, causing some jobs to crash. Palpable tension is in the air and\ntempers are short.\n As high performance computing systems grow in size and number of users, it\nbecomes necessary to add some management to the system to bring order to chaos\nand get the most performance from the hardware. Installation of a batch scheduler\ncan save the day (figure 15.1). User jobs can be run, and the exclusive use of the hard-\nware as a resource becomes a reality. However, the use of a batch system is not a pana-\ncea. While this type of software offers much to the users of the cluster or high\nperformance computing system, batch schedulers require significant system adminis-\ntration time and the establishment of different queues and policies. With good poli-\ncies, you can obtain privately allocated compute nodes for your exclusive use for a\nfixed block of time. \n The order provided by the system management software is absolutely essential for\nachieving performance on your parallel applications. The historical work on batch",9196
198-15.3 Submitting your first batch script.pdf,198-15.3 Submitting your first batch script,"530 CHAPTER  15 Batch schedulers: Bringing order to chaos\nschedulers in Beowulf clusters (mentioned in section 15.6.1) gives a good perspective\no n  t h e  i m p o r t a n c e  o f  s c h e d u l e r s .  I n  t h e  l a t e  1 9 9 0 s ,  B e o w u l f  c l u s t e r s  e m e r g e d  a s  a\nwidespread movement to build computing clusters out of commodity computers. The\nBeowulf community soon realized that it was not enough to have a collection of com-\nputing hardware; it was necessary to have some software control and management to\nmake it a productive resource.\n15.2 How not to be a nuisance when working \non a busy cluster\nBusy clusters have lots of users and lots of work. A batch system is often implemented\nto manage the workload and get the most out of the system. These clusters are a dif-\nferent environment than a standalone, single-user workstation. When working on\nthese busy clusters, it is essential to know how to effectively use the system while being\nconsiderate of other users. We’ll give you some of the stated and unstated social rules\nso as to not become a pariah on the busy cluster. But first, let’s consider how these typ-\nical systems are set up.\n15.2.1 Layout of a batch system for busy clusters\nMost clusters have some nodes set aside to be front ends. These front-end nodes are\nalso called login nodes  because that is where you will be when you log in to the system.\nThe rest of the system is then set up as back-end nodes that are controlled and allocatedCompute\nnode 1Compute\nnode 2Compute\nnode 3Compute\nnode 4mpirun -n 4 myjobmpirun -n 4 myjob\nmpirun -n 4 myjob\nmpirun -n 4 myjob\nmpirun -n 4 myjobmpirun -n 4 myjob\nmpirun -n 4 myjobmpirun -n 4 myjob\nmpirun -n 4 myjob\nWithout a batch systemmpirun -n 4 myjob\nCompute\nnode 1Compute\nnode 2Compute\nnode 3Compute\nnode 4mpirun -n 4 myjobmpirun -n 4 myjob\nmpirun -n 4 myjob\nmpirun -n 4 myjob\nmpirun -n 4 myjobmpirun -n 4 myjob\nWith a batch systemBatch\nsystem\nFigure 15.1 Batch systems are like the supermarket checkout queueing \nsystem for a computer cluster. These help to make better use of the resources \nand bring more efficiency to your jobs.\n531 How not to be a nuisance when working on a busy cluster\nby the batch system. These back-end nodes are organized into one or more queues.\nEach queue has a set of policies for things like the size of jobs (such as the number of\nprocessors or memory) and how long these jobs can run. \n15.2.2 How to be courteous on busy clusters and HPC sites: \nCommon HPC pet peeves\nFor interactive work\nCheck the load on your front end with the top command and move to a lightly\nloaded front-end node. There is usually more than one front-end node with\nnumbers such as fe01, fe02, and fe03.\nWatch for heavy file-transfer jobs on the front end. Some sites have special\nqueues for these types of jobs. If you get a node that has heavy file usage, you\nmay find that your compiles or other jobs might take much longer than usual\neven if the load does not appear to be high.\nSome sites want you to compile on the back end and others on the front end.\nCheck the policies on your cluster.\nDon’t tie up nodes with batch interactive sessions and then go off to attend a\nmeeting for several hours.\nRather than get a second batch interactive session, export an X terminal or\nshell from your first session.\nFor light work, look for queues for shared usage that allow over-subscription.\nMany sites have special queues for debugging. Use these when you need to\ndebug, but don’t abuse these debug queues.\nFor big jobs\nBig parallel jobs should be run on the back-end nodes through the batch sys-\ntem queues. \nKeep the number of jobs in the queue small: don’t monopolize the queues.\nTry to run your big jobs during non-work hours so other users can get interac-\ntive nodes for their work. \nFor storage\nStore large files in the appropriate place. Most sites have large parallel file sys-\ntems, scratch, project, or work directories for output from calculations.\nMove files to long-term storage for your site.\nKnow the purging policies for file storage. Large sites will purge files in some of\nthe scratch directories on a periodic basis.\nClean up your files regularly and keep file systems below 90% full. File system\nperformance drops off as file systems become full.\nNOTE Don’t be afraid to send private messages to users who are causing prob-\nlems, but be courteous. They may not realize that their work is bringing many\nworkflows to a standstill. \n532 CHAPTER  15 Batch schedulers: Bringing order to chaos\nFurther cluster wisdom includes the following:\nHeavy usage of the front-end nodes can cause instabilities and crashes. These\ninstabilities affect the whole system as jobs can no longer be scheduled for the\nback-end nodes. \nOften projects get resource allocations that are used for prioritizing jobs using a\n“fair-share” scheduling algorithm. In these cases, you may need to submit an\napplication for the resources that you need for your project.\nEach site can set policies that implement rules, but these cannot cover every sit-\nuation. You should follow the spirit of the rules as well as the actual implemen-\ntation. In other words, don’t game the system. It is not an inanimate object but\nrather your fellow users. They are also trying to get work done.\nRather than gaming the system, you should optimize your code and your file\nstorage. The savings will allow you to get more work done and will let others get\ntheir work done on the cluster as well.\nSubmitting several hundred jobs into a queue when only a few can run at a time\nis inconsiderate. We generally submit a maximum of ten or so at a time and\nthen submit additional jobs when each of those completes. There are many\nways of doing this through shell scripts or even the batch dependency tech-\nniques (discussed later in this chapter).\nFor jobs that require run times much longer than the maximum batch time\nallowed, you should implement checkpointing (section 15.4). Checkpointing\ncatches batch termination signals or uses wall clock timings to get the most\neffective use of the whole batch time. A subsequent job then starts where the\nlast one stopped.\n15.3 Submitting your first batch script\nIn this section, we’ll go through the process of submitting your first batch script. Batch\nsystems require a different way of thinking. Instead of just launching a job whenever\nyou want, you have to think about organizing your work. Planning results in better use\nof resources even before your jobs get submitted. How do you use these batch sys-\ntems? As figure 15.2 shows, there are two basic system modes.\n Most of the commands used in one mode can also be used in the other. Let’s work\nthrough a couple of examples to see how these modes function. We’ll work with the\nSlurm batch scheduler in this first set of examples. We’ll start with an interactive\nexample and modify the example into a batch file form.\n The interactive command-line mode is generally used for program development,\ntesting, or short jobs. For submitting longer production jobs, it is more common to\nuse a batch file to submit a batch job. The batch file allows the user to run applications\novernight or unattended. Batch scripts can even be written to automatically restart\njobs if there is some catastrophic system event. We’ll show the translation in syntax\nfrom the command-line option to a batch script. But first we need to go over the basic\nstructure of a batch script.\n533 Submitting your first batch script\nExample: Interactive command line\nLet’s start on the front end of the cluster, where everybody logs in. Now we want two\ncompute nodes ( -N 2) with a total of 32 processors ( -n 32 ) for an hour ( -t 1:00:00 ).\nNotice the difference in capitalization for number of nodes ( N) and number of proces-\nsors ( n). Also, you can limit your run to specified minutes, although many systems\nhave a minimum and maximum run-time policy. Some systems even have minimums\nand maximums for the number of compute nodes you use. The salloc  command for\nthis specific request would be\nfrontend> salloc -N 2 -n 32 -t 1:00:00\nThe salloc  command allocates and logs into two compute nodes. Note that the fol-\nlowing command prompt changes to indicate that we are a on different system. The\nspecific prompt is highly dependent on your system and environment settings. Once\nwe have two nodes, we can launch our parallel application with:\ncomputenode22> mpirun -n 32 ./my_parallel_app\nThis example shows starting up a parallel job with mpirun. As mentioned in section\n8.1.3, the command to start a parallel job might be different on your system. When\nwe are done, we just exit:\ncomputenode22> exitCompute\nnode 1Compute\nnode 2Compute\nnode 3Compute\nnode 4Batch\nsystem\nsalloc -N 2 -n 32 -t 1:00:00\ncomputenode2> mpirun -n 32 ./my_parallel_app\ncomputenode2> exitfrontend> salloc -N 2 -n 32 -t 1:00:00\nmymonitorCompute\nnode 1Compute\nnode 2Compute\nnode 3Compute\nnode 4Batch\nsystem\nmp run -n 32 ./my_parallel_app ifrontend> sbatch my_batch_job#SBATCH -N 2\n#SBATCH -n 32\n#SBATCH -t 1:00:00\nmpirun -n32 ./my_parallel_app\n#SBATCH -N 2\n#SBATCH -n 32\n#SBATCH -t 1:00:00\nInteractive use examplemy_batch_job\nBatch use example\nFigure 15.2 Batch systems are typically used in either an interactive mode or a batch usage model.\n534 CHAPTER  15 Batch schedulers: Bringing order to chaos\nWe show some of the more common options for Slurm in table 15.1.\nLet’s go ahead and put this all together into our first full Slurm batch script as the fol-\nlowing listing shows. This example is included with the associated code for the book at\nhttps:/ /github.com/EssentialsofParallelComputing/Chapter15 . As always, we encour-\nage you to follow along with the examples for this chapter.\n 1 #!/bin/sh\n 2 #SBATCH -N 1    \n 3 #SBATCH -n 4            \n 5 #SBATCH -t 01:00:00          \n 6 \n 7 # Do not place bash commands before the last SBATCH directiveExample: Batch file syntax\nThe job attributes are specified with the following syntax:\n#SBATCH <option>\nThe file also contains the commands to execute, such as the mpirun command:\nmpirun -n 32 ./my_parallel_app\nThe batch file is then submitted with the sbatch  command:\nsbatch < my_batch_job or sbatch my_batch_job\nYou can specify the options either on the interactive command line or with the\nSBATCH keyword. There is both a long-form option and a short-form syntax. For exam-\nple, time=1:00:00  is the long form and -t=1:00:00  is the short form.\nTable 15.1 Slurm command options\nOption Function Example\n[--time|-t]=hr:min:sec Requests a maximum run time -t=8:00:00\n[--nodes|-N]=# Requests a number of nodes --nodes=2\n[--ntasks|-n]=nprocs Requests a number of processors -n=8\n[--job-name|-J]=name Names your job -J=job22\n[--output|-o]=filename Writes standard output to the specified filename -o=run.out\n[--error|-e]=filename Writes error output to the specified filename -e=run.err\n[--exclusive] Specifies the exclusive use of nodes --exclusive\n[--oversubscribe|-s] Indicates the oversubscribe resources --oversubscribe\nListing 15.1 Slurm batch script for a parallel job\nSpecifies one compute node\nIndicates four processors\nRuns job one hour\n535 Submitting your first batch script\n 8 # Behavior can be unreliable\n 9\n10 mpirun -n 4 ./testapp &> run.out\nThe -N on line 2 can alternatively be specified with --nodes . The -N has a different\nmeaning in other batch schedulers and MPI implementations, leading to incorrect\nvalues and errors. You should be on the lookout for inconsistencies in syntax for the\nset of batch systems and MPIs that you use. We then submit this job with sbatch <\nfirst_slurm_batch_job . We’ll get the equivalent of the batch job in an interactive\njob with\nfrontend> salloc -N 1 -n 4 -t 01:00:00\ncomputenode22> mpirun -n 4 ./testapp\ncomputenode22> exit\nNOTE The options are the same in both the batch file and on the command\nline.\nWe need to make a special mention of the exclusive  and oversubscribe  options.\nOne of the major reasons for using a batch system is to get exclusive use of the\nresource for more efficient application performance. Nearly every major computing\ncenter sets the default behavior to exclusive use of the resource. But the configuration\nmay set one partition to be shared for particular use cases. You can use these com-\nmand options, exclusive  and oversubscribe , for the sbatch  and srun  commands to\nrequest a different behavior than the system configuration. However, you cannot over-\nride the shared configuration setting for a partition.\n Most large computing systems are composed of many nodes with identical charac-\nteristics. It is, however, increasingly common to have systems with a variety of node\ntypes. Slurm provides commands that can request nodes with special characteristics.\nFor example, you can use --mem=<#>  to get large memory nodes with the requested\nsize in MB. There are many other special requests that can be made through the batch\nsystem. A batch script for the PBS batch scheduler is similar, but with a different syn-\ntax. Some of the most common PBS options are shown in table 15.2.\nTable 15.2 PBS command options\nOption Function Example\n-l [nodes|walltime|cput|mem \n     |ncpus|ppn|procs]Catch-all for parallel requirements -l nodes=2,procs=4\n-N <name> Names the job -N job22\n-o <filename> Writes standard output to the \nspecified filename-o run.out\n-e <filename> Writes error output to the specified \nfilename-e run.err\n536 CHAPTER  15 Batch schedulers: Bringing order to chaos\nThe -l option is a catch-all that is used for a variety of options. Let’s put together the\nequivalent PBS batch script for the same job as in listing 15.1. The following listing\nshows the PBS script.\n 1 #!/bin/sh\n 2 #PBS -l nodes=1              \n 3 #PBS -l procs=4              \n 5 #PBS -l walltime=01:00:00    \n 6 \n 7 # Do not place bash commands before the last PBS directive\n 8 # Behavior can be unreliable\n 9\n10 mpirun -n 4 ./testapp &> run.out\nFor PBS, we submit the job with qsub < first_pbs_batch_job . To get an interactive\nallocation in PBS, we use the -I option to qsub :\nfrontend> qsub -I -l nodes=1,procs=4,walltime=01:00:00\ncomputenode22> mpirun -n 4 ./testapp &> run.out\ncomputenode22> exit\nYou may need to specify a queue or other site-specific information for these examples.\nMany sites have different queues for long, short, large, and other specialized situa-\ntions. Consult the local site documentation for these important details.\n We’ve seen a couple of batch scheduler commands in the previous discussion. To\neffectively use the system, you will need more commands, found below. These batch\nscheduler commands check on the status of your job, get information on the system\nresources, and cancel jobs. We summarize the most common commands for both the\nSlurm and PBS schedulers next.-q <queue> Queues for job submission (queue \nnames are site specific)-q standard\n-j Joins standard and error output -j -o run.out\nListing 15.2 PBS batch script for a parallel job\nSlurm reference guide\nsalloc [--nodes|-N]=N [-ntasks|-n]=N  allocates nodes for a batch job:\nfrontend> salloc -N 1 -n 32\nsbatch  submits a batch job to the batch scheduler (see examples earlier in\nthis section).Table 15.2 PBS command options  (continued)\nOption Function Example\nPBS keywords \nand syntax\n537 Submitting your first batch script\nscancel  cancels a job either running or waiting in queue:\nfrontend> scancel <SLURM_JOB_ID>\nsinfo  provides information on the status of the system controlled by the\nbatch scheduler:\nfrontend> sinfo\nPARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST\nstandard*    up    4:00:00      1  drain n02\nstandard*    up    4:00:00      5   resv n[03-07]\nstandard*    up    4:00:00      2  alloc n[08-09]\nstandard*    up    4:00:00      1   idle n01\ndebug        up    1:00:00      1   idle n10\nsqueue  shows the jobs in the queue and their status (such as running or wait-\ning to run). Most common usage is squeue -u <username>  for just your own\nuser jobs or squeue  for all jobs.\nfrontend> squeue\nJOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n35456  standard    sim_2      jrr PD       0:00      1 (Resources)\n35455  standard    sim_1      jrr  R    2:26:54      1 n08\nsrun [--nodes|-N]=N [--ntasks|-n]=N <exec> , a replacement for mpi-\nrun, contains additional capabilities for placement and binding. If the affinity\nplugin is enabled, these additional options are available:\n--sockets-per-node=S\n--cores-per-socket=C\n--threads-per-core=T\n--ntasks-per-core=n\n--ntasks-per-socket=n\n--cpu-bind=[threads|cores|sockets]\n--exclusive\n--share\nFor example\nfrontend> srun -N 1 -n 16 --cpu-bind=cores my_exec\nscontrol  views or modifies Slurm components:\nfrontend> scontrol show job <SLURM_JOB_ID>\nJobID=35456 JobName=sim2\n  UserID=jrr <...and much more...>\nThe following table lists some environment variables in Slurm that can be useful in\nyour batch scripts.\n538 CHAPTER  15 Batch schedulers: Bringing order to chaos\n(continued)\nPBS reference guide\nqsub submits a batch job, where\n–-I is an interactive job \n– The batch equivalent for this command is #PBS interactive=true .\n–-W block=true  waits until job completion \n– The batch equivalent is #PBS block=true .\n–qdel deletes a batch job: frontend> qdel <job ID>\n–qsig sends a signal to the batch job: frontend> qsig 23 56\n–qstat  shows the status of batch jobs\nfrontend> qstat or qstat -u jrr\n                                                Req’d  Elap\nJobID    User   Queue Jobname Sess NDS  TSK Mem Time S Time\n-------- ------ ----- ------- ---- ---- --- --- ---- - ----\n56.base  jrr    standard sim2 --   --     l  -- 0:30 R 0:02\nqmsg sends a message to a batch job:\nfrontend> qmsg “message to standard error” 56\nThe following table lists some environment variables in PBS that can be useful in your\nbatch scripts.Slurm environment variable Function\nSLURM_NTASKS Formerly SLURM_NPROCS, the total number of proces-\nsors requested\nSLURM_CPUS_ON_NODE CPUs on node\nSLURM_JOB_CPUS_PER_NODE CPUS requested for each node\nSLURM_JOB_ID Job ID\nSLURM_JOB_NODELIST List of nodes allocated for job\nSLURM_JOB_NUM_NODES Number of nodes for job\nSLURM_SUBMIT_DIR Directory from which job was submitted\nSLURM_TASKS_PER_NODE Number of tasks to be started on each node",18528
199-15.4 Automatic restarts for long-running jobs.pdf,199-15.4 Automatic restarts for long-running jobs,"539 Automatic restarts for long-running jobs\n15.4 Automatic restarts for long-running jobs\nMost high-performance computing sites limit the maximum time that a job can run.\nSo how do you run longer jobs? The typical approach is for applications to periodi-\ncally write out their state into files and then a follow-on job is submitted that reads the\nfile and starts at that point in the run. This  process, as illustrated in figure 15.3, is\nreferred to as checkpointing  and restarting .\nThe checkpointing process is useful for dealing with a limited time for a batch job and\nfor handling system crashes or other job interruptions. You might restart your jobs\nmanually for a small number of cases, but as the number of restarts gets larger, it\nbecomes a real burden. If this is the case, you should add the capability to automate\nthe process. It takes a fair amount of effort to do this and requires changes to your\napplication and more sophisticated batch scripts. We show a skeleton application\nwhere we have done this. \n First, the batch script needs to signal your application that it is reaching the end of\nits allocated time. Then the script needs to resubmit itself recursively until your job\nreaches completion. The following listing shows such a script for Slurm.  \nAutomaticRestarts/batch_restart.sh\n 1 #!/bin/sh\n    < ... usage notes ... >\n13 #SBATCH -N 1\n14 #SBATCH -n 4\n15 #SBATCH --signal=23@160     Listing 15.3 Batch script to automatically restartPBS environment variable Function\nPBS_JOBDIR Job execution directory\nPBS_TMPDIR Temporary directory or scratch space\nPBS_O_WORKDIR Current working directory where qsub  command was executed\nPBS_JOBID Job ID\nJob\nMax time reachedCheckpoint\nJob + n\nRestart at checkpointFigure 15.3 A checkpoint file \nthat saves the state of the \ncalculation is written out to \ndisk at the conclusion of a \nbatch job and then the next \nbatch job reads the file and \nrestarts the calculation where \nthe previous job left off.\nSends application a signal 23 (SIGURG) \n160 s before termination\n540 CHAPTER  15 Batch schedulers: Bringing order to chaos\n16 #SBATCH -t 00:08:00\n17 \n18 # Do not place bash commands before the last SBATCH directive\n19 # Behavior can be unreliable\n20 \n21 NUM_CPUS=${SLURM_NTASKS}\n22 OUTPUT_FILE=run.out\n23 EXEC_NAME=./testapp\n24 MAX_RESTARTS=4         \n25 \n26 if [ -z ${COUNT} ]; then          \n27    export COUNT=0                 \n28 fi\n29 \n30 ((COUNT++))                       \n31 echo ""Restart COUNT is ${COUNT}""  \n32 \n33 if [ ! -e DONE ]; then            \n34    if [ -e RESTART ]; then                  \n35       echo ""=== Restarting ${EXEC_NAME} ==="" \           \n               >> ${OUTPUT_FILE}\n36       cycle=`cat RESTART`    \n37       rm -f RESTART\n38    else\n39       echo ""=== Starting problem ===""  \\n               >> ${OUTPUT_FILE}\n40       cycle=""""\n41    fi\n42 \n43    mpirun -n ${NUM_CPUS} ${EXEC_NAME} \     \n             ${cycle} &>> ${OUTPUT_FILE}       \n44    STATUS=$?\n45    echo ""Finished mpirun""  \\n               >> ${OUTPUT_FILE}\n46 \n47    if [ ${COUNT} -ge ${MAX_RESTARTS} ]; then     \n48       echo ""=== Reached maximum number of restarts ==="" \\n               >> ${OUTPUT_FILE}\n49       date > DONE\n50    fi\n51 \n52    if [ ${STATUS} = ""0"" -a ! -e DONE ]; then\n53       echo ""=== Submitting restart script ===""  \\n               >> ${OUTPUT_FILE}\n54       sbatch <batch_restart.sh     \n55    fi\n56 fi\nThis script has a lot of moving parts. Much of this is to avoid a runaway situation where\nmore batch jobs are submitted than needed. The script also requires cooperation with\nthe application. This cooperation includes these tasks:\nThe batch system sends a signal and the application catches it.\nThe application writes out to a file named DONE when complete.Maximum number of \nscript submissions\nCounts the \nnumber of \nsubmissions\nChecks for DONE file\nChecks for \nRESTART file\nGets the iteration number \nfor the command line\nInvokes MPI job with \ncommand-line arguments\nExits if reached \nmaximum restarts\nSubmits next \nbatch job\n541 Automatic restarts for long-running jobs\nThe application writes out the iteration number to a file named RESTART.\nThe application writes out a checkpoint file and reads it on restart.\nThe signal number might need to vary depending on what the batch system and MPI\nalready use. We also caution you not to put shell commands before any of the Slurm\ncommands. While the script might seem to work, we found that the signals did not\nfunction properly; therefore, order does matter and you won’t always get an obvious\nfailure. Listing 15.4 shows a skeleton of an application code in C to demonstrate the\nautomatic restart functionality. \nNOTE The example codes at https:/ /github.com/EssentialsofParallelCom-\nputing/Chapter15  also contain a Fortran example of an automatic restart.\nAutomaticRestarts/testapp.c\n 1 #include <unistd.h>\n 2 #include <time.h>\n 3 #include <stdio.h>\n 4 #include <stdlib.h>\n 5 #include <signal.h>\n 6 #include <mpi.h>\n 7 \n 8 static int batch_terminate_signal = 0;    \n 9 void batch_timeout(int signum){           \n10    printf(""Batch Timeout : %d\n"",signum);\n11    batch_terminate_signal = 1;            \n12    return;\n13 }\n14 \n15 int main(int argc, char *argv[])\n16 {\n17    MPI_Init(&argc, &argv);\n18    char checkpoint_name[50];\n19    int mype, itstart = 1;\n20    MPI_Comm_rank(MPI_COMM_WORLD, &mype);\n21 \n22    if (argc >=2) itstart = atoi(argv[1]);\n            // < ... read restart file ... >    \n24 \n25    if (mype ==0) signal(23, batch_timeout);    \n26 \n27    for (int it=itstart; it < 10000; it++){\n28       sleep(1);             \n29 \n30       if ( it%60 == 0 ) {\n            // < ... write out checkpoint file ... >            \n40       }\n41       int terminate_sig = batch_terminate_signal;\n42       MPI_Bcast(&terminate_sig, 1, MPI_INT, 0, MPI_COMM_WORLD);\n43       if ( terminate_sig ) {\n            // < ... write out RESTART and        \n             //   special checkpoint file ... >   Listing 15.4 Sample application for testing\nGlobal variable \nfor batch signal\nCallback function sets \nthe global variable\nIf a restart, reads \nthe checkpoint file\nSets the callback \nfunction for signal 23\nStands in for \ncomputational workWrites out \ncheckpoint every \n60 iterations\nWrites out special checkpoint \nfile and a file named RESTART\n542 CHAPTER  15 Batch schedulers: Bringing order to chaos\n54          MPI_Finalize();\n55          exit(0);\n56       }\n57 \n58    }\n59 \n            // < ... write out DONE file ... >     \n67    MPI_Finalize();\n68    return(0);\n69 }\nThis may appear to be a short and simple code, but there is a lot packed into these\nlines. A real application would need hundreds of lines to fully implement checkpoint-\ning and restart, completion criteria, and input handling. We also caution that develop-\ners need to carefully check their code to prevent runaway conditions. The signal\ntiming also needs to be tuned for how long it takes to catch the signal, complete the\niterations, and write out the restart file. For our little skeleton for an automatic restart\napplication, we start the submission with \nsbatch < batch_restart.sh \nand get the following output:\n=== Starting problem === \nApp launch reported: 2 (out of 2) daemons - 0 (out of 4) procs\n 60 Checkpoint: Mon May 11 20:06:08 2020\n120 Checkpoint: Mon May 11 20:07:08 2020\n180 Checkpoint: Mon May 11 20:08:08 2020\n240 Checkpoint: Mon May 11 20:09:08 2020\nBatch Timeout : 23\n297 RESTART: Mon May 11 20:10:05 2020\nFinished mpirun\n=== Submitting restart script === \n=== Restarting ./testapp === \nApp launch reported: 2 (out of 2) daemons - 0 (out of 4) procs\n300 Checkpoint: Mon May 11 20:10:11 2020\n < ... skipping output ... >\n1186 RESTART: Mon May 11 20:25:05 2020\nFinished mpirun\n=== Reached maximum number of restarts === \nFrom the output, we see that the application writes out periodic checkpoint files every\n60 iterations. Because the stand-in for computation work is actually a sleep  command\nof 1 s, the checkpoints are 1 min apart. After approximately 300 s, the batch system\nsends the signal and the test application reports that it was caught. At that point, the\nscript writes out a file named RESTART that contains the iteration number. The script\nthen writes out a message that the restart script was resubmitted. The output also\nshows the application starting back up. In the output, we skipped showing the addi-\ntional restarts and just showed the message that the maximum number of restarts has\nbeen reached.Writes out DONE file \nwhen application meets \ncompletion criteria",8745
200-15.5 Specifying dependencies in batch scripts.pdf,200-15.5 Specifying dependencies in batch scripts,"543 Specifying dependencies in batch scripts\n15.5 Specifying dependencies in batch scripts\nDo batch systems have built-in support for sequences of batch jobs? Most have a\ndependency feature that allows you to specify how one job depends on another. Using\nthis dependency capability, we can get our subsequent jobs submitted earlier in the\nqueue by submitting the next batch job prior to running our application. As figure 15.4\nshows, this may give us higher priority for starting up the next batch job, depending\non the policies of the site. Regardless, your jobs will be in the queue, and you don’t\nhave to worry about whether the next job will be submitted.\nWe can make this change to the batch script by adding the dependency clause (on\nline 33 in the following listing). This batch script is submitted first, before we begin\nour work, but with a dependency on the completion of this current batch job.\nPrestart/batch_restart.sh\n 1 #!/bin/sh\n    < ... usage notes ... >\n13 #SBATCH -N 1\n14 #SBATCH -n 4\n15 #SBATCH --signal=23@160\n16 #SBATCH -t 00:08:00\n17 \n18 # Do not place bash commands before the last SBATCH directive\n19 # Behavior can be unreliable\n20 \n21 NUM_CPUS=4\n22 OUTPUT_FILE=run.outListing 15.5 Batch script to submit first to restart scriptSubmitting restart batch job at end of batch script Submitting restart batch job at start of batch scriptPriority\nis by\nwhen job\nis submitted.Priority\nis by\nwhen job\nis submitted.my_job1\njob2023\njob1298\njob2456#SBATCH ...\n#SBATCH ...\nmy_job2Queue\njob0198\njob2164my_batch_job\n=== Starting problem ===\nApp launch ...\n60 Checkpoint ...\n< ... skipping output ... >\nBatch Timeout : 23\n297 RESTART: ...\nFinished mpirun\n=== Submitting restart script ===\n===Restarting ./testapp ===\nApp launch ...\n300 Checkpoint ...\n< ... skipping output ... >\n1186 RESTART: ...\nFinished mpirun\n=== ...maximum ... restarts ===#SBATCH ...\n#SBATCH ...Queue my_batch_job\nmy_job1\nmy_job2\njob2023\njob1298\njob2456\njob0198\njob2164== Starting problem === =\n== Submitting restart script === =\nAp launch ... p\n60 Checkpoint ...\n<. . skipping output ... > .\nBa ch Timeout : 23 t\n29 RESTART: ... 7\nFi ished mpirun n\n== Restarting ./testapp === =\nAp launch ... p\n30 Checkpoint ... 0\n< .. skipping output ... > .\n11 6 RESTART: ... 8\nFi ished mpirun n\n== ...maximum ... restarts === =\nFigure 15.4 Automatic restart submitted at start of batch job will have more time in queue, which can give your \nrestart job higher priority than one submitted at end of batch job (dependent on local scheduling policies).\n544 CHAPTER  15 Batch schedulers: Bringing order to chaos\n23 EXEC_NAME=./testapp\n24 MAX_RESTARTS=4\n25 \n26 if [ -z ${COUNT} ]; then\n27    export COUNT=0\n28 fi\n29 \n30 ((COUNT++))\n31 echo ""Restart COUNT is ${COUNT}""\n32 \n33 if [ ! -e DONE ]; then\n34    if [ -e RESTART ]; then\n35       echo ""=== Restarting ${EXEC_NAME} ===""  \\n               >> ${OUTPUT_FILE}\n36       cycle=`cat RESTART`\n37       rm -f RESTART\n38    else\n39       echo ""=== Starting problem ===""  \\n               >> ${OUTPUT_FILE}\n40       cycle=""""\n41    fi\n42 \n43    echo ""=== Submitting restart script ===""  \\n            >> ${OUTPUT_FILE}\n44    sbatch --dependency=afterok:${SLURM_JOB_ID} \\n             <batch_restart.sh              \n45 \n46    mpirun -n ${NUM_CPUS} ${EXEC_NAME} ${cycle}  \\n            &>> ${OUTPUT_FILE}\n47    echo ""Finished mpirun""   \\n             >> ${OUTPUT_FILE}\n48 \n49    if [ ${COUNT} -ge ${MAX_RESTARTS} ]; then\n50       echo ""=== Reached maximum number of restarts ==="" \\n             >> ${OUTPUT_FILE}\n51       date > DONE\n52    fi\n53 fi\nThis listing showed how to use dependencies in your batch scripts for the simple case\nof a checkpoint/restart, but dependencies are useful for many other situations. More\ncomplicated workflows might have pre-processing steps that need to complete before\nthe main work and then a post-processing step afterward. Some more complex work-\nflows need more than a dependency on whether the previous job completed. Fortu-\nnately, batch systems provide other types of dependencies between jobs. Table 15.3\nshows the various possible options. PBS has similar dependencies for batch jobs that\ncan be specified with -W depend=<type:job id> .\n \n \n Submit this batch job \nfirst with a dependency \non its completion",4389
201-15.6 Further explorations.pdf,201-15.6 Further explorations,,0
202-15.6.2 Exercises.pdf,202-15.6.2 Exercises,"545 Further explorations\n15.6 Further explorations\nThere are general reference materials for the Slurm and PBS schedulers, but you\nshould also look at the documentation for your site. Many sites have customized set-\nups and added commands and features for their specific needs. If you think you might\nwant to set up a computing cluster with a batch system, you may want to research new\ninitiatives such as OpenHPC and the Rocks Cluster distributions that have recently\nbeen released for different HPC computing niches.\n15.6.1 Additional reading\nBoth freely available and commercially supported versions of Slurm are available from\nSchedMD. Not surprisingly, the SchedMD site has a lot of documentation on Slurm.\nAnother good reference site is Lawrence Livermore National Laboratory where Slurm\nwas originally developed.\nSchedMD and Slurm documentation at https:/ /slurm.schedmd.com .\nBlaise Barney, “Slurm and Moab,” Lawrence Livermore National Laboratory,\nhttps:/ /computing.llnl.gov/tutorials/moab/ .\nThe best information on PBS is the PBS User Guide:\nAltair Engineering, PBS User Guide, https:/ /www.altair.com/pdfs/pbsworks/PBS\nUserGuide2021.1.pdf .\nThough somewhat dated, the following online reference to setting up a Beowulf clus-\nter is a good historical perspective on the emergence of cluster computing and how to\nset up cluster management, including the PBS batch scheduler:\nEdited by William Gropp, Ewing Lusk, Thomas String, Beowulf Cluster Computing\nwith Linux , 2nd ed. (Massachusetts Institute of Technology, 2002, 2003), http:/ /\netutorials.org/Linux+systems/cluster+computing+with+linux/ .\nHere are some sites with information on current HPC software management systems:\nOpenHPC, http:/ /www.openhpc.community.\nRocks Cluster, http:/ /www.rocksclusters.org.Table 15.3 Dependency options for batch jobs\nDependency option Function\nafter Job can begin after specified job(s) have started.\nafterany Job can begin after specified job(s) have terminated with any status.\nafternotok Job can begin after specified job(s) have failed.\nafterok  Job can begin after specified job(s) have successfully completed.\nsingleton Job can begin after all jobs with same name and user have completed.",2233
203-16.2 Standard file operations A parallel-to-serial interface.pdf,203-16.2 Standard file operations A parallel-to-serial interface,"546 CHAPTER  15 Batch schedulers: Bringing order to chaos\n15.6.2 Exercises\n1Try submitting a couple of jobs, one with 32 processors and one with 16 proces-\nsors. Check to see that these are submitted and whether they are running.\nDelete the 32 processor job. Check to see that it got deleted.\n2Modify the automatic restart script so that the first job is a preprocessing step to\nset up for the computation before the restarts run the simulation.\n3Modify the simple batch script in listing 15.1 for Slurm and 15.2 for PBS to\nclean up on failure by removing a file called simulation_database.\nSummary\nBatch schedulers allocate resources so that you can use a parallel cluster effi-\nciently. It is important to learn how to use these to run on larger, high-perfor-\nmance computing systems.\nThere are many commands to query your job and its status. Knowing these\ncommands allows you to better utilize the system.\nYou can use automatic restarts and chaining of jobs to run larger simulations\nand workflows. Adding this capability to your application makes it possible to\nscale to problems that you would not otherwise be able to do.\nBatch job dependencies give the capability of controlling complex workflows.\nBy using dependencies between multiple jobs, you can stage data, preprocess it\nfor a calculation, or launch a post-processing job. \n547File operations\nfor a parallel world\nFilesystems create a streamlined workflow of retrieving, storing, and updating data.\nFor any computing work, the product is the output, whether it be data, graphics, or\nstatistics. This includes final results but also intermediate output for graphics,\ncheckpointing, and analysis. Checkpointing is a special need on large HPC systems\nwith long-running calculations that might span days, weeks, or months.\nDEFINITION Checkpointing  is the practice of periodically storing the state of\na calculation to disk so that the calculation can be restarted in the event\nof system failures or because of finite length run times in a batch system\nWhen processing data for highly parallel applications, there needs to be a safe and\nperformant way of reading and storing data at run time. Therein lies the need toThis chapter covers\nModifying a parallel application for standard \nfile operations\nWriting out data using parallel file operations \nwith MPI-IO and HDF5\nTuning parallel file operations for different \nparallel filesystems\n548 CHAPTER  16 File operations for a parallel world\nunderstand file operations in a parallel world. Some of the concerns you should keep\nin mind are correctness, reducing duplicate output, and performance.\n It is important to be aware that the scaling of the performance of filesystems has\nnot kept up with the rest of the computing hardware. We are scaling calculations up to\nbillions of cells or particles, which is putting severe demands on the filesystems. With\nthe advent of machine learning and data science, many more applications need big\ndata that requires large files sets and complex workflows with intermediate file storage.\n Adding an understanding of file operations to your HPC toolset is becoming more\nand more important. In this chapter, we introduce how to modify file operations for a\nparallel application so that you are writing out data efficiently and making the best use\nof the available hardware. Though this topic may not be heavily covered in many par-\nallel tutorials, we think it’s a baseline essential for today’s parallel applications. You\nwill learn how to speed up the file-writing operation by orders of magnitude while\nmaintaining correctness. We will also look at the different software and hardware that\nare typically used for large HPC systems. We will use the example of writing out the\ndata from the domain decomposition of a regular grid with halo cells using different\nparallel file software. We encourage you to follow along with the examples for this\nchapter at https:/ /github.com/EssentialsOfParallelComputing/Chapter16.git .\n16.1 The components of a high-performance filesystem\nWe first review what hardware comprises a high-performance filesystem. Traditionally,\nfile operations store data to a hard disk with a mechanical mechanism that writes a\nseries of bits to a magnetic substrate. Like many other parts of HPC systems, the stor-\nage hardware has become more complex with deeper hierarchies of hardware and dif-\nferent performance characteristics. This evolution of storage hardware is similar to\nthe deepening of the cache hierarchy for processors as these increased in perfor-\nmance. The storage hierarchy also helps to cover the large disparity in bandwidth at\nthe processor level, compared to mechanical disk storage. This is because it is much\nharder to reduce the size of mechanical components than electrical circuits. The\nintroduction of solid-state drives (SSDs) and other solid-state devices has helped to\nprovide a way around the scaling of physical spinning disks. \n Let’s first specify what might comprise an HPC storage system as illustrated in fig-\nure 16.1. Typical storage hardware components include the following:\nSpinning disk —Electro-mechanical device where data is stored in an electro-\nmagnetic layer through the movement of a mechanical recording head.\nSSD—A solid-state drive (SSD) is a solid-state memory device that can replace a\nmechanical disk.\nBurst buffer —Intermediate storage hardware layer composed of NVRAM and\nSSD components. It is positioned between the compute hardware and the main\ndisk storage resources.\nTape —A magnetic tape with auto-loading cartridges.\n549 Standard file operations: A parallel-to-serial interface\nThe storage schematic in figure 16.1 illustrates the storage hierarchy between the\ncompute system and the storage system. Burst buffers are inserted in between the\ncompute hardware and the main disk storage to cover the increasing gap in perfor-\nmance. Burst buffers can either be placed on each node or on the IO nodes and\nshared via a network with the other compute nodes. \n With the rapid development of solid-state storage technology, the burst buffer\ndesigns will continue to evolve in the near future. Besides helping with the gap in\nlatency and bandwidth performance, new storage designs are increasingly driven by\nthe need to reduce power requirements as systems grow in size. A magnetic tape has\ntraditionally been used for long-term storage, but some designs have even looked at a\n“dark disk,” where spinning disks are used but turned off when not needed.\n16.2 Standard file operations: A parallel-to-serial interface\nLet’s first take a look at standard file operations. For our parallel applications, the\nconventional file-handling interface is still a serial operation. It is not practical to have\na hard disk for every processor. Even a file per process is only viable in limited situa-\ntions and at small scale. The result is that for every file operation, we go from parallel\nto serial. A file operation needs to be treated as a reduction (or expansion for reads)\nin the number of processes, requiring special handling for parallel applications. You\ncan handle this parallelism with some simple modifications to standard file input and\noutput (IO).\n A large portion of the modifications for parallel applications is at the file-operation\ninterface. We should first review our prior examples that involved file operations. For\nan example of file input, section 8.3.2 shows how to read in data on one process and\nthen broadcast it to other processes. In section 8.3.4, we used an MPI gather opera-\ntion so that output from processes is written out in a deterministic order.\n (Pro tip)  To avoid later complications, the first step you should take in paralleliz-\ning an application is to go through the code and insert an if (rank  == 0) in front ofCPU\nGPU\nCompute\nresourcesBurst\nbuﬀerSharedNode-local\nNode-local CPU\nGPU\nSpinning\ndiskTape\nstorageDiskDisk Tape\nTape\nFigure 16.1 Schematic showing positioning of burst buffer hardware in between the \ncompute resources and disk storage. Burst buffers can either be node-local or shared \namong nodes via a network.\n550 CHAPTER  16 File operations for a parallel world\nevery input and output statement. While going through the code, you should identify\nwhich file operations need additional treatment. These operations include the follow-\ning (illustrated in figure 16.2).\nOpening files on only one process and then broadcasting the data to other\nprocesses\nDistributing data that needs to be partitioned across processes with a scatter\noperation\nEnsuring that output should come from only one process\nCollecting the distributed data with a gather operation before it is output\nA common inefficiency is to open a file on every process; you can imagine it being\nequivalent to a dozen people trying to open a door at the same time. While your pro-\ngram might not crash, it causes problems at scale (imagine 1,000 people opening that\nsame door). There’s a lot of contention for the file metadata and the lock for correct-\nness that it causes, which can take minutes at larger process counts. We can avoid this\ncontention by opening the file on just one process. By adding parallel communication\ncalls at each of the transition points from serial to parallel and parallel to serial, we\ncan make modest parallel applications work using standard files. This is sufficient for\nthe vast majority of parallel applications.\n As our applications grow in size, we can no longer easily gather or scatter the data\nto a single process. Our biggest limitation is memory; we don’t have enough memory\nresources on a single process to bring the data from thousands of other processes\ndown to just one. Thus, we have to have a different, more scalable approach to file\noperations. That is the subject of the next two sections on MPI file operations, called\nMPI-IO, and Hierarchical Data Format v5 (HDF5). In these sections, we show how\nthese two libraries permit a parallel application to treat file operations in a parallel\nmanner. There are other parallel file libraries that we will mention in section 16.5.Storage system\nProcess Process Process ProcessTime\nRead input ﬁle on rank 0 and broadcastScatter data from rank 0 to other processesGather data and write out frrom rank 0\nGather from processes and write from rank 0\nFigure 16.2 Modifications for a \nparallel application to work with \na standard filesystem. All file \noperations are done from rank 0.",10572
204-16.3 MPI file operations MPI-IO for a more parallel world.pdf,204-16.3 MPI file operations MPI-IO for a more parallel world,"551 MPI file operations (MPI-IO) for a more parallel world\n16.3 MPI file operations (MPI-IO) for a more parallel world\nThe best way to learn MPI-IO is to see how it is used in a realistic scenario. We’ll take a\nlook at the example of writing out a regular mesh that has been distributed across pro-\ncessors with halo cells using MPI-IO. Through this example, you will become familiar\nwith the basic structure that occurs with MPI-IO and some of its more common func-\ntion calls.\n The first parallel file operations were added to MPI in the MPI-2 standard in the\nlate 1990s. The first widely available implementation of the MPI file operations,\nROMIO, was led by Rajeev Thakur at Argonne National Laboratory (ANL). ROMIO\ncan be used with any MPI implementation. Most MPI distributions include ROMIO as\na standard part of their software release. MPI-IO has a lot of functions, all beginning\nwith the prefix MPI_File . In this section, we will cover just a subset of the most com-\nmonly used operations (see table 16.1). \n There are different ways to use MPI-IO. We are interested in the highly parallel ver-\nsion, the collective form that has the processes work together to write to their section\nof the file. In order to do this, we’ll utilize the ability to create a new MPI data type\nthat was first introduced in section 8.5.1. \n The MPI-IO library has both a shared file pointer across all processes and indepen-\ndent file pointers for each process. Using the shared pointer causes a lock to be\napplied for each process and serializes the file operations. To avoid the locks, we use\nthe independent file pointers for better performance. \n File operations are broken down into collective and non-collective operations. Col-\nlective operations use the MPI collective communication calls, and all members of the\ncommunicator must make the call or it will hang. Non-collective calls  are serial opera-\ntions that are invoked separately for every process. Table 16.1 shows some general-pur-\npose operations and the respective commands for each.\nThe file open and close operations are self-explanatory. The seek operation moves the\nindividual file pointer to the specified location for each process. You can use MPI_\nFile_set_info  to communicate both general- and vendor-specific hints. There is also\nan MPI_File_delete , but it is a non-collective call. In this case, we mean a non-collective\ncall to be a serial call: every process deletes the file. For C and C++ programs, theTable 16.1 MPI general file routines\nCommand Description\nMPI_File_open Collective file open\nMPI_File_seek Moves individual file pointers to this location in file\nMPI_File_set_size Allocates the file space specified\nMPI_File_close Collective file close\nMPI_File_set_info Communicates hints to the MPI-IO library for more optimized MPI operations\n552 CHAPTER  16 File operations for a parallel world\nremove  function works just as well. Calling MPI_File_set_size  with the expected size\nof your file can be more efficient than the file being incrementally increased in size\nwith each write. \n We’ll start by looking at the independent file operations for the read and write\noperations. When each process operates on its independent file pointer, it’s known as\nan independent file operation . Independent file operations are useful for writing out rep-\nlicated data across processes. For this common data, you can write it out from a single\nrank with the routines in table 16.2.\nYou should write out distributed data with collective operations (table 16.3). When\nprocesses operate collectively on the file, it’s known as a collective file operation . The\nwrite and read functions are similar to the independent file operations but with an\n_all  appended to the function name. To make the best use of the collective opera-\ntions, we need to create complex MPI data types. The MPI_File_set_view  function is\nused to set the data layout in the file.\nFor this example, we’ll break up the code into four blocks. (The full code for this\nexample is included with the code for the chapter.) To begin, we must start with the\ncreation of an MPI data type for the memory layout of the data and another for the\nfile layout; these are referred to as memspace and filespace, respectively. Figure 16.3\nshows these data types for a smaller 4×4 version of our example. For simplicity, we only\nshow four processes, each with a 4×4 grid surrounded by a one cell halo. The halo\ndepth size in the figure is ng, short for number of ghost cells.Table 16.2 MPI independent file routines\nCommand Description\nMPI_File_read Each process reads from its current file pointer position.\nMPI_File_write Each process writes to its current file pointer position.\nMPI_File_read_at Moves the file pointer to the specified location and reads the data\nMPI_File_write_at Moves the file pointer to the specified location and writes the data\nTable 16.3 MPI collective file routines\nCommand Description\nMPI_File_set_view View of file visible to each process. Sets file pointers to zero.\nMPI_File_read_all All processes collectively read from their current, independent file pointer.\nMPI_File_write_all All processes collectively write from their current, independent file pointer.\nMPI_File_read_at_all All processes move to specified file location and read the data.\nMPI_File_write_at_all All processes move to specified file location and write the data.\n553 MPI file operations (MPI-IO) for a more parallel world\nThe first block of code in listing 16.1 shows the creation of these two data types. This\nonly needs to be done once at the start of the program. The data types should then be\nfreed at the end of the program in the finalize routine.\nMPI_IO_Examples/mpi_io_block2d/mpi_io_file_ops.c\n10 void mpi_io_file_init(int ng, int ndims, int *global_sizes,\n11     int *global_subsizes, int *global_starts, MPI_Datatype *memspace,\n       MPI_Datatype *filespace){\n12   // create data descriptors on disk and in memory\n13 \n14   // Global view of entire 2D domain -- collates decomposed subarrays\n15   MPI_Type_create_subarray(ndims,             \n       global_sizes, global_subsizes,            \n16     global_starts, MPI_ORDER_C, MPI_DOUBLE,   \n       filespace);                               \n17   MPI_Type_commit(filespace);     \n18 \n19   // Local 2D subarray structure -- strips ghost cells on node\n20   int ny = global_subsizes[0], nx = global_subsizes[1];\n21   int local_sizes[]    = {ny+2*ng,   nx+2*ng};\n22   int local_subsizes[] = {ny,        nx};\n23   int local_starts[]   = {ng,        ng};Listing 16.1 Setting up MPI-IO dataspace types102103104 101106107108 105110 111 112 109114 115 116 113\n202203204 201206207208 205210 211 212 209214215216 213 314315316 313\n310 311 312 309\n306307308 305\n302303304 301CPU memory layouts\nHalo cells Real data14 15 16 13\n10 11 12 9\n678 5\n234 1\n14 15 16 13\n10 11 12 9\n678 5\n234 114 15 16 13\n10 11 12 9\n678 5\n234 1 102 103 104 101106 107 108 105110 111 112 109114 115 116 113\n202 203 204 201206 207 208 205210 211 212 209214 215 216 213 314 315 316 313\n310 311 312 309\n306 307 308 305\n302 303 304 301nx,ny\nnx_oﬀset,\nny_oﬀsetnx_oﬀset,\nny_oﬀsetnx_oﬀset,\nny_oﬀsetnx_oﬀset,\nny_oﬀsetnx,ny nx,ny nx,nyng, ng ng, ng ng, ng ng, ngnx, ny nx, ny nx, ny nx, nynx+2*ng,\nny+2*ngnx+2*ng,\nny+2*ngnx+2*ng,\nny+2*ng\nMemspace\nFilespace\nFile layout12341011021031042012022032043013023033045678105106107108205206207208305306307308910111210911011111220921021121230931031131213141516113114115116213214215216313314315316\nFigure 16.3 The 4x4 blocks of data from each process written without the halo cells to contiguous sections \nof the output file. The top row is the memory layout on the process, referred to as the memspace . The middle \nrow is the memory in the file with the halo cells stripped off, referred to as the filespace . The memory in the file \nis actually linear, so it takes the form in the last row.\nCreates the data \ntype for the file \ndata layout\nCommits the file data type\n554 CHAPTER  16 File operations for a parallel world\n24 \n25   MPI_Type_create_subarray(ndim, local_sizes,   \n        local_subsizes, local_starts,              \n26      MPI_ORDER_C, MPI_DOUBLE, memspace);        \n27   MPI_Type_commit(memspace);     \n28 }\n29 \n30 void mpi_io_file_finalize(MPI_Datatype *memspace, \n        MPI_Datatype *filespace){\n31   MPI_Type_free(memspace);      \n32   MPI_Type_free(filespace);     \n33 }\nIn this first step, we created the two data types from figure 16.1. Now we need to write\nthese data types out to the file. There are four steps to the writing process as shown in\nlisting 16.2:\n1Create the file\n2Set the file view\n3Write out each array with collective call\n4Close the file\nMPI_IO_Examples/mpi_io_block2d/mpi_io_file_ops.c\n35 void write_mpi_io_file(const char *filename, double **data,\n36     int data_size, MPI_Datatype memspace, MPI_Datatype filespace,\n       MPI_Comm mpi_io_comm){\n37   MPI_File file_handle = create_mpi_io_file(        \n38     filename, mpi_io_comm, (long long)data_size);   \n39 \n40   MPI_File_set_view(file_handle, file_offset,   \n41     MPI_DOUBLE, filespace, ""native"",            \n            MPI_INFO_NULL);                        \n42   MPI_File_write_all(file_handle,   \n       &(data[0][0]), 1, memspace,     \n       MPI_STATUS_IGNORE);             \n43   file_offset += data_size;\n44 \n45   MPI_File_close(&file_handle);     \n46   file_offset = 0;\n47 }\n48 \n49 MPI_File create_mpi_io_file(const char *filename, MPI_Comm mpi_io_comm,\n50         long long file_size){\n51   int file_mode = MPI_MODE_WRONLY | MPI_MODE_CREATE |\n                     MPI_MODE_UNIQUE_OPEN;\n52 \n53   MPI_Info mpi_info = MPI_INFO_NULL;  // For MPI IO hints\n54   MPI_Info_create(&mpi_info);\n55   MPI_Info_set(mpi_info,            \n       ""collective_buffering"", ""1"");   Listing 16.2 Writing an MPI-IO fileCreates the data \ntype for the memory \ndata layout\nCommits the \nmemory data type\nFrees the \ndata types\nCreates \nthe file\nSets file view\nWrites out \narrays\nCloses the file\nCommunicates hints \nfor collective operation\n555 MPI file operations (MPI-IO) for a more parallel world\n56   MPI_Info_set(mpi_info,            \n       ""striping_factor"", ""8"");        \n57   MPI_Info_set(mpi_info,            \n        ""striping_unit"", ""4194304"");   \n58 \n59   MPI_File file_handle = NULL;\n60   MPI_File_open(mpi_io_comm, filename, file_mode, mpi_info,\n                   &file_handle);\n61   if (file_size > 0)                            \n       MPI_File_set_size(file_handle, file_size);  \n62   file_offset = 0;\n63   return file_handle;\n64 }\nThere are a few optimizations that can be provided during the open with hints in an\nMPI_Info  object (line 53). A hint could be that the file operations should be done\nusing collective operations, collective_buffering , as on line 55. Or a hint can be\none that’s filesystem specific to stripe across eight hard disks, striping_factor  = 8, as\non line 56. We will discuss hints more in section 16.6.1. \n We can also preallocate the file space, as shown on line 61, so that it doesn’t have\nto be increased during the writes. Reading the file has the same four steps as the writ-\ning process listed previously and is shown in the following listing. \nMPI_IO_Examples/mpi_io_block2d/mpi_io_file_ops.c\n66 void read_mpi_io_file(const char *filename, double **data, int data_size,\n67     MPI_Datatype memspace, MPI_Datatype filespace, MPI_Comm mpi_io_comm){\n68   MPI_File file_handle = open_mpi_io_file(    \n        filename, mpi_io_comm);                  \n69 \n70   MPI_File_set_view(file_handle, file_offset,  \n71      MPI_DOUBLE, filespace, ""native"",          \n        MPI_INFO_NULL);                           \n72   MPI_File_read_all(file_handle,    \n        &(data[0][0]), 1, memspace,    \n        MPI_STATUS_IGNORE);            \n73   file_offset += data_size;\n74 \n75   MPI_File_close(&file_handle);    \n76   file_offset = 0;\n77 }\n78 \n79 MPI_File open_mpi_io_file(const char *filename, MPI_Comm mpi_io_comm){\n80   int file_mode = MPI_MODE_RDONLY | MPI_MODE_UNIQUE_OPEN;\n81 \n82   MPI_Info mpi_info = MPI_INFO_NULL; // For MPI IO hints\n83   MPI_Info_create(&mpi_info);\n84   MPI_Info_set(mpi_info, ""collective_buffering"", ""1"");\n85 \n86   MPI_File file_handle = NULL;\n87   MPI_File_open(mpi_io_comm, filename, file_mode, mpi_info,\n                   &file_handle);Listing 16.3 Reading an MPI-IO fileCommunicates \nhints for striping on \nLustre filesystem\nPreallocates file space \nfor better performance\nOpens the file\nSets file view\nCollective read \nof arrays\nCloses the file\n556 CHAPTER  16 File operations for a parallel world\n88   return file_handle;\n89 }\nThe read operation requires fewer hints and settings than the write operation. This is\nbecause some of the settings for a read are determined from the file. So far, these\nMPI-IO file operations have been written in a general form that can be called for any\nproblem. Now let’s take a look at the main application code in the following listing\nthat sets up the calls.\nMPI_IO_Examples/mpi_io_block2d/mpi_io_block2d.c\n  9 int main(int argc, char *argv[])\n 10 {\n 11   MPI_Init(&argc, &argv);\n 12 \n 13   int rank, nprocs;\n 14   MPI_Comm_rank(MPI_COMM_WORLD, &rank);\n 15   MPI_Comm_size(MPI_COMM_WORLD, &nprocs);\n 16 \n 17   // for multiple files, subdivide communicator and \n      //    set colors for each set\n 18   MPI_Comm mpi_io_comm = MPI_COMM_NULL;\n 19   int nfiles = 1;\n 20   float ranks_per_file = (float)nprocs/(float)nfiles;\n 21   int color = (int)((float)rank/ranks_per_file);\n 22   MPI_Comm_split(MPI_COMM_WORLD, color, rank, &mpi_io_comm);\n 23   int nprocs_color, rank_color;\n 24   MPI_Comm_size(mpi_io_comm, &nprocs_color);\n 25   MPI_Comm_rank(mpi_io_comm, &rank_color);\n 26   int row_color = 1, col_color = rank_color;\n 27   MPI_Comm mpi_row_comm, mpi_col_comm;\n 28   MPI_Comm_split(mpi_io_comm, row_color, rank_color, &mpi_row_comm);\n 29   MPI_Comm_split(mpi_io_comm, col_color, rank_color, &mpi_col_comm);\n 30 \n 31   // set the dimensions of our data array and the number of ghost cells\n 32   int ndim = 2, ng = 2, ny = 10, nx = 10;\n 33   int global_subsizes[] = {ny, nx};\n 34 \n 35   int ny_offset = 0, nx_offset = 0;\n 36   MPI_Exscan(&nx, &nx_offset, 1, MPI_INT, MPI_SUM, mpi_row_comm);\n 37   MPI_Exscan(&ny, &ny_offset, 1, MPI_INT, MPI_SUM, mpi_col_comm);\n 38   int global_offsets[] = {ny_offset, nx_offset};\n 39 \n 40   int ny_global, nx_global;\n 41   MPI_Allreduce(&nx, &nx_global, 1, MPI_INT, MPI_SUM, mpi_row_comm);\n 42   MPI_Allreduce(&ny, &ny_global, 1, MPI_INT, MPI_SUM, mpi_col_comm);\n 43   int global_sizes[] = {ny_global, nx_global};\n 44   int data_size = ny_global*nx_global;\n 45 \n 46   double **data = (double **)malloc2D(ny+2*ng, nx+2*ng);\n 47   double **data_restore = (double **)malloc2D(ny+2*ng, nx+2*ng);\n     < ... skipping data initialization ... >\n 54 Listing 16.4 Main application code\n557 MPI file operations (MPI-IO) for a more parallel world\n 55   MPI_Datatype memspace = MPI_DATATYPE_NULL, \n                   filespace = MPI_DATATYPE_NULL;\n 56   mpi_io_file_init(ng, global_sizes,      \n          global_subsizes, global_offsets,    \n 57       &memspace, &filespace);             \n 58 \n 59   char filename[30];\n 60   if (ncolors > 1) {\n 61     sprintf(filename,""example_%02d.data"",color);\n 62   } else {\n 63     sprintf(filename,""example.data"");\n 64   }\n 65 \n 66   // Do the computation and write out a sequence of files\n 67   write_mpi_io_file(filename, data,     \n         data_size, memspace, filespace,    \n         mpi_io_comm);                      \n 68   // Read back the data for verifying the file operations\n 69   read_mpi_io_file(filename, data_restore,   \n 70      data_size, memspace, filespace,         \n         mpi_io_comm);                           \n 71   \n 72   mpi_io_file_finalize(&memspace, &filespace);    \n 73   \n   < ... skipping verification code ... >\n105   \n106   free(data);\n107   free(data_restore);\n108   \n109   MPI_Comm_free(&mpi_io_comm);\n110   MPI_Comm_free(&mpi_row_comm);\n111   MPI_Comm_free(&mpi_col_comm);\n112   MPI_Finalize();\n113   return 0;\n114 }\nThis setup takes a little explanation. This code supports the ability to write out more\nthan one MPI data file. This is commonly called NxM file writes where N processes\nwrite out M files and where M is greater than one but much smaller than the number\nof processes (figure 16.4). The reason for this technique is that at larger problem\nsizes, writing to a single file does not always scale well.\n We can break up the processes into groups by colors as shown in figure 16.4. In\nlines 1722 in listing 16.4, we set up a new communicator based on M colors, where M\nis the number of files. The number of files is set on line 19 and our color is computed\non lines 20 and 21. We use a floating-point type for ranks_per_file  to handle an\nuneven division of the ranks. We then get our new rank within our color. Each com-\nmunication group on the right side of figure 16.4 has 4,096 processes or ranks. The\norder of the ranks is the same as in the global communication group. If there is more\nthan one file, the filenames include a color number on lines 59–64. This code currently\nonly sets one color and only writes one file as shown on the left side of figure 16.4, but it\nis written to support more files. Initializes and sets \nup the data types\nWrites out \nthe data\nReads \nthe data\nCloses the file and \nfrees the data types\n558 CHAPTER  16 File operations for a parallel world\nWe also need to know where the starting x and y values are for each process. For data\ndecompositions that have the same number of rows and columns for each process, the\ncalculation only needs to know the location of the process in the global set. But when\nthe number of rows and columns varies across processes, we need to sum all the sizes\nbelow our position. As we have previously discussed in section 5.6, this operation is a\ncommon parallel pattern called a scan. To do this calculation, in lines 22–34 we create\ncommunicators for each row and column. These perform an exclusive scan operation\nto get the starting location of x and y for each process. In this code, we only partition\nthe data in the x-coordinate direction to keep it a little simpler. The global and pro-\ncess sizes in the array subsizes  are set in lines 27–44. This includes the data offsets\ncalculated using the exclusive scans.\n Now that we have all the necessary information about the data decomposition, we\ncan call our mpi_io_file_init  subroutine on line 52 to set up the MPI data types for\nthe memory and filesystem layout. This only has to be done once, at startup. We are\nthen free to call our subroutines for writes, write_mpi_io_file , and reads, read_mpi_\nio_file , on lines 63 and 65. We can call these as many times as needed during the run.\nIn our example code, we then verify the data read in, compare it to the original data,\nand print an error if it occurs. Finally, we open the file on a single process and use a stan-\ndard C binary read to show how the data is laid out in the file. This is done by reading\neach value from the file in sequential order and printing it out. \n Now to compile and run the example. The build is a standard CMake build, and\nwe’ll run it on four processors.\nmkdir build && cd build\ncmake ..\nmake\nmpirun -n 4 ./mpi_io_block2dFile 0\nFile 3File 2File 1Comm groups\nRed\nBlue\nGreen\nYellowProcess rank\n0\n16383File 0Comm groups\nRedProcess rank\n0\n63122874095\n8191\nSmall number of processes\nwrite to a single ﬁle.Large number of processes can be\nsplit into colored communication groups\nto write out data across multiple ﬁles.FilesFiles\nProc 0 Data\nFigure 16.4 At large sizes, the processes can be broken up into communication groups by colors so \nthey write out to separate files. The ranks of the subgroups are in the same order as the ranks in the \noriginal communicator.",20148
205-16.4 HDF5 is self-describing for better data management.pdf,205-16.4 HDF5 is self-describing for better data management,"559 HDF5 is self-describing for better data management\nFigure 16.5 shows the output from a standard C binary read for the 10×10 grid on each\nprocessor.\n16.4 HDF5 is self-describing for better data management\nWith traditional data file formats, the data is meaningless without the code that is used\nto write and read the file. The Hierarchical Data Format (HDF), version 5, takes a dif-\nferent approach. HDF5 provides a self-describing parallel data format. HDF5 is called\nself-describing because the name and characteristics are stored in the file with the\ndata. In HDF5, with the description of the data contained in the file, you no longer\nneed the source code and can read the data by just querying the file. HDF5 also has a\nrich set of command-line utilities (such as h5ls and h5dump) that you can use to\nquery the contents of a file. You will find that the utilities are useful when checking\nthat your files are properly written. \n W e  w a n t  t o  w r i t e  d a t a  i n  b i n a r y  f o r m a t  b e c a u s e  o f  s p e e d  a n d  p r e c i s i o n .  B u t\nbecause it is in binary format, it is difficult to check if the data is correctly written. If\nwe read the data back in, the problem could be in the reading process as well. A utility\nthat can query the file provides a way to check the write operation separately from the\nread. In figure 16.4 in the previous section on MPI-IO, we needed a small program to\nread the contents of the file. For HDF5, it is unnecessary because the utility is already\nprovided. In figure 16.6 (shown later in this section), we used the h5dump command-\nline utility to look at the contents. You can avoid the need to write code for many com-\nmon operations by using the already existing HDF5 utilities.\nFigure 16.5 Output from a small binary read code for the MPI-IO shows what the file contains. With \nMPI-IO, we had to write a small utility to check the file contents.\n560 CHAPTER  16 File operations for a parallel world\n The parallel HDF5 code is implemented by using MPI-IO. Because it is built on\nMPI-IO, the structure of HDF5 is similar. Although similar, the terminology and indi-\nvidual function calls are different enough to cause some difficulty. We’ll cover the\nfunctions that are needed to write a similar parallel file-handling routine as we did for\nMPI-IO. The HDF5 library is divided into lower-level functionality groupings. These\nfunctional groups are conveniently distinguished by prefixes for all of the calls in the\ngroup. The first group is the obligatory file handling operations (table 16.4) that col-\nlectively handle file open and close operations.\nNext, we need to define new memory types. These are used to specify the portions of\ndata to write and their layout. In HDF5, these memory types are called dataspaces.\nThe dataspace operations in table 16.5 include ways to extract patterns from a multidi-\nmensional array. You can find information on the many additional routines in the fur-\nther reading section at the end of the chapter (16.7.1).\nThere are other dataspace operations, including point-based operations, that we haven’t\ncovered here. Now we need to apply these dataspaces to a set of multidimensional arrays\n(table 16.6). In HDF5, a multidimensional array is called a dataset , which is generally a\nmultidimensional array or some other form of data within the application.Table 16.4 HDF5 collective file routines\nCommand Description\nH5Fcreate Collective file open that will create the file if it doesn’t exist\nH5Fopen Collective file open of a file that already exists\nH5Fclose Collective file close\nTable 16.5 HDF5 dataspace routines\nCommand Description\nH5Screate_simple Creates a multidimensional array type\nH5Sselect_hyperslab Creates a hyperslab region type of parts of a multidimensional array\nH5Sclose Releases a dataspace\nTable 16.6 HDF5 dataset routines\nCommand Description\nH5Dcreate2 Creates the space for a dataset in the file\nH5Dopen2 Opens an existing dataset as described within the file\nH5Dclose Closes the dataset within the file\n561 HDF5 is self-describing for better data management\nThere is only one operation group left that we need. This group, called property lists ,\ngives you a way to modify or supply hints to operations as table 16.7 shows. We can use\nproperty lists for setting attributes to use collective operations with reads or writes.\nProperty lists can also be used to pass hints to the underlying MPI-IO library.\nLet’s move on to an example. We start this HDF5 example with the code to create the\nfile and the memory dataspaces. The following listing shows this process. All the argu-\nments to HDF5 are bolded in the listing. \nHDF5Examples/hdf5block2d/hdf5_file_ops.c\n11 void hdf5_file_init(int ng, int ndims, int ny_global, int nx_global, \n12     int ny, int nx, int ny_offset, int nx_offset, MPI_Comm mpi_hdf5_comm,\n13     hid_t *memspace, hid_t *filespace){\n14   // create data descriptors on disk and in memory\n15   *filespace = create_hdf5_filespace(ndims,     \n        ny_global, nx_global, ny, nx,              \n16      ny_offset, nx_offset, mpi_hdf5_comm);      \n17   *memspace  = \n          create_hdf5_memspace(ndims ny, nx, ng);   \n18 }\n19 \n20 hid_t create_hdf5_filespace(int ndims, int ny_global, int nx_global,\n21     int ny, int nx, int ny_offset, int nx_offset,\n       MPI_Comm mpi_hdf5_comm){\n22   // create the dataspace for data stored on disk\n     //    using the hyperslab callH5Dwrite Writes a dataset to the file using filespace  and memspace  \nH5Dread Reads a dataset from the file using filespace  and memspace\nTable 16.7 HDF5 property list routines\nCommand Description\nH5Pcreate Creates a property list\nH5Pclose Frees a property list\nH5Pset_dxpl_mpio Sets the data transfer property list\nH5Pset_coll_metadata_write Sets the collective metadata writes for all processes in a group\nH5Pset_fapl_mpio Stores the MPI-IO properties to the file-access property list\nH5Pset_all_coll_metadata_ops Sets the parallel metadata read operations in the file access \nproperty list\nListing 16.5 Setting up HDF5 dataspace typesTable 16.6 HDF5 dataset routines  (continued)\nCommand Description\nCreates the file \ndataspace\nCreates the memory \ndataspace\n562 CHAPTER  16 File operations for a parallel world\n23   hsize_t dims[] = {ny_global, nx_global};\n24 \n25   hid_t filespace = H5Screate_simple(ndims,    \n                       dims, NULL);                \n26 \n27   // determine the offset into the filespace for the current process\n28   hsize_t  start[] = {ny_offset, nx_offset};\n29   hsize_t stride[] = {1,         1};\n30   hsize_t  count[] = {ny,        nx};\n31 \n32   H5Sselect_hyperslab(filespace, H5S_SELECT_SET,    \n33                 start, stride, count, NULL);        \n34   return filespace;\n35 }\n36 \n37 hid_t create_hdf5_memspace(int ndims, int ny, int nx, int ng) {\n38   // create a memory space in memory using the hyperslab call\n39   hsize_t dims[] = {ny+2*ng, nx+2*ng};\n40 \n41   hid_t memspace = H5Screate_simple(ndims, dims, NULL);   \n42 \n43   // select the real data out of the array\n44   hsize_t  start[] = {ng,   ng};\n45   hsize_t stride[] = {1,    1};\n46   hsize_t  count[] = {ny,   nx};\n47 \n48   H5Sselect_hyperslab(memspace, H5S_SELECT_SET,      \n49                       start, stride, count, NULL);\n50   return memspace;\n51 }\n52 \n53 void hdf5_file_finalize(hid_t *memspace, hid_t *filespace){\n54   H5Sclose(*memspace);\n55   *memspace = H5S_NULL;\n56   H5Sclose(*filespace);\n57   *filespace = H5S_NULL;\n58 }\nIn listing 16.5, we used the same pattern when creating the two dataspaces: create the\ndata object, set the data size arguments, and then select a rectangular region of the\narray. First, we created the global array space with the H5Screate_simple  call. For\nthe file dataspace, we set the dimensions to the global array size of nx_global  and\nny_global  on line 23 and then used those sizes on line 25 to create the dataspace.\nWe then selected a region of the file dataspace for each processor with the H5Sselect\n_hyperslab  calls on lines 32 and 48. A similar process is then done for the memory\ndataspace.\n Now that we have the dataspaces, the process of writing out the data into the file is\nstraightforward. We open the file, create the dataset, and write it. If there are more\ndatasets, we continue to write these out, and when finished, we close the file. The fol-\nlowing listing shows how this is done.\n Creates the \nfilespace object\nSelects the filespace \nhyperslab\nCreates the \nmemspace \nobject\nCreates the \nmemspace hyperslab\n563 HDF5 is self-describing for better data management\nHDF5Examples/hdf5block2d/hdf5_file_ops.c\n 60 void write_hdf5_file(const char *filename, double **data1,\n 61     hid_t memspace, hid_t filespace, MPI_Comm mpi_hdf5_comm) {\n 62   hid_t file_identifier = create_hdf5_file(      \n        filename, mpi_hdf5_comm);                    \n 63 \n 64   // Create property list for collective dataset write.\n 65   hid_t xfer_plist = H5Pcreate(H5P_DATASET_XFER);\n 66   H5Pset_dxpl_mpio(xfer_plist, H5FD_MPIO_COLLECTIVE);\n 67 \n 68   hid_t dataset1 = create_hdf5_dataset(     \n         file_identifier, filespace);           \n 69   //hid_t dataset2 = create_hdf5_dataset(file_identifier, filespace);\n 70 \n 71   // write the data to disk using both the memory space\n      //   and the data space.\n 72   H5Dwrite(dataset1, H5T_IEEE_F64LE,     \n         memspace, filespace, xfer_plist,    \n 73      &(data1[0][0]));                    \n 74   //H5Dwrite(dataset2, H5T_IEEE_F64LE,\n      //       memspace, filespace, xfer_plist,\n 75   //       &(data2[0][0]));\n 76 \n 77   H5Dclose(dataset1);\n 78   //H5Dclose(dataset2);\n 79 \n 80   H5Pclose(xfer_plist);\n 81 \n 82   H5Fclose(file_identifier);    \n 83 }\n 84 \n 85 hid_t create_hdf5_file(const char *filename, MPI_Comm mpi_hdf5_comm){\n 86   hid_t file_creation_plist = H5P_DEFAULT;                  \n 87   // set the file access template for parallel IO access\n 88   hid_t file_access_plist   = H5P_DEFAULT;         \n 89   file_access_plist = H5Pcreate(H5P_FILE_ACCESS);\n 90 \n 91   // set collective mode for metadata writes\n 92   H5Pset_coll_metadata_write(file_access_plist, true);\n 93 \n 94   MPI_Info mpi_info = MPI_INFO_NULL;   \n 95   MPI_Info_create(&mpi_info);\n 96   MPI_Info_set(mpi_info, ""striping_factor"", ""8"");\n 97   MPI_Info_set(mpi_info, ""striping_unit"", ""4194304"");\n 98 \n 99   // tell the HDF5 library that we want to use MPI-IO to do the writing\n100   H5Pset_fapl_mpio(file_access_plist, mpi_hdf5_comm, mpi_info);\n101 \n102   // Open the file collectively\n103   // H5F_ACC_TRUNC - overwrite existing file. \n      //    H5F_ACC_EXCL - no overwrite\n104   // 3rd argument is file creation property list. Using default here\n105   // 4th argument is the file access property list identifierListing 16.6 Writing to an HDF5 file\nCalls the subroutine \nto create the file\nCalls the subroutine \nto create the dataset\nWrites the \ndataset\nCloses the objects \nand the data file\nCreates\nfile\ncreation\nproperty\nlistCreates file \naccess property\nCreates MPI \nIO hints\n564 CHAPTER  16 File operations for a parallel world\n106   hid_t file_identifier = H5Fcreate(filename,    \n107      H5F_ACC_TRUNC, file_creation_plist,         \n         file_access_plist);                         \n108 \n109   // release the file access template\n110   H5Pclose(file_access_plist);\n111   MPI_Info_free(&mpi_info);\n112 \n113   return file_identifier;\n114 }\n115 \n116 hid_t create_hdf5_dataset(hid_t file_identifier, hid_t filespace){\n117   // create the dataset\n118   hid_t link_creation_plist    = H5P_DEFAULT;     \n119   hid_t dataset_creation_plist = H5P_DEFAULT;    \n120   hid_t dataset_access_plist   = H5P_DEFAULT;    \n121   hid_t dataset = H5Dcreate2(                    \n122     file_identifier,          // Arg 1: file identifier\n123     ""data array"",             // Arg 2: dataset name\n124     H5T_IEEE_F64LE,           // Arg 3: datatype identifier\n125     filespace,                // Arg 4: filespace identifier\n126     link_creation_plist,      // Arg 5: link creation property list\n127     dataset_creation_plist,   // Arg 6: dataset creation property list\n128     dataset_access_plist);    // Arg 7: dataset access property list\n129 \n130   return dataset;\n131 }\nIn listing 16.6, the main write_hdf5_file  routine uses the filespace  dataspace that we\ncreated in listing 16.5. We then wrote out the dataset with the H5Dwrite  routine on line\n72, using both the memspace  and filespace  dataspaces. We also created and passed in a\nproperty list to tell HDF5 to use collective MPI-IO routines. Finally, on line 82, we closed\nthe file. We also closed the property list and dataset on previous lines to avoid memory\nleaks. For the routine to create the file, we finally call H5Fcreate  on line 106, but we\nneed several lines to set up the hints. We wrapped the property list setup for the collec-\ntive write and the MPI-IO hints along with the call and put these into a separate routine.\nWe also took the same approach with the HDF5 call on line 121 for creating the dataset\nso we could detail the different property lists that you can use.\n The routine to read the HDF5 data file, shown in the following listing, has the\nsame basic pattern as the earlier write operation. The biggest difference between this\nlisting and listing 16.6 is that there are fewer hints and attributes needed.\nHDF5Examples/hdf5block2d/hdf5_file_ops.c\n135 void read_hdf5_file(const char *filename, double **data1,\n136     hid_t memspace, hid_t filespace, MPI_Comm mpi_hdf5_comm) {\n137   hid_t file_identifier =                        \n         open_hdf5_file(filename, mpi_hdf5_comm);    \n138 \n139   // Create property list for collective dataset write.Listing 16.7 Reading an HDF5 fileHDF5 routine \ncreates the file.\nCreates\nthe link\ncreation\nproperty\nlist Creates the dataset \ncreation property list\nCreates\nthe\ndataset\naccess\nproperty\nlistHDF5 routine \ncreates the \ndataset.\nCalls the subroutine \nto open the file\n565 HDF5 is self-describing for better data management\n140   hid_t xfer_plist = H5Pcreate(H5P_DATASET_XFER);\n141   H5Pset_dxpl_mpio(xfer_plist, H5FD_MPIO_COLLECTIVE);\n142 \n143   hid_t dataset1 = \n            open_hdf5_dataset(file_identifier);      \n144   // read the data from disk using both the memory space\n      //    and the data space.\n145   H5Dread(dataset1, H5T_IEEE_F64LE, memspace,    \n146      filespace, H5P_DEFAULT, &(data1[0][0]));    \n147   H5Dclose(dataset1);\n148 \n149   H5Pclose(xfer_plist);\n150 \n151   H5Fclose(file_identifier);     \n152 }\n153 \n154 hid_t open_hdf5_file(const char *filename, MPI_Comm mpi_hdf5_comm){\n155   // set the file access template for parallel IO access\n156   hid_t file_access_plist = H5P_DEFAULT;   // File access property list\n157   file_access_plist = H5Pcreate(H5P_FILE_ACCESS);\n158 \n159   // set collective mode for metadata reads (ops)\n160   H5Pset_all_coll_metadata_ops(file_access_plist, true);\n161 \n162   // tell the HDF5 library that we want to use MPI-IO to do the reading\n163   H5Pset_fapl_mpio(file_access_plist, mpi_hdf5_comm, MPI_INFO_NULL);\n164 \n165   // Open the file collectively\n166   // H5F_ACC_RDONLY - sets access to read or write\n      //    on open of an existing file.\n167   // 3rd argument is the file access property list identifier\n168   hid_t file_identifier = H5Fopen(filename,     \n         H5F_ACC_RDONLY, file_access_plist);        \n169 \n170   // release the file access template\n171   H5Pclose(file_access_plist);\n172 \n173   return file_identifier;\n174 }\n175 \n176 hid_t open_hdf5_dataset(hid_t file_identifier){\n177   // open the dataset\n178   hid_t dataset_access_plist = H5P_DEFAULT;    \n179   hid_t dataset = H5Dopen2(                    \n180     file_identifier,        // Arg 1: file identifier\n181     ""data array"",           // Arg 2: dataset name to match for read\n182     dataset_access_plist);  // Arg 3: dataset access property list\n183 \n184   return dataset;\n185 }\nBecause the file already exists, we use an open call on line 168 in listing 16.7 to specify\nread-only mode. (Using read-only mode allows additional optimizations.) The accessed\nfile already has some attributes that were specified during the write. Some of theseCalls the subroutine \nto create the \ndataset\nReads the \ndataset\nCloses the objects \nand the data file\nHDF5 routine \nopens the file.\nCreates dataset \naccess property list\nHDF5 routine \ncreates the dataset.\n566 CHAPTER  16 File operations for a parallel world\nattributes do not need to be specified in the read. The HDF5 listings so far might com-\nprise a general-purpose library within an application. The next listing shows the calls\nthat would be placed at different points in the main application.\nHDF5Examples/hdf5block2d/hdf5block2d.c\n 52   hid_t memspace = H5S_NULL, filespace = H5S_NULL;\n 53   hdf5_file_init(ng, ndims, ny_global,             \n         nx_global, ny, nx, ny_offset, nx_offset,      \n 54      mpi_hdf5_comm, &memspace, &filespace);        \n 55 \n 56   char filename[30];\n 57   if (ncolors > 1) {\n 58     sprintf(filename,""example_%02d.hdf5"",color);\n 59   } else {\n 60     sprintf(filename,""example.hdf5"");\n 61   }\n 62 \n 63   // Do the computation and write out a sequence of files\n 64   write_hdf5_file(filename, data, memspace,      \n                      filespace, mpi_hdf5_comm);     \n 65   // Read back the data for verifying the file operations\n 66   read_hdf5_file(filename, data_restore,         \n           memspace, filespace, mpi_hdf5_comm);      \n 67 \n 68   hdf5_file_finalize(&memspace, &filespace);    \nIn listing 16.8, the initialization operation to set up the dataspaces on line 53 can be\ndone once, at the start of your program. Then you might write out the data in your\nprogram at periodic intervals for graphics and checkpointing. The read would then\ntypically be done when restarting from a checkpoint at the start of a run. Lastly, the\nfinalize call should be done at the end of the program before terminating the calcula-\ntion. Now to compile and run the example. The build is a standard CMake build.\nWe’ll run it on four processors:\nmkdir build && cd build\ncmake ..\nmake\nmpirun -n 4 ./hdf5block2d\nIn a single install, the HDF5 package can be either installed as a parallel or as a serial\nversion but not both. A common problem is to link the wrong version into your appli-\ncation. We added some special code to the CMake build system to preferentially select\na parallel version as the next listing shows. The program then fails if the HDF5 version\nis not parallel so that we don’t get an error during the build.\n \n Listing 16.8 Main application file\nSets up the memory \nand file dataspaces\nWrites the \nHDF5 data file\nReads in the data from \nthe HDF5 data file\nFrees the dataspace \nobjects\n567 HDF5 is self-describing for better data management\nHDF5Examples/hdf5block2d/CMakeLists.txt\n14 set(HDF5_PREFER_PARALLEL true)\n15 find_package(HDF5 1.10.1 REQUIRED)\n16 if (NOT HDF5_IS_PARALLEL)\n17     message(FATAL_ERROR "" -- HDF5 version is not parallel."")\n18 endif (NOT HDF5_IS_PARALLEL)\nThe example code does a verification test to check that the data read back from the\nfile is the same as the data that we started with. We can also use the h5dump utility to\nprint the data in the file. You can use the following command to look at your data file.\nFigure 16.6 shows the output from the command.\nh5dump -y example.hdf5Listing 16.9 Checking for a parallel HDF5 package\nFigure 16.6 Using the h5dump command-line utility shows what is contained in the HDF5 \nfile without having to write any code.",19874
206-16.5 Other parallel file software packages.pdf,206-16.5 Other parallel file software packages,,0
207-16.6 Parallel filesystem The hardware interface.pdf,207-16.6 Parallel filesystem The hardware interface,,0
208-16.6.1 Everything you wanted to know about your parallel file setup but didnt know how to ask.pdf,208-16.6.1 Everything you wanted to know about your parallel file setup but didnt know how to ask,"568 CHAPTER  16 File operations for a parallel world\n16.5 Other parallel file software packages\nIn this section, we briefly cover a couple of the more common parallel file software\npackages: PnetCDF and Adios. PnetCDF, short for Parallel Network Common Data\nForm, is another self-describing data format that is popular in the Earth Systems com-\nmunity and among organizations funded by the National Science Foundation (NSF).\nWhile originally a completely separate software source, the parallel version is built on\ntop of HDF5 and MPI-IO. The decision of whether to use PnetCDF or HDF5 is strongly\ninfluenced by your community. Because the files generated by your application are\noften used by others, using the same data standard is important.\n ADIOS, or the Adaptable Input/Output System, is also a self-describing data for-\nmat from Oak Ridge National Laboratory (ORNL). ADIOS has its own native binary\nformat, but it can also use HDF5, MPI-IO, and other file-storage software.\n16.6 Parallel filesystem: The hardware interface\nWith increasing data demands, more complex filesystems become necessary. In this\nsection, we will introduce these parallel filesystems. A parallel filesystem can greatly\nspeed up file writes and reads by spreading out the operations across several hard\ndisks with multiple file writers or readers. While we now have some parallelism at the\nfilesystem, it is not a simple situation. There is still a mismatch between the applica-\ntion parallelism and the parallelism provided by the filesystem. Because of this, the\nmanagement of the parallel operations is complex and highly dependent on the hard-\nware configurations and application demands. To deal with the complexity, many of the\nparallel filesystems use an object-based file structure. Object-based filesystems are a nat-\nural fit for these challenges. But the performance and robustness of the parallel filesys-\ntem is often limited by the metadata describing the locations of the file data.\nDEFINITION Object-based filesystem  is a system that’s organized based on objects\nrather than on files in a folder. An object-based filesystem requires a database\nor metadata to store all the information describing the object.\nThe writing of parallel file operations is highly intertwined with the parallel filesystem\nsoftware. This requires the knowledge of which parallel filesystem is being used and\nthe settings available for that installation and filesystem. Tuning your parallel file soft-\nware can sometimes yield significant performance gains.\n16.6.1 Everything you wanted to know about your parallel file setup \nbut didn’t know how to ask\nAs you get into the interaction of the parallel file operations with the filesystem, it is\nhelpful to see more information about the parallel library settings. The settings can be\nset differently for each installation. You can also get some high-level statistics that can\nhelp with debugging performance issues. \n Most MPI-IO libraries are one of two implementations, either ROMIO, which is\ndistributed with MPICH and many system vendor implementations, or OMPIO, which\n569 Parallel filesystem: The hardware interface\nis the default on newer versions of OpenMPI. Let’s first go over how to get informa-\ntion from OpenMPI’s OMPIO plugin or how to switch back to using ROMIO. To\nextract information on OpenMPI’s OMPIO settings, use the following commands:\n--mca  io [ompio|romio]  \nSpecifies the IO plugin, either OMPIO or ROMIO. Older releases use ROMIO\nas the default plugin, while OMPIO is the default on newer releases.\nompi_info  --param  <component>  <plugin>  --level  <int>  \nDisplays information on the local OpenMPI configuration for that plugin.\n--mca  io_ompio_verbose_info_parsing  1 \nShows the hints parsed from a program's MPI_Info_set  calls.\nFirst, you can get the names of the IO plugins with the ompi_info  command. We just\nwant the IO component plugins, so we filter the output for these:\nompi_info |grep ""MCA io:""\nMCA io: romio321 (MCA v2.1.0, API v2.0.0, Component v4.0.3)\nMCA io: ompio (MCA v2.1.0, API v2.0.0, Component v4.0.3)\nThen you can get the individual settings available for each plugin. Using the\nompi_info  command, we get the following abbreviated output:\nompi_info --param io ompio --level 9 | grep "": parameter""\nMCA io ompio: parameter ""io_ompio_priority"" (current value: ""30"" …\nMCA io ompio: parameter ""io_ompio_delete_priority"" (current value: ""30"" …\nMCA io ompio: parameter ""io_ompio_record_file_offset_info"" (current value: ""0"" …\nMCA io ompio: parameter ""io_ompio_coll_timing_info"" (current value: ""1"" …\nMCA io ompio: parameter ""io_ompio_cycle_buffer_size"" (current value: ""536870912"" …\nMCA io ompio: parameter ""io_ompio_bytes_per_agg"" (current value: ""33554432"" …\nMCA io ompio: parameter ""io_ompio_num_aggregators"" (current value: ""-1"" …\nMCA io ompio: parameter ""io_ompio_grouping_option"" (current value: ""5"" …\nMCA io ompio: parameter ""io_ompio_max_aggregators_ratio"" (current value: ""8"" …\nMCA io ompio: parameter ""io_ompio_aggregators_cutoff_threshold"" (current value: ""3"" \n…\nMCA io ompio: parameter ""io_ompio_overwrite_amode"" (current value: ""1"" …\nMCA io ompio: parameter ""io_ompio_verbose_info_parsing"" (current value: ""0"" \n...\nYou can also verify how the MPI_Info_set  calls are interpreted by the MPI-IO library\nwith the following run-time option. This can be a good way to check that your code is\ncorrectly written for your filesystem and parallel file operation libraries.\nmpirun --mca io_ompio_verbose_info_parsing 1 -n 4 ./mpi_io_block2d\nFile: example.data info: collective_buffering value true enforcing using \nindividual fcoll component\n < ... repeated three more times ... >\nFor the ROMIO parallel file software included with MPICH, we have different mecha-\nnisms to query the software installation. Cray adds some additional environment\n570 CHAPTER  16 File operations for a parallel world\nvariables for their implementations of ROMIO. We’ll list some of these and then see\nexamples that use these.\nROMIO recognizes the following hint:\n–ROMIO_PRINT_HINTS=1\nCray provides these additional environment variables:\n–MPICH_MPIIO_HINTS_DISPLAY=1\n–MPICH_MPIIO_STATS=1\n–MPICH_MPIIO_TIMERS=1\nThe following shows the output when using ROMIO_PRINT_HINTS :\nexport ROMIO_PRINT_HINTS=1; mpirun -n 4 ./mpi_io_block2d\nkey = cb_buffer_size            value = 16777216  \nkey = romio_cb_read             value = automatic \nkey = romio_cb_write            value = automatic \nkey = cb_nodes                  value = 1         \nkey = romio_no_indep_rw         value = false     \nkey = romio_cb_pfr              value = disable   \nkey = romio_cb_fr_types         value = aar       \nkey = romio_cb_fr_alignment     value = 1         \nkey = romio_cb_ds_threshold     value = 0         \nkey = romio_cb_alltoall         value = automatic \nkey = ind_rd_buffer_size        value = 4194304   \nkey = ind_wr_buffer_size        value = 524288    \nkey = romio_ds_read             value = automatic \nkey = romio_ds_write            value = automatic \nkey = striping_unit             value = 4194304   \nkey = cb_config_list            value = *:1       \nkey = romio_filesystem_type     value = NFS:      \nkey = romio_aggregator_list     value = 0         \nkey = cb_buffer_size            value = 16777216  \nkey = romio_cb_read             value = automatic \nkey = romio_cb_write            value = automatic \nkey = cb_nodes                  value = 1         \nkey = romio_no_indep_rw         value = false     \nkey = romio_cb_pfr              value = disable   \nkey = romio_cb_fr_types         value = aar       \nkey = romio_cb_fr_alignment     value = 1         \nkey = romio_cb_ds_threshold     value = 0         \nkey = romio_cb_alltoall         value = automatic \nkey = ind_rd_buffer_size        value = 4194304   \nkey = ind_wr_buffer_size        value = 524288    \nkey = romio_ds_read             value = automatic \nkey = romio_ds_write            value = automatic \nkey = cb_config_list            value = *:1       \nkey = romio_filesystem_type     value = NFS:      \nkey = romio_aggregator_list     value = 0\nexport MPICH_MPIIO_HINTS_DISPLAY=1; srun -n 4 ./mpi_io_block2d\nPE 0: MPICH MPIIO environment settings:\nPE 0:   MPICH_MPIIO_HINTS_DISPLAY                 = 1\nPE 0:   MPICH_MPIIO_HINTS                         = NULL\n571 Parallel filesystem: The hardware interface\nPE 0:   MPICH_MPIIO_ABORT_ON_RW_ERROR             = disable\nPE 0:   MPICH_MPIIO_CB_ALIGN                      = 2\nPE 0:   MPICH_MPIIO_DVS_MAXNODES                  = -1\nPE 0:   MPICH_MPIIO_AGGREGATOR_PLACEMENT_DISPLAY  = 0\nPE 0:   MPICH_MPIIO_AGGREGATOR_PLACEMENT_STRIDE   = -1\nPE 0:   MPICH_MPIIO_MAX_NUM_IRECV                 = 50\nPE 0:   MPICH_MPIIO_MAX_NUM_ISEND                 = 50\nPE 0:   MPICH_MPIIO_MAX_SIZE_ISEND                = 10485760\nPE 0: MPICH MPIIO statistics environment settings:\nPE 0:   MPICH_MPIIO_STATS                         = 0\nPE 0:   MPICH_MPIIO_TIMERS                        = 0\nPE 0:   MPICH_MPIIO_WRITE_EXIT_BARRIER            = 1\nMPIIO WARNING: DVS stripe width of 8 was requested but DVS set it to 1\nSee MPICH_MPIIO_DVS_MAXNODES in the intro_mpi man page.\nPE 0: MPIIO hints for example.data:\n          cb_buffer_size           = 16777216\n          romio_cb_read            = automatic\n          romio_cb_write           = automatic\n          cb_nodes                 = 1\n          cb_align                 = 2\n          romio_no_indep_rw        = false\n          romio_cb_pfr             = disable\n          romio_cb_fr_types        = aar\n          romio_cb_fr_alignment    = 1\n          romio_cb_ds_threshold    = 0\n          romio_cb_alltoall        = automatic\n          ind_rd_buffer_size       = 4194304\n          ind_wr_buffer_size       = 524288\n          romio_ds_read            = disable\n          romio_ds_write           = automatic\n          striping_factor          = 1\n          striping_unit            = 4194304\n          direct_io                = false\n          aggregator_placement_stride = -1\n          abort_on_rw_error        = disable\n          cb_config_list           = *:*\n          romio_filesystem_type    = CRAY ADIO:\nexport MPICH_MPIIO_STATS=1; srun -n 4 ./mpi_io_block2d\n+--------------------------------------------------------+\n| MPIIO write access patterns for example.data\n|   independent writes      = 0\n|   collective writes       = 4\n|   independent writers     = 0\n|   aggregators             = 1\n|   stripe count            = 1\n|   stripe size             = 4194304\n|   system writes           = 2\n|   stripe sized writes     = 0\n|   aggregators active      = 4,0,0,0 (1, <= 1, > 1, 1)\n|   total bytes for writes  = 3600\n|   ave system write size   = 1800\n|   read-modify-write count = 0\n|   read-modify-write bytes = 0\n|   number of write gaps    = 0",10935
209-16.6.2 General hints that apply to all filesystems.pdf,209-16.6.2 General hints that apply to all filesystems,"572 CHAPTER  16 File operations for a parallel world\n|   ave write gap size      = NA\n| See ""Optimizing MPI I/O on Cray XE Systems"" S-0013-20 for explanations.\n+--------------------------------------------------------+\n+--------------------------------------------------------+\n| MPIIO read access patterns for example.data\n|   independent reads       = 0\n|   collective reads        = 4\n|   independent readers     = 0\n|   aggregators             = 1\n|   stripe count            = 1\n|   stripe size             = 524288\n|   system reads            = 1\n|   stripe sized reads      = 0\n|   total bytes for reads   = 3200\n|   ave system read size    = 3200\n|   number of read gaps     = 0\n|   ave read gap size       = NA\n| See ""Optimizing MPI I/O on Cray XE Systems"" S-0013-20 for explanations.\n+--------------------------------------------------------+\n16.6.2 General hints that apply to all filesystems\nIt is sometimes useful to give some hints about the type of file operations you will use\nin your application. You can modify the parallel file settings with environment vari-\nables, a hints file, or at run time with MPI_Info_set . This provides the appropriate\nmethod for handling different scenarios if you don’t have access to the program\nsource to add the MPI_Info_set  command. To set parallel file options in this case, use\nthe following commands:\nCray MPICH\nMPICH_MPIIO_HINTS=”*:<key>=<value>:<key>=<value>\nFor example\nexport MPICH_MPIIO_HINTS=\\n   ”*:striping_factor=8:striping_unit=4194304”\nROMIO\nROMIO_HINTS=<filename> \nFor example: ROMIO_HINTS=romio-hints\nwhere the romio-hints file includes \nstriping_factor 8      // file is broken into 8 parts and\n                       // is written in parallel to 8 disks\nstriping_unit 4194304  // the size in bytes of each \n                       // block to be written\nOpenMPI OMPI\nOMPI_MCA_<param_name> <value> \nFor example: export  OMPI_MCA_io_ompio_verbose_info_parsing=1\n573 Parallel filesystem: The hardware interface\nThe OpenMPI mca run-time option as an argument to the mpirun command is\nmpirun --mca io_ompio_verbose_info_parsing  1 -n 4 <exec>\nThe default location of the OpenMPI file is in $HOME/.openmpi/mca-params.conf\nor it can be set with the following:\n--tune <filename> \nmpirun --tune mca-params.conf -n 2 <exec>\nThe most important hint that you can set is whether to use collective operations or\ndata sieving. We’ll first look at the collective operations and then the data sieving\noperations.\n Collective operations harness MPI collective communication calls and use a two-\nphase I/O approach that collects the data for aggregators that then write or read from\nyour file. Use the following commands for collective I/O:\nROMIO and OMPIO\n–cb_buffer_size=integer  specifies the buffer size in bytes for a two-phase\ncollective I/O. It should be a multiple of the page size.\n–cb_nodes=integer  sets the maximum number of aggregators.\nROMIO only\n–romio_cb_read=[enable|automatic|disable]  specifies when to use collec-\ntive buffering for reads.\n–romio_cb_write=[enable|automatic|disable]  specifies when to use collec-\ntive buffering for writes.\n–cb_config_list=*:<integer>  sets the number of aggregators per node.\n–romio_no_indep_rw=[true|false]  specifies whether to use any independent\nI/O. If none are allowed, no file operations (including file open) will be\ndone on non-aggregator nodes.\nOMPIO only\n–collective_buffering=[true|false]  uses collective operations when writ-\ning from a parallel job to the filesystem.\nData sieving does a single read (or write), spanning a file block and then parcels out\nthe data to the individual process reads. This avoids a lot of smaller reads and the con-\ntention between file readers that might occur. Use the following commands for data\nsieving with ROMIO:\nromio_ds_read=[enable|automatic|disable]\nromio_ds_write=[enable|automatic|disable]\nind_rd_buffer_size=integer  (bytes for read buffer)\nind_wr_buffer_size=integer  (bytes for write buffer)",4045
210-16.6.3 Hints specific to particular filesystems.pdf,210-16.6.3 Hints specific to particular filesystems,"574 CHAPTER  16 File operations for a parallel world\n16.6.3 Hints specific to particular filesystems\nSome hints only apply to a particular filesystem, such as Lustre or GPFS. We can detect\nthe filesystem type from within our program and set the appropriate hints for the\nfilesystem. The fs_detect.c program in the examples does this. This program uses the\nstatfs  command as the next listing shows and you can find it in the examples direc-\ntory for this chapter.\nMPI_IO_Examples/mpi_io_block2d/fs_detect.c\n 1 #include <stdio.h>\n 2 #ifdef __APPLE_CC__\n 3 #include <sys/mount.h>\n 4 #else\n 5 #include <sys/statfs.h>\n 6 #endif\n 7 // Filesystem types are listed in the system \n   //   include directory in linux/magic.h\n 8 // You will need to add any additional \n   //   parallel filesystem magic codes\n 9 #define LUSTRE_MAGIC1     0x858458f6         \n10 #define LUSTRE_MAGIC2     0xbd00bd0          \n11 #define GPFS_SUPER_MAGIC  0x47504653         \n12 #define PVFS2_SUPER_MAGIC 0x20030528         \n13 #define PAN_KERNEL_FS_CLIENT_SUPER_MAGIC  \  \n                             0xAAD7AAEA         \n14 \n15 int main(int argc, char *argv[])\n16 {\n17   struct statfs buf;\n18   statfs(""./fs_detect"", &buf);     \n19   printf(""File system type is %lx\n"",buf.f_type);\n20 }\nWe included the magic number for some of the parallel filesystems in this listing.\nWhen using this for other applications, replace the filename on line 18 with an appro-\npriate filename for the directory where your files are written. Build the fs_detect pro-\ngram and then run the following command to get the filesystem type:\nmkdir build && cd build\ncmake ..\nmake\ngrep `./fs_detect | cut -f 4 -d' '` /usr/include/linux/magic.h ../fs_detect.c\nNow we are ready for the filesystem-specific hints. We don’t list all the possible hints.\nYou can get the current list by using the commands previously shown.\nLUSTRE  FILESYSTEM : THE MOST COMMON  FILESYSTEM  IN HIGH PERFORMANCE  COMPUTING  CENTERS\nLustre is the dominant filesystem on the largest high performance computing sys-\ntems. Originating at Carnegie Mellon University, its primary development and owner-\nship has been passed through Intel, HP, Sun, Oracle, Intel, Whamcloud, and others.Listing 16.10 Filesystem detection program\nMagic numbers \nfor parallel \nfilesystem type \nGets filesystem \ntype\n575 Parallel filesystem: The hardware interface\nIn this process, it has passed from commercial to open source and back. Currently it is\nunder the Open Scalable File Systems (OpenSFS) and European Open File Systems\n(EOFS) banners. \n Lustre is built on the concept of object storage with Object Storage Servers (OSSs)\nand Object Storage Targets (OSTs). When we specify a striping_factor  of 8 on line\n56 of listing 16.2 and line 96 of listing 16.6, we are telling the ROMIO library to use\nLustre to break up the writes (and reads) into eight pieces and send them to eight\nOSTs, effectively writing out the data in eight-way parallelism. The striping_unit\nhint tells ROMIO and Lustre to use 4 MiB stripe sizes. Lustre also has Metadata Serv-\ners (MDS) and Metadata Targets (MDT) to store the critical descriptions of where\neach part of the file is stored. For striping operations, use the following:\nMPICH (ROMIO)\n–striping_unit=<integer>  sets the stripe size in bytes.\n–striping_factor=<integer>  sets the number of stripes, where -1 is auto-\nmatic.\nOpenMPI (OMPIO)\n–fs_lustre_stripe_size=<integer>  sets the stripe size in bytes.\n–fs_lustre_stripe_width=<integer>  sets the number of stripes, where -1 is\nautomatic.\nWe can confirm the Lustre parameters for OpenMPI with a command-line query: \nompi_info --param fs lustre --level 9\nMCA fs lustre: parameter ""fs_lustre_priority"" (current value: ""20"" …\nMCA fs lustre: parameter ""fs_lustre_stripe_size"" (current value: ""0"" …\nMCA fs lustre: parameter ""fs_lustre_stripe_width"" (current value: ""0"" …\nGPFS: A FILESYSTEM  FROM IBM\nIBM systems have the General Parallel File System (GPFS), also part of their Spectrum\nScale product, that offers striping and parallel file operations on their systems. GPFS\nis an enterprise storage product with the corresponding support infrastructure and\nservices. GPFS stripes across all available devices by default. The MPI hints may not\nhave as much effect on this filesystem, however. For MPICH (ROMIO), use this com-\nmand to help with large memory writes/reads:\nIBM_largeblock_io=true\nDATAWARP: A FILESYSTEM  FROM CRAY\nCray’s DataWarp integrates burst buffer hardware on top of another parallel filesys-\ntem, such as their version of Lustre. Taking advantage of burst buffers is still in its\ninfancy, though, but Cray has been a leader in this effort.\nPANASAS ®: A COMMERCIAL  FILESYSTEM  REQUIRING  FEWER  HINTS FROM USERS\nPanasas® is a commercial parallel filesystem that is composed of object storage and meta-\ndata servers. Panasas has also contributed to the extension to the Network File System\n576 CHAPTER  16 File operations for a parallel world\n(NFS) to support parallel operations. Panasas was used in some of the top-ten computing\nsystems at LANL, although it is not so prevalent there today. For MPICH (ROMIO), use\nthese commands to set the strip size and the number of stripes, respectively:\npanfs_layout_stripe_unit=<integer>\npanfs_layout_total_num_comps=<integer>\nORANGE FS (PVFS): T HE MOST POPULAR  OPEN-SOURCE  FILESYSTEM  \nOrangeFS, previously known as the Parallel Virtual File System (PVFS), is an open\nsource parallel filesystem from Clemson University and Argonne National Laboratory.\nIt is popular on Beowulf clusters. Besides being a scalable parallel filesystem, OrangeFS\nhas been integrated into the Linux kernel. You can use the following commands for\nMPICH (ROMIO) to set the stripe size (in bytes) and to number the stripes (with –1\nbeing automatic), respectively:\nstriping_unit=<integer>\nstriping_factor=<integer>\nBEEGFS: A NEW OPEN SOURCE  FILESYSTEM  THAT IS GAINING  IN POPULARITY\nBeeGFS, formerly FhGFS, was developed at the Fraunhofer Center for High Perfor-\nmance Computing and is freely available. It is popular because of its open source\ncharacteristics.\nDISTRIBUTED  APPLICATION  OBJECT STORAGE  (DAOS): S ETTING  NEW BENCHMARKS  FOR PERFORMANCE\nIntel is developing their new, open source DAOS object-storage technology under the\nDepartment of Energy (DOE) FastForward program. DAOS ranks first in the 2020 ISC\nIO500 supercomputing file-speed list ( https:/ /www.vi4io.org ). It’s scheduled to be\ndeployed on the Aurora supercomputer, Argonne National Laboratory’s first exascale\ncomputing system, in 2021. DAOS is supported in the ROMIO MPI-IO library, avail-\nable with MPICH, and is portable to other MPI libraries.\nWEKAIO: A NEWCOMER  FROM THE BIG DATA COMMUNITY\nWekaIO is a fully POSIX-compliant filesystem that provides a large shared namespace\nwith highly optimized performance, low latency, and high bandwidth, and uses the lat-\nest solid-state hardware components. WekaIO is an attractive filesystem for applica-\ntions that require large amounts of high-performing data file manipulation and is\npopular in the big data community. WekaIO took top honors in the 2019 SC IO500\nsupercomputing file speed list.\nCEPH FILESYSTEM : AN OPEN SOURCE  DISTRIBUTED  STORAGE  SYSTEM\nCeph originated at Lawrence Livermore National Laboratory. The development is\nnow led by RedHat for a consortium of industrial partners and has been integrated\ninto the Linux kernel.\nNETWORK  FILESYSTEM  (NFS): T HE MOST COMMON  NETWORK  FILESYSTEM  \nNFS is the dominant cluster filesystem for the networks in local organizations. It is not\na recommended system for highly parallel file operations, although with the proper\nsettings, it functions correctly.",7799
211-16.7 Further explorations.pdf,211-16.7 Further explorations,,0
212-16.7.2 Exercises.pdf,212-16.7.2 Exercises,"577 Further explorations\n16.7 Further explorations\nMuch of the current documentation on parallel file operations is in presentations and\nacademic conferences. One of the best conferences is the Parallel Data Systems Work-\nshop (PDSW), held in conjunction with The International Conference for High Per-\nformance Computing, Networking, Storage, and Analysis (otherwise known as the\nyearly Supercomputing Conference). \n You can use the micro benchmarks, IOR and mdtest, to check the best perfor-\nmance of a filesystem. The software is documented at https:/ /ior.readthedocs.io/en/\nlatest/  and hosted by LLNL at https:/ /github.com/hpc/ior . \n16.7.1 Additional reading\nThe addition of the MPI-IO functions to MPI is described in the following text. It\nremains one of the best descriptions of MPI-IO.\nWilliam Gropp, Rajeev Thakur, and Ewing Lusk. Using MPI-2: Advanced Features of\nthe Message Passing Interface  (MIT Press, 1999).\nThere are a couple of good books on writing high performance parallel file opera-\ntions. We recommend the following:\nPrabhat and Quincey Koziol, editors, High Performance Parallel I/O  (Chapman\nand Hall/CRC, 2014).\nJohn M. May, Parallel I/O for High Performance Computing  (Morgan Kaufmann,\n2001).\nThe HDF Group maintains the authoritative website on HDF5. You can get more\ninformation at\nThe HDF Group, https:/ /portal.hdfgroup.org/display/HDF5/HDF5 .\nNetCDF remains popular within certain HPC application segments. You can get\nmore information on this format at the NetCDF site hosted by Unidata. Unidata is\none of the University Corporation for Atmospheric Research (UCAR)’s Community\nPrograms (UCP).\nUnidata, https:/ /www.unidata.ucar.edu/software/netcdf/ .\nA parallel version of NetCDF, PnetCDF, was developed by Northwestern University\nand Argonne National Laboratory independently from Unidata. More information on\nPnetCDF is at their GitHub documentation site:\nNorthwestern University and Argonne National Laboratory, https:/ /parallel-netcdf\n.github.io .",2026
213-17 Tools and resources for better code.pdf,213-17 Tools and resources for better code,"578 CHAPTER  16 File operations for a parallel world\nADIOS is one of the leading parallel file operations libraries maintained by a team led\nby Oak Ridge National Laboratory (ORNL). To learn more, see their documentation\nat the following website:\nOak Ridge National Laboratory, https:/ /adios2.readthedocs.io/en/latest/index.html .\nSome good presentations on tuning performance for filesystems include\nPhilippe Wautelet, “Best practices for parallel IO and MPI-IO hints” (CRNS/\nIDRIS, 2015), http:/ /www.idris.fr/media/docs/docu/idris/idris_patc_hints_\nproj.pdf .\nGeorge Markomanolis, ORNL Spectrum Scale (GPFS) https:/ /www.olcf.ornl\n.gov/wp-content/uploads/2018/12/spectrum_scale_summit_workshop.pdf .\n16.7.2 Exercises\n1Check for the hints available on your system using the techniques described in\nsection 16.6.1.\n2Try the MPI–IO and HDF5 examples on your system with much larger datasets\nto see what performance you can achieve. Compare that to the IOR micro\nbenchmark for extra credit.\n3Use the h5ls and h5dump utilities to explore the HDF5 data file created by the\nHDF5 example.\nSummary\nThere is a proper way to handle standard file operations for parallel applica-\ntions. The simple techniques introduced in this chapter, where all IO is per-\nformed from the first processor, are sufficient for modest parallel applications.\nThe use of MPI-IO is an important building block for parallel file operations.\nMPI-IO can dramatically speed up the writing and reading of files.\nThere are advantages of using the self-describing parallel HDF5 software. The\nHDF5 format can improve how your application manages data while also get-\nting fast file operations.\nThere are ways to query and set the hints for the parallel file software and\nfilesystem. This can improve your file writing and reading performance on par-\nticular systems.\n579Tools and resources\nfor better code\nWhy a whole chapter on tools and resources? Though we’ve mentioned tools and\nresources in previous chapters, this chapter further discusses the wide variety and\nalternatives available to high-performance computing programmers. From version\ncontrol systems to debugging, the available capabilities, whether commercial or open\nsource, are essential to enable the rapid iterations of parallel application develop-\nment. Nonetheless, these tools are not mandatory. Having an understanding of and\nembedding these into your workflow often yields tremendous benefits, far out-\nweighing the time spent learning how to use them.\n Tools are an important piece of the high-performance computing development\nprocess. Not every tool works on every system; therefore, availability of alternatives is\nimportant. In the previous chapters, we wanted to focus on the process and not get\nbogged down in the details of how to use every possible tool. We chose to present the\nsimplest, most available tool for each need. We also preferred the command-line andThis chapter covers\nPotential tools for your development toolbelt\nVarious resources to guide your application \ndevelopment\nCommon tools to work on large computing sites\n580 CHAPTER  17 Tools and resources for better code\ntext-based tools over the fancy graphical interface tools because using graphics inter-\nfaces over slow networks can be difficult or even impossible. Graphical tools also tend to\nbe more vendor- or system-centric and often change. Despite these drawbacks, we\ninclude many of these vendor tools in this chapter because they can greatly improve\nyour code development for high-performance computing applications.\n Resources such as a wide variety of benchmark applications, are valuable because\napplications don’t come in just one flavor. For these specialized application domains,\nwe need more appropriate benchmarks and mini-apps that explore the best approach\nfor algorithm development and the right programming pattern for each architecture.\nWe strongly recommend that you learn from these resources rather than reinventing\nthe techniques from scratch. For most of the tools, we give brief instructions on instal-\nlation and where to find some documentation. We also provide more detail in the com-\npanion code for this chapter at https:/ /github.com/EssentialsofParallelComputing/\nChapter17 . \n We are strongly vendor-agnostic and stress portability as well. Although we cover a\nlot of tools, it just isn’t possible to go into detail on all of them. In addition, the rate of\nchange for these tools exceeds that of the rest of the high-performance computing\necosystem. History has shown that the support for good tool development is fickle.\nThus, the tools come and go and change ownership more quickly than documenta-\ntion can be updated. \n For a quick reference, table 17.1 provides a summary of the tools we cover in this\nchapter. These are shown in their corresponding categories to help you find the best\ntools for your needs. We included a wide variety of tools because there may be only one\nthat works on a particular hardware or operating system or may have specialized capabil-\nities. We have chosen to give more details on some of the simpler, more useful and com-\nmonly used tools in the following sections of this chapter as indicated in the table. \nTable 17.1 Summary of tools covered in this chapter\n17.1 Version \ncontrol systems17.1.1 Centralized version control Subversion\nCVS\n17.1.2 Distributed version control Git\nMercurial\n17.2 Timer routines clock_gettime  (with a CLOCK_MONOTONIC  type)\nclock_gettime  (with a CLOCK_REALTIME  type)\ngettimeofday\ngetrusage\nhost_get_clock_service ( for MacOS C++ high resolution clock )\n17.3 Profilers 17.3.1 Simple text-based profilers Likwid\ngprof\ngperftools\ntimemory\nOpen|SpeedShop\n581\n17.3.2 High-level profilers Kcachegrind\nArm MAP\n17.3.3 Medium-level profilers Intel® Advisor\nIntel® Vtune\nCrayPat\nAMD µProf\nNVIDIA Visual Profiler\nCodeXL\n17.3.4 Detailed profilers HPCToolkit\nOpen|SpeedShop\nTAU\n17.5 Memory error tools Free software 17.5.1 Valgrind\n17.5.2 Dr. Memory\n17.5.3 Commercial software Purify\nInsure++\nIntel® Inspector\nTotalView memory checker\n17.5.4 Compiler-based MemorySanitizer (LLVM)\nAddressSanitizer (LLVM)\nThreadSanitizer (LLVM)\nmtrace (GCC)\n17.5.5 Out-of-bounds checkers Dmalloc\nElectric Fence\nMemwatch\n17.5.6 GPU memory tools CUDA-MEMCHECK\n17.6 Thread checkers 17.6.1 Intel® Inspector\n17.6.2 Archer\n17.7 Debuggers Commercial 17.7.1 TotalView\n17.7.2 ARM DDT\n17.7.3 Linux debuggers GDB\ncgdb\nDDD\n17.7.4 GPU debuggers CUDA-GDB\nROCgdb\n17.8 File operation profiler Darshan\n17.9 Package managers 17.9.1 MacOS Homebrew\nMacPorts\n17.9.2 HPC Spack\n17.10 Modules 17.10.1 Modules\n17.10.2 LmodTable 17.1 Summary of tools covered in this chapter",6774
214-17.1 Version control systems It all begins here.pdf,214-17.1 Version control systems It all begins here,,0
215-17.1.2 Centralized version control for simplicity and code security.pdf,215-17.1.2 Centralized version control for simplicity and code security,"582 CHAPTER  17 Tools and resources for better code\n17.1 Version control systems: It all begins here\nVersion control for software is one of the most basic of software engineering practices\nand critically important when developing parallel applications. We covered the role of\nversion control in parallel application development in section 2.1.1. Here, we go into\nmore detail on the various version control systems and their characteristics. Version\ncontrol systems can be broken down into two major categories, distributed and cen-\ntralized, as figure 17.1 shows.\nIn a centralized version control system there is just one central repository. This\nrequires a connection to the repository site to do any operations on the repository. In\na distributed version control system, various commands, such as clone , create a dupli-\ncate (remote) version of the repository and a checkout of the source. You can commit\nyour changes to your local version of the repository while traveling, then push or\nmerge the changes into the main repository at a later time. No wonder distributed ver-\nsion control systems have gained popularity in recent years. That said, these also come\nwith another layer of complexity.\n17.1.1 Distributed version control fits the more mobile world\nMany code teams are scattered across the globe or on the move all the time. For them,\na distributed version control system makes the most sense. The two most common\nfreely-available distributed version control systems are Git and Mercurial. There are\nseveral other smaller distributed version control systems as well. All of these imple-\nmentations support a variety of developer workflows. my_repo@hpc_site\nmy_repo@laptopClone\nCheckout\nmy_checkout@my_repomy_repo@desktop\nCheckout\nmy_checkout@my_repoClone my_repo@hpc_site\nmy_checkout@my_repoCheckout\nDistributed Version Control Centralized Version Control\nFigure 17.1 Selecting a type of version control is dependent on your work pattern. Centralized \nversion control is for when everyone is at a location with access to a single server. Distributed \nversion control gives you a full copy of your repository on your laptop and desktop and allows \nyou to go worldwide and mobile.",2223
216-17.2 Timer routines for tracking code performance.pdf,216-17.2 Timer routines for tracking code performance,"583 Timer routines for tracking code performance\n Despite claims to be easy to learn, these are complex tools to fully understand and\nuse properly. It would take a full book to cover each of these. Fortunately, there are\nmany web tutorials and books that cover their use. A good starting point for Git\nresources is the Git SCM site: https:/ /git-scm.com .\n Mercurial is a bit simpler and has a cleaner design than Git. Additionally, the Mer-\ncurial website has a lot of tutorials to get you started.\nMercurial at https:/ /www.mercurial-scm.org/wiki/Mercurial\nBryan O’Sullivan, Mercurial: The Definitive Guide  (O’Reilly Media, 2009), http:/ /\nhgbook.red-bean.com\nThere are also some commercially distributed version control systems. Perforce and\nClearCase are the best known. With these products, you can get more support, which\nmight be important for your organization.\n17.1.2 Centralized version control for simplicity and code security\nWhile there are many centralized version control systems that have been developed\nover a long history of software configuration management, the two most commonly\nused today are Concurrent Versions System (CVS) and Subversion (SVN). Both are a\nbit dated these days as interest has shifted towards distributed version control. If used\nin the proper way for a centralized repository, however, these are both effective and\nmuch simpler to use. \n Centralized version control also provides better security for proprietary codes by\nhaving only one place where the repository needs to be protected. For this reason,\ncentralized version control is still popular in the corporate environment, where limit-\ning access to the source code history is of paramount importance. CVS has a simple\nbranching operation that works well. There is documentation at the CVS website and\na widely available book:\nCVS (Free Software Foundation, Inc., 1998) at https:/ /www.nongnu.org/cvs/\nPer Cederqvist, Version Management with CVS  (Network Theory Ltd, December,\n2002), available at various sites online and in print\nSubversion was developed as a replacement for CVS. Although in many respects, it is\nan improvement over CVS, the branching function is a bit weaker than that in CVS.\nThere is a good book on Subversion and development is ongoing:\nBen Collins-Sussman, Brian W. Fitzpatrick, and C. Michael Pilato, Version Control\nwith Subversion  (Apache Software Foundation, 2002), http:/ /svnbook.red-bean.com\n17.2 Timer routines for tracking code performance\nIt is helpful to put internal timers into your application to track performance as you\nwork on it. We show a representative timing routine in listings 17.1 and 17.2 that you\ncan use in C, C++, and Fortran with a Fortran wrapper routine. This routine uses the\nclock_gettime  routine with a CLOCK_MONOTONIC  type to avoid problems with clock\ntime adjustments.\n584 CHAPTER  17 Tools and resources for better code\ntimer.h\n1 #ifndef TIMER_H\n2 #define TIMER_H\n3 #include <time.h>\n4 \n5 void cpu_timer_start1(struct timespec *tstart_cpu);\n6 double cpu_timer_stop1(struct timespec tstart_cpu);\n7 #endif\ntimer.c\n 1 #include <time.h>\n 2 #include ""timer.h""\n 3 \n 4 void cpu_timer_start1(struct timespec *tstart_cpu)\n 5 {\n 6    clock_gettime(CLOCK_MONOTONIC, tstart_cpu);     \n 7 }\n 8 double cpu_timer_stop1(struct timespec tstart_cpu)\n 9 {\n10    struct timespec tstop_cpu, tresult;\n11    clock_gettime(CLOCK_MONOTONIC, &tstop_cpu);     \n12    tresult.tv_sec = tstop_cpu.tv_sec - tstart_cpu.tv_sec;\n13    tresult.tv_nsec = tstop_cpu.tv_nsec - tstart_cpu.tv_nsec;\n14    double result = (double)tresult.tv_sec +\n                      (double)tresult.tv_nsec*1.0e-9;\n15 \n16    return(result);\n17 }\nThere are other timer implementations that you can use if you need an alternative\nroutine. Portability is one reason you may want another implementation. The clock\n_gettime  routine has been supported on the macOS since Sierra 10.12, which has\nhelped with some of the portability issues. \nALTERNATIVE  TIMER  IMPLEMENTATIONS\nIf you are using C++ with 2011 standards, you can use the high resolution clock,\nstd::chrono::high_resolution_clock . Here we show a list of alternative timers you\ncan use with portability across C, C++, and Fortran.\nclock_gettime  with the CLOCK_MONOTONIC  type\nclock_gettime  with the CLOCK_REALTIME  type\ngettimeofday\ngetrusage\nhost_get_clock_service  for MacOS\nclock  std:chrono_high_resolution_clock  (C++ high resolution, C++ 2011\nstandard)Listing 17.1 Timer header file\nListing 17.2 Timer source file\nCalls clock_gettime \nrequesting a \nmonotonic clock",4634
217-17.3.3 Medium-level profilers to guide your application development.pdf,217-17.3.3 Medium-level profilers to guide your application development,"585 Profilers: You can’t improve what you don’t measure\nThe clock_gettime  function has two versions. Although the CLOCK_MONOTONIC  is pre-\nferred, it’s not a required type for Portable Operating System Interface (POSIX), the\nstandard for portability across operating systems. In the timers directory of the exam-\nples that accompany this chapter, we include a version with the CLOCK_REALTIME  timer\ntype. The gettimeofday  and getrusage  functions are widely portable and might work\non systems where clock_gettime  does not.\n17.3 Profilers: You can’t improve what you don’t measure\nA profiler is a programmer tool that measures some aspect of the performance of an\napplication. We covered profiling earlier in sections 2.2 and 3.3 as a key part of the\napplication development process and introduced a couple of the simpler profiling\ntools. In this section, we’ll cover some of the alternative profiling tools that you might\nconsider for your application development and introduce you to how to use some\nmore of the simpler profilers. Profilers are important tools in developing parallel\napplications when:\nYou want to work on a section of code that has the most impact in improving\nthe performance of your application. This section of code is often referred to as\nthe bottleneck .\nYou want to measure your performance improvement on various architec-\ntures. After all, we are all about performance in high performance computing\napplications.\nProfilers come in a variety of shapes and sizes. We’ll break down our discussion into\ncategories that reflect their broad characteristics. It is important to use a tool from the\nappropriate category. It is not advisable to use a heavy-weight profiling tool when all\nyou want is to find the biggest bottleneck. The wrong tool will bury you in an ava-\nlanche of information that will leave you digging yourself out for hours or days. Save the\nheavy-weight tools when you really need to dive down into the low-level details of your\napplication. We suggest starting with simple profilers and working up to the detailedExample: Experiment with timers\nUse the code at https:/ /github.com/EssentialsofParallelComputing/Chapter17  in\nthe timers directory. In that directory, run the following commands:\nmkdir build && cd build\ncmake ..\nmake\n./runit.sh\nThis example builds the various timer implementations and runs them. This example\ngives you some ideas for alternate options if the default version does not work or\ndoesn’t behave well on your system. \n586 CHAPTER  17 Tools and resources for better code\nprofilers when needed. Our categories of profilers follow this simple-to-complex hierar-\nchy with some subjective judgment where each profiling tool falls in the list.\nHigh-level is not indicative of high-detail; these tools give the 25,000 ft picture of an\napplication’s performance. We find ourselves returning to the simpler profiling tools,\nsuch as the simple text-based and high-level profilers because these are quick to use\nand don’t take up most of the day.\n17.3.1 Simple text-based profilers for everyday use\nSimple text-based profilers like LIKWID, gprof, gperftools, timemory, and Open|Speed-\nShop are easy to incorporate into your daily application development workflow. These\nprovide a quick insight on performance.\n The likwid (Like I Knew What I’m Doing) suite of tools was first introduced in sec-\ntion 3.3.1 and also used in chapters 4, 6, and 9. We used it extensively because of its\nsimplicity. There is ample documentation at the likwid website: \nlikwid performance tools at https:/ /hpc.fau.de/research/tools/likwid/\nThe venerable gprof tool has been a mainstay for profiling applications on Linux for\nmany years. We used it in section 13.4.2 for a quick profile of our application. Gprof\nuses a sampling approach to measure where the application is spending its time. It is a\ncommand-line tool that is enabled by adding -pg when compiling and linking your\napplication. Then, when your application runs, it produces a file called gmon.out at\ncompletion. The command-line gprof utility then displays the performance data as\ntext output. Gprof comes with most Linux systems and is part of the GCC and\nClang/LLVM compilers. Gprof is relatively dated, but is readily available and simple\nto use. The gprof documentation is fairly simple and is widely available at the follow-\ning site:\nGNU Binutils documentation from The Free Software Foundation at https:/ /\nsourceware.org/binutils/docs/gprof/index.html\nThe gperftools suite (originally Google Performance Tools) is a newer profiling tool\nsimilar in functionality to gprof. The suite of tools also comes with TCMalloc, a fast\nmalloc for applications that use threads. It also throws in a memory leak detector andTable 17.2 Categories of profiling tools (simple to complex)\nSimple text-based profilers Returns a short text-based summary of performance\nHigh-level profilers A top-down profiler that highlights the routines needing improvement, \noften in a graphical user interface\nMedium-level profilers Profilers that give a manageable amount of performance data\nDetailed profilers Bury me with data, please\n587 Profilers: You can’t improve what you don’t measure\na heap profiler. The gperftools CPU profiler has a website that has a short introduc-\ntion to the tool:\nGperftools (Google) at https:/ /gperftools.github.io/gperftools/cpuprofile.html\nThe timemory tool from the National Energy Research Scientific Computing Center\n(NERSC) is a simple tool built on top of many other performance measurement\ninterfaces. The simplest tool in this suite, timem, is a replacement for the Linux\ntime  command, which can also output additional information such as the memory\nused and the number of bytes read and written. Notably, it has an option to auto-\nmatically generate a roofline plot. The tool has extensive use information at its doc-\numentation website:\ntimemory documentation at https:/ /timemory.readthedocs.io  \nOpen|SpeedShop has a command-line option and Python interface that might make\nit a possible substitute for these simple tools. It is a more powerful tool, which we’ll\ndiscuss in section 17.3.4.\n17.3.2 High-level profilers for quickly identifying bottlenecks\nHigh-level tools are the best choice for a quick overview of the performance of your\napplication. These tools distinguish themselves by focusing on identifying the high-\ncost parts of your code and giving a robust graphics-based overview of application per-\nformance. Unlike the simple profilers, you must often step out of your workflow and\nstart a graphics application in order to use these high-level profilers.\n We first talked about Cachegrind in section 3.3.1. Cachegrind specializes in show-\ning you the high-cost paths through your code, enabling you to focus on the perfor-\nmance critical parts. It has a simple graphical user interface that is easy to understand.\nCachegrind, a cache and branch-prediction profiler (Valgrind™ Developers) at\nhttps:/ /valgrind.org/docs/manual/cg-manual.html  \nAnother good high-level profiler is the Arm MAP profiler, previously named Allinia\nMap or Forge Map. MAP is a commercial tool and its parent firm has changed a few\ntimes. It utilizes a graphic user interface that gives more detail than KCachegrind, but\nstill focuses on the most salient details. The MAP tool has a companion tool, the DDT\ndebugger, that comes in the Arm Forge suite of high-performance computing tools.\nWe’ll discuss the DDT debugger later in the chapter in section 17.7.2. There is exten-\nsive documentation, tutorials, webinars, and a user guide at the ARM website:\nArm MAP (Arm Forge) at http:/ /mng.bz/n2x2\n588 CHAPTER  17 Tools and resources for better code\n17.3.3 Medium-level profilers to guide your application development\nMedium-level profilers are often used when trying to fine-tune optimizations. Many of\nthe graphical user interface tools designed to guide your application development fall\ninto this category. These include Intel® Advisor, VTune, CrayPat, AMD µProf, NVIDIA\nVisual Profiler, and CodeXL (formerly a Radeon tool and now part of the GPUOpen\ninitiative). We start with the more general and popular tools for CPUs and then work\ninto the specialized tools for GPUs.\n Intel® Advisor is targeted at guiding the use of vectorization with Intel compilers. It\nshows which loops are vectorized and suggests changes to vectorize others. While it is\nespecially useful for vectorizing code, it is also good for general profiling. Advisor is a\nproprietary tool, but recently has been made freely available for many users. You can\ninstall Intel Advisor using the Ubuntu package manager. You need to add the Intel\npackage and then use apt-get  to install the version with OneAPI.\nwget -q https:/  /apt.repos.intel.com/intel-gpg-keys/GPG-PUB-KEY-INTEL-SW-\nPRODUCTS-2023.PUB\napt-key add GPG-PUB-KEY-INTEL-SW-PRODUCTS-2023.PUB\nrm -f GPG-PUB-KEY-INTEL-SW-PRODUCTS-2023.PUB\necho ""deb https:/  /apt.repos.intel.com/oneapi all main"" >>\n    /etc/apt/sources.list.d/oneAPI.list\necho ""deb [trusted=yes arch=amd64] \nhttps:/  /repositories.intel.com/graphics/ubuntu bionic\n    main"" >> /etc/apt/sources.list.d/intel-graphics.list\napt-get update\napt-get install intel-oneapi-advisor\nComplete instructions on installing the Intel OneAPI software from its package repos-\nitory can be found at http:/ /mng.bz/veO4 .\n Intel® VTune is a general-purpose optimization tool that helps to identify bottle-\nnecks and potential improvements. It is also another proprietary tool that’s freely\navailable. VTune can be installed with apt-get  from the OneAPI suite.\nwget -q https:/  /apt.repos.intel.com/intel-gpg-keys/GPG-PUB-KEY-INTEL-SW-\nPRODUCTS-2023.PUB\napt-key add GPG-PUB-KEY-INTEL-SW-PRODUCTS-2023.PUB\nrm -f GPG-PUB-KEY-INTEL-SW-PRODUCTS-2023.PUB\necho ""deb https:/  /apt.repos.intel.com/oneapi all main"" >>\n    /etc/apt/sources.list.d/oneAPI.list\necho ""deb [trusted=yes arch=amd64] \nhttps:/  /repositories.intel.com/graphics/ubuntu bionic\n    main"" >> /etc/apt/sources.list.d/intel-graphics.list\napt-get update\napt-get install intel-oneapi-vtune\nThe CrayPat tool is a proprietary tool that is only available on Cray Operating Systems.\nIt is an excellent command-line tool that gives simple feedback on optimization of\nloops and threading. If you are working on one of the many high-performance\n589 Profilers: You can’t improve what you don’t measure\ncomputing sites that use the Cray Operating System, this tool may be worth investigat-\ning. Unfortunately, it is not available elsewhere.\n AMD µProf is the profiling tool from AMD for their CPUs and APUs. Accelerated\nProcessing Unit (APU) is the AMD term for a CPU with an integrated GPU that was\nfirst introduced when AMD bought out ATI, manufacturer of the Radeon GPU. The\nintegrated unit is more tightly coupled than a typical integrated GPU and is part of the\nHeterogeneous System Architecture concept from AMD. You can install the AMD µProf\ntool with package installers on Ubuntu or Red Hat Enterprise Linux. The download\nrequires a manual acceptance of the EULA. To install AMD µProf, follow these steps:\n1Go to https:/ /developer.amd.com/amd-uprof/  \n2Scroll down to the bottom of the page and select the appropriate file \n3Accept the EULA to start the download with the package manager\nUbuntu: dpkg --install amduprof_x.y-z_amd64.deb\nRHEL: yum install amduprof-x.y-z.x86_64.rpm \nMore details on installation are given in the user guide, which is available at the AMD\ndeveloper website: https:/ /developer.amd.com/wordpress/media/2013/12/User_\nGuide.pdf .\n NVIDIA Visual Profiler is part of the CUDA software suite. It is being incorporated\ninto the NVIDIA® Nsight suite of tools. We covered this tool in section 13.4.3. The\nNVIDIA tools can be installed on the Ubuntu Linux distribution with the following\ncommands:\nwget -q https:/  /developer.download.nvidia.com/\n   compute/cuda/repos/ubuntu1804/x86_64/cuda-repo-ubuntu1804_10.2.89-\n1_amd64.deb\ndpkg -i cuda-repo-ubuntu1804_10.2.89-1_amd64.deb\napt-key adv --fetch-keys https:/  /developer.download.nvidia.com/ \n   compute/cuda/repos/ubuntu1804/x86_64/7fa2af80.pub\napt-get update\napt-get install cuda-nvprof-10-2 cuda-nsight-systems-10-2 cuda-nsight-\ncompute-10-2\nCodeXL is the GPUOpen code development workbench with profiling support for\nRadeon GPUs. It is part of the GPUOpen open source initiative begun by AMD. The\nCodeXL tool combines both debugger and profiler functionality. The CPU profiling\nhas been moved to the AMD µProf tool so that the CodeXL tool could be moved to an\nopen source status. Follow the instructions to install CodeXL on Ubuntu or RedHat\nLinux distributions.\nwget https:/  /github.com/GPUOpen-Archive/  \n          CodeXL/releases/download/v2.6/codexl-2.6-302.x86_64.rpm\nRHEL or CentOS: rpm -Uvh --nodeps codexl-2.6-302.x86-64.rpm\nUbuntu: apt-get install rpm\n        rpm -Uvh --nodeps codexl-2.6-302.x86-64.rpm",13033
218-17.4 Benchmarks and mini-apps A window into system performance.pdf,218-17.4 Benchmarks and mini-apps A window into system performance,"590 CHAPTER  17 Tools and resources for better code\n17.3.4 Detailed profilers give the gory details of hardware performance\nThere are several tools that produce detailed application profiling. If you need to\nextract every bit of performance from your application, you should learn to use at\nleast one of these tools. The challenge with these tools is that they produce so much\ninformation, it can be time-consuming to understand and use the results. You will also\nneed to have some hardware architecture expertise to really make sense of the profil-\ning data. The tools from this category should be used after you have gotten what you\ncan out of the simpler profiling tools. The detailed profilers that we cover in this sec-\ntion are HPCToolkit, Open|SpeedShop, and TAU. \n HPCToolkit is a powerful, detailed profiler developed as an open source project by\nRice University. HPCToolkit uses hardware performance counters to measure perfor-\nmance and presents the data using graphical user interfaces. The development for\nextreme scale on the latest high-performance computing systems is sponsored by the\nDepartment of Energy (DOE) Exascale Computing Project. Its hpcviewer GUI shows\nperformance data from a code perspective while the hpctraceviewer presents a time\ntrace of the code execution. More information and detailed user guides are available\nat the HPCToolkit website. HPCToolkit can be installed with the Spack package man-\nager with spack  install  hpctoolkit .\nHPCToolkit at http:/ /hpctoolkit.org\nOpen|SpeedShop is another profiler that can produce detailed program profiles. It\nhas both a graphical user interface and a command-line interface. The Open|Speed-\nShop tool runs on all the latest high-performance computing systems as a result of\nDOE funding. It has support for MPI, OpenMP, and CUDA. Open|Speedshop is open\nsource and can be freely downloaded. Their website has detailed user guides and tuto-\nrials. Open|Speedshop can be installed with the Spack package manager with spack\ninstall  openspeedshop .\nOpen|Speedshop at https:/ /openspeedshop.org\nTAU is a profiling tool developed primarily at the University of Oregon. This freely\navailable tool has a graphical user interface that is easy to use. TAU is used on many of\nthe largest high-performance computing applications and systems. There is extensive\ndocumentation on using TAU at the tool’s website. TAU can be installed with the\nSpack package manager with spack  install  tau.\nPerformance Research Lab (University of Oregon) at http:/ /www.cs.uoregon.edu/\nresearch/tau/home.php",2590
219-17.4.1 Benchmarks measure system performance characteristics.pdf,219-17.4.1 Benchmarks measure system performance characteristics,,0
220-17.4.2 Mini-apps give the application perspective.pdf,220-17.4.2 Mini-apps give the application perspective,"591 Benchmarks and mini-apps: A window into system performance\n17.4 Benchmarks and mini-apps: A window into \nsystem performance\nWe noted the value of benchmarks and mini-apps for assessing the performance of\nyour applications in chapter 3. Benchmarks are more appropriate for measuring the\nperformance of a system. Mini-apps are more focused on application areas and how\nbest to implement the algorithms for various architectures, but the difference\nbetween these can be blurred at times.\n17.4.1 Benchmarks measure system performance characteristics\nThe following is a list of benchmarks that can be useful measures for your potential\nsystem performance. We have extensively used the STREAM Benchmark in our per-\nformance studies, but there may be more appropriate benchmarks for your applica-\ntion. For example, if your application loads a single data value from scattered memory\nlocations, the Random benchmark would be the most appropriate.\nLinpack at http:/ /www.netlib.org/benchmark/hpl/ —Used for the Top 500\nHigh Performance Computers list.\nSTREAM at https:/ /www.cs.virginia.edu/stream/ref.html —A benchmark for\nmemory bandwidth. You can find a version in the Git repository at https:/ /\ngithub.com/jeffhammond/STREAM.git .\nRandom at http:/ /icl.cs.utk.edu/projectsfiles/hpcc/RandomAccess/ —A bench-\nmark for random memory access performance.\nNAS Parallel Benchmarks at http:/ /www.nas.nasa.gov/publications/npb.html —\nNASA benchmarks, first released in 1991, include some of the most heavily\nused benchmarks for research.\nHPCG at http:/ /www.hpcg-benchmark.org/software/ —New conjugate gradient\nbenchmark developed as an alternative to Linpack. HPCG gives a more realistic\nperformance benchmark for current algorithms and computers.\nHPC Challenge Benchmark at http:/ /icl.cs.utk.edu/hpcc/ —A composite\nbenchmark.\nParallel Research Kernels at https:/ /github.com/ParRes/Kernels —Various small\nkernels from typical scientific simulation codes and in several parallel imple-\nmentations.\n17.4.2 Mini-apps give the application perspective\nApplications have had to make many adaptations for new architectures. With the use\nof mini-apps, you can highlight the performance of a simple application type on a tar-\nget system. This section presents a list of mini-apps developed by the Department of\nEnergy (DOE) laboratories, which might be a valuable reference implementation for\nyour application. \n The DOE laboratories have been tasked with the development of exascale computers,\nwhich provide the leading edge of high-performance computing. These laboratories\n592 CHAPTER  17 Tools and resources for better code\nhave created mini-apps and proxy applications for hardware designers and applica-\ntion developers to experiment with how to get the most out of these exascale systems.\nEach of these mini-apps has a different purpose. Some reflect the performance of a\nlarge application, while others are meant for algorithmic exploration. To begin, let’s\ndefine a couple of terms to help us categorize the mini-apps.\nProxy mini-app —An extract or smaller form of a larger application that captures\nits performance characteristics. Proxies are useful to hardware vendors in a co-\ndesign process as a smaller application that they can use in the hardware design\nprocess. \nResearch mini-app —A simpler form of a computational approach that is useful\nfor researchers to explore alternative algorithms and methods for improved\nperformance and new architectures.\nThe categorization of mini-apps is not perfect. Each author of a mini-app has their\nown reason for their creation, which often doesn’t fit into neat categories.\nEXASCALE  PROJECT  PROXY  APPS: A CROSS -SECTION  OF SAMPLE  APPLICATIONS\nThe DOE has developed some sample applications for use in benchmarking systems,\nperformance experiments, and algorithm development. Many of these have been\norganized by the DOE Exascale Computing Project at https:/ /proxyapps.exascaleproj\nect.org/ .\nAMG —Algebraic multi-grid example\nExaMiniMD —Proxy application for particle and molecular dynamics codes\nLaghos —Unstructured, compressible shock hydrodynamics\nMACSio —Scalable I/O tests\nminiAMR —Block-based adaptive mesh refinement mini-app\nminiQMC —Quantum Monte Carlo mini-app\nNEKbone —Incompressible Navier-Stokes solver using spectral elements\nPICSARlite —Electromagnetic particle-in-cell\nSW4lite —3D seismic modeling kernels\nSWFFT —Fast Fourier transform\nThornado-mini —Finite element, moment-based radiation transport\nXSBench —Kernel from a Monte Carlo neutronics app\nThe Exascale Project proxy applications are selected from the many proxy applica-\ntions developed by national laboratories. In the following sections, we list other proxy\nand mini-applications developed by various national laboratories for scientific applica-\ntions that are important to the mission for their research laboratory. These applica-\ntions are made available to the public and hardware developers as part of the national\ncodesign strategy. The codesign process  is where hardware developers and application\ndevelopers work closely together in a feedback loop that iterates the features of these\nexascale systems. \n593 Benchmarks and mini-apps: A window into system performance\n Often, the applications that these mini-apps mirror tend to be proprietary and,\ntherefore, cannot be shared outside the corresponding laboratory. With the release of\nsome of these mini-apps, we recognize that current applications are more complex\nand stress the hardware in different ways than the simple kernels previously available.\nLAWRENCE  LIVERMORE  NATIONAL  LABORATORY  PROXIES\nLawrence Livermore National Laboratory has been one of the leading proponents of\nproxy development. Their LULESH proxy is one of the most heavily studied by ven-\ndors and academic researchers. Some of the Lawrence Livermore National Labora-\ntory proxies include\nLULESH—Explicit Lagrangian shock hydrodynamics on an unstructured mesh\nrepresentation\nKripke—Sweep-based deterministic transport\nQuicksilver—Monte Carlo particle transport\nFor more detail on the Lawrence Livermore National Laboratory proxies, see their\nwebsite at https:/ /computing.llnl.gov/projects/co-design/proxy-apps .\nLOS ALAMOS  NATIONAL  LABORATORY  PROXY  APPLICATIONS\nLos Alamos National Laboratory also has many interesting proxy applications. Some\nof the more popular are listed here.\nCLAMR —Cell-based adaptive mesh refinement mini-app\nNuT —Monte Carlo proxy for neutrino transport\nPennant —Unstructured mesh hydrodynamics mini-app\nSNAP —SN (Discrete Ordinates) application proxy\nFor more detail on the Los Alamos National Laboratory proxies, see their website at\nhttps:/ /www.lanl.gov/projects/codesign/proxy-apps/lanl/index.php .\nSANDIA  NATIONAL  LABORATORIES  MANTEVO  SUITE OF MINI-APPS\nSandia National Laboratories has put together a branded mini-app suite called\nMantevo, which includes their mini-apps and a few from other organizations such\nas the United Kingdom’s Atomic Weapons Establishment (AWE). Here is list of their\nmini-apps:\nCloverLeaf —Cartesian grid compressible fluids hydrocode mini-app\nCoMD —Molecular dynamics mini-app\nEpetraBenchmarkTest —Dense math-solver kernels\nMiniAero —Unstructured compressible Navier-Stokes\nminiFE —Proxy application for unstructured implicit finite element codes\nminiGhost —Proxy application for ghost cell updates\nminiSMAC2D —Body-fitted incompressible Navier-Stokes solver\nminiXyce —Circuit simulation mini-app\nTeaLeaf —Proxy application for unstructured implicit finite element codes",7657
221-17.5 Detecting and fixing memory errors for a robust application.pdf,221-17.5 Detecting and fixing memory errors for a robust application,,0
222-17.5.2 Dr. Memory for your memory ailments.pdf,222-17.5.2 Dr. Memory for your memory ailments,"594 CHAPTER  17 Tools and resources for better code\nMore information on the Mantevo min-app suite is available at https:/ /mantevo\n.github.io .\n17.5 Detecting (and fixing) memory errors \nfor a robust application\nFor robust applications, you need a tool to detect and report memory errors. In this\nsection, we discuss the capabilities and the pros and cons of a number of tools that\ndetect and report memory errors. The memory errors that occur in applications can\nbe broken down into these categories:\nOut-of-bound errors —Attempting to access memory beyond the array bounds.\nFence-post checkers and some compilers can catch these errors.\nMemory leaks —Allocating memory and never freeing it. Malloc replacement\ntools are good at catching and reporting memory leaks.\nUninitialized memory —Memory that is used before it is set. Because memory is\nnot set before its use, it has whatever value is in memory from previous use. The\nresult is that the behavior of the application can vary from run to run. This type\nof error is difficult to find, and tools specifically designed to catch these are\nessential.\nOnly a few tools handle all of these categories of memory errors. Most of the tools\nhandle the first two categories to some degree. Uninitialized memory checks are an\nimportant check and supported by just a few tools. We’ll cover those tools first.\n17.5.1 Valgrind Memcheck: The open source standby\nValgrind checks uninitialized memory with its default Memcheck tool. We first pre-\ns e n t e d  V a l g r i n d  f o r  t h i s  p u r p o s e  i n  s e c t i o n  2 . 1 . 3 .  V a l g r i n d  i s  a  g o o d  c h o i c e  b o t h\nbecause it is open source and freely available and because it is one of the best tools at\ndetecting memory errors in all three categories.\n It’s best to use Valgrind with the GCC compiler. The GCC team uses it for their\ndevelopment, and as a result cleaned up their generated code so that a suppression\nfile for false positives is not needed for their serial applications. For parallel applica-\ntions, you can also suppress the false positives detected by Valgrind with OpenMPI by\nusing a suppression file provided by the OpenMPI package. For example\nmpirun -n 4 valgrind \\n   --suppressions=$MPI_DIR/share/openmpi/openmpi-valgrind.supp <my_app>\nThere are only a few command-line options, and the Valgrind tool often suggests\nwhich options to use in its report. For more information on the usage, see the Val-\ngrind website ( https:/ /valgrind.org ).\n595 Detecting (and fixing) memory errors for a robust application\n17.5.2 Dr. Memory for your memory ailments\nYes, really, that’s its name. Dr. Memory is a similar tool to Valgrind but newer and\nfaster. Like Valgrind, Dr. Memory detects memory errors and problems within your\nprogram. It is an open source project, freely available across a variety of chip architec-\ntures and operating systems. \n There are many other tools besides Dr. Memory in this suite of run-time tools.\nBecause Dr. Memory is a relatively simple tool, we’ll present a quick example of how\nto use it. Let’s first set up Dr. Memory for use.\nWe’ll try out Dr. Memory on the example in the repository at https:/ /github.com/\nEssentialsofParallelComputing/Chapter17 . The following listing is a copy of the code\nfrom listing 4.1 of chapter 4. The code is just a fragment to check that the syntax cor-\nrectly compiles. \nDrMemory/memoryexample.c\n 1 #include <stdlib.h>\n 2 \n 3 int main(int argc, char *argv[])\n 4 {\n 5   int j, imax, jmax;\n 6 \n 7   // first allocate a column of pointers of type pointer to double\n 8   double **x = (double **)              \n        malloc(jmax * sizeof(double *));   \n 9 \n10   // now allocate each row of data\n11   for (j=0; j<jmax; j++){             \n12      x[j] = (double *)malloc(imax * sizeof(double));\n13   }\n14 }\nRunning this example takes just a few commands. Retrieve the code from the supple-\nmental examples for the chapter and build it:\n Example: Using Dr. Memory to detect memory errors\nGo to https:/ /github.com/DynamoRIO/drmemory/wiki/Downloads  and download the\nlatest version for Linux:\ntar -xzvf DrMemory-Linux-2.3.0-1.tar.gz\nThen add it to your path with\nexport PATH=${HOME}/DrMemory-Linux-2.3.0-1/bin64:$PATH\nListing 17.3 DrMemory test example\nUninitialized memory \nread of variable jmax\nMemory leak for variable x\n596 CHAPTER  17 Tools and resources for better code\ngit clone --recursive \\n    https:/  /github.com/EssentialsofParallelComputing/Chapter17\ncd DrMemory\nmake\nNow run the example by executing drmemory , followed by two dashes and then the\nname of the executable: drmemory  -- memoryexample . Figure 17.2 shows the report\nthat Dr. Memory produces.\nDr. Memory correctly flags that jmax  was not initialized when used on line 11. It also\nshows a leak on line 12. To fix these, we initialize jmax  and then free each x[j]\npointer and the x array, then try again with drmemory  -- memoryexample . Figure 17.3\nshows the report.~~Dr.M~~ Dr. Memory version 2.3.0\n~~Dr.M~~\n~~Dr.M~~                               reading register edx Error #1: UNINITIALIZED READ:\n~~Dr.M~~ # 0 main [C apterl7/DrMemory/memoryexample.c:l1] h\n~~Dr.M~~ Note: @0:00:00.401 in thread 146899\n~~Dr.M~~ Note: instruction: cmp  %eax %edx\n~~Dr.M~~\n~~Dr.M~~                               0x0000000000607710-0x0000000000607718 + 33567080 indirect bytes Error #2: LEAK 8 direct bytes\n~~Dr.M~~ # 0 replace_malloc   [/drmemory_package/common/alloc_replace.c:2577]\n~~Dr.M~~ # 1 main [C apter17/DrMemory/memoryexample.c:8] h\n~~Dr.M~~\n~~Dr.M~~ ERRORS FOUND:\n~~Dr.M~~       0 unique,     0 total unaddressable access(es)\n~~Dr.M~~ 1 unique,     2 total uninitialized access(es)\n~~Dr.M~~       0 unique,     0 total invalid heap argument(s)\n~~Dr.M~~       0 unique,     0 total warning(s)\n~~Dr.M~~ 1 unique,     1 total, 33567088 byte(s) of leak(s)\n~~Dr.M~~       0 unique,     0 total,      0 byte(s) of possible leak(s)\n~~Dr.M~~ ERRORS IGNORED:\n~~Dr.M~~      24 unique,    46 total,  14053 byte(s) of still-reachable allocation(s)\n~~Dr.M~~          (re-run with ""-show_reachable"" for details)\n~~Dr.M~~ Details:\nChapter17/DrMemory/DrMemory-Linux-2.3.0-1/drmemory/logs/DrMemory-memoryexample.146899.000/results.txtUninitialized memory report\nMemory leak report\nFigure 17.2 Report from Dr. Memory shows that an uninitialized read at line 11 and a memory leak for memory \nallocated at line 8.\n~~Dr.M~~ Dr. Memory version 2.3.0\n~~Dr.M~~\n~~Dr.M~~ NO ERRORS FOUND:\n~~Dr.M~~ 0 unique, 0 total unaddressable access(es)\n~~Dr.M~~ 0 unique, 0 total uninitialized access(es)\n~~Dr.M~~ 0 unique, 0 total invalid heap argument(s)\n~~Dr.M~~ 0 unique, 0 total warning(s)\n~~Dr.M~~ 0 unique, 0 total, 0 byte(s) of leak(s)\n~~Dr.M~~ 0 unique, 0 total, 0 byte(s) of possible leak(s)\n~~Dr.M~~ ERRORS IGNORED:\n~~Dr.M~~ 24 unique, 46 total,  14053 byte(s) of still-reachable allocation(s)\n~~Dr.M~~ (re-run with ""-show_reachable"" for details)\n~~Dr.M~~ Details:\nChapter17/DrMemory/DrMemory-Linux-2.3.0-1/drmemory/logs/DrMemory-memoryexample.147746.000/results.txtNo errors reported!\nFigure 17.3 This Dr. Memory report shows that the uninitialized memory error and the leak are fixed.",7255
223-17.5.3 Commercial memory tools for demanding applications.pdf,223-17.5.3 Commercial memory tools for demanding applications,,0
224-17.5.4 Compiler-based memory tools for convenience.pdf,224-17.5.4 Compiler-based memory tools for convenience,,0
225-17.5.5 Fence-post checkers detect out-of-bounds memory accesses.pdf,225-17.5.5 Fence-post checkers detect out-of-bounds memory accesses,"597 Detecting (and fixing) memory errors for a robust application\nThe report from Dr. Memory in figure 17.3 shows no errors after our fix. Note that Dr.\nMemory does not flag that imax  is uninitialized. For more information on Dr. Memory\nfor Windows, Linux, and Mac, see https:/ /drmemory.org .\n17.5.3 Commercial memory tools for demanding applications\nPurify and Insure++ are commercial tools that detect memory errors, including some\nform of uninitialized memory check. TotalView includes a memory checker in its most\nrecent versions. If you have a demanding application that requires extreme quality\ncode, and you are looking for vendor support for your memory checking tool, one of\nthese commercial tools may be a good choice.\n17.5.4 Compiler-based memory tools for convenience\nMany compilers are incorporating memory tools into their products. The LLVM com-\npiler has a set of tools that includes memory checker functionality. This includes\nMemorySanitizer, AddressSanitizer, and ThreadSanitizer. GCC includes the mtrace\ncomponent which detects memory leaks.\n17.5.5 Fence-post checkers detect out-of-bounds memory accesses\nSeveral tools place blocks of memory before and after memory allocations to detect\nout-of-bounds memory accesses and to also track memory leaks. These types of mem-\nory checkers are referred to as fence-post memory checkers. These are fairly simple\ntools to implement and are usually provided as a library. Additionally, these tools are\nportable and easy to add to a regular regression testing system. \n Here we discuss dmalloc in detail and how to use a fence-post memory checker.\nElectric Fence and Memwatch are two other packages that provide fence-post memory\nchecks and have an analogous use model, but dmalloc is the best known fence-post\nmemory checker. It replaces the malloc library with a version that provides memory\nchecking. \nExample: Setting up dmalloc\nTo download and install dmalloc, run the following code:\nwget https:/  /dmalloc.com/releases/dmalloc-5.5.2.tgz\ntar -xzvf dmalloc-5.5.2.tgz\ncd dmalloc-5.5.2/\n./configure --prefix=${HOME}/dmalloc\nmake\nmake install\nTo add dmalloc to your executable path, on the command line or in your environment\nsetup file, set your PATH variable:\nexport PATH=${PATH}:${HOME}/dmalloc/bin\n598 CHAPTER  17 Tools and resources for better code\nFor our source code in the following listing, we added the dmalloc header file with an\ninclude  directive on line 3 so that we get line numbers in our report.\nDmalloc/mallocexample.c\n 1 #include <stdlib.h>\n 2 #ifdef DMALLOC\n 3 #include ""dmalloc.h""   \n 4 #endif\n 5 \n 6 int main(int argc, char *argv[])\n 7 {\n 8    int imax=10, jmax=12;\n 9 \n10    // first allocate a block of memory for the row pointers\n11    double *x = (double *)malloc(imax*sizeof(double *));\n12 \n13    // now initialize the x array to zero\n14    for (int i = 0; i < jmax; i++) {    \n15       x[i] = 0.0;                      \n16    }\n17    free(x);\n18    return(0);\n19 }\nWe’ve included an out-of-bounds access on the x array on lines 14 and 15. Now we can\nbuild our executable and run it:\nmake\n./mallocexample(continued)\nSet the DMALLOC_OPTIONS  variable. The backticks execute the command and set the\nvariable.\nexport `dmalloc -l logfile -i 100 low`\nThe DMALLOC_OPTIONS variable should now be set in your environment. It may look\nsomething like this:\nDMALLOC_OPTIONS=debug=0x4e48503,inter=100,log=logfile\nWe need to add these changes to the makefile to link in the dmalloc library and\ninclude the header file:\nCFLAGS = -g -std=c99 -I${HOME}/dmalloc/include -DDMALLOC \\n         -DDMALLOC_FUNC_CHECK\nLDLIBS=-L${HOME}/dmalloc/lib -ldmalloc\nListing 17.4 Dmalloc example code\nIncludes the \ndmalloc header file\nWrites past the \nend of the x array",3807
226-17.5.6 GPU memory tools for robust GPU applications.pdf,226-17.5.6 GPU memory tools for robust GPU applications,,0
227-17.6.1 Intel Inspector A race condition detection tool with a GUI.pdf,227-17.6.1 Intel Inspector A race condition detection tool with a GUI,"599 Thread checkers for detecting race conditions\nBut the output to the terminal reports a failure:\ndebug-malloc library: dumping program, fatal error\n   Error: failed OVER picket-fence magic-number check (err 27)\nAbort trap: 6\nLet’s get more information about the problem from the log file shown in figure 17.4.\nDmalloc has detected the out-of-bounds access. Great! You can find more information\non dmalloc at its website ( https:/ /dmalloc.com ).\n17.5.6 GPU memory tools for robust GPU applications\nGPU vendors are developing memory tools for detecting memory errors for applica-\ntions running on their hardware. NVIDIA has released a corresponding tool, and\nother GPU vendors are sure to follow. The NVIDIA CUDA-MEMCHECK tool checks\nfor out-of-bounds memory references, data race detections, synchronization usage\nerrors, and uninitialized memory. The tool can be run as a standalone command:\ncuda-memcheck [--tool memcheck|racecheck|initcheck|synccheck] <app_name> \nDocumentation on the tool usage is available on the NVIDIA website:\nCUDA-MEMCHECK, CUDA Toolkit Documentation at https:/ /docs.nvidia.com/\ncuda/cuda-memcheck/index.html  \n17.6 Thread checkers for detecting race conditions\nTools to detect thread race conditions (also called data hazards ) are critical in develop-\ning OpenMP applications. It is impossible to develop robust OpenMP applications\nwithout a race detection tool. Yet, there are few tools that can detect race conditions.\nTwo tools that are effective are Intel Inspector and Archer, which we discuss next.1595103932: 1: Dmalloc version '5.5.2' from 'http://dmalloc.com/'\n1595103932: 1: flags = 0x4e48503, logfile 'logfile'\n1595103932: 1: interval = 100, addr = 0, seen # = 0, limit = 0\n1595103932: 1: starting time = 1595103932\n1595103932: 1: process pid = 22944\n1595103932: 1:  error details: checking user pointer\n1595103932: 1: pointer '0xl0a4eaf88' from 'unknown' prev access 'mallocexample.c:11'\n1595103932: 1:  dump of proper fence-top bytes: 'i\336\312\372'\n1595103932: 1:  dump of '0xl0a4eaf88'+64:\n‘\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000'\n1595103932: 1: next pointer '0xl0a4eb000' (size 0) may have run under from 'unknown'\n1595103932: 1: ERROR: _dmalloc_chunk_heap_check: failed OVER picket-fence magic-number check (err\n27)Out-of-bounds\nmemory report\nFigure 17.4 The dmalloc log file shows an out-of-bounds memory access at line 11.",2452
228-17.6.2 Archer A text-based tool for detecting race conditions.pdf,228-17.6.2 Archer A text-based tool for detecting race conditions,"600 CHAPTER  17 Tools and resources for better code\n17.6.1 Intel® Inspector: A race condition detection tool with a GUI\nIntel® Inspector is a tool with a graphical user interface that is effective at detecting\nrace conditions in OpenMP code. We discussed Intel Inspector earlier in section 7.9.\nThough Inspector is an Intel proprietary tool, it is now freely available. On Ubuntu, it\ncan be installed from the OneAPI suite from Intel:\nwget -q https:/  /apt.repos.intel.com/intel-gpg-keys/GPG-PUB-KEY-INTEL-SW-\nPRODUCTS-2023.PUB\napt-key add GPG-PUB-KEY-INTEL-SW-PRODUCTS-2023.PUB\nrm -f GPG-PUB-KEY-INTEL-SW-PRODUCTS-2023.PUB\necho ""deb https:/  /apt.repos.intel.com/oneapi all main"" >>\n    /etc/apt/sources.list.d/oneAPI.list\necho ""deb [trusted=yes arch=amd64] \nhttps:/  /repositories.intel.com/graphics/ubuntu bionic\n    main"" >> /etc/apt/sources.list.d/intel-graphics.list\napt-get install intel-oneapi-inspector\n17.6.2 Archer: A text-based tool for detecting race conditions\nArcher is an open source tool built on LLVM’s ThreadSanitizer (TSan) and adapted\nfor detecting thread race conditions in OpenMP. Using the Archer tool is basically just\nreplacing the compiler command with clang-archer  and linking in the Archer\nlibrary with -larcher . Archer outputs its report as text. \n You can manually install Archer with the LLVM compiler, or install with the Spack\npackage manager using spack  install  archer . We have included some build scripts with\nthe accompanying examples at https:/ /github.com/EssentialsofParallelComputing/\nChapter17  for installation. Once the Archer tool is installed, you can build our exam-\nple in the Archer subdirectory of the examples. In the example, we use one of the\nstencil codes from section 7.3.3. We then modify the CMake build system by changing\nthe compiler command to clang-archer  and by adding the Archer libraries to the\nlink command as the following listing shows.\nArcher/CMakeLists.txt\n 1 cmake_minimum_required (VERSION 3.0)\n 2 project (stencil)\n 3 \n 4 set (CC clang-archer)    \n 5 \n 6 set (CMAKE_C_STANDARD 99)\n 7 \n 8 set(CMAKE_C_FLAGS ""${CMAKE_C_FLAGS} -g -O3"")\n 9 \n10 find_package(OpenMP)\n11 \n12 # Adds build target of stencil with source code files\n13 add_executable(stencil stencil.c timer.c timer.h malloc2D.c malloc2D.h)\n14 set_target_properties(stencil PROPERTIES \n     COMPILE_FLAGS ${OpenMP_C_FLAGS})Listing 17.5 Archer example code\nSets the compiler \ncommand to clang-archer\n601 Thread checkers for detecting race conditions\n15 set_target_properties(stencil PROPERTIES LINK_FLAGS ""${OpenMP_C_FLAGS}\n         -L${HOME}/archer/lib -larcher "")   \nCompile the code and run it is as before:\nmkdir build && cd build\ncmake ..\nmake\n./stencil\nWe get the Archer tool output mixed in with the normal output as figure 17.5 shows.\nThere are some reports of race conditions reported at startup that appear to be false\npositives, but no additional messages during the run. For more information, check\nout the following documentation:\n“Archer PRUNERS: Providing Reproducibility for Uncovering Non-deterministic\nErrors in Runs on Supercomputers” (2017), https:/ /pruners.github.io/archer/  \nArcher repository at https:/ /github.com/PRUNERS/archer  Adds the archer \nlibraries to LINK_FLAGS\nFigure 17.5 Output from the Archer data race detection tool",3354
229-17.7 Bug-busters Debuggers to exterminate those bugs.pdf,229-17.7 Bug-busters Debuggers to exterminate those bugs,,0
230-17.7.1 TotalView debugger is widely available at HPC sites.pdf,230-17.7.1 TotalView debugger is widely available at HPC sites,,0
231-17.7.3 Linux debuggers Free alternatives for your local development needs.pdf,231-17.7.3 Linux debuggers Free alternatives for your local development needs,"602 CHAPTER  17 Tools and resources for better code\n17.7 Bug-busters: Debuggers to exterminate those bugs\nYou spend much of your application development time fixing bugs. This is especially\ntrue in parallel application development. Any tool that helps with this process is vitally\nimportant. Parallel programmers also need additional capabilities targeted at dealing\nwith multiple processes and threads.\n The debuggers used for large parallel applications at high performance comput-\ning sites generally include a couple of commercial offerings. This includes the power-\nful and easy-to-use TotalView and Arm DDT debuggers. But most code development is\ninitially done on laptops, desktops, and local clusters outside of large centers, so you\nmay not have access to a commercial debugger on these smaller systems. The non-\ncommercial debuggers available for smaller clusters, desktops, and laptops are more\nlimited in parallel programming features and harder to use. In this section, we begin\nwith a discussion of the commercial debuggers.\n17.7.1 TotalView debugger is widely available at HPC sites\nTotalView has extensive support for leading high performance computing systems,\nincluding MPI and OpenMP threading. TotalView has some support for debugging\nNVIDIA GPUs using CUDA. It uses a graphical user interface and is easy to navigate; it\nalso has a great depth of features that take some exploration. TotalView is generally\ninvoked by prefixing the command line with totalview . The -a flag indicates that the\nrest of the arguments are to be passed to the application:\ntotalview mpirun -a -n 4 <my_application>\nLawrence Livermore National Laboratory has a good tutorial on Totalview. Detailed\ninformation is available at the TotalView websites:\nTotalView (Lawrence Livermore National Laboratory) at https:/ /computing\n.llnl.gov/tutorials/totalview/\nTotalView (Perforce) at https:/ /totalview.io\n17.7.2 DDT is another debugger widely available at HPC sites\nThe ARM DDT debugger is another popular commercial debugger used at high per-\nformance computing sites. It has extensive support for MPI and OpenMP. It also has\nsome support for debugging CUDA code. The DDT debugger uses a graphical user\ninterface that is very intuitive. In addition, DDT has support for remote debugging. In\nthis case, the graphical client interface is run on your local system, and the application\nthat is being debugged is remotely launched on the high performance computing sys-\ntem. To start a debug session with DDT, just prepend ddt to your command line:\nddt <my_application>\nThe Texas Advanced Computing Center has a good introduction to DDT. There is\nalso more information at the DDT websites:",2718
232-17.8 Profiling those file operations.pdf,232-17.8 Profiling those file operations,"603 Bug-busters: Debuggers to exterminate those bugs\nARM DDT Debugger tutorials (TACC, Texas Advanced Computing Center) at\nhttps:/ /portal.tacc.utexas.edu/tutorials/ddt\nARM DDT (ARM Forge) at https:/ /www.arm.com/products/development-tools/\nserver-and-hpc/forge/ddt\n17.7.3 Linux debuggers: Free alternatives for your local \ndevelopment needs\nThe standard Linux debugger, GDB, is ubiquitous on Linux platforms. Its command-\nline interface requires some work to learn. For a serial executable, GDB runs with\nthe command\ngdb <my_application>\nGDB does not have built-in parallel MPI support. You may be able to debug parallel\njobs by launching multiple GDB sessions with the mpirun  command. The xterms can-\nnot be launched in all environments, so this is not a fool-proof technique.\nmpirun -np 4 xterm -e gdb ./<my_application>\nMany higher-level user interfaces are built on top of GDB. The simplest of these is\ncgdb, which is a curses-based interface that has a strong similarity to the vi editor. The\ncurses interface is a character-based windows system. It has the advantage of better\nnetwork performance characteristics than a full-fledged, bit-mapped graphical user\ninterface. cgdb is widely available along with its documentation here:\ncgdb, the curses debugger, at https:/ /cgdb.github.io\nA full graphical user interface to GDB is available in the DataDisplayDebugger, known\nas DDD. The DDD debugger website gives more information on DDD and other simi-\nlar debuggers:\nDDD, the DataDisplayDebugger, at https:/ /www.gnu.org/software/ddd/\nNeither cgdb nor DDD includes explicit parallel support. Other higher-level user\ninterfaces such as the Eclipse IDE provide a parallel debugger interface on top of the\nGDB debugger. The Eclipse IDE is available for a wide range of languages and pro-\nvides the foundation for programming tools for CPUs and GPUs.\nDesktop IDEs (Eclipse Foundation) at https:/ /www.eclipse.org/ide/  \n17.7.4 GPU debuggers can help crush those GPU bugs\nThe availability of debuggers for the development of GPU code is a critical game\nchanger. The development of GPU code has been seriously hampered by the difficulty\nof debugging on GPUs. The GPU debugging tools discussed in this section are still\nimmature, but any capability is sorely needed. These GPU debuggers heavily leverage\nthe open source tools such as GDB and DDD introduced in the previous section.\n604 CHAPTER  17 Tools and resources for better code\nCUDA-GDB: A DEBUGGER  FOR THE NVIDIA GPU S\nCUDA has a command-line debugger based on GDB called CUDA-GDB. There is also\na version of CUDA-GDB with a graphical user interface in NVIDIA’s Nsight™ Eclipse\ntool as part of their CUDA toolkit. CUDA-GDB has also been integrated into DDD and\nEmacs. To use CUDA-GDB with DDD, launch DDD using ddd --debugger  cuda-gdb .\nYou’ll find the CUDA-GDB documentation at https:/ /docs.nvidia.com/cuda/cuda-gdb/ .\nROC GDB: A DEBUGGER  FOR THE RADEON  GPU S\nThe AMD ROCm debugger, part of the Radeon Open Compute initiative, is based on\nthe GDB debugger but with initial support for the AMD GPUs. The ROCm website has\ndocumentation on ROCgdb, but it is largely the same as the GDB debugger. \nThe site for the AMD ROCm debugger is at https:/ /rocmdocs.amd.com/en/\nlatest/ROCm_Tools/ROCgdb.html .\nThe ROCm website is https:/ /rocmdocs.amd.com .\nCheck for updates for the ROCm debugger in the ROCgdb User Guide at\nhttps:/ /github.com/RadeonOpenCompute/ROCm/blob/master/Debugging\n%20with%20ROCGDB%20User%20Guide%20v4.1.pdf .\n17.8 Profiling those file operations\nFilesystem performance is often an afterthought with high performance computing\napplication development. In today’s world of big data, and with filesystem perfor-\nmance lagging other parts of the computing system, filesystem performance is a grow-\ning issue. The necessary tools for measuring filesystem performance are scarce. The\nDarshan tool was developed to fill this gap. Darshan, an HPC I/O characterization\ntool, specializes at profiling an application’s use of the filesystem. Since its release,\nDarshan has achieved widespread use at high performance computing centers.\nExample: Installing the Darshan tool\nIn this example, you install the Darshan tool into your home directory. You may want\nto build the run-time tools on your compute cluster and the analysis tools on another\nsystem such as your laptop. The analysis tools require portions of a LaTex distribu-\ntion and some simple graphics utilities that might not be on the computer cluster. If\nyou run into problems with missing utilities, the DockerFile with accompanying exam-\nples at https://github.com/EssentialsofParallelComputing/Chapter17.git  lists all the\npackages necessary for running the analysis tools. To begin, download and unpack\nthe Darshan distribution:\nwget ftp:/  /ftp.mcs.anl.gov/pub/darshan/releases/darshan-3.2.1.tar.gz\ntar -xvf darshan-3.2.1.tar.gz\nLoad or install your MPI package. If it is not installed in the standard location, set the\npaths with the following export  commands:\nexport CFLAGS=-I<MPI_INCLUDE_PATH>\nexport LDFLAGS=-L<MPI_LIB_PATH>\n605 Profiling those file operations\nWe made these changes to the CMakeLists.txt file in the MPI_IO_Examples/mpi_io_\nblock2d directory at https:/ /github.com/EssentialsofParallelComputing/Chapter17.git .\nThis is the same MPI-IO example we presented in section 16.3 but with a larger\n1000x1000 mesh and with the verification code commented out. Now you can build\nand run the executable as before:\nmkdir build && cd build\ncmake ..\nmake\nmpirun -n 4 mpi_io_block2d\nYou should find the Darshan logs organized by date in your ~/darshan-logs sub-\ndirectories.Build the Darshan run-time tools:\ncd darshan-3.2.1/darshan-runtime\n./configure --prefix=${HOME}/darshan --with-log-path=${HOME}/darshan-logs\n            --with-jobid-env=SLURM_JOB_ID --enable-mpiio-mod\nmake\nmake install\nNow build the Darshan analysis tools:\ncd ../darshan-util\n./configure --prefix=${HOME}/darshan\nmake\nmake install\nThen set the path to the Darshan executables:\nexport PATH=${PATH}:${HOME}/darshan/bin\nRun the Darshan script to set up the date directories in the Darshan log directory:\ndarshan-mk-log-dirs.pl\nAdd the Darshan libraries to your build using your LINK_FLAGS . You can get the proper\nflags by executing the darshan-config  utility with the --dyn-ld-flags  option:\ndarshan-config --dyn-ld-flags\nIn the CMake build system, we can capture the output from the command and use it\nto set our DARSHAN_LINK_FLAGS  variable:\nexecute_process(COMMAND darshan-config --dyn-ld-flags\n           OUTPUT_STRIP_TRAILING_WHITESPACE\n           OUTPUT_VARIABLE DARSHAN_LINK_FLAGS)\nThen we add the DARSHAN_LINK_FLAGS  to the LINK_FLAGS  variable:\nset_target_properties(mpi_io_block2d PROPERTIES LINK_FLAGS\n          ""${MPI_C_LINK_FLAGS} ${DARSHAN_LINK_FLAGS}"")\n606 CHAPTER  17 Tools and resources for better code\nThe Darshan analysis tool outputs a few pages of text and graphics information on the\nfile operations in your application in portable document format (PDF). We show a\npart of the output in figure 17.6.Example: Using the Darshan analysis tool\nYou can run the Darshan analysis tool on the generated Darshan log file:\ndarshan-job-summary.pl <darshan log file>\nYou’ll find the output will have the same file name as the log file, but with a .pdf exten-\nsion added. You can look at the output with your favorite PDF viewer.\n020406080100\nPOSIX MPI-IOPercentage of run timeAverage I/O cost per process\nRead\nWrite\nMetadata\nOther (including application compute)–20–1001020304050\nRead Write Open Stat Seek Mmap FsyncOps (total, all processes)I/O operation counts\nPOSIX\nMPI-IO Indep.MPI-IO Coll.\n00.511.52\n0-100 101-1K 1K-10K 10K-100K 100K-1M 1M-4M 4M-10M 10M-100M 100M-1G 1G+Count (total, all processes)POSIX Access Sizes\nRead Write00.511.522.533.54\n0-100 101-1K 1K-10K 10K-100K 100K-1M 1M-4M 4M-10M 10M-100M 100M-1G 1G+Count (total, all processes)MPI-IO Access Sizes ‡\nRead Write\nFigure 17.6 The graphs are part of the output from the Darshan I/O characterization tool. Both the standard \nIO (POSIX) and MPI-IO are shown. From the graph on the upper right, we can confirm that MPI-IO used collective \nrather than independent operations.",8300
233-17.9 Package managers Your personal system administrator.pdf,233-17.9 Package managers Your personal system administrator,,0
234-17.9.2 Package managers for Windows.pdf,234-17.9.2 Package managers for Windows,"607 Package managers: Your personal system administrator\nWe built the run-time tool with support for both POSIX and MPI-IO profiling. POSIX,\nan acronym for Portable Operating System Interface, is the standard for portability for a\nwide range of system-level functions such as regular filesystem operations. For our mod-\nified test, we turned off all of the verification and other standard IO operations so that\nwe can focus on the MPI-IO parts of the code. We also made the arrays larger. This test\nwas done on the NFS filesystem that is used for our home directory. In the figure, we can\nsee that we did both an MPI-IO write and read and that the write is slightly slower than\nthe read. We can also see that the cost of the MPI metadata operations is much higher.\nThe writing of file metadata records information about where the file is located, its per-\nmissions, and its access times. By its nature, writing metadata is a serial operation. \n Darshan also has some support for profiling HDF5 file operations. You can get more\ninformation on the Darshan HPC I/O characterization tool at the project website:\nhttps:/ /www.mcs.anl.gov/research/projects/darshan/  \n17.9 Package managers: Your personal system \nadministrator\nPackage managers have become critical tools for simplifying software package installa-\ntion on a variety of systems. These tools first appeared on Linux systems with the Red\nHat package manager to manage software installation, but these have since become\nwidespread in many operating systems. Using package managers to install tools and\ndevice drivers can greatly simplify the installation process and keep your system more\nstable and up-to-date. \n Linux operating systems heavily rely on the use of package management. You should\nuse your Linux package system to install software whenever possible. Unfortunately, not\nall software packages, and particularly vendor device drivers, are set up for installing\nwith package managers. Without the use of a package manager, software installation is\nmore difficult and error-prone. Most high performance computing software packages\nfor Linux are distributed as Debian (.deb) or as Red Hat Package Manager (.rpm) pack-\nage formats. These package formats can be installed on most Linux distributions. \n17.9.1 Package managers for macOS\nFor the Mac operating system (macOS), the two major package managers are Home-\nbrew and MacPorts. In general, both are good choices for installing software pack-\nages. Because macOS is a derivative of the Berkeley Software Distribution (BSD) Unix,\nmany open source tools are available. But with recent changes to macOS to improve\nsecurity, some tools have dropped support for the latest releases for the platform. And\nwith recent changes to the Mac hardware, there may be significant changes to pack-\nage management. More information on Homebrew and MacPorts is available at their\nrespective websites:\nHomebrew at https:/ /brew.sh  \nMacPorts at https:/ /www.macports.org",3011
235-17.10 Modules Loading specialized toolchains.pdf,235-17.10 Modules Loading specialized toolchains,"608 CHAPTER  17 Tools and resources for better code\n17.9.2 Package managers for Windows\nThe heavily proprietary Windows operating system has long been a mixed bag for soft-\nware installation and support. Some software has been well supported and other soft-\nware not at all. Things are changing at Microsoft as it embraces the open source\nmovement. Windows is just now coming to the party with its new Windows Subsytem\nLinux (WSL). WSL sets up a Linux environment within a shell and should permit\nmost Linux software to work without changes. A recent announcement that WSL\nwould support transparent access to the GPU has generated excitement in the high\nperformance community. Of course, the main targets are gaming and other mass-mar-\nket applications, but we’ll be happy to ride the coattails if possible.\n17.9.3 The Spack package manager: A package manager for high \nperformance computing \nSo far, we have discussed package managers focused around specific computing plat-\nforms. The challenges of a tool for high performance computing are much greater\nthan those for traditional package managers because of the larger number of operat-\ning systems, hardware, and compilers that need to be simultaneously supported. It\ntook until 2013, when Todd Gamblin at Lawrence Livermore National Laboratory\nreleased the Spack package manager, to address these issues. One of this book’s authors\ncontributed a couple of packages to the Spack list when there were fewer than a dozen\npackages in the whole system. Now there are over 4,000 supported packages and many\nof these are unique to the high performance computing community. \nExample: Quick-start guide to Spack\nTo install Spack, type\ngit clone https:/  /github.com/spack/spack.git\nThen add to your environment the path and setup script. You can add these to your\n./bash_profile or ./bashrc file so that you will have Spack ready to go at anytime.\nexport SPACK_ROOT=/path/to/spack\nsource $SPACK_ROOT/share/spack/setup-env.sh\nTo configure Spack, first set up Spack for your compilers:\nspack compiler find\nIf the compiler is loaded from a module, add the load to the Spack compiler\nconfiguration\nspack config edit compilers \nor edit \n~/.spack/linux/compiler.yaml\n609 Modules: Loading specialized toolchains\nYou’ll find many Spack commands. Table 17.3 provides a few to get you started. \nSpack has extensive documentation and an active development community. Check\ntheir site for up-to-date information: https:/ /spack.readthedocs.io . \n17.10 Modules: Loading specialized toolchains\nThe realities of software development on large computing sites are that these sites\nhave to simultaneously support multiple environments. Because of this, you can load\ndifferent versions of the GCC and MPI for testing. You might be able to load these dif-\nferent development toolchains, but the software modules do not come with the exten-\nsive testing that is done with most vendor distributions. \nWARNING  Errors with toolchain software installed from the Modules package\ncan occur. The advantages for high performance applications, however, are\nlargely worth the potential difficulties. \nNow let’s look at the typical commands you might use with a toolchain system installed\nwith the Modules package as table 17.4 shows.You may want to add some of the system packages that already exist to the default\nconfiguration so these don’t get built. To do this, use your favorite editor and edit\n~/.spack/linux/packages.yaml\nTable 17.3 Using Spack\nCommand Description\nspack list Lists available packages\nspack install <package_name> Installs the requested package\nspack find Lists the packages that have already been built\nspack load <package_name> Loads the package into your environment\nTable 17.4 Toolchain module commands: Quick start\nCommand Description\nmodule avail Lists modules available on the system\nmodule list Lists modules that are loaded into your current environment\nmodule purge Unloads all modules and restores the environment to before \nmodules loaded\nmodule show <module_name> Shows what changes will be done to your environment\n610 CHAPTER  17 Tools and resources for better code\nBecause the module  show  command displays the actions executed by the module, let’s\nlook at a couple of examples for the GCC compiler suite and for CUDA.module unload <module_name> Unloads the module and removes changes to the environment\nmodule swap <module_name> \n<module_name>Replaces one module package with another\nExample: module show gcc/9.3.0\n/opt/modulefiles/centos7/gcc/9.3.0:\nmodule-whatis      This loads the GCC 9.3.0 environment. \nprepend-path PATH /projects/opt/x86_64/gcc/9.3.0/bin \nprepend-path LD_LIBRARY_PATH\n    /opt/x86_64/gcc/9.3.0/lib64:/opt/x86_64/gcc/9.3.0/lib \nprepend-path MANPATH /opt/x86_64/gcc/9.3.0/share/man \nsetenv     CC gcc \nsetenv     CXX g++ \nsetenv     CPP cpp \nsetenv     FC gfortran \nsetenv     F77 gfortran \nsetenv     F90 gfortran \nconflict     gcc\nIn this example, the GCC v9.3.0 module adds the GCC 9.3.0 directory to the path\nwith prepend-path  and to the environment with the LD_LIBRARY_PATH setting. It\nalso sets some environment variables with setenv  to direct which compiler to use.\nExample: module show cuda/10.2\n/opt/modulefiles/centos7/cuda/10.2:\nconflict    cuda \nmodule-whatis     load NVIDIA CUDA 10.2 environment \nmodule-whatis     Modifies: PATH, LD_LIBRARY_PATH \nmodule-whatis     IMPORTANT: the OpenCL libraries are \n    installed by the NVIDIA driver, not this module \nsetenv     CUDA_PATH /opt/centos7/cuda/10.2 \nsetenv     CUDADIR /opt/centos7/cuda/10.2 \nsetenv     CUDA_INSTALL_PATH /opt/centos7/cuda/10.2 \nsetenv     CUDA_LIB /opt/centos7/cuda/10.2/lib64 \nsetenv     CUDA_INCLUDE /opt/centos7/cuda/10.2/include \nsetenv     CUDA_BIN /opt/centos7/cuda/10.2/bin \nprepend-path PATH /opt/centos7/cuda/10.2/bin \nprepend-path LD_LIBRARY_PATH /opt/centos7/cuda/10.2/lib64 \nsetenv     OPENCL_LIBS /opt/centos7/cuda/10.2/lib64 Table 17.4 Toolchain module commands: Quick start  (continued)\nCommand Description\n611 Modules: Loading specialized toolchains\nAs you can see from the examples of these Modules commands, the modules are sim-\nply setting some environment variables. This is why Modules is not foolproof. Here\nare some important hints for using Modules that we learned the hard way. We begin\nwith the following:\nConsistency is important.  Set the same modules for compiling and running your\ncode. If the path to the library changes, your code may crash or give you the\nwrong results.\nAutomate as much as possible.  If you neglect to do so, your first build (or run) will\nfail before you realize you forgot to load your modules.\nAlso, there are different approaches to loading module files. Each is filled with advan-\ntages and disadvantages. These approaches are\nShell startup scripts\nInteractive at command line\nBatch submission scripts\nUse interactive shell startup scripts, not batch startup scripts (e.g., load Modules in a\n.login file instead of a .cshrc). Parallel jobs propagate their environment to remote\nnodes. If you load Modules in the wrong shell startup script, your remote nodes can have\ndifferent modules than your head node. This could have unexpected consequences.\n Use module  purge  in batch scripts before loading Modules. If you have Modules\nloaded, the module load can fail because of a conflict, potentially causing your pro-\ngram to fail. (Note that it is unreliable to use module  purge  on Cray systems.)\n Set run paths in program builds. Embedding run paths in your executable\nthrough the rpaths  link option or other build mechanisms, helps to make your appli-\ncation less sensitive to changing Modules environments and paths. The disadvantage\nis that your application may not run on another system if the compilers are not in the\nsame location. Note that this technique does not help with getting the wrong version\nof a program such as mpirun from your PATH variable.\n Load specific versions of compilers (e.g., GCC v9.3.0 rather than just GCC).\nOften a particular compiler version is set as default, but this will change at some\npoint, breaking your application or build. Also, defaults are not going to be the\nsame on all systems.\n There are two major software packages that implement basic Modules commands.\nThe first is called module, often called TCL modules, and the second is Lmod. We dis-\ncuss these in the following sections.setenv     OPENCL_INCLUDE /opt/centos7/cuda/10.2/include \nsetenv     CUDA_SDK /opt/centos7/cuda/10.2/samples \nThis CUDA module example sets paths, include directories, and library locations. It\nalso sets the paths for the NVIDIA OpenCL implementation.",8747
236-17.10.1 TCL modules The original modules system for loading software toolchains.pdf,236-17.10.1 TCL modules The original modules system for loading software toolchains,,0
237-17.10.2 Lmod A Lua-based alternative Modules implementation.pdf,237-17.10.2 Lmod A Lua-based alternative Modules implementation,,0
238-Appendix AReferences.pdf,238-Appendix AReferences,"612 CHAPTER  17 Tools and resources for better code\n17.10.1 TCL modules: The original modules system for \nloading software toolchains\nYeah, this is confusing. The Modules package created the category that now more or\nless uses the same name—module. In 1991, John Furlani at Sun Microsystems created\nmodule and then released it as open source software. The module tool is written in\nthe Tool Command Language, better known as TCL. It has proven to be an essential\ncomponent at major computing centers. The module document is at https:/ /modules\n.readthedocs.io/en/stable/module.html . \n17.10.2 Lmod: A Lua-based alternative Modules implementation\nLmod is a Lua-based Modules system that dynamically sets up a user’s environment. It\nis a newer implementation of the environment modules concept. The lmod documen-\ntation is at https:/ /lmod.readthedocs.io/en/latest .\n17.11 Reflections and exercises\nWe wish we had the time and space to go through in better detail how to use each of\nthese tools. Unfortunately, it would take another book (even several books) to explore\nthe world of tools for high performance computing. \n We have gone through some of the simpler tools, presenting both their power and\nusefulness. Just like you shouldn’t judge a book by its cover, don’t judge a tool by the\nfancy interface. Instead, you should look at what the tool does and how easy it is to\nuse. Our experience has been that fancy user interfaces, instead of functionality, often\nbecome the goal. In addition, tools should be simple. We have grown weary of facing\nanother 600-page quick start guide to just learn the next tool. Yes, the tool might be\ngreat and do wondrous things, but an application developer has a lot of other things\nto master as well. The best tools can be picked up and made useful in a couple of\nhours.\n Now we turn some of the effort over to you to try these tools, and hopefully, you\nwill find some that will expand your developer’s toolset. The addition of just a couple\nof tools makes you a better and more effective programmer. Here are a few exercises\nto get you started. \n1Run the Dr. Memory tool on one of your small codes or one of the codes from\nthe exercises in this book.\n2Compile one of your codes with the dmalloc library. Run your code and view\nthe results.\n3Try inserting a thread race condition into the example code in section 17.6.2\nand see how Archer reports the problem.\n4Try the profiling exercise in section 17.8 on your filesystem. If you have more\nthan one filesystem, try it on each. Then change the size of the array in the\nexample to 2000x2000. How does it change the filesystem performance results?\n5Install one of the tools using the Spack package manager.\n613 Summary\nSummary\nBetter software development practices start with version control. Creating a\nsolid software development environment results in faster and better code\ndevelopment.\nUse timers and profilers to measure the performance of your applications. Mea-\nsuring performance is the first step towards improving application performance.\nExplore the various mini-apps to see programming examples relevant to your\napplication area. Learning from these examples will help you avoid reinventing\nthe methods and improve your application.\nUse tools that help with detecting problems in your application. This improves\nyour program quality and robustness.",3402
239-A.1 Chapter 1 Why parallel computing.pdf,239-A.1 Chapter 1 Why parallel computing,,0
240-A.3 Chapter 3 Performance limits and profiling.pdf,240-A.3 Chapter 3 Performance limits and profiling,"614appendix A\nReferences\nWe have already provided a list of additional resources at the end of each chapter\nthat we suggest for learning more about topics covered in the chapter. In each\nchapter, we placed the materials that we think would be most valuable to most read-\ners. The references in this appendix are for those interested in the source materials\nthat were used in developing the book. The citations are partially to give credit to\nthe original authors of research and technical reports. These are also important for\nthose conducting more in-depth research on a particular topic. \nA.1 Chapter 1: Why parallel computing?\n■Amdahl, Gene M. “Validity of the single processor approach to achieving\nlarge scale computing capabilities.” Proceedings of the April 18–20, 1967,\nSpring Joint Computer Conference. (1967):483–48. https:/ /doi.org/10.1145/\n1465482.1465560 .\n■Flynn, Michael J. “Some Computer Organizations and Their Effectiveness.”\nIn IEEE Transactions on Computers , Vol. C-21, no. 9 (September, 1972): 948–\n960.\n■Gustafson, John L. “Reevaluating Amdahl’s Law.” In Communications of the\nACM , Vol. 31, no. 5 (May, 1988):532–533. http:/ /doi.acm.org/10.1145/\n42411.42415 .\n■Horowitz, M., Labonte, F., and Rupp, K., et al. “Microprocessor Trend Data.”\nAccessed February 20, 2021. https:/ /github.com/karlrupp/microprocess\nor-trend-data .\nA.2 Chapter 2: Planning for parallelism\n■CMake. https:/ /cmake.org/ .",1443
241-A.5 Chapter 5 Parallel algorithms and patterns.pdf,241-A.5 Chapter 5 Parallel algorithms and patterns,"615 Chapter 4: Data design and performance models\nA.3 Chapter 3: Performance limits and profiling\nTools\n■Empirical Roofline Toolkit (ERT). https:/ /bitbucket.org/berkeleylab/cs-roof\nline-toolkit .\n■4Intel® Advisor. https:/ /software.intel.com/en-us/advisor .\n■likwid. https:/ /github.com/RRZE-HPC/likwid .\n■STREAM download. https:/ /github.com/jeffhammond/Stream.git .\n■Valgrind. http:/ /valgrind.org/ .\nArticles\n■McCalpin, J. D. “STREAM: Sustainable Memory Bandwidth in High Perfor-\nmance Computers.” Accessed February 20, 2021. https:/ /www.cs.virginia.edu/\nstream/ .\n■Peise, Elmar. “Performance Modeling and Prediction for Dense Linear Algebra.”\narXiv:1706.01341 (June, 2017). Preprint: https:/ /arxiv.org/abs/1706.01341 .\n■Williams, S. W., D. Patterson, et. al. “The Roofline Model: A pedagogical tool\nfor auto-tuning kernels on multicore architectures.” In Hot Chips, A Symposium\non High Performance Chips , Vol. HC20 (August 10, 2008).\nA.4 Chapter 4: Data design and performance models\nResources\n■Data-oriented design. https:/ /github.com/dbartolini/data-oriented-design .\nArticles and books\n■Bird, R. “Performance Study of Array of Structs of Arrays.” Los Alamos National\nLab (LANL). Paper in preparation.\n■Garimella, Rao, and Robert W. Robey. “A Comparative Study of Multi-material\nData Structures for Computational Physics Applications,” no. LA-UR-16-23889.\nLos Alamos National Lab (LANL) (January, 2017). \n■Hennessy, John L., and David A. Patterson. Computer architecture: A Quantitative\nApproach . 5th ed. San Francisco, CA, USA: Morgan Kaufmann, 2011.\n■Hofmann, Johannes, Jan Eitzinger, and Dietmar Fey. “Execution-Cache-Memory\nPerformance Model: Introduction and Validation.” arXiv:1509.03118 (March,\n2017). Preprint: https:/ /arxiv.org/abs/1509.03118 .\n■Hollman, David, Bryce Lelbach, H. Carter Edwards, et al. “mdspan in C++: A\nCase Study in the Integration of Performance Portable Features into Interna-\ntional Language Standards.” IEEE/ACM International Workshop on Perfor-\nmance, Portability and Productivity in HPC (P3HPC) (November, 2019):60–70. \n■Treibig, Jan, and Georg Hager. “Introducing a performance model for band-\nwidth-limited loop kernels.” International Conference on Parallel Processing\nand Applied Mathematics (May, 2009):615–624.",2301
242-A.6 Chapter 8 MPI The parallel backbone.pdf,242-A.6 Chapter 8 MPI The parallel backbone,,0
243-A.8 Chapter 10 GPU programming model.pdf,243-A.8 Chapter 10 GPU programming model,"616 APPENDIX  AReferences\nA.5 Chapter 5: Parallel algorithms and patterns\n■Ahrens, Peter, Hong Diep Nguyen, and James Demmel. “Efficient Reproducible\nFloating Point Summation and BLAS.” In EECS Department, University of Califor-\nnia, Berkeley, Techical Report , No. UCB/EECS-2015-229 (December, 2015).\n■Alcantara, Dan A., Andrei Sharf, Fatemeh Abbasinejad, et al. “Real-time parallel\nhashing on the GPU.” In ACM Transactions on Graphics (TOG) , Vol. 28, no. 5\n(December, 2009):154.\n■Anderson, Alyssa. “Achieving Numerical Reproducibility in the Parallelized Float-\ning Point Dot Product.” (April, 2014). https:/ /digitalcommons.csbsju.edu/hon\nors_theses/30/ .\n■Blelloch, Guy E. “Scans as primitive parallel operations.” In IEEE Transactions on\ncomputers , Vol. 38, no. 11 (November, 1989):1526–1538.\n■Blelloch, Guy E. Vector models for data-parallel computing . Cambridge, MA, USA:\nThe MIT Press, 1990.\n■Chapp, Dylan, Travis Johnston, and Michela Taufer. “On the Need for Repro-\nducible Numerical Accuracy through Intelligent Runtime Selection of Reduc-\ntion Algorithms at the Extreme Scale.” 2015 IEEE International Conference on\nCluster Computing (October, 2015):166–175.\n■Cleveland, Mathew A., Thomas A. Brunner, et al. “Obtaining identical results\nwith double precision global accuracy on different numbers of processors in\nparallel particle Monte Carlo simulations.” In Journal of Computational Physics ,\nVol. 251 (October, 2013):223–236.\n■Harris, Mark, Shubhabrata Sengupta, and John D. Owens. “Parallel Prefix Sum\n(Scan) with CUDA.” In GPU Gems 3 , no. 39 (April, 2007):851-876.\n■Lessley, Brenton. “Data-Parallel Hashing Techniques for GPU Architectures.” In\nEurographics Conference on Visualization  (EuroVis), Vol. 37, no. 3 (July, 2018).\nA.6 Chapter 8: MPI: The parallel backbone\n■Hoefler, Torsten, and Jesper Larsson Traff. “Sparse collective operations for\nMPI.” 2009 IEEE International Symposium on Parallel & Distributed Processing\n(July, 2009):18.\n■Thakur, Rajeev, and William Gropp. “Test suite for evaluating performance of\nmultithreaded MPI communication.” In Parallel Computing , Vol. 35, no. 12\n(December, 2009):608–617.\nA.7 Chapter 9: GPU architectures and concepts\n■Yang, Charlene, Thorsten Kurth, and Samuel Williams. “Hierarchical Roofline\nanalysis for GPUs: Accelerating performance optimization for the NERSC-9\nPerlmutter system.” In Concurrency and Computation: Practice and Experience\n(November, 2019). https:/ /doi.org/10.1002/cpe.5547 .",2504
244-A.9 Chapter 12 GPU languages Getting down to basics.pdf,244-A.9 Chapter 12 GPU languages Getting down to basics,,0
245-A.10 Chapter 13 GPU profiling and tools.pdf,245-A.10 Chapter 13 GPU profiling and tools,,0
246-A.12 Chapter 16 File operations for a parallel world.pdf,246-A.12 Chapter 16 File operations for a parallel world,"617 Chapter 14: Affinity: Truce with the kernel\nA.8 Chapter 10: GPU programming model\n■CUDA Toolkit Documentation. “Compute Capabilities.” CUDA C++ Program-\nming Guide, v11.2.1 (NVIDIA Corporation, 2021). https:/ /docs.nvidia.com/\ncuda/cuda-c-programming-guide/index.html#compute-capabilities .\nA.9 Chapter 12: GPU languages: Getting down to basics\n■Harris, Mark. “Optimizing Parallel Reduction in CUDA.” (NVIDIA Corporation).\nhttps:/ /developer.download.nvidia.com/assets/cuda/files/reduction.pdf .\nA.10 Chapter 13: GPU profiling and tools\n■BBC News. “Indonesia tsunami: How a volcano can be the trigger.” BBC Global\nNews Ltd (December, 2018). http:/ /mng.bz/y92d.\nA.11 Chapter 14: Affinity: Truce with the kernel\n■Broquedis, François, Jérôme Clet-Ortega, et al. “hwloc: A Generic Framework\nfor Managing Hardware Affinities in HPC Applications.” Proceedings of the\n18th Euromicro International Conference on Parallel, Distributed and Net-\nwork-based Processing (PDP2010). IEEE Computer Society Press (February,\n2010):180–186. https:/ /ieeexplore.ieee.org/document/5452445 .\n■Hewlett Packard Enterprise, Original process placement program, xthi.c. CLE\nUser Application Placement Guide (CLE 5.2.UP04) S-2496, pg 87. http:/ /\nmng.bz/MgWB.\n■“OpenMP Application Programming Interface,” v5.0. OpenMP Architecture\nReview Board (November, 2018). https:/ /www.openmp.org/wp-content/uploads/\nOpenMP-API-Specification-5.0.pdf .\n■Samuel K. Gutiérrez, “Adaptive Parallelism for Coupled, Multithreaded Mes-\nsage-Passing Programs.” (December, 2018). https:/ /www.cs.unm.edu/~samuel/\npublications/2018/skgutierrez-dissertation.pdf .\n■Samuel K. Gutiérrez, Davis, Kei, et al. “Accommodating Thread-Level Hetero-\ngeneity in Coupled Parallel Applications.” Proceedings of the IEEE Interna-\ntional Parallel and Distributed Processing Symposium (May, 2017). https:/ /\ngithub.com/lanl/libquo/blob/master/docs/publications/quo-ipdps17.pdf .\n■Squyres, Jeff. “Process Placement.” (September, 2014). Accessed February 20,\n2021. https:/ /github.com/open-mpi/ompi/wiki/ProcessPlacement .\n■Treibig, J., G. Hager and G. Wellein. “LIKWID: A lightweight performance-\noriented tool suite for x86 multicore environments.” arXiv:1004.4431 (June,\n2010). Preprint: http:/ /arxiv.org/abs/1004.4431 .",2295
247-Appendix BSolutions to exercises.pdf,247-Appendix BSolutions to exercises,"618 APPENDIX  AReferences\nA.12 Chapter 16: File operations for a parallel world\nTools\n■BeeGFS (The leading parallel file system).  https:/ /www.beegfs.io/c/ .\n■Lustre®. OpenSFS and EOFS. http:/ /lustre.org .\n■The OrangeFS Project. http:/ /www.orangefs.org .\n■Panasas PanFS Parallel File System. https:/ /www.panasas.com/panfs-architec\nture/panfs/ .\nArticles and books\n■Gropp, William. “Lecture 33: More on MPI I/O Best practices for parallel IO\nand MPI-IO hints.” Accessed February 20, 2021. http:/ /wgropp.cs.illinois.edu/\ncourses/cs598-s15/lectures/lecture33.pdf .\n■Mendez, Sandra, Sebastian Lührs, et al. “Best Practice Guide—Parallel I/O.”\nAccessed February 20, 2021. https:/ /prace-ri.eu/wp-content/uploads/Best-\nPractice-Guide_Parallel-IO.pdf .\n■Thakur, Rajeev, Ewing Lusk, and William Gropp. Users guide for ROMIO: A high-\nperformance, portable MPI-IO implementation . ANL/MCS-TM-234. Artonne, IL,\nUSA: Argonne National Laboratory (October, 1997).\n■Thakur, Rajeev, William Gropp, and Ewing Lusk. “Data sieving and collective I/O\nin ROMIO.” Proceedings. Frontiers’ 99. Seventh Symposium on the Frontiers\nof Massively Parallel Computation (February, 1999):182–189. \nA.13 Chapter 17: Tools and resources for better code\n■Stepanov, Evgeniy, and Konstantin Serebryany. “MemorySanitizer: fast detector\nof uninitialized memory use in C++.” 2015 IEEE/ACM International Sympo-\nsium on Code Generation and Optimization (CGO) (February, 2015):46–55.",1469
248-B.4 Chapter 4 Data design and performance models.pdf,248-B.4 Chapter 4 Data design and performance models,"619appendix B\nSolutions to exercises\nB.1 Chapter 1: Why parallel computing?\n1What are some other examples of parallel operations in your daily life? How\nwould you classify your example? What does the parallel design appear to\noptimize for? Can you compute a parallel speedup for this example?\nAnswer: Examples of parallel operations in daily life include multi-lane high-\nways, class registration queues, and mail delivery. There are many others.\n2For your desktop, laptop, or cellphone, what is the theoretical parallel pro-\ncessing power of your system in comparison to its serial processing power?\nWhat kinds of parallel hardware are present in it?\nAnswer: It can be hard to penetrate the marketing and hype and find the\nreal specifications. Most devices, including handheld, have multi-core pro-\ncessors and at least an integrated graphics processor. Desktops and laptops\nhave some vector capabilities except for very old hardware.\n3Which parallel strategies do you see in the store checkout example in fig-\nure 1.1? Are there some present parallel strategies that are not shown? How\nabout in your examples from exercise 1?\nAnswer: Multiple instruction, multiple data (MIMD), distributed data, pipe-\nline parallelism, and out-of-order execution with specialized queues.\n4You have an image-processing application that needs to process 1,000 images\ndaily, which are 4 mebibytes (MiB, 220 or 1,048,576 bytes) each in size. It\ntakes 10 min in serial to process each image. Your cluster is composed of\nmulti-core nodes with 16 cores and a total of 16 gibibytes (GiB, 230 bytes, or\n1024 mebibytes) of main memory storage per node. (Note that we use the\n620 APPENDIX  BSolutions to exercises\nproper binary terms, MiB and GiB, rather than MB and GB, which are the met-\nric terms for 106 and 109 bytes, respectively.)\naWhat parallel processing design best handles this workload?\nbNow customer demand increases by 10x. Does your design handle this? What\nchanges would you have to make?\nAnswer: Threading on a single compute node along with vectorization. 4MiB ×\n1000 = 4 Gb. But to process 16 images at a time, only 64 MiB is needed, well\nunder 1 GiB on each node (workstation) of the cluster. The time would be 10\nmin × 1000 or 167 min in serial and 10.4 min on 16 cores in parallel. Vectoriza-\ntion could reduce this to under 5 min. A demand increase of 10x would make\nthis 100 min. This may be Ok, but it might also be time to think about message\npassing or distributed computing.\n5An Intel Xeon E5-4660 processor has a thermal design power of 130 W; this is\nthe average power consumption rate when all 16 cores are used. Nvidia’s Tesla\nV100 GPU and AMD’s MI25 Radeon GPU have a thermal design power of 300\nW. Suppose you port your software to use one of these GPUs. How much faster\nshould your application run on the GPU to be considered more energy effi-\ncient than your 16-core CPU application?\nAnswer: 300 W / 130 W. It needs to have a 2.3x speedup to be more energy effi-\ncient.\nB.2 Chapter 2: Planning for parallelism\n1You have a wave height simulation application that you developed during grad-\nuate school. It is a serial application and because it was only planned to be the\nbasis for your dissertation, you didn’t incorporate any software engineering\ntechniques. Now you plan to use it as the starting point for an available tool that\nmany researchers can use. You have three other developers on your team. What\nwould you include in your project plan for this?\nAnswer: The preparation steps would include\n– Establishing a version control system with Git\n– Creating a set of tests with known results\n– Running memory correctness tools on the tests in the test suite\n– Profiling the hardware and your application\n– Creating a plan for the next step in your agile management strategy\n2Create a test using CTest\nAnswer: To create a test using CTest, because CTest detects any error status\nfrom a command, a test can be made from a build instruction. Sometimes\ninstalling the CTest files will strip the executable bit from the permissions and\n621 Chapter 3: Performance limits and profiling\ncause the test to fail with no clear error message. To avoid this, we can add a test\nto detect if the CTest script is executable. In the following code, the $0 is the\nCTest script with the full path so that it works for out-of-tree builds.\n– In CMakeLists.txt, add\nenable_testing()\nadd_test(NAME make WORKING_DIRECTORY ${CMAKE_BINARY_DIRECTORY}\n            COMMAND ${CMAKE_CURRENT_SOURCE_DIR}/build.ctest)\n– Add the build.ctest file with\n#!/bin/sh\nif [ -x $0 ]\nthen\n   echo ""PASSED - is executable""\nelse\n   echo ""Failed - ctest script is not executable""\n   exit -1\nfi\n3Fix the memory errors in listing 2.2\nAnswer: You can fix the memory errors in listing 2.2 by changing or adding the\nfollowing lines:\n  4    int ipos=0, ival;\n  7    for (int i = 0; i<10; i++){ iarray[i] = ipos; }\n  8    for (int i = 0; i<10; i++){\n 11    free(iarray);\n4Run valgrind on a small application of your choice\nAnswer: On your own\nB.3 Chapter 3: Performance limits and profiling\n1Calculate the theoretical performance of a system of your choice. Include the\npeak flops, memory bandwidth, and machine balance in your calculation.\nAnswer: On your own\n2Download the Roofline Toolkit from https:/ /bitbucket.org/berkeleylab/cs-roof\nline-toolkit.git  and measure the actual performance of your selected system.\nAnswer: On your own\n3With the Roofline Toolkit, start with one processor and incrementally add opti-\nmization and parallelization, recording how much improvement you get at\neach step.\nAnswer: On your own\n622 APPENDIX  BSolutions to exercises\n4Download the STREAM benchmark from https:/ /www.cs.virginia.edu/stream/\nand measure the memory bandwidth of your selected system.\nAnswer: On your own\n5Pick one of the publicly available benchmarks or mini-apps listed in section\n17.4 and generate a call graph using KCachegrind.\nAnswer: On your own\n6Pick one of the publicly available benchmarks or mini-apps listed in section 17.4\nand measure its arithmetic intensity with either Intel Advisor or the likwid tools.\nAnswer: On your own\n7Using the performance tools presented in this chapter, determine the average\nprocessor frequency and energy consumption for a small application.\nAnswer: On your own\n8Using some of the tools from section 2.3.3, determine how much memory an\napplication uses.\nAnswer: On your own\nB.4 Chapter 4: Data design and performance models\n1Write a 2D contiguous memory allocator for a lower-left triangular matrix.\nAnswer: Listing B.4.1 shows the code to allocate a lower-left triangular array.\nAssume array indexing is C, with the lower left element at [0][0] . Also, the\nmatrix must be a square matrix. We use the same code as in listing 4.3 but with\nthe length of imax  reduced by 1 for each row. Note that the number of ele-\nments in the triangular array can be calculated by jmax*(imax+1)/2 . \nExerciseB.4.1/malloc2Dtri.c\n 1 #include <stdlib.h>\n 2 #include ""malloc2Dtri.h""\n 3\n 4 double **malloc2Dtri(int jmax, int imax)\n 5 {\n 6    double **x =                                 \n        (double **)malloc(jmax*sizeof(double *) +  \n 7      jmax*(imax+1)/2*sizeof(double));           \n 8\n 9    x[0] = (double *)(x + jmax);    \n10\n11    for (int j = 1; j < jmax; j++, imax--) {    \n12      x[j] = x[j-1] + imax;    \n13    }\n14Listing B.4.1 Triangular matrix allocation\nFirst allocate a block of \nmemory for the row \npointers and the 2D array\nNow assign the start of the block of memory \nfor the 2D array after the row pointers\nReduce imax by 1 \neach iteration Last, assign the memory \nlocation to point to for \neach row pointer\n623 Chapter 4: Data design and performance models\n15    return(x);\n16 }\n2Write a 2D allocator for C that lays out the memory the same way as Fortran.\nAnswer: Let’s assume that we want to address the array as x(j,i)  in Fortran.\nThe array will be addressed as x[i][j]  i n  C .  I f  w e  c r e a t e  a  m a c r o  #define\nx(j,i) x[i-1][j-1] , then the code could use the Fortran array notation. The\n2D memory allocator from listing 4.3 can be used by interchanging i and j and\nimax  and jmax . The following listing shows the resulting code.\nExercise4.2/malloc2Dfort.c\n 1 #include <stdlib.h>\n 2 #include ""malloc2Dfort.h""\n 3\n 4 double **malloc2Dfort(int jmax, int imax)\n 5 {\n 6    double **x =                                   \n        (double **)malloc(imax*sizeof(double *) +    \n 7      imax*jmax*sizeof(double));                   \n 8\n 9    x[0] = (double *)(x + imax);    \n10 \n11    for (int i = 1; i < imax; i++) {\n12       x[i] = x[i-1] + jmax;      \n13    }\n14\n15    return(x);\n16 }\n3Design a macro for an Array of Structure of Arrays (AoSoA) for the RGB color\nmodel in section 4.1.\nAnswer: We want to retrieve the data with the normal array index and color\nname:\n#define VV = 4\n#define color(i,C) AOSOA[(i)/VV].C[(i)%4-1]\ncolor(50,B)\n4Modify the code for the cell-centric full matrix data structure to not use a condi-\ntional and estimate its performance.\nAnswer: The following figure shows the code with the if statement removed.\nFrom this modified code, the performance model counts look like the following:\nMemops = 2 * NcNm + 2 * Nc = 102 M MemopsListing B.4.2 Triangular matrix allocation\nFirst allocate a block of \nmemory for the column \npointers and the 2D array\nNow assign the start of the block of memory \nfor the 2D array after the column pointers\nLast, assign the memory \nlocation to point to for \neach column pointer",9656
249-B.7 Chapter 7 OpenMP that performs.pdf,249-B.7 Chapter 7 OpenMP that performs,"624 APPENDIX  BSolutions to exercises\n 1: for all  cells, C, up to Nc do\n 2:    ave ← 0.0\n 3:    for all  material IDs, m, up to Nm do\n 4:          ave ← ave + ρ[C][m] ∗ f [C][m]       # 2NcNm loads (ρ, f )\n                                                                           # 2NcNm flops (+, ∗)\n 5:    end for\n 6:    ρave[C] ← ave/V[C]                                 # Nc stores (ρave), Nc loads ( V)\n                                                                          # Nc flops (/)\n 7: end for\nPerformance Model = 61.0 ms. This performance estimate is slightly faster than the version with \nthe if statement.\n5How would an AVX-512 vector unit change the ECM model for the stream triad?\nAnswer: The performance analysis with the ECM model in section 4.4 uses an\nAVX-256 vector unit that could process all the needed floating-point operations\nin 1 cycle. The AVX-512 would still need 1 cycle but would only have half of its\nvector units busy and could do twice the work if it were present. Because the\ncompute operation time, TOL, remains at 1 cycle, the performance would not\nchange at all.\nB.5 Chapter 5: Parallel algorithms and patterns\n1A cloud collision model in an ash plume is invoked for particles within a 1 mm\ndistance. Write pseudocode for a spatial hash implementation. What complex-\nity order is this operation?\nAnswer: The pseudocode for the collision operation is as follows:\n    1. Bin particles into 1 mm spatial bins\n    2. For each bin\n    3.    For each particle, i, in the bin\n    4.       For all other particles, j, in this bin or adjacent bins\n    5.          if  |Pi - Pj| < 1 mm\n    6.              compute collision\nThe operation is O(N2) in the local region, but as the mesh grows larger, the\ndistance between the particles does not have to be computed for larger regions,\nthus, the operation approaches O(N).\n2How are spatial hashes used by the postal service?\nAnswer: Zip codes. The hashing function encodes the state and region in the\nfirst three digits with the remaining two encoding first the large towns and then\nalphabetical order for the rest.\n3Big data uses a map-reduce algorithm for efficient processing of large data sets.\nHow is it different than the hashing concepts presented here?\n625 Chapter 6: Vectorization: FLOPs for free\nAnswer: Although developed for different problem domains and scales, the map\noperation in the map-reduce algorithm is a hash. So these both do a hashing step\nfollowed by a second local operation. The spatial hash has a concept of a distance\nrelationship between bins, whereas the map-reduce intrinsically does not.\n4A wave simulation code uses an AMR mesh to better refine the shoreline. The\nsimulation requirements are to record the wave heights versus time for speci-\nfied locations where buoys and shore facilities are located. Because the cells are\nconstantly being refined, how could you implement this?\nAnswer: Create a perfect spatial hash with the bin size the same as the smallest\ncell and store the cell index in the bins underlying the cell. Calculate the bin\nfor each station and get the cell index from the bin.\nB.6 Chapter 6: Vectorization: FLOPs for free\n1Experiment with auto-vectorizing loops from the multimaterial code in section 4.3\n(https:/ /github.com/LANL/MultiMatTest.git ). Add the vectorization and loop\nreport flags and see what your compiler tells you.\nAnswer: On your own\n2Add OpenMP SIMD pragmas to help the compiler vectorize loops to the loop\nyou selected in the first exercise.\nAnswer: On your own\n3For one of the vector intrinsic examples, change the vector length from four\ndouble precision values to an eight-wide vector width. Check the source code\nfor this chapter for examples of working code for eight-wide implementations.\nAnswer: In kahan_fog_vector.cpp, change 4s to 8s and change Vec4d  to Vec8d .\nAdd mprefer-vector-width=512  -DMAX_VECTOR_SIZE=512  to CXXFLAGS . The\nchanged code and Makefile are included in the source code for this chapter.\n4If you are on an older CPU, does your program from exercise 3 successfully\nrun? What is the performance impact?\nAnswer: For Intel 256-bit vector units, the Intel intrinsics do not work and must\nbe commented out. The GCC and Fog versions still work, however. The timing\nresults from a 2017 Mac laptop show the superiority of Agner Fog’s vector class\nlibrary with the eight-wide vectors producing better results than the four-wide.\nIn contrast, the GCC implementation for the eight-wide vector is slower than\nthe four-wide version. Here's the output:\nSETTINGS INFO -- ncells 1073741824 log 30\nInitializing mesh with Leblanc problem, high values first\n  relative diff  runtime    Description\n    8.423e-09    1.461642   Serial sum\n            0    3.283697   Kahan sum with double double accumulator\n626 APPENDIX  BSolutions to exercises\n 4 wide vectors serial sum\n   -3.356e-09    0.408654   Serial sum (OpenMP SIMD pragma)\n   -3.356e-09    0.407457   Intel vector intrinsics Serial sum\n   -3.356e-09    0.402928   GCC vector intrinsics Serial sum\n   -3.356e-09    0.406626   Fog C++ vector class Serial sum\n 4 wide vectors Kahan sum\n            0    0.872013   Intel Vector intrinsics Kahan sum\n            0    0.873640   GCC vector extensions Kahan sum\n            0    0.872774   Fog C++ vector class Kahan sum\n 8 wide vector serial sum\n   -1.986e-09    1.467707   8 wide GCC vector intrinsic Serial sum\n   -1.986e-09    0.586075   8 wide Fog C++ vector class Serial sum\n 8 wide vector Kahan sum\n   -1.388e-16    1.914804   8 wide GCC vector extensions Kahan sum\n   -1.388e-16    0.545128   8 wide Fog C++ vector class Kahan sum\n   -1.388e-16    0.687497    Agner C++ vector class Kahan sum\nB.7 Chapter 7: OpenMP that performs\n1Convert the vector add example in listing 7.8 into a high-level OpenMP follow-\ning the steps in section 7.2.2.\nAnswer: Converting to high-level OpenMP, we end up with the code shown in\nthe following listing with just a single pragma to open the parallel region.\nExerciseB.7.1/vecadd.c\n11 int main(int argc, char *argv[]){\n12    #pragma omp parallel\n13    {\n14       double time_sum;\n15       struct timespec tstart;\n16       int thread_id = omp_get_thread_num();\n17       int nthreads  = omp_get_num_threads();\n18       if (thread_id == 0){\n19          printf(""Running with %d thread(s)\n"",nthreads);\n20       }\n21       int tbegin = ARRAY_SIZE * ( thread_id     ) / nthreads;\n22       int tend   = ARRAY_SIZE * ( thread_id + 1 ) / nthreads;\n23\n24       for (int i=tbegin; i<tend; i++) {\n25          a[i] = 1.0;\n26          b[i] = 2.0;\n27       }\n28\n29       if (thread_id == 0) cpu_timer_start(&tstart);\n30       vector_add(c, a, b, ARRAY_SIZE);\n31       if (thread_id == 0) {\n32          time_sum += cpu_timer_stop(tstart);\n33          printf(""Runtime is %lf msecs\n"", time_sum);\n34       }\n35    }\n36 }Listing B.7.1 High-level OpenMP\n627 Chapter 7: OpenMP that performs\n37\n38 void vector_add(double *c, double *a, double *b, int n)\n39 {\n40    int thread_id = omp_get_thread_num();\n41    int nthreads = omp_get_num_threads();\n42    int tbegin = n * ( thread_id     ) / nthreads;\n43    int tend   = n * ( thread_id + 1 ) / nthreads;\n44    for (int i=tbegin; i < tend; i++){\n45       c[i] = a[i] + b[i];\n46    }\n47 }\n2Write a routine to get the maximum value in an array. Add an OpenMP pragma\nto add thread parallelism to the routine.\nAnswer: The reduction routine uses the reduction(max:xmax)  clause as the fol-\nlowing listing shows.\nExerciseB.7.2/max_reduction.c\n 1 #include <float.h>\n 2 double array_max(double* restrict var, int ncells)\n 3 {\n 4    double xmax = DBL_MIN;\n 5    #pragma omp parallel for reduction(max:xmax)\n 6    for (int i = 0; i < ncells; i++){\n 7       if (var[i] > xmax) xmax = var[i];\n 8    }\n 9 }\n3Write a high-level OpenMP version of the reduction in the previous exercise.\nAnswer: In high-level OpenMP, we manually divide up the data. The data\ndecomposition is done in lines 6-9 in listing B.7.3. Thread 0 allocates the xmax-\n_thread  shared data array on line 13. Lines 18–22 find the maximum value for\neach thread and store the result in the xmax_thread  array. Then, on lines 26–\n30, one thread finds the maximum across all the threads.\nExerciseB.7.3/max_reduction.c\n 1 #include <stdlib.h>\n 2 #include <float.h>\n 3 #include <omp.h>\n 4 double array_max(double* restrict var, int ncells)\n 5 {\n 6    int nthreads = omp_get_num_threads();\n 7    int thread_id = omp_get_thread_num();\n 8    int tbegin = ncells * ( thread_id     ) / nthreads;\n 9    int tend   = ncells * ( thread_id + 1 ) / nthreads;\n10    static double xmax;Listing B.7.2 OpenMP max reduction\nListing B.7.3 High-level OpenMP",8793
250-B.11 Chapter 11 Directive-based GPU programming.pdf,250-B.11 Chapter 11 Directive-based GPU programming,"628 APPENDIX  BSolutions to exercises\n11    static double *xmax_thread;\n12    if (thread_id == 0){\n13       xmax_thread = malloc(nthreads*sizeof(double));\n14       xmax = DBL_MIN;\n15    }\n16 #pragma omp barrier\n17\n18    double xmax_thread_private = DBL_MIN;\n19    for (int i = tbegin; i < tend; i++){\n20       if (var[i] > xmax_thread_private) xmax_thread_private = var[i];\n21    }\n22    xmax_thread[thread_id] = xmax_thread_private;\n23\n24 #pragma omp barrier\n25\n26    if (thread_id == 0){\n27       for (int tid=0; tid < nthreads; tid++){\n28          if (xmax_thread[tid] > xmax) xmax = xmax_thread[tid];\n29       }\n30    }\n31\n32 #pragma omp barrier\n33 \n34    if (thread_id == 0){\n35       free(xmax_thread);\n36    }\n37    return(xmax);\n38 }\nB.8 Chapter 8: MPI: The parallel backbone\n1Why can’t we just block on receives as was done in the send/receive in the ghost\nexchange using the pack or array buffer methods in listings 8.20 and 8.21,\nrespectively?\nAnswer: The version using the pack or array buffers schedules the send, but\nreturns before the data is copied or sent. The standard for the MPI_Isend  says,\n“The sender should not modify any part of the send buffer after a nonblocking send opera-\ntion is called, until the send completes. ” The pack and array versions deallocate the\nbuffers after the communication. So these versions might delete the buffers\nbefore these are copied, causing the program to crash. To be safe, the status of\nthe send must be checked before the buffer is deleted.\n2Is it safe to block on receives as shown in listing 8.8 in the vector type version of\nthe ghost exchange? What are the advantages if we only block on receives?\nAnswer: The vector version sends the data from the original arrays instead of\nmaking a copy. This is safer than the versions that allocate a buffer, which will\nbe deallocated. If we only block on receives, the communication can be faster.\n3Modify the ghost cell exchange vector type example in listing 8.21 to use block-\ning receives instead of a waitall . Is it faster? Does it always work?\n629 Chapter 9: GPU architectures and concepts\nAnswer: Even with the vector version of the ghost cell exchange, we have to be\ncareful that we do not modify the buffers that are still in the process of being\nsent. The odds of this happening can be small when we are not sending cor-\nners. But it still can occur. To be absolutely safe, we need to check for comple-\ntion of the sends before changing the arrays.\n4Try replacing the explicit tags in one of the ghost exchange routines with\nMPI_ANY_TAG . Does it work? Is it any faster? What advantage do you see in using\nexplicit tags?\nAnswer: Using MPI_ANY_TAG  for the tag argument works fine. It can be slightly\nfaster though it is unlikely that it will be significant enough to be measurable.\nUsing explicit tags adds another check that the right message is being received.\n5Remove the barriers in the synchronized timers in one of the ghost exchange\nexamples. Run the code with the original synchronized timers and the unsyn-\nchronized timers.\nAnswer: Removing the barriers in the timers should give better performance\nand allow the processes to operate more independently (asynchronous). It can\nbe more difficult to understand the timing measurements though.\n6Add the timer statistics from listing 8.11 to the stream triad bandwidth measure-\nment code in listing 8.17.\nAnswer: On your own\n7Apply the steps to convert high-level OpenMP to the hybrid MPI plus OpenMP\nexample in the code that accompanies the chapter (HybridMPIPlusOpenMP\ndirectory). Experiment with the vectorization, number of threads, and MPI ranks\non your platform.\nAnswer: On your own\nB.9 Chapter 9: GPU architectures and concepts\n1Table 9.7 shows the achievable performance for a 1 flop/load application. Look\nup the current prices for the GPUs available on the market and fill in the last\ntwo columns to get the flop per dollar for each GPU. Which looks like the best\nvalue? If turnaround time for your application runtime is the most important\ncriteria, which GPU would be best to purchase? \nTable 9.7 Achievable performance for a 1 flop/load application with various GPUs\nGPUAchievable \nPerformance Gflops/sec Price Flops/$\nV100 108.23\nVega 20 91.38\n630 APPENDIX  BSolutions to exercises\nAnswer: On your own\n2Measure the stream bandwidth of your GPU or another selected GPU. How\ndoes it compare to the ones presented in the chapter?\nAnswer: On your own\n3Use the likwid performance tool to get the CPU power requirements for the\nCloverLeaf application on a system where you have access to the power hard-\nware counters.\nAnswer: On your own\nB.10 Chapter 10: GPU programming model\n1You have an image classification application that will take 5 ms to transfer each\nfile to the GPU, 5 ms to process and 5 ms to bring back. On the CPU, the pro-\ncessing takes 100 ms per image. There are one million images to process. You\nhave 16 processing cores on the CPU. Would a GPU system do the work faster?\nAnswer:\nTime on a CPU—100 ms × 1,000,000/16 /1,000 = 6,250 s\nTime on a GPU—(5 ms + 5 ms + 5 ms) × 1,000,000/1,000 = 15,000 s\nThe GPU system would not be faster. It would take about 2.5 times as long.\n2The transfer time for the GPU in problem 1 is based on a third generation PCI\nbus. If you can get a Gen4 PCI bus, how does that change the design? A Gen 5\nPCI bus? For image classification, you shouldn’t need to bring back a modified\nimage. How does that change the calculation? \nAnswer: A fourth-generation PCI bus is twice as fast as a third-generation PCI bus.\n(2.5 ms + 5 ms + 2.5 ms) × 1,000,000/1,000 = 10,000 s\nA fifth-generation PCI bus would be four times as fast as the original third-\ngeneration PCI bus.\n(1.25 ms + 5 ms + 1.25 ms) × 1,000,000/1,000 = 7,500 sP100 74.69\nGeForce GTX1080Ti 44.58\nQuadro K6000 31.25\nTesla S2050 18.50Table 9.7 Achievable performance for a 1 flop/load application with various GPUs  (continued)\nGPUAchievable \nPerformance Gflops/sec Price Flops/$\n631 Chapter 11: Directive-based GPU programming\nIf we don’t have to transfer the results back, we are now just as fast on the GPU\nas on the CPU.\n(1.25 ms + 5 ms) × 1,000,000/1,000 = 6,250 s\n3For your discrete GPU (or NVIDIA GeForce GTX 1060, if none), what size 3D\napplication could you run? Assume 4 double-precision variables per cell and a\nusage limit of half the GPU memory so you have room for temporary arrays.\nHow does this change if you use single precision?\nAnswer: An NVIDIA GeForce GTX 1060 has a memory size of 6 GiB. It has\nGDDR5 with a 192-bit wide bus and 8GHz memory clock. \n(6 GiB/2/4 doubles/8bytes × 10243)1/3 = 465 × 465 × 465 3D mesh\nFor single precision\n(6 GiB/2/4floats/4bytes × 10243)1/3 = 586 × 586 × 586 3D mesh\nIf we are dividing up our computational domain into this 3D mesh, this is a 25%\nimprovement in resolution.\nB.11 Chapter 11: Directive-based GPU programming\n1Find what compilers are available for your local GPU system. Are both Open-\nACC and OpenMP compilers available? If not, do you have access to any systems\nthat would allow you to try out these pragma-based languages?\nAnswer: On your own\n2Run the stream triad examples from the OpenACC/StreamTriad and/or the\nOpenMP/StreamTriad directories on your local GPU development system. You’ll\nfind these directories at https:/ /github.com/EssentialsofParallelComputing/\nChapter11 .\nAnswer: On your own\n3Compare your results from exercise 2 to BabelStream results at https:/ /uob-\nhpc.github.io/BabelStream/results/ . For the stream triad, the bytes moved are\n3 * nsize * sizeof(datatype) .\nAnswer: From the performance results in the chapter for the NVIDIA V100\nGPU\n3 × 20,000,000 × 8 bytes/.586 ms) × (1000 ms/s) / (1,000,000,000 bytes/GB) =\n819 GB/s \nThis is about 50% greater than the peak shown for the BabelStream benchmark\nfor the NVIDIA P100 GPU.\n4Modify the OpenMP data region mapping in listing 11.16 to reflect the actual\nuse of the arrays in the kernels.\n632 APPENDIX  BSolutions to exercises\nAnswer: The arrays are only used on the GPU, so these can be allocated there\nand deleted at the end. Therefore, the changes are\n13 #pragma omp target enter data map( alloc:a[0:nsize], b[0:nsize],\n                                           c[0:nsize])\n36 #pragma omp target exit data map( delete:a[0:nsize], b[0:nsize],\n                                           c[0:nsize])\nThe full listing of this change is in Stream_par7.c in the examples for the chap-\nter.\n5Implement the mass sum example from listing 11.4 in OpenMP.\nAnswer: We just need to change the one pragma as the following listing shows.\nExerciseB.11.5/mass_sum.c\n 1 #include ""mass_sum.h""\n 2 #define REAL_CELL 1\n 3 \n 4 double mass_sum(int ncells, int* restrict celltype,\n 5        double* restrict H, double* restrict dx, double* restrict dy){\n 6    double summer = 0.0;\n 7 #pragma omp target teams distribute \\n               parallel for simd reduction(+:summer)\n 8    for (int ic=0; ic<ncells ; ic++) {\n 9       if (celltype[ic] == REAL_CELL) {\n10          summer += H[ic]*dx[ic]*dy[ic];\n11       }\n12    }\n13    return(summer);\n14 }\n6For x and y arrays of size 20,000,000, find the maximum radius for the arrays\nusing both OpenMP and OpenACC. Initialize the arrays with double-precision\nvalues that linearly increase from 1.0 to 2.0e7 for the x array and decrease from\n2.0e7 to 1.0 for the y array.\nAnswer: The following listing shows a possible implementation of finding the\nmaximum radius using OpenACC. \nExerciseB.11.6/MaxRadius.c or Chapter11/OpenACC/MaxRadius/MaxRadius.c\n 1 #include <stdio.h>\n 2 #include <math.h>\n 3 #include <openacc.h>\n 4 \n 5 int main(int argc, char *argv[]){\n 6    int ncells = 20000000;\n 7    double* restrict x = acc_malloc(ncells * sizeof(double));\n 8    double* restrict y = acc_malloc(ncells * sizeof(double));Listing B.11.5 GPU version of OpenMP\nListing B.11.6 OpenACC version of Max Radius",10028
251-B.12 Chapter 12 GPU languages Getting down to basics.pdf,251-B.12 Chapter 12 GPU languages Getting down to basics,"633 Chapter 12: GPU languages: Getting down to basics\n 9 \n10    double MaxRadius = -1.0e30;\n11 #pragma acc parallel deviceptr(x, y)\n12    {\n13 #pragma acc loop\n14       for (int ic=0; ic<ncells; ic++) {\n15          x[ic] = (double)(ic+1);\n16          y[ic] = (double)(ncells-ic);\n17       }\n18 \n19 #pragma acc loop reduction(max:MaxRadius)\n20       for (int ic=0; ic<ncells ; ic++) {\n21          double radius = sqrt(x[ic]*x[ic] + y[ic]*y[ic]);\n22          if (radius > MaxRadius) MaxRadius = radius;\n23       }\n24    }\n25    printf(""Maximum Radius is %lf\n"",MaxRadius);\n26 \n27    acc_free(x);\n28    acc_free(y);\n29 }\nB.12 Chapter 12: GPU languages: Getting down to basics\n1Change the host memory allocation in the CUDA stream triad example to use\npinned memory (listings 12.1–12.6). Did you get a performance improvement?\nAnswer: To get pinned memory, replace malloc  in the host-side memory alloca-\ntion with cudaHostMalloc  and the free memory with cudaFreeHost  as listing\nB.12.1 shows. In the listing, we only display the lines that need to be changed.\nCompare the performance to the code from chapter12 in CUDA/StreamTriad\ndirectory. The data transfer time should be at least a factor of two times faster\nwith pinned memory.\nExerciseB.12.1/StreamTriad.cu\n31    // allocate host memory and initialize\n32    double *a, *b, *c;\n33    cudaMallocHost(&a,stream_array_size*sizeof(double));\n34    cudaMallocHost(&b,stream_array_size*sizeof(double));\n35    cudaMallocHost(&c,stream_array_size*sizeof(double));\n       < ... steam triad code ... >\n86    cudaFreeHost(a);\n87    cudaFreeHost(b);\n88    cudaFreeHost(c);\n2For the sum reduction example, try an array size of 18,000 elements all initial-\nized to their index value. Run the CUDA code and then the version in SumRe-\nductionRevealed. You may want to adjust the amount of information printed.\nAnswer: On your ownListing B.12.1 Pinned memory version of stream triad\n634 APPENDIX  BSolutions to exercises\n3Convert the CUDA reduction example to HIP by hipifying it.\nAnswer: On your own\n4For the SYCL example in listing 12.20, initialize the a and b arrays on the GPU\ndevice.\nAnswer: The following listing shows a version with the a and b arrays initialized\non the GPU.\n14    // host data\n15    vector<double> a(nsize);\n16    vector<double> b(nsize);\n17    vector<double> c(nsize);\n18 \n19    t1 = chrono::high_resolution_clock::now();\n20 \n21    Sycl::queue Queue(sycl::cpu_selector{});\n22 \n23    const double scalar = 3.0;\n24 \n25    Sycl::buffer<double,1> dev_a { a.data(), Sycl::range<1>(a.size()) };\n26    Sycl::buffer<double,1> dev_b { b.data(), Sycl::range<1>(b.size()) };\n27    Sycl::buffer<double,1> dev_c { c.data(), Sycl::range<1>(c.size()) };\n28 \n29    Queue.submit([&](sycl::handler& CommandGroup) {\n30 \n31       auto a = \n            dev_a.get_access<Sycl::access::mode::write>(CommandGroup);\n32       auto b =\n            dev_b.get_access<Sycl::access::mode::write>(CommandGroup);\n33       auto c =\n            dev_c.get_access<Sycl::access::mode::write>(CommandGroup);\n34 \n35       CommandGroup.parallel_for<class StreamTriad>( \n             Sycl::range<1>{nsize},  [=] (Sycl::id<1> it) {\n36           a[it] =  1.0;\n37           b[it] =  2.0;\n38           c[it] = -1.0;\n39       });\n40    });\n41    Queue.wait();\n42 \n43    Queue.submit([&](sycl::handler& CommandGroup) {\n44 \n45       auto a = dev_a.get_access<Sycl::access::mode::read>(CommandGroup);\n46       auto b = dev_b.get_access<Sycl::access::mode::read>(CommandGroup);\n47       auto c =\n            dev_c.get_access<Sycl::access::mode::write>(CommandGroup);\n48 \n49       CommandGroup.parallel_for<class StreamTriad>( \n             Sycl::range<1>{nsize}, [=] (Sycl::id<1> it) {Listing B.12.4 Initializing arrays a and b in SYCL",3839
252-B.13 Chapter 13 GPU profiling and tools.pdf,252-B.13 Chapter 13 GPU profiling and tools,,0
253-B.15 Chapter 15 Batch schedulers Bringing order to chaos.pdf,253-B.15 Chapter 15 Batch schedulers Bringing order to chaos,"635 Chapter 14: Affinity: Truce with the kernel\n50           c[it] = a[it] + scalar * b[it];\n51       });\n52    });\n53    Queue.wait();\n54 \n55    t2 = chrono::high_resolution_clock::now();\n5Convert the two initialization loops in the Raja example in listing 12.24 to the\nRaja:forall  syntax. Try running the example with CUDA.\nAnswer: The initialization loop needs the changes shown in the following listing.\nThen the stream triad code is built and run the same way as in section 12.5.2.\nExerciseB.12.5/StreamTriad.cc\n19 RAJA::forall<RAJA::omp_parallel_for_exec>(RAJA::RangeSegment(0,\n       nsize), [=] (int i) {\n20   a[i] = 1.0;\n21   b[i] = 2.0;\n22 });\nWith these changes, the run time compared to the original version in section\n12.5.2 drops from around 6.59 ms to 1.67 ms.\nB.13 Chapter 13: GPU profiling and tools\n1Run nvprof on the STREAM Triad example. You might try the CUDA version\nfrom chapter 12 or the OpenACC version from chapter 11. What workflow did\nyou use for your hardware resources? If you don’t have access to an NVIDIA\nGPU, can you use another profiling tool?\nAnswer: On your own\n2Generate a trace from nvprof and import it into NVVP. Where is the run time\nspent? What could you do to optimize it?\nAnswer: On your own\n3Download a prebuilt Docker container from the appropriate vendor for your\nsystem. Start up the container and run one of the examples from chapter 11 or\n12.\nAnswer: On your own\nB.14 Chapter 14: Affinity: Truce with the kernel\n1Generate a visual image of a couple of different hardware architectures. Dis-\ncover the hardware characteristics for these devices.\nAnswer: Use the lstopo tool to generate an image of your architecture.Listing B.12.5 Adding Raja to the initialization loop of stream triad\n636 APPENDIX  BSolutions to exercises\n2For your hardware, run the test suite using the script in listing 14.4. What do\nyou discover about how to best use your system?\nAnswer: On your own\n3Change the program used in the vector addition (vecadd_opt3.c) example in\ns e c t i o n  1 4 . 3  t o  i n c l u d e  m o r e  f l o a t i n g - p o i n t  o p e r a t i o n s .  T a k e  t h e  k e r n e l  a n d\nchange the operations in the loop to the Pythagorean formula:\nc[i] = sqrt(a[i] * a[i] + b[i] * b[i]);\nHow do your results and conclusions about the best placement and bindings\nchange? Do you see benefit from hyperthreads now (if you have those)?\nAnswer: On your own\n4For the MPI example in section 14.4, include the vector add kernel and gener-\nate a scaling graph for the kernel. Then replace the kernel with the Pythago-\nrean formula used in exercise 3.\nAnswer: On your own\n5Replace the kernel with the Pythagorean formula used in exercise 3.\nAnswer: On your own\n6Combine the vector add and Pythagorean formula in the following routine\n(either in a single loop or two separate loops) to get more data reuse: \nc[i] = a[i] + b[i];\nd[i] = sqrt(a[i]*a[i] + b[i]*b[i]);\nHow does this change the results of the placement and binding study?\nAnswer: On your own\n7Add code to set the placement and affinity within the application from one of\nthe previous exercises.\nAnswer: On your own\nB.15 Chapter 15: Batch schedulers: Bringing order to chaos\n1Try submitting a couple of jobs, one with 32 processors and one with 16 proces-\nsors. Check to see that these are submitted and whether they are running.\nDelete the 32 processor job. Check to see that it got deleted.\nAnswer: On your own\n2Modify the automatic restart script so that the first job is a preprocessing step to\nset up for the computation and the restarts are for running the simulation.\n637 Chapter 15: Batch schedulers: Bringing order to chaos\nAnswer: To insert a preprocessing step, we need to insert another conditional\ncase as the following listing shows on lines 31–36 and then use the PREPRO-\nCESS_DONE file to indicate that the preprocessing has been done.\nExerciseB.15.2/Preprocess_then_restart.sh\n 1 #!/bin/sh\n 2 #SBATCH -N 1\n 3 #SBATCH -n 4\n 4 #SBATCH --signal=23@160\n 5 #SBATCH -t 00:08:00\n 6 \n 7 # Do not place bash commands before the last SBATCH directive\n 8 # Behavior can be unreliable\n 9 \n10 NUM_CPUS=4\n11 OUTPUT_FILE=run.out\n12 EXEC_NAME=./testapp\n13 MAX_RESTARTS=4\n14 \n15 if [ -z ${COUNT} ]; then\n16    export COUNT=0\n17 fi\n18 \n19 ((COUNT++))\n20 echo ""Restart COUNT is ${COUNT}""\n21 \n22 if [ ! -e DONE ]; then\n23    if [ -e RESTART ]; then\n24       echo ""=== Restarting ${EXEC_NAME} ===""            >> ${OUTPUT_FILE}\n25       cycle=`cat RESTART`\n26       rm -f RESTART\n27    elif [ -e PREPROCESS_DONE ]; then\n28       echo ""=== Starting problem ===""                   >> ${OUTPUT_FILE}\n29       cycle=""""\n30    else\n31       echo ""=== Preprocessing data for problem ===""     >> ${OUTPUT_FILE}\n32       mpirun -n ${NUM_CPUS} ./preprocess_data          &>> ${OUTPUT_FILE}\n33       date > PREPROCESS_DONE\n34       sbatch \                                   \n            --dependency=afterok:${SLURM_JOB_ID} \  \n            <preprocess_then_restart.sh             \n35       exit\n36    fi\n37 \n38    echo ""=== Submitting restart script ===""             >> ${OUTPUT_FILE}\n39    sbatch \                                      \n         --dependency=afterok:${SLURM_JOB_ID} \     \n         <preprocess_then_restart.sh                \n40 \n41    mpirun -n ${NUM_CPUS} ${EXEC_NAME} ${cycle}         &>> ${OUTPUT_FILE}\n42    echo ""Finished mpirun""                               >> ${OUTPUT_FILE}\n43 Listing B.15.2a Inserting preprocessing step and then automatically restarting\nSubmits first \ncalculation job \nafter preprocess\nSubmits \nrestart job\n638 APPENDIX  BSolutions to exercises\n44    if [ ${COUNT} -ge ${MAX_RESTARTS} ]; then\n45       echo ""=== Reached maximum number of restarts ==="" >> ${OUTPUT_FILE}\n46       date > DONE\n47    fi\n48 fi\nOften the preprocessing step needs a different number of processors. In this case, we\ncan use a separate batch script for the preprocessing, shown in the following listing.\nExerciseB.15.2/Preprocess_batch.sh\n 1 #!/bin/sh\n 2 #SBATCH -N 1\n 3 #SBATCH -n 1\n 5 #SBATCH -t 01:00:00\n 6\n 7 sbatch --dependency=afterok:${SLURM_JOB_ID} <batch_restart.sh\n 9\n10 mpirun -n 4 ./preprocess &> preprocess.out\n3Modify the simple batch script in listing 15.1 for Slurm and 15.2 for PBS to\nclean up on failure by removing a file called simulation_database.\nAnswer: Change the Slurm batch script to check the status of the command and\nremove the simulation database. There are several different ways to do the\ncleanup. Here are three. The first two in listings B.15.3a and b use the exit code\nfrom the mpirun  command.\nExerciseB.15.3/batch_simple_error.sh\n 1 #!/bin/sh\n 2 #SBATCH -N 1\n 3 #SBATCH -n 4\n 5 #SBATCH -t 01:00:00\n 6\n 7 mpirun -n 4 ./testapp &> run.out || \    \n      rm -f simulation_database             \nExerciseB.15.3/batch_simple_error.sh\n 1 #!/bin/sh\n 2 #SBATCH -N 1\n 3 #SBATCH -n 4\n 5 #SBATCH -t 01:00:00\n 6\n 7 mpirun -n 4 ./testapp &> run.out\n 8 STATUS=$?\n 9 if [ ${STATUS} != “0” ]; then\n10    rm -f simulation_database\n11 fiListing B.15.2b Smaller preprocessing step and then automatic restart\nListing B.15.3a OpenACC version of Max Radius\nListing B.15.3b OpenACC version of Max RadiusThe || symbol executes \nthe command for non-\nzero status values",7359
254-B.16 Chapter 16 File operations for a parallel world.pdf,254-B.16 Chapter 16 File operations for a parallel world,,0
255-B.17 Chapter 17 Tools and resources for better code.pdf,255-B.17 Chapter 17 Tools and resources for better code,"639 Chapter 17: Tools and resources for better code\nThe third version in listing B.15.3.b uses the status condition of the batch job\nthrough a dependency flag to invoke a cleanup job. The types of errors that are\nhandled are different than the first two methods.\nExerciseB.15.3/batch.sh\n 1 #!/bin/sh\n 2 #SBATCH -N 1\n 3 #SBATCH -n 4\n 5 #SBATCH -t 01:00:00\n 6\n 7 sbatch --dependency=afternotok:${SLURM_JOB_ID} <batch_cleanup.sh\n 9\n10 mpirun -n 4 ./testapp &> run.out\nExerciseB.15.3/batch_cleanup.sh\n 1 #!/bin/sh\n 2 #SBATCH -N 1\n 3 #SBATCH -n 1\n 5 #SBATCH -t 00:10:00\n 6 rm -f simulation_database\nB.16 Chapter 16: File operations for a parallel world\n1Check for the hints available on your system using the techniques described in\nsection 16.6.1.\nAnswer: On your own\n2Try the MPI–IO and HDF5 examples on your system with much larger datasets\nto see what performance you can achieve. Compare that to the IOR micro\nbenchmark for extra credit.\nAnswer: On your own\n3Use the h5ls and h5dump utilities to explore the HDF5 data file created by the\nexample.\nAnswer: On your own\nB.17 Chapter 17: Tools and resources for better code\n1Run the Dr. Memory tool on one of your small codes or one of the codes from\nthe exercises in this book.\nAnswer: On your own\n2Compile one of your codes with the dmalloc library. Run your code and view\nthe results.\nAnswer: On your ownListing B.15.3b OpenACC version of Max Radius\n640 APPENDIX  BSolutions to exercises\n3Try inserting a thread race condition into the example code in section 17.6.2\nand see how Archer reports the problem.\nAnswer: On your own\n4Try the profiling exercise in section 17.8 on your filesystem. If you have more\nthan one filesystem, try it on each one. Then change the size of the array in the\nexample to 2000x2000. How does it change the filesystem performance results?\nAnswer: On your own\n5Install one of the tools using the Spack package manager.\nAnswer: On your own",1960
256-Appendix CGlossary.pdf,256-Appendix CGlossary,"641appendix C\nGlossary\n3DNow! An AMD vector instruction set that first supported single-precision oper-\nations.\nAffinity. Assigning a preference for the placement of a process, rank, or thread to a\nparticular hardware component. This is also called pinning or binding.\nAlgorithmic complexity. A measure of the number of operations that it would take\nto complete an algorithm. Algorithmic complexity is a property of the algo-\nrithm and is a measure of the amount of work or operations in a procedure.\nAliasing. Where pointers point to overlapping regions of memory. In this situation,\nthe compiler cannot tell if it is the same memory, and in these instances, it\nwould be unsafe to generate vectorized code or other optimizations.\nAnti-flow dependency. A variable within the loop is written after being read, known\nas a write-after-read (WAR).\nArithmetic intensity. The number of floating-point operations (flops) relative to\nthe memory loads (data) that your application or kernel (loop) performs.\nThe arithmetic intensity is an important measure to understand the limiting\ncharacteristics of an application.\nAsymptotic notation. An expression that specifies the limiting bound on perfor-\nmance. Basically, does the run time grow linearly or worse with the size of a\nproblem? The notation uses various forms of O, such as O(n), O(n log2n) or\nO(n2). The O can be thought of as “order” as in “scales on an order of.”\nAsynchronous. This call is non-blocking and only initiates an operation.\n642 APPENDIX  CGlossary\nAuto-vectorization. The vectorization of the source code by the compiler for standard\nC, C++, or Fortran language source code.\nAVX. Advanced Vector Extensions (AVX) is a 256-bit vector hardware unit and\ninstruction set.\nAVX2. An improvement to AVX hardware to support fused multiply adds (FMA).\nAVX512. Extends the AVX hardware to 512-bit vector widths.\nBandwidth. The best rate at which data can be moved through a given path in the sys-\ntem. This can refer to memory, disk, or network throughput.\nBinary data format. The machine representation of the data that is used by the pro-\ncessor and stored in main memory. Usually this term refers to the data format\nstaying in binary form when it is written out to the hard disk.\nBlocking. An operation that does not complete until a specific condition is fulfilled.\nBranch miss. The cost encountered when the predicted branch in an if statement is\nincorrect.\nBucket. A storage location holding a collection of values. Hashing techniques are\nused to store the values for keys in a bucket because there might be multiple val-\nues for that location.\nCache. A faster block of memory that is used to reduce the cost of accessing the slower\nmain memory by storing blocks of data or instructions that might be needed.\nCache eviction. The removal of blocks of data, called cache lines, from one of the var-\nious levels of the cache hierarchy.\nCache line. The block of data loaded into cache when memory is accessed.\nCache misses. Occur when the processor tries to access a memory address and it is\nnot in the cache. The system then has to retrieve the data from main memory at\na cost of 100s of cycles of time.\nCache thrashing. A condition where one memory load evicts another and then the\noriginal data is needed again, causing loading, eviction, and reloading of data.\nCache update storms. On a multiprocessor system, when one processor modifies\ndata that is in another processor's cache, the data has to be reloaded on those\nother processors. \nCall stack. The list of called subroutines that has to be unwound by a return at the\nend of the subroutine where it jumps back to the previous calling routine.\nCapacity misses. The misses that are caused by the limited size of the cache.\n643 Glossary\nCatastrophic cancellation. The subtraction of two almost equal numbers, causing the\nresult to have only a few significant digits.\nCentralized version control system. A version control system implemented as a single\ncentralized system.\nCheckpoint/Restart. The periodic writing out of the state of an application followed\nby the starting up of the application in a later job.\nCheckpointing. The practice of periodically storing the state of a calculation to disk so\nthat the calculation can be restarted due to system failures or finite length run\ntimes in a batch system. See checkpoint/restart.\nClock cycle. The small intervals of time between operations in the computer based\non the clock frequency of the system.\nCluster. A small group of distributed memory nodes connected by a commodity\nnetwork.\nCoalesced memory loads. The combination of separate memory loads from groups\nof threads into a single cache-line load.\nCoarse-grained parallelism. A type of parallelism where the processor operates on\nlarge blocks of code with infrequent synchronization.\nCode coverage. A  m e t r i c  o f  h o w  m a n y  l i n e s  o f  t h e  s o u r c e  c o d e  a r e  e x e c u t e d  a n d ,\ntherefore, “covered” by running a test suite. It is usually expressed as a percent-\nage of the source lines of code.\nCoherency misses. Cache updates needed to synchronize the caches between multi-\nprocessors when data is written to one processor’s cache that is also held in\nanother processor’s cache.\nCold cache. A cache that does not have any of the data to be operated on in cache\nfrom a previous operation when the current operation begins.\nCollisions (hash). When more than one key wants to store its value in the same bucket.\nCommit tests. A test suite that is run prior to committing any code to the repository.\nCompact hash. A hash that is compressed into a smaller memory size. A compact\nhash must have a way to handle collisions.\nComparative speedups. Short for comparative performance speedups between architectures .\nThis is the relative performance between two hardware architectures, often\nbased on a single node or a fixed power envelope.\nCompressed sparse data structures. A space-efficient way to represent a data space\nthat is sparse. The most notable example is the Compressed Sparse Row (CSR)\nformat used for sparse matrices.\n644 APPENDIX  CGlossary\nCompulsory misses. Cache misses are those that are necessary to bring in the data\nwhen it is first encountered.\nComputational complexity. The number of steps needed to complete an algorithm.\nThis complexity measure is an attribute of the implementation and the type of\nhardware that is being used for the calculation.\nComputational kernel. A section of the application that is both computationally inten-\nsive and conceptually self-contained.\nComputational mesh. A collection of cells or elements that covers the simulation\nregion.\nCompute device (OpenCL). Any computational hardware that can perform computa-\ntion and supports OpenCL is a compute device. This can include GPUs, CPUs,\nor even more exotic hardware such as embedded processors or FPGAs.\nConcurrency. The operation of parts of a program in any order with the same result.\nConcurrency was originally developed to support concurrent computing or\ntimesharing by interleaving computing on a limited set of resources.\nConflict misses (cache). Misses caused by the loading of another block of memory\ninto a cache line that is still needed by the CPU.\nContiguous memory. Memory that is composed of an uninterrupted sequence of\nbytes.\nContinuous integration. An automatic testing process that is invoked with every com-\nmit to the repository.\nCore. Core or computational core is the basic element of the system that does the\nmathematical and logical operations.\nCPU. The discrete processing device (the central processing unit) composed of one\nor more computational cores that is placed on the socket of a circuit board to\nprovide the main computational operations.\nData parallel. A type of parallelism where the data is partitioned among the proces-\nsors or threads and operated on in parallel.\nDedicated GPU. A GPU on a separate peripheral card. Also known as a discrete GPU.\nDereferencing. An operation where the memory address is obtained from the pointer\nreference so that the cache line is for the memory data instead of for the pointer.\nDescriptive directives and clauses. These directives give the compiler information\nabout the following loop construct and give the compiler some freedom to gen-\nerate the most efficient implementation.\n645 Glossary\nDirect-mapped cache. A cache for which a memory address has only one location in\nthe cache where it can be loaded. This can lead to conflicts and evictions if\nanother block of memory also maps to this location. See N-way set associative\ncache for a type of cache that avoids this problem.\nDirective. An instruction to a Fortran compiler to help it interpret the source code.\nThe form of the instruction is a comment line starting with !$.\nDiscretization. The process of breaking up a computational domain into smaller cells\nor elements, forming a computational mesh. Calculations are then performed\non each cell or element.\nDistributed array. An array that is partitioned and split across the processors. For\nexample, an array containing 100 values might be divided up across four proces-\nsors with 25 values on each processor.\nDistributed computing. Applications and loosely coupled workflows that span multi-\nple computers and use communication across the network to coordinate the\nwork. Examples of distributed computing applications include searches via\nbrowsers on the internet and multiple clients interacting with a database on a\nserver.\nDistributed memory. More than one block of memory, each existing in its own\naddress space and control.\nDistributed version control system. A version control system that allows multiple\nrepository databases rather than a single centralized system.\nDomain-boundary halos. Halo cells used for imposing a specific set of boundary con-\nditions\nDope vector. The metadata for an array in Fortran composed of the start, stride, and\nlength for each dimension. The meaning is from the slang “give me the dope\non” or information on someone or something.\nDRAM. Dynamic Random Access Memory. This memory needs to have its state\nrefreshed frequently and the data it stores is lost when the power is turned off.\nDynamic range. The range of the working set of real numbers in a problem.\nEviction. See cache eviction.\nFine-grained parallelism. A type of parallelism where computational loops or other\nsmall blocks of code are operated on by multiple processors or threads and may\nneed frequent synchronization.\nFirst touch. The first touch of an array causes the memory to be allocated. It is allo-\ncated near to the thread location where the touch occurs. Prior to the first\n646 APPENDIX  CGlossary\ntouch, the memory only exists as an entry in virtual memory. The physical mem-\nory that corresponds to the virtual memory is created when it is first accessed.\nFlow dependency. A variable within the loop is read after being written, known as a\nread-after-write (RAW).\nFLOPs. Floating-point operations such as addition, subtraction, and multiplication on\nsingle- or double-precision data types.\nFlynn’s Taxonomy. A categorization of computer architectures based on whether the\ndata and instructions are either single or multiple.\nGather memory operation. Memory loaded into a cache line or vector unit from non-\ncontiguous memory locations.\nGeneration (PCIe). The PCI Special Interest Group (PCI SIG) is a group representing\nindustry partners that establishes a PCI Express Specification, commonly\nreferred to as generation  or gen for short.\nGhost cells. A set of cells that contain adjacent processor(s) data for use on the local\nprocessor so that the processor can operate in large blocks without issuing com-\nmunication calls.\nGlobal sum issue. The difference in a global sum in a parallel calculation compared\nto a serial or run on a different number of processors.\nGNU Compiler Collection (GCC). An open-source, publically available compiler suite,\nincluding C, C++, Fortran, and many other languages.\nGNU's Not Unix (GNU). A free, Unix-like operating system. \nGraphical user interface (GUI). An interface composed of visual elements and inter-\nactive components that can be manipulated with a mouse or other advanced\ninput devices.\nGraphics processing unit (GPU) or general-purpose graphics processing unit (GPGPU),\nintegrated or discrete (external). A device whose primary purpose is drawing graph-\nics to the computer monitor. It is composed of many streaming multiprocessors\nand its own RAM memory, capable of executing tens of thousands of threads in\none clock cycle.\nHAL. A small rogue computer that precedes IBM in lexicographic order. HAL is a fic-\nt i o n a l  c o m p u t e r  i n  A r t h u r  C .  C l a r k e ' s  2001: A Space Odyssey . HAL goes rogue\nbecause it interprets its instructions differently than intended, with deadly con-\nsequences. HAL is just one letter off from IBM. HAL’s lesson is to be careful with\nyour programming; you never know what the results might be.\nHalo cells. Any set of cells surrounding a computational mesh domain.\n647 Glossary\nHang. When one or more processors is waiting on an event that can never occur.\nHash or hashing. A computer data structure that maps a key to a value.\nHash load factor. The number of filled buckets divided by the total number of buck-\nets in the hash.\nHash sparsity. The amount of empty space in a hash.\nHeap. A region of memory for the program that is used to provide dynamic memory\nfor the program. The malloc  routines and the new operator get memory from\nthis region. The second region of memory is stack memory.\nHigh Performance Computing (HPC). Computing that focuses on extreme perfor-\nmance. The computing hardware is generally more tightly coupled. The term\nHigh Performance Computing has mostly replaced the older nomenclature of\nsupercomputing.\nHyperthreading. An Intel technology that makes a single processor appear to be two\nvirtual processors to the operating system through sharing of hardware\nresources between two threads.\nInline (routines). Rather than make a function call, compilers insert the code at the\ncall point to avoid call overhead. This only works for smaller routines and for\nsimpler code.\nInterconnects. The connections between compute nodes, also called a network . Gen-\nerally the term refers to higher performance networks that tightly couple the\noperations on a parallel computing system. Many of these interconnects are ven-\ndor proprietary and include specialized topologies such as fat-tree, switches,\ntorus, and dragonfly designs.\nInter-process communication (IPC). Communication between processes on a com-\nputer node. The various techniques to communicate between processes form\nthe backbone of client/server mechanisms in distributed computing.\nInstruction cache. The storage of instructions in fast memory close to the processor.\nInstructions can be for memory movement, or integer or floating-point opera-\ntions. The data that is operated on has its own separate data cache.\nIntegrated GPU. A graphics processor engine that is contained on the CPU.\nLambda expressions. An unnamed, local function that can be assigned to a variable\nand used locally or passed to a routine.\nLanes (vector lanes). Pathways for data in a vector operation. For a 256-bit vector\nunit operating on double-precision values, there are four lanes allowing four\nsimultaneous operations with one instruction in one clock cycle.\n648 APPENDIX  CGlossary\nLatency. The time required for the first byte or word of data to be transferred (see\nalso memory latency).\nLoad factor (hash). The fraction of a hash that is filled with entries.\nMachine balance. The ratio of flops to memory loads that a computer system can per-\nform.\nMain memory. Also called DRAM or RAM, it is the large block of memory for the\ncompute node.\nMemory latency. The time it takes to retrieve the first byte of memory from a level of\nthe memory hierarchy.\nMemory leaks. Allocating memory and never freeing it. Malloc replacement tools are\ngood at catching and reporting memory leaks.\nMemory overwrites. Writing to memory that is not owned by a variable in the program.\nMemory paging. In multi-user, multi-application operating systems, the process of\nmoving memory pages temporarily out to disk so that another process can take\nplace.\nMemory pressure. The effect of the computational kernel resource needs on perfor-\nmance of GPU kernels. Register pressure is a similar term, referring to demands\non registers in the kernel.\nMethod invocation. In object-oriented programming, the call to a piece of code\nwithin the object that operates on data in the object. These small pieces of code\nare called methods and the call to these is termed an invocation.\nMIMD. Multiple instruction, multiple data is a component of Flynn’s Taxonomy rep-\nresented by a multi-core system.\nMinimal perfect hash. A hash with one and only one entry in each bucket.\nMISD. Multiple instruction, single data is a component of Flynn’s Taxonomy describ-\ning a redundant computer for high reliability or a parallel pipeline parallelism.\nMMX. Earliest x86 vector instruction set released by Intel.\nMotherboard. The main system board of a computer.\nMulti-core. A CPU that contains more than one computational core.\nNetwork. The connections between compute nodes over which data flows.\nNode. A basic building block of a compute cluster with its own memory and a network\nto communicate with other compute nodes and to run a single image of an oper-\nating system.\n649 Glossary\nNon-Uniform Memory Access (NUMA). On some computing nodes, blocks of mem-\nory are closer to some processors than others. This situation is called Non-Uni-\nform Memory Access (NUMA). Often this is the case when a node has two CPU\nsockets with each socket having its own memory. The access to the other block of\nmemory typically takes twice the time as its own memory.\nN-way set associative cache. A cache that allows N locations for a memory address\nto be mapped into the cache. This reduces the conflicts and evictions associated\nwith direct-mapped cache.\nObject-based filesystem. A system that is organized based on objects rather than\nbased on files in a folder. An object-based filesystem requires a database or meta-\ndata to store all the information describing the object.\nOperations (OPs). Operations can be integer, floating-point, or logic.\nOut-of-bounds (memory access). Attempting to access memory beyond the array\nbounds. Fence-post checkers and some compilers can catch these errors.\nOutput dependency. A variable is written to more than once in the loop.\nPageable memory. Standard memory allocations that can be paged out to disk. See\npinned memory for an alternative type that cannot be paged out.\nParallel algorithm. A well-defined, step-by-step computational procedure that empha-\nsizes concurrency to solve a problem.\nParallel computing. Computing that operates on more than one thing at a time.\nParallel pattern. A common, independent, concurrent component of code that\noccurs in diverse scenarios with some frequency. By themselves, these compo-\nnents generally do not solve complete problems of interest.\nParallel speedup. Performance of a parallel implementation relative to a baseline\nserial run.\nParallelism. The operation of parts of a program across a set of resources at the same\ntime.\nPattern rule. A specification to the make utility that gives a general rule on how to\nconvert any file with one suffix pattern to a file with another suffix pattern.\nPCI bus. Peripheral Component Interconnect bus is the main data pathway between\ncomponents on the system board, including the CPU, main memory, and the\ncommunication network.\nPeel loop. A loop to execute for misaligned data so that the main loop would then\nhave aligned data. Often the peel loop is conditionally executed at run time if\nthe data is discovered to be misaligned.\n650 APPENDIX  CGlossary\nPerfect hash. A hash where there are no collisions; there is at most one entry in each\nbucket.\nPerfectly nested loops. Loops that only have statements in the innermost loop. That\nmeans that there are no extraneous statements before or after each loop block.\nPerformance model. A simplified representation of how the operations in a program\ncan be converted into an estimate of the code’s run time.\nPinned memory. Memory that cannot be paged out from RAM. It is especially useful\nfor memory transfers because it can be directly sent without making a copy.\nPOSIX standard. The Portable Operating System Interface (POSIX) standard is an\nIEEE standard for Unix and Unix-like operating systems to facilitate portability.\nThe standard specifies the basic operations that should be provided by the OS.\nPragma. An instruction to a C or C++ compiler to help it interpret the source code.\nThe form of the instruction is a preprocessor statement starting with #pragma .\nPrescriptive directives and clauses. These are directives from the programmer that\ntell the compiler specifically what to do. \nPrivate variable (OpenMP). In the context of OpenMP, a private variable is local and\nonly visible to its thread.\nProcess. An independent unit of computation that has ownership of a portion of\nmemory and control over resources in user space. \nProcessing core or (simply) core. The most basic unit capable of performing arith-\nmetic and logical operations.\nProfilers. A programming tool that measures the performance of an application.\nProfiling. The run-time measurement of some aspects of application performance;\nmost commonly, the time it takes to execute parts of a program.\nRace conditions. A situation where multiple outcomes are possible and the result is\ndependent on the timing of the contributors.\nRandom access memory (RAM). Main system memory where any needed data can be\nretrieved without having to read sequentially through the data.\nReduction operation. Any operation where a multidimensional array from 1 to N\ndimensions is reduced to a least one dimension smaller and often to a scalar\nvalue.\nRegister pressure. Register pressure refers to the effect of register needs on the per-\nformance of GPU kernels.\n651 Glossary\nRegression tests. Test suites that are run at periodic intervals such as nightly or\nweekly.\nRelaxed memory model. The value of the variables in main memory or caches of all\nthe processors are not updated immediately.\nRemainder loop. A loop that executes after the main loop to handle a partial set of\ndata that is too small for a full vector length.\nRemote procedure call (RPC). A call to the system to execute another command.\nReplicated array. A dataset that is duplicated across all the processors.\nScalar operation. An operation on a single value or one element of an array.\nScatter memory operation. Store from a cache line or vector unit to non-contiguous\nmemory locations.\nShared memory. A block of memory that is accessible and modifiable by multiple pro-\ncesses or threads of execution. The block of memory is from the programmer’s\nperspective.\nShared variable (OpenMP). In the context of OpenMP, a shared variable is visible and\nmodifiable by any thread.\nSIMD. Single instruction, multiple data is a component of Flynn’s Taxonomy describ-\ning a parallelism such as that found in vectorization, where a single instruction is\napplied across multiple data items.\nSIMT. Single instruction, multiple thread is a variant of SIMD, where multiple threads\noperate concurrently on multiple data.\nSISD. Single instruction, single data is a component of Flynn’s Taxonomy that describes\na traditional serial architecture.\nSocket. The location where a processor is inserted on a motherboard. Motherboards\nnormally are either single or dual socket, allowing one or two processors to be\ninstalled, respectively.\nSource code repository. Storage for source code that tracks changes and can be shared\nbetween a project’s code developers.\nSpatial locality. Data with nearby locations in memory that are often referenced close\ntogether.\nSSE. Streaming SIMD Extensions, a vector hardware and instruction set released by\nIntel that first supported floating-point operations.\nSSE2. An improved SSE instruction set that supports double-precision operations.\n652 APPENDIX  CGlossary\nStack memory. Memory within a subroutine is often created by pushing the objects\nonto the stack after the stack pointer. These are usually small memory objects\nthat exist for only the duration of the routine and disappear at the end of the\nroutine when the instruction pointer jumps back to the previous location.\nStreaming kernels. Blocks of computational code that load data in a nearly optimal\nway to effectively use the cache hierarchy.\nStreaming multiprocessor (SM). Usually used to describe the multiprocessors of a\nGPU that are designed for streaming operations. These are tightly-coupled, sym-\nmetric processors (SMP) that have a single instruction stream operating on mul-\ntiple threads.\nStreaming store. A store of a value directly to main memory, bypassing the cache\nhierarchy.\nStride (arrays). Distance between indexed elements in an array. In C, in the x dimen-\nsion, the data is contiguous or a stride of 1. In the y dimension, the data has a\nstride of the length of the row.\nSuper-linear speedup. Performance that is better than the ideal strong scaling curve.\nThis can happen because the smaller array sizes fit into a higher level of cache,\nresulting in better cache performance.\nSymmetric processors (SMP). All cores of the multicore processor operate in unison\nin a single-instruction, multiple-thread (SIMT) fashion.\nTask. Work that is divided into separate pieces and parceled out to individual pro-\ncesses or threads.\nTask parallel. A form of parallelism where processors or threads work on separate\ntasks.\nTemporal locality. Recently referenced data that is likely to be referenced in the near\nfuture.\nTest-driven development (TDD). A process of code development where the tests are\ncreated first.\nTest suite. A set of problems that exercise parts of an application to guarantee that\nparts of the code are still working.\nThread. A separate instruction pathway through a process created by having more\nthan one instruction pointer.\nTightly-nested loops. Two or more loops that have no extra statements between the\nfor or do statements or the end of the loops.\n653 Glossary\nTime complexity. Time complexity takes into account the actual cost of an operation\non a typical modern computing system. The largest adjustment for time is to\nconsider the cost of memory loads and caching of data.\nTranslation lookaside buffer (TLB). The table of entries to translate virtual memory\naddresses to physical memory. The limited size of the table means that only\nrecently used page locations are held in memory, and a TLB miss occurs if it is\nnot present, incurring a significant performance hit.\nUnified memory. Memory that has the appearance of being a single address space for\nboth the CPU and the GPU.\nUnit testing. Testing of each individual component of a program.\nUninitialized memory. Memory that is used before its values are set.\nUser space. The scope of control of operations for a program such that it is isolated\nfrom the purview of the operating system.\nValidated results. Results of a calculation that are compared favorably to experimen-\ntal or real-world data.\nVector (SIMD) instruction set. The set of instructions that extend the regular scalar\nprocessor instructions to utilize the vector processor.\nVector lane. A pathway through a vector operation on vector registers for a single\ndata element much like a lane on a multi-lane freeway.\nVector length. The number of operations done in a single cycle by a vector unit.\nVector operation. An operation on two or more elements of an array with a single\noperation or instruction being supplied to the processor.\nVector width. The width of the vector unit, usually expressed in bits.\nVectorization. The process of grouping operations together so more than one can be\ndone at a time. \nVersion Control System. A database that tracks the changes to your source code, simpli-\nfies the merging of multiple developers, and provides a way to roll back changes.\nWarm cache. When a cache has data to be operated on in the cache from a previous\noperation as the current operation begins.\nWarp. An alternate term for a thread workgroup.\nWord (size). The size of the basic type being used. For single precision, this is four\nbytes and for double, it is eight bytes. \nWorkgroup. A group of threads operating together with a single instruction queue.\n654 APPENDIX  CGlossary\nNVIDIA GPU\nPCI slots\nCMOS\n(battery)\nBIOS\nMotherboardMain memory\n(DRAM)CPU fanSocket\n(under fan)\nFigure C.1 Desktop motherboard with Intel CPU and discrete NVIDIA GPU\nThermal Paste\nSocket\nCPU retention\nleverCPU pinsCPU (bottom)\nCPU (top)\nFigure C.2 Intel CPU installed in socket and underside with CPU data pins. The data transfer to \nthe CPU is limited by the number of pins that can be physically fit onto the surface of the CPU.",29339
257-index.pdf,257-index,,0
258-Symbols.pdf,258-Symbols,,0
259-B.pdf,259-B,"655index\nSymbols\n!#acc routine directive 394\nA\nAccelerated Processing Unit \n(APU) 312, 368, 589\naccelerator devices 24, 311\nacc enter data create \ndirective 473\nacc exit data directive 473\nacc keyword 373\nadaptive mesh refinement \n(AMR) 133\naddress generation units \n(AGUs) 118\nADIOS (Adaptable Input/\nOutput System) 568\nAdios package 568\nAdvanced Vector Extensions \n(AVX) 117, 177\naffinity 491–527\nchanging process affinities \nduring run time 522–524\ncontrolling from command \nline 516–520\nhwloc-bind 516 –518\nlikwid-pin 518 –520\nfor MPI plus OpenMP\n511–516\nimportance of 492–493\nprocess affinity with MPI\n503–511\nbinding processes to hard-\nware components 511default process placement \nwith OpenMPI 504\nmapping processes to pro-\ncessors or other \nlocations 510\nordering of MPI ranks\n510–511\nspecifying process place-\nment in OpenMPI\n504–509\nresources for 525–526\nsetting affinities in \nexecutable 521–522\nthread affinity with OpenMP\n495–503\nunderstanding architecture\n493–494\nAGUs (address generation \nunits) 118\nAI (artificial intelligence) 3\nalgorithmic complexity 125\nalgorithms 4, 124–169\ndefined 130\nfuture of research 167\nhash functions, defined\n131–132\noverview 125–126\nparallel global sum\n161–166\nperformance models vs. algo-\nrithmic complexity\n126–130\nprefix sum pattern 157–161\nredesigning for parallel 53\nresources for 168\nspatial hashing 132–157task-based support \nalgorithm 250–251\naliasing 183\nAllinea/ARM Map 248\nALUs (arithmetic logic \nunits) 351\nAmdahl’s Law 11–12\nAMR (adaptive mesh \nrefinement) 133\nAoS (Array of Structures) 197\nperformance assessment 96\nSoA vs. 94–100\nAoSoA (Array of Structures of \nArrays) 100–101\napplication/software model\n25–29\nprocess-based \nparallelization 26–27\nstream processing through \nspecialized \nprocessors 28–29\nthread-based \nparallelization 27–28\nvectorization 28\nAPU (Accelerated Processing \nUnit) 312, 368, 589\nARB (Architecture Review \nBoard) 371\nArcher 600–601\narithmetic intensity 60\narithmetic logic units \n(ALUs) 351\nArray of Structures. See AoS\nartificial intelligence (AI) 3\nassembler instructions\n195–196",2171
260-D.pdf,260-D,"INDEX 656\nassociativity, addressing with \nparallel global sum\n161–166\nasymptotic notation 125\nasynchronous calls 261, 264\nasynchronous operations, in \nOpenACC 394\natomic directive 412\nAtomic Weapons Establishment \n(AWE) 593\nauto-vectorization 179–183\nAVX (Advanced Vector \nExtensions) 117, 177\nAWE (Atomic Weapons \nEstablishment) 593\nB\nbandwidth\ncalculating machine balance \nbetween flops and 71\nempirical measurement \nof67–69\nGPUs\nachieved bandwidth 480\ncalculating theoretical \npeak 319 –320\nPCI bus 326–329\nmaximum transfer \nrate 327 –328\noverhead rates 328\nPCIe lanes 327\nreference data for 329\ntheoretical memory \nbandwidth 66–67\nBasic Linear Algebra System \n(BLAS) 179\nbatch schedulers 528–546\nautomatic restarts 539–542\nchaos of unmanaged \nsystems 529–530\ncommon HPC pet \npeeves 531–532\nlayout of batch system for busy \nclusters 530–531\nresources for 545\nspecifying dependencies in \nbatch scripts 543–544\nsubmitting batch scripts\n532–536\nBeeGFS 576\nbenchmarking 52, 62–71, \n591\nbenchmark application for \nPCI bus 329–332calculating machine balance \nbetween flops and \nbandwidth 71\ncalculating theoretical maxi-\nmum flops 65\nempirical measurement of \nbandwidth and flops\n67–69\nmeasuring GPU stream \nbenchmark 321\nmemory hierarchy and theo-\nretical memory \nbandwidth 66–67\ntools for gathering system \ncharacteristics 62–64\nbinding 301, 491\nbins 131\nBLAS (Basic Linear Algebra \nSystem) 179\nblocking 261\nbottlenecks 585\nbranch miss 107\nbranch penalty (Bp) 107\nbranch prediction cost (Bc) 106\nbubble sort 130\nbuckets 131\nbuild command 481\nburst buffer 548\nC\ncache conflict 102\ncache line 61, 67, 102\ncache misses 101–105\ncache thrashing 102\ncache update storms 105\ncache used (Ucache) 61\ncall graphs 72–78\ncapacity misses 102\nCartesian topology, support for \nin MPI 292–296\ncatastrophic cancellation 161\nC ceil function 358, 426\ncell-centric compressed sparse \nstorage 112–114\ncentralized version control 38, \n583\ncentral processing unit \n(CPU) 7, 310\nCEPH filesystem 576\ncheckpointing 532, 539, 547\nCLAMR application 593\nclang-archer command 600\nclinfo command 439\nclock_gettime function 585clone command 582\nclose keyword 495\ncloud computing\ncost reduction with GPUs 342\nGPU profiling 485–486\nCloverLeaf mini-app 593\nCMake module\nautomatic testing with 41–45\nsetting compiler flags\n200–201\ncoalesced memory loads 359\ncoalescing 359\ncoarse-grained parallelism 228\ncode modularity 53\ncode portability, improving 50\ncodesign process 592\nCodeXL suite 478\ncoherency, defined 105\ncold cache 105, 226\nCollaborative Testing System \n(CTS) 48\ncollective_buffering \noperation 555\ncollective file operation 552\ncollisions 132\ncolumn major 90\nCoMD (molecular dynamics) \nmini-app 593\ncomm groups 302\ncommiting workflow step 55\ncommunicators 256\ncompact hashing\nface neighbor finding\n152–153\nneighbor finding 149–151\nremaps 153–157\ncomparative speedup 31–32\ncompiler flags 198–201\ncompiler hints 183–190\ncompiler wrappers 256–257\nCompressed Sparse Row \n(CSR) 105\ncompressed sparse storage \nrepresentations 112–116\ncell-centric compressed sparse \nstorage 112–114\nmaterial-centric compressed \nsparse storage 114–116\ncomputational kernel 36\ncomputational mesh 17\ncompute device 314\ncompute directive 389\nCompute Unified Device Archi-\ntecture. See CUDA\ncompute units (CUs) 315, 479",3425
261-F.pdf,261-F,"INDEX 657\nconcurrency 4\nConcurrent Versions System \n(CVS) 583\nconflict misses 480\ncost reduction\nin cloud computing with \nGPUs 342\nparallel computing and\n10–11\nCPU (central processing \nunit) 7, 310\nCPU RAM 310\nCray compiler 374\ncross-node parallel method\n22–23\nCSR (Compressed Sparse \nRow) 105\nCTest, automatic testing \nwith 41–45\nCTS (Collaborative Testing \nSystem) 48\nCUB (CUDA UnBound) 435\nCUDA (Compute Unified \nDevice Architecture) 311\nGPU languages and 420–438\nHIPifying code 435 –438\nreduction kernel 429 –435\nwriting and building \napplications 421 –429\ninteroperability with \nOpenACC 395–396\ncudaFree 427\nCUDA-GDB 604\ncudaHostMalloc function 429\nCUDPP (CUDA Data Parallel \nPrimitives Library) 161\nCUs (compute units) 315, 479\nCVS (Concurrent Versions \nSystem) 583\nD\nDAOS (distributed application \nobject storage) 576\nDarshan libraries 605\ndata hazards 599\ndata-oriented design 88–101\nAoS\nperformance assessment\n96\nSoA vs. 94 –100\nAoSoA 100–101\nmultidimensional arrays\n90–94\nresources for 122–123SoA\nAoS vs. 94 –100\nperformance assessment\n96–100\ndata parallel approach 16–22\ndefining computational ker-\nnel or operation 17–18\ndiscretizing problem 17\noff-loading calculation to \nGPUs 21–22\nprocesses 19–21\nthreads 19\nvectorization 19\nData Parallel C++ (DPCPP) \ncompiler 449\ndata pragma 385\ndataset 560\nDataWarp 575\nddt command 602\n.deb (Debian Package \nManager) 607\ndebuggers 602–604\nDDT 602\nGPU debuggers 603–604\nCUDA-GDB 604\nROCgdb 604\nLinux debuggers 603\nTotalView debugger 602\ndeclare target directive 411\ndedicated GPUs 313\ndependency analysis, call graphs \nfor72–78\ndereferencing 99\ndescriptive directives and \nclauses 414\ndifferential discretized data 134\ndimensions, in OpenCL 357\nDIMMs (dual in-line memory \nmodules) 310\ndirective-based GPU \nprogramming 371–416\nOpenACC 374–396\nadvanced techniques\n393–396\ncompiling code 375 –377\noptimizing kernels 387 –393\nparallel compute \nregions 377 –383\nsummary of performance \nresults for stream \ntriad 393\nusing directives 383 –387\nOpenMP 396–414\nadvanced techniques\n411–414compiling code 397 –398\ncreating data regions\n402–406\ngenerating parallel \nwork 398 –402\noptimizing 406 –410\nprocess to apply directives and \npragmas for GPU \nimplementation 373–374\nresources for 414–415\nOpenACC 415\nOpenMP 415\ndirectives 208\ndirect-mapped cache 102\ndiscrete GPUs 313\ndiscretizing problems 17\ndistributed application object \nstorage (DAOS) 576\ndistributed arrays 13\ndistributed memory \narchitecture 22–23\ndistributed version control 38, \n582–583\nDocker containers 480–483\ndomain-boundary halos 278\ndope vector 91\ndouble hashing 151\nDPCPP (Data Parallel C++ ) \ncompiler 449\nDRAM (Dynamic Random \nAccess Memory) 22, 310\nDraw_Line function 89\nDr. Memory 595–597\ndual in-line memory modules \n(DIMMs) 310\ndynamic memory \nrequirements 343\nDynamic Random Access Mem-\nory (DRAM) 22, 310\ndynamic range 162\nE\nECM (Execution Cache \nMemory) 116\nedge compute 9\nempirical bandwidth (BE) 61, \n67\nempirical machine balance \n(MBE) 71\nenergy efficiency\nwith GPUs 337–342\nwith parallel computing 9–10\nenter/exit data directive 385\nenter data directive 385",3241
262-I.pdf,262-I,"INDEX 658\nenter directive 391\nEpetraBenchmarkTest mini-app\n593\nEUs (execution units) 316\neviction 102\nExaMiniMD 592\nExascale Project proxy \napps 592–593\nexclusive command 535\nExecution Cache Memory \n(ECM) 116\nexecution dependency 480\nexit data directive 385\nexit directive 391\nexport command 604\nEZCL library 438\nF\nfeeds 59\nFFT (Fast Fourier \ntransform) 179\nfield-programmable gate arrays \n(FPGAs) 314, 349, 438\nfile operations 547–578\ncomponents of high-perfor-\nmance filesystem\n548–549\nhardware interface\n568–576\nBeeGFS 576\nCEPH filesystem 576\nDataWarp 575\ndistributed application \nobject storage 576\ngeneral hints 572 –573\nGeneral Parallel File \nSystem 575\nLustre filesystem 574 –575\nnetwork filesystem 576\nOrangeFS 576\noverview 568 –570\nPanasas 575 –576\nWekaIO 576\nHDF5 559–566\nMPI-IO 551–559\nparallel-to-serial \ninterface 549–550\nprofiling 604–607\nresources for 577–578\nfilled fraction (Ff) 109\nfind_package command 437\nCUDA 422\nHDF5 567\nHIP 436–437Kokkos 453\nMPI 44, 258\nOpenACC 376\nOpenCL 440\nOpenMP 456, 600\nRaja 456\nVector 200\nfine-grained parallelization 228\nfirst touch 209, 495\nflops (floating-point \noperations) 59, 87\ncalculating machine balance \nbetween bandwidth \nand 71\ncalculating peak theoretical \nflops 316–318\ncalculating theoretical \nmaximum 65\nempirical measurement \nof67–69\nflow dependency 188\nflush operation 210\nFlynn’s Taxonomy 29\nFMA (fused multiply-add) 65, \n118\nfor pragma 238\nFPGAs (field-programmable \ngate arrays) 314, 349, 438\nfree function 427\nfull matrix data \nrepresentations 109–111\nfull matrix cell-centric \nstorage 109–110\nfull matrix material-centric \nstorage 110–111\nfunction calls, MPI 256\nbarrier 267\nbroadcast 268–269\ngather 273–276\nreduction 269–273\nscatter 274–276\nfunction-level OpenMP\n229–231\nfused multiply-add (FMA) 65, \n118\nG\ngangs 387\ngang scheduling 492\ngather/scatter memory load \noperation 119\ngather operations\nputting order in debug \nprintouts 273–274sending data out to processes \nfor work 274–276\nGCC (GNU Compiler \nCollection) 40, 42, 374\nGCP (Google Cloud \nPlatform) 485\ngen (generation) 327\ngeneral heterogeneous \nparallel architecture \nmodel 24–25\nGeneral Parallel File System \n(GPFS) 575\ngeneral-purpose graphics pro-\ncessing unit (GPGPU) 24, \n311\nget_global_id function 355\ngetrusage function 585\ngettimeofday function 585\nghost cell exchanges\nin 2D mesh 277–285\nin 3D stencil calculation\n285–286\nperformance tests of \nvariants 297\nglobal information 357\nglobal sum, using OpenMP \nthreading 227\nglobal sum issue 162\nGNU Compiler Collection \n(GCC) 40, 42, 374\nGoogle Cloud Platform \n(GCP) 485\nGPFS (General Parallel File \nSystem) 575\nGPGPU (general-purpose \ngraphics processing \nunit) 24, 311\nGPU (graphics processing unit) \nlanguages 417–459\nCUDA and 420–438\nHIPifying code 435 –438\nreduction kernel 429 –435\nwriting and building \napplications 421 –429\nfeatures of 419–420\nhigher-level languages\n452–457\nKokkos 452 –455\nRAJA 455 –457\nOpenCL 438–449\nreductions in 445 –449\nwriting and building \napplications 439 –445\nresources for 457–458\nSYCL 449–452\nINDEX 659\nGPU (graphics processing unit) \nprofiling and tools 460–487\ncloud options 485–486\nDocker containers 480–483\nmetrics 479–480\nachieved bandwidth 480\nissue efficiency 480\noccupancy 479\noverview 460–461\nprofiling workflow 467–478\nCodeXL suite 478\ndata movement \ndirectives 473 –474\nguided analysis 474 –475\nNVIDIA Nsight suite\n476–477\nOpenACC compute \ndirectives 471 –473\nprofile CPU code 470\nrunning application\n467–469\nresources for 486–487\nselecting good workflow\n462–463\nshallow water simulation \nexample 463–467\nvirtual machines 483–485\nGPU (graphics processing unit) \nprogramming model\n346–370\nasynchronous computing \nthrough queues 365–366\ncode structure for 355–360\naddressing memory \nresources 359 –360\nindex sets 358\nparallel kernel 356 –357\nthread indices 357 –358\ndeveloping plan to parallelize \napplications for 366–368\n3D atmospheric \nsimulation 367\nunstructured mesh \napplication 368\ndirective-based GPU \nprogramming 371–416\nOpenACC 374 –396\nOpenMP 396 –414\nprocess to apply directives \nand pragmas for GPU \nimplementation\n373–374\nresources for 414 –415\nGPU programming \nabstractions 354–355optimizing GPU resource \nusage 361–362\noccupancy 362\nregisters used by kernel\n361\nprogramming \nabstractions 348–355\ninability to coordinate \namong tasks 349\nmassive parallelism 348\nreduction pattern 364–365\nresources for 369–370\nterminology 349\ndata decomposition into \nindependent units of \nwork 350 –352\nsubgroups, warps, and \nwavefronts 353 –354\nwork groups 353\nwork items 354\nGPU RAM 310\nGPUs (graphics processing \nunits) 309–345\nas thread engine 313–318\ncalculating peak theoretical \nflops 316 –318\ncompute unit 316\nmultiple data operations by \neach element 316\nprocessing elements \n(PEs) 316\ncharacteristics of GPU mem-\nory spaces 318–326\ncalculating theoretical peak \nmemory bandwidth\n319–320\nmeasuring GPU stream \nbenchmark 321\nroofline performance \nmodel for GPUs 322\nusing mixbench perfor-\nmance tool to choose \nbest GPU for workload\n324–326\nCPU-GPU system as acceler-\nated computational \nplatform 311–313\ndedicated GPUs 313\nintegrated GPUs 312\ndebuggers 603–604\nCUDA-GDB 604\nROCgdb 604\nmemory tools 599\nmulti-GPU platforms and \nMPI 332–334higher performance alter-\nnative to PCI bus 334\noptimizing data movement \nbetween graphics pro-\ncessing units (GPUs) \nacross network 333\nPCI bus 326–332\nbenchmark application \nfor 329 –332\ntheoretical bandwidth \nof 326 –329\npotential benefits of GPU-\naccelerated \nplatforms 334–342\ncloud computing cost \nreduction 342\nreducing energy use\n337–342\nreducing time-to-\nsolution 335 –336\nresources for 343–344\nwhen to use 343\ngraded mesh 136\ngroup information, in \nOpenCL 357\nGustafson-Barsis’s Law 12–15\nH\nH5Dclose command 560\nH5Dcreate2 command 560\nH5Dopen2 command 560\nH5Dread command 561\nh5dump command 559\nH5Dwrite command 561\nH5Fclose command 560\nH5Fcreate command 560\nH5Fopen command 560\nh5ls command 559\nH5Pclose command 561\nH5Pcreate command 561\nH5Pset_all_coll_metadata_ops \ncommand 561\nH5Pset_coll_metadata_write \ncommand 561\nH5Pset_dxpl_mpio command\n561\nH5Pset_fapl_mpio command\n561\nH5Sclose command 560\nH5Screate_simple command\n560\nH5Sselect_hyperslab command\n560\nhalo cells 278",6485
263-J.pdf,263-J,,0
264-K.pdf,264-K,,0
265-O.pdf,265-O,"INDEX 660\nhalo updates 278\nhangs 261\nhardware model 22–25\naccelerator devices 24\ndistributed memory \narchitecture 22–23\ngeneral heterogeneous paral-\nlel architecture \nmodel 24–25\nshared memory architecture\n23\nvector units 23–24\nhash functions\ndefined 131–132\nprefix sum pattern 157–161\nfor large arrays 160 –161\nstep-efficient parallel scan \noperation 158\nwork-efficient parallel \nscan operation\n159–160\nspatial hashing 132–157\ncompact hashing\n149–157\nperfect hashing 135 –148\nhash load factor 150\nhash sort 130\nhash sparsity 150\nHBM2 (High-Bandwidth \nMemory) 319\nHC (Heterogeneous Compute) \ncompiler 349\nHDF (Hierarchical Data \nFormat) 559\nHDF5 (Hierarchical Data For-\nmat v5) 559–566\nheaps 95\nHeterogeneous Compute (HC) \ncompiler 349\nHeterogeneous Interface for \nPortability (HIP) 349, \n435\nHierarchical Data Format \n(HDF) 559\nHierarchical Data Format v5 \n(HDF5) 559–566\nHigh-Bandwidth Memory \n(HBM2) 319\nhigh-level OpenMP 218, 237\nexample of 234–237\nimplementing 232–234\nimproving parallel scalability \nwith 231\nHigh Performance Computing \n(HPC) 10, 89High Performance Conjugate \nGradient (HPCG) 52\nHIP (Heterogeneous Interface \nfor Portability) 349, \n435\nHIP_ADD_EXECUTABLE \ncommand 437\nhost 355\nhost_data directive 395\nhost_data use_device(var) \ndirective 396\nhot-spot analysis, call graphs \nfor72–78\nHPC (High Performance \nComputing) 10, 89\nHPCG (High Performance \nConjugate Gradient) 52\nhwloc-bind 516–518\nhybrid threading 237–240\nhydrostatic pressure 465\nhyperthreading 493\nI\nICD (Installable Client Driver)\n439\nimplementation workflow \nstep 54\ninclude directive 598\nindependent file operation\n552\nindex sets 358\ninlining 89\nInstallable Client Driver (ICD)\n439\nintegrated GPUs 312\nIntel Inspector 248–249, 600\ninter-process communication \n(IPC) 27\nIPC (inter-process \ncommunication) 27\nirregular memory access 343\nJ\nJIT (just-in-time) \ncompiling 420, 441\nK\nKahan summation \nimplementation 244\nkernels directive 381–382\nkernels pragma 378–380\nKokkos 452–455L\nLaghos 592\nlambda expressions 356\nlanes (vector lanes) 23\nLAPACK (linear algebra \npackage) 179\nlatency 59\nLawrence Livermore National \nLaboratory proxies 593\nLD_LIBRARY_PATH 610\nlikwid (“Like I Knew What I’m \nDoing”) 518, 586\nlikwid-mpirun command 519\nlikwid-perfctr markers 78\nlikwid-pin 518\ncontrolling affinity 518–520\npinning MPI ranks 519–520\npinning OpenMP \nthreads 518–519\nlikwid-powermeter \ncommand 83\nlinear algebra package \n(LAPACK) 179\nLinux debuggers 603\nList under Mantevo suite\n593–594\nLmod 612\nload command 194\nload factor 132\nlocal (tile) information 357\nlogin nodes 530\nloop cost (Lc) 107\nloop directive 382, 389, 413\nloop-level OpenMP 217–228\nperformance of 226–227\npotential issues with\n227–228\nreduction example of global \nsum using OpenMP \nthreading 227\nstencil example 224–226\nstream triad example\n222–224\nvector addition example\n220–222\nloop penalty (Lp) 107\nloop pragma 381\nLos Alamos National Labora-\ntory proxy applications 593\nlscpu command 64, 494, 504, \n506, 510\nlspci command 64, 331\nlstopo command 510, 516\nLustre filesystem 574–575\nINDEX 661\nM\nmachine balance 60\nmachine-specific registers \n(MSR) 76\nmake test command 47\n–mapby ppr:N:socket:PE=N \ncommand 513\nmap directive 402\nmaster keyword 496\nmaterial-centric compressed \nsparse storage 114–116\nMath Kernel Library (MKL) 179\nMDS (Metadata Servers) 575\nMDT (Metadata Targets) 575\nmembytes 113\nmemops (memory loads and \nstores) 106, 113\nmemory bandwidth 67\nmemory channels (Mc) 66\nmemory dependency 480\nmemory error detection and \nrepair 594–599\ncommercial memory \ntools 597\ncompiler-based memory \ntools 597\nDr. Memory 595–597\nfence-post checkers 597–599\nGPU memory tools 599\nValgrind Memcheck 594\nmemory handling 420\nmemory latency 67\nmemory leaks 594\nmemory paging 429\nmemory pressure 361\nmemory throttle 480\nmemory transfer rate (MTR) 66\nmeshes\ncompact hashing for spatial \nmesh operations 149–157\nface neighbor finding\n152–153\nneighbor finding 149 –151\nremap calculations\n153–157\ndefining computational ker-\nnel to conduct on each \nelement of mesh 17–18\nghost cell exchanges in 2D \nmesh 277–285\nperfect hashing for spatial \nmesh operations 135–148\nneighbor finding 135 –141\nremap calculations 142sorting mesh data 146 –148\ntable lookups 143\ntable lookups using spatial \nperfect hash 145 –146\nunstructured mesh \napplications 368\nmessage passing 26–27\nMessage Passing Interface. See \nMPI\nMetadata Servers (MDS) 575\nMetadata Targets (MDT) 575\nMIMD (multiple instruction, \nmultiple data) 29\nMiniAero app 593\nminiAMR (adaptive mesh \nrefinement) 592\nmini-apps 52, 591–594\nExascale Project proxy \napps 592–593\nLawrence Livermore National \nLaboratory proxies 593\nLos Alamos National Labora-\ntory proxy \napplications 593\nSandia National Laboratories \nMantevo suite 593–594\nminiFE app 593\nminiGhost app 593\nminimal perfect hash 131\nminiQMC (Quantum Monte \nCarlo) 592\nminiSMAC2D app 593\nminiXyce app 593\nMISD (multiple instruction, sin-\ngle data) 29\nmixbench performance \ntool 324–326\nMKL (Math Kernel Library) 179\nmodularity 37\nmodule avail command 609\nmodule list command 609\nmodule purge command 609\nmodules 609–612\nLmod 612\nTCL modules 612\nmodule show command 610\nmodule show <module_name> \ncommand 609\nmodule swap <module_name> \n<module_name> \ncommand 610\nmodule unload <mod-\nule_name> command 610\nmolecular dynamics (CoMD) \nmini-app 593motherboard 66\nMPI (Message Passing Interface)\n27, 42, 254–304, 491\nadvanced functionality\n286–297\nCartesian topology support \nin 292 –296\ncustom data types 287 –291\nperformance tests of ghost \ncell exchange \nvariants 297\nbasics for minimal \nprogram 255–259\ncompiler wrappers 256 –257\nfunction calls 256\nminimum working \nexample 257 –259\nparallel startup \ncommands 257\ncollective \ncommunication 266–276\nbarrier 267\nbroadcast 268 –269\ngather 273 –276\nreduction 269 –273\nscatter 274 –276\ndata parallel examples\n276–286\nghost cell exchanges in 2D \nmesh 277 –285\nghost cell exchanges in 3D \nstencil calculation\n285–286\nstream triad to measure \nbandwidth on node 276\nfile operations 551–559\nhybrid MPI plus \nOpenMP 299–302\nbenefits of 299 –300\nMPI plus OpenMP\n300–302\nmulti-GPU platforms \nand 332–334\nhigher performance alter-\nnative to PCI bus 334\noptimizing data movement \nbetween GPUs across \nnetwork 333\nplus OpenMP 218–219, \n300–302, 511–516\nprocess affinity with 503–511\nbinding processes to hard-\nware components 511\ndefault process placement \nwith OpenMPI 504\nINDEX 662\nMPI (Message Passing Interface) \n(continued)\nmapping processes to pro-\ncessors or other \nlocations 510\nordering of MPI ranks\n510–511\nspecifying process place-\nment in OpenMPI\n504–509\nresources for 303\nsend and receive commands \nfor process-to-process \ncommunication\n259–266\nMPI_Allreduce 270, 273\nMPI_Barrier 267\nMPI_Bcast 268–269\nMPI_BYTE 265\nMPI_Cart_coords 292\nMPI_Cart_create 292\nMPI_Cart_shift 292\nmpicc command 256–257\nMPICH (ROMIO) \ncommand 575\nMPI_COMM_WORLD \n(MCW) 266, 269, 508\nmpicxx command 257\nMPI_Dims_create function 292\nMPI_DOUBLE 285\nmpiexec command 257\nMPI_File 551\nMPI_File_close command 551\nMPI_File_delete command 551\nMPI_File_open command 551\nMPI_File_read_all command 552\nMPI_File_read_at_all \ncommand 552\nMPI_File_read_at command\n552\nMPI_File_read command 552\nMPI_File_seek command 551\nMPI_File_set_info command\n551\nMPI_File_set_size command\n551\nMPI_File_set_view command\n552\nMPI_File_set_view function 552\nMPI_File_write_all command\n552\nMPI_File_write_at_all \ncommand 552\nMPI_File_write_at command\n552MPI_File_write command 552\nMPI_Finalize 271, 288\nmpifort command 257\nMPI_Gather 274\nMPI_Info_set command 572\nMPI_Init 300\nMPI_Init_thread 300–301\nMPI-IO (MPI file \noperations) 551–559\nMPI-IO library 551, 568\nMPI_Irecv 264, 285\nMPI_Isend 264–265, 280, 285\nMPI_Neighbor_alltoallw 295\nMPI_PACK 265\nMPI_Pack 278, 280, 285\nMPI_PACKED 265\nMPI_Probe 266\nMPI_Recv function 261\nMPI_Reduce 269\nMPI_Request_free 265\nmpirun command 257, 504, \n508, 510, 515, 517, 573, \n603\nMPI_Scatter 274\nMPI_Scatterv 274\nMPI_Send function 261\nMPI_Sendrecv function\n263–264\nMPI_Test 265\nMPI_THREAD_FUNNELED\n300\nMPI_THREAD_MULTIPLE\n300\nMPI_THREAD_SERIALIZED\n300\nMPI_THREAD_SINGLE 300\nMPI_Type_Commit 287\nMPI_Type_contiguous \nfunction 287\nMPI_Type_create_hindexed \nfunction 287\nMPI_Type_create_struct \nfunction 287\nMPI_Type_create_subarray 289\nMPI_Type_create_subarray \nfunction 287\nMPI_Type_Free 287\nMPI_Type_indexed \nfunction 287\nMPI_Type_vector function 287\nMPI_Wait 265\nMPI_Waitall 264\nMPI_Wtime function 267\nMSR (machine-specific \nregisters) 76MT/s (millions of transfers \nper sec) 66\nMTR (memory transfer rate)\n66\nmultidimensional arrays 90–94\nmultiple instruction, multiple \ndata (MIMD) 29\nmultiple instruction, single data \n(MISD) 29\nN\nNDRange (N-dimensional \nrange) 350–352\nneighbor finding\nface neighbor finding for \nunstructured meshes\n152–153\nusing spatial perfect \nhash 135–141\nwith write optimizations and \ncompact hashing\n149–151\nNEKbone 592\nnetloc command 62\nnetwork interface card \n(NIC) 299\nnetwork messages 119–122\nnew operator 96\nNFS (network filesystem) 576\nNIC (network interface \ncard) 299\nnodes 19\nnon-collective calls 551\nnon-contiguous bandwidth \n(Bnc) 61\nNUMA (Non-Uniform Memory \nAccess) 24, 210, 218, 492\nnumactl command 222, 509\nnumastat command 222\nNuT 593\nnvcc command 361\nNVIDIA Nsight suite 461, \n476–477\nNVIDIA nvidia-smi 461\nNVIDIA nvprof 461\nNVIDIA NVVP 461\nNVIDIA PGPROF 461\nNVIDIA SMI (System Manage-\nment Interface) 461\nnvprof command 472\nNVVP (NVIDIA Visual \nProfiler) 461\nnvvp command 472\nN-way set associative cache 102",9896
266-P.pdf,266-P,"INDEX 663\nO\nobjdump command 195\nobject-based filesystem 568\nObject Storage Servers \n(OSSs) 575\nObject Storage Targets \n(OSTs) 575\noccupancy 362\nompi_info command 569\nomp keyword 373\nomp parallel do pragmas 232\nomp parallel pragma 216\nomp_set_num_threads() \nfunction 212\nomp target data directive\n403\nomp target enter data \ndirective 403\nomp target exit data \ndirective 403\none-sided communication\n303\non-node parallel method 23\nOpenACC\ncompute directives 471–473\ndirective-based GPU \nprogramming 374–396\nadvanced techniques\n393–396\nasynchronous operations\n394\natomics 394\ncompiling code 375 –377\ninteroperability with \nCUDA libraries or \nkernels 395 –396\nmanaging multiple \ndevices 396\noptimizing kernels\n387–393\nparallel compute \nregions 377 –383\nperformance results 393\nroutine directive 394\nunified memory 395\nusing directives 383 –387\nresources for 415\nopen addressing 151\nOpenCL (Open Computing \nLanguage) 311, 438–449\nreductions in 445–449\nwriting and building \nOpenCL applications\n439–445OpenMP (Open Multi-\nProcessing) 207–253\nadvanced examples 240–247\nKahan summation imple-\nmentation OpenMP \nthreading 244\nstencil example with sepa-\nrate pass for x and y \ndirections 240 –243\nthreaded implementation \nof prefix scan \nalgorithm 246 –247\nconcepts 208–211\ndirective-based GPU \nprogramming 396–414\naccessing special memory \nspaces 412 –413\nadvanced techniques\n411–414\nasynchronous operations\n412\natomics 412\ncompiling code 397 –398\ncontrolling kernel \nparameters 411\ncreating data regions\n402–406\ndeclaring device \nfunction 411\ndeep copy support 413\ngenerating parallel \nwork 398 –402\nnew loop directive 413 –414\nnew scan reduction \ntype 411\noptimizing 406 –410\nfunction-level OpenMP\n229–231\nhigh-level OpenMP 231–237\nexample of 234 –237\nimplementing 232 –234\nhybrid MPI plus 299–302\nhybrid threading and vector-\nization with 237–240\nloop-level OpenMP 219–228\nperformance of 226 –227\npotential issues with\n227–228\nreduction example of \nglobal sum using \nOpenMP threading\n227\nstencil example 224 –226\nstream triad example\n222–224vector addition \nexample 220 –222\nMPI plus 218–219, 300–302, \n511–516\noverview 208–217\nresources for 251–252, 415\nSIMD directives 203–205\nSIMD functions 205\nsimple program 211–217\ntask-based support \nalgorithm 250–251\nthread affinity with 495–503\nthreading tools 247–249\nAllinea/ARM Map 248\nIntel Inspector 248 –249\nuse cases 217–219\nhigh-level OpenMP 218\nloop-level OpenMP\n217–218\nMPI plus OpenMP 218 –219\nvariable scope 228–229\nOpenMPI\ndefault process placement \nwith 504\nspecifying process placement \nin504–509\nOpenSFS (Open Scalable File \nSystems) 575\nOps (operations) 59\noptimized libraries 179\nOrangeFS 576\nOSSs (Object Storage \nServers) 575\nOSTs (Object Storage Targets)\n575\nout-of-bound errors 594\noutput dependency 188\noversubscribe command 535\nP\npackage managers 607–609\nfor macOS 607\nfor Windows 608\nSpack package manager\n608–609\npageable memory 332, 428\nPanasas 575–576\nparallel algorithms 124–169\ndefined 130\nfuture of research 167\nhash functions, defined\n131–132\noverview 125–126\nparallel global sum 161–166\nINDEX 664\nparallel algorithms (continued)\nperformance models vs. algo-\nrithmic complexity\n126–130\nprefix sum pattern 157–161\nfor large arrays 160 –161\nstep-efficient parallel scan \noperation 158\nwork-efficient parallel scan \noperation 159 –160\nresources for 168\nspatial hashing 132–157\ncompact hashing 149 –157\nperfect hashing 135 –148\nparallel computing 3–34\napplication/software model \nfor25–29\nprocess-based \nparallelization 26 –27\nstream processing through \nspecialized \nprocessors 28 –29\nthread-based \nparallelization 27 –28\nvectorization 28\ncategorizing parallel \napproaches 29–30\ncautions regarding 11\nfundamental laws of 11–15\nAmdahl’s Law 11 –12\nGustafson-Barsis’s Law\n12–15\nhardware model for 22–25\naccelerator devices 24\ndistributed memory \narchitecture 22 –23\ngeneral heterogeneous par-\nallel architecture \nmodel 24 –25\nshared memory \narchitecture 23\nvector units 23 –24\noverview of book 32–33\nparallel speedup vs. compara-\ntive speedup 31–32\npotential benefits of 8–11\ncost reduction 10 –11\nenergy efficiency 9 –10\nfaster run time with more \ncompute cores 9\nlarger problem sizes with \nmore compute nodes 9\nreasons for learning about\n6–11\nresources for 33sample app 16–22\ndefining computational \nkernel or operation\n17–18\ndiscretizing problem 17\noff-loading calculation to \nGPUs 21 –22\nprocesses 19 –21\nthreads 19\nvectorization 19\ntask parallelism 30\nParallel Data Systems Workshop \n(PDSW) 577\nparallel development \nworkflow 35–57\ncommiting 55\nimplementation 54\nplanning 51–53\nalgorithms 53\nbenchmarks and mini-\napps 52\ndesign of core data struc-\ntures and code \nmodularity 53\npreparation 36–50\nfinding and fixing memory \nissues 48 –50\nimproving code \nportability 50\ntest suites 39 –48\nversion control 37 –39\nprofiling 51\nresources for 56\nparallel directive 233, 381, 496\nparallel do directive 218\nparallel_for command 452\nparallel_for pattern 455\nparallel for pragma 218, 226\nparallel global sum 161–166\nparallelism, lack of 343\nparallel kernel 356–357\nparallel loop pragma 381–383\nparallel patterns 157–161\nfor large arrays 160–161\nstep-efficient parallel scan \noperation 158\nwork-efficient parallel scan \noperation 159–160\nparallel pragma 381\nparallel speedup (serial-to-paral-\nlel speedup) 31–32\nparallel-to-serial interface\n549–550\nParallel Virtual File System \n(PVFS) 576partial differential equations \n(PDEs) 18\npartial_load function 193\npattern rule 422\nPBS (Portable Batch \nSystem) 529\nPCI (Peripheral Component \nInterconnect) 24, 312\nPCI bus 326–332\nbenchmark application \nfor329–332\nhigher performance alterna-\ntive to 334\ntheoretical bandwidth \nof326–329\nmaximum transfer \nrate 327 –328\noverhead rates 328\nPCIe lanes 327\nreference data for 329\nPCIe (Peripheral Component \nInterconnect Express) 310, \n326\nPCIe lanes 327\nPCI SIG (PCI Special Interest \nGroup) 327\nPDEs (partial differential \nequations) 18\nPDF (portable document \nformat) 606\nPDSW (Parallel Data Systems \nWorkshop) 577\nPennant app 593\nperfect hashing 135–148\nneighbor finding 135–141\nremaps 142\nsorting mesh data 146–148\ntable lookups 143–146\nperformance limits 58–85\nbenchmarking 62–71\ncalculating machine bal-\nance between flops and \nbandwidth 71\ncalculating theoretical max-\nimum flops 65\nempirical measurement of \nbandwidth and \nflops 67 –69\nmemory hierarchy and the-\noretical memory \nbandwidth 66 –67\ntools for gathering system \ncharacteristics 62 –64\nknowing potential limits\n59–61",6754
267-Q.pdf,267-Q,,0
268-S.pdf,268-S,"INDEX 665\nperformance limits (continued)\nprofiling 71–84\nempirical measurement of \nprocessor clock fre-\nquency and energy \nconsumption 82 –83\ntools for 72 –82\ntracking memory during \nrun time 83 –84\nresources for 84\nperformance models\nadvanced 116–119\nalgorithmic complexity \nvs.126–130\nsimple 105–116\ncompressed sparse storage \nrepresentations\n112–116\nfull matrix data \nrepresentations\n109–111\nPeripheral Component Inter-\nconnect (PCI) 24, 312\nPEs (processing elements), \nOpenCL 316\npgaccelinfo command 353, 375, \n388\nPGI compiler 374\nPICSARlite 592\npinned memory 332, 428\npinning 301, 491\npipeline busy 480\nplacement 491\nplanning workflow step 51–53\nalgorithms 53\nbenchmarks and mini-\napps 52\ndesign of core data structures \nand code modularity 53\nPnetCDF (Parallel Network \nCommon Data Form) 568\nPortable Batch System \n(PBS) 529\nportable document format \n(PDF) 606\nPOSIX (Portable Operating Sys-\ntem Interface) 585, 607\npotential speedup 189\npower wall 23\npragmas 183–190, 208\ndirective-based GPU \nprogramming 373–374\nkernels pragma 378–380\nparallel loop pragma\n381–383prefetch cost (Pc) 106\nprefix sum operations \n(scans) 157–161\nfor large arrays 160–161\nstep-efficient parallel scan \noperation 158\nthreaded implementation \nof246–247\nwork-efficient parallel scan \noperation 159–160\npreparation workflow step\n36–50\nfinding and fixing memory \nissues 48–50\nimproving code portability 50\ntest suites 39–48\nautomatic testing with \nCMake and CTest\n41–45\nchanges in results due to \nparallelism 40 –41\nkinds of code tests 45 –47\nrequirements of ideal test-\ning system 48\nversion control 37–39\nprescriptive directives and \nclauses 414\nprimary keyword 495\nprintf command 259\nprivate directive 229\nprocess-based \nparallelization 26–27\nprocesses 19–21\nprofilers 585–590\ndetailed profilers 590\nhigh-level profilers 587\nmedium-level profilers\n588–589\ntext-based profilers 586–587\nprofiling 71–84\nempirical measurement of \nprocessor clock fre-\nquency and energy \nconsumption 82–83\ntools for 72–82\ncall graphs 72 –78\nlikwid-perfctr markers 78\nroofline plots 78 –82\ntracking memory during run \ntime 83–84\nprofiling workflow step 51\nproxy mini-app 592\nps command 83\nPVFS (Parallel Virtual File \nSystem) 576Q\nqcachegrind command 73\nquadratic probing 151\nQuantum Monte Carlo \n(miniQMC) 592\nqueues (streams), asynchronous \ncomputing through\n365–366\nQUO library 520\nR\nR (replicated array) 13\nrace conditions 209\nRadeon Open Compute plat-\nform (ROCm) 435\nRAJA 455–457\nranks 26\nRAW (read-after-write) 188\nreal cells 186\nreceive keyword 261\nrecursive algorithms 343\nRed Hat Package Manager \n(.rpm) 607\nreduction operation 122, 162, \n227\nreduction pattern\ngetting single value from \nacross all processes\n269–273\nOpenCL 445–449\nsynchronization across work \ngroups 364–365\nregister pressure 361\nrelaxed memory model 209\nremapping operation 135\nremaps\nhierarchical hash technique \nfor156–157\nusing spatial perfect hash 142\nwith write optimizations and \ncompact hashing\n153–156\nremote procedure call (RPC) 27\nremove function 552\nreplicated array 13\nresearch mini-app 592\nrestarting 539\nrestrict keyword 179\nRGB (red, green, blue) color sys-\ntem structure, in C 95\nROCgdb 604\nROCm (Radeon Open Compute \nplatform) 435",3349
269-U.pdf,269-U,"INDEX 666\nROMIO library 575\nROMIO MPI-IO library 576\nroofline plots 78–82\nroutine directive 394\nrow major 90\nRPC (remote procedure call) 27\n.rpm (Red Hat Package \nManager) 607\nS\nsalloc command 533\nsbatch command 534–535\nscalability 13\nSCALAPACK (scalable linear \nalgebra package) 179\nscalar operation 23\nscatter operations 274–276\nscp (secure copy) 463\nSDE (Software Development \nEmulator) package 81\nsend and receive \ncommands 259–266\nsend keyword 261\nserial directive 383\nserial-to-parallel speedup (paral-\nlel speedup) 31–32\nshader processors 316\nshallow water simulation example\noverview 463–467\nprofiling workflow 467–478\nCodeXL suite 478\ndata movement \ndirectives 473 –474\nguided analysis 474 –475\nNVIDIA Nsight suite\n476–477\nOpenACC compute \ndirectives 471 –473\nprofile CPU code 470\nrunning application\n467–469\nshared memory 23, 302\nSIMD (single instruction, multi-\nple data) architecture\nGPU programming \nmodel 354–355\nOpenMP SIMD \ndirectives 203–205\nOpenMP SIMD functions 205\noverview 176–177\nsimd pragma 238\nSimple Linux Utility for \nResource Management \n(Slurm) 529SIMT (single instruction, multi-\nthread) 30, 314, 354\nsleep command 542\nslice operator 91\nSlurm (Simple Linux Utility for \nResource Management)\n529\nSMs (streaming \nmultiprocessors) 21, 24, \n316, 479\nSNAP (SN application \nproxy) 593\nSoA (Structures of Arrays) 197\nAoS vs. 94–100\nperformance assessment\n96–100\nsockets, on motherboards 66\nSoftware Development Emulator \n(SDE) package 81\nsolid-state drive (SSD) 548\nspack find command 609\nspack list command 609\nspack load <package_name> \ncommand 609\nSpack package manager 608–609\nspatial hashing 132–157\ncompact hashing 149–157\nface neighbor finding\n152–153\nneighbor finding 149 –151\nremaps 153 –157\nperfect hashing 135–148\nneighbor finding 135 –141\nremaps 142\nsorting mesh data 146 –148\ntable lookups 143 –146\nspatial locality 105\nspeeds 59\nspeedup 9\nspinning disk 548\nspread keyword 495\nsrun command 535\nSSD (solid-state drive) 548\nSSE2 (Streaming SIMD \nExtensions) 177\nstatfs command 574\nstencil calculations\nghost cell exchanges in 3D \nstencil calculation\n285–286\nloop-level OpenMP 224–226\nwith separate pass for x and y \ndirections 240–243\nstep-efficient parallel scan \noperation 158STL (Standard Template \nLibrary) 420\nstore command 194\nstore operation 102\nSTREAM Benchmark 67\nstreaming kernels 119\nstreaming multiprocessors \n(SMs) 21, 24, 316, 479\nstreaming store 119\nstream processing through spe-\ncialized processors 28–29\nstreams 420\nstream triad\nloop-level OpenMP 222–224\nmeasuring bandwidth on \nnode 276\nperformance results for 393\nstride 91\nstrong scaling 12\nsubgroups 353–354\nSVN (Subversion) 583\nSW4lite 592\nSWFFT 592\nSYCL 449–452\nSycl function 451\nsynchronization 420, 480\nsysctl command 64\nsystem_profiler command 64\nT\ntable lookups, using spatial per-\nfect hash 143, 145–146\ntape 548\ntarget directive 50\ntask-based support \nalgorithm 250–251\ntask parallelism 30\ntaskset command 509\nTCL modules 612\nTDD (test-driven \ndevelopment) 46\nTDP (thermal design \npower) 337\nTeaLeaf mini-app 593\nteams directive 411\ntemporal locality 105\ntest suites 39–48\nautomatic testing with CMake \nand CTest 41–45\nchanges in results due to \nparallelism 40–41\nkinds of code tests 45–47\nrequirements of ideal testing \nsystem 48",3402
270-V.pdf,270-V,,0
271-W.pdf,271-W,,0
272-X.pdf,272-X,"INDEX 667\ntexture busy 480\ntheoretical machine balance \n(MBT) 71\ntheoretical memory bandwidth \n(BT) 66\nThornado-mini 592\nthread divergence 149, 343\nthreading\nglobal sum using OpenMP \nthreading 227\nhybrid threading with \nOpenMP 237–240\nKahan summation implemen-\ntation with OpenMP \nthreading 244\nsample app 19\nthread-based \nparallelization 27–28\nthread checkers 599–601\nArcher 600 –601\nIntel Inspector 600\nthreaded implementation of \nprefix scan \nalgorithm 246–247\nthread engine, GPU as\n313–318\ncalculating peak theoretical \nflops 316 –318\ncompute unit 316\nmultiple data operations by \neach element 316\nprocessing elements 316\nthread indices 357–358\ntools for 247–249\nAllinea/ARM Map 248\nIntel Inspector 248 –249\nthreadprivate directive 230\nthreads 16\ntightly-nested loops 390\ntime command 587timer routines 585\ntop command 83, 531\ntotalview command 602\nTotalView debugger 602\nTSan (ThreadSanitizer) 600\nU\nunified memory 429\nuninitialized memory 594\nunstructured data regions 403\nunstructured mesh boundary \ncommunications 302\nV\nvalgrind command 49\nValgrind Memcheck 49–50, \n594\nvariable scope 228–229\nvector intrinsics 190–194\nvectorization 175–206\ncompiler flags 198–201\nhardware trends for 177\nmethods for 178–196\nassembler instructions\n195–196\nauto-vectorization 179 –183\ncompiler hints 183 –190\noptimized libraries 179\nvector intrinsics 190 –194\nmultiple operations with one \ninstruction 28\nOpenMP SIMD \ndirectives 203–205\nOpenMP SIMD functions 205\noverview 176–177\nprogramming style for\n196–198\nresources for 206sample app 19\nwith OpenMP 237–240\nvector lanes (lanes) 23\nvector length 23\nvector_length(x) directive 388\nvector operation 19\nvector units 23–24\nversion control 37–39, 582–583\ncentralized version \ncontrol 583\ndistributed version \ncontrol 582–583\nVirtualBox 483–485\nVMs (virtual machines)\n483–485\nW\nWAR (write-after-read) 188\nwarm cache 226\nwarps 353–354\nwavefronts 353–354\nweak scaling 13\nWekaIO 576\nWindows Subsytem Linux \n(WSL) 608\nwmic command 64\nwork-efficient parallel scan \noperation 159–160\nworkers 387\nwork groups 353, 364–365\nwork items 354\nwork sharing 209\nwrite-allocate 118\nWSL (Windows Subsytem \nLinux) 608\nX\nXSBench 592\nFor ordering information go to www.manning.comRELATED MANNING TITLES\nModern Fortran\nby Milan Curcic\nForeword by Damian Rouson\nISBN 9781617295287 \n416 pages, $47.99\nOctober 2020 \nConcurrency in .NET\nby Riccardo Terrell\nISBN 9781617292996 \n568 pages, $47.99\nJune 2018 \nC++ Concurrency in Action, 2E\nby Anthony Williams\nISBN 9781617294693 \n592 pages, $55.99\nFebruary 2019 \nRobey ● Zamora\nISBN: 978-1-61729-646-8\nWrite fast, powerful, energy effi   cient programs that \nscale to tackle huge volumes of data. Using parallel programming, your code spreads data processing tasks \nacross multiple \nCPU s for radically better performance. With a \nlittle help, you can create software that maximizes both speed and effi   ciency.\nParallel and High Performance Computing  off ers techniques \nguaranteed to boost your code’s eff  ectiveness. You’ll learn to \nevaluate hardware architectures and work with industry-standard tools such as OpenMP and MPI. You’ll master the data structures and algorithms best suited for high perfor-mance computing and learn techniques that save energy on handheld devices. You’ll even run a massive tsunami simula-tion across a bank of GPUs. \nWhat’s Inside\n● Planning a new parallel project\n● Understanding diff  erences in CPU  and GPU  architecture\n● Addressing underperforming kernels and loops\n● Managing applications with batch scheduling\nFor experienced programmers profi  cient with a high-perfor-\nmance computing language like C, C++, or Fortran.\nRobert Robey  works at Los Alamos National Laboratory and \nhas been active in the fi  eld of parallel computing for over \n30 years. Yuliana Zamora  is currently a PhD student and \nSiebel Scholar at the University of Chicago, and has lectured on programming modern hardware at numerous national conferences.\nRegister this print book to get free access to all ebook formats. \nVisit https:/ /www.manning.com/freebook\n$69.99 / Can $92.99  [INCLUDING eBOOK]\nParallel and High Performance ComputingSOFTWARE ENGINEERING/PARALLEL PROGRAMING\nMANNING“If you want to learn about \nparallel programming and \nhigh-performance computing \nbased on practical and \nworking examples, this \n  book is for you.” \n—T uan A. T ran, ExxonMobil\n“A great survey of recent \nadvances on parallel and \nmulti-processor software \n techniques.” \n—Albert Choy\nOSI Digital Grid Solutions\n“An in-depth treatise on \nparallel computing from both \na software- and hardware-\n  optimized standpoint.”—Jean François Morin\nLaval University \n“Th is book will show you \nhow to design code that takes \nadvantage of all the \ncomputing power modern \ncomputers off  er.” \n—Alessandro Campeis, Vimar\nSee first page",4968
