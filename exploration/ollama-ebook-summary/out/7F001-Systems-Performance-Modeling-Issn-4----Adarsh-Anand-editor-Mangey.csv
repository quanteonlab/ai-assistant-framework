filename,title,text,len
01-Acknowledgment.pdf,01-Acknowledgment,"Acknowledgment\nThe editors acknowledge Walter de Gruyter and the editorial team for their ade-\nquate and professional support during the preparation of this book. They thank allauthors who have contributed to this editorial work. Reviewers, who have helped\nthrough their comments and suggestions in improving the quality of the chapters,\ndeserve significant praise for their assistance.\nFinally and most importantly, editors dedicate this editorial book to their family\nand friends.\nAdarsh Anand\nUniversity of Delhi, India\nMangey Ram\nGraphic Era Deemed to be University, India\nhttps://doi.org/10.1515/9783110619058-202",629
02-Preface.pdf,02-Preface,"Preface\nVirtually all countries now depend on complex computer-based systems. Infrastructure\nand utilities rely on the computer-based syst e m s ,a n da l m o s te v e r y t h i n gt h a ts u r r o u n d s\nus includes a computer and controlling so ftware. Information and communication\ntechnologies have been at the heart of economic changes for more than a decade, nota-bly in all areas of businesses. In line with this, this edited issue of System Performance\nModeling includes invited papers that deal with the modeling and analysis of software\nsystems with the aid of computers. Emphasi si so nc o n c e p t s ,t h e o r i e s ,a n dt e c h n i q u e s\ndeveloped in the infocom discipline. The topics covered are organized as follows:\nChapter 1 discusses software vulnerability patch management, which is one of\nthe newer areas of research.\nChapter 2 discusses the debugging process for modeling quality and reliability\naspects of the software project management.\nIn Chapter 3, an analysis pertaining to vehicular cloud computing has been\npresented.\nChapter 4 presents a comparative study dealing with the agile methodology in-\nculcating increasing failure rate software reliability models.\nIn today ’s time, everyone is talking and dealing with the three Vs, which we\nknow by the name of big data. Chapter 5 presents a mathematical framework tomodel the fault big data analysis based on effort estimation and artificial intelli-\ngence for Open Source Software Project.\nEvery system, be it hardware or software, requires data. The more accurate the\ndata, the more are the chances that the results will be error-free. Chapter 6 presentsa modeling framework that deals with the meaning of data streams and its impact\non the system performance.\nChapter 7 discusses assessing the reliability of public switched telephone net-\nwork, which is the most useful telecommunication network in general.\nNo software is complete unless some articulation is talked about for hard-\nware, as both go hand in hand when talked about a complex system. Chapter 8\ncontains a description of the utility of Weibull failure laws for reliability measures\nof a series –parallel system.\nAdarsh Anand\nUniversity of Delhi, India\nMangey Ram\nGraphic Era Deemed to be University, India\nhttps://doi.org/10.1515/9783110619058-203",2330
03-About the editors.pdf,03-About the editors,"About the editors\nAdarsh Anand did his doctorate in the area of Software Reliability Assessment and Innovation\nDiffusion Modeling in Marketing. Presently, he works as an assistant professor in the Department\nof Operational Research, University of Delhi (INDIA). He has been conferred with Young PromisingResearcher in the field of Technology Management and Software Reliability by Society forReliability Engineering, Quality and Operations Management (SREQOM) in 2012. He is a lifetimemember of the Society for Reliability Engineering, Quality and Operations Management (SREQOM).He is also on the editorial board of International Journal of System Assurance and Engineering\nManagement (Springer). He has guest edited several special issues for journals of international re-\npute. He has publications in journals of national and international repute. His research interest\nincludes software reliability growth modeling, modeling innovation adoption and successive gen-\nerations in marketing, and social network analysis. He has worked with CRC Press for two editorialprojects; “System Reliability Management: Solutions and Technologies ”and “Recent Advancements\nin Software Reliability Assurance. ”He has also authored one textbook with CRC group “Market\nAssessment with OR Applications. ”\nDr. Mangey Ram received the Ph.D. degree major in mathematics and minor in computer science\nfrom G. B. Pant University of Agriculture and Technology, Pantnagar, India. He has been a facultymember for around 12 years and has taught several core courses in pure and applied mathematicsat undergraduate, postgraduate, and doctorate levels. He is currently a professor at Graphic Era(Deemed to be University), Dehradun, India. Before joining the Graphic Era, he was a deputy man-ager (probationary officer) with Syndicate Bank for a short period. He is the editor in chief ofInternational Journal of Mathematical, Engineering and Management Sciences , and the guest editor\nand member of the editorial board of various journals. He is a regular reviewer for internationaljournals, including IEEE, Elsevier, Springer, Emerald, John Wiley, Taylor & Francis, and many otherpublishers. He has published 175 plus research publications in IEEE, Taylor & Francis, Springer,Elsevier, Emerald, World Scientific, and many other national and international journals of reputeand also presented his works at national and international conferences. His fields of research arereliability theory and applied mathematics. Dr. Ram is a senior member of the IEEE, life member ofOperational Research Society of India, Society for Reliability Engineering, Quality and OperationsManagement in India, Indian Society of Industrial and Applied Mathematics, member of InternationalAssociation of Engineers in Hong Kong, and Emerald Literati Network in the UK. He has been a mem-ber of the organizing committee of a number of international and national conferences, seminars,and workshops. He has been conferred with the Young Scientist Award by the Uttarakhand State\nCouncil for Science and Technology, Dehradun, in 2009. He has been awarded the Best Faculty\nAward in 2011; Research Excellence Award in 2015; and recently, Outstanding Researcher Award in\n2018 for his significant contribution in academics and research at Graphic Era Deemed to beUniversity, Dehradun, India.\nhttps://doi.org/10.1515/9783110619058-204",3390
04-Contents.pdf,04-Contents,"Contents\nAcknowledgment V\nPreface VIIAbout the editors IX\nList of contributing authors XIII\nAdarsh Anand, Jasmine Kaur, Anu A. Gokhale, and Mangey Ram\n1 Impact of available resources on software patch management 1\nShinji Inoue and Shigeru Yamada\n2 Debugging process modeling for quality/reliability assessment of\nsoftware system 13\nShivani Gupta and Vandana Gupta\n3 Availability analysis of vehicular cloud computing 21\nGabriel Ricardo Pena and Nestor Ruben Barraza\n4 Increasing failure rate software reliability models for agile projects:\na comparative study 45\nYoshinobu Tamura and Shigeru Yamada\n5 Fault big data analysis based on effort prediction models and AI for\nopen-source project 67\nMario Diván and María Laura Sánchez-Reynoso\n6 Modeling the meaning of data streams and its impact on the system\nperformance 79\nKuldeep Nagiya and Mangey Ram\n7 Performance evaluation of switched telephone exchange network 123\nS. C. Malik and S. K. Chauhan\n8 On use of Weibull failure laws for reliability measures of a series –parallel\nsystem 141\nIndex 177",1073
05-List of contributing authors.pdf,05-List of contributing authors,"List of contributing authors\nAdarsh Anand\nDepartment of Operational Research\nUniversity of DelhiDelhi, Indiaadarsh.anand86@gmail.com\nNestor Ruben Barraza\nUniversidad Nacional de Tres de FebreroBuenos Aires, Argentinanbarraza@untref.edu.ar\nS. K. Chauhan\nShaheed Rajguru College of Applied Sciencesfor WomenUniversity of DelhiNew Delhi, Indiastatskumar1@gmail.com\nMario Diván\nNational University of La PampaLa Pampa, Argentinamjdivan@eco.unlpam.edu.ar\nAnu A. Gokhale\nIllinois State UniversityNormal, Illinois, USAaagokhale@ilstu.edu\nShivani Gupta\nUniversity of DelhiDelhi, Indiashivani222gupta@gmail.com\nVandana Gupta\nUniversity of Delhi\nDelhi, Indiakhaitan.vandana@gmail.com\nShinji Inoue\nKansai UniversityOsaka, Japanino@kansai-u.ac.jpJasmine Kaur\nUniversity of Delhi\nDelhi, Indiajasminekaur.du.aor@gmail.com\nS. C. Malik\nMaharshi Dayanand UniversityRohtak, Indiasc_malik@rediffmail.com\nKuldeep Nagiya\nGraphic Era Deemed to be UniversityDehradun, Uttarakhand, Indiakuldeepnagiya@gmail.com\nGabriel Ricardo Pena\nUniversidad Nacional de Tres de FebreroBuenos Aires, Argentinagpena@untref.edu.ar\nMangey Ram\nDepartment of Mathematics, ComputerScience and EngineeringGraphic Era Deemed to be UniversityDehradun, Uttarakhand, Indiadrmrswami@yahoo.com\nMaría Laura Sánchez Reynoso\nNational University of La PampaLa Pampa, Argentinamlsanchezreynoso@eco.unlpam.edu.ar\nYoshinobu Tamura\nTokyo City University\nTokyo, Japan\ntamuray@tcu.ac.jp\nShigeru Yamada\nTottori UniversityTottori, Japanyamada@tottori-u.ac.jp\nhttps://doi.org/10.1515/9783110619058-206",1573
06-1. Impact of available resources on software patch management.pdf,06-1. Impact of available resources on software patch management,"Adarsh Anand, Jasmine Kaur, Anu A. Gokhale, and Mangey Ram\n1 Impact of available resources on software\npatch management\nAbstract: Software security has been an area of immense research as most of the\nthings surrounding us are technology based. Much has been talked about vulner-\nabilities, their categories and types. Some studies elaborated and extended theavailable discovery models but few have considered the correction process in the\nsame work. In this study, an approach to deal with software vulnerability through\nthe release of patch/updates has been presented. The methodical work pre-sented here discusses a mathematical mo del for optimal allocation of resources\nto remove vulnerabilities through an update.\nKeywords: severity, software patch, software security, software updates, vulnerability,\nvulnerability correction model\n1.1 Introduction\nS o f t w a r es e c u r i t yi sam a t t e ro fg r a v ec o n c e r n ,a n dt h en e e df o rs e c u r es o f t w a r eh a s\nbeen stressed upon enough. The first hal fo ft h ey e a r2 0 1 9h a da l r e a d ys e e n3 , 8 0 0\ncases of publicly disclosed software breache s and 4.1 billion records exposed [1]. With\nthe hacking tools getting more and more adv anced, it is a tough fight to keep the sys-\ntem safe from the intruders. Even minor loopholes or oversights leave the software\nhighly vulnerable. Often, the exploited loopholes have a fix available, but due to lackof comprehension of the situation ’s gravity, they are never plugged. Such faults or\nglitches in the system architecture, design, code or implementation that compromisesthe software ’s security are termed as software vulnerabilities. The most common vul-\nnerabilities are caused by memory safety violations, input validation error, privilegeconfusion bugs, privilege escalation, race conditions, side channel attack, and user in-\nterface failure. The most common types of vulnerabilities are SQL injection, command\ninjection, buffer overflow, uncontrolled f ormat string, integer overflow, cross-site\nscripting, and so on. The software vulnerabilities are also categorized on the basis\nof the extent of damage their presence or exploitability causes. The Common\nVulnerabilities and Exposures (CVE) database provides a Common Vulnerability\nScoring System (CVSS) score to the reported vulnerabilities known as the severity\nAdarsh Anand, Jasmine Kaur, University of Delhi, Delhi, India\nAnu A. Gokhale, Illinois State University, USA\nMangey Ram, Graphic Era Deemed to be University Dehradun, Uttarakhand, India\nhttps://doi.org/10.1515/9783110619058-001\n\nindex on an ascending scale of 1 to 10 [2]. On the basis of severity score, vulner-\nabilities are categorized as low, medium, high, and critical. The vulnerability dis-\ncovery process is modeled through Vulnerability Discovery Models (VDMs), whichhelp quantify the vulnerabilities discovered and understand their detection behaviorwith time. The foremost software VDM was the Anderson ’s thermodynamic model [3].\nRescorla [4] presented a linear and exponential trend in the vulnerability detectionprocess. The Alhazmi-Malaiya model, an S-shaped, logistic VDM defined the threephases in the vulnerability discovery process as linear, learning, and saturation [5].\nAn effort-based model was proposed by Alhazmi and Malaiya [6] to model the effort\nconsumed in terms of resources and budget in finding the vulnerabilities. Arora et al.[7] had discussed the optimal policy for vulnerability disclosure. Kapur et al. [8] pro-\nposed a logistic rate in the vulnerability detection model. The model by Anand and\nBhatt [9] proposed a hump-shaped vulnerability detection rate. Bhatt et al. [10] dis-cussed a VDM that categorizes the discovered vulnerabilities as leading vulnerabil-ities and additional vulnerabilities. Vulnerability discovery process over multiple\nversions of the software has been modeled by Anand et al. [11].\nThe software firms tend to test the software for vulnerabilities and release a re-\nmedial patch, or a bunch of patches simultaneously known as updates, dependingon the number of vulnerabilities to be catered. Software patches are sets of correc-\ntive code meant to replace the defective, fault-causing code and thus prevent ex-ploitability. Beattie et al. [12] presented a mathematical model to determine the best\ntime to apply a security patch for software vulnerability. The utility of patches in\nimproving the software was first mathematically discussed in a Software ReliabilityGrowth Model (SRGM) by Jiang and Sarkar [13], Arora et al. [14] had discussed the\nimpact of patching on the software quality. Das et al. and Deepika et al. [15, 16] had\nexplored the role of the tester and the user in software reliability growth via thepatch service. Anand et al. [17] highlighted the importance of providing patching\nservice in the software and proposed a scheduling policy for the optimal release of\nsoftware. Anand et al. [18] considered faults and vulnerabilities simultaneously andpresented optimal patch release policy. Recently, Kaur et al. [19] presented the firstvulnerability correction model (VCM) and discussed the mathematical frame for the\ntime gap between the correction of leading and dependent vulnerabilities.\nPrecautions need to be taken during the software development process to mini-\nmize the possibility of vulnerabilities. Better coding practices, better organizationalpractices, exhaustive testing, and so on, can help reduce the number of vulnerabil-\nities in the software. The presence of vulnerability itself in the software is not themain concern but rather their exploitability is. It is not possible to ensure vulnera-\nbility free software but their number can be minimized so as to avoid damage\nthrough exploitability. After the release of the software the only way to ensuresoftware security is continuous and exhaustive testing of the software. The devel-\nopmental resources in the form of manpow e r ,b u d g e t ,t i m e ,a n ds oo n ,a v a i l a b l e\nthroughout the project are in a limited supply. The available resources are allocated2 Adarsh Anand et al.\n\nto each phase of the Software Development Life Cycle (SDLC) and a major portion of\nit goes in software testing (both before and after software release). We know that test-\ning the software in-house is cheaper as compared to field testing, thus release ofpatches and updates as part of maintenance activities is expensive. During the vul-nerability removal process, usually preference is given to vulnerabilities with higher\nseverity as they are likely to cause bigger damage. But the less severe vulnerabilities\ncannot be ignored as they too are dangerous and can cause critical damage upon ex-ploitation. The number of vulnerabilities that can be dealt in an update depends on\nmany factors such as the number of vulnerabilities in the software, the resources\navailable to patch them, and the nature/severity of the vulnerability. Hence, an opti-mal allocation of resources is very essential for the operational phase. A trade-off\nneeds to be understood to plan an update so that the maximum number of vulner-\nabilities can be catered with the proper utilization of the limited resources at hand.Since the number of vulnerabilities in software is usually large, not all vulnerabilitiescan be handled in a single update. For a singular issue or for dealing with a handful\nof vulnerabilities, patches are released. But when a large number of vulnerabilities\nneed to be dealt with, the patches are clubbed together and released to the user as anupdate. For a particular version of the software, multiple singular patches or updates\ncan be released depending on the need of the software.\nIn literature, the resource allocation pr oblem has been extensively explored in the\ncontext of software reliability growth models [20 –30]. A lot has been discussed and\npresented in past in the context of SRGMs but there is limited work done when VDM is\ntalked about from resource allocation point of view. A recent work by Bhatt et al. [31] is\nthe first to discuss an optimal resource allocation model for the vulnerability discovery\nprocess.\nThe optimization problem to maximize a quantity (vulnerability correction)\nwhile trying to minimize the use of the other (resource utilization), has been takenup in the current work. An effort has been made to allocate an optimal amount of\nresources for software vulnerability correction process. The allocation of the resour-ces will lead to increased security, improved reliability, and quality of the software.\nThe flow of the model can be understood through Figure 1.1.\nPatch 1 0 Patch 2 Patch 3 Patch 4 Nth Patch T\nFigure 1.1: Flow of the model.1 Impact of available resources on software patch management 3\n\nThe chapter has been bifurcated as follows: Section 1.2 discusses the notations and\nmodel development, Section 1.3 presents a numerical example to validate the pro-\nposed model, and Section 1.4 concludes the chapter and is followed by a list of thereferences used.\n1.2 Model development\n1.2.1 Notations\nThe notations used in the model development are:M: Number of severity level groups pertaining to software vulnerability\ni: Variable representing the severity level group, i=1,2,... ,M\nN\nRi: Expected number of vulnerabilities of ith severity group\nrRi: Vulnerability removal rate\nyiðtÞ: Resource utilization at time tforith severity group and\nYitðÞ=Ðt\n0yiwðÞdwforith severity group\nY*\ni: Optimal value of Yi,i=1,2,... ,M\nZ: Total resources available\nΩRiðtÞ: Number of vulnerabilities removed in ð0,t/C138of the ith severity group,\nthat is, mean value function of the Non Homogeneous Poisson Process(NHPP)\nΩ\nRiðYiðtÞÞ: Cumulative number of vulnerabilities removed using resources YiðtÞin\ntime ð0,t/C138\nT: Total time available for the vulnerability removal process\n1.2.2 Resource allocation problem\nThe work in vulnerability modeling extensively deals with either the vulnerability de-tection process or the vulnerability exploitation process. The vulnerability correctionprocess has not obtained much attention even though it is a very important aspect of\nthe vulnerability life cycle. The VCM presented by Kaur et al. [19] describes the vulnera-\nbility removal/fixation phenomenon for the detected vulnerabilities. They had furthercategorized the removed vulnerabilities as leading and dependent. In this section, we\nshall extend upon their work and present another VCM that will then be used to allo-\ncate resources for releasing u pdates. The work of Rescorla [4] along with the proposals\nof Alhazmi and Malaiya [6] has been considered to design our present mathematical\nstructure.4 Adarsh Anand et al.\n\nRecently, Bhatt et al. [31] had presented a model to dynamically allocate resour-\nces to discover vulnerabilities of varying severity levels. The following differential\nequation defines the relation between the effort consumed in detecting vulnerabil-ities of ith severity level and the number of vulnerabilities discovered:\ndΩitðÞ\ndt/C30\nxitðÞ=riNi−ΩitðÞ ðÞ ,i=1,2, ...,n, (1:1)\nwhere ΩitðÞdenotes the number of vulnerabilities of ith severity detected till time t;\nNiis the number of vulnerabilities detected of ith severity and rirepresents their detec-\ntion rate, while xitðÞrepresents the resources or effort spent in vulnerability detection.\nSolution of the eq. (1.1) gives\nΩitðÞ=Ni1−e−ri.XitðÞ/C16/C17\n, i=1,2, ...,n, (1:2)\nUsing the above-mentioned analogy, the effort consumption in the vulnerability re-\nmoval phenomena can be defined on the lines of effort-based vulnerability detec-\ntion phenomena. According to the work of Kaur et al. [19], the VCM is an NHPP andeffort-based VDM is NHPP; hence, the effort-based VCM would also follow NHPP.\nThe VCM can thus be defined as\ndΩRitðÞ\ndt/C30\nyitðÞ=rRiNRi−ΩRitðÞ ðÞ ,i=1,2, ...,M. (1:3)\nSolving the differential equation with initial conditions t=0,YitðÞ=0 and ΩRitðÞ=0,\nwe obtain the relation as\nΩRitðÞ=NRi1−e−rRi.YitðÞ/C16/C17\n,i=1,2, ...,M. (1:4)\nUsing the exponential distribution to define the behavior of the effort function as\ndescribed by Bhatt et al. [31], we obtain\ndYitðÞ\ndt=giλi−YitðÞ ðÞ ,i=1,2, ...,M, (1:5)\nwhere girepresents the rate at which the available resources are consumed in the\nvulnerability removal of the ith severity level group and λidenotes the total resour-\nces that are available to remove vulnerabilities of a given severity level group. The\nsolution of the above-mentioned differential equation gives us\nYitðÞ=λi1−e−gi.t/C0/C1\n,i=1,2, ...,M. (1:6)1 Impact of available resources on software patch management 5\n\nThe purpose of our optimization problem is to remove the maximum number of vul-\nnerabilities of varying severity levels in a given update keeping in mind the limited\nresources available. Hence, an optimization problem can be formulated as:\nMaximizeXM\ni=1ΩRiðYiÞ=XM\ni=1NRið1−e−rRiYiÞ\nsubject toXM\ni=1Yi≤Z,Yi≥0,where i=1, ...,M.(1:7)\nThe solution of the optimization problem allocates resources in a particular up-\ndate, that is, it helps the debugging team allocate its limited resources based on thenumber and nature and of the vulnerability. The leftover vulnerabilities will be ca-tered to in the next update via a software patch. A run-through of the model has\nbeen shown via a numerical illustration in the following section.\n1.3 Numerical illustration\nFor the purpose of model validation a simulated dataset has been considered,\nwhich contains the vulnerability removal data. The dataset contains 636 vulnerabil-ities that are further categorized on the basis of their severity levels. Six severity\nlevel groups have been designed as shown in the first column of Table 1.1. It has\nbeen assumed that we have a fixed supply of resources of 3,100 units. But we havenot allocated all our resources in a single go and wish to divide it proportionately\nbetween the multiples updates as discussed earlier. For the first update, we have\nlimited ourselves to 1,000 units of resources. We have used the above-discussedVCM (eqs. (1.4) and (1.6)) to estimate the value of r\nRand allocate these 1,000 units\nof resources for the different severity groups. Software packages SPSS and LINGOhave been utilized for parameter estimation and for solving the optimization prob-lem respectively. Table 1.1 represents the dynamically allocated resources ( Y) for\neach severity group, number of vulnerabilities removed ( Ω\nR)on the basis of the al-\nlocated resources.\nAs given in Table 1.1, approximately 72% of the initial 636 vulnerabilities have\nbeen removed in the first update itself. This amount contains a propionate re-\nmoval from each severity group based on their initial content. This high percentage\nof removal has been achieved due to various factors such as rigorous debugging(both due to in-house and field testing) and user participation. It is important to re-\nmove the major portion of the vulnerabilities in the first update itself so that the reli-\nability can be maximized as soon as possible.6 Adarsh Anand et al.\n\nIn the second update the leftover vulnerabilities from the first update will be\ncatered to. We have allotted 800 units of available resources to this update. Following\nthe approach discussed earlier, we run the second iteration to remove the leftovervulnerabilities.\nTable 1.2 shows that out of the approximately 178 vulnerabilities to be dealt in\nthis update, 63% of them have been removed and approximately 66 vulnerabilitiesare still latent in the system. Thus, the debugging process continues and the need\nof another patch comes into picture. The third update has been allotted 700 units of\nresources to remove the 65 vulnerabilities.\nBy the third update a very large portion of the initial vulnerabilities has been re-\nmoved and the software will be perceived to be quite reliable. As can be seen inTable 1.3, approximately 58% of the vulnerabilities of the third update have been re-moved. Now we are left with only 28 vulnerabilities out of the initial 636. So we allo-\ncate our remaining 600 units of resources to remove these vulnerabilities.\nTable 1.4 shows that with the given amount of resources we have been able to\nremove 50% of the vulnerabilities of the fourth update. By the time the fourth up-\ndate reaches the user, the useful life of the version is almost over and further re-\nsources will not be spent on it. If the leftover vulnerabilities are crucial and\nremoving them is of utmost importance, then the firm will have to allocate moreresources. If the initial count of the vulnerabilities in the system is compared to the\nnumber of vulnerabilities still latent after the last update, we can see that approxi-\nmately 98% of the vulnerability content could be eliminated using our approach.Thus, the software can be considered to be highly secure. Further, our approach\nhas given equal importance to vulnerabilities of all severity groups and not just fo-\ncused on the high severity groups while ignoring the low severity vulnerabilities.An implicit result that can also be drawn from the above-mentioned numericalTable 1.1: Resource allocations for update 1.\nSeverity\ngroupsNR rR Y ΩR Percentage ofvulnerabilitiesremovedPercentage ofvulnerabilitiesremainingNumber ofleftovervulnerabilities\n–  .  .  .  .  .  .  .\n  .  .  .  .  .  .  .\n  .  .  .  .  .  .  .\n–  .  .  .  .  .  .  .\n–  .  .  .  .  .  .  .\n  .  .  .  .  .  .  .\nTotal . – ,.  .  .  .  .1 Impact of available resources on software patch management 7\n\nillustration is that, based on a given resource pool and the number of leftover vul-\nnerabilities, we can determine the number of patches/updates that will be re-quired to deal with all the vulnerabilities of a particular version of the software.Table 1.3: Resource allocations for update 3.\nSeverity\ngroupsNR rR Y ΩR Percentage ofvulnerabilitiesremovedPercentage ofvulnerabilitiesremainingNumber ofleftovervulnerabilities\n–  .  .  .  .  .  .  .\n  .  .  .  .  .  .  .\n .  .  .  .  .  .  .\n– .  .  .  .  .  .  .\n–  .  .  .  .  .  .  .\n  .  .  .  .  .  .  .\nTotal . – .  .  .  .  .Table 1.2: Resource allocations for update 2.\nSeveritygroupsN\nR rR Y ΩR Percentage ofvulnerabilitiesremovedPercentage ofvulnerabilitiesremainingNumber ofleftovervulnerabilities\n–  .  .  .  .  .  .  .\n  .  .  .  .  .  .  .\n  .  .  .  .  .  .  .\n– .  .  .  .  .  .  .\n–  .  .  .  .  .  .  .\n  .  .  .  .  .  .  .\nTotal . – .  .  .  .  .8 Adarsh Anand et al.\n\n1.4 Conclusion\nThe work presents a vulnerability removal model. An optimizati on model has been uti-\nlized to allocate the resources to remove the vulnerabilities of different severity that is\nachieved via multiple patch/updates. The proposed model has been validated on simu-lated vulnerability data. Approximately 98 percent of the vulnerabilities were removed\nthrough the proposed model. Thus, with the help of the current work, the resource al-\nlocation for vulnerability remo val can be optimally achieved.\nBibliography\n[1] https://us.norton.com/internetsecurity-emerging-threats-2019-data-breaches.html\n[2] https://www.cvedetails.com[3] Anderson, R. (2002). Security in open versus closed systems –the dance of Boltzmann,\nCoase and Moore, Technical report, Cambridge University, England.\n[4] Rescorla, E. (2005). Is finding security holes a good idea?, IEEE Security & Privacy, 3(1),\n14–19.\n[5] Alhazmi, O.H. and Malaiya, Y.K. (2005a). Quantitative vulnerability assessment of systems\nsoftware, In proceedings of Annual Reliability and Maintainability Symposium, IEEE,615–620.\n[6] Alhazmi, O.H. and Malaiya, Y.K. (2005b). Modeling the vulnerability discovery process, In\n16th IEEE International Symposium on Software Reliability Engineering (ISSRE ’05), IEEE,\nChicago, IL, 138 –147.\n[7] Arora, A., Telang, R. and Xu, H. (2008). Optimal policy for software vulnerability disclosure,\nManagement Science, 54(4), 642 –656.Table 1.4: Resource allocations for update 4.\nSeveritygroupsN\nR rR Y ΩR Percentage ofvulnerabilitiesremovedPercentage ofvulnerabilitiesremainingNumber ofleftovervulnerabilities\n– .  .  .  .  .  .  .\n .  .  .  .  .  .  .\n .  .  .  .  .  .  .\n– .  .  .  .  .  .  .\n– .  .  .  .  .  .  .\n  .  .  .  .  .  .  .\nTotal . – .  .  .  .  .1 Impact of available resources on software patch management 9\n\n[8] Kapur, P.K., Sachdeva, N. and Khatri, S.K. (2015). Vulnerability Discovery Modeling,\nInternational Conference on Quality, Reliability, Infocom Technology and Industrial\nTechnology Management, 34 –54.\n[9] Anand, A. and Bhatt, N. (2016). Vulnerability Discovery Modeling and Weighted Criteria Based\nRanking, Journal of the Indian Society for Probability and Statistics, 1 –10.\n[10] Bhatt, N., Anand, A., Yadavalli, V.S.S. and Kumar, V. (2017). Modeling and Characterizing\nSoftware Vulnerabilities, International Journal of Mathematical, Engineering andManagement Sciences (IJMEMS), 2(4), 288 –299.\n[11] Anand, A., Das, S., Aggrawal, D. and Klochkov, Y. (2017a). Vulnerability discovery modelling\nfor software with multi-versions, Advances in Reliability and System Engineering, Springer,Cham, 255 –265.\n[12] Beattie, S., Arnold, S., Cowan, C., Wagle, P., Wright, C. and Shostack, A. (2002 November).\nTiming the Application of Security Patches for Optimal Uptime, LISA, 2, 233– 242.\n[13] Jiang, Z. and Sarkar, S. (2003 December). Optimal software release time with patching\nconsidered, Workshop on Information Technologies and Systems, Seattle, WA, USA.\n[14] Arora, A., Caulkins, J.P. and Telang, R. (2006). Research note –Sell first, fix later: Impact of\npatching on software quality, Management Science, 52(3), 465 –471.\n[15] Das, S., Anand, A., Singh, O. and Singh, J. (2015). Influence of Patching on Optimal Planning\nfor Software Release and Testing Time, Communications in Dependability and QualityManagement, –An International Journal, Serbia, 18(4), 81– 92.\n[16] Deepika, Anand, A., and Singh, N. (2016). Software Reliability Modeling Based on in-house\nand Field Testing, Communications in Dependability and Quality Management- AnInternational Journal, Serbia, 19(1), 74– 84.\n[17] Anand, A., Agarwal, M., Tamura, Y. and Yamada, S. (2017b). Economic impact of software\npatching and optimal release scheduling, Quality and Reliability Engineering International,33(1), 149 –157.\n[18] Anand, A., Gupta, P., Klochkov, Y. and Yadavalli, V.S.S. (2018). Modeling Software Fault\nRemoval and Vulnerability Detection and Related Patch Release Policy, System ReliabilityManagement: Solutions and Technologies, 19– 34.\n[19] Kaur, J., Anand, A. and Singh, O. (2019). Modeling Software Vulnerability Correction/Fixation\nProcess Incorporating Time Lag, Recent Advancements in Software Reliability Assurance,39–58.\n[20] Ohtera, H. and Yamada, S. (1990). Optimal allocation and control problems for software-\ntesting resources, IEEE Transactions on Reliability, 39(2), 171 –176.\n[21] Yamada, S., Ichimori, T. and Nishiwaki, M. (1995). Optimal allocation policies for testing-\nresource based on a software reliability growth model, Mathematical and ComputerModelling, 22(10 –12), 295 –301.\n[22] Xie, M. and Yang, B. (2001). Optimal testing-time allocation for modular systems,\nInternational Journal of Quality & Reliability Management, 18(8), 854 –863.\n[23] Huang, C.Y., Lo, J.H., Kuo, S.Y. and Lyu, M.R. (2004). Optimal allocation of testing-resource\nconsidering cost, reliability, and testing-effort, In proceedings of 10th IEEE Pacific RimInternational Symposium on Dependable Computing, IEEE: 103– 112.\n[24] Jha, P.C., Gupta, D., Anand, S. and Kapur, P.K. (2006). An imperfect debugging software\nreliability growth model using lag function with testing coverage and related allocation oftesting effort problem, Communications in Dependability and Quality Management- AnInternational Journal, Serbia, 9(4), 148 –165.\n[25] Kapur, P.K., Chanda, U. and Kumar, V. (2010). Dynamic allocation of testing effort when\ntesting and debugging are done concurrently, Communications in Dependability and QualityManagement- An International Journal, Serbia, 13(3), 14 –28.10 Adarsh Anand et al.\n\n[26] Kumar, V. and Sahni, R. (2016a). An effort allocation model considering different budgetary\nconstraint on fault detection process and fault correction process, Decision Science Letters,\n5(1), 143 –156.\n[27] Kumar, V., Mathur, P., Sahni, R. and Anand, M. (2016b). Two-dimensional multi-release\nsoftware reliability modeling for fault detection and fault correction processes, InternationalJournal of Reliability, Quality and Safety Engineering, 23(3), 1640002.\n[28] Cao, Z., Lin, J., Wan, C., Song, Y., Zhang, Y. and Wang, X. (2017). Optimal cloud computing\nresource allocation for demand side management in smart grid, IEEE Transactions on SmartGrid, 8(4), 1943 –1955.\n[29] Kapur, P.K., Jha, P.C. and Bardhan, A.K. (2003). Dynamic programming approach to testing\nresource allocation problem for modular software, Ratio Mathematica, 14(1), 27 –40.\n[30] Anand, A., Das, S., Singh, O. and Kumar, V. (2019, February). Resource Allocation Problem for\nMulti Versions of Software System, In 2019 Amity International Conference on ArtificialIntelligence (AICAI), IEEE: 571 –576.\n[31] Bhatt, N., Anand, A. and Aggrawal, D. (2019). Improving system reliability by optimal\nallocation of resources for discovering software vulnerabilities, International Journal ofQuality & Reliability Management.1 Impact of available resources on software patch management 11",26686
07-2. Debugging process modeling for quality reliability assessment of software system.pdf,07-2. Debugging process modeling for quality reliability assessment of software system,"Shinji Inoue and Shigeru Yamada\n2 Debugging process modeling for quality/\nreliability assessment of software system\nAbstract: Debugging activities in a testing phase of software development are im-\nportant for eliminating software faults remained and for shipping highly reliable\nsoftware system to the user. Therefore, reflecting the debugging situation in testingactivities on software reliability assessment must be one of the useful approaches\nfor managing the software development project with accurate information on soft-\nware quality/reliability and conducting quality-oriented software management. Weintroduce two types of software fault debugging-oriented stochastic modeling ap-\nproaches for conducting mathematical model-based quality/reliability assessment.\nThese modeling approaches discussed in this chapter are expected to improve theaccuracy of model-based software reliability assessment.\nKeywords: software reliability, reliability growth models, debugging process,\ninfinite server queueing, phase-type distribution\n2.1 Introduction\nSoftware fault debugging in a testing phase is regarded as the final activity for elim-\ninating software faults still remained in the software system and for shipping highly\nreliable software systems. Generally, a software fault is debugged through the soft-ware failure observation, cause analysis, and software fault removal/elimination\nactivities. The growth of the reliability can be observed during the testing activities\nas the results of the perfect software fault removal activities. That is, the softwaredebugging activities influence on the growth process observed.\nAs the widely applied assessment technology, software reliability growth mod-\nels [1, 2] are mathematical models for describing the growth process observed, as-sessing the software reliability quantitatively, and predicting the future trend of thesoftware reliability growth process. Furthermore, these models provide us with\nsome useful information on quality-oriented software development management,\nsuch as optimal shipping time estimation and statistical testing process control.However, most of the models seem like black box type models, which do not incor-\nporate the effect of the debugging process mentioned earlier in the testing activities.\nShinji Inoue, Kansai University, Osaka, Japan\nShigeru Yamada, Tottori University, Tottorri, Japan\nhttps://doi.org/10.1515/9783110619058-002\n\nIn our knowledge, Yamada and Osaki [3] have been first proposed a nonhomoge-\nneous Poisson process-based stochastic model [4] describing the so-called S-shaped\nreliability growth process considering the two-stage software fault debugging pro-cesses, such as the software failure occurrence and the fault isolation processes.Concretely, this model has been developed by focusing on the behavior of the ex-\npected counts for software failures occurred and for software faults perfectly de-\nbugged by using the differential equations for the mean value function of thenonhomogeneous Poisson process. However, it is difficult to see the uncertainty of\nsoftware failure occurrence and the fault isolation in the debugging process di-\nrectly. And there is the lack of flexibility in modeling, that is, it is difficult to expandthis model if we need to add other activities of the debugging process and if we\nneed to tailor this model for adjusting to the testing environment managed.\nWe introduce two types of debugging process-oriented modeling approaches –\ninfinite server queueing and phase-type modeling approaches –for software reli-\nability assessment with debugging activities. These modeling technologies help us\nto develop a plausible model adjusting to the testing environment managed and toclearly understand the structure and the uncertainties in the debugging process.\n2.2 Infinite server queueing modeling approach\nThis modeling approach is for describing the uncertainty in software fault debugging\nprocesses, which consists of software failure occurrence and fault-removal processes.\nThis modeling approach is regarded as an extended model of Yamada delayed S-shaped model [3] because this modeling approach describes the uncertainties in the\nsoftware failure occurrence phenomenon and in fault-removal time flexibly based on\nthe basic assumption of their model, in which the debugging activities consists of thetwo-stage debugging processes. For the infinite server queueing approach, we as-\nsume the following situations being related to the debugging activities:\n–The total number of software failure detected follow a nonhomogeneous\nPoisson process with mean HtðÞ.\n–A software fault is completely debugged through the two-stage processes. That\nis, the software failure occurred is detected through the observation process,\nthen the fault caused this failure is debugged through the removal processes.The fault removal process contains the failure analysis activities.\n–The fault removal times are randomly behaved independently and identicallywith the cumulative probability distribution function GtðÞ.\nFollowing the basic assumptions mentioned earlier, we formulate the system for de-\nscribing software reliability growth process. Figure 2.1 shows the system configurationfor this infinite server queueing system based on the above-mentioned assumptions.14 Shinji Inoue and Shigeru Yamada\n\nNow we introduce the stochastic or counting process AtðÞ,t≥0 fg , which is the\ncumulative number of software failures observed during (0, t] and we also introduce\nanother process BtðÞ,t≥0 fg denoting the number of faults removed over (0, t].\nConsidering that the testing activities start at t=0, the stochastic process BtðÞis for-\nmulated as\nPrBtðÞ=b fg =X∞\ni=0PrBtðÞ=b,AtðÞ=i\nAtðÞ=i/C26/C27HtðÞi\ni!exp−HtðÞ ½/C138 . (2:1)\nThe conditional probability in eq. (2.1), which is the probability that bsoftware\nfaults are perfectly/completely debugged during (0, t] given that isoftware failures\nhave been observed over (0, t], is given by assuming ctðÞrepresenting the probabil-\nity that a software fault causing an observed software failure is removed by time t.\nThat is, the conditional probability is given as\nPrBtðÞ=b,AtðÞ=i\nAtðÞ=i/C26/C27\n=i\nb !\nctðÞb1−ctðÞ fgi−b, (2:2)\nwhere ctðÞis given by\nctðÞ=ðt\n0Gt−yðÞ\nHtðÞdH yðÞ. (2:3)\nIn eq. (2.3), ctðÞis formulated by following the Stieltjes convolution of the software\nfault removal time distribution and the conditional distribution of the time of a soft-\nware failure given AtðÞ=i. It is worth mentioning that this conditional software fail-\nure occurrence time distribution per software failure is derived by following the\nSoftware failure\nobservationSoftware faultremovalG(t) B(t)\nFault removed\nFault removed\nTime tA(t)\nFigure 2.1: Configuration of infinite server queueing system.2 Debugging process modeling for quality/reliability assessment of software system 15\n\nnotion of conditional arrival times distribution in infinite server queueing theory [4].\nSubstituting eqs. (2.2) and (2.3) into (2.1), we can rewrite eq. (2.1) as\nPrBtðÞ=b fg =1\nb!ðt\n0Gt−yðÞ dH yðÞ""# b\nexp −ðt\n0Gt−yðÞ dH yðÞ""#\n. (2:4)\nIn eq. (2.4)), we can see the process BtðÞ,t≥0 fg can be essentially treated as the\nnonhomogeneous Poisson process, and its expectation is finally formulated by theappropriate functions of GtðÞandHtðÞ, respectively.\nAs shown in eq. (2.4), several functions can be developed for the expectation\nof the stochastic process BtðÞ,t≥0 fg , which follows the nonhomogeneous Poisson\nprocess, by assuming the suitable functions for the GtðÞand HtðÞ, respectively.\nThat is, this modeling approach enables us to explain the details on the physical\ninterpretation on the software debuggi ng process. For example, if we assume\nthat the behavior of the fault removing time follows GtðÞ=1−exp−θt½/C138 Þ ,w h i c h\nis the exponential distribution with parameter θð>0Þ, and the software failure\nobservations follow a nonhomogeneous Poisson process with mean HtðÞ=\nω1−exp−θt½/C138 ðÞ , which is the well-known exponential (Goel –Okumoto) model [1, 2],\nwe can obtain\nð\nt\n0Gt−yðÞ dH yðÞ=ω1−1+θt ðÞ exp−θt½/C138 ½/C138 , (2:5)\nwhich is the same mathematical structure as Yamada-delayed S-shaped model. Ineq. (2.5), ωis the expected initial content in the software system and θis the soft-\nware failure occurrence and software fault-removal rates. As shown in eq. (2.5), we\ncan develop several types of models by reflecting actual situations on software fault\ndebugging activities.\n2.3 Phase-type modeling approach\nA phase-type modeling approach is rega rded as one of the modeling techniques\nfor generalizing software reliability growth models proposed so far and for devel-\noping a more plausible model, which describes a well observed software reliability\ngrowth data [5]. This approach can be applied for describing software debugging\nprocess more flexibly than the infinite server queueing approach because possiblesoftware debugging processes can be described flexibly by using a continuous-time\nMarkov chain. It should be noted that the infinite sever queueing approach describes\nonly the software debugging process developed by the two debugging processes.16 Shinji Inoue and Shigeru Yamada\n\nFor overviewing this approach, the following basic modeling assumptions for the\nphase-type modeling approach has been introduced:\n–The software contains Ω0software faults before testing, and Ω0is a random\nvariable and takes a nonnegative integer.\n–The software failure observation and fault-removing processes are contained asthe successive debugging process. The completion time for the successive pro-\ncess follows an independent and identical cumulative probability distributionfunction E\nPHtðÞ.\n–Any faults are not introduced within the process and faults causing observed soft-ware failure are perfectly/completely debugged through the debugging process.\nNow we define the process B\nPHtðÞ,t≥0 fg denoting the number of faults removed\nover (0, t], and we see that the process follows\nPr B PHtðÞ=b fg =X i\nb !\nEPHtðÞb1−EPHtðÞ fgi−bPrΩ0=i fg , (2:6)\nfrom the assumptions mentioned earlier. Assuming Ω0obeys a Poisson distribution\nwith mean αð>0Þ, the process BPHtðÞ,t≥0 fg in eq. (2.6) can be rewritten as\nPr B PHtðÞ=b fg =1\nb!αEPHtðÞ fgbexp−αEPHtðÞ ½/C138 b=0,1,2,... ðÞ . (2:7)\nFrom eq. (2.7), we see that the stochastic process BPHtðÞ,t≥0 fg has essentially the\nsame mathematical structure as eq. (2.4). However, the process has a different time-dependent expectation, that is, αE\nPHtðÞ.\nLet us consider EPHtðÞexpressing the uncertainty of the completion time of the\nsuccessive processes, which contains the soft ware failure observation and perfect fault\nremoving. Regarding the debugging process being related to the fault removal aftersoftware failure observation, it is possible to consider several processes developed by\nnot only one process but also by multiple or parallel processes. For describing such\ndebugging process flexibly, we apply a phas e-type probability distribution [6] for\nE\nPHtðÞ. The phase-type distribution describes the time uncertainty from the initial state\nto the absorption in the continuous-time absorbing Markov chain. Figure 2.2 shows a\nbasic configuration of the continuous-ti me absorbing Markov chain. It is worth men-\ntioning that the phase means the transie nt states in the absorbing Markov chain.\nNow we mention the mathematical discussion about the absorbing continuous-\ntime Markov chain, which consists of VT=1,2,..., n fg , the transient states set,\nand the absorbing state VA=n+1 fg . Generally, the instantaneous behavior within\nthe states can be expressed by an infinitesimal generator Ias\nI=SA 1\n00 !\n, (2:8)2 Debugging process modeling for quality/reliability assessment of software system 17\n\nwhere Sisn×nsubmatrix representing the instantaneous transition rate within the set\nofVTand A1is the column vector representing the rate from VTtoVA, respectively.\nFurther, 0 in eq. (2.8) is the row vector consisting of 0 elements representing no transi-\ntion from VAtoVTand no transition within the absorbing state. As mentioned earlier,\nthe phase-type distribution expresses the randomness of the time from the initial stateto the absorption. Therefore, we can consequently obtain the cumulative probabilitydistribution for the time to absorption from the initial states as\nE\nPHtðÞ=PrT≤t fg =1−π0expSt½/C1381t≥0ðÞ , (2:9)\nwhere π0represents the initial state vector and 1 is the column vector whose ele-\nments is 1. From the modeling framework in eqs. (2.7)–(2.9), we can obtain stochas-\ntic models for software reliability assessment by developing suitable continuous-\ntime absorbing Markov chain reflecting software fault debugging processes.\nOf course, this modeling approach gives us analysis method for the uncer-\ntainty of the software fault debugging processes of several models proposed sofar. For example, the debugging process of the delayed S-shaped model can beanalyzed by following this modeling approach. Figure 2.3 shows the diagram ex-\npressing state transitions of this model. The diagram shown in Figure 2.3 consists\nof the two processes: (1) the software fail ure observation process and (2) the fault-\nremoval or fault isolation process, respectively. From Figure 2.3, the infinitesimal\ngenerator for the absorbing Markov chain is\nI=−dd 0\n0−dd\n00 00\nB@1\nCA, (2:10)Transient states (phases) Absorbing state\nFigure 2.2: Configuration of continuous-time absorbing Markov chain.18 Shinji Inoue and Shigeru Yamada\n\nandπ0=100ðÞ . Further, we can see that VT=1,2fg ,VA=3fgand\nS=−dd\n0−d !\n, (2:11)\nrespectively. Then, we can obtain the phase-type probability distribution as\nEPHtðÞ=1−100ðÞ exp−dd\n0−d !\nt""# 1\n110\nB@1\nCA=1−1+dtðÞ exp−dt½/C138 . (2:12)\nSubstituting eq. (2.12) into eq. (2.7), the mean value function reflecting the software\nfaults debugging processes can be obtained as αE\nPHtðÞ=α1−1+dtðÞ exp−dt½/C138 fg ,\nwhich is essentially the same mathematical structure as the delayed S-shaped\nmodel. Several types of model reflecting debugging processes can be developed by\nfollowing the procedure mentioned earlier. However, it is observed that the numberof parameters might increase if the details in the debugging process and different\ntransition rates within the chain are considered. Therefore, useful procedures for\nestimating the parameters, such as the EM algorithm needs to be investigated [5].\n2.4 Conclusion\nWe slightly introduce the software fault debugging process-oriented reliability\ngrowth modeling approaches, such as infinite server queueing and phase-type\nmodeling approaches. These two modeling approaches yield software reliabilitygrowth modeling frameworks reflecting the uncertainties of software fault de-bugging processes on software reliability assessment and prediction. Some other\nmodels could be developed based on these modeling frameworks with the diffi-\nculty of fault debugging or some attribute s being related to the debugging activi-\nties. Further, these debugging process-oriented modeling techniques lead to\nquality-based software dev elopment project management by analyzing the effi-\nciency of debugging activities in the testing activities, which are cost- and time-\nconsuming activities.\n11 23\n(2) (1)dd\nFigure 2.3: Absorbing Markov chain for the delayed S-shaped model.2 Debugging process modeling for quality/reliability assessment of software system 19\n\nBibliography\n[1] Pham, H. (2000). Software Reliability, Springer-Verlag, Singapore.\n[2] Yamada, S. (2014). Software Reliability Modeling –Fundamentals and Applications, Springer\nJapan, Tokyo, Japan.\n[3] Yamada, S. and Osaki, S. (1985). Software reliability growth modeling: Models and\napplications, IEEE Transactions on Software Engineering, SE-11, 1431 –1437.\n[4] Ross, S.M. (2010). Introduction to Probability Models, Elsevier, San Diego, CA, USA.[5] Okamura, H. and Dohi, T. (2016). Phase type software reliability model: parameter estimation\nalgorithms with grouped data, Annals of Operations Research, 244, 177 –208.\n[6] Buchholz, P., Kriege, J. and Felko, I. (2014). Input Modeling with Phase-Type Distributions and\nMarkov Models: Theory and Applications, Springer, Cham, Switzerland.20 Shinji Inoue and Shigeru Yamada",16379
08-3. Availability analysis of vehicular cloud computing.pdf,08-3. Availability analysis of vehicular cloud computing,"Shivani Gupta and Vandana Gupta\n3 Availability analysis of vehicular cloud\ncomputing\nAbstract: Vehicular cloud computing is a new paradigm that makes use of cloud\ncomputing resources to overcome the restraints of vehicular computing. It allows\nthe sharing of resources such as storage capacity, computational power, andInternet connectivity from those vehicles in which these resources remain idle for\nlong hours (in parking or heavy traffic jams). This chapter presents an availability\nstudy of the vehicular cloud. Due to its multilayered architecture, composite model-ing technique is desired for availability analysis of vehicular clouds. Distinct models\nare developed for each subsystem using reliability block diagrams and semi-Markov\nprocess, and the models are then combined to evaluate the availability of the com-plete system. Two different sensitivity analysis techniques (partial derivatives andpercentage difference) to determine the parameters that cause the greatest impact on\nthe availability of the vehicular cloud are also applied. The analysis reflects that the\navailability can be improved by aiming on reduced set of parameters that affect theavailability most.\nKeywords: Vehicular cloud computing, availability analysis, composite modeling,\nreliability block diagram, semi-Markov process, sensitivity analysis\n3.1 Introduction\nVehicular ad hoc network (VANET) communicates information (in the form of data\npackets) regarding incidents such as congestions on road, accidents, and other\nroad conditions that are away from the driver ’s knowledge. This significantly en-\nhances the reliability and effectiveness of the transportation system. The amalgam-\nation of numerous devices such as global positioning system (GPS), general packetradio service (GPRS), various sensors, and interfaces has contributed in making the\nvehicles smart. Apart from providing the above-mentioned security services, smart\nvehicles also facilitate the users with various infotainment services (e-mail, videodownloading and sharing, web browsing, etc.). Due to the increase in number of\napplications, the demand for resources such as storage capacity, internet connectiv-\nity, and computational power is increasing rapidly.\nGupta Shivani, Vandana Gupta, University of Delhi, Delhi, India\nhttps://doi.org/10.1515/9783110619058-003\n\nIn such situations, a single vehicle is sometimes unable to provide all the re-\nquired resources. At the same time, the computational resources of some vehicles\nremain idle for long hours (in parking or heavy traffic jams). Therefore, sharing ofthese resources is suggested to utilize the idle resources, and also to provide satis-factory services to the users. This sharing of under-utilized resources is termed as\nvehicular cloud computing (VCC). It is inspired by cloud computing (CC) and mobile\ncloud computing (MCC). CC is the sharing of computing services such as storage,networking, servers, databases, intelligence, and analytics through the internet.\nMCC can be simply characterized as an infrastructure in which both the data proc-\nessing and data storage take place outside the mobile device. It is based on the per-ception that businesses can operate by borrowing the necessary software and\ninfrastructure without spending money in purchasing them.\nIn a VCC network, vehicles connect with each other to either share their resour-\nces or to rent out. The roadside infrastructure is also a part of VCC as they can alsorent out their resources. Each vehicle in a VCC network can act as both a service\nuser and as a service provider. As an example, a VCC network can be created byusing computing resources of the vehicles parked in the parking of a company. Inthis VCC network, the company doesn’ t need to spend money in buying computing\nresources, and the owners also get some money for sharing their idle resources. Inthis way it helps both the company as well as the owners of the vehicles. Similarly,a VCC network can also be formed in traffic jams to update people stuck in traffic\nabout traffic conditions and also to transmit the data efficiently.\nThese days every automobile-making company has completely accepted that\ncloud is necessary to provide competitively distinctive services and features for cur-rent and future users. As per Gartner, quarter billion vehicles will be connected on\nroad by 2020 [1]. Due to such huge numbers, it is necessary to deliver trustworthyservices, that is, dependable services.\nThe term dependability is used to define the system’ s ability to facilitate the\nusers with those services that can be trusted justifiably within a time period [2].Dependability is critical for both service providers and users. It is a comprehensive\ntheory that includes measures of system’ s availability, reliability, maintainability,\nsecurity, integrity, and so on. [2 –4]. Availability is one of the key attributes of de-\npendability that directly relates to the fraction of time the system is in functioning\nstate [5]. There are numerous types of models that are employed to evaluate the an-\nalytical measures, which are analogous to system’ s availability. These models are\nbroadly divided into two categories: state-space models and nonstate-space models.\nMarkov chains and stochastic petri nets are the examples of state-space models\nwhereas, fault trees, reliability block diagrams (RBDs) are the examples of non-\nstate-space models. State-space models facilitate the portrayal of intricate connec-tions among system’ s components, while nonstate-space models acknowledge a\nmuch compact representation of the system. Hierarchical combination of both the22 Shivani Gupta and Vandana Gupta\n\nmodels, that is, state-space models and nonstate-space models provide us with the\nbest of both the modeling approaches [6].\nIn this chapter, we adopt the hierarchical modeling approach to perform the\navailability analysis of VCC. Distinct models are proposed for each subsystem of the\nVCC, and the developed state-space and nonstate-space models are then combined\nto evaluate the availability for the complete VCC network. Sensitivity analysis is\nalso performed to determine those parameters that impact the steady-state avail-ability the most, that is, a small change in the value of the input parameter will sig-\nnificantly affect the availability. We have used two different techniques, namely,\npartial derivative technique and percentage difference technique for the sensitivityanalysis. Two different techniques are used so that the results obtained from one\ntechnique can be verified by the other.\n3.2 Literature review\nDue to the advancement in the field of communication and computational technol-\nogies, automobile industry is experiencing a progressive change. CC is a paradigmthat can utilize the advance computing resources available in the smart vehicles.\nThe integration of CC with the vehicular networking develops a new paradigm,\nknown as VCC. Authors in [7 –9] have explained the concept of VCC, which facili-\ntates the sharing of internet connectivity, storage, and computing power among the\nusers. Plenty of research is available in literature giving an overview of the VCC ’s\narchitecture, features, applications, and security challenges [10 –13].\nIn [14] authors have suggested a new scheme to enhance the availability of\ncloud services and to minimize the end to end latency in vehicular communication.\nAuthors in [15] have proposed a message confirmation system to make the commu-nication more reliable in VCC. In [16] authors have studied the effect of social rela-\ntion on mobile video offloading services. They have proposed a resource allocation\nscheme based on continuous-time Markov decision process. It enhances the qualityof entertainment demands. Authors in [17] have proposed a resource schedulingand task allocation algorithm to accomplish numerous task requests coming from\nthe VCC users in an adequate manner. It upgrades the quality of service of the VCC\nnetwork. In [18] authors have highlighted the issue of inefficiency in informationtransmission in vehicular cloud. They h ave proposed a network coding method\nthat helps in improving the reliability and efficiency in the information transmis-sion. Authors in [19] have addressed the i s s u eo fs e c u r i t ya n dp r i v a c yi nV C C ,a n d\ndeveloped a security and privacy-aware information transmission environment\nbetween the cloud infrastructure and vehi cular nodes. Authors in [20] have dealt\nwith the dynamic nature of the vehicular cloud. They have proposed a vehicular\ncloud model that possesses different task requests types and different resource3 Availability analysis of vehicular cloud computing 23\n\ncapability types. It reduces the failure rate in the vehicular cloud. In all these pa-\npers discussed earlier, we feel that availability study of the VCC architecture is im-\nmensely ignored. This motivated us to study the availability of a VCC network usinganalytical modeling approach. None of the authors have focused on the availability\nanalysis of the vehicular cloud. The next section discusses about the major contribu-\ntion of this chapter.\n3.2.1 Our contribution\nThe major contributions of this chapter are listed as follows:\n–In this chapter the availability of the VCC architecture is evaluated using hier-\narchical modeling approach.\n–Sensitivity analysis is also performed using two different techniques to compute\nthe effect of each input parameter on the steady-state availability. Two different\ntechniques have been used so that the results obtained from one technique canbe verified by the other.\n–With the help of sensitivity analysis those parameters that cause great impact\non the availability of the VCC architecture are identified.\nThe chapter is summarized as follows: In Section 3.3 the architecture of VCC is dis-cussed. In Section 3.4 different availability models for all the components of VCC\narchitecture are proposed. Further, in Section 3.5 steady-state availability of thewhole VCC architecture is evaluated. Also, sensitivity analysis is performed through\ntwo different techniques. Finally, the chapter is concluded in Section 3.6.\n3.3 Vehicular cloud computing architecture\nThe VCC architecture is divided into three layers: Inside Vehicle ,Communication\nlayer ,and Cloud [21]. The hierarchical architecture of VCC is shown in Figure 3.1. In\nthe inside vehicle layer, the major component considered is the On-board unit (OBU).The OBU consists of a control unit (CU), GPS, GPRS, various input/output interfaces,\nand some sensors (such as body sensors, environmental sensors, and driver ’s behav-\nior recognition). The second layer of this architecture is the communication layer.\nTwo types of communication are possible in vehicular communication network: (i)\nvehicle-to-vehicle communication (V2V), and (ii) vehicle-to-infrastructure communi-cation (V2I). In V2V communication vehicles communicate with each other through\nDedicated short-range communication (DSRC), whereas in V2I communication ve-\nhicles communicate with the roadside infrastructure (street light, traffic signal poles,etc.) into which the equipment for wireless network such as 3G or Wi-Fi is installed.24 Shivani Gupta and Vandana Gupta\n\nThe upper most layer of this architecture is the cloud , which is further divided into\nthree sublayers: cloud infrastructure ,cloud platform ,a n d cloud primary and real-time\napplication services . Cloud infrastructure is divided into two parts: cloud storage and\ncloud computation . The data collected by the inside vehicle layer is stored in this\ncloud storage. Numerous government and private firms, especially the police depart-\nment can use this stored data in the cloud for various studies [21]. Cloud computation\nperforms computing tasks using the data stored in cloud storage. It can carry outcomplex calculations in nominal time. In t he cloud primary and real-time applica-\ntion services, numerous applications and services are provided, which can beInside vehicle\nOBURoad\ninformationTraffic\ninformation\nGISStorage\nservicesDriver \nbehavior\ninformation\ninfrastructureCloud Cloud \ncomputationCloud \nstorageNaaS\nSTaaSIaaSData center\nPay\nas you goENaaS PiCaaS\nSaaS\nINaaS CaaSCloud primary application services\nVehicle−to−vehicle\ncommunicationVehicle−to−infrastructure\ncommunicationCommunication layerCloud platformEnvironmental\nrecognitionActivity\nrecognitionHealth\nrecognitionFuel\nfeedbackReal-time applications\nFigure 3.1: Vehicular cloud computing architecture.3 Availability analysis of vehicular cloud computing 25\n\ndirectly accessed by the users. Health reco gnition, environmental recognition,\nand fuel feedback are some examples of real-time applications provided by the\ncloud. The primary application services in the cloud are broadly divided into twocategories: Infrastructure as a Service (IaaS) and Software as a Service (SaaS).IaaS includes those applications in which s ome virtual as well as physical storage\noptions, servers, or networks are required such as Network as a Service (NaaS),Storage as a Service (STaaS), Data Center, pay-as-you-go, whereas SaaS is relatedto infotainment and driver comforting applications such as Entertainment as a\nService (ENaaS), Cooperation as a Service ( C a a S ) ,P i c t u r e so nw h e e l sa saS e r v i c e\n(PiCaaS), and so on [22]. NaaS can also be delivered as a SaaS.\nIn this chapter, in cloud platform layer, we majorly focus on IaaS model. It is\nthe most fundamental level of cloud services. An open source IaaS service provider\nis EUCALYPTUS –Elastic Utility Computing Architecture Linking Your Programs to\nUseful Systems. Its interface is appropriate for the economic services such as\nAmazon EC2 (Elastic Compute Cloud) and Amazon S3 (Simple Storage Service) [23].\nThe architecture of EUCALYPTUS consists of five high-level components: cloud con-troller (CLC), cluster controller (CC), node controller (NC), storage controller (SC),and Walrus. Each of these components has its own web interface. These compo-\nnents are explained as follows [24]:\n–CLC is the entry point of the complete cloud infrastructure. Besides receiving\nrequests from client side through user interface, it also interacts with the re-maining components of the EUCALYPTUS.\n–CCis the entry point of the cluster . It interacts with both NCs and CLC. Its major\ntasks are to determine which NC will perform the incoming service requests, tocollect information regarding the nodes that constitute its cluster, and to man-\nage the virtual network overlay.\n–NCis possessed by every physical node that is assumed to run virtual machines\n(VM). It manages the implementation, analysis, and completion of VM instan-ces on the hosts where it runs.\n–SCfacilitates with constant block storage that is used by VM instances.\n–Walrus provides a mechanism for storing VM images. It is an interface adapt-\nable file-based data storage service. It also allows user to flow data inside andoutside the cloud.\nIn this chapter, we consider the EUCALYPTUS-based cloud environment to developthe availability model and assume that it has three clusters. Each cluster has oneCC, one SC, and several NC. Components of each cluster communicate with the CLC\nand Walrus in order to manage service requests.26 Shivani Gupta and Vandana Gupta\n\n3.4 Availability models for vehicular cloud\nThe availability model for the VCC architecture discussed in Section 3.3 is shown in\nFigure 3.2. The architecture is modeled through an RBD. RBD is a tool to modelcomplex systems. The different blocks of the RBD represent the various components\nof the VCC such as OBU, V2I, V2V, Cloud Storage, CLC, and CC subsystems. Once\nthe blocks are figured correctly, various system measures such as reliability, avail-ability, MTBF, and failure rate, can be calculated from the RBD. To evaluate the\navailability of vehicular cloud from the RBD shown in Figure 3.2, distinct availabil-\nity models for each block is developed, and then the results obtained from the sub-models are combined to evaluate the system’ s availability.\nHence, essentially in this section, the availability models developed for each\ncomponent of the VCC network is discussed, and then the availability of the com-plete VCC network by the amalgamation of all the submodels will be evaluated.\n3.4.1 Availability model for on-board unit\nThe availability model for the OBU is shown in Figure 3.3. The model is mappedthrough an RBD. We have considered various components of OBU that can affect itsavailability. A brief introduction of its components is as follows [25]:\n–CUis one of the major components of OBU. It facilitates the OBU in maintain-\ning records regarding the incidents disclosed, processed, and transmitted to itby the roadside infrastructure. It also allows the OBU to coordinate and connect\namong its external devices.\n–GPS provides the geolocation and time information to the users.\n–GPRS enables wireless communication in vehicular environment. It also pro-\nvides Internet facility.\n–Input/output (I/O) interface provides communication links between internal\nsystem and input –output devices.\n–Various sensors are available inside and outside the vehicles. They collect in-\nformation regarding pressure and temperature inside the car, driver ’s behavior,\nand mood as well as send all these data to cloud storage or to the software for\nvarious real-time application services.\nWe consider a series combination of all the components of OBU because unavail-ability of any one of the components affects the availability of the OBU. The closed-\nform equation for availability of the RBD shown in Figure 3.3 is given as\nA\nOBU =ACU×AGPS×AGPRS ×AI=O×ASensors . (3:1)3 Availability analysis of vehicular cloud computing 27\n\nOBUCC SC\nV2I\nV2VCloud\nStorage\nCLC Walrus CC\nCCNC1\nNCn\nNC1\nNCn\nNC1\nNCnSC\nSCCloud\nComputation\nCloud\nplatform Cloud\ninfrastructureCommunication\nlayerInside\nvehicle\nFigure 3.2: Availability model for vehicular cloud architecture.28 Shivani Gupta and Vandana Gupta\n\nEach term Aiin the above-mentioned equation represents the availability of the\nith component, where iɛ{CU, GPS, GPRS, I/O, Sensors}. The availability of the ith\ncomponent is calculated using the formula given as follows:\nAvailability =MTBF\nMTBF +MTTR. (3:2)\nNote that in this chapter the values of MTBF and MTTR are not evaluated by us and\nare actually borrowed from the literature.\n3.4.2 Availability model for vehicle-to-vehicle communication\nIn V2V communication, vehicles communicate with each other. The availability of\nV2V communication network depends upon the number of OBUs (i.e., vehicles) in\nfunctioning state among all the OBUs that can communicate within the transmission\nrange [25]. Let us consider that the transmission range of an OBU is Rand within the\ntransmission range there can be at most Nvehicles. Therefore, the V2V communica-\ntion network will be available if there are at least two OBUs in functioning conditionamong the NOBUs. Considering that the OBUs in all the vehicles are identical, the\navailability for V2V communication can be mathematically expressed as\nA\nV2V=XN\nK=2N\nk !\nAk\nOBU1−AOBU ðÞN−k, (3:3)\nwhere AOBUcan be obtained from eq. (3.1).\n3.4.3 Availability model for vehicle-to-infrastructure\ncommunication\nIn V2I communication vehicles communicate with the roadside infrastructure. Theequipment for wireless network (such as 3G/4G/5G or Wi-Fi) is installed into these\nroadside infrastructures. In this chapter, we assume that only 5G network is available\nfor communication. Therefore, for the availability of V2I communication network, atleast one OBU and the wireless network should be in functioning condition. Hence,\nthe availability of V2I communication network can be mathematically expressed asGPS\nmoduleGPRS\nmoduleI/O\ninterfaceVarious\nsensorsControl\nunit\nFigure 3.3: Availability model for OBU.3 Availability analysis of vehicular cloud computing 29\n\nAV2I=AV2V×A5G, (3:4)\nwhere A5Gdenotes the availability of the 5G network. AV2Vis evaluated using eq. (3.3),\nandA5Gis given as\nA5G=μ5G\nμ5G+λ5G. (3:5)\nHere the parameters µ5Gandλ5Gare the repair and failure rates, respectively, of the\n5G network.\n3.4.4 Availability model for cloud storage\nIn cloud storage the data is stored in logical pools. The physical storage connects\nwith different servers (sometimes in different locations), and this physical envi-\nronment is inherited and governed by a hosting company. These companies areresponsible for the applicability and achievability of the data and to keep it run-ning. Further, maintaining the integrity and accessibility of data is also their re-\nsponsibility. Cloud storag e services are accessed by the users through application\nprogramming interface (API) or by applications that handle the API such as web-\nbased content management systems, cl oud storage gateway, or cloud desktop\nstorage [26]. The availability model that represents the cloud storage is shown inF i g u r e3 . 4 .T h em o d e li sd e v e l o p e dt h r o u g ha nR B D .I nt h eR B Dm o d e lw eh a v econsidered n locations in parallel since in cloud storage identical data is stored in\ndifferent servers at different locations f or backup. A closed-form equation for the\navailability of cloud storage is given as follows (Figure 3.4):\nA\nStorage =AAPI×Astorage pool ×1−1−AVCserver ðÞ ×1−APSserver ðÞ ðÞn(3:6)\nwhere AAPI,Astorage_pool ,AVC_server ,a n d APS_server represent the availability of API, log-\nical storage pool, virtual compute server, and physical storage server, respectively.\nThe availability of all these components can be evaluated from the formula given ineq. (3.2). The equation is obtained through the general series –parallel equations for\nRBDs as discussed in [27].\n3.4.5 Availability model for cloud controller\nThe CLC is a single component unlike the other components of VCC network dis-\ncussed earlier. We assume that a redundant CLC is also available in warm stand-by to\nprovide uninterrupted service. Therefore, to evaluate the availability of CLC, we makeuse of state-space model. The availability model for CLC is represented in Figure 3.5.30 Shivani Gupta and Vandana Gupta\n\nPublic\nAPIVirtual \ncompute \nserver\nPhysical\nstorage\nserver\nVirtual \ncompute \nserver\nPhysical\nstorage\nserverLogical\nstorage poolLocation1\nLocation n\nFigure 3.4: Availability model for cloud storage.\nAW\n0\nAD\n1DS\n2\nDA\n3\nDD\n4Repair\nSwitchingRepair\nRepairFailureFailure\nFailureFailure\nFigure 3.5: Availability model for cloud controller and cluster controller.3 Availability analysis of vehicular cloud computing 31\n\nThe proposed state-space model has five states: AW,AD,DS,DA,a n d DD. The first\nletter of the state represents the status of primary CLC and the second letter repre-\nsents the status of the secondary CLC. The letter Adenotes active, Wdenotes waiting,\nSdenotes switching, and Ddenotes down. A brief description of all the states is dis-\ncussed as follows:\n–State AW: Primary CLC is active and secondary CLC is waiting in warm stand-\nby mode.\n–State AD: Primary CLC is active and secondary CLC got down while working in\nwarm stand-by mode.\n–State DS: Primary CLC is down and switching of secondary CLC over primary is\nin process.\n–State DA: Primary CLC is down and secondary CLC is active.\n–State DD: Both the primary and secondary CLCs are in down state.\nInitially, the system is in state AW, that is, the primary CLC is active and secondaryCLC is in warm stand-by mode. When the primary CLC fails, the system moves tothe state DS. Here, a switching of stand-by CLC to primary CLC is performed. Next,the system moves to the state DA, where primary CLC is down and secondary CLC is\nactive. If the secondary CLC got down before the repair of the primary CLC, then the\nsystem moves to the state DD. In this model we assume that there is a single repairfacility, and the primary CLC repair has preference over the secondary CLC repair.\nHence, whenever the system reaches to state DD, there is only one possibility of\ngoing to state AD. Also, if the secondary CLC got down in stand-by mode then thesystem reaches to state AD from state AW.\nIt is noticeable that the transition from one state to another is affected by multi-\nple factors such as system parameters and it shows random behavior. This randombehavior can be modeled by stochastic process such as Markov chain, Poisson pro-\ncess, SMP, and so on. Since the switching time from the primary CLC to the second-\nary CLC is deterministic, therefore, the time spent in state DS, that is, its sojourntime is nonexponential. Hence, this model cannot be mapped through a simpleMarkov chain. Further, if we consider the sojourn times of all the states to be expo-\nnentially distributed, then this may produce some critical errors. Therefore, in order\nto capture real-time situations, we consider that the sojourn time of different statesmay or may not be exponentially distributed, and hence an SMP can be used to\nmodel the availability of the CLC. To further facilitate the modeling using an SMP\nwe assume that the Markov property is held at each transition epoch. Hence, wemodel the state transition diagram for CLC as an SMP [4]. The list of cumulative dis-\ntributive functions (CDFs) of time spent in each state along with the distribution\nfunction and parameters is mentioned in Table 3.1. In the table, F\nlmdenotes the\nCDF related to the arc l!m(where l,m= 0, 1, 2, 3, 4).32 Shivani Gupta and Vandana Gupta\n\nIn the SMP discussed earlier, system is available to the users in all the states\nexcept the down state. Therefore, the steady-state availability is obtained as\nACLC=π1+π2+π3=1−π4 (3:7)\nwhere πIrepresents the steady-state probability of state i(where i= 0,1,2,3,4).\nThese steady-state probabilities are evaluated from the steady-state analysis of\nthe SMP [4; 28].\n3.4.6 Availability model for cluster controller\nSimilar to CLC, the CC is also a single component for which warm stand-by replica-tion strategy is assumed. Therefore, the availability model for CC is identical to the\navailability model for CLC is shown in Figure 3.5. The meaning of all the states ofthe availability model for CC shown in Figure 3.5 is same as defined for CLC. The\nsojourn time of all the states also follow the same distribution as in the case of CLC.\nHence, the steady-state analysis is also identical for both CLC and CC. Therefore,the steady-state availability of CC is also obtained as\nA\nCLC=π1+π2+π3=1−π4 (3:8)\nwhere πIrepresents the steady-state probability of state i.\n3.4.7 Availability of vehicular cloud computing architecture\nIn this section, we evaluate the availability of the whole VCC architecture by combin-ing the availabilities of the submodels obtained in the earlier sections. A closed-formTable 3.1: List of distributions.\nCDF Distribution Parameter\nF\nF\nF\nF\nF\nF\nF\nFExponential\nExponential\nExponential\nExponentialDeterministicExponentialExponentialExponentialλ\n\nλ\nλ\nλ\nTλ\n\nλ\nλ3 Availability analysis of vehicular cloud computing 33\n\nequation for availability of the VCC system, obtained from the top-level RBD shown\nin Figure 3.2 is as follows:\nAsys=AOBU ×ð1−ð1−AV2IÞ×ð1−AV2VÞÞ×ð1−ð1−AStorage Þ×ð1−ACompÞÞ\n×ACLC×AWalrus ×ð1−ð1−ACC×ASC×ð1−Πn\ni=11−ANCi/C0/C1nÞÞ3Þ\nEach term Axin the equation is obtained by solving their corresponding submodels,\nwhere xɛ{OBU, V2I, V2V, Storage, Comp, CLC, Walrus, CC, SC, NC i};i=1,n. The\navailabilities of the blocks illustrating cloud computation, Walrus, SC, NC iare cal-\nculated by using eq. (3.2).\n3.5 Numerical illustration and sensitivity analysis\nIn this section, we perform numerical experiment and evaluate the steady-state\navailabilities of all the components of the VCC that have been discussed in the ear-lier section. The values of the input parameters for the OBU are given in Table 3.2.The values of MTBF and MTTR of all the components of OBU are taken from [25].\nFurther in Table 3.3 values of the input parameters for the communication layer are\nmentioned. The parameter values for 5G communication are referred from [5]. InTables 3.4 and 3.5, the values of MTBF and MTTR of all the components of cloud\ninfrastructure layer and cloud platform layer are given. These values are adopted\nfrom [24]. Table 3.6 contains the values of the transition rates of the SMP model forCC and CLC as shown in Figure 3.5. Same set of values are considered for both the\nCC and the CLC model. These values are also taken from [24].\nTable 3.2: Input parameters for the OBU.\nComponent MTBF (h) MTTR (h)\nControl unit\nGPSGPRS.\n.\n.\nI/O interface  \nVarious sensors   .\nTable 3.3: Input parameters\nfor communication layer.\nParameter Value\nλG .\nµG\nN.\n34 Shivani Gupta and Vandana Gupta\n\n3.5.1 Availability analysis\nThe availability measures are computed for each submodel discussed in Section 3.4\nusing the input parameters mentioned earlier. Then the availability of the wholeVCC network is evaluated using eq. (3.9). Along with the steady-state availability,the following two measures are also evaluated:\n–Number of nines: It provides a logarithmic perspective of availability. It is cal-\nculated by using the following formula:\nNumber of nines =−log\n10xTable 3.4: Input parameters for the cloud infrastructure\nlayer.\nComponent MTBF (h) MTTR (h)\nAPI  \nStorage_pool . .\nVC_server , .\nPS_server , .\nCloud computation . .\nTable 3.5: Input parameters for the cloud platform layer.\nComponent MTBF (h) MTTR (h)\nWalrus .\nSC .\nNC .\nTable 3.6: Input parameters for the CC and CLC.\nParameter Value (h) Parameter Value (h)\nλ . λ .\nλ . λ .\nλ . λ .\nλ . T .3 Availability analysis of vehicular cloud computing 35\n\nwhere xrepresents the unavailability of the system. If a system is available to the\nusers throughout the year then it is said that it has 5 nines availability or class 5\navailability.\n–Downtime: Downtime provides the total hours in a year for which the system\nis in down state.\nThe results of the analysis are summarized in Table 3.7. As per the analysis, thesteady-state availability of the VCC network is 0.991501. It has approximately 2\nnines availability. The downtime value of 74.5 h/year indicates that the system is\nunavailable to the users for more than three days in a year. All these results reflectthat there is a need to increase the availability of the system.\nIn the next section, we have applied different sensitivity analysis techniques to rec-\nognize those parameters that can affect the availability of the system.\n3.5.2 Sensitivity analysis\nSensitivity analysis is a technique that helps in identifying those input parametersthat impact the output measure most, that is, a small change in the value of inputparameter will significantly affect the output measure. The main objective of sensi-\ntivity analysis is to determine those parameters that are the bottlenecks for the\nsteady-state availability. It also helps us to select those parameters that have mini-mal effect on the steady-state availability, so that we can fairly ignore them. Thereare numerous methods for performing sensitivity analysis. The fundamental\nmethod among all is to change one parameter at a time while keeping the others\nconstant. A sensitivity ranking is obtained after performing sensitivity analysis bynoting the changes to the output measure. In this chapter two techniques, namely,\npartial derivative technique and percentage difference technique are used to per-\nform sensitivity analysis. We are using two different techniques so that the resultsobtained from one technique can be verified by the other.Table 3.7: Availability results.\nAvailability Number of nines Downtime (h/year)\n.  .36 Shivani Gupta and Vandana Gupta\n\n3.5.2.1 Partial derivative technique\nDifferential analysis is the basis of many sensitivity analysis techniques [5]. In this\ntechnique we simply evaluate the partial derivative of the measure with respect toeach input parameter. The sensitivity coefficient SS of the measure Z with respect tothe input parameter ϴis evaluated with the help of eqs. (3.10) and (3.11) [29]:\nS\nθZðÞ =∂Z\n∂θ(3:10)\nSSθZðÞ =∂Z\n∂θθ\nZ/C18/C19\n(3:11)\nHere, in eq. (3.11) the term ( ϴ/Z) is a normalizing coefficient that is introduced to\nremove the effect of unit.\nTo determine those parameters that significantly affect the steady-state avail-\nability, we evaluate sensitivity coefficient for each parameter and then arrange theirnonnegative values in decreasing order. The sensitivity ranking obtained through\nthis technique also facilitates in identifying those parameters that have less impact\non the availability of the system. Parameters having higher sensitivity coefficientvalues affect the availability most. On the other hand, the parameters having lowersensitivity coefficient values have least effect on the availability.\nTable 3.8 displays the sensitivity ranking of steady-state availability of VCC net-\nwork for each input parameter by using eqs. (3.10) and (3.11).\nIn the table the top-ranked parameters are the failure and repair rates of storage\npool, input –output interface, API, Walrus, and CLC, respectively. Input–output in-\nterface and API both are interfaces that facilitate the user to access the services ofvehicular cloud system. On failure of any of these, the VCC services will become\ndisabled for the users which will affect its availability. Also, storage pool and\nWalrus act as storage systems in which all the data related to VCC services arestored. Hence, their failure will make the VCC system unavailable to the users. CLC\ncontrols the complete cloud infrastructure. Therefore, failure of CLC will result in\nthe unavailability of the VCC services for the users. Hence, all the above-discussedparameters are of more importance as compared to other parameters and should begiven higher priority as compared to others in improving the availability of the\nsystem.\nThe next set of parameters are related to the inside vehicular environment.\nAvailability of these components is also important so that the user can connect andcoordinate with the outside environment. Failure and repair rate of 5G network is\nalso important as it is the only medium that facilitates communication among theusers.\nIn the sensitivity ranking the parameters appearing at the last are related to\ncloud storage and cloud platform. Due to its parallel structure in the RBD it is ex-pected that they will have lower sensitivity ranking.3 Availability analysis of vehicular cloud computing 37\n\nFigure 3.6 provides a graphical representation of the steady-state availability\nwith respect to the first 15 parameters of Table 3.8. Only first 15 parameters have\nbeen presented in Figure 3.6, because for the other parameters the variation in theavailability is unnoticeable. It just confirms the conclusion that the low-ranked pa-rameters should get less importance when one is aiming to improve the availability\nmeasure.\nTo further verify the results obtained from partial derivatives, sensitivity analy-\nsis through percentage difference technique is performed.\n3.5.2.2 Percentage difference technique\nIn this technique we vary one input parameter from its minimum value to its maxi-\nmum value. The main advantage of this technique over partial derivative technique isTable 3.8: Sensitivity ranking based on partial derivatives.\nSensitivity ranking Parameter ( ϴ) Description |SSϴ[A)|\n λstorage pool Failure rate-storage pool .\n µstorage_pool Repair rate-storage pool .\n λI=O Failure rate-I/O interfaces .\n µI/O Repair rate-I/O interfaces .\n λAPI Failure rate-API .\n µAPI Repair rate-API .\n λWalrus Failure rate-Walrus .\n µWalrus Repair rate-Walrus .\n λrep CLC Repair rate-CLC .\n λsensors Failure rate-Sensors .\n µsensors Repair rate-Sensors .\n λGPRS Failure rate-GPRS .\n µGPRS Repair rate-GPRS .\n λCU Failure rate-CU .\n µCU Repair rate-CU .\n λGPS Failure rate-GPS .\n µGPS Repair rate-GPS .\n λ5G Failure rate- G .\n µG Repair rate- G .\n λPSserver Failure rate-physical storage server .\n µPS_server Repair rate-physical storage server .\n λVCserver Failure rate-virtual compute server .\n µVC_server Repair rate-virtual compute server .\n λSC Failure rate-storage controller .\n µSC Repair rate-storage controller .\n λrep CC Failure rate-CC .\n λNC Failure rate-node controller .\n µNC Repair rate-node controller .38 Shivani Gupta and Vandana Gupta\n\n123456789 1 0\nFailure rate-storage pool ×10 –30.991490.9914920.9914940.9914960.9914980.99150.9915020.9915040.9915060.9915080.99151Steady-state availabilityBaseline availability Baseline availability\n0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1\nRepair rate-storage pool0.991470.9914750.991480.9914850.991490.9914950.99150.991505Steady-state availability\n0.9 1 1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8\nFailure rate-input/output interface0.990.99020.99040.99060.99080.9910.99120.99140.99160.9918Steady-state availability\n×10 –3Baseline availability\nBaseline availability\n0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5 0.55 0.6\nRepair rate-input/output interface0.980.9820.9840.9860.9880.990.992Steady-state availability\nBaseline availability\n123456789 1 0\nFailure rate-API0.99140.991420.991440.991460.991480.9915Steady-state availability\n×10 –3123456789 1 0\nFailure rate-Walrus0.9720.9740.9760.9780.980.9820.9840.9860.9880.99Steady-state availability\n×10 –3Baseline availability\n1 1.5 2 2.5 3 3.5 4 4.5 5 5.5\nRepair rate-Walrus0.9840.9850.9860.9870.9880.9890.990.9910.9920.993Steady-state availabilityBaseline availability\nFigure 3.6: Graphical representation of variation in steady-state availability.3 Availability analysis of vehicular cloud computing 39\n\n123456789 1 0\nRepair rate-sensors0.99060.99080.9910.99120.99140.99160.9918Steady-state availabilityBaseline availability Baseline availability\n123456789 1 0\nRepair rate-sensors0.99060.99080.9910.99120.99140.99160.9918Steady-state availability\nBaseline availability\n123456789 1 0\nFailure rate-GPRS0.9890.98950.990.99050.9910.99150.992Steady-state availability\n×10 –3Baseline availability\n123456789 10\nRepair rate-GPRS0.990.99020.99040.99060.99080.9910.99120.99140.99160.99180.992Steady-state availability\n123456789 1 0\nFailure rate-CU0.9890.98950.990.99050.9910.99150.992Steady-state availabilityBaseline availability\n×10–3Baseline availability\n123456789 1 0\nRepair rate-CU0.990.99020.99040.99060.99080.9910.99120.99140.99160.99180.992Steady-state availability\nBaseline availability\nBaseline availability\n123456789 1 0\nFailure rate-GPS0.990.99020.99040.99060.99080.9910.99120.99140.99160.99180.992Steady-state availability\n×10–3123456789 1 0\nRepair rate-GPS0.99040.99060.99080.9910.99120.99140.9916Steady-state Availability\nFigure 3.6 (continued)40 Shivani Gupta and Vandana Gupta\n\nthat it utilizes the complete range of all possible values of a parameter to evaluate the\nsensitivities. The mathematical expression for this approach is shown as follows [5]:\nSθZðÞ =maxZθðÞ−minZθðÞ\nmaxZθðÞ(3:12)\nwhere\nmaxZ θðÞ=max fZθ1ðÞ ,Zθ2ðÞ ,..., ZθnðÞ /C138 ,\nand\nminZ θðÞ=minfZθ1ðÞ ,Zθ2ðÞ ,...,ZθnðÞ /C138 .\nwhere Z(θÞis the value of the measure Zfor the input value θand max Z(θÞand min\nZ(θÞare the maximum and minimum output values, respectively, obtained by vary-\ning the input over its entire range [29].\nSimilar to the partial derivative technique, we first evaluate the sensitivity coef-\nficient for all the parameters and then arrange them in decreasing order. Table 3.9\nrepresents the sensitivity ranking based on the percentage difference. We havementioned the ranking of only top 18 parameters. We have used the same range of\nvalues for all the parameters that we have used to produce the graphical results. It\nis observed that almost all the top-ranked parameters obtained from percentage\nTable 3.9: Sensitivity ranking based on percentage difference.\nSensitivity ranking Parameter |S(A)|\n λSC .\n λWalrus .\n μI=O .\n μSC .\n λCU .\n λSensors .\n λGPRS .\n λI=O .\n λGPS .\n μsensors .\n μGPRS .\n μWalrus .\n μGPS .\n μCU .\n μstorage pool . E-\n μAPI . E-\n λAPI . E-\n λstorage pool . E-3 Availability analysis of vehicular cloud computing 41\n\ndifference technique are same as those obtained from partial differentiation tech-\nnique except two parameters ( λSCandµSC).\n3.6 Conclusion\nThis chapter analyzes the availability of the VCC network using hierarchical model-\ning technique. Because of the complex architecture of VCC, distinct models havebeen developed for each subsystem of VCC, and the results obtained from the sub-\nmodels are then combined to evaluate the availability of the whole network. RBD\nand SMP are used to develop the availability models. Sensitivity analysis is also per-formed to calculate the impact of each parameter on the availability. Two different\ntechniques (partial derivatives technique and percentage difference technique) are\nused for the sensitivity analysis. The study reveals that the steady-state availabilityof the system can be significantly improved by just concentrating on a group of pa-rameters that affect the steady-state availability substantially as compared with the\nother parameters. Based on the two different sensitivity analysis techniques, we get\ntwo sensitivity ranking tables providing the list of all those parameters that affectavailability the most. It is worth noticing that almost all the parameters are identi-\ncal in both the tables. According to the tables, the higher ranked parameters are\neither related to the storage unit of the VCC system or providing access to the usersto utilize the VCC facilities. A graphical representation of the steady-state availabil-\nity with respect to first 15 parameters of the sensitivity ranking table is also pro-\nvided in the chapter.\nAcknowledgment: This research work was aided by the University of Delhi. The au-\nthor (Shivani Gupta) thanks UGC for their financial assistance.\nReferences\n[1] The Car In The Cloud. By Scott Frank, Vice President of Airbiquity. (Accessed March 4, 2019,\nat https://www.airbiquity.com/blog-events/blog-posts/car-cloud.)\n[2] Avizienis, A., Laprie, J.C., Randell, B. et al. (2004). Basic concepts and taxonomy of\ndependable and secure computing, IEEE Transactions on Dependable and Secure Computing,\n1(1), 11 –33.\n[3] Avizienis, A., Laprie, J.C. and Randell, B. (2001). Fundamental concepts of dependability.\nUniversity of Newcastle upon Tyne, Computing Science, Apr, 19.\n[4] Gupta, V. and Dharmaraja, S. (2011). Semi-Markov modelling of dependability of VoIP\nnetwork in the presence of resource degradation and security attacks, Reliability Engineering& System Safety, Dec, 1, 96(12), 1627 –1636.42 Shivani Gupta and Vandana Gupta\n\n[5] Matos, R., Araujo, J., Oliveira, D. et al. (2015). Sensitivity analysis of a hierarchical model of\nmobile cloud computing, Simulation Modelling Practice and Theory, Jan(50), 1, 151 –164.\n[6] Dantas, J., Matos, R., Araujo, J. et al. (2012). Models for dependability analysis of cloud\ncomputing architectures for eucalyptus platform, International Transactions on Systems\nScience and Applications, Dec, 8(5), 13 –25.\n[7] Abuelela, M. and Olariu, S. Taking vanet to the clouds. In Proceedings of the 8th International\nConference on Advances in Mobile Computing and Multimedia. ACM 2010, 6 –13.\n[8] Arif, S., Olariu, S., Wang, J. et al. (2012). Data centre at the Airport: Reasoning about\nTime-Dependent Parking Lot Occupancy, IEEE Transactions Parallel Distributed Systems.,\n23(11), 2067– 2080.\n[9] Olariu, S., Hristov, T. and Yan, G. (2012). The Next Paradigm Shift: From Vehicular Networks\nto Vehicular Clouds, Mobile Ad Hoc Network-ing: The Cutting-Edge Directions.\n[10] Abdelhamid, S., Benkoczi, R. and Hassanein, H.S. (2017). Vehicular clouds: ubiquitous\ncomputing on wheels, In Emergent Computation, 435 –452.\n[11] Gu, L., Zeng, D. and Guo, S. (2013). Vehicular cloud computing: A survey, In 2013 IEEE\nGlobecom Workshops (GC Workshops), Dec, 9, 403 –407.\n[12] Mekki, T., Jabri, I., Rachedi, A. et al. (2017). Vehicular cloud networks: Challenges,\narchitectures, and future directions, Vehicular Communications, Jul, 1(9), 268– 280.\n[13] Yan, G., Wen, D., Olariu, S. et al. (2012). Security challenges in vehicular cloud computing,\nIEEE Transactions on Intelligent Transportation Systems, Sep, 3, 14(1), 284 –294.\n[14] Al Ridhawi, I., Aloqaily, M., Kantarci, B. et al. (2018). A continuous diversified vehicular cloud\nservice availability framework for smart cities, Computer Networks, Nov, 9(145), 207 –218.\n[15] Limbasiya, T. and Das, D. (2019). Secure message confirmation scheme based on batch\nverification in vehicular cloud computing, Physical Communication, Jun, 1(34), 310– 320.\n[16] Hou, L., Zheng, K., Chatzimisios, P. et al. (2018). A Continuous-Time Markov decision\nprocess-based resource allocation scheme in vehicular cloud for mobile video services,\nComputer Communications, Mar, 1(118), 140 –147.\n[17] Midya, S., Roy, A., Majumder, K. et al. (2018). Multi-objective optimization technique for\nresource allocation and task scheduling in vehicular cloud architecture: A hybrid adaptivenature inspired approach, Journal of Network and Computer Applications, Feb, 1(103), 58 –84.\n[18] Talebifard, P. and Leung, V.C. (2013). Towards a content-centric approach to crowd-sensing in\nvehicular clouds, Journal of Systems Architecture, Nov, 1, 59(10), 976– 984.\n[19] Safi, Q.G., Luo, S., Wei, C. et al. (2018). Cloud-based security and privacy-aware information\ndissemination over ubiquitous VANETs, Computer Standards & Interfaces, Feb, 1(56),107–115.\n[20] Al-Rashed, E., Al-Rousan, M. and Al-Ibrahim, N. (2017). Performance evaluation of\nwide-spread assignment schemes in a vehicular cloud, Vehicular Communications, Jul, 1(9),144–153.\n[21] Whaiduzzaman, M., Sookhak, M., Gani, A. et al. (2014). A survey on vehicular cloud\ncomputing, Journal of Network and Computer applications, Apr, 1(40), 325 –344.\n[22] Understanding Cloud: Infrastructure-as-a-Service vs. Platform-as-a-Service. (Accessed\non March 10, 2019, at https://www.abcservices.com/understanding-cloud-infrastructure-as-a-service-vs-platform-as-a-service/)\n[23] Nurmi, D., Wolski, R., Grzegorczyk, C., et al. The eucalyptus open-source cloud-computing\nsystem. In Proceedings of the 2009 9th IEEE/ACM International Symposium on ClusterComputing and the Grid 2009 May 18, 124 –131.\n[24] Matos, R., Dantas, J., Araujo, J. et al. (2017). Redundant eucalyptus private clouds:\nAvailability modelling and sensitivity analysis, Journal of Grid Computing, Mar, 1, 15(1), 1 –22.3 Availability analysis of vehicular cloud computing 43\n\n[25] Dharmaraja, S., Vinayak, R. and Trivedi, K.S. (2016). Reliability and survivability of\nvehicular ad hoc networks: An analytical approach, Reliability Engineering & System Safety,\nSep, 1(153), 28 –38.\n[26] Cloud Storage. (Accessed on March 20, 2019 at https://en.wikipedia.org/wiki/Cloud\_storage)[27] Trivedi, K.S., Kim, D.S., Roy, A., et al. Dependability and security models. In 2009 7th\nInternational Workshop on Design of Reliable Communication Networks 2009, 11 –20.\n[28] Kulkarni, V.G. (2016). Modelling and analysis of stochastic systems, CRC Press.[29] Hamby, D.M. (1994). A review of techniques for parameter sensitivity analysis of\nenvironmental models, Environmental Monitoring and Assessment., Sep, 1, 32(2), 135 –154.44 Shivani Gupta and Vandana Gupta",48306
09-4. Increasing failure rate software reliability models for agile projects a comparative study.pdf,09-4. Increasing failure rate software reliability models for agile projects a comparative study,"Gabriel Ricardo Pena and Nestor Ruben Barraza\n4 Increasing failure rate software reliability\nmodels for agile projects: a comparative\nstudy\nAbstract: A new software reliability model is proposed. The model is inspired in the\nPolya stochastic process, which is the asymptotic limit of the Polya urn model for\ncontagion that is well described as a pure birth process. Modeling software reliabil-ity with this type of process introduces failure rate functions that depend not only\non time but also on the number of failures previously detected. Since the failure\nrate function of the Polya stochastic process results in a linear-over-time meannumber of failures, we propose a new pure birth process with a different failure rate\nfunction. This proposal results in a nonlinear-over-time mean number of failures\nand allows to model not only cases with increasing in time failure rate but also areliability growth. We consider that failure reports collected under modern softwareengineering methodologies will require not just reliability growth models but also\nthose which take into account increasing failure rate cases, if they are to be applied\nat the very first stage of development and testing, when code is constantly addedeither to fix failures or to accomplish new requirements. We show applications of\nour proposed model to several datasets and compare the performance with nonho-\nmogeneous Poisson process models.\nKeywords: software reliability, delayed S-shaped, Goel –Okumoto, logistic, contagion,\nAgile processes, pure birth process\n4.1 Introduction\nSince software is quite an important component in many technological projects,\nsoftware quality has became a vital issue in software engineering. It is in that sensethat software reliability engineering proposes models that help determine how reli-\nable a delivered software product is or when it will be ready for release. There is an\nextensive bibliography and reviews on software reliability models. It exceeds thepurpose of this work to make even a short list. This study initiates the group of\nmodels that can be described by pure birth processes. Pure birth processes are sto-\nchastic processes that describe the behavior of a population where individuals can\nGabriel Ricardo Pena, Nestor Ruben Barraza, Universidad Nacional de Tres de Febrero,\nBuenos Aires, Argentina\nhttps://doi.org/10.1515/9783110619058-004\n\nonly be born, see for example [1], and hence they can then be used to model the\nrandom failure detection process. Many software reliability models are special\ntypes of pure birth processes, such as the well known based on nonhomogeneousPoisson processes, first developed in the 1970s by authors such as Musa, Okumoto,and Yamada, which are still a matter of study. Nonhomogeneous Poisson processes\nare pure birth processes where the mean number of failures is a nonlinear function\nof time that models a reliability growth. The general point of view under pure birthprocesses allows us to think about general dependencies of the failure rate not only\non time but also on the number of previously detected failures. We point out that\npure birth processes have been already used in software reliability, though withoutthe general analysis we make in this work. One of the first software reliability mod-\nels was proposed in [2], the well-known Jelinski –Moranda model formulated as a\nbirth process is shown in [3] and some recent applications can be found in [4, 5].\nSeveral proposals for the failure (birth) rate that depend on time and on the\nnumber of previously detected failures allow us to capture physical mechanisms\nin either introduced or detected failures . That is the case of contagion. Contagion\ncan be modeled by the Polya urn model, which consists of extracting balls withreplacement from an urn and at the same time adding the same number of balls of\nt h es a m ec o l o r ,r i s i n gt h ep r o b a b i l i t yo fe xtracting a ball of th es a m ec o l o ri nt h e\nnext extraction. This model leads to the Polya stochastic process as the limit from\ndiscrete to continuous time. The Polya process is a particular case of a pure birth\nprocess where the birth (failure) rate depends on time and on the number of indi-\nviduals (failures) in the population.\nA stochastic software failure detection process, modeled by a contagion pro-\ncess, could reveal a contagion phenomena in introduced or detected failures. Thisfact would be indicative of a sort of interaction, perhaps due to the relations be-tween programmers, software modules, or testers.\nA contagious software reliability model could model the software failure sto-\nchastic process at the very start of the testing stage, where new code is added to fixfailures, or in agile environments, where testing and development phases are simul-\ntaneously carried out. We consider that the well-known stochastic processes devel-\noped decades ago need to be improved in order to take into account the recentadvances in software development engineering as well as testing processes. Despitethe fact that some improvements have been made (see, for example [6], where the\ninclusion of human factors, actual process monitoring, and performability evalua-\ntion is considered and a Markovian model is introduced), we consider that such im-provements should involve cases in which development and testing are performed\nsimultaneously. For example, at the very start of a software project or when devel-\nopment is performed under modern methodologies such as Agile, which includemany innovative practices such as test-driven development (TDD). Then, if we are\ninterested in applying a software reliability model since the beginning of the proj-\nect, we should consider models that allow an increasing failure rate.46 Gabriel Ricardo Pena and Nestor Ruben Barraza\n\nThere are no many recent datasets with failure reports collected at the very first\nstage of testing and development that take into account modern software engineer-\ning methodologies. This lack of datasets agrees with the fact that most of the re-search in software reliability remains in the academy, as shown in recent studies,see for example [7]. Industrial papers are mainly focused on pre- and postrelease\nstudies, where the development process is complete, see for example [8 –10].\nThe analysis of software reliability models as pure birth processes has already\nbeen considered in the literature. A general survey and discussion of pure birthprocesses applied to software reliability can be found in [11]. This type of pro-\ncesses are also applied in the literature as continuous-time Markov chains. Ournovel approach is to take advantage of this general formulation to consider more\ngeneral functional forms of the failure rate and the introduction of contagion as a\nnew concept.\nConversely, simulation of software failures is another concern in software reli-\nability, see for example, [12, 13]. Our general point of view of software reliabilitymodels as pure birth processes yields a simulation method through the exponentialwaiting time. This simulation produces a failure after a random exponential waitingtime that depends on the proposed failure rate.\nAs mentioned earlier, the Polya stochastic process is a pure birth process with a\nbirth rate that is directly proportional to the number of individuals in the popula-tion and inversely proportional to the elapsed time, see [1]. Despite the interest in\nthe Polya model in order to fit increasing failure rate cases, it gives as a result a\nmean number of failures that depends linearly on time. Then, though simulationshave demonstrated that this model fits relatively well some decreasing reliability\ncases, it is useless when we try to fit a mean number of failures curve. Hence, an-\nother contagious model that is a slight modification of the Polya process is pro-posed. We propose a failure rate that depends on time and previously detected\nfailures, obtaining a nonlinear time-dependent mean number of failures. As an in-\nteresting result, this nonlinear dependency can model increasing failure rate aswell as reliability growth cases where the curve for the first derivative of the meannumber decreases.\nConsidering increasing failure rate cases is not new and comes from the very\nbeginning of the speciality. It was introduced with the S-shaped model in [14]. Ourproposal is to consider these cases as coming from a new concept like contagion.\nWe applied our model to several real datasets analyzed in the literature and\ncompared the results obtained with those obtained after applying some well-knownNHPP models such as the Yamada delayed S-shaped, logistic, and Goel –Okumoto\nmodels.\nWe analyze increasing failure rate and reliability growth cases. Our model com-\npares them thus becoming a useful model for several software failure detection sto-\nchastic processes.4 Increasing failure rate software reliability models for agile projects 47\n\n4.2 Modern practices in software reliability\nSoftware development and testing has been evolved since the waterfall model,\npopular in the 1970s. There are various practices that can be grouped into threecategories: traditional, introduced by Agile, and shared. A nonexhaustive list is as\nfollows:\nTraditional\n–Destructive testing\n–Detailed designs/design specifications\n–Formal estimation (e.g., COCOMO, FP)\n–Formal specification\n–Model checking\n–Prototyping\n–Security testing\n–Use case modeling (as requirements engineering practice)\nIntroduced by Agile\n–Iteration/sprint reviews\n–Limit working progress (e.g., using Kanban board)\n–Onsite customer\n–Pair programming\n–Refactoring\n–Release planning\n–Retrospectives\n–Scrum of Scrums\n–Test-driven development (TDD)\n–User stories (as requirements engineering practice)\n–Velocity-based planning\nShared\n–End to end (system) testing\n–Expert/team-based estimation (e.g., Planning Poker)\n–Iteration planning\nMany of the above-mentioned practices (iteration/sprint reviews, Scrum of Scrums,\ntest-driven development, iteration planning) involve an iteration mechanism that\nimplies a simultaneous development and testing processes. This stage is not a reli-\nability growth one, but contrarily, a stage where we can expect an increasing failurerate stochastic process. Then, the focus is on models that take into account an in-\ncreasing failure rate first stage and try to predict when the reliability growth starts,\nsee [15] and references therein.48 Gabriel Ricardo Pena and Nestor Ruben Barraza\n\n4.3 Proposed model\n4.3.1 Mathematical background\nWe propose to get the probability of having rfailures in a given time tfrom a pure\nbirth equation.\nPure birth processes describe the behavior of a population where individuals\ncan only be born and are not allowed to die. The probability of having rindividuals\nin the population at a given time t,Pr(t), is given by the solution of the following\ndifferential equation (4.1):\nP′rtðÞ=−λrtðÞPrtðÞ+λr−1tðÞPr−1tðÞ\nP′0tðÞ=−λ0tðÞP0tðÞ(\n(4:1)\nwhere λr(t)is the birth (failure) rate, as shown in [1]. The process given in (4.1) is\nalso Markov ’s, where the number of individuals corresponds to the state of the sys-\ntem, Sr(t). The only transitions allowed (actually, other transitions are infinitesimals\nof higher order) are Sr(t)→Sr(t+d t ) and Sr(t)→Sr+1(t+d t ) with probabilities given\nby (4.2) and (4.3):\nPS rtðÞ! Sr+1t+dtðÞ ðÞ =λrtðÞdt (4:2)\nPS rtðÞ! Srt+dtðÞ ðÞ =1−λrtðÞdt (4:3)\nThe dependency of λr(t)ontdefines the type of the process. If λr(t)is a function of t\nor a constant, the process is nonhomogeneous or homogeneous respectively but\nwith independent increments. If λr(t)is a function of r, the process is of dependent\nincrements.\nFrom (4.1), the probability of having no births in a given time interval greater\nthan t–sgiven the system is at the state rby the time sis given by the well-known\nexponential waiting time (4.4):\nPno births 2T>t−s ðÞ =exp −ðt\nsλrtðÞdt !\nt≥s (4:4)\nThe integrate of λr(t) (4.5) is the mean number of births (failures):\nðt\nsλrtðÞdt=μtðÞ−μsðÞ (4:5)\nThe mean number of individuals in a given time (4.6)\nMtðÞ=X+∞\nr=0rPrtðÞ (4:6)4 Increasing failure rate software reliability models for agile projects 49\n\ncan be obtained from summing up (4.1) multiplied by r, and getting this way a dif-\nferential equation for the mean (4.7):\nM′tðÞ=X+∞\nr=1λr−1Pr−1tðÞ (4:7)\n4.3.2 Proposed failure rate\nTaking into account that in a dynamic project, like those developed under Agile\nmethodologies, new code is constantly added either to fix failures or in order to\nmeet new requirements, we can consider that failures are removed on one hand,\nand new ones are introduced on the other. Under the hypothesis that new failuresare introduced at a rate proportional to the number of failures previously detected\n(contagion), the proposed failure rate results in a factor that increases proportion-\nally to the previous number of failures and a denominator that increases on time.Then the following mathematical form (4.8) is proposed:\nλ\nrtðÞ=1\na1+br\n1+at(4:8)\nIfbis zero, the model behaves like the Musa –Okumoto software reliability growth\nmodel (see Table 4.1). Equation (4.8) is similar to that of the Polya contagion pro-cess given by (4.9) (see [16]):\nλ\nrtðÞ=ρr+γ\n1+ρt(4:9)\nWith our proposal, we get a quite different mean number of failures as it is ex-\nplained later. As it was stated earlier, looking at (4.8), our model is a nonhomoge-\nneous Markov with dependent increments process.\nTable 4.1: NHPP software reliability models.\nModel μ(t)\nGoel– Okumoto a1−exp−btðÞ ðÞ\nMusa –Okumoto 1\nθlnμ0θt+1/C0/C1\nDelayed S-shaped a1−1+btðÞ exp−btðÞ ðÞ\nLogistic a\n1+exp−bt−cðÞ ðÞ50 Gabriel Ricardo Pena and Nestor Ruben Barraza\n\nLooking at (4.8) the main assumptions of our model can be stated as follows:\n–Failures are continuously introduced and removed.\n–Code is being constantly added either to fix failures and/or to meet new\nrequirements.\n–The new added code introduces failures at a rate proportional to the existing\nnumber of failures (contagion).\n–Because of the failures fixing process, the failure intensity decreases inversely\nproportional to the elapsed execution time. This assumption is that of the\nMusa –Okumoto model.1\nUsing (4.8), the mean number of failures as obtained from (4.7) is obtained by solv-ing the following differential equation:\nM′tðÞ=a\n1+at1+bM tðÞ ðÞ (4:10)\nresulting in the mean value function given by (4.11):\nMtðÞ=1\nb1+atðÞb−1/C16/C17\n(4:11)\nOur proposal, inspired in the Polya contagion process, gives an interesting func-\ntion of the mean number of failures, resulting in either positive or negative con-\ncavities according to whether the bparameter is greater or lower than one,\nallowing modeling not just i ncreasing failure rate cases but also a reliability\ngrowth. The probability of failures can be expressed in an integral form as shownin [17]. Our proposed pure birth process can be simulated in a similar way as it\nwas done for the Polya process in [16] and other pure birth processes as shownin [18], by getting a failure after a randomly generated exponential waiting\ntime.\n4.4 Nonhomogeneous poisson processes\nused for comparison\nT h eP o i s s o np r o c e s sa r i s e sf r o map u r eb i r t hp r o c e s sw h e nt h eb i r t hr a t ei sc o n -\nstant: λr(t) = λ. Nonhomogeneous Poisson processes arise when the birth rate is a\n1Actually, in the Musa– Okumoto model, it can be shown that the failure intensity decays expo-\nnentially with failures experienced. This factor should be better referred to as Musa– Okumoto like\nfactor.4 Increasing failure rate software reliability models for agile projects 51\n\nfunction of time: λr(t)=λ(t). The probability of having rfailures in a time interval\n(s,t)i sg i v e nb y\nPN t ðÞ−NsðÞ ðÞ =r ðÞ =μtðÞ−μsðÞ ðÞr\nr!exp−μtðÞ−μsðÞ ðÞðÞ (4:12)\nDifferent software reliability models based on nonhomogeneous Poisson processes\nhave been proposed in the literature, a short list of them that are used further forcomparison is shown in Table 4.1, besides the Musa –Okumoto model mentioned in\nthe previous section. A more extensive list can be found in [19].\n4.5 Mean time between failures\nThe mean time between failures (MTBF) is calculated as usual from the densityfunction of the time to the next failure. That density function is obtained as the first\nderivative of the distribution function F\nT(t)=P(T≤t), which corresponds to the\ncomplement of the exponential waiting time (4.4). This distribution function is\ngiven by\nFTt;r,sðÞ =1−exp −ðt\nsλrtðÞdt !\n(4:13)\nThus, by differentiating (4.13), we get the probability density function of the time tothe next failure:\nf\nTt;r,sðÞ =λrtðÞexp −ðt\nsλrtðÞdt !\n(4:14)\nBy taking the expectation of (4.14) we get the conditional MTBF predicted by ourmodel given rfailures were detected by the time s(4.15):\nMTBF r,sðÞ =1\na1+as\nbr,r=1,2,3,... (4:15)\nThe obtained MTBF depends on two competitive factors: a reliability growth factordepending on time, and a second factor that is inversely proportional to the number\nof failures, leading to a decrease in the MTBF. Replacing the mean number of fail-\nures (4.11) in (4.15) as r=M (s) leads to\nMTBF sðÞ=1\na1+as\n1+asðÞb−1(4:16)52 Gabriel Ricardo Pena and Nestor Ruben Barraza\n\nBy looking at (4.16), we observe that the asymptotic behavior for great values of sis\nas follows (4.17):\nMTBF sðÞ !1\naasðÞb−1(4:17)\nWe can see again the different behaviors depending on bis greater or lower than\none. Both behaviors will be analyzed in the experiments (see Section 4.6).\nA failure dataset modeled by our model could reveal a contagion process either\nduring the development or testing phases. This contagion phenomenon must be an-\nalyzed in each particular case and could be due to a sort of interaction between pro-grammers or testers, some characteristics of a particular piece of code, using a\ngiven module several times, and so on.\nFor NHPP, the MTBF is calculated with the standard formulation, from the den-\nsity function of the time to failure number k(see [19] Eq. (6.78)). Let T\nkdenote the\ntime until the k-th failure; then, its expected value is given by (4.18):\nET k½/C138 =Ð+∞\n0zλzðÞμzðÞ½/C138k−1exp−μzðÞðÞ dzÐa\n0zk−1exp−zðÞ dz,a=lim\nt!+∞μtðÞ (4:18)\nFinally, by defining Xk=T k–Tk–1, the MTBF is obtained as follows:\nEX k½/C138 =ET k½/C138−ET k−1½/C138 ,k=1,2,3,... (4:19)\n4.6 Experiments\nIn this section, we analyze the application of three well-known models based on\nnonhomogeneous Poisson processes (the Goel –Okumoto model, the Yamada de-\nlayed S-shaped model, and the logistic model), and our proposed contagion model\nto several failure datasets comparing the results. In order to evaluate the goodness\nof fit, we calculate the predictive ratio risk (PRR) and the Akaike information crite-rion (AIC) for every model. Two different parameter estimation procedures were per-formed for each of the three NHPP models: least-squares over the mean number of\nfailures curve, and maximum likelihood (taking the least-squares fitted parameters\nas the initial approximation to solve the maximum likelihood equations). Due tothe lack of a closed formula for the failure time pdf (see [17]) we are unable to per-\nform a maximum likelihood estimation on the contagion model nor calculate the\nexact MTBFs; however, we use (4.16) in order to compute conditional MTBFs andMTTFs based on the datasets. In order to evaluate the predictive validity of each\nmodel, we calculate the PRR on the whole dataset through estimating the parame-\nters by short stages. The MTBF curves for these types of projects exhibit a first in-creasing failure rate stage, what resembles the well-known hardware reliability4 Increasing failure rate software reliability models for agile projects 53\n\nbathtub curve, with an infant mortality first stage. This is an important character-\nistic we want to remark for these type of projects developed under modern soft-\nware and testing methodologies. The integral in the numerator of (4.18) must becalculated by the saddle point method for values of kgreater than 100 (see [20]).\n4.6.1 The NTDS project\nThis is a classic dataset that has been extensively analyzed in the literature, see forexample [21] or [19]. The data come from the Naval Fleet Computer ProgrammingCenter and corresponds to the development of the Naval Tactical Data System’ s\nsoftware core. We considered only the first 26 failures, which were found during theproduction phase (for a total of 250 days).\nThe development of this project was ma de under traditional methodologies,\nthat is, the waterfall model. Besides the typical reliability growth stage obtainedfor this type of development methodology, the dataset exhibits an S-shapedcurve, with an increasing failure rate first stage. The four processes we analyzedwere able to fit this dataset smoothly. Table 4.2 shows the estimated parameters.\nFigure 4.1 shows the mean value curve for every model, obtained by the least-\nsquares parameters.\nSince the NTDS was developed under the traditional paradigm (waterfall lifecycle),\nit is reasonable to expect convex curves due to reliability growth. The real data does\nnot exhibit a pure reliability growth behavior but two well-distinguished stages,\ngiving as a result an S-shaped curve. The DS and logistic models fit well the firststage and predict a reliability growth. The Goel –Okumoto model fits the best the\nlate stage of the project (large tand n). Table 4.3 shows the PRR and the AIC for\neach of the four models.\nThe best results for this project are obtained for the logistic model, which\npredicts very accurately the number of f ailures at every stage of the project.Table 4.2: NTDS project, estimated parameters.\nParameter Goel –Okumoto Delayed S-shaped Logistic Our model\nLS ML LS ML LS ML LS\na .  .  .  .  .  .  .\nb .  .  .  .  .  .  .\nc –––– .  . –54 Gabriel Ricardo Pena and Nestor Ruben Barraza\n\nThis can be observed in both the fit metrics and the curves. The extremely low\nPRR values suggest that the fit is especially good on the latest part of the curve. Itis also remarkable that the least-squar es and maximum likelihood methods give\nsimilar results. On the other hand, PRR is significantly higher for the DS model,due to an underestimation of the number of failures at the beginning. Our modelbehaves better than the DS, but is still outperformed by the other two. Table 4.4\nalso shows calculations of the PRR over time.\nFigure 4.2 shows the (interpolated) MTBF curves for every model (including the\nconditional MTBF for our proposed model). The Goel –Okumoto model predicts in-\ncreasing MTBFs due to reliability growth. The other models, however, show more ups\nand downs. The logistic model predicts a decrease in the MTBFs as ngrows, and it\nhas the lowest MTBF value of all four models for n=26; hence, it may not be predict-\ning a reliability growth. Finally, our model gives a slowly increasing MTBF, since itcorresponds to a reliability growth model when its bparameter is lower than one.25Real data (NTDS)\nGoel–okumoto\nDelayed S-shapedLogistic\nOur model20\n15\n10Number of failures\n5\n0\n0 50 100 150\nTime (days)200 250\nFigure 4.1: NTDS data, mean value curves.\nTable 4.3: NTDS project, fit metrics.\nGoel –Okumoto Delayed S-shaped Logistic Our model\nPRR (LS) . .  .  .\nPRR (ML) . .  . –\nAIC . .  . –4 Increasing failure rate software reliability models for agile projects 55\n\n4.6.2 The mixed waterfall and agile project\nThis dataset corresponds to a client –server system developed between 2001 and\n2005, which includes around 250 C language kLoC. The project involves both the\nserver and remote terminals applications, linked by a X25/IP WAN. The code also\nincludes a UI for the operation and a relational database managing system.\nDevelopment and testing were performed under a combined waterfall/agile\nmethodology, and they both continued after the release (which happened around110 days after the beginning of the failure report). The report shows a total of 886failures found within 209 days and registered on the “failures per day ”format.\nTable 4.5 shows the estimated parameters for the four models analyzed; Figure 4.3shows the mean value curves fitted by the least-squares method.Real data (NTDS)\nGoel–Okumoto\nDelayed S-shaped\nLogistic\nOur model20.0\n17.5\n15.0\n12.5\n10.0\n7.5\n5.0\n2.5\n0.0\n0 5 10 15 20\nFailure numberMean time between failures\n25\nFigure 4.2: NTDS data, MTBF curves.Table 4.4: NTDS project, PRR over time.\nDay Goel –Okumoto Delayed S-shaped Logistic Our model\n N/A .  .  .\n N/A .  .  .\n  . .  .  .56 Gabriel Ricardo Pena and Nestor Ruben Barraza\n\nA look at the curve of the real data clearly displays an increasing failure rate\nstage until the day 60. Then, up to the day 110, the data exhibits the reliability\ngrowth behavior. From day 110 to the end of the report, the curve is almost linear,\nwhich indicates an almost constant failure rate. Figure 4.3 indicates that theGoel –Okumoto model follows almost perfectly the constant failure rate stage, but\ni sn o ta b l et op r e d i c tt h ep r o j e c ts t a t ea tt h es t a r to ft h ep r o j e c t ,a se x p e c t e d .O nthe contrary, DS model curve performs a reasonable fit on the whole dataset.Neither of the models show a considerable difference between the least-squares\nand the maximum likelihood estimations, as seen in Table 4.5. The PRR metrics\nfor the four models are depicted in Table 4.6. We observe large values for the DSmodel produced by the model underestimation of the early days failure numbers;Delayed S-shapedReal data (mixed waterfall–agile)800\n600\n400\n200\n0\n0 50 100 150 200\nTime (days)Number of failures\nGoel–Okumoto\nLogistic\nOur model\nFigure 4.3: Mixed waterfall– agile data, mean value curves.Table 4.5: Mixed waterfall– agile project, estimated parameters.\nParameter Goel –Okumoto Delayed S-shaped Logistic Our model\nLS ML LS ML LS ML LS\na  .  .  .  .  .  .  .\nb .  .  .  .  .  .  .\nc – ––– .  . –4 Increasing failure rate software reliability models for agile projects 57\n\non the other hand, the logistic and our proposed model give the best performance\nmetric in order to adjust the whole dataset. Analysis of the PRR over time is\ns h o w ni nT a b l e4 . 7 .\nPredicted MTBFs shown in Figure 4.4 for the first 100 days of the dataset are quite\ndifferent for the models analyzed. Goel –Okumoto model predicts an almost con-\nstant MTBF, while DS model shows an unusually high MTBF on the early stagesthat decays almost exponentially and then becomes almost linear (and still decreas-ing). Logistic model predicts the exact opposite: an almost constant but slightly in-\ncreasing MTBF in the early stages that suddenly increases exponentially; this\nimplies that the MTTF also has exponential behavior. Our model predicts a MTBFthat grows almost linearly and gradually.\n4.6.3 The agile #1 project\nThe dataset we named Agile #1 belongs to a noncritical real-time system developedunder Agile paradigm in 2017, consisting in 47 modules with 26,729 Java languageand 9,465 XML lines of code. The entire dataset was obtained with the project still\nin development, with testing performed at the same time when a new code was in-\ntroduced; an increasing failure rate is expected due to this fact. Since no reliabilitygrowth phase is present in the data, the Goel –Okumoto model fit could not beTable 4.7: Mixed waterfall– agile project, PRR over time.\nDay Goel –Okumoto Delayed S-Shaped Logistic Our model\n N/A ,. N/A .\n N/A ,. N/A .\n N/A ,. N/A .\n N/A ,.  .  .Table 4.6: Mixed waterfall– agile project, fit metrics.\nGoel –Okumoto Delayed S-shaped Logistic Our model\nPRR (LS) .  .  .  .\nPRR (ML) .  .  . –\nAIC  .  .  . –58 Gabriel Ricardo Pena and Nestor Ruben Barraza\n\nperformed. Table 4.8 shows the obtained parameters for DS, logistic and con-\ntagion models, and Figure 4.5 depicts the curves. Again, it is observed that LS\nand ML estimates are exactly the same for the logistic model, as already shownin Section 4.6.1.\nGoodness-of-fit metrics (Table 4.9) show that the logistic model outperforms the\nDS significantly. This is true even if the AIC takes a slightly higher value becauseof the complexity penalty. Our model behaves even better than the logistic.\nTable 4.10 also shows the PRR calculations over time, obtained by the least-\nsquares estimates.Table 4.8: Agile #1 project, estimated parameters.\nParameter Delayed S-shaped Logistic Our model\nLS ML LS ML LS\na .  .  .  .  .\nb .  .  .  .  .\nc –– .  . –Goel–Okumoto\nDelayed S–shaped\nLogistic\nOur model\n0 100 200 300 400 500 600 700\nFailure numberMean time between failures\n0.000.250.500.751.001.251.501.752.00\nFigure 4.4: Mixed waterfall– agile data, MTBF curves.4 Increasing failure rate software reliability models for agile projects 59\n\nThe four MTBF curves, including the conditional MTBF curve for the contagion\nmodel, are depicted in Figure 4.6. All the models predict similar MTBFs for the most\npart of the dataset, the logistic model is the only one that behaves differently over\nthe early stages. A significant decreasing on the MTBF is observed as the failure\nnumber increases.Table 4.9: Agile #1 project, fit metrics.\nDelayed S-shaped Logistic Our model\nPRR (LS) . . .\nPRR (ML) . . –\nAIC . . –\n0Number of failures\n051015202530 Real data (agile #1)\nDelayed S-shaped\nLogistic\nOur model\n100 200 300 400\nTime (days)\nFigure 4.5: Agile #1 data, mean value curves.\nTable 4.10: Agile #1 project, PRR over time.\nDay Delayed S-shaped Logistic Our model\n . . .\n . N/A .\n . . .60 Gabriel Ricardo Pena and Nestor Ruben Barraza\n\n4.6.4 The agile #2 project\nAgile #2 dataset was analyzed in [22], it corresponds to a browser-based testing de-\nvelopment tool for web apps, which was written in JavaScript, XML, HTML, and\nCSS languages and developed under agile methodologies. The failure report in-\ncludes post-release data from after 11 releases, and the reliability growth stage hasnot been reached. Delayed S-shaped, logistic, and contagion model estimates are\nshown in Table 4.11, and its least-squares curves in Figure 4.7. Maximum likelihood\nfit for logistic model could not be performed due to convergence issues.\nFit metrics are displayed in Table 4.12. The DS model does not fit well, showing a\nPRR 20 times larger than the logistic ’s one. The curve shown in Figure 4.7 also\nFailure number50\n40\n30\n20\n10\n0\n0 5 10 15 20 25 30Mean time between failuresReal data (agile #1)\nDelayed S-shaped\nLogistic\nOur model\nFigure 4.6: Agile #1 data, MTBF curves.\nTable 4.11: Agile #2 project, estimated parameters.\nParameter Delayed S-shaped Logistic Our model\nLS ML LS ML LS\na .  .  . N/A .\nb .  .  . N/A .\nc –– . N/A –4 Increasing failure rate software reliability models for agile projects 61\n\nshows a poor fit. Our model ’s PRR is slightly better than the logistic, proving to be\nthe best choice (at least in the PRR sense) for the two agile projects analyzed here.\nPRR over time results are shown in Table 4.13.\nMTBF curves are shown in Figure 4.8. As in the mixed waterfall–agile dataset,\nthe observed time between failures was not registered. Moreover, this report con-sists on grouped data over nonequal intervals, which means no information is avail-\nable about each day ’s failure population; in consequence, formula (4.16) cannot be\napplied for each day, but only for those whose failure population was registered.\nThis can be clearly seen on the contagion model curve, which consists on multiple\nlinear segments that do not cover the entire time axis. The behavior observed for\nthe predicted MTBFs on the agile #1 project persists here, with a rapidly decreasingDS and contagion curves, and a logistic one that also has a local maximum on the\nearly time stages.\n0 200 400 600 800 1,00\nTime (days)250\n150\n100200\n50\n0Number of failuresReal data (agile #2)\nDelayed S-shaped\nLogistic\nOur model\nFigure 4.7: Agile #2 data, mean value curves.\nTable 4.12: Agile #2 project, fit metrics.\nDelayed S-shaped Logistic Our model\nPRR (LS) .  . .\nPRR (ML) . N/A –\nAIC  . N/A –62 Gabriel Ricardo Pena and Nestor Ruben Barraza\n\n4.7 Conclusion\nA new software reliability model was proposed. The proposed model is a special\ncase of nonhomogeneous Markov pure birth process with a failure rate that de-\npends on the number of failures previously detected and on the elapsed time. The\nproposed failure rate includes the introduction of new failures as well as the failureremoval, as it happens when new code is added to fix failures on the one hand, and\nto meet new requirements on the other. Our model can be applied both to increas-\ning failure rate and to reliability growth cases. Increasing failure rate cases may\nDelayed S-shaped\nLogistic\nOur model\n0 100 50 200 150 250 300\nFailure number10\n8\n6\n4\n2\n0Mean time between failures\nFigure 4.8: Agile #2 data, MTBF curves.Table 4.13: Agile #2 project, PRR over time.\nDay Delayed S-shaped Logistic Our model\n . . .\n . N/A .\n . . .\n . N/A .\n . . .4 Increasing failure rate software reliability models for agile projects 63\n\narise when software reliability models are applied at the very first stage of develop-\nment and testing, especially when they are performed under modern software engi-\nneering methodologies, such as agile or test-driven development, where testing anddevelopment are simultaneously performed. Failure reports that can be modeled byour model could reveal the presence of a contagion phenomena, a sort of interac-\ntion between programmers, testers, or pieces of reused code. Applications of our\nmodel were compared with the Yamada delayed S-shaped and logistic models inthe increasing failure rate stage and with the Goel –Okumoto software reliability\ngrowth model when the failure rate decreases. Results of applications show thatour model compares well under the PRR (predictive ratio risk) and outperforms theother models in modern projects developed under agile methodologies.\nAcknowledgment: This work was supported by Universidad Nacional de Tres de\nFebrero under grant no. 32/473 A.\nBibliography\n[1] Feller, W. (1968). An Introduction to Probability Theory and Its Applications, Vol. 1, 3rd, Wiley,\nJan.\n[2] Kremer, W. (1983). Birth-death and bug counting, IEEE Transactions on Reliability, R-32(1),\n37–47. April.\n[3] Jelinski, Z. and Moranda, P. (1972). Software reliability research, Statistical Computer\nPerformance Evaluation, Freiberger, W., Ed, Academic Press, New York, 465 –484.\n[4] Barraza, N.R., “A parametric empirical bayes model to predict software reliability growth, ”\nProcedia Computer Science, vol. 62, pp. 360 –369, 2015, proceedings of the 2015\nInternational Conference on Soft Computing and Software Engineering (SCSE ’15). [Online].\nAvailable: http://www.sciencedirect.com/science/article/pii/S187705091502551X\n[5] Barraza, N.R., “A New Homogeneous Pure Birth Process Based Software Reliability Model, ”\nin Proceedings of the 38th International Conference on Software Engineering Companion, ser.\nICSE ’16. New York, NY, USA: ACM, 2016, pp. 710 –712. [Online]. Available: http://doi.acm.org/\n10.1145/2889160.2892645\n[6] Yamada, S., Software Reliability Modeling: Fundamentals and Applications. Springer, 2013.\n[Online]. Available: https://books.google.com.ar/books?id=T-wlswEACAAJ\n[7] Febrero, F., Calero, C. and Moraga, M.A. (2014). A systematic mapping study of software\nreliability modeling, Information and Software Technology, 56(8), 839 –849. [Online].\nAvailable:. http://www.sciencedirect.com/science/article/pii/S0950584914000676.\n[8] Rotella, P., Chulani, S. and Goyal, D., “Predicting field reliability, ”in 2015 IEEE/ACM\n3rd International Workshop on Release Engineering, May 2015, pp. 12 –15.\n[9] Rotella, P. and Chulani, S., “Predicting release reliability, ”in 2017 IEEE International\nConference on Software Quallity, Reliability and Security (QRS2017), Jul 2017, pp. 39– 46.\n[10] Jalote, P. and Murphy, B., “Reliability growth in software products, ”in 15th International\nSymposium on Software Reliability Engineering, Nov 2004, pp. 47– 53.\n[11] Okamura, H. and Dohi, T., “Unification of software reliability models using markovian arrival\nprocesses, ”in 17th IEEE Pacific Rim International Symposium on Dependable Computing,64 Gabriel Ricardo Pena and Nestor Ruben Barraza\n\nPRDC 2011, Pasadena, CA, USA, December 12 –14, 2011, Alkalai, L., Tsai, T. and Yoneda, T.,\nEds. IEEE Computer Society, 2011, pp. 20 –27. [Online]. Available: https://doi.org/10.1109/\nPRDC.2011.12\n[12] Gokhale, S.S., Lyu, M.R. and Trivedi, K.S. (2006). Incorporating fault debugging activities into\nsoftware reliability models: a simulation approach, IEEE Transactions on Reliability, 55(2),\n281–292. June.\n[13] Lin, C.-T. and Li, Y.-F. (2014). Rate-based queueing simulation model of open source software\ndebugging activities, Software Engineering, IEEE Transactions on, 40(11), 1075 –1099. Nov.\n[14] Yamada, S., Ohba, M. and Osaki, S. (1983). S-shaped reliability growth modeling for software\nerror detection, IEEE Transactions on Reliability, R-32(5), 475 –484. Dec.\n[15] Barraza, N.R. (2019). Software reliability modeling for dynamic development environments,\nAnand, A. and Ram, M., editors, Recent Advancements in Software Reliability Assurance,Advances in Mathematics and Engineering, Chapter 3, CRC Press, pages 29– 37.\n[16] Barraza, N.R., “Software reliability modeled on contagion, ”in 2016 IEEE International\nSymposium on Software Reliability Engineering Workshops (ISSREW), Oct 2016, pp. 49 –50.\n[17] Sendova, K.P. and Minkova, L.D. (2019). Introducing the nonhomogeneous compound-birth\nprocess, Stochastics, 0(0), 1 –19.\n[18] Barraza, N.R., “Mining bugzilla datasets with new increasing failure rate software reliability\nmodels, ”in XLIII Latin American Computing Conference, CLEI 2017, Cordoba, Argentina,\nSeptember 4 –8, 2017. [Online]. Available: http://www.clei2017-46jaiio.sadio.org.ar/sites/\ndefault/files/Mem/SLISW/slisw-04.pdf\n[19] Pham, H. (2010). System Software Reliability, 1st, Springer Publishing Company,\nIncorporated.\n[20] Mathews, J. and Walker, R., Mathematical Methods of Physics, ser. Addison-Wesley World\nStudent Series. W. A. Benjamin, 1970. [Online]. Available: https://books.google.com.ar/books?id=1iHvAAAAMAAJ\n[21] Hossain, S.A. and Dahiya, R.C. (1993). Estimating the Parameters of a Non-homogeneous\nPoisson-Process Model for Software Reliability, IEEE Transactions on Reliability, 42(N. 4).December.\n[22] Mohamad, S. and McBride,, “Open source, agile and reliability measures, ”in Software\nQuality Engineering: Proceedings of the CONQUEST 2009, Jan 2009, pp. 103– 117.4 Increasing failure rate software reliability models for agile projects 65",39722
10-5. Fault big data analysis based on effort prediction models and AI for open-source project.pdf,10-5. Fault big data analysis based on effort prediction models and AI for open-source project,"Yoshinobu Tamura and Shigeru Yamada\n5 Fault big data analysis based on effort\nprediction models and AI for open-source\nproject\nAbstract: A huge number of open-source software (OSS) is embedded in various\nsoftware systems and services. In particular, the software systems become increas-\ningly complicated by using many OSS components. Many OSS are embedded in sev-eral areas because of the standardization, quick delivery, cost reduction, and so on.\nAlso, various software reliability growth models have been structured in the past.\nIn this chapter, we discuss several effort prediction models based on the jump\ndiffusion and Wiener processes considering several external factors of OSS project.Then, the method of maximum likelihood, deep learning, and genetic algorithm is\nused as the parameter estimation method for stochastic differential equation andjump diffusion process models.\nMoreover, several numerical examples based on the fault big data in actual OSS\nprojects are shown by using the effort prediction models discussed in this chapter.Finally, the results of parameter estimation in effort prediction models are pre-sented. Furthermore, the method of parameter estimation will be useful to assess\nthe quality and reliability of OSS developed under the open-source project.\nKeywords: Fault big data, deep learning, effort prediction model, open-source\nproject\n5.1 Introduction\nRecently, various open-source software (OSS) is used in many software systems and\nservices. Also, many software systems have been developed by using several OSScomponents because of the quick delivery, standardization, cost reduction, and so\non. Then, various software reliability growth models for OSS system have been pro-\nposed in the past [1 –3].\nThis chapter discusses several effort prediction models considering the Wiener\nprocess and AI in order to comprehend several external factors of OSS project. In par-ticular, we discuss the jump diffusion process model. Then, the method of genetic\nYoshinobu Tamura, Tokyo City University, Tokyo, Japan\nShigeru Yamada, Tottori University, Tottorri, Japan\nhttps://doi.org/10.1515/9783110619058-005\n\nalgorithm (GA), deep learning, and maximum likelihood is used as the parameter es-\ntimation method based on AI for stochastic differential equation (SDE) models.\nMoreover, several numerical examples based on the actual fault big data on the\nprojects of OSS are presented by using the effort prediction models proposed in this\nchapter. Then, the numerical illustrations of parameter estimation based on AI are\ndiscussed in this chapter. Finally, we prove that the proposed effort prediction mod-\nels will be useful to predict software effort for the quality and reliability of OSS de-veloped under the open source project.\n5.2 Flexible effort prediction model based on jump\ndiffusion processes\nWe discuss the modeling of jump diffusion processes to control the OSS maintenanceeffort during the operation. Let Z tðÞbe the cumulative OSS maintenance effort expen-\nditures up to time tt≥0ðÞ in the operation. Z tðÞare the real values continuously.\nThen, Z tðÞgradually increases as the progress of OSS operation, because the mainte-\nnance effort is recorded in the OSS operation. By using the modeling technique ofclassical software reliability modelling [4 –7], the following equation considering the\nOSS maintenance effort is given as follows:\ndZtðÞ\ndt=βtðÞα−ZtðÞ fg , (5:1)\nwhere βtðÞis the effort expenditure rate of OSS at time t.αrepresents the estimated\nmaintenance effort of OSS expended in the period of specified version.\nConsidering Brownian motion, eq. (5.1) is represented to the following SDE [8, 9]:\ndZtðÞ\ndt=βtðÞ+σνtðÞ fg α−ZtðÞ fg , (5:2)\nwhere σis added as a positive value meaning a level of the irregular continuous\nfluctuation, and νtðÞa standardized Gaussian white noise due to development envi-\nronment. Then, eq. (5.2) is extended to the following SDE of an It ^o type:\ndZtðÞ=βtðÞ−1\n2σ2/C8/C9\nα−ZtðÞ fg dt+σα−ZtðÞ fg dw tðÞ, (5:3)\nwhere wtðÞmeans one-dimensional Wiener process. wtðÞcan be represented as the\nwhite noise νtðÞ.\nThe jump term is embedded to the SDE models of eq. (5.3) by considering the\nunexpected irregular situation at time tby many external complicated project fac-\ntors. The process of jump diffusion is obtained as follows:\ndZjtðÞ=βtðÞ−1\n2σ2/C8/C9\nα−ZjtðÞ/C8/C9\ndt+σα−ΨjtðÞ/C8/C9\ndw tðÞ+dPYtλðÞ\ni=1ðVi−1Þ/C26/C27\n, (5:4)68 Yoshinobu Tamura and Shigeru Yamada\n\nwhere a Poisson point process with frequency λat time tis represented as YtλðÞ,\nthat is, the number of jumps, and λthe rate of jump. Vimeans the range of i-th\njump. We assume that wtðÞ,YtλðÞ, and Viare independent mutually. Moreover, the\nincreasing rates of OSS maintenance effort for βtðÞare shown as\nÐt\n0βsðÞds=.dR *tðÞ\ndt\nα−R*tðÞ,(5:5)\nRetðÞ=a1−e−bt/C0/C1\n, (5:6)\nRstðÞ=a1−1+btðÞ e−bt/C8/C9\n. (5:7)\nIn this chapter, βtðÞis assumed the mean value functions in eqs. (5.6) and (5.7)\nfrom NHPP (nonhomogeneous Poisson process) models as the OSS effort expendi-\nture function of our model, where a=.αis the expected cumulative number of latent\nfaults, and b=.βthe detection rate per fault in terms of software reliability growth\nmodels.\nBased on It ^o’s formula [10], Z j*tðÞin eq. (5.4) can be derived as\nZjetðÞ=α1−exp −βt−σwtðÞ−XYtλðÞ\ni=1logV i()""#\n, (5:8)\nZjstðÞ=α1−1+βt ðÞ exp −βt−σvtðÞ−XYtλðÞ\ni=1logVi() ""#\n. (5:9)\nMoreover, we extend the existing jump diffusion process model obtained from eq. (5.4)to the following time-delay jump diffusion processes:\nIn case of t≥0ðÞ :\ndZ\nfjtðÞ=βtðÞ−1\n2σ2/C26/C27\nα−ZfjtðÞ/C8/C9\ndt+σα−ZfjtðÞ/C8/C9\ndw tðÞ+dXYtλ1ðÞ\ni=0ðV1\ni−1Þ()\n.(5:10)\nIn case of t≥0,t′≥t1/C0/C1\n:\ndZfjtðÞ=βtðÞ−1\n2σ2/C8/C9\nα−ZfjtðÞ/C8/C9\ndt+σα−ZfjtðÞ/C8/C9\ndw tðÞ\n+dPYtλ1ðÞ\ni=0ðV1\ni−1Þ/C26/C27\n+dPYt′λ2ðÞ\ni=0ðV2\ni−1Þ()\n,(5:11)\nwhere Ytλ1ðÞ and Yt′λ2ðÞ are Poisson point processes with parameter λ1and λ2at\neach operation time t≥0ðÞ and t′≥t1/C0/C1\n, respectively. Moreover, V1\niand V2\niareith\njump ranges in each operation time t≥0ðÞ and t′≥t1/C0/C1\n, respectively. We assume\nthat Ytλ1ðÞ,Ytλ2ðÞ,V1\ni, and V2\niare mutually independent in this paper.5 Fault big data analysis based on effort prediction models 69\n\nFrom It ^o’s formula [10], the solution of eqs. (5.10) and (5.11) can be obtained as\nfollows:\nIn case of t≥0ðÞ :\nZfjetðÞ=α1−exp −βt−σwtðÞ−XYtλ1ðÞ\ni=1logV1\ni()""#\n, (5:12)\nZfjstðÞ=α1−1+βt ðÞ ·exp −βt−σwtðÞ−XYtλ1ðÞ\ni=1logV1\ni() ""#\n. (5:13)\nIn case of t≥0,t′≥t1/C0/C1\n:\nZfjetðÞ=α1−exp −βt−σwtðÞ−XYtλ1ðÞ\ni=1logV1\ni−XYt′λ2ðÞ\ni=1logV2\ni()""#\n, (5:14)\nZfjstðÞ=α1−1+βt ðÞ ·exp −βt−σwtðÞ−XYtλ1ðÞ\ni=1logV1\ni−XYt′λ2ðÞ\ni=1logV2\ni() ""#\n. (5:15)\nConsidering the time delay over t2t2≥t1 ðÞ , we can formulate the flexible jump diffu-\nsion process models as follows:\nZfjetðÞ=α1−exp −βt−σwtðÞ−XK\nk=1XYtkλkðÞ\ni=1logVk\ni8\n<\n:9\n=\n;2\n435, (5:16)\nZ\nfjetðÞ=α1−1+βt ðÞ exp −βt−σwtðÞ−XK\nk=1XYtkλkðÞ\ni=1logVk\ni8\n<\n:9\n=\n;2\n435, (5:17)\nwhere t\nkk=1,2,...,K ðÞ means kth specific time for major version upgrade, and K\nare the number of major version upgrade.\n5.3 Parameter estimation for noise parameters\nof flexible effort prediction models\n5.3.1 Method of maximum likelihood for the drift term\nWe estimate several unknown parameters α,β,b,a n d σ1in eqs. (5.16) and (5.17) in this\nsection. Note that σ2andlare the known parameters, because σ2andlare obtained as\nthe network factor. The joint probability distribution function for Z tðÞis defined as\nPt1,y1;t2,y2;...;tK,yK ðÞ ≡Pr½Zt1ðÞ≤y1,...,ZtKðÞ≤yKjZt0ðÞ =0/C138. (5:18)70 Yoshinobu Tamura and Shigeru Yamada\n\nFrom. eq. (5.18), the probability density is shown as follows:\npt1,y1;t2,y2;...;tK,yK ðÞ ≡∂KPt1,y1;t2,y2;...;tK,yK ðÞ\n∂y1∂y2/C1/C1/C1∂yK. (5:19)\nThen, the likelihood function, λ, for the actual effort data tk,yk ðÞ k=1,2,...,K ðÞ is\nconstructed as\nλ=pt1,y1;t2,y2;...;tK,yK ðÞ . (5:20)\nAs the technique for mathematical manipulations, we consider the following loga-\nrithmic likelihood function:\nL=logλ . (5:21)\nThen, the estimates α*,β*,b*, and σ*\n1are estimated by using the following simulta-\nneous likelihood equations. Then, the estimates are given by making Lin eq. (5.21)\nmaximize\n∂L\n∂α=∂L\n∂β=∂L\n∂b=∂L\n∂σ1=0. (5:22)\n5.3.2 Genetic algorithm approach for the jump terms\nOur model includes the mixed distribution such as jump diffusion and Wiener pro-\ncesses. Therefore, the complicated likelihood function is structured from our model.Then, it is difficult to estimate the unknown parameters of jump terms of our model.\nAlso, several estimation methods of unknown parameter included on jump terms\nhave been proposed in the past. However, the novel estimation method has only afew presented. We discuss that the estimation methods based on GA in order to esti-\nmate the unknown parameters in the jump terms in this section. The estimation pro-\ncedure of unknown parameters of our jump model is as follows:\nThen, we consider γ,μ, and τas the unknown parameters in the jump term.\nThen, the parameters μandτstructure the jump range V\ni.\nIn particular, the fitness function is structured from the estimate and actual\ndata. The following error function as the fitness function is defined in this chapter,for example, the error between the estimate and the actual values:\nmin\nθFiθðÞ,\nFi=PK\ni=0ZjiðÞ−yi/C8/C92,5 Fault big data analysis based on effort prediction models 71\n\nwhere MjiðÞis the cumulative software operation effort at time iin our model based\non jump diffusion process, yithe cumulative software effort. Also, θmeans the pa-\nrameter set in terms of γ,μ, and τ.\nThe above-mentioned processes are applied to the unknown parameter of jump\nterms [11, 12].\n5.3.3 Deep learning approach for the jump terms\nWe use the method of maximum likelihood for the unknown parameters α,β,b,\nandσ. In particular, it is very difficult to make a decision of the unknown parame-\nters of jump terms in our models because of the complexity in likelihood function\nincluding multiple distributions based on the Wiener process and jump diffusionone. Several estimation methods for jump parameters of jump diffusion process\nmodel have been proposed by the specified researchers. However, there are no ef-\nfective methods of such estimation. This chapter discusses the estimation methodof parameters in terms of jump terms. Then, a deep learning is used in this chapterin order to make a decision the jump parameters of the discussed model.\nFor example, we assume that our jump diffusion process models includes the pa-\nrameters λ\n1andλ2forYtandYt′, similarly, μ1,μ2,τ1,a n d τ2forV1\niandV2\niin eqs.(5.16)\nand (5.17). Then, the set parameters Jin terms of λ1,μ1,a n d τ1are estimated by deep\nlearning algorithm in case of t≥0ðÞ . Similarly, the set parameters J'i nt e r m so f λ2,μ2,\nandτ2make a decision by using the deep learning algorithm in case of ( t'≥t1).\nThe deep learning structure in this chapter is represented in Figure 5.1. In\nFigure 5.1, zll=1,2,...,L ðÞ , and zmm=1,2,..., M ðÞ means the pretraining units\nin hidden layer. Also, onn=1,2,..., N ðÞ is the unit of output layer as the com-\npressed characteristic. In this chapter, we apply the deep feedforward neural net-\nwork from among several algorithms of deep learning in order to learn the OSS\nfault big data on bug tracking systems. We apply the following input data sets interms of the each unit on the input layer. Then, the unknown parameters as the ob-jective variable are given as the parameter set Jin terms of λ\n1,μ1, and τ1. The follow-\ning nine items as explanatory variables are set to the units of input layer:\n–Date and time\n–OSS product name\n–OSS component name\n–Name of OSS version\n–Reporter (nickname)\n–Assignee (nickname)\n–Fault status\n–OS name\n–Level of severity in terms of fault72 Yoshinobu Tamura and Shigeru Yamada\n\nThen, the input value for each input units will be changed from the character to\nthe numerical value [13].\n5.4 Numerical examples\nThe Apache HTTP Server [14] from Apache Software Foundation well-known as OSS.\nFigure 5.2 is the estimated operation effort expenditures based on exponential effort\nprediction model by using the GA. Also, the dot –dash line shows the start line of\nbeta version 7.0.0 major-version-upgraded from version 6.x line. From Figure 5.2, we\nfind that the jump becomes large after the time 1,826 days. Similarly, Figure 5.3 repre-\nsents the estimated cumulative OSS operation effort expenditures based on S-shaped\neffort prediction model by using the GA. From Figure 5.3, we find that the S-shapedeffort prediction model fits better than exponential effort prediction model for the ac-\ntual data sets, where the long-dash line shows the start line of version 4.1.31 major-\nversion-upgraded from version 3.x line in In Figures 5.2 and 5.3.\n[Pretraining units]\nfirst\ninput and output\nlayer\n[Pretraining units]\nm th\ninput and output layer\nas\nhidden layerContinued\ndeep\nlearningCompressed\ncharacteristics\n1\n21\n21\n2\nN M LOn\nOnZi\nZlZm\nFigure 5.1: The feed forward deep neural network structure as deep learning.5 Fault big data analysis based on effort prediction models 73\n\nActual\n4e+08Man*days effort2e+08\n0e+00\n0 2,000 4,000\nTime (days)6,000DATA Jump diffusion process Model Estimate\nFigure 5.2: The estimated cumulative OSS operation effort expenditures based on exponential\neffort prediction model by using the GA.\nActual\n4e+08Man*days effort\n2e+08\n0e+00\n0 2,000 4,000\nTime (days)6,000DATA Jump diffusion process Model Estimate\nFigure 5.3: The estimated cumulative OSS operation effort expenditures based on S-shaped effort\nprediction model by using the GA.74 Yoshinobu Tamura and Shigeru Yamada\n\nSimilarly, Figures 5.4 and 5.5 show the estimated cumulative OSS operation effort\nexpenditures based on exponential effort prediction model by using the deep learn-\ning, and the estimated cumulative maintenance effort expenditures based on S-shaped effort prediction model by using the deep learning. In particular, the datasets of specified phases are estimated by the deep learning in Figures 5.4 and 5.5.\nFrom Figures 5.4 and 5.5, we found that the estimates by deep learning can show\nfor each phase in detail.\n5.5 Concluding remarks\nThis chapter focusses on the software effort control for OSS projects. The optimal esti-mation of OSS effort indirectly relates to the OSS quality, OSS reliability, and OSS\ncost reduction. In this chapter, we have discussed the method of OSS effort assess-\nment considering the irregular situations with jump term from the characteristics in-cluding several OSS version upgrade. It is difficult for the OSS project managers to\nestimate the many parameters of jump terms. Then, we discussed several methods of\nparameter estimation in our effort prediction models. The proposed parameter esti-mation methods will be useful as the estimation method of the progress with OSS\nversion upgrade from the standpoint of the project management.\nActual\n4e+08Man*days effort2e+08\n0e+00\n0 2,000 4,000\nTime (days)6,000Data JDP model Estimate\nFigure 5.4: The estimated cumulative OSS operation effort expenditures based on exponential\neffort prediction model by using the deep learning.5 Fault big data analysis based on effort prediction models 75\n\nAcknowledgments: This work was supported in part by the JSPS KAKENHI Grant\nNo. 20K11799 in Japan.\nReferences\n[1] Yamada, S. and Tamura, Y. (2016). OSS Reliability Measurement and Assessment, Springer\nInternational Publishing, Switzerland.\n[2] Norris, J. (2004). Mission-critical development with open source software, IEEE Software\nMagazine, 21(1), 42 –49.\n[3] Zhou, Y. and Davis, J., “OSS reliability model: an empirical approach, ”Proceedings of the\nFifth Workshop on OSS Engineering, pp. 67 –72, 2005.\n[4] Yamada, S. (2014). Software Reliability Modeling: Fundamentals and Applications, Springer-\nVerlag, Tokyo/Heidelberg.\n[5] Lyu., M.R. ed. (1996). Handbook of Software Reliability Engineering, IEEE Computer Society\nPress, Los Alamitos, CA, U.S.A.\n[6] Musa, J.D., Iannino, A. and Okumoto, K. (1987). Software Reliability: Measurement,\nPrediction, Application, McGraw-Hill, New York.\n[7] Kapur, P.K., Pham, H., Gupta, A. and Jha, P.C. (2011). Software Reliability Assessment with OR\nApplications, Springer-Verlag, London.\n[8] Arnold, L. (1974). Stochastic Differential Equations– Theory and Applications, John Wiley &\nSons, New York.\nActual\n4e+08Man*days effort2e+08\n0e+00\n0 2,000 4,000\nTime (days)6,000Data JDP model Estimate\nFigure 5.5: The estimated cumulative OSS operation effort expenditures based on S-shaped effort\nprediction model by using the deep learning.76 Yoshinobu Tamura and Shigeru Yamada\n\n[9] Wong, E. (1971). Stochastic Processes in Information and Systems, McGraw-Hill, New York.\n[10] Yamada, S., Kimura, M., Tanaka, H. and Osaki, S. (1994). Software reliability measurement\nand assessment with stochastic differential equations, IEICE Transsactions on Fundamentals,E77–A(1), 109 –116.\n[11] Tamura, Y., Sone, H. and Yamada, S. (2019). Productivity assessment based on jump\ndiffusion model considering the effort management for OSS project, International Journal ofReliability, Quality and Safety Engineering, 26(5), 1950022-1 –1950022-22. World Scientific.\n[12] Tamura, Y. and Yamada, S. (2019). Maintenance effort management based on double jump\ndiffusion model for OSS project, Annals of Operations Research, Springer, US, Online First,\n1–16. Doi: 10.1007/s10479-019-03170-w.\n[13] Tamura, Y. and Yamada, S. (2017). Fault identification and reliability assessment tool based\non deep learning for fault big data, Journal of Software Networking, 2017(Issue 1), 161– 176.\nDoi: 10.13052/jsn2445-9739.2017.008.\n[14] The Apache Software Foundation, The Apache HTTP Server Project, http://httpd.apache.org/5 Fault big data analysis based on effort prediction models 77",18017
11-6. Modeling the meaning of data streams and its impact on the system performance.pdf,11-6. Modeling the meaning of data streams and its impact on the system performance,"Mario Diván and María Laura Sánchez-Reynoso\n6 Modeling the meaning of data streams\nand its impact on the system performance\nA perspective of the data stream content in the data-driven\ndecision-making\nAbstract: Under the category of the data stream engine, it is possible to find software\nthat is able to process data arriving in real-time from different data sources. In this\nsense, the Internet-Of-Things (IoT) has played a key role because it provides thebases for implementing a cheap and interesting alternative for data collecting, de-\nploying different kind of devices along the monitoring field. However, IoT is charac-\nterized for the heterogeneity, which is associated with different kinds of devices,proprietary data formats related to each device, jointly with the differences betweenprecision and accuracy depending on the associated technology. This level of hetero-\ngeneity is useful because it allows implementing nonproprietary solutions and be\nable to choose between wider alternatives, but also incorporates an additional levelof complexity derived from its heterogeneity. As the main contribution, the idea of\ncooperative and exclusive data streams jointly with its impact on the system perfor-\nmance is explained. Thus, an alternative to model the structure and the meaning re-lated to data streams related to measurement coming from IoT devices are outlined.\nOn the one hand, the modeling strategy pretends to describe the heterogeneous data\nsources through which a set of measures are jointly informed under the same datastream. On the other hand, the modeling strategy in those cases in which each metric\nis individually informed is described. Both perspectives are mutually exclusive be-\ncause the first one interprets each stream as a common channel through which differ-ent metrics and measures are informed (i.e., a cooperative data stream), while inthe second one, each data stream is exclusive for a given metric and their associated\nmeasures (i.e., an exclusive data stream). An alternative for modeling each one and\nto translate the definition between them is presented. In addition, a simulation asso-ciated with the overhead associated with both kinds of data processing is described.\nKeywords: data streams, data meaning, real-time data processing, system perfor-\nmance, exclusive data streams, cooperative data streams\nMario Diván, María Laura Sánchez-Reynoso, National University of La Pampa, La Pampa,\nArgentina\nhttps://doi.org/10.1515/9783110619058-006\n\n6.1 Introduction\nThe dynamism and adaptability are two properties of the system introduced in the\ngeneral system theory [1], which enable it to be adjustable according to the contex-tual changes. On the one hand, the adaptability refers to the system’ s ability to ade-\nquate to its environment in order to follow satisficing its aim. On the other hand,the dynamism is associated with the velocity in which a system is able to adapt to acontextual change. Both properties play an essential role in relation to system per-\nformance and the ability to manage the available resources [2].\nGlobalization tends to establish interdependence among countries from the\neconomical point of view, which implies (among other things) the necessity to beconstantly communicated between stakeholders to keep the link in an appropriated\nway [3]. In this sense, communication technologies played an essential role, andtoday, to be communicated with people around the world is quite simple and acces-\nsible for any citizen.\nIn this sense, the decision-making processes have changed for adapting to a\nglobalized scenario, extending its boundaries and turning on a distributed context.\nExtending the boundaries enable to incorporate new perspectives and point of view\nrelated to cultural and social factors, at the same time in which the distributed con-\ntext provides risk and responsibility distribution among the players, avoiding theconcentration and fostering the balanced and wide participation.\nA globalized environment requires a sensible orchestration of information that\nneeds to be available to each stakeholder in order to support the data-driven deci-sion-making. Intuition is important in some situations in which a person needs to\nreact to deal with the uncertainty. However, when it is possible to have enough in-\nformation for reducing the uncertainty, it always will be a better option becauseeach decision is based on previous knowledge or experiences. The orchestration im-\nplies a certain level of synchronization in order to warranty that each part has\nenough and updated data for deciding.\nThe real-time decision-making elevates the level of required synchronization up to\nan extreme, implying that each decision should be sustained by the last known datacoming in a direct way from the source. It sounds easy to say but it has a lot of chal-lenges related to (i) Data collecting: the way in which each data is obtained; (ii) Datau a l i t y :i ti sr e l a t e dt od i f f e r e n td a t as o u r c ea s p e c t ss u c ha st h ec o n f i d e n c e ,a c c u r a c y ,\nand precision; (iii) Data transporting: it refers to the way in which the data are carried\nfrom the data source to each stakeholder involved throughout the decision-making;(iv) Data processing: It indicates the way in which each data is processed in order to\nsupport the decision-making, keeping in min d that new data are continuously arriving\nand the processing resources are limited ( i.e., memory, processor, etc.); and (v)\nDecision-making process: it focuses on the u sed decision-making schemas in a distrib-\nuted environment [4, 5].80 Mario Diván and María Laura Sánchez-Reynoso\n\nData stream incorporates a continuous data processing paradigm able to deal\nwith heterogeneous data sources providing data such as they are. The data sources\nare autonomous; they can generate an unbounded data stream varying the arrivingrate without any previous notice. The paradigm supposes that a data stream engineis able to take different data sources and to process them in different ways (e.g.,\nsynthesizing, joining, grouping, etc.) with the aim of generating new outcomes or\neven answer different kinds of queries (be it planned or not). However, an impor-tant difference with traditional data management is that in data streaming the data\nare processed at the arriving time, being discarded after that. That is to say, the\ndata is useful and valuable when they arrive because they are describing the situa-tion in a given instant, however, always there will be newly updated data coming\nfor being processed and analyzed in order to describe the evolution of the situation\nbeing monitored [6, 7].\nInternet-of-Things (IoT) is a concept related to heterogeneous, tiny, available,\nand cheap devices able to be used as data collectors, allowing a wide applicationarea. This versatility derives in a challenge given by the complexity incorporatedthrough the heterogeneity itself [8]. That is to say, different devices will have differ-ent data formats, precision, accuracy, reliability, among other aspects that coexist\nunder the same collecting strategy articulated through network communications. In\naddition to the heterogeneity and considering the wide application of this kind ofdevices, challenges related to security [9], articulation with Fog computing [10], Big\nData [11], Blockchain [12], among others. The versatility and accessibility of IoT de-\nvices as data collectors added to the real-time processing abilities related to datastreaming, foster an ideal scenario for data analytics or monitoring applications in\ngeneral [13].\nAs main contributions, (i) Exclusive and cooperative data streams are intro-\nduced as a way for modeling the heterogeneity from the data sources, allowing dis-criminating from the beginning the kind of expected behavior in relation to each\ndata source jointly with the associated processing requirements; (ii) A translatingschema between exclusive and cooperative data streams is shown with the aim of\nmaking both interoperable; and (iii) An analysis of the potential overhead associ-\nated with the translating schema between cooperative and exclusive data streamsis outlined.\nThe chapter is structured in nine sections. Section 6.2 describes some approaches\nrelated to the data organization associated with the data steaming ’s environment.\nSection 6.3 introduces the systematic mapping of literature on data stream modeling.Section 6.4 describes the necessity of a framework throughout the measurement pro-\ncess. Section 6.5 synthesizes the idea related to the processing strategy. Section 6.6\nintroduces the modeling of the exclusive and cooperative data streams respectively.Section 6.7 outlines some basic operations over exclusive and cooperative data sour-\nces, while Section 6.8 analyzes the processing overhead associated with it. Finally,\nsome conclusions and future works are presented.6 Modeling the meaning of data streams and its impact on the system performance 81\n\n6.2 Current approaches of the data organization\nin streaming\nIn [14] the authors propose an architecture for recreating data streams from web data\nand/or sensors ’data stored on the Cloud. They indicate that the underlying idea is to\ntake advantage of the huge volume of data available on the web, but they cannot be\ndirectly queried because it does not contain an associated API. Thus, the architecture\nnamed Sensorizer is able to get heterogeneous data from the web by means of con-tainers that are able to send under an integrated stream all the data together. The\nconcept of virtual sensor nodes is introduced with the aim of implementing multiple\nvirtual transducers, where each transducer is related to single web content. Thus,each virtual sensor node could be associated with a lot of heterogeneous data sources\nthat will provide data according to the monitored web content.\nIn [15] authors synthesize Facebook ’s ecosystem associated with real-time data\nprocessing. In that context, data coming from users and their activities are processedand analyzed at the light of performance, fault tolerance, scalability, correctness,\nand ease of use. In this case, all the processing schemas are related to heterogeneousdata coming from different systems in an integrated way, for that reason, the originaldata stream and those generated from them could be related to different user data\nsources.\nUp to here, the analyzed data streams in the previous proposals correspond to\nheterogeneous data that are informed through a common media that is understoodas a data stream. On the one hand, in [14] the data streams are integrated from data\nconsumed from the web depending on the containers and defined transducers. Onthe other hand, in [15] the streams are consumed, processed, and derived from users ’\nactivity jointly with involved system information. In [16] a load-aware shedding algo-rithm applied to data stream systems is proposed. However, the data stream is under-stood as an unbounded sequence of “tuples, ”where the tuple is a set of (key, value)\npairs that could be combined for supporting complex data structures. That is to say,this case incorporates a certain level of structuration when the previous ones did notestablish explicit restrictions around it. This is particularly important to mention be-cause it does not seem to exist in homogeneity around the concept of the data\nstream, however, there is a consensus around the idea of data streams that could be\nsynthetically defined as an unbounded data sequence.\nIn [17] the data stream is referred to as an unbounded sequence of real-time\ndata, where the data is understood as a tuple. Each tuple has attributes characteriz-ing some aspect jointly with a special attribute that represents the timestamp. Inother words, the data stream is supposed ordered in terms of its timestamp. This\nperspective of tuple implies a bidimensional point of view where the attributes play\nthe role of columns, while each transmitted record with that structure represents atuple or record itself. This perspective of the data stream has a certain level of82 Mario Diván and María Laura Sánchez-Reynoso\n\nstructuration in terms of the relational paradigm, generating different questions,\nfor example, when the data is a likelihood distribution related to one attribute, is\nthe sequence of pairs (value, likelihood) informed into one tuple or by means of aset of tuples? In the first case, what data model is employed for interpreting thedata organization? In the last case, how can the relationship be established for indi-\ncating that all the tuples correspond to the same likelihood distribution? In some\nsituations, the business logic is embedded into the streaming application, whichimplies a dependence on the written code in place of the data meaning, mixing the\ndata layer with the processing layer. The last represents a risk because of the cou-\npling between layers is increased in place of fostering the uncoupling, which wouldmake easy the applications ’maintenance.\nIn [18], a model for the online reconstruction of sessions is introduced, articulat-\ning the batch environment jointly with the data stream context. Data from the logsare incorporated through the figure of records, which represents a similar meaning to\nthe tuple. However, in this case, authors do not restrict the record to a bidimensional\ndata organization, being possible to integrate a record with different elements comingfrom the logs.\nIn [19], a survey related to GeoStreams is introduced describing it such as a\ndata stream containing temporal and spatial data. However, the data stream is de-fined as a permanently updating source of data coming from active origins. An in-teresting difference in comparison to the previous perspectives is the term “active\norigins, ”which explicitly highlights the push mechanism related to the data gener-\nators, and the independence between the data generated in the source with respectto the data once they enter the data stream engine. As a corollary, the data coming\nfrom the active origins are not plain, that is to say, it is not just a number. On the\ncontrary, it is complex data that contains temporal and spatial information that isuseful in these kinds of geographic applications.\nIn [20] the Spark Structured Streaming is introduced. The new model simulates\na live data streaming such as a bidimensional table that always is growing up.Thus, the concept of a tuple is associated with a set of attributes or columns, while\nthe table is composed of a set of tuples. The particularity of this new concept of\ntable is that it has no end, always there will be a new tuple or record to append.This incorporation is a complement of the previous streaming architecture orientedto provide user facilities and to make easy the possibilities of application.\nIn [21] a session reconstruction schema is outlined based on the servers ’logs.\nThe underlying idea is to collect data from different logs, integrating and processingthem with the aim of recreating user sessions for being used in mining process mod-\nels. In this case, even when the native source is active, the data generation is not pri-\nmarily oriented to low latency schema. For that reason, a continuous reading,processing, and adapting from the logs need to be addressed for generating the data\nstream in a continuous way. Also, the data streams are not necessarily integrated by\na single value, but also it could be composed of semistructured content.6 Modeling the meaning of data streams and its impact on the system performance 83\n\nIn [22] a library named IoTPy with the aim of helping to develop stream applica-\ntions in an easier way is presented. In this context, the data stream is defined as an\nunbounded sequence of items, in where the stream itself cannot be modified in anyway. The only enabled operation on the data stream is the incorporation of a newitem at the end of the data stream. It initially outlines a difference with the previous\nproposals in two perspectives: (i) The first one is that the proposal refers to items\nand not tuples. The item is a value collected from an IoT sensor, which implies thatthe data stream is an unbounded sequence of values coming from the sensors, and\n(ii) The second one indicates that each data stream is related to data coming from a\nspecific sensor, which implies that only data from that sensor will be enqueued andno other.\nIn [23] a hash table data structure for detecting duplicate on data streams is in-\ntroduced. This perspective is interesting to be considered because it needs to under-stand the data organization of each data stream’ s element to indicate whether it is\nduplicated or not. Thus, the authors define the data stream as an infinite stream ofsymbols, which introduces a meaning closer to the Chandy proposal [22]. That is tosay, Géraud talks about symbols while Chandy refers to items. Both seem to be incontraposition to the tuple concept or semistructured content previously presented\nin other proposals.\n6.3 Data stream modeling: current trends\nWith the aim of identifying the main tendencies in relation to data stream modeling, a\nSystematic Mapping Study (SMS) was carried out based on “Data Stream Modelling ”\nas the main terms. The steps and procedures for implementing SMS are described in[24, 25].\nBriefly, the SMS ’s basic steps are organized around six stages. The first stage\nestablishes the aim of the study. The second stage describes some research ques-tions aligned to the defined objective. The third step is to define the search strategy,indicating the terms to be queried. The fourth stage describes the data extractionstage through which one gets descriptive data about the listed articles into the re-\nsult, obtaining characteristics that are useful for filtering. In the fifth phase, synthe-\nsis is carried forward around the extracted data. Finally, the assurance of results ismonitored.\nIn this case, the aim is established to identify different modeling alternatives of\nthe data stream from the structural point of view. That is to say, our idea is to analyzetendencies related to how the structure of the data flow is modeled in a context asso-\nciated with heterogeneous data sources, which need to be processed in real-time.\nThe Research Questions (RQ) derived from the indicated objective could be syn-\nthesized as follows: (1) RQ1: What kind of data organization the data stream has?84 Mario Diván and María Laura Sánchez-Reynoso\n\n(2) RQ2: How the processing is affected by the data organization? On the one hand,\nthe motivation related to the first research question focuses on the way in which\nthe structure (i.e., the data organization) allows jointly integrate or not data comingfrom heterogeneous data sources under the same stream. On the other hand,the second research question is oriented to analyze the impact that one organiza-\ntion or others have on the data processing itself.\nIn this way, defined the objective jointly with its associated research questions, it\nis possible to write the basic search string. Thus, the search string is indicated as“data stream modelling, ”and it was performed on the Scopus database. The consid-\nered documents were articles, book chapters, and conference papers between 2016\nand 2020, written in English and corresponding to the computer science field. This is\nimportant because the underlying idea consisted in analyzing the current situations\nand the early publications related to the next year in terms of trends. Table 6.1 syn-thesizes the search string performed on the Scopus database.\nThus, 30 documents were obtained, and their metadata were exported to an Excel\nfile for its analysis (e.g., DOI, authors, publishing, abstracts, and keywords). Fromthere, an individual reading was made from the abstract to determine whether each\none was applicable to the specific subject or not. This is important, because the ini-\ntial searching on the database is syntactical, that is to say, the search engine looksfor the specific indicated terms, but it does not analyze the meaning of each word\nin the context. For this reason, the individual reading of the abstract or full text is a\nkey stage.\nOnce the main filters (e.g., the period) were applied (see Table 6.1), the filtering\nstage is associated with specifying the retaining and exclusion criteria. In this case,the inclusion criteria refer to those works that explicitly describe aspects of the dataorganization in the data stream or real-time data processing. However, keynotes,Table 6.1: Search string performed on the Scopus database on November 15 of 2019, 20:05\n(Argentina time zone).\nMain term Alternative terms Performed search query on Scopus\n“Data Stream\nModelling ”“Real-Time Data Modelling ”\nor“Data Flow Modelling ”TITLE-ABS-KEY ( “Data Stream Modeling ”OR“Real-\nTime Data Modeling ”OR“Data Flow Modeling ”) AND\n( LIMIT-TO ( DOCTYPE, “cp”) OR LIMIT-TO ( DOCTYPE,\n“ar”) OR LIMIT-TO ( DOCTYPE, “ch”)) AND ( LIMIT-TO\n( SUBJAREA, “COMP ”)) AND ( LIMIT-TO ( LANGUAGE,\n“English ”)) AND ( LIMIT-TO ( PUBYEAR,  )O R\nLIMIT-TO ( PUBYEAR,  ) OR LIMIT-TO ( PUBYEAR,\n ) OR LIMIT-TO ( PUBYEAR,  ) OR LIMIT-TO\n( PUBYEAR,  ))6 Modeling the meaning of data streams and its impact on the system performance 85\n\ninvited talks, and surveys are not considered, they will be rejected. Thus, from the\noriginal 30 papers in the answer, only 10 were retained in which their metadata are\nsynthesized in Table 6.2 introduced in the next page.\nFigure 6.1 synthesizes the evolution related to the kinds of publications for this\nspecific subject, highlighting the associated publishers. It is important to observe\nthat only three from the ten records correspond to conference papers, being the high-\nest proportion associated with journals. That suggests that concerning this subject,the publications have a certain level of maturity in terms of its associated results.\nThe idea is to analyze how each of the earlier-listed papers deals with the data stream\nconcept from the morphological point of view and its impact on the data stream proc-\nessing. Identifying the main tendencies about data stream modeling between 2016and 2020 is especially interesting, because it is possible to appreciate the last per-\nspectives and alternatives to model the data stream itself, independently of the previ-\nous interpretations.\nLughofer et al. [26] introduce a proposal of architecture oriented to the general-\nized evolving fuzzy systems. The underlying idea resides in an agile detection ofdata concept drifts without using additional parameters (e.g., a threshold). Data arereceived as a series of multidimensional vectors that are immediately analyzed, in-\ncorporating a rule splitting methodology working around an incremental way,\nwhich implies that each vector is read up to once time.\nLughofer [27] introduced advances, challenges, and tendencies of the online ac-\ntive learning and its mutual impact on between machine learning techniques and\nits context. Perspectives related to the classical batch off-line learning is contrasted\nto online active learning, exposing similarities and differences between them. Inthis context, data streams are understood as a sequence of data that could be sam-\npled in different ways, employing different kinds of techniques. Different learning\ntechniques are outlined and compared to them.\nGeoris-Creuseveau et al. [28] proposed a framework oriented to work around\nspatial data infrastructures. In this study, the data collecting strategy is based onthe capturing of online questionnaires and semistructured interviews.\nJournal\n2019Conference Journal\n2018Conference Journal\n2017 20163\n2\n1\n0\nACM Elsevier Inc. SAGE Publications Inc. SciTePr ess Springer Verlag Ta ylor & Fr ancis Ltd. IEEEConference\nFigure 6.1: Evolution of the kinds of publication by year highlighting their associated publishers.86 Mario Diván and María Laura Sánchez-Reynoso\n\nTable 6.2: Retained papers once the individual reading was made, considering the citation index on November 15 of 2019, 20:05 (Argentina time zone).\nAuthors Title Year Source Publisher First\nauthor ’s\ncountryCited\nby\nLughofer E.,Pratama M., Skrjanc I.“Incremental rule splitting in\ngeneralized evolving fuzzy systems forautonomous drift compensation ” IEEE Transactions on Fuzzy Systems IEEE Austria \nLughofer E. “On-line active learning: A new\nparadigm to improve practical usabilityof data stream modeling methods ” Information Sciences Elsevier Austria \nGeoris-Creuseveau J.,Claramunt C.,Gourmelon F.“A modelling framework for the study\nof Spatial Data Infrastructures appliedto coastal management and planning ” International Journal of GeographicalInformation ScienceTaylor &Francis Ltd.France \nKamburugamuve S.,Wickramasinghe P.,Ekanayake S., Fox G. C.“Anatomy of machine learning\nalgorithm implementations in MPI,Spark, and Flink ” International Journal of High PerformanceComputing ApplicationsSAGEPublicationsInc.UnitedStates\nKoek P., Geuns S. J.,Hausmans J. P. H. M.,Corporaal H.,Bekooij M. J. G.“CSDFa: A model for exploiting the\ntrade-off between data and pipelineparallelism ” Proceedings of the th International\nWorkshop on Software and Compilers forEmbedded Systems, SCOPES ACM Netherlands \nDubrulle P., Gaston C.,Kosmatov N., Lapitre A.,Louise S.“A data flow model with frequency\narithmetic” Lecture Notes in Computer Science(including subseries Lecture Notes inArtificial Intelligence and Lecture Notes inBioinformatics)SpringerVerlagFrance \n(continued)6 Modeling the meaning of data streams and its impact on the system performance 87\n\nTable 6.2 (continued)\nAuthors Title Year Source Publisher First\nauthor ’s\ncountryCited\nby\nMeinig M., Tröger P.,Meinel C.“Finding classification zone violations\nwith anonymized message flowanalysis ” ICISSP  –Proceedings of the th\nInternational Conference on InformationSystems Security and PrivacySciTePress Germany \nMasulli F., Rovetta S. “The Challenges of Big Data and the\nContribution of Fuzzy Logic” Lecture Notes in Computer Science(including subseries Lecture Notes inArtificial Intelligence and Lecture Notes inBioinformatics)SpringerVerlagItaly \nChadli N., Kabbaj M. I.,Bakkoury Z.“Detection of dataflow anomalies in\nbusiness process an overview ofmodeling approaches ” ACM International Conference ProceedingSeriesACM Morocco \nMackie I. “A geometry of interaction machine for\ngödel ’s system t ” Lecture Notes in Computer Science(including subseries Lecture Notes in\nArtificial Intelligence and Lecture Notes in\nBioinformatics)SpringerVerlagUnitedKingdom88 Mario Diván and María Laura Sánchez-Reynoso\n\nKamburugamuve et al. [29] described the perspectives related to data manage-\nment and its impact on platforms such as Apache Spark, Flink, and MPI. In general,\nthey indicate that the platforms consider the data and their processing through differ-ent variations of graph model, while that data itself is conceptualized using a bidi-mensional and immutable data structures known as Resilient Distributed Datasets\n(RDD) in Spark or DataSet in Flink. The data stream could be varied throughout the\nprocessing line implemented in each engine, considering them as a sequence bidi-mensional of tuples that are input for a set of operation, while it could be the output\nfor another set of operations.\nKoek et al. [30] analyzed the challenge related to the data communication under\na context of parallelism. Basically, the data streams are understood as a sequence ofatomic data (e.g., an integer) able to communicate two or more tasks (i.e., operators).\nThey propose a model for coordinating the data flow with eventual auto-concurrencybetween operations throughout the whole data processing.\nDubrulle et al. [31] outlined a proposal for modeling data pipelines with restric-\ntions between producers and consumers. The graph theory is used for representingas edge a consumer or producer, that it will be determined based on the sense ofthe arc. When the arc arrives at an edge, the edge will be consuming the data, but\nwhen the arc is departing from an edge, it will play a producer role. Annotations\ncould be employed for discriminating the behavior between different data sources(e.g., the data frequency). Also, the challenge related to the data fusion unifying\nthe data produced from a set of heterogeneous data sources is introduced.\nMeinig et al. [32] dealt with the challenge of identifying the classification zone\nviolation. For that purpose, the logs from servers are used as input. However, giventhe heterogeneity and different data formats related to each one, a data conversion\nstage is necessary. In this sense, the authors describe basically the necessary oper-ations for translating the unstructured raw data in each log to a structured data or-\nganization able to be used for generating models. Thus, the raw data are converted\ninto a structured and bidimensional sequence of data following the idea of tuplescharacterized through a set of attributes.\nMasulli et al. [33] described the challenges associated with the big data environ-\nment, its projection over time, and the role of the fuzzy logic. They differentiate twodimensions: data itself and the content. On the one hand, the data refer to the cap-tured fact, that is to say, those records that need to be kept in an organization for\neventual using. On the other hand, the content talks about the data meaning, its\nrole, and impacts on the knowledge. The authors make focus on the necessity ofdeveloping tools oriented to data stream modeling, an aspect that has taken a sig-\nnificant role in past times. For additional details, interesting perspectives from au-\nthors can be analyzed in terms of the clustering of nonstationary streams [34] andtracking time-evolving data streams [35].\nChadli et al. [36] proposed different approaches for dealing with the data flow\nassociated with the business processes jointly with their associated challenges6 Modeling the meaning of data streams and its impact on the system performance 89\n\naround the anomaly detection (e.g., missing values). The work refers to data items\nembedded in the data flow, which implies a sequence of tuples organized as a set\nof columns flowing between processes. They analyze the using of a data-flow matrixfor studying the challenge, while the Petri ’s net is used for anomaly detection\nthroughout the dataflow exchanged between processes.\nMackie et al. [37] proposed a dataflow model. They talk about tokens for repre-\nsenting the data that is traveling throughout a network. The underlying idea of thesequence is associated with an atomic sequence of data being communicated be-\ntween computation components that are previously determined.\n6.4 Measurement: the necessity of a framework\nIt is possible to define the measurement as the process in which an object or subject\nunder analysis needs to be quantified through one or more attributes that help to\ncharacterize it. The measurement consists of a quantification schema where the\nidea is to contrast the obtained value about some known pattern. For example, theheight of a person requires having a reference pattern such as the meter, the weightof a car requires to know the idea of the kilogram, and so on. These examples out-\nline two different challenges. On the one hand, the necessity of quantifying a cer-\ntain number of characteristics related to an object, subject, or concept. On the otherhand, the necessity of having a reference pattern useful for comparison.\nThese challenges carry us to ask questions such as, Why we need to measure?\nand Why we need to compare the result with some pattern? Instinctively, personstend to analyze each behavior, phenomenon, object, concept, and/or subject that\nsurround them. The evolution of the human being required a deep understanding\nof the whole context, the different kinds of interactions and comprehension of eachrole played over time. The necessity of quantifying is early discovered in our world\nas a key asset associated with the progress of different activities, for example, the\nidea of knowing what is cool and hot, cheap and expensive, high and low, andheavy and light.\nOne of the first concepts developed by humans was the number. That concept\nanswered the necessity of accounting each object available in the context (e.g., food).That was how the quantification was early applied for knowing the volume of thingsavailable. However, the requirement of comparing previous values with current val-\nues emerged as a necessity. That is to say, being possible to represent a concept (e.g.,\nanimals) through a number indicating the volume of available things, now the neces-sity falls into knowing whether the current quantity is upper than before or not.\nThus, the comparison between magnitudes was part of a natural process [38].\nThus, the answer to why we measure for is associated with quantifying a char-\nacteristic and from each quantified value, a comparison pattern is employed for90 Mario Diván and María Laura Sánchez-Reynoso\n\ndetermining whether a concept has been increased or not. The answer to why a\ncomparison pattern is necessary, it falls into the requirement of establishing a com-\nmon and uniform reference useful for anyone who needs to establish an analogywith a concept. It was one of the original motives for creating, for example, the met-ric system, among others.\nNowadays, the measurement as a process is applied in different fields, and it\ncould be indicated as a transversal and nonexclusive practice. In effect, the mea-surement process could be used from computer science to life science on different\nconcepts, from the processor ’s temperature to the heartbeat rate respectively. This\naspect constitutes a huge virtue but at the same time a challenge, because the abil-\nity to be deployed on different fields implies the ability to deal with the heterogene-\nity, which impacts directly on the problem complexity. Imagine for a moment the\ndifferent kinds of devices that could be used in different situations, the associatedaccuracy, required precision, methods, and so on. This allows introducing the con-\ntext in which each measurement process is performed, that is to say, the employed\ninstruments, methods, and concept under study is not isolated from the world, bythe contrary, all of them they are embedded in their specific environment, mutuallyaffecting each other.\nUp to here, it is worthy to mention that the measurement is focused on a quanti-\nfication that needs to be compared with a certain pattern, which is influencedthrough the environment and affected by the involved heterogeneity. However, some-\nthing that seems to be easy not always is trivial to implement. For example, it is pos-\nsible to try to measure the corporal temperature of a person using a thermometer, butone thing is to get a measure with a device positioned in the internal region of the\near, and another very different thing is to get the measure from the axillar region.\nEven with the same device under the same environmental conditions, the resultcould vary depending on the employed method [39]. This is important to highlight\nbecause the essence of the measurement is the comparability of values.\nIn this way, it is possible to appreciate that the intention of measuring to quan-\ntify a concept has a set of involved agreements. In other words, the concept relatedto the measurement needs to be identified jointly with the characteristics that de-\nscribe it. From the descriptive characteristics, how each quantitative value is ob-tained should be defined. Also, the strategy for making comparable each value overtime is a critical thing considering the necessity to determine levels of evolution or\nnot based on the patterns taken as a reference.\nThe current world and associated economies require a measurement process as\nagile as possible, tending to be closer to the real-time data processing than before.The distributed, heterogeneous, dynamic, unpredictable, and unexplored environ-\nment in which the transactions currently take place imposes a set of additional con-ditions that need to be attended for ensuring the comparability [40]. For example,\nthe fact of measuring stock variations based on different markets could be not6 Modeling the meaning of data streams and its impact on the system performance 91\n\ndirectly comparable because each market could have associated particularities\n(e.g., volatility, regulations, etc.).\nDue to the expected times for the measurement process, the idea is to approach\nan agile alternative as extensible, reliable, and stable as possible. Thus, the autom-\natization of the measurement process is a must for governments, businesses, and\norganizations in general, which need to optimize different budgets in order to reach\ntheir respective aims.\nThere are a lot of concepts that need to be agreed before implementing a measure-\nment process, for example, the underlying idea of metric, measure, measurement,\nscale, unit, method, among other concepts jo intly with the relationship between them.\nThis constitutes an important aspect because if two persons understand different\nthings for the concept of metric, the value co mparability would be directly affected.\nFor that reason, before implementing or automatizing any measurement process,\nthe fact of sharing and agreeing with the involved concepts to be used throughout theprocess is critical for ensuring the whole reliability of the system [41]. Figure 6.2 syn-\nthesizes a global perspective of the involved concepts in the measurement process.\nAs it is possible to appreciate from the previous figure, the concept to be monitored\n(entity in Figure 6.1) needs to be described by a set of attributes (e.g., heartbeatrate). This implies a discrete representation of the aspects to be analyzed for the\ngiven entity. In addition, the entity is immersed in a context, which is characterized\nusing the context properties (e.g., environmental temperature). The last ones repre-sent a discrete interpretation of the characteristics of the environment in which\neach monitored concept is acting. Both attributes and context properties are studied\ntogether to analyze the mutual incidence on each one.\nEnvironmental temperature\nEnvironmental humidityCorporal temperatureHearbeat rateMaximum arterial pressureMinimum arterial pressure\nQuantified by\nEntity 1Attributes\nProject definitionProject \ndefinition 2\nKnowledge and \nprevious experiencesDecision\nmakerActionsProject \ndefinition 3\nProject \ndefinition “n”\nEntity 2Entity 3Entity “n”\n...IndicatorIt contains:\nInterpreted by Analyzed by\nSupported byQuantified byContext properties\nValues domain\nScaleUnitMethodDevice ...Values domain\nScale\nUnit\nDecision creteriaScenarios\nStates ...Metrics\nFigure 6.2: A Global perspective of the involved concepts in a measurement process.92 Mario Diván and María Laura Sánchez-Reynoso\n\nEach attribute or context property is quantified by means of a metric. Each one\nhas the expected values domain, associated scale, unit, method to be used to get\nthe quantitative value, the device employed jointly with the method, and so on. Thenumerical value obtained from a metric is known as a measure. This definition isessential because it allows to know whether two measures are comparable or not.\nFor example, the previous example related to measuring the corporal temperature\nwould arrive at no comparable measures due to the metric uses different methods(i.e., axillar method versus in-ear method).\nHowever, each measure only indicates a numerical value, but it does not say\nanything about how to interpret it. With that aim is incorporated into the concept ofthe indicator. The indicator consumes one or more measures, incorporating the nec-\nessary decision criteria based on the entity ’s states and current scenarios for analyz-\ning the magnitude in context. Each interpretation will provide the interpretation\nbut does not say much about the courses of action to follow.\nThe figure of the decision-maker reviews the interpretation of each indicator\nsupported by the previous experiences and knowledge from experts, which is perti-nent to the measurement project. Thus, it is able to provide some recommendationsand/or courses of action to be implemented. For example, it could send an alarm to\nentity 1 indicating that the relationship between heartbeat and environmental tem-\nperature is near to a dangerous threshold.\nAs it is possible to appreciate, the previous experiences are shared throughout\nthe different active project definitions, even being possible that each entity depend-ing on its project definition, be different and it has different defined states and sce-narios. All this heterogeneity and complexity needs to be defined under a consistent\nway, interpretable for humans and machines when the aim is the automatization of\nthe measurement process.\nThis is just one interpretation of the involved concepts related to the measure-\nment process, and it is highly possible that there exist other variations or definitions.Independent of that, the key asset is that each set of definitions is well-defined andorganized to be understandable, communicable, and sharable for anyone who needs\nto use it. This point is where the framework is essential. The measurement framework\nmust provide all the terms, concepts jointly with all the available relationships thatare essential to implement a measurement process. For example, an alternative toformalize one is by means of ontology, making it communicable and processable.\nThus, once the framework is agreed, the measurement process could be effec-\ntively implemented in an automatic or manual way to ensure the repeatability, exten-sibility, and consistency of the process. The repeatability constitutes an essential\naspect to know that new measures can be obtained following the same process defi-\nnition. The extensibility refers to the possibility to add new requirements or updatethe previous ones keeping the descendant compatibility. The consistency implies\nthat any change made on the project definition does not affect negatively the meas-\nures comparability.6 Modeling the meaning of data streams and its impact on the system performance 93\n\n6.5 Choosing the processing strategy\nChoose the point throughout the processing chain in which the data starts to be mod-\nified, summarized, or transformed in any way has a huge impact on the whole datacollecting strategy. That is to say, on thing is to try to process data on the same col-\nlecting device, while other thing is to process the data in a central virtual unit far\nfrom the data collectors. The first difference resides on the computing power neces-sary in each location (i.e., be it central or near to data sources), while the second as-\npect is the traveled path by the data. Farther the data are from the data source,\nbigger the data traffic and the resources consuming on the network are.\nEach decision associated with the place where the processing could start has its\nadvantages and disadvantages. On the one hand, when the data processing is nearto the data sources, the data volume interchanged along the network is decreasedbut the processing requirements and storage capacities on mobile devices are in-\ncreased. Also, the data processing needs to have a certain level of independence\nabout other data sources, that is to say, the idea of processing data as near of sen-sors as possible implies a low level of dependence (i.e., coupling) on other sensorsthat are not located in the same processing point. On the other hand, the fact of\nprocessing the data using a common processing point implies the possibility of\nusing a uniform logic, having a whole perspective about the data collecting andprocessing strategy previously not available. Besides, this approach would increase\nthe data traffic on networks due to all the sensors should inform continuously the\ndata to be processed, consuming an important volume of resources and incorporat-ing a delay associated with the data travel time between the sensor and the process-\ning unit.\nA balance between consumed resources, processing pertinence, business re-\nquirements, processing, and communication available technology needs to bereached to determine the feasible point in which processing and storage need to be\nlocated. Thus, at the light of the global data processing strategy, the processingcould be distributed as near to sensors as possible, or by the contrary, to be located\nat a common processing point. Also, when the decision is to distribute the process-\ning, it could be located in some intermediary point between sensors and the mainprocessing unit, or alternatively, as near to sources as possible. Here, these alterna-tives are discussed to contrast the strengths and weaknesses of each one.\n6.5.1 A centralized data processing approach\nThis kind of approach consists of a central unit able to process data in an integratedway, collecting data from different data sources. The application ’s logic is inte-\ngrated jointly with the processing unit, sharing the available resources for carryingforward their functionalities.94 Mario Diván and María Laura Sánchez-Reynoso\n\nIn this kind of environment, sensors are cheap, accessible, available, in limited\ncapacity, and easily replaceable because the main function is to collect data itself. A\npriori, it sounds like something good, but if it is necessary to make some kind of localprocessing in the sensor, it is really a limitation. In this sense, it is important to de-sign the primary use of the sensors and the expected role in relation to the whole\ndata collecting strategy. Because sensors have limited resources in this architecture,\nthey will transmit all the data as soon as they obtain each value, increasing the trans-mitted data volume. In case of some inconvenience with the network, this kind of\ndevices will discard data due to inadequate processing and storage capacity.\nEach sensor is independent of the processing unit, unknowing any aspect of the\nglobal processing logic. Each one only sees its immediate collecting environment,limiting its behavior to the local data gathering, and even not always is autonomous.\nThis implies that the sensor itself not always could collect data by its own media, butit should be linked with another device that provides energy. For example, the\nDS18b20 sensor is able to measure temperature but if and only if it is connected to\nsome processing board such as Arduino One [42].\nOn the one hand, the unification of the data processing provides a global per-\nspective that allows articulating the application ’s logic with the data meaning\nspread out along the field. On the other hand, the complexity is increased due to allthe collected data from heterogeneous data sensors are received in a unique point,being necessary the interpretation, some kind of conversion, among other aspects\ndepending on the type of sensors and associated data formats. Thus, those addi-\ntional functionalities are absorbed by the central processing unit, increasing theglobal processing time.\n6.5.2 A distributed data processing approach\nIn this perspective, all or a part of the application ’s logic is distributed among all the\ncomponents that integrate the processing architecture. The processing unit is not lo-\ncated at one given point, on the contrary; it is spread out along the components that\nintegrate it. This allows distributing the business logic carrying it as near as thesource as it is possible and necessary. On the one hand, it decreases the transmitteddata volume due to part of the processing happens near to the collecting device. On\nthe other hand, it increases the requirements of coordination in the collecting data\nstrategy for avoiding risks of isolations.\nBecause each data collector requires a certain level of autonomy and processing\nability, this incorporates some kind of local buffer and the possibility of data proc-essing. For example, in this architecture, it is possible to find equipment such asArduino Mega or Raspberry Pi articulating other sensors but acting as responsible\nfor the data collecting from all the connected sensors. That is to say, as a difference\nwith the previous architecture, in this case, the concentrator device is responsible6 Modeling the meaning of data streams and its impact on the system performance 95\n\nfor informing and processing the measures of a set of sensors (i.e., The Arduino or\nRaspberry Pi) [43].\nThis possibility of incorporating storage and processing capacities in this type\nof sensors ’concentrator device allows providing partial results near to the data\nsource, being able to detect early different risks depending on the monitoring re-quirements (e.g., fire and flood).\nOne of the challenges in this distributed environment is how the application ’s\nlogic is distributed and balanced among components, discriminating each played role(e.g., collector, gateway, processor, and an alyzer). Another no minor aspect falls into\nthe heterogeneity associated with the components, which will affect directly on how\nthe level of workload is shared out. It is impo rtant to highlight that even when the data\nvolume related to measures is decreased du e to the local processing, the processing\noverhead related to the coordi nation and the provision of par tial results are increased.\nFigure 6.3 synthesizes the general perspective of the data processing from the\ncollecting point of view. Figure 6.3(a) represents a centralized collecting behavior,where sensors play a passive role in the sense that they only provide data. The com-munication is established directly between sensors and the central processing unit(CPU), which eventually could be partially or totally virtualized employing cloud\nresources. In this scenario, users send queries to the CPU to obtain updated data.\nOn the other hand, Figure 6.3(b) schematizes a distributed collecting behavior, where\nsensors are communicated through a collector. The collector is a component with a cer-\ntain level of storage and processing, able to work in a collaborative and distributedway with other collectors. Thus, the collecting strategy is globally distributed, keeping\na certain level of autonomy in each collector. In this case, sensors can interact with\n(b) Distributed processing unit (a)River flow\nCollectorCloudCloud\nCentral\nprocessing\nunitUserUser\nUserEnvironmental\nTempereture\nVelocityEnvironmental\nHumiduty\n...\nFigure 6.3: A general perspective of the data processing from the collecting point of view.96 Mario Diván and María Laura Sánchez-Reynoso\n\ncollectors, deploying an active behavior and being part of the answer to users. Also,\ncollectors could implement all or part of its fun ctionalities using virtualized resources\nin the cloud.\nTable 6.3 allows to complement Figure 6.2 establishing a comparative perspective\nbetween centralized and distributed data collecting strategies. In a centralized strategythe dataflow is unidirectional because the sensor only provides measures without in-\nteraction capacities, while in a distributed environment each collector interacts withsensors (e.g., reviewing the current status) being able to interchange its own measures\nor from third collectors (i.e., indirect transmission). As previously was said, the data\nprocessing and business logic are distributed due to the collectors have resources andabilities to collaborate, while is limited to one unique place in the centralized architec-\nture. Thus, sensors in centralized architecture only provide measures, while in a dis-\ntributed environment they are additionally able to interchange processing results,metadata (i.e., description about other data), and so on. Clearly, the sensor ’s behavior\nin a centralized environment is passive (i.e., such as data provider), while it is activein a distributed context. The possibility of answer queries is another distinctive aspectin where the distributed environment is able to provide approximated data using col-lectors, while the centered context only answers the CPU but with full visibility of\ndata. Finally, because the collectors have certain autonomy, they are responsible for\nobtaining and informing data (e.g., in case of missing value, it could detect and fix thesituation discarding the miscalibration or estimating a value), while in the centered\ncontext the only responsible for data are the sensors. In this way, it is important to\nmention that the sensors involved in a centralized data collecting strategy are cheaperthan the distributed environment due to the necessity of articulation and autonomy of\nthe last one, which provokes that its budget is increased.\nTable 6.3: Synthesis comparative between a centralized and distributed data collecting\narchitectures.\nConcept Centralized Distributed\nKind of dataflow Unidirectional Bidirectional\nData processing and\nbusiness logicCentralized Distributed\nKind of interchangeddata from sensorsBehaviorOnly measuresPassive sensors.Monitoring withoutinteractionMeasures, results, metadata, etc.Active behavior. Sensors and collectors\nData transmission Direct Direct and indirect\nQuery Central processing unit Collectors in a collaborative way\nData responsibility Each sensor Collectors\nCost Cheap sensors (low) Sensors articulated with collecting devices\n(more expensive than centralized)6 Modeling the meaning of data streams and its impact on the system performance 97\n\nT h ed e c i s i o na b o u tt h ek i n do fa r c h i t e c t u r et ob eu s e di nad a t ac o l l e c t i n g\nstrategy will be based on a set of requirements. In this aspect, it is important to\nmention that there is not an ideal architecture for all the environments, but thereare different architectures that could fit better or not to certain contexts depend-ing on the necessities. The essential aspec t in a data collecting strategy is clearly\nto know what the entity under monitoring is, the characteristics being monitored,the eventual relationship between characteristics, the way in which measures areobtained, and how to interpret each value under given scenarios and entity ’s\nstates. Said in another way, the data collecting strategy should be supported by ameasurement framework and all the components (i.e., sensors, collectors, etc.) bealigned with it.\n6.6 Modeling the data stream ’s content\nIn general terms, a data stream could be represented as an unbounded sequence ofdata. However, the grey zone in this definition is related to the origin and the meaningof data. On the one hand, the common assumption is that the unbounded sequence ofdata is originated from one unique data source over which there is no control or some\nway of influencing. On the other hand, the second assumption is related to the mean-\ning of data, that is to say, the data could be one unique kind of value provided overtime with its corresponding variations, or alternatively, it could be understood such as\na data structure with certain structure (i.e., a tuple) continuously provided over time.\nBoth the origin such as the provided data structure have a special interest in any\nmeasurement process because the original data structure affects directly the trace-ability and reliability, while the provided data structure has a direct influence on the\ncontent itself and the aim of the measurement process. For that reason, acquiring anagreement around these two aspects constitutes a key asset to foster the repeatability,\nextensibility, and consistency of any quantification process that needs to interpret\nvalues at the light of indicators using a set of given decision criteria.\nThis kind of discrimination between the origin (or data source) and the data\nstructure associated with the unbounded sequence allows to introduce the idea of\ncooperative and exclusive data streams that are explained accordingly in the fol-\nlowing sections to model each alternative.\n6.6.1 The exclusive data streams\nAn exclusive data stream is defined as an unbounded sequence of domain-known\nand atomic data with an autonomous, simple, and independent data source .98 Mario Diván and María Laura Sánchez-Reynoso\n\nWhen definition refers to a sequence, it indicates a list of data ordered, following\na timestamp that represents the order in which data were generated. Thus, the order-\ning has a relationship to the generation time itself that simultaneously depends onthe data source and the monitored event. This is important to highlight because thesequence implies the existence of some event jointly with a device (virtual or not)\nactively monitoring it. In other words, the underlying idea related to the “sequence ”\nis to receive continuously the updated situation related to an event, and not one spo-\nradic data.\nThe indication related to “unbounded ”makes focuses on the data volume,\nwithout previously establishing any limitation to it. Thus, the idea is to provide asmuch data as necessary about an event, the data generation has not a given bound,\nand the data rate depends on the data source. This is important because it is not\npossible to anticipate a data size, or even, predict the rhythm in which they willarrive. Even more, such rhythm could be absolutely variable, implying that the data\nrates go changing over time without any predefined periodicity.\nThe atomic data is associated with the representation of a single value for a\nunique concept or meaning under analysis, being this immutable. Thus, the mean-\ning that is being monitored does not change over time, for example, if the device\nmeasures the environmental temperature, only such concept will be informed using\nthis stream. The atomicity is similar to the relational model, and it implies that onlyone single value is communicated by each timestamp. It could be associated with a\nnumber, a position, bytes, among others but always one single value at the end.\nThe kind of value domain related to each informed single value is previously\nknown and it is immutable. That is to say, the defined data type jointly with the setof constraints is established to bind the normal and expected values’ range. Any in-\ntents of changing the value domain (i.e., data type or constraints) associated with a\ndata stream will imply a redefinition of it.\nFollowing the definition, the data source should be autonomous, simple and\nindependent: (1) By autonomous, it is indicated that it should be able to keep run-ning independently of who is reading its values. The collection method is depen-\ndent on the own device; (2) By simple it is understood that each value has a unique\norigin that affects in a direct way the device traceability; and finally (3) By indepen-dent it indicates that it is not influenced by external sources.\nAn attribute represents a concept to be quantified related to some concept or\nevent that needs to be monitored. Equation. (6.1) defines the set “A”as a set of po-\ntential attributes:\nA=aja is an attributefg (6:1)\nEquation (6.2) describes the way in which a value is obtained from an attribute.That is to say, given an attribute, it is possible to define a method by means of a\nfunction able to get a value (e.g., a measure). The set “M”indicates all the values6 Modeling the meaning of data streams and its impact on the system performance 99\n\nobtained by the application of the function over a given attribute. The kind of re-\nsults will depend on the type of defined function and the used method:\nM=fm2Djm=faðÞ,∀a2Ag (6:2)\nEquation (6.3) formally defines an exclusive positional data stream , representing\nthat the unbounded sequence of values on an attribute “a”is ordered based on the\narriving (i.e., a certain value of “i”) of each obtained value, employing the defined\nfunction in eq. (6.2). However, this stream contains single values without any tem-poral stamp. The notion of order is limited to the position, assuming that the\nelement m\niis previous to m i+1and that way successively:\n∀a2A,i2N=Sp\nex=faðÞi,faðÞi+1,faðÞi+2,.../C8/C9\n=fmi,mi+1,mi+2,...g (6:3)\nDepending on the value domain “D”defined in eq. (6.2), it is possible to deal with a\nnumerical, categorical, or ordinal data stream. An exclusive numerical positional\ndata stream contains only numerical values. An exclusive categorical positional\ndata stream contains categorical values as text. Finally, an exclusive ordinal posi-\ntional data stream contains ordinal values. The ordinal values could be representedas a numerical set where the value represents a given order, not necessarily a mag-nitude, or alternatively, it could be represented as text.\nFor example, it supposes that the chosen attribute is the corporal temperature\nof a person and using some device, it is possible to obtain a value, an exclusivenumerical positional data stream as in eq. (6.4):\nS\np\nexCorporal TemperatureðÞ =36.0,36.1,36.1,36.08,36.07,36.06,... fg (6:4)\nSubtly different is an exclusive categorical positional data stream related to the\nsensing of colors as a text, as expressed eq. (6.5):\nSp\nexcolourðÞ =Red ,Blue, Yellow, Red ,Green ,Pink ,... fg (6:5)\nFollowing the analysis, an exclusive ordinal positional data stream could be repre-\nsented by means of text (see eq. (6.6)) or numbers (see eq. (6.7)). For example, it sup-\nposes that the value of the temperature is interpreted by an indicator informing ina continuous way one of the following values: 1. Hypothermia, 2. Low Temperature,3. Normal, 4. Fever, and 5. Very high fever:\nS\np\nexcorporal temperatureðÞ =Normal ,Fever ,Fever ,Fever ,Very High Fever ,... fg\n(6:6)\nSpexcorporal temperatureðÞ =3,4,4,4,5,... fg (6:7)\nEquation (6.8) defines an exclusive temporal data stream that describes an un-\nbounded sequence of values based on an attribute “a”jointly with a timestamp\n(i.e., “t”). Both value and timestamp correspond with the monitored attribute, that100 Mario Diván and María Laura Sánchez-Reynoso\n\nis to say, the value m iis obtained at the time tifor the attribute “a”and they need\nto be view as an ordered pair or vector. An important aspect in this kind of data\nstream is the mandatory existence of the timestamp, that is to say, given an orderedpair ( m\ni,ti) the value of micould be missing but the timestamp must be always pres-\nent, else the ordered pair is not considered because that directly would affect the\norder ’s principle indicated in the following equation:\n∀a2A,t2T,i2N=St\nex=faðÞi,ti/C0/C1\n,faðÞi+1,ti+1/C0/C1\n,faðÞi+2,ti+2/C0/C1\n,.../C8/C9\n=mi,ti ðÞ ,mi+1,ti+1 ðÞ ,mi+2,ti+2 ðÞ ,... fg(6:8)\nSimilarly, depending on the kind of definition in the set “D”in eq. (6.2), the exclu-\nsive temporal data stream could be derived in a numerical, ordinal, or categorical\ndata stream.\nParticularly, the ordering constitutes an important aspect in measurement proj-\nects where the order in which each value arrives could be determinant dependingon the kind of analysis carried out, for example, to determine whether a personhas fever or not. In case some project doe s not require specification about the\norder, eqs. (6.8) and (6.3) could be unified extracting the component that defines\nthe order (be it the position or timestamp). Thus, an exclusive data stream is an\nunbounded sequence of single values on an attribute “a,”employing the defined\nfunction in eq. (6.2) in a successive way (represented it in eq. (6.9) through theaccumulation of the apostrophe character) without that necessarily implies a given\norder for the processing:\n∀a2A=S\nex=faðÞ,faðÞ′,faðÞ′′,.../C8/C9\n=mi,m′i,m′′i,.../C8/C9\n(6:9)\nIn other kinds of data models, such as the relational or columnar data model, the\nscope of the set is limited, which implies that each one could be as bigger as benecessary, but they have a size at the end. These sets could be processed through\ndifferent operators that could be synthetically indicated as blocking or nonblock-\ning. A nonblocking operation (e.g., a pr ojection in the relation algebra) allows\ncontinuing the data processing without incorporated delay. However, a blockingoperation (e.g., an “order by ”in SQL) does not provide a result till the operation\nreaches the end. In other words, the data processing does not continue till the re-sult of blocking operation is available to be incorporated as an input of the nextoperation inside an execution plan.\nUp to here, it was incorporated the positional exclusive data stream and the tem-\np o r a le x c l u s i v ed a t as t r e a m .I ne a c hk i n da n dd e p e n d i n go nt h ed e f i n e dd o m a i n ,v a r i a -tions related to numerical, ordinal or categorical values could be present.\nIt is known that a way to process a data subset from a data stream is by means\nof windows [44, 45]. The window represents a finite subset of data created by appli-cations of restrictions over an unbounded data stream.6 Modeling the meaning of data streams and its impact on the system performance 101\n\nBecause the positional data streams does not have a concept of time associated,\nwe define the concept of the arriving time as the instant in which data arrives at the\nprocessor unit, without it has a relationship with the time in which the data wasgenerated. The arriving time (i.e., “at”in eq. (6.10)) depends on the time where the\nprocessing unit has its first contact with the datum, and that it is independent of\nthe monitored attribute or the kind of received value:\n∀s2S\np\nex^i2N=t=at s iðÞ ^ t≤RTS (6:10)\nBasically, given a set of positional exclusive streams and “s”a stream belonging to\nit, the arriving time of an “i”element to the processing unit is known using at ( si),\nobtaining a timestamp that will be equal or lesser than a reference timestamp (RTS)\nof the local clock. In this sense, it is worthy to mention that the timestamp is not\nassociated with the data generation time but the arriving time at the processing\nunit.\nThus, a physical window is time-based and its definition for exclusive data\nstream is synthesized in eqs. (6.11) and (6.12):\n∀s2Sp\nex9wT/C26s=wT:RTS−wishedTime ≤at s iðÞ≤RTS (6:11)\n∀s2St\nex9wT/C26s=wT:RTS−wishedTime ≤ti≤RTS (6:12)\nEquation (6.11) establishes that the temporal window “wT”(a subset of “s”) will con-\ntain data values that have arrived at the processing unit between RTS and RTS-\nwishedTime , in where “wishedTime ”is a relative temporal magnitude (e.g., 1 min).\nOn the contrary, eq. (6.12) defines that the window “wT”will contain data values in\nwhich their generation timestamp falls in the interval [CTS- wishedTime ; CTS].\nThe logical window is data volume based, which implies that a certain number\nof elements are retained based on a threshold (e.g., 1000). The number of elements\ninside a window is represented as the cardinality (e.g., | w|). As it is possible to ap-\npreciate in eqs. (6. 13) and (6.14), there is no structural difference in the definition,though the elements in each set are different (positional single values versus tem-poral time-based ordered pairs):\n∀s2S\np\nex9wL/C26s=wL:wLjj≤Threshold (6:13)\n∀s2Stex9wL/C26s=wL:wLjj≤Threshold (6:14)\nAs it was introduced, the physical windows are based on the time, while the logical\nwindows are limited by the data volume. However, it is possible to update the defi-nition of sliding and landmark windows that refer to the way in which each window\nupdates its content. The sliding window keeps the established limits, but it updates the\nextremes for replacing old elements by new elements. In case of physical windows, it102 Mario Diván and María Laura Sánchez-Reynoso\n\nis possible to indicate that the window needs to contain the last 5 min of data.\nUpdating eqs. (6.11) and (6.12) with the current timestamp (CTS), it would be sliding\nw i n d o w sa sd e f i n e di ne q s .( 6 . 1 5 )a n d( 6 . 1 6 ) :\n∀s2Sp\nex9wT/C26s=wT:CTS−wishedTime ≤at s iðÞ≤CTS (6:15)\n∀s2Stex9wT/C26s=wT:CTS−wishedTime ≤ti≤CTS (6:16)\nAs a difference with eqs. (6.11) and (6.12) , the lower and upper endpoints are var-\niable because they are permanently updated with the current timestamp. By def-\ninition, data could be present in the window in a given moment, but it will bediscarded only with the pass of the time. Of course, the retention criteria be-\ntween eqs. (6.15) and (6.16) continue being different, because of the positionaldata stream that employees the data arriving time, while the temporal datastream uses the data generation time.\nI nt h ec a s eo ft h el o g i c a lw i n d o w s ,i ti sp o s s i b l et od e f i n es o m e t h i n gl i k e\n“LAST 1000. ”Thus, the window size follows being the same, but with each new\narrival, new data is added, and the oldest data is discarded in order to keep up-dated its contents.\nAlternatively, it is possible to define landmark windows like those in which one\npoint (i.e., initial or final) that defines the window is updated against the occur-rence of an event, being the data volume related to its content something variable.\nFor example, the event could be the accident in a factory (i.e., milestone), each\ntime that an accident happens, the window content could be restarted as it is de-scribed in eqs. (6.17) and (6.18):\n∀s2S\np\nex9wT/C26s=wT:milestone ≤at s iðÞ≤CTS (6:17)\n∀s2St\nex9wT/C26s=wT:milestone ≤ti≤CTS (6:18)\nFigure 6.4 graphically describes the main perspectives related to exclusive data\nstreams introduced previously through eqs. (6.1) to (6.18). The positional data\nstreams correspond with single values that are organized based on their arrival, ob-\ntaining eventually the notion of time from the arriving time given by its processingunit. That is to say, the timestamp in each positional data stream corresponds withthe instant in which the data processing unit has read the data. The temporal data\nstreams, on the other hand, could be viewed as an ordered pair in which each pair\nis composed of the measure or value related to a concept jointly with the timestampin which the value or measure has been taken from the data source. This is a key\ndifference in terms of data traceability because the temporal data stream can estab-\nlish a relationship between the data and the moment in which it was generated, onthe contrary, the positional data stream has an artificial timestamp derived from its\nprocessing order.6 Modeling the meaning of data streams and its impact on the system performance 103\n\nThe notion of the subset of the data streams is modeled through the concept of\nwindows. They could be physical or logical, updating their content in a sliding or\nlandmark way. Anyway, data could belong to a given window during a certain pe-riod; however, the data will soon be discarded to be replaced by new data. This is\npart of the essence of the data streams, to keep the content as updated as possible\nin order to process or analyze the current or last known data for making decisions.\nThe data joining or matching operations could be carried out by mean of the data\nvalue, its position, or the given timestamps. Even, it is possible to process temporaland positional data streams together, only how the data will be crossed needs to beestablished. As a result of the chosen operation, a new data stream is created; how-\never, the result may not necessarily be strictly temporal. That is to say, if the process-\ning unit makes the matching of the data streams described in Figure 6.4 based on itsposition, the first result would be (36.0; (36.0; t\no)). The value of “to”indicates the gen-\neration time of the last one 36 but not of the first one transitively. The timestamp isexclusive of the associated datum and it is not transitive in relation to other data. Inthis way, the new data stream in this example will contain part of their data with thedata generation timestamp coming from the data source, being able to incorporate the\narriving time to the processing unit for the positional data (i.e., ((36.0; at\n0);(36.0; t0))).\n6.6.2 The cooperative data streams\nThe underlying idea associated with the cooperative word is to use a common car-rier to transport different concepts together, optimizing the use of resources and de-\ncreasing the idle time. The ideal situation for a data channel is to keep it as near aspossible to 100 percent of capacity but avoiding the overflows. The overflow repre-\nsents a situation in which the data production rate exceeds the channel capacity for\ndata transmission, provoking the loosing of data in the data source.\nPositional\ndata streamWL (last 100)\nOldest\ndataCurrent\ntimePositionWT (last minute)\natn−3\nn−3 n−2 n−1 natn−2 atn−1 atn36.0 36.1 36.2 ... Per coming...\n... Per coming...36.0\n(36.0;\ntn−3)(36.1;\ntn−2)(36.0;\ntn−1)(36.0;\ntn)Temporal\ndata stream\nDiscarded\nFigure 6.4: Perspectives from the exclusive data streams.104 Mario Diván and María Laura Sánchez-Reynoso\n\nAcooperative data stream is defined as an unbounded sequence of composed\ndata with a given immutable and well-known data structure associated with one or\nmore autonomous, simple, and independent data sources gathered under the conceptof a collector.\nT h ef i g u r eo ft h e collector represents a component in which a set of data sour-\nces are fully communicated with a processing endpoint, using it as an intermedi-ary. Data sources provide continuously values to the collector, while the collectordefines the data transmission policy articulating the requirements of all the sour-\nces jointly. In other words, between data source and collector there are two roles\ndifferentiated. On the one hand, the data so urce is responsible for providing data\n(e.g., a measure, picture, and audio). On the other hand, the collector is responsi-\nble for locally storing the received data, f u s i n gi ti nt e r m so ft h et a r g e td a t af o r m a t\nkeeping immutable its meaning, providing transport services to the processing\nendpoint, and provide approximate answers to users based on the local data (e.g.,\nan edge device):\n∀a2A,j2N=~a=a\n1,a2,...,aj/C0/C1\n(6:19)\nEquation (6.19) describes the data structure (i.e., DS) to be monitored through a co-\noperative data stream. Up to here, nothing has been said about the timestamp, the\nequation earlier is limited to describe the set of attributes to be informed, their\norder, and from the attribute definition, their value domains are known. Once thedata structure has been defined, the data stream will not change it, otherwise the\ndata stream definition would be changing:\n~M=f9~m\nij~mi=ma1,ma2,...,maj/C0/C1\n=f~aðÞ ...,∀a/C26~a^i,j2Ng (6:20)\nEquation (6.20) represents the set of valued vectors (i.e., ~m) from the set of defined\nattributes (i.e., ~a), that integrates the set of all the valued vectors known as ~M.\nForm eqs. (6.19), (6.20) and (6.3), it is possible to derive the definition of a coop-\nerative positional data stream as an unbounded sequence of valued vectors from a\nset of attributes (i.e., ~a), ordered based on its arriving:\n∀a/C26~a,i2N=Sp\nco=f~aðÞi,f~aðÞi+1,f~aðÞi+2,.../C8/C9\n=~mi,~mi+1,~mi+2,... fg (6:21)\nEquation (6.21) describes the formal definition for the cooperative positional data\nstream. In such sense, each “~a”could be considered as a tuple in analogy to the\nrelational model. In this way, the cooperative positional data stream could be de-scribed as an unbounded sequence of tuples ordered in accordance with its arrival.\nIt is worthy to mention that there is no warranty that each element at the vector\nwill be always valued, that is to say, it is possible that some attribute “a”in a given\ntime, does not have a value. This will imply that some attributes in the vector havemissing values. However, as it was indicated in eq. (6.20), the vector must exist,6 Modeling the meaning of data streams and its impact on the system performance 105\n\nwhich implies that at least one value “mai”must be available. In effect, a vector\nwith all its attributes without value does not represent any received data, and it\nwould consume resources without apparent necessity:\n∀a/C26~a,t2T,i2N=St\nco=f~aðÞi,ti/C0/C1\n,f~aðÞi+1,ti+1/C0/C1\n,f~aðÞi+2,ti+2/C0/C1\n,.../C8/C9\n= ~mi,ti ðÞ ,~mi+1,ti+1 ðÞ ,~mi+2,ti+2 ðÞ ,... fg(6:22)\nAtemporal cooperative data stream could be defined as an unbounded sequence of\nimmutable vectors of attributes simultaneously valued and ordered based on its col-\nlecting timestamp. In this sense, the data structure related to each vector is fixed,and it cannot be changed over time. As it was said before, each vector at least must\ncontain a value for an attribute, even when the rest of the attributes that compose\nthe vector “~a”have no value. However, eq. (6.22) defines the temporal cooperative\ndata stream with a strong assumption that all the values of attributes have been\nprovided at the same instant. This is important to highlight because the cooperative\nstreams are provided through collectors, and each collector is linked a set of sen-sors. In this way, the collector must warranty that the informed values through thiskind of data stream correspond to the same timestamp.\nBecause each attribute “a”could be quantitative, ordinal, or categorical, each\nvector could be integrated of a combination of such kinds of attributes, for such rea-son, these streams are known as mix temporal cooperative data streams , or simply\ntemporal cooperative data streams because of the common situation. In case of allattributes of a vector “~a”be of the same kind, the data streams could be indicated as:\n(1)Categorical temporal cooperative data stream: when all the attributes that com-\npose it are nominal or categorical; (2) Ordinal Temporal Cooperative data stream:\nwhen all the attributes that compose it are ordinal; and (3) Quantitative Temporal\nCooperative data stream: when all the attributes that compose it are quantitative.\nThe concept of the physical window could be extended based on eqs. (6.11) and\n(6.12) as shown in eqs. (6.23) and (6.24). It is possible to appreciate that the mainchange is associated with the structure of the data stream and not about the\ntimestamp:\n∀s2S\np\nco9wT/C26s=wT:RTS−wishedTime ≤at s iðÞ≤RTS (6:23)\n∀s2St\nco9wT/C26s=wT:RTS−wishedTime ≤ti≤RTS (6:24)\nIn the same way for the logical windows , they could be extended based on eqs. (6.13)\nand (6.14) as shown in eqs. (6.25) and (6.26):\n∀s2Sp\nco9wL/C26s=wL:wLjj≤Threshold (6:25)\n∀s2Stco9wL/C26s=wL:wLjj≤Threshold (6:26)106 Mario Diván and María Laura Sánchez-Reynoso\n\nIt is important to highlight that the cardinality given in eqs. (6.25) and (6.26) corre-\nsponds with the number of contained vectors, independently the number of attrib-\nutes in each vector.\nSomething similar happens in relation to the sliding and landmark windows.\nBased on eqs. (6.15) to (6.18), sliding windows are specified by eqs. (6.27) and (6.28)\nfor positional and temporal cooperative data streams respectively. Thus, landmark\nwindows are described by eqs. (6.29) and (6.30) accordingly:\n∀s2Sp\nco9wT/C26s=wT:CTS−wishedTime ≤at s iðÞ≤CTS (6:27)\n∀s2Stco9wT/C26s=wT:CTS−wishedTime ≤ti≤CTS (6:28)\n∀s2Spco9wT/C26s=wT:milestone ≤at s iðÞ≤CTS (6:29)\n∀s2Stco9wT/C26s=wT:milestone ≤ti≤CTS (6:30)\nThe data joining or matching operations could be carried out by the mean of the data\nvalue, its position, or the given timestamps contained in an attribute. For example,algorithms such as the Symmetric Hash Join could be employed [46]. As a differencewith the previous exclusive data streams, the cooperative data streams contain a set\nof attributes between which it is possible to choose one or more, through which the\njoint operation could be carried out. Thus, each cooperative stream (be it positionalor temporal) could be analyzed as an unbounded sequence of tuples, simulating\neach data stream as an infinite bidimensional data organization (e.g., a table).\nHowever, it is important to mention that the matching done is not dependent of\nthe relational model; it only represents an interpretation by an analogy.\nThe matching or joining operations are not exclusive of the cooperative data\nstreams, which implies that a joining operation between a positional exclusive data\nstream and a temporal cooperative data stream could be carried out in the measure\nthat the crossing criteria are defined. Once defined, the result will be a cooperative\ndata stream depending on the number of projected attributes.\nIn this way, the operations defined over the data streams allow giving as result\nnew data streams derived from those employed as the origins, following some pre-defined criteria or restrictions. In other words, the cooperative and exclusive datastreams could be interpreted from the bidimensional point of view and the previousworks on bidimensional data models could be reused (e.g., bitemporal model, rela-\ntional model, columnar model) [47 –50].\nThe contrast between Figures 6.5 and 6.4 allows visually to see the differences\nbetween cooperative and exclusive data streams. As illustrated in Figure 6.5, eachdata item in a cooperative data stream has a structure determined by an ordered\nsequence of attributes (i.e., a\ni). Such an ordered sequence could be known as a vec-\ntor or tuple in an analogy with math or relational model respectively. But in the\ntemporal cooperative data streams, all the attribute values simultaneously depend\non the same timestamp ( tn–1in Figure 6.5). This is an important difference of the6 Modeling the meaning of data streams and its impact on the system performance 107\n\nexclusive data stream because the last one has a timestamp specifically associated\nwith its value, while the cooperative data stream is shared along with all the ele-\nments that compose the vector ~a.\nPositional and temporal cooperative data streams have time differences in,\nwhile the first one imposes its time by the arrival time, the second one brings the\ntimestamp from the data source as part of their data. In both cases, always therewill be new data arriving for replacing the oldest data, being the last ones discardedby obsolescence.\nThe concept of logical and temporal windows has a behavior similar to the ex-\nclusive data streams, but the structure related to each data item is subtly differentas it can be appreciated in Figure 6.5. Also, the landmark and sliding windows\ncould be extended for being used with cooperative data streams.\nIn this scenario, one could think that there is not space for semistructured data or\nunstructured data that would be a mistake. Why? Because from eqs. (6.2) and (6.20),\nthe set Mis defined as a valuation of certain attribute “a”giving a result known as\n“m,”which it would be informed into a data stream. It is not necessary that “m”must\nPositional\ndata\nstreamPositional\nData Stream\nTemporal\ndata\nstreamWL (last 100)\nPositionWT (last minute)\na1: 36.0\nan‒3 an‒2 an‒1ana2: Good\na3: red\nan: 1.87\n(tn‒3)( tn‒2)( tn‒1)...\na1: 36.0\n36.1 Good green 1.86 ...a2: Good\na3: red\nan: 1.87...a1: 36.2\na2: Bad\na3: blue\nan: 1.84...a1: 36.1\na2: Good\na3: green\nan: 1.86...\na1: 36.2\na2: Bad\na3: blue\nan: 1.84...a1: 36.1\na2: Good\na3: green\nan 1.86...\n(tn)a1: 36.1\na2: Good\na3: green\nan: 1.86...a1: 36.3\na2: Bad\na3: red\nan: 1.85...... Per coming...\n... Per coming...\nDiscarded\nOldest\ndataCurrent\ntimen−3\na−1a−2 a−3(an−1, tn−1)\ntn−1aann−2 n−1 n\nFigure 6.5: Perspectives from the cooperative data streams.108 Mario Diván and María Laura Sánchez-Reynoso\n\nbe an atomic value in terms of the relational model. In other words, “m”represents the\nbucket in which data arrives to be processed and the structure of such data will be in\nterms of its value domain. Thus, “m”could be an XML file generated from a given data\nsource or data collector, which needs to communicate it to the data processing unit.Thus, it is possible to see the exclusive data streams as a sequence of buckets (be it\ntemporal or not), while the cooperat i v ed a t as t r e a m sa sas e q u e n c eo f “rows” com-\np o s e do fb u c k e t si nw h i c hi ne a c hp o s i t i o nc o u l dh a v ed i f f e r e n td o m a i n s .\nAs it is possible to appreciate in Figure 6.6, the main distinctive characteristics\nthat allow adequate discriminating between data streams could be synthesized\naround the timestamp, timestamp origin, data structure, number of associated at-tributes, data order, use of intermediaries, and the supporting of windows for its\ndata processing.\n6.7 Operating over exclusive and cooperative\ndata streams\nWith the aim of analyzing the exclusive and cooperative data streams at the light of\nbasic operations, they are organized from two points of view. On the one hand, op-erations related to the set theory, that is to say, the union, intersection, difference,\nand cartesian product. On the other hand, operations associated with the projec-\ntion, restriction, joining, and division.\nFor better organization, this section introduces a notation to be used for ex-\npressing the mentioned operations. Later, each operation will be individually intro-duced jointly with its exemplification.\n6.7.1 Notation\nFigure 6.7 synthesizes the proposed idea in relation to the notation for exclusiveand temporal data streams. The differentiation between exclusive and cooperativeis derived from the number of attributes that compose it. That is to say, when thenumber of attributes between brackets is upper or equal to two, the data stream is\ncooperative, while when the number of attributes specified between brackets is just\none, the data stream is exclusive.\nAs it is possible to appreciate in the mentioned figure, the positional and tempo-\nral aspects are separated from the list of attributes. This is due to both aspects haveno dependency on the attributes or their contents, they are independent of their val-ues. For example, if it is necessary to reach values of certain attributes between posi-\ntions 2 and 5, that kind of restriction will have no relationship with the contained\nvalues in those attributes but with the indicated position.6 Modeling the meaning of data streams and its impact on the system performance 109\n\nTimestamp ArtificialPositional Temporal Positional TemporalExclusiveData streams\nCooperative\nArtificial - shared Intrinsic - shared\nCollectorIntrinsic\nData source\nOne + timestamp “n” + timestamp\nShared timestamp Timestamp\nNo(a, ti) (a, ti)Arrival time\nArrival position\nArrival positiononeaArrival time\nArrival position\nYes (collector) Yes (collector)aTimestamp origin\nStrtucture\nNumber of attributes\nOrder based on the\nIntermediary\nLogical windows\nPhysical windows\nSliding windows\nLandmark windows“n”\nFigure 6.6: Comparative synthesis of the main characteristics of the kinds of data streams.110 Mario Diván and María Laura Sánchez-Reynoso\n\nFor a better description of this proposed notation, Table 6.4 describes some ex-\namples jointly with its description.\nAn alias establishes a synonym that allows to manage different expressions in\na friendly way. The way in which an alias is defined is similar to the Structured\nQuery Language (SQL), employing the reserved word “as”. For example, it would\nbe a good idea to define an alias for obtaining the last ten elements from the posi-\ntional data stream as it is shown in eq. (6.31):\nmyPosp*½/C138.p between last ðÞ−10and last ðÞAS last 10 (6 :31)\nThe “*”indicates all the attributes defined in the data stream. Thus, the last ten\ndata items arriving through the myPospdata stream will be informed with all the\nattributes under the synonym named “last10” . When the expression last10[colour]\nis present, it really represents the last ten colors arriving by the data streammyPos\np. An alias could be part of a new alias definition, for example, last10[col-\nours] as myLast10Colours . The underlying idea is to simplify the involved expres-\nsions related to the data communication.\n6.7.2 Operations\nThe projection operation consists in extracting a subset of attributes defined on a\ndata stream and exporting them as a new data stream:\nnewDS =measurestdataSource, value ,collector ½/C138 (6:32)\nEquation (6.32) creates a new temporal data stream (i.e., the origin is completely\ntemporal) named newDS that is composed of three attributes known as dataSource,\nvalue , and collector. Even, when it is possible that the data source named “meas-\nures ”has more than three attributes, only the indicated attributes between square\nbrackets will be projected:\nmeasurest*½/C138 ðweight <19and colour =blueÞ (6:33)\nAttributes that compose it Restrictioins applied to attributesLogical Operators: AND, OR, ...Positional\nTemporal\n(p|t)\nmyStream [a1. a2, ...,an] [r1 <logical op> r2 <logical op> rn]\nUnique Name\nFigure 6.7: Notation for temporal and positional data streams.6 Modeling the meaning of data streams and its impact on the system performance 111\n\nTable 6.4: Synthesis comparative between a centralized and distributed data collecting architectures.\nNotation Description\nmyStreamtcolour ,height ½/C138 A temporal data stream named “stream, ”which contains\ntwo attributes defined as color and height.\nmyStreamt.t between now ðÞ−5secs and now ðÞ The temporal stream restricts the contained data items to\nthe last five seconds, independently the values containedin the attributes color and height.\nmyStream\ntcolour½/C138 ð colour =blueORheightbetween5and10 Þ The temporal stream restricts the contained data items tothose that contain the value “blue ”for the attribute color,\nor alternatively those in which the height is between and\n. As a result of restrictions, the attribute “colour” is\ninformed.\nmyStream\ntcolour½/C138 ð colour =blueORheightbetween5and10 Þ.tbetween now ðÞ−5secs and now ðÞSimilar to the previous one but limiting the restrictions tothe data items contained in the last s.\nmyPos\npcolour ,weight ½/C138 A positional data stream named “myPos, ”which contains\ntwo attributes defines as color and weight.\nmyPosp.p beetween 5and 8 A positional data stream that continuously informs the dataitems located between positions andincluded.\nmyPos\npcolour½/C138 weight between 5and 7 ðÞ .p\nbetween last ðÞ−5and last ðÞIt informs the last positions for the attribute “colour” but\nonly for those in which weight is between and.112 Mario Diván and María Laura Sánchez-Reynoso\n\nTherestriction operation limits the number of data items in the data stream, follow-\ning a logical expression that will return TRUE or FALSE. When the expression re-\nturns TRUE, the data item is retained and informed in the result, otherwise, it is notconsidered. Figure 6.33 shows an example in which all the attributes related to thedata stream named “measures ”are projected but only to those data items that si-\nmultaneously have a weight lower than 19 and their color is blue.\nTraditionally, the cartesian product operation realizes a combination of ele-\nments, combining them all against all. The challenge in this context is that given\ntwo data streams, neither has a limit nor size, and this is the reason for which the\ncombination of all against all is relative because the stream’ s end is never known.\nAn alternative for implementing it is through the windows, in which certain limits\nor sizes to each set could be established:\nmeasures\nta,b,c,e,f,g ½/C138 =measures ata,b,c ½/C138 ∪measures bt½e,f,g/C138/C138 (6:34)\nmeasuresta,b,c,e,f,g ½/C138 =measures ata,b,c ½/C138 ∪measures bp½e,f,g/C138/C138 (6:35)\nTheunion operation generates a new data stream integrated by all the data items and\nattributes coming from the data sources that give origin to it (see eq. (6.34)). The chal-lenge in this kind of operation has a relation to the managing of temporality. That is to\nsay, given two temporal data streams, the u nion will order the new data stream based\non their respective temporal dimension. However, when two structurally different data\nstreams (i.e., one positional and one temporal) need to be united, the new data stream\nwill order data, mixing arriving, and genera tion timestamps. As you can appreciate in\neq. (6.35), once the new data stream has been created, the data items will be consid-\nered as temporal and assuming the arriving timestamp as a generation timestamp.\nThe intersection operation generates a new data stream integrated by those data\nitems that are present in both streams. Analogously to the cartesian product, this opera-tion requires a previous knowledge about t he volume of involved data. This could be\nimplemented using windows based on data stre ams structurally compatible; otherwise,\nthe result would be empty. The underlying idea around the concept “structurally com-\npatible ”is that at least there exists one common attribute between attributes and based\non that, the common values between the common attributes will integrate the result.\nThedifference operation has the same limitations that the intersection and car-\ntesian product operations, they need to have a limited set in a context in whicheach data stream is unbounded. That is to say, given an expression such as A-B the\nidea is that in the result will be all the data belonging to the set “A”not present in\nthe set “B”. The adopted strategy for carrying forward these operations is through\nthe employing of windows:\nmeasures\npa,b,c,f,g ½/C138 =measures apa,b,c ½/C138 natural join measures bpa,f,g ½/C138\n(6:36)6 Modeling the meaning of data streams and its impact on the system performance 113\n\nmeasuresta,b,c,d,f,g ½/C138 =measures ata,b,c ½/C138 as ma inner join measures btd,f,g ½/C138 as mb\non ma a ½/C138=mb d½/C138 ðÞ (6:37)\nThe natural join operation creates a new data stream based on other data streams\nfollowing established criteria. For example, on the one hand, eq. (6.36) creates anew positional data stream composed of the attributes “a,b,c,f, and g,”matching\nthe values for the attributes with the same name (i.e., attribute “a”). On the other\nhand, eq. (6.37) describes a new data stream generated through an inner join,matching the attributes “a”and “d”using aliases:\nmeasures\nta½/C138=measures ata,c½/C138 =measures btc½/C138 (6:38)\nThe division operation is between one cooperative and one exclusive data stream.\nThe binary cooperative data stream has two attributes and it is compared with the\navailable values in the exclusive data stream, each matched value will imply a\nnew data item to be included in the result. The result will be integrated by theattribute of the cooperative data stream not present in the exclusive data stream.Equation (6.38) shows an example dividing a temporal cooperative data stream\nwith a temporal exclusive data stream, the common attribute is “c”reason for\nwhich the result will be integrated of the attribute “a.”Each data item containing\nthe attribute “a”emerges from those data items in which the value for the attri-\nbute “c”is coincident between both data streams. It is worthy to mention that the\nexclusive data stream needs to be implemented using windows (i.e., divisor) be-\ncause a finite size is required in order to contrast values coming from unbounded\ndata streams (i.e., dividend) with its divisor.\nFigure 6.8 represents an example of the division operation between one cooper-\native and one exclusive data stream for eq. (6.38). The number 5.0 in the result ap-pears because the same value contains for the attribute “c”the values “ds1”and\n“ds2”corresponding to the divisor ’s values.\nFigure 6.8: An example of the division operation between the cooperative and exclusive data\nstreams.114 Mario Diván and María Laura Sánchez-Reynoso\n\n6.8 About the overhead related to the processing\nperspective of the exclusive and cooperative\ndata streams\nFor analyzing the overhead related to the processing perspective of the exclusive\nand cooperative data streams, a new library named streamCE was proposed as a\nproof of concept. It was developed in Java and released under the terms of the\nApache 2.0 General Agreement License. The source code is freely available on\nGithub, introducing a basic implementation of exclusive and cooperative datastreams jointly with the implementation of the operations of Union and projection.\n1\nTo analyze the involved times associated with each operation, two simulations\nwere carried out on the same hardware. The host was a MacBook Pro 2017 with MacOS Catalina 10.15.1, Processor Intel I7 with four cores, 16 GB 2133 MHz LPDDR3, and\nRadeon Pro 560 4 GB Graphic Board.\nThe first simulation was carried out on the operation of the union. Two cooper-\native data streams were defined as inputs: streamA and streamB . The streamA had\nthe numerical attributes named “a,”“b,”and “c,”while the stream had the numeri-\ncal attributes named “d,”“e,”“f,”“g,”“h,”and “i.”Each attribute was randomly\nfilled avoiding null values to analyze the full required processing time. The result of\nthe union operation was modeled as a cooperative data stream, taking the time-\nstamp from the most recently updated data stream. In case one of the data streams\nis updated (i.e., streamA orstream B ), the union took the updated value jointly with\nthe last known data item from the second data stream, representing the situation in\nwhich one of them could have been interrupted.\nThe simulation created the data streams, after that the operator was created\nand linked to the data streams acting as inputs (i.e., streamA and streamB).\nOnce the operator and input were established, a set of threads was created for\nacting on the data streams, producing the random values for each attribute. Theprocessing time was analyzed continuously for 10 min. In this sense, the time\nconsumed for the garbage collector had a huge impact on the unitary processing\nrate.\nFigure 6.9 describes the mentioned unitary processing time throughout the\n10 min of continuous processing. The peaks are related to the consumed time of thegarbage collector, which incorporates important jumps in relation to the general\nunitary processing time. On the one hand, the unitary processing rate tends to beunder 0.01 ms, even reaching rates such as 0.004 ms. On the other hand, the uni-\ntary processing rate starts around 0.04, decreasing continuously at the time in\nwhich the necessary resources are adequately allocated in memory.\n1https://github.com/mjdivan/streamCE6 Modeling the meaning of data streams and its impact on the system performance 115\n\n0,180\n0,160\n0,140\n0,120\n0,100\n0,080Union (ms)\nTime (s)0,0040,158\n0,060\n0,040\n0,020\n0 100 200 300 400 500 600–\nFigure 6.9: The unitary processing times of the union operation in the streamCE library.116 Mario Diván and María Laura Sánchez-Reynoso\n\nFigure 6.9 expresses the elapsed time of the simulation on the axis of the ab-\nscissas in seconds, while the axis of the ordinate represents the unitary processing\ntime for the union operation expressed in milliseconds. The peaks observed in thef i g u r ea r er e l a t e dt ot h ea d d i t i o n a lc o n s u m e dt i m eo ft h eg a r b a g ec o l l e c t o r .However, it is possible to appreciate through the concentration of points the gen-\neral behavior and its tendency to progressively decrease the unitary rate up to\nreach values around 0.004 ms per operation.\nSimilarly, the projection operation was simulated throughout 10 min using a\ndata stream named streamA that contained the nu meric attributes “a,”“b,”“c,”\n“d,”“e,”“f,”“g,”“h,”and “i.”The result consisted of a new cooperative data\nstream with the following attributes ordered as follows: “c,”“e,”“i,”and “h.”\nFigure 6.10 represents the elapsed time of the simulation on the axis of the\nabscissas in seconds, while the axis of the ordinate represents the unitary process-ing time for the projection operation expressed in milliseconds. The peaks ob-\nserved in the figure are related to the additional consumed time of the garbage\ncollector.\nThe individual processing rates started around 0.04 ms, progre ssively decreasing\nat the time in which all the necessary resources were allocated in memory. Near to\nthe 10 min, the unitary processing rate reached values around 0.003 ms per\noperation.\n100 ––0,0200,0400,0600,080Projection (ms)0,1000,1200,1400,1600,180\n200 300 400 500 600 Time (s)0,0030,165\nFigure 6.10: The unitary processing times of the projection operation in the streamCE library.6 Modeling the meaning of data streams and its impact on the system performance 117\n\nThe library is open to anyone who wants to use or study it. This could be ex-\ntended incorporating new operations, specializing data streams based on different\nkinds of requirements, among other aspects. The idea is to provide a conceptual im-plementation of the perspective given for the cooperative and exclusive datastreams in order to materialize the concepts. The obtained times are limited to the\nsimulation and they are useful for describing a conceptual idea but not for any kind\nof statistical generalization.\n6.9 Conclusions\nIn this chapter a discussion around the concept of data streams, the interpretation\nof its meaning in different contexts, and the representation of its content was out-\nlined. It was possible to appreciate the absence of a consensus about the idea of thecontent; some authors refer the data streams such as an unbounded sequence of\ntuples, while other authors understood the data stream as an unbounded sequence\nof data, independently its internal structure. This amplitude of interpretations gen-erates a gray zone in which it is a challenge to provide a complete definition of theconcept itself.\nThe current approaches related to different ways to interpret the content were\nintroduced, while the SMS was performed forward to analyze the tendencies in thissubject. SMS was driven on the Scopus database for works published from 2016 up\nto now.\nThe data stream environment is mainly focused on to measure and evaluates\ndifferent entities, events, or aspects that are necessary for making decisions. Inthis sense, data-driven decision-making has emerged as a real alternative for sup-\nporting the decision-making processes in all kinds of habitats in which a decision\nneeds to be taken. For that reason, Section 6.4 introduced the importance of the\nmeasurement framework along with the measurement process and the data-\ndriven decision-making.\nAdvancing on the importance of different perspectives to implement active\nmonitoring, the centered and distributed processing strategies were introduced.They were synthetically described, schematized and compared, highlighting the en-\nvironments in which each one could be applied. The impact of current technologiesrelated to fog computing, the underlying idea associated with its data processing\nand closeness to users to provide answers were analyzed.\nIn this way, the definition and description of the exclusive and cooperative\ndata streams were incorporated jointly with its formal definition. They were com-pared, describing the effects and limitations of using each one. This allows to\nmodel the content of each data stream, describing the relationship between the118 Mario Diván and María Laura Sánchez-Reynoso\n\ntimestamp and the rest of the contained data, how the data structure could be de-\nfined and the restrictions about each associated values’ domain.\nAlso, a notation for managing cooperative and exclusive data streams were pro-\nposed jointly with how the operations could be defined. The operations naturally\nprovide how the data streams could be processed and transformed from one type to\nanother. For example, the union could receive one positional data stream jointly\nwith a temporal data stream, giving a temporal data stream as a result. The operatorallows extending the way in which each data stream could be transformed follow-\ning a given logic (e.g., union and projection).\nThe streamCE library was released under the terms of the Apache 2 General\nAgreement License, implementing the concepts here described as a proof of con-cept. In this way, anyone could use or extend its content, contrasting the ideas here\ndescribed and even extending them to other data stream platforms.\nFinally, a simulation on the projectio na n du n i o no p e r a t i o n sw a sc a r r i e do u t\non common hardware, using cooperativ e data streams and processing it over\n10 min. The unitary processing rates were decreasing in each case at the time in\nwhich the necessary processing resources were allocated in memory, reachingunitary rates around 0.003 and 0.004 ms per operation for the projection and\nunion, respectively.\nAs future work, application scenarios for cooperative and exclusive data streams\nwill be analyzed, fostering its implementation on other data stream platforms (e.g.,\nApache Storm or Apache Spark).\nBibliography\n[1] Von Bertalanffy, L. (1972). The History and Status of General Systems Theory, Academy of\nManagement Journal, 15(4), 407 –426. https://doi.org/10.5465/255139.\n[2] Bassett, D.S., Wymbs, N.F., Porter, M.A., Mucha, P.J., Carlson, J.M. and Grafton, S.T. (2011).\nDynamic reconfiguration of human brain networks during learning, Proceedings of the\nNational Academy of Sciences, 108(18), 7641 –7646. https://doi.org/10.1073/\npnas.1018985108.\n[3] Almeida, P. and Chase-Dunn, C. (2018). Globalization and Social Movements, Annual Review\nof Sociology, 44(1), 189 –211. https://doi.org/10.1146/annurev-soc-073117-041307.\n[4] Dolan, J.G. and Fraenkel, L. (2017). Shared Decision-Making, Multi-Criteria Decision Analysis\nto Support Healthcare Decisions, Cham, Springer International Publishing, 199 –215.\nhttps://doi.org/10.1007/978-3-319-47540-0_11.\n[5] Schwarting, W., Alonso-Mora, J. and Rus, D. (2018). Planning and Decision-Making for\nAutonomous Vehicles, Annual Review of Control, Robotics, and Autonomous Systems, 1(1),187–210. https://doi.org/10.1146/annurev-control-060117-105157.\n[6] Ramírez-Gallego, S., Krawczyk, B., García, S., Wo źniak, M. and Herrera, F. (2017). A survey on\ndata preprocessing for data stream mining: Current status and future directions,Neurocomputing, 239, 39– 57. https://doi.org/10.1016/j.neucom.2017.01.078.6 Modeling the meaning of data streams and its impact on the system performance 119\n\n[7] Safaei, A.A. (2017). Real-time processing of streaming big data, Real-Time Systems, 53(1),\n1–44. https://doi.org/10.1007/s11241-016-9257-0.\n[8] Khan, M.A. and Salah, K. (2018). IoT security: Review, blockchain solutions, and open\nchallenges, Future Generation Computer Systems, 82, 395 –411. https://doi.org/10.1016/j.\nfuture.2017.11.022.\n[9] Ammar, M., Russello, G. and Crispo, B. (2018). Internet of Things: A survey on the security of\nIoT frameworks, Journal of Information Security and Applications. https://doi.org/10.1016/j.\njisa.2017.11.002.\n[10] Ni, J., Zhang, K., Lin, X. and Shen, X.S. (2018). Securing Fog Computing for Internet of Things\nApplications: Challenges and Solutions, IEEE Communications Surveys & Tutorials, 20(1),601–628. https://doi.org/10.1109/COMST.2017.2762345.\n[11] Ge, M., Bangui, H. and Buhnova, B. (2018). Big Data for Internet of Things: A Survey, Future\nGeneration Computer Systems. https://doi.org/10.1016/j.future.2018.04.053.\n[12] Reyna, A., Martín, C., Chen, J., Soler, E. and Díaz, M. (2018). On blockchain and its integration\nwith IoT. Challenges and opportunities, Future Generation Computer Systems.https://doi.org/10.1016/j.future.2018.05.046.\n[13] Jang, J., Jung, I. and Park, J.H. (2018). An effective handling of secure data stream in IoT,\nApplied Software Computer, 68, 811 –820. https://doi.org/10.1016/j.asoc.2017.05.020.\n[14] Nakazawa, J., Tokuda, H. and Yonezawa, T. (2015). Sensorizer: An Architecture for\nRegenerating Cyber Physical Data Streams from the Web. In Adjunct Proceedings of the 2015ACM International Joint Conference on Pervasive and Ubiquitous Computing and Proceedingsof the 2015 ACM International Symposium on Wearable Computers (pp. 1599 –1606).\nNew York, NY, USA: ACM. https://doi.org/10.1145/2800835.2801627\n[15] Chen, G.J., Wiener, J.L., Iyer, S., Jaiswal, A., Lei, R., Simha, N. . . . Yilmaz, S. (2016).\nRealtime Data Processing at Facebook. In Proceedings of the 2016 InternationalConference on Management of Data (pp. 1087 –1098). New York, NY, USA: ACM.\nhttps://doi.org/10.1145/2882903.2904441\n[16] Rivetti, N., Busnel, Y. and Querzoni, L. (2016). Load-aware Shedding in Stream\nProcessing Systems. In Proceedings of the 10th ACM International Conference onDistributed and Event-based Systems (pp. 61 –68). New York, NY, USA: ACM.\nhttps://doi.org/10.1145/2933267.2933311\n[17] Gulisano, V., Nikolakopoulos, Y., Cederman, D., Papatriantafilou, M. and Tsigas, P. (2017).\nEfficient Data Streaming Multiway Aggregation Through Concurrent Algorithmic Designs andNew Abstract Data Types, ACM Transactions Parallel Computer, 4(2), 11, 1 –11. 28.\nhttps://doi.org/10.1145/3131272.\n[18] Chothia, Z., Liagouris, J., Dimitrova, D. and Roscoe, T. (2017). Online Reconstruction of\nStructural Information from Datacenter Logs. In Proceedings of the Twelfth EuropeanConference on Computer Systems (pp. 344 –358). New York, NY, USA: ACM.\nhttps://doi.org/10.1145/3064176.3064195\n[19] Brandt, T. and Grawunder, M. (2018). GeoStreams: A Survey, ACM Computer Survey, 51(3),\n44, 1 –44. 37. https://doi.org/10.1145/3177848.\n[20] Ivanov, T. and Taaffe, J. (2018). Exploratory Analysis of Spark Structured Streaming. In\nCompanion of the 2018 ACM/SPEC International Conference on Performance Engineering\n(pp. 141 –146). New York, NY, USA: ACM. https://doi.org/10.1145/3185768.3186360\n[21] Ganibardi, A. and Ali, C.A. (2018). Weblog Data Structuration: A Stream-centric Approach for\nImproving Session Reconstruction Quality. In Proceedings of the 20th International\nConference on Information Integration and Web-based Applications & Services (pp. 263 –271).\nNew York, NY, USA: ACM. https://doi.org/10.1145/3282373.3282379120 Mario Diván and María Laura Sánchez-Reynoso\n\n[22] Chandy, K.M. and Bunn, J. (2019). Compositional Structures for Streaming Applications. In\nProceedings of the 20th International Conference on Distributed Computing and Networking\n(pp. 352 –361). New York, NY, USA: ACM. https://doi.org/10.1145/3288599.3288642\n[23] Géraud, R., Lombard-Platet, M. and Naccache, D. (2019). Quotient Hash Tables:\nEfficiently Detecting Duplicates in Stre aming Data. In Proceedings of the 34th ACM/\nSIGAPP Symposium on Applied Computing (pp. 582 –589). New York, NY, USA: ACM.\nhttps://doi.org/10.1145/3297280.3297335\n[24] Kitchenham, B.A., Budgen, D. and Pearl Brereton, O. (2011). Using mapping studies as the\nbasis for further research –A participant-observer case study, Information and Software\nTechnology, 53(6), 638– 651. https://doi.org/10.1016/j.infsof.2010.12.011.\n[25] Petersen, K., Vakkalanka, S. and Kuzniarz, L. (2015). Guidelines for conducting systematic\nmapping studies in software engineering: An update, Information and Software Technology,64, 1 –18. https://doi.org/10.1016/j.infsof.2015.03.007.\n[26] Lughofer, E., Pratama, M. and Skrjanc, I. (2018). Incremental rule splitting in generalized\nevolving fuzzy systems for autonomous drift compensation, IEEE Transactions on FuzzySystems, 26(4), 1854 –1865. https://doi.org/10.1109/TFUZZ.2017.2753727.\n[27] Lughofer, E. (2017). On-line active learning: A new paradigm to improve practical useability of\ndata stream modeling methods, Information Sciences, 415 –416, 356– 376.\nhttps://doi.org/10.1016/j.ins.2017.06.038.\n[28] Georis-Creuseveau, J., Claramunt, C. and Gourmelon, F. (2017). A modelling framework for the\nstudy of Spatial Data Infrastructures applied to coastal management and planning,International Journal of Geographical Information Science, 31(1), 122 –138.\nhttps://doi.org/10.1080/13658816.2016.1188929.\n[29] Kamburugamuve, S., Wickramasinghe, P., Ekanayake, S. and Fox, G.C. (2018). Anatomy of\nmachine learning algorithm implementations in MPI, Spark, and Flink, International Journalof High Performance Computing Applications, 32(1), 61 –73. https://doi.org/10.1177/\n1094342017712976.\n[30] Koek, P., Geuns, S.J., Hausmans, J.P.H.M., Corporaal, H. and Bekooij, M.J.G. (2016). CSDF\na:\nA model for exploiting the trade-off between data and pipeline parallelism. In Proceedings ofthe 19th International Workshop on Software and Compilers for Embedded Systems, SCOPES2016 (pp. 30 –39). https://doi.org/10.1145/2906363.2906364\n[31] Dubrulle, P., Gaston, C., Kosmatov, N., Lapitre, A. and Louise, S. (2019). A data flow model\nwith frequency arithmetic. Lecture Notes in Computer Science (including subseries LectureNotes in Artificial Intelligence and Lecture Notes in Bioinformatics), Vol. 11424 LNCS.https://doi.org/10.1007/978-3-030-16722-6_22.\n[32] Meinig, M., Tröger, P. and Meinel, C. (2019). Finding classification zone violations with\nanonymized message flow analysis. In ICISSP 2019 –Proceedings of the 5th International\nConference on Information Systems Security and Privacy (pp. 284 –292).\n[33] Masulli, F. and Rovetta, S. (2019). The Challenges of Big Data and the Contribution of Fuzzy\nLogic. Lecture Notes in Computer Science (including subseries Lecture Notes in ArtificialIntelligence and Lecture Notes in Bioinformatics), Vol. 11291 LNAI. https://doi.org/10.1007/978-3-030-12544-8_25.\n[34] Abdullatif, A., Masulli, F. and Rovetta, S. (2018). Clustering of nonstationary data streams:\nA survey of fuzzy partitional methods, Wiley Interdisciplinary Reviews: Data Mining andKnowledge Discovery, 8(4). https://doi.org/10.1002/widm.1258.\n[35] Abdullatif, A., Masulli, F. and Rovetta, S. (2017). Tracking Time Evolving Data Streams for\nShort-Term Traffic Forecasting, Data Science and Engineering, 2(3), 210– 223.\nhttps://doi.org/10.1007/s41019-017-0048-y.6 Modeling the meaning of data streams and its impact on the system performance 121\n\n[36] Chadli, N., Kabbaj, M.I. and Bakkoury, Z. (2018). Detection of dataflow anomalies in business\nprocess an overview of modeling approaches. In ACM International Conference Proceeding\nSeries. https://doi.org/10.1145/3289402.3289537\n[37] Mackie, I. (2017). A geometry of interaction machine for gödel ’s system t. Lecture Notes in\nComputer Science (including subseries Lecture Notes in Artificial Intelligence and LectureNotes in Bioinformatics), Vol. 10388 LNCS. https://doi.org/10.1007/978-3-662-55386-2_16.\n[38] Godin, B. (2002). Outline for a history of science measurement, Science Technology and\nHuman Values. https://doi.org/10.1177/016224390202700101.\n[39] Tramontini, C.C. and Graziano, K.U. (2007). Hypothermia control in elderly surgical patients\nin the intraoperative period: evaluation of two nursing interventions, Revista Latino-Americana de Enfermagem, 15(4), 626 –631. https://doi.org/10.1590/S0104-\n11692007000400016.\n[40] Witkowski, K. (2017). Internet of Things, Big Data, Industry 4.0 –Innovative Solutions in\nLogistics and Supply Chains Management, Procedia Engineering, 182, 763 –769.\nhttps://doi.org/10.1016/j.proeng.2017.03.197.\n[41] Diván, M.J. (2017). Real-Time Measurement and Evaluation as System Reliability Driver,\nSystem Reliability Management, Vol. 4, Boca Raton, Taylor & Francis, a CRC title, part of theTaylor &: CRC Press, 161– 188. https://doi.org/10.1201/9781351117661-11.\n[42] Pan, T. and Zhu, Y. (2018). Getting Started with Arduino, Designing Embedded Systems with\nArduino, Singapore, Springer Singapore, 3 –16. https://doi.org/10.1007/978-981-10-4418-2_1.\n[43] Glória, A., Cercas, F. and Souto, N. (2017). Design and implementation of an IoT gateway to\ncreate smart environments, Procedia Computer Science, 109, 568 –575. https://doi.org/\n10.1016/j.procs.2017.05.343.\n[44] Chakravarthy, S. and Jiang, Q. (2009). Stream Data Processing: A Quality of Service\nPerspective. Processing, Vol. 36, Boston, MA, Springer US. https://doi.org/10.1007/978-0-387-71003-7.\n[45] Chaudhry, N. (2005). Stream Data Management, Chaudhry, N.A., Shaw, K. and\nAbdelguerfi, M., Eds., Database, Vol. 30, Boston, MA, Springer US. https://doi.org/10.1007/b106968.\n[46] Oguz, D., Yin, S., Hameurlain, A., Ergenc, B. and Dikenelli, O. (2016). Adaptive Join Operator\nfor Federated Queries over Linked Data Endpoints, Lecture Notes in Computer Science(including subseries Lecture Notes in Artificial Intelligence and Lecture Notes inBioinformatics), 275 –290. https://doi.org/10.1007/978-3-319-44039-2_19.\n[47] Codd, E.F. (1983). A relational model of data for large shared data banks, Communications of\nthe ACM, 26(1), 64 –69. https://doi.org/10.1145/357980.358007.\n[48] Pilman, M., Kaufmann, M., Köhl, F., Kossmann, D. and Profeta, D. (2016). ParTime: Parallel\ntemporal aggregation. In Proceedings of the ACM SIGMOD International Conference onManagement of Data. https://doi.org/10.1145/2882903.2903732\n[49] Santos, M.Y. and Costa, C. (2016). Data models in NoSQL databases for big data contexts,\nLecture Notes in Computer Science (including subseries Lecture Notes in ArtificialIntelligence and Lecture Notes in Bioinformatics). https://doi.org/10.1007/978-3-319-40973-3_48.\n[50] Zhao, G., Huang, W., Liang, S. and Tang, Y. (2013). Modeling MongoDB with relational model.\nIn Proceedings –4th International Conference on Emerging Intelligent Data and Web\nTechnologies, EIDWT 2013. https://doi.org/10.1109/EIDWT.2013.25122 Mario Diván and María Laura Sánchez-Reynoso",117901
12-7. Performance evaluation of switched telephone exchange network.pdf,12-7. Performance evaluation of switched telephone exchange network,"Kuldeep Nagiya and Mangey Ram\n7 Performance evaluation of switched\ntelephone exchange network\nAbstract: Public switched telephone network is the most useful telecommunication\nnetwork. This model calculates the different reliability standard of a basic telephone\nexchange system. This model contains the subscriber loop systems. In this model, var-ious subsystems are also included such as distributed cables (DC), feeder point (FP),\nmain distribution frame (MDF), and distribution point (DP). In this system, DC are\nconnected with FP, feeder cables are connected with MDF, and the drop wires are con-nected to wire pairs in DP. All the failure and service rates are general. The different\nreliability characteristics are found by using Laplace transformation and Markov\nprocess.\nKeywords: Telephone exchange, system integration, fault-tolerant systems, switched\nnetwork\n7.1 Introduction\nA telephone network is the most useful telecommunication network. The complex-\nity of the telephone network is managed by using a hierarchical structure, world-\nwide standardization, and decentralization of administration, operation, andmaintenance. Generally, four levels of cabling are used. The drop wires are con-\nnected to a distribution point (DP) at the subscriber end. The drop wires are con-\nnected to wire pairs in the distribution cables (DC) at the DP. In a feeder point (FP),various DC are terminated. The feeder cables are terminated on a main distribution\nframe (MDF) at the telephone exchange. Subscriber pairs and exchange pairs are\ninterconnected at MDF by means of jumpers and MDF provides a flexible intercon-nection mechanism.\nMany scholars have done a lot of research in this field. Flowers [1] studied on\nelectronic telephone exchange and comparing the automatic telephone exchangeand exchange using cheaper and more reliabile electromechanical switches. Healso discussed about the possibility of using time division multiplex and multiple\nfrequency division type of connector switches for speech and line signal transmis-\nsion. Palmer [2] discussed the principles for maintaining the automatic telephoneexchange plant and analyzed that the maintenance of equipment that are used in\nKuldeep Nagiya, Mangey Ram, Department of Mathematics, Graphic Era University, Dehradun,\nUttarakhand, India\nhttps://doi.org/10.1515/9783110619058-007\n\nautomatic telephone exchange was very easy than the maintenance of equipment\nthat are used in earlier one because in earlier the error finding is very much difficult.\nHe suggested that the preventive maintenance was required for routine checkup ofthe condition of the plant. Depp and Townsend [3] studied on electronic branch tele-phone exchange switching system. They described the overall system design and op-\neration. This study tells the superiority of the built-in speed of electronic devices.\nWarman and Bear [4] analyzed the trunking and traffic aspects of a telephone ex-change system, which was divided into parts. They described a study for the design\nof a telephone exchange system in which a network is divided into different parts.\nThey also determined that which part is best for each call. They also described theworking of reed relay exchange system that was controlled electronically. Strandberg\nand Ericsson [5] analyzed the reliability prediction in telephone system engineering.\nThey discussed some aspects of the definition and application of new concepts andthe classification and choice of appropriate measures. They measured the maintain-ability, traffic ability, availability, and system effectiveness, quality of measures as\nwell as reliability of the system. Malec [6] analyzed the reliability optimization in de-\nsign of a telephone switching system and described that the optimization techniquesare used in both allocation and modeling. He also discussed about the objectives of\nreliability and different methods of reliability allocation. Baron et al. [7] studied\nabout the behavior of telephone relays and connectors in various antipathetic envi-ronments. They conducted three types of studies. A study on atmospheric analysis in\ntelephone offices, surface analysis of contacts from telephone exchanges, and a\nstudy on laboratory simulation tests. Tortorella [8] discussed the cutoff calls and thetelephone equipment reliability, and described a mathematical model for the rate of\ncutoff calls caused by failure and malfunctions in telephone equipment. This model\ncompared the rate of cutoff calls produced by failures in the equipment and its sub-systems to the failure modes in the equipment, their severity and frequency of occur-rence, and the call holding time distribution. The author used a model as a c-server\nqueuing system. Lelievre and Goarin [9] discussed the consequence conditions on theaccuracy in working of electronic components in telephone exchanges. They col-lected numerous data and used sophisticated data processing in this study. They con-\nducted a physical analysis for the components failure and also studied about factors\naffecting the reliability of the components. Kolte [10] described the cooling systemBPA-105 for small telephone exchanges and described that the cooling system\nmust meet the following requirement suc ha sh i g hr e l i a b i l i t y ,c o o l i n gr e s e r v e s\nduring mains failure and minimum maintenance. The different system units such\nas cooling, pump, control, supervision, and ventilation and stated that the cooling\nsystem has the following features easy installation, small risk of condensation,\ngood environment and minimum maintenance. Kanoun et al. [11] developed soft-ware for reliability analysis and prediction to the TROPICO-R switching system.They presented a method that allows living reliability growth models. They de-\nscribed that the hyper exponential mode is allowed forecasting of the software124 Kuldeep Nagiya and Mangey Ram\n\nresidual failure rate. This method is applied to the Brazilian electronic switching\nsystem TROPICO-R. Fagerstrom and Healy [1 2] discussed the reliability of local ex-\nchange carrier telephone network. They de scribed that the reliability of local ex-\nchange carrier (LEC) networks was obtained from Bell core ’sO u t a g eP e r f o r m a n c e\nMonitoring (OPM) processes. Kuhn [13] desc ribed the roots of failure in the public\nswitched telephone network. The failures in the public switched telephone net-\nwork were due to the human intervention. Hellstrom et al. [14] described themethod of cooling for telephone exchang es using borehole heat exchanges in var-\nious atmospheres. They described that there is no need of cooling machine, andthe consumption of electric power is very low. The main advantages of this systemare very high air temperatures, excellent reliability, and very low cost of mainte-\nnance. They developed a system that cont ains factory-assembled system control\nunit, atmosphere cooling unit, outdoor re cooling unit, ground cooling unit using\nborehole heat exchange. Mirjalily et al. [15, 16] analyzed the centers of telephoneexchange over metro Ethernet networks by using DiffServ-MPLS. They studied\nthe performance of connecting telephone exchange centers over a metro Ethernet\nnetwork using MPLS and different server quality of service model and imple-\nmented some simulation techniques to evaluate the performance in terms of\ndelay, jitter, and loss. They also discussed the performance evaluation of differ-ent quality of service models for connecting telephone exchange centers overmetro Ethernet networks.\n7.2 System description, assumptions, and notations\nInitially, all the components are in working conditions. If the FP fails, then the sys-tem goes into a degraded or partial failed state. If the DP fails, then the system willgoes into a partially failed stage. The system is completely failed due to failure of\nMDF. The system has also completely failed due to the failure of the power station\nand DC respectively. If either the DP is failed after the failure of FP or the FP isfailed after the failure of DP then the system is also completely failed. The flow offailure can be seen in transition state diagram in Figure 7.1.\nIn this model, the following assumptions are taken:\n(i) All the subsystems are in good working conditions.(ii) Three types of states are considered: good, partially, and completely failed\nstates.\n(iii) Constant failure and repair rates.(iv) Sufficient repair facilities are available.\n(v) After repairing, the model goes in working conditions.7 Performance evaluation of switched telephone exchange network 125\n\nWe use the following notations in this model:\nThe descriptions of various states involved in the designed system have been\nshown in the following Table 7.1.P4(x,t)\nS4\nP5(x,t)\nS5P6(x,t)\nS6P0(t)\nS0\nP2(t)\nS2P3(x,t)\nS3P1(t)\nS1\nλMDF\nλMDF\nλDPλDCλFPλDP\nλDCλDCλPSλPSλPS\nλFP\nλMDFμμ\nμ\nμ μμ\nFigure 7.1: Transition state diagram.\nt Time unit\ns Variable used in Laplace transforms\nP(t) Probability at time tin initial state\nPi(t) Probability at time tinith state; i=,\nPj(x,t) P.d.f. of the system is in jth state where j=,,,\nλMDF Main distribution frame failure rate\nλPS Power supply failure rate\nλDP Distributed point failure rate\nλFP Feeder point failure rate\nλDC Distributed cable failure rate\nµ Repairing rate\nTable 7.1: State descriptions.\nState Descriptions of different state\nS This is the starting stage of the model. All subsystems are in perfectly good working conditions.\nS InS, subsystem feeder point is failed and other subsystems are in working conditions,\ntherefore this state is partially failed.126 Kuldeep Nagiya and Mangey Ram\n\n7.3 Formation and solution of the differential\nequations\nOn the basis of following transition state diagram, we developed the following set\nof differential equations:\n∂\n∂t+λPS+λFP+λDC+λDP+λMDF/C20/C21\nP0ðtÞ=μP1ðtÞ+P2ðtÞ fg +X\njð∞\n0Pjðx,tÞμdx;j=3t o 6\n(7:1)\n∂\n∂t+λPS+λDC+λDP+λMDF +μ/C20/C21\nP1ðtÞ=λFPP0ðtÞ (7:2)\n∂\n∂t+λPS+λDC+λDP+λMDF +μ/C20/C21\nP2ðtÞ=λDPP0ðtÞ (7:3)\n∂\n∂t+∂\n∂x+μ/C20/C21\nP3ðx,tÞ=0( 7:4)\n∂\n∂t+∂\n∂x+μ/C20/C21\nP4ðx,tÞ=0( 7:5)\n∂\n∂t+∂\n∂x+μ/C20/C21\nP5ðx,tÞ=0( 7:6)\n∂\n∂t+∂\n∂x+μ/C20/C21\nP6ðx,tÞ=0( 7:7)Table 7.1 (continued)\nState Descriptions of different state\nS InS,subsystem distributed point is failed and other subsystems are in working conditions,\ntherefore this state is also partially failed.\nS InS,subsystem main distribution frame is failed and other subsystems are in working\nconditions, therefore the state Sis completely failed state.\nS InS,subsystem power supply is failed and other subsystems are in working conditions,\ntherefore this state is also completely failed state.\nS InS,subsystem distributed cable is failed and other subsystems are in working conditions,\ntherefore this state is also completely failed state.\nS InS,subsystem feeder point and distributed point are failed and other subsystems are in\nworking conditions, therefore this state is also completely failed state.7 Performance evaluation of switched telephone exchange network 127\n\nWe are using the different boundary conditions:\nP3ð0,tÞ=λMDF P0ðtÞ+P1ðtÞ+P2ðtÞ fg (7:8)\nP4ð0,tÞ=λPSP0ðtÞ+P1ðtÞ+P2ðtÞ fg (7:9)\nP5ð0,tÞ=λDCP0ðtÞ+P1ðtÞ+P2ðtÞ fg (7:10)\nP6ð0,tÞ=λDPP1ðtÞ+λFPP2ðtÞ (7:11)\nWe know that at t=0 ,P0(t) = 1 and other probabilities are zero.\nNow, we take the Laplace transformation of eqs. (7.1) to (7.11):\ns+λPS+λFP+λDC+λDP+λMDF ½/C138 P0ðsÞ=1+μP1ðsÞ+P2ðsÞ/C8/C9\n+X\njð∞\n0Pjðx,sÞμdx;j=3t o6\n(7:12)\ns+λPS+λDC+λDP+λMDF +μ ½/C138 P1ðsÞ=λFPP0ðsÞ (7:13)\ns+λPS+λDC+λDP+λMDF +μ ½/C138 P2ðsÞ=λDPP0ðsÞ (7:14)\ns+∂\n∂x+μ/C20/C21\nP3ðx,sÞ=0( 7:15)\ns+∂\n∂x+μ/C20/C21\nP4ðx,sÞ=0( 7:16)\ns+∂\n∂x+μ/C20/C21\nP5ðx,sÞ=0( 7:17)\ns+∂\n∂x+μ/C20/C21\nP6ðx,sÞ=0( 7:18)\nFrom boundary conditions,\nP3ð0,sÞ=λMDF P0ðsÞ+P1ðsÞ+P2ðsÞ/C8/C9\n(7:19)\nP4ð0,sÞ=λPSP0ðsÞ+P1ðsÞ+P2ðsÞ/C8/C9\n(7:20)\nP5ð0,sÞ=λDCP0ðsÞ+P1ðsÞ+P2ðsÞ/C8/C9\n(7:21)\nP6ð0,sÞ=λDPP1ðsÞ+λFPP2ðsÞ (7:22)\nFrom eqs. (7.15) to (7.18), we get,\nPjðx,sÞ=Pjð0,sÞexpf−sx−ðx\n0μdxg;j=3t o6 ( 7:23)128 Kuldeep Nagiya and Mangey Ram\n\nSolution for the differential equations is given as:\nP0ðsÞ=1\nc−μc1−μc2−λMDF +λPS+λDC ðÞ 1+c1+c2 ðÞ +2λDPc1 fg SðsÞ(7:24)\nP1ðsÞ=c1P0ðsÞ (7:25)\nP2ðsÞ=c2P0ðsÞ (7:26)\nP3ðsÞ=λMDF1+c1+c2 ðÞ1−SðsÞ\ns/C18/C19\nP0ðsÞ (7:27)\nP4ðsÞ=λPS1+c1+c2 ðÞ1−SðsÞ\ns/C18/C19\nP0ðsÞ (7:28)\nP5ðsÞ=λDC1+c1+c2 ðÞ1−SðsÞ\ns/C18/C19\nP0ðsÞ (7:29)\nP6ðsÞ=2λDPc11−SðsÞ\ns/C18/C19\nP0ðsÞ (7:30)\nwhere\nc=s+λPS+λFP+λDC+λDP+λMDF (7:31)\nc1=λFP\ns+λPS+λDP+λDC+λMDF +μ(7:32)\nc2=λDP\ns+λPS+λDP+λDC+λMDF +μ(7:33)\nThe up and down state probabilities at any time ‘t’are given later:\nPupðsÞ=P0ðsÞ+P1ðsÞ+P2ðsÞ=1+c1+c2 ðÞ P0ðsÞ (7:34)\nPdownðsÞ=P3ðsÞ+P4ðsÞ+P5ðsÞ+P6ðsÞ\n=λMDF +λPS+λDC ðÞ 1+c1+c2 ðÞ +2λDPc1 fg1−SðsÞ\ns/C18/C19\nP0ðsÞ (7:35)\n7.4 Particular cases\nIn this section, the various reliability characteristics such as availability, reliability,\nMTTF, expected profit, as well as sensitivity analysis of the system are analyzed.7 Performance evaluation of switched telephone exchange network 129\n\n7.4.1 Availability\nAvailability is the probability of an operable system at a given specified time,\nthat is, the amount of time a system is actually operating as the percentage of thetotal time, and it should be operated. For finding the availability of the system,\nwe assume the different failure rates as λ\nMDF= 0.001, λDP= 0.003, λFP= 0.003,\nλDC=0 . 0 0 4 , λPS=0 . 0 0 2 , a n d µ= 1 in eq. (7.34) and after replacing these values,\ninverse Laplace transformation is calcul ated. We find the following availability\nof the system:\nPupðtÞ=0.9930311886 +1.987166831 e−1.013t−1.98019802e−1.01t(7:36)\nIncreasing the time from t=0t o t= 15 by unit increment in Pup(t), we obtained the\ndifferent values of availability that are shown in Table 7.2. With the help of these\nvalues a graph that shows the trend of availability of the system is illustrated inFigure 7.2.\nTable 7.2: Availability as function of time.\nTime ( t) Availability Pup(t)\n .\n .\n .\n .\n .\n .\n .\n .\n .\n .\n .\n .\n .\n .\n .\n .130 Kuldeep Nagiya and Mangey Ram\n\n7.4.2 Reliability\nFor finding the reliability of the system, we assume repair rate µ= 0 in eq. (7.34).\nAfter that we will take the inverse Laplace transformation. The reliability of the sys-\ntem is given as follows:\nRðtÞ=λDP+λFP ðÞ e−λMDF +λPS+λDP+λDC ðÞ t−λDPe−λDC+λPS+λFP+λDP+λMDF ðÞ t\nλFP(7:37)\nIncreasing the time from t=0 t o t= 15 by unit increment in R(t), we obtained the\ndifferent values of availability that are shown in Table 7.3. With the help of thesevalues a graph that shows the trend of availability of the system is illustrated inFigure 7.3.02468 1 0 1 2 1 4 1 60.9920.9930.9940.9950.9960.9970.9980.9991.0001.001Availability Pup(t)\nTime (t)\nFigure 7.2: Availability as function of time.\nTable 7.3: Reliability as function of time.\nTime Reliability\n .\n .\n .\n .7 Performance evaluation of switched telephone exchange network 131\n\nTable 7.3 (continued)\nTime Reliability\n .\n .\n .\n .\n .\n .\n .\n .\n .\n .\n .\n .\n02468 1 0 1 2 1 4 1 60.900.920.940.960.981.00Reliability R(t)\nTime (t)\nFigure 7.3: Reliability as function of time.132 Kuldeep Nagiya and Mangey Ram\n\n7.4.3 Mean time to failure (MTTF)\nMean time to failure (MTTF) is the length of time a device is expected to last in oper-\nation. MTTF represents how long a device can reasonably be expected to perform inthe field based on specific testing.\nTo find the MTTF by letting the repair rate µ= 0 and taking the Laplace varia-\nble’s’tends to zero ( s→0) in eq. (34):\nMTTF =λPS+λDC+λMDF +λFP+2λDP\nλPS+λDP+λDC+λMDF ðÞ λPS+λDP+λDC+λMDF +λFP ðÞ(7:38)\nAssuming λMDF= 0.001, λPS= 0.002, λDP= 0.003, λFP= 0.003, and λDC= 0.004 and\nvarying λMDF,λPS,λDP,λFP, and λDC, respectively, as 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7,\n0.8, 0.9 in eq. (7.38), the variation of MTTF with respect to failure rates as shown inTable 7.4 and graphically in Figure 7.4.\n7.4.4 Expected profit\nWe can find the expected profit [17] in the interval (0, t] by using the equation,\nEPðtÞ=K1ðt\n0PupðtÞdt−tK2 (7:39)\nIn this equation, K1and K2represent the revenue and service cost per unit time,\nrespectively.Table 7.4: MTTF as function of different failure rates.\nVariation in λMDF,λPS,\nλDP,λFP, and λDCMTTF\nλMDF λPS λDP λFP λDC\n. .  .  .  .  .\n. .  .  .  .  .\n. .  .  .  .  .\n. .  .  .  .  .\n. .  .  .  .  .\n. .  .  .  .  .\n. .  .  .  .  .\n. .  .  .  .  .\n. .  .  .  .  .7 Performance evaluation of switched telephone exchange network 133\n\nPutting the value of Pup(t) in eq. (7.39) and integrate within the limit, we ob-\ntained the expected:\nEpðtÞ=0.9930311886 t−1.961665184 e−1.013t+1.960592099 e−1.01t+0.001073084603 −tK2\n(7:40)\nPutting K1=1 a n d K2= 0.01, 0.02, 0.04, 0.06, 0.08, 0.1, 0.2, 0.4, respectively, in\neq. (7.40), we get Table 7.5 and Figure 7.5, respectively.\nTable 7.5: Values of expected profit at various service rates.\nTime\n(t)Expected Profit Ep(t)\nK=. K=. K=. K=. K=. K=. K=. K=.\n .  .  .  .  .  .  .  .\n .  .  .  .  .  .  .  .\n .  .  .  .  .  .  .  .\n .  .  .  .  .  .  .  .\n .  .  .  .  .  .  .  .\n .  .  .  .  .  .  .  .0.2 0.4 0.6 0.8 1.0020406080100\nλDPλMDF, λPS & λDCλFPMTTF\nVariation in failure rates\nFigure 7.4: Graph of MTTF as function of different failure rates.134 Kuldeep Nagiya and Mangey Ram\n\n7.4.5 Post optimality analysis (sensitivity analysis)\nWith the help of sensitivity analysis we can find out how sensitive an output is to\nany change in an input while keeping others input constant. Sensitivity analysis ofa factor can be found by partial derivative of the function with respect to that factor.\nIn the analyses, different failure rates represent the following factors [18].02468 1 00246810Expected Profit Ep(t)\nTime (t)\nFigure 7.5: Graph of expected profit at various service rates.Table 7.5 (continued)\nTime\n(t)Expected Profit Ep(t)\nK=. K=. K=. K=. K=. K=. K=. K=.\n .  .  .  .  .  .  .  .\n .  .  .  .  .  .  .  .\n .  .  .  .  .  .  .  .\n .  .  .  .  .  .  .  .7 Performance evaluation of switched telephone exchange network 135\n\n(i) Sensitivity of reliability\nTo find the sensitivity analysis of reliability, we differentiate the equation of reli-\nability partially with respect to the failure rate λMDF,λPS,λDP,λFP, and λDC, respec-\ntively, by using the values of λMDF= 0.001, λPS= 0.002, λDP= 0.003, λFP= 0.003, and\nλDC= 0.004. We find∂RðtÞ\n∂λMDF,∂RðtÞ\n∂λPS,∂RðtÞ\n∂λDP,∂RðtÞ\n∂λFP,and∂RðtÞ\n∂λDCand putting the value of t=0\ntot= 10, we observed the following values and graph in Table 7.6 and Figure 7.6,\nrespectively.\n(ii) Sensitivity of MTTF\nTo find the sensitivity analysis of MTTF, we differentiate the equation of MTTF par-\ntially with respect to the failure rate λMDF,λPS,λDP,λFP, and λDC, respectively, by\nusing the values of λMDF= 0.001, λPS= 0.002, λDP= 0.003, λFP= 0.003, and λDC=\n0.004. We find∂MTTF\n∂λMDF,∂MTTF\n∂λPS,∂MTTF\n∂λDP,∂MTTF\n∂λFP,and∂MTTF\n∂λDC. Now apply the variations in\nfailure rates respectively as 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, and 0.9 sensitivity ofMTTF; we find the following values and observed a graph in Table 7.7 and\nFigure 7.7.Table 7.6: Sensitivity of reliability as a function of time.\nTime ∂RðtÞ\n∂λMDF∂RðtÞ\n∂λPS∂RðtÞ\n∂λDP∂RðtÞ\n∂λFP∂RðtÞ\n∂λDC\n \n −. −. −. −. −.\n −. −. −. −. −.\n −. −. −. −. −.\n −. −. −. −. −.\n −. −. −. −. −.\n −. −. −. −. −.\n −. −. −. −. −.\n −. −. −. −. −.\n −. −. −. −. −.\n −. −. −. −. −.136 Kuldeep Nagiya and Mangey Ram\n\n7.5 Results discussion and conclusions\nWith the help of mathematical modeling, Laplace transformation, and supplementary\nvariable technique, we obtain the different reliability characteristics of telephone ex-change system. This model is useful for finding the different reliability characteristicsTable 7.7: Sensitivity of MTTF as a function of failure rates.\nVariation in λMDF,λPS,\nλDP,λFP,andλDC∂MTTF\n∂λMDF∂MTTF\n∂λPS∂MTTF\n∂λDP∂MTTF\n∂λFP∂MTTF\n∂λDC\n. −. −. −. −. −.\n. −. −. −. −. −.\n. −. −. −. −. −.\n. −. −. −. −. −.\n. −. −. −. −. −.\n. −. −. −. −. −.\n. −. −. −. −. −.\n. −. −. −. −. −.\n. −. −. −. −. −.02468 1 0–10–8–6–4–20Reliability Sensitivity\nTime (t)λDPλFP\nλMDF, λPS & λDC\nFigure 7.6: Graph of sensitivity of reliability as a function of time.7 Performance evaluation of switched telephone exchange network 137\n\nby assuming the various values of different failures. This model evaluates the avail-\nability, reliability, MTTF, expected profit, and post optimality analysis of telephone\nexchange system that contains MDF, power supply, DP, FP, and DC.\nWith the help of Figure 7.2, we observe a graph of availability with respect to\na function of time. The graph of reliability shows the sharply decrement from t=\n0 to 6, some very less increment from t= 7 to 11, and then constant. In Figure 7.3,\nwe see a graph of reliability with respect to a function of time. With the help of\ngraph of reliability we observe that the reliability of the system decreases with\nrespect to increases in time. The Figure 7.4 shows the graph of MTTF as a function\nof various failure rates. With the help of a graph of MTTF, we observe that the\nMTTF with respect to failures of DP, DC, MDF, and power supply are near aboutsame, but the MTTF with respect to failure of FP is the largest. The expected profit\ndecreases due to the increments in servi ce cost, the graph of Figure 7.5 explains\nthis statement. This graph shows the variations by assuming the different values\nof service cost. Finally, Figures 7.6 and 7.7 show the graph of sensitivity analysis\nof reliability and MTTF. On observing the graph of Figure 7.6, we can say that the\nsystem is more sensitive with respect to failures of DC, power supply, and MDF,\nand the graph of Figure 7.7 shows the system is more sensitive with respect to\nfailure of DP.0.2 0.4 0.6 0.8 1.0–180–160–140–120–100–80–60–40–20020MTTF Sensitivity\nVariation in failure ratesλDPλFP\nλMDF, λPS & λDC\nFigure 7.7: Graph of sensitivity of MTTF as a function of failure rates.138 Kuldeep Nagiya and Mangey Ram\n\nReferences\n[1] Flowers, T.H. (1952). Electronic telephone exchanges, Proceedings of the IEE-Part I: General,\n99(119), 181 –193.\n[2] Palmer, R.W. (1955). Maintenance principles for automatic telephone exchange plant,\nProceedings of the IEE-Part B: Radio and Electronic Engineering, 102(4), 453 –468.\n[3] Depp, W.A. and Townsend, M.A. (1964). An electronic-private-branch-exchange telephone\nswitching system, IEEE Transactions on Communication and Electronics, 83(73), 329 –331.\n[4] Warman, J.B. and Bear, D. (1966). Trunking and traffic aspects of a sectionalised telephone\nexchange system. Electrical Engineers, Proceedings of the Institution of, 113(8), 1331 –1343.\n[5] Strandberg, K. and Ericsson, L.M. (1973). Reliability Prediction in Telephone Systems\nEngineering. 7 th InternationaL TeLetraffic Congress, StockhoLm.\n[6] Malec, H.A. (1977). Reliability Optimization in Telephone Switching Systems Design, IEEE\nTransactions on Reliability, 26(3), 203– 208.\n[7] Baron, J., Traon, J.L., Riou, M. and Auregan, M. (1977). Contact Behavior of Telephone Relays\nand Connectors in Various Aggressive Environments, IEEE Transactions on Parts, Hybrids and\nPackaging, 13(1), 68 –72.\n[8] Tortorella, M. (1981). Cutoff calls and telephone equipment reliability, Bell Syst. Tech. J,\n60(8), 1861 –1890.\n[9] Lelievre, A. and Goarin, R. (1987). The influence of climatic conditions on the reliability of\nelectronic components in telephone exchanges, Quality and Reliability Engineering:International, 3(2), 93– 98.\n[10] Kolte, J. (1989). Cooling system BPA-105 for small telephone exchanges, Ericsson Review,\n66(2), 58 –63.\n[11] Kanoun, K., de Bastos Martini, M.R. and de Souza, J.M. (1991). A method for software\nreliability analysis and prediction application to the TROPICO-R switching system, IEEETransactions on Software Engineering, 17(4), 334 –344.\n[12] Fagerstrom, R. and Healy, J. (1993). The reliability of LEC telephone networks, IEEE\nCommunications Magazine, 31(6), 44 –48.\n[13] Kuhn, D.R. (1997). Sources of failure in the public switched telephone network, Computers,\n30(4), 31 –36.\n[14] Hellström, G., Enlund, S. and Paksoy, H. (2000). Direct Cooling of Telephone Switching\nExchanges Using Borehole Heat Exchangers in Different Climates. In Proc. of 8th Int. Conf. onThermal Energy Storage, Terrastock.\n[15] Mirjalily, G., Bastani, P. and Almodarresi, S.M.T. (2008). Connecting telephone exchange\ncenters over metro Ethernet networks by using Diffserv-MPLS. In Computer and InformationTechnology, 2008. ICCIT 2008. 11th International Conference on (pp. 708 –713). IEEE.\n[16] Mirjalily, G., Bastani, P. and Almodarresi, S.M.T. (2009). Performance evaluation of different\nQoS models for connecting telephone exchange centers over metro Ethernet networks,Journal of Networks, 4(10), 960– 967.\n[17] Manglik, M. and Ram, M. (2014). Stochastic modeling of a multi-state manufacturing system\nunder three types of failures with perfect fault coverage, Journal of Engineering Science andTechnology, 77 –90.\n[18] Nagiya, K. and Ram, M. (2014). Performance evaluation of a computer workstation under ring\ntopology, Journal of Engineering Science and Technology, 91– 103.7 Performance evaluation of switched telephone exchange network 139",27358
13-8. On use of Weibull failure laws for reliability measures of a seriesparallel system.pdf,13-8. On use of Weibull failure laws for reliability measures of a seriesparallel system,"S.C. Malik and S.K. Chauhan\n8 On use of Weibull failure laws for reliability\nmeasures of a series –parallel system\nAbstract: Here, the reliability measures of a series-parallel system of (m, n) order have\nbeen obtained. The system has ‘m’subsystems connected in series and each subsys-\ntem has ‘n’components connected in parallel. The use of Weibull failure laws has\nbeen made to derive the expressions for mean time to system failure (MTSF) and reli-\nability of the system. The expressions for these measures have also been obtained for\nthe particular cases of the Weibull distribution i.e. for the Rayleigh and Exponentialdistributions. The results are derived for arbitrary values of the parameters related to\nnumber of components, operating time of the components and failure rate of the com-\nponents. The behaviour of MTSF and reliability has been observed graphically for aparticular system of (10, 10) order by considering all components identical in nature.The effect of operating time, scale and shape parameters on MTSF and reliability has\nalso been shown graphically and numerically.\nKeywords: structural design, reliability measures, arbitrary distribution, MTSF\n8.1 Introduction\nStructural designs of the components play an important role to develop reliable sys-\ntems. The system designers and engineers have succeeded in identifying some better\nand effective structures of the components including series, parallel, series –parallel,\nand parallel –series. And, the parallel structure of the components has been sug-\ngested as the best one so far as reliability is concerned [1, 2]. But, the structuresseries –parallel and parallel –series [3, 4] involve a combination of several compo-\nnents and are very complex in nature. Therefore, it becomes necessary to know the\neffect of the number of components in the system, their qualities, and the structurein which they are arranged before developing the system. [5] developed reliability\nmeasures of a series –parallel system with some arbitrary distributions. [6] considered\nselective maintenance for multistate series –parallel system under different condi-\ntions. [7] discussed reliability equivalence factors of a series –parallel system in\nWeibull distribution. The purpose of the present chapter is to derive expressions for\nreliability and mean time to system failure (MTSF) of a series –parallel system of “m”\nS. C. Malik, Department of Statistics, M. D. University, Rohtak, India\nS. K. Chauhan, Department of Statistics, Shaheed Rajguru College of Applied Sciences for\nWomen University of Delhi, New Delhi, India\nhttps://doi.org/10.1515/9783110619058-008\n\nsubsystems each having “n”nonidentical components with Weibull failure laws. The\nexpressions of these measures are also obtained for identical components. The values\nof MTSF and reliability of this system have been evaluated for arbitrary values of thenumber of components, operating time of the components, and failure rate of thecomponents. The behavior of MTSF and reliability has been observed for monotonic\nfailure nature of the components, that is,. by considering a particular case of Weibull\nfailure laws. To make the study more effective, the trend of MTSF and Reliability hasbeen observed numerically as well as graphically for a particular system of (10, 10)\norder under different operating time, scale, and shape parameters.\n8.2 Notations\nThe following notations are used to represent different functions:\nRstðÞ= Reliability of the system, RitðÞ= Reliability of the ith component\nRtðÞ= Reliability of identical components\nh(t) = Instantaneous failure rate of the system\nhitðÞ= Instantaneous failure rate of ith component, λ= constant failure rate\nT= Lifetime of the system, Ti= Lifetime of the ith component.\nm= Number of subsystems, n= Number of components connected in series\n8.3 System description\nA series –parallel configuration of order ( m,n) is a system having “m”independent\nseries of subsystems each has “n”components arranged in parallel network as\nshown in Figure 8.1:\n12 j m\nOut In1\n3\ni\nn21\n3\ni\nn21\n3\ni\nn21\n3\ni\nn2\nFigure 8.1: Series –parallel system of “n”components.142 S.C. Malik and S.K. Chauhan\n\nThe system reliability at time “t”is given by\nRstðÞ=Ym\nj=11−Yn\ni=11−RitðÞ ðÞ/C20/C21\n(8:1)\nwhere 1 −Qn\ni=11−RitðÞ ðÞ is the reliability of a parallel system of “n”components.\nAlso, the MTSF is given by\nMTSF =ð∞\n0RtðÞdt=ð∞\n0Ym\nj=11−Yn\ni=11−RitðÞ ðÞ/C20/C21\ndt (8:2)\nWhen components in each subsystem are identical, that is, having the same reli-\nability then, we have\nRstðÞ=1−1−RtðÞ ðÞn½/C138m\nAnd,\nMTSF =ð∞\n0RtðÞdt=ð∞\n01−1−RtðÞ ðÞn½/C138mdt\n8.4 Reliability measures\nThe reliability and MTSF of the series –parallel system have been obtained by consider-\ning Weibull distribution for failure rate of the components. The values of these reliabil-ity measures have also been evaluated for particular cases of the Weibull distribution.\n8.5 Reliability measures by Weibull distribution\nSuppose, failure rate of all the components are governed by Weibull failure laws,that is, h\nitðÞ=λitβi\nThe components reliability is given by\nRitðÞ=e−Ðt\n0hiuðÞdu\n=e−Ðt\n0λiuβidu\n=e−λitβi+1\nβi+1\nTherefore, the system reliability is given by\nRstðÞ=Ym\nj=11−Yn\ni=11−e−λitβi+1\nβi+1 !""#\n(8:3)8 On use of Weibull failure laws for reliability measures of a series –parallel system 143\n\nand\nMTSF =ð∞\n0RstðÞdt=ð∞\n0Ym\nj=11−Yn\ni=11−e−λitβi+1\nβi+10\nB@1\nCA2\n643\n75dt (8:4)\nFor identical components, we have hitðÞ=λtβ\nThe component reliability is given by RitðÞ=e−λtβ+1\nβ+1\nTherefore, the system reliability is given by\nRstðÞ=1−1−e−λtβ+1\nβ+1/C18/C19 n""#m\n(8:5)\nand\nMTSF =ð∞\n01−1−e−λtβ+1\nβ+1/C18/C19 n""#m\ndt (8:6)\n8.6 Reliability measures for arbitrary values\nof the parameters\nReliability and MTSF of the system have been obtained for arbitrary values of the param-\neters associated with number of subsystems (m), number of components (n), scale pa-rameter ( λ), operating time of the component (t), and shape parameter ( β). The results\nare shown numerically in the tables 8.1 to 8.5 and graphically in the figures 8.2 to 8.6.\nTable 8.1: Reliability versus no. of subsystems (m ) and components (n ).\nmn λ=.,\nβ=.,t=λ=.,\nβ=.,t=λ=.,\nβ=.,t=λ=.,\nβ=.,t=λ=.,\nβ=.,t=\n  . .  .  .  .\n . .  .  .  .\n . .  .  .  .\n . .  .  .  .\n . .  .  .  .144 S.C. Malik and S.K. Chauhan\n\nTable 8.1 (continued)\nmn λ=.,\nβ=.,t=λ=.,\nβ=.,t=λ=.,\nβ=.,t=λ=.,\nβ=.,t=λ=.,\nβ=.,t=\n  . .  .  .  .\n . .  .  .  .\n . .  .  .  .\n . .  .  .  .\n .  .  .  .  .\n  . .  .  .  .\n . .  .  .  .\n . .  .  .  .\n . .  .  .  .\n .  .  .  .  .\n  . .  .  .  .\n . .  .  .  .\n . .  .  .  .\n . .  .  .  .\n .  .  .  .  .\n  . .  .  .  .\n . .  .  .  .\n . .  .  .  .\n . .  .  .  .\n .  .  .  .  .\n  . .  .  .  .\n . .  .  .  .\n . .  .  .  .\n . .  .  .  .\n .  .  .  .  .8 On use of Weibull failure laws for reliability measures of a series –parallel system 145\n\nTable 8.1 (continued)\nmn λ=.,\nβ=.,t=λ=.,\nβ=.,t=λ=.,\nβ=.,t=λ=.,\nβ=.,t=λ=.,\nβ=.,t=\n  . .  .  .  .\n . .  .  .  .\n . .  .  .  .\n . .  .  .  .\n .  .  .  .  .\n  . .  .  .  .\n . .  .  .  .\n . .  .  .  .\n . .  .  .  .\n . .  .  .  .\n  . .  .  .  .\n . .  .  .  .\n . .  .  .  .\n . .  .  .  .\n . .  .  .  .\n   . .  .  .  .\n . .  .  .  .\n . .  .  .  .\n . .  .  .  .\n . .  .  .  .146 S.C. Malik and S.K. Chauhan\n\nTable 8.2: MTSF versus no. of subsystems (m ) and components (n ).\nmn λ=.,\nβ=.,t=λ=.,\nβ=.,t=λ=.,\nβ=.,t=λ=.,\nβ=.,t=λ=.,\nβ=.,t=\n   .  .  .  .  .\n  .  .  .  .  .\n  .  .  .  .  .\n  .  .  .  .  .\n  .  .  .  .  .\n   .  .  .  .  .\n  .  .  . .  .\n  .  .  .  .  .\n  .  .  .  .  .\n  .  .  .  .  .\n   .  .  .  .  .\n  .  .  .  .  .\n  .  .  .  .  .\n  .  .  .  .  .\n  .  .  .  .  .00.20.40.60.811.2\n1234 512 34 51234 512 34 512 34 51234 512 34 512 34 51234 512 34 5\n1234 56 78 9 1 0Reliability →\nNo. of Subsystems and Components (m,n)  →λ=0.01,β=0.1,t=10\nλ=0.02,β=0.1,t=10\nλ=0.03,β=0.1,t=10\nλ=0.04,β=0.1,t=10\nλ=0.05,β=0.1,t=10\nFigure 8.2: Reliability versus no. of subsystems (m ) and components (n ).8 On use of Weibull failure laws for reliability measures of a series –parallel system 147\n\nTable 8.2 (continued)\nmn λ=.,\nβ=.,t=λ=.,\nβ=.,t=λ=.,\nβ=.,t=λ=.,\nβ=.,t=λ=.,\nβ=.,t=\n   .  .  .  .  .\n  .  .  .  .  .\n  .  .  .  .  .\n  .  .  .  .  .\n  .  .  .  .  .\n   .  .  .  .  .\n  .  .  .  .  .\n  .  .  .  .  .\n  .  .  .  .  .\n  .  .  .  .  .\n   .  .  .  .  .\n  .  .  .  .  .\n  .  .  .  .  .\n  .  .  .  .  .\n  .  .  .  .  .\n   .  .  .  .  .\n  .  .  .  .  .\n  .  .  .  .  .\n  .  .  .  .  .\n  .  .  .  .  .\n   .  .  .  .  .\n  .  .  .  .  .\n  .  .  .  .  .\n  .  .  .  .  .\n  .  .  .  .  .148 S.C. Malik and S.K. Chauhan\n\nTable 8.2 (continued)\nmn λ=.,\nβ=.,t=λ=.,\nβ=.,t=λ=.,\nβ=.,t=λ=.,\nβ=.,t=λ=.,\nβ=.,t=\n  .  .  .  .  .\n  .  .  .  .  .\n  .  .  .  .  .\n  .  .  .  .  .\n  .  .  .  .  .\n   .  .  .  .  .\n  .  .  .  .  .\n  .  .  .  .  .\n  .  .  .  .  .\n  .  .  .  .  .\n020406080100120140160\n12 34 51234 51234 51234 51234 512 34 512 34 512 34 512 34 512 34 5\n1234 56 78 9 1 0MTSF →\nNo. of Subsystems and Components (m,n) →λ=0.01,β=0.1,t=10\nλ=0.02,β=0.1,t=10\nλ=0.03,β=0.1,t=10\nλ=0.04,β=0.1,t=10\nλ=0.05,β=0.1,t=10\nFigure 8.3: MTSF versus no. of subsystems (m ) and components (n ).8 On use of Weibull failure laws for reliability measures of a series –parallel system 149\n\nTable 8.3: Reliability versus no. of subsystems (m ) and components (n )\nmn β=.,\nλ=.,t=β=.,\nλ=.,t=β=.,\nλ=.,t=β=.,\nλ=.,t=β=.,\nλ=.,t=\n  .  .  . .  .\n .  .  . .  .\n .  .  . .  .\n .  .  . .  .\n .  .  .  .  .\n  .  .  . .  .\n .  .  . .  .\n .  .  . .  .\n .  .  . .  .\n .  .  .  .  .\n  .  .  . .  .\n .  .  . .  .\n .  .  . .  .\n .  .  . .  .\n .  .  . .  .\n  .  .  . .  .\n .  .  . .  .\n .  .  . .  .\n .  .  . .  .\n .  .  . .  .\n  .  .  . .  .\n .  .  . .  .\n .  .  . .  .\n .  .  . .  .\n .  .  . .  .150 S.C. Malik and S.K. Chauhan\n\nTable 8.3 (continued)\nmn β=.,\nλ=.,t=β=.,\nλ=.,t=β=.,\nλ=.,t=β=.,\nλ=.,t=β=.,\nλ=.,t=\n  .  .  . .  .\n .  .  . .  .\n .  .  . .  .\n .  .  . .  .\n .  .  . .  .\n  .  .  . .  .\n .  .  . .  .\n .  .  . .  .\n .  .  . .  .\n .  .  . .  .\n  .  .  . .  .\n .  .  . .  .\n .  .  . .  .\n .  .  . .  .\n .  .  . .  .\n  .  .  . .  .\n .  .  . .  .\n .  .  . .  .\n .  .  . .  .\n .  .  . .  .\n   .  .  . .  .\n .  .  . .  .\n .  .  . .  .\n .  .  . .  .\n .  .  . .  .8 On use of Weibull failure laws for reliability measures of a series –parallel system 151\n\nTable 8.4: MTSF versus no. of subsystems (m ) and components (n ).\nmn β=.,\nλ=.,t=β=.,\nλ=.,t=β=.,\nλ=.,t=β=.,\nλ=.,t=β=.,\nλ=.,t=\n   .  .  .  .  .\n  .  .  .  .  .\n  .  .  .  .  .\n  .  .  .  .  .\n  .  .  .  .  .\n   .  .  .  .  .\n  .  .  .  .  .\n  .  .  .  .  .\n  .  .  .  .  .\n  .  .  .  .  .\n   .  .  .  .  .\n  .  .  .  .  .\n  .  .  .  .  .\n  .  .  .  .  .\n  .  .  .  .  .00.20.40.60.811.2\n12345123451234512345123451234512345123451234512345\n12 34 56 78 9 1 0Reliability →\nNo. of Subsystems and Components (m,n) →λ=0.01,β=0.1,t=10\nλ=0.01,β=0.2,t=10\nλ=0.01,β=0.3,t=10\nλ=0.01,β=0.4,t=10\nλ=0.01,β=0.5,t=10\nFigure 8.4: Reliability versus no. of subsystems (m ) and components (n ).152 S.C. Malik and S.K. Chauhan\n\nTable 8.4 (continued)\nmn β=.,\nλ=.,t=β=.,\nλ=.,t=β=.,\nλ=.,t=β=.,\nλ=.,t=β=.,\nλ=.,t=\n   .  .  .  .  .\n  .  .  .  .  .\n  .  .  .  .  .\n  .  .  .  .  .\n  .  .  .  .  .\n   .  .  .  .  .\n  .  .  .  .  .\n  .  .  .  .  .\n  .  .  .  .  .\n  .  .  .  .  .\n   .  .  .  .  .\n  .  .  .  .  .\n  .  .  .  .  .\n  .  .  .  .  .\n  .  .  .  .  .\n   .  .  .  .  .\n  .  .  .  .  .\n  .  .  .  .  .\n  .  .  .  .  .\n  .  .  .  .  .\n   .  .  .  .  .\n  .  .  .  .  .\n  .  .  .  .  .\n  .  .  .  .  .\n  .  .  .  .  .8 On use of Weibull failure laws for reliability measures of a series –parallel system 153\n\nTable 8.4 (continued)\nmn β=.,\nλ=.,t=β=.,\nλ=.,t=β=.,\nλ=.,t=β=.,\nλ=.,t=β=.,\nλ=.,t=\n  .  .  .  .  .\n  .  .  .  .  .\n  .  .  .  .  .\n  .  .  .  .  .\n  .  .  .  .  .\n   .  .  .  .  .\n  .  .  .  .  .\n  .  .  .  .  .\n  .  .  .  .  .\n  .  .  .  .  .\n020406080100120140160\n1352413524135241352413524\n12 34 56 78 9 1 0MTSF →\nNo. of Subsystems and Components (m,n) → λ=0.01,β=0.1,t=10\nλ=0.01,β=0.2,t=10\nλ=0.01,β=0.3,t=10\nλ=0.01,β=0.4,t=10\nλ=0.01,β=0.5,t=10\nFigure 8.5: MTSF versus no. of subsystems ( m) and components (n ).154 S.C. Malik and S.K. Chauhan\n\nTable 8.5: Reliability versus no. of subsystems (m ) and components (n).\nmn t=,\nλ=.,β=.t=,\nλ=.,β=.t=,\nλ=.,β=.t=,\nλ=.,β=.t=,\nλ=.,β=.\n  . . .  .  .\n . . .  .  .\n . . .  .  .\n .  . .  .  .\n .  .  .  .  .\n  . . .  .  .\n . . .  .  .\n . . .  .  .\n . . .  .  .\n .  .  .  .  .\n  . . .  .  .\n . . .  .  .\n . . .  .  .\n . . .  .  .\n .  .  .  .  .\n  . . .  .  .\n . . .  .  .\n . . .  .  .\n . . .  .  .\n .  .  .  .  .\n  . . .  .  .\n . . .  .  .\n . . .  .  .\n . . .  .  .\n .  .  .  .  .8 On use of Weibull failure laws for reliability measures of a series –parallel system 155\n\nTable 8.5 (continued)\nmn t=,\nλ=.,β=.t=,\nλ=.,β=.t=,\nλ=.,β=.t=,\nλ=.,β=.t=,\nλ=.,β=.\n  . . .  .  .\n . . .  .  .\n . . .  .  .\n . . .  .  .\n .  .  .  .  .\n  . . .  .  .\n . . .  .  .\n . . .  .  .\n . . .  .  .\n .  .  .  .  .\n  . . .  .  .\n . . .  .  .\n . . .  .  .\n . . .  .  .\n .  . .  .  .\n  . . .  .  .\n . . .  .  .\n . . .  .  .\n . . .  .  .\n .  . .  .  .\n   . . .  .  .\n . . .  .  .\n . . .  .  .\n . . .  .  .\n .  . .  .  .156 S.C. Malik and S.K. Chauhan\n\n8.7 Particular cases of Weibull distribution Rayleigh\ndistribution\nThe Rayleigh distribution is a special case of Weibull distribution with the shape\nparameter β= 1. The component reliability in this case is given by\nRitðÞ=e−Rt\n0hiuðÞdu\n=e−Rt\n0λiudu\n=e−λit2\n2 (8:7)\nwhere hi(t)=λit. Therefore, the system reliability is given by\nRstðÞ=Ym\nj=11−Yn\ni=11−e−λit2\n2/C18/C19/C26/C27\n(8:8)\nand\nMTSF =Ð∞\n0RstðÞdt=Ð∞\n0Qm\nj=11−Qn\ni=11−e−λit2\n2/C18/C19/C26/C27\ndt(8:9)\nFor identical components, we have hitðÞ=λt\nThe component reliability is given by RitðÞ=e−λt2\n2\nThen the system reliability is given by\nRstðÞ=1−1−e−λt2\n2/C18/C19n /C20/C21 m\nand MTSF =ð∞\n01−1−e−λt2\n2/C18/C19n /C20/C21 m\ndt00.20.40.60.811.2\n12345123451234512345123451234512345123451234512345\n12 34 56 78 9 1 0Reliability →\nNo. of Subsystems and Components (m,n) →λ=0.01,β=0.1,t=5\nλ=0.01,β=0.1,t=10\nλ=0.01,β=0.1,t=15\nλ=0.01,β=0.1,t=20\nλ=0.01,β=0.1,t=25\nFigure 8.6: Reliability versus no. of subsystems (m ) and components (n ).8 On use of Weibull failure laws for reliability measures of a series –parallel system 157\n\n8.8 Reliability measures for arbitrary values\nof the parameters\nReliability and MTSF of the system have been obtained for arbitrary values of the\nparameters associated with number of subsystems (m), number of components (n),failure rate ( λ) and operating time of the components (t). The results are shown nu-\nmerically in the tables 8.6 to 8.8 and graphically in figures 8.7 to 8.9.\nTable 8.6: Reliability versus no. of subsystems (m ) and components (n ).\nmn λ=.,t= λ=.,t= λ=.,t= λ=.,t= λ=.,t=\n  .  .  .  .  .\n .  .  .  .  .\n .  .  .  .  .\n .  .  .  .  .\n .  .  .  .  .\n  .  .  .  .  .\n .  .  .  .  .\n .  .  .  .  .\n .  .  .  .  .\n .  .  .  .  .\n  .  .  .  .  .\n .  .  .  .  .\n .  .  .  .  .\n .  .  .  .  .\n .  .  .  .  .\n  .  .  .  .  .\n .  .  .  .  .\n .  .  .  .  .\n .  .  .  .  .\n .  .  .  .  .158 S.C. Malik and S.K. Chauhan\n\nTable 8.6 (continued)\nmn λ=.,t= λ=.,t= λ=.,t= λ=.,t= λ=.,t=\n  .  .  .  .  .\n .  .  .  .  .\n .  .  .  .  .\n .  .  .  .  .\n .  .  .  .  .\n  .  .  .  .  .\n .  .  .  .  .\n .  .  .  .  .\n .  .  .  .  .\n .  .  .  .  .\n  .  .  .  .  .\n .  .  .  .  .\n .  .  .  .  .\n .  .  .  .  .\n .  .  .  .  .\n  .  .  .  .  .\n .  .  .  .  .\n .  .  .  .  .\n .  .  .  .  .\n .  .  .  .  .\n  .  .  .  .  .\n .  .  .  .  .\n .  .  .  .  .\n .  .  .  .  .\n .  .  .  .  .8 On use of Weibull failure laws for reliability measures of a series –parallel system 159\n\nTable 8.6 (continued)\nmn λ=.,t= λ=.,t= λ=.,t= λ=.,t= λ=.,t=\n   .  .  .  .  .\n .  .  .  .  .\n .  .  .  .  .\n .  .  .  .  .\n .  .  .  .  .\nTable 8.7: MTSF vs no. of subsystems (m ) and components (n ).\nmn λ=.,t= λ=.,t= λ=.,t= λ=.,t= λ=.,t=\n   . .  .  .  .\n  .  .  .  .  .\n  .  .  . .  .\n  .  .  .  .  .\n  .  .  .  .  .00.20.40.60.811.2\n1 23451234512345123451 234512345123451 23451234512345\n1234 56 78 9 1 0\nNo. of Subsystems and Components (m, n) →λ=0.01,t=10\nλ=0.02,t=10\nλ=0.03,t=10\nλ=0.04,t=10\nλ=0.05,t=10\nFigure 8.7: Reliability versus no. of subsystems (m ) and components (n ).160 S.C. Malik and S.K. Chauhan\n\nTable 8.7 (continued)\nmn λ=.,t= λ=.,t= λ=.,t= λ=.,t= λ=.,t=\n  .  .  .  .  .\n  . .  .  .  .\n  .  .  .  .  .\n  .  .  .  .  .\n  .  .  .  .  .\n  .  .  .  .  .\n  . .  .  .  .\n  . .  .  .  .\n  .  .  .  .  .\n  .  .  .  .  .\n  .  .  .  .  .\n  . .  .  .  .\n  . .  .  .  .\n  .  .  .  .  .\n  .  .  .  .  .\n  . .  .  .  .\n . .  .  .  .\n  . .  .  .  .\n  . .  .  .  .\n  .  .  .  .  .\n  . .  .  .  .\n . .  .  .  .\n  . .  .  .  .\n  . .  .  .  .\n  .  .  .  .  .8 On use of Weibull failure laws for reliability measures of a series –parallel system 161\n\nTable 8.7 (continued)\nmn λ=.,t= λ=.,t= λ=.,t= λ=.,t= λ=.,t=\n  . .  .  .  .\n . .  .  .  .\n  . .  .  .  .\n  . .  .  .  .\n  . .  .  .  .\n  . .  .  .  .\n . .  .  .  .\n  . .  .  .  .\n  . .  .  .  .\n  . .  .  .  .\n  . .  .  .  .\n . .  .  .  .\n  . .  .  .  .\n  . .  .  .  .\n  . .  .  .  .\n   . .  .  .  .\n . .  .  .  .\n  . .  .  .  .\n  . .  .  .  .\n  . .  .  .  .162 S.C. Malik and S.K. Chauhan\n\nTable 8.8: Reliability versus no. of subsystems (m ) and components (n ).\nmn λ=.,t=λ=.,t= λ=.,t= λ=.,t= λ=.,t=\n  . .  .  .  .\n . .  .  .  .\n . .  .  .  .\n . .  .  .  .\n .  .  .  .  .\n  . .  .  .  .\n . .  .  .  .\n . .  .  .  .\n . .  .  .  .\n .  .  .  .  .\n  . .  .  .  .\n . .  .  .  .\n . .  .  .  .\n . .  .  .  .\n .  .  .  .  .0510152025\n12 34 512 34 51234 512 34 512 34 512 34 512 34 512 34 512 34 512 34 5\n1234 56 78 9 1 0MTSF →\nNo. of Subsystems and Components (m, n) →λ=0.01,t=10\nλ=0.02,t=10\nλ=0.03,t=10\nλ=0.04,t=10\nλ=0.05,t=10\nFigure 8.8: MTSF versus no. of subsystems (m ) and components (n ).8 On use of Weibull failure laws for reliability measures of a series –parallel system 163\n\nTable 8.8 (continued)\nmn λ=.,t=λ=.,t= λ=.,t= λ=.,t= λ=.,t=\n  . .  .  .  .\n . .  .  .  .\n . .  .  .  .\n . .  .  .  .\n .  .  .  .  .\n  . .  .  .  .\n . .  .  .  .\n . .  .  .  .\n . .  .  .  .\n .  .  .  .  .\n  . .  .  .  .\n . .  .  .  .\n . .  .  .  .\n . .  .  .  .\n .  .  .  .  .\n  . .  .  .  .\n . .  .  .  .\n . .  .  .  .\n . .  .  .  .\n . .  .  .  .\n  . .  .  .  .\n . .  .  .  .\n . .  .  .  .\n . .  .  .  .\n .  .  .  .  .164 S.C. Malik and S.K. Chauhan\n\n8.9 Exponential distribution\nThe exponential distribution is a special case of Weibull distribution with the shape\nparameter β= 0. the component reliability is given by\nRitðÞ=e−λit(8:10)Table 8.8 (continued)\nmn λ=.,t=λ=.,t= λ=.,t= λ=.,t= λ=.,t=\n  . .  .  .  .\n . .  .  .  .\n . .  .  .  .\n . .  .  .  .\n .  .  .  .  .\n   . .  .  .  .\n . .  .  .  .\n . .  .  .  .\n . .  .  .  .\n . .  .  .  .\n00.20.40.60.811.2\n1 23451 23451 23451 234512345123451 2345123451234512345\n12 34 56 78 9 1 0Reliability →\nNo. of Subsystems and Components (m, n) →λ=0.01,t=5\nλ=0.01,t=10\nλ=0.01,t=15\nλ=0.01,t=20\nλ=0.01,t=25\nFigure 8.9: Reliability versus no. of subsystems (m ) and components ( n).8 On use of Weibull failure laws for reliability measures of a series –parallel system 165\n\nwhere hi(t)=λi. Therefore, the system reliability is given by\nRstðÞ=Ym\nj=11−Yn\ni=11−e−λit/C0/C1/C20/C21\n(8:11)\nwhere 1 −Qn\ni=11−RitðÞ ðÞ is the reliability of a parallel system.\nand\nMTSF =Ð∞\n0RtðÞdt=Ð∞\n0Qm\nj=11−Qn\ni=11−e−λit/C0/C1/C20/C21\ndt (8:12)\nFor identical components, we have λi=λ\nThe system reliability is given by\nRstðÞ=1−1−e−λt/C0/C1 nhim\n(8:13)\nAnd MTSF =Ð∞\n0RstðÞdt=Ð∞\n01−1−e−λt/C0/C1 nhim\ndt\nTake 1 –e−λt=yso that dt=dy\nλ1−yðÞ\nNow MTSF =1\nλÐ1\n01−ynðÞm−1\n1−yðÞdy=1\nλÐ1\n01−ynðÞm−11−ynðÞ\n1−yðÞdt\nAgain take yn=zso that y=z1=nand\ndy=1\nnzn−1\nn\nMTSF =1\nnλXn−1\ni=0ð1\n0zi+1n−11−zðÞm−1dz=1\nnλXn−1\ni=0Bi+1\nn;m/C18/C19\n=1\nnλXn−1\ni=0Γi+1\nn/C18/C19\nmðÞ\ni+1\nn+m\nMTSF =1\nnλXn−1\ni=0i+1\nn−1/C18/C19\n!m−1 ðÞ !\ni+1\nn+m−1/C18/C19\n!=m−1 ðÞ !\nnλXn−1\ni=0i+1\nn−1/C18/C19\n!\ni+1\nn+m−1/C18/C19\n!(8:14)\nSubcase\nSuppose system has only one subsystem, that is, m= 1, then\nMTSF =1\nnλXn−1\ni=01\ni+1=n=1\nλXn−1\ni=01\ni+1=1\nλ1+1\n2+1\n3+/C1/C1/C1+1\nn/C20/C21\n=1\nλXn\ni=11\ni\nThe result obtained for MTSF in this case is same as the MTSF of the parallel system\nof“n”components.166 S.C. Malik and S.K. Chauhan\n\n8.10 Reliability measures for arbitrary values\nof the parameters\nReliability and MTSF of the system have been obtained for arbitrary values of the\nparameters associated with number of subsystems ( m), number of components ( n),\nfailure rate ( λ) and operating time of the component ( t). The results are shown nu-\nmerically in the tables 8.9 to 8.11 and graphically in the figures 8.10 to 8.12.\nTable 8.9: Reliability versus no. of subsystems (m ) and components (n ).\nmn λ=.,t= λ=.,t= λ=.,t= λ=.,t= λ=.,t=\n  . .  .  .  .\n . .  .  .  .\n . .  .  .  .\n . .  .  .  .\n .  .  .  .  .\n  . .  .  .  .\n . .  .  .  .\n . .  .  .  .\n . .  .  .  .\n . .  .  .  .\n  . .  .  .  .\n . .  .  .  .\n . .  .  .  .\n . .  .  .  .\n . .  .  .  .\n  . .  .  .  .\n . .  .  .  .\n . .  .  .  .\n . .  .  .  .\n . .  .  .  .8 On use of Weibull failure laws for reliability measures of a series –parallel system 167\n\nTable 8.9 (continued)\nmn λ=.,t= λ=.,t= λ=.,t= λ=.,t= λ=.,t=\n  . .  .  .  .\n . .  .  .  .\n . .  .  .  .\n . .  .  .  .\n . .  .  .  .\n  . .  .  .  .\n . .  .  .  .\n . .  .  .  .\n . .  .  .  .\n . .  .  .  .\n  . .  .  .  .\n . .  .  .  .\n . .  .  .  .\n . .  .  .  .\n . .  .  .  .\n  . .  .  .  .\n . .  .  .  .\n . .  .  .  .\n . .  .  .  .\n . .  .  .  .\n  . .  .  .  .\n . .  .  .  .\n . .  .  .  .\n . .  .  .  .\n . .  .  .  .168 S.C. Malik and S.K. Chauhan\n\nTable 8.10: MTSF versus no. of subsystems (m ) and components (n ).\nmn λ=.,t= λ=.,t= λ=.,t= λ=.,t= λ=.,t=\n    .  .\n    . \n  .  .  .  .  .\n  .  .  .  .  .\n  .  .  .  .  .\n    .  . \n  .  .  .  .  .\n  .  .  .  .  .\n  .  .  .  .  .\n  .  .  .  .  .Table 8.9 (continued)\nmn λ=.,t= λ=.,t= λ=.,t= λ=.,t= λ=.,t=\n   . .  .  .  .\n . .  .  .  .\n . .  .  .  .\n . .  .  .  .\n . .  .  .  .\n00.20.40.60.811.2\n1 23451 23451 23451 23451 23451 23451 23451 23451 23451 2345\n12 34 56 78 9 1 0Reliability →\nNo. of Subsystems and Components (m,n) →λ=0.01,t=10\nλ=0.02,t=10\nλ=0.03,t=10\nλ=0.04,t=10\nλ=0.05,t=10\nFigure 8.10: Reliability versus no. of subsystems (m ) and components (n ).8 On use of Weibull failure laws for reliability measures of a series –parallel system 169\n\nTable 8.10 (continued)\nmn λ=.,t= λ=.,t= λ=.,t= λ=.,t= λ=.,t=\n   .  .  .  .  .\n      .  . \n  .  .  .  .  .\n  .  .  .  .  .\n  .  .  .  .  .\n   . .  . \n  .  .  .  .  .\n  .  .  .  .  .\n  .  .  .  .  .\n  .  .  .  .  .\n   .  .  \n  .  .  .  .  .\n  .  .  .  .  .\n  .  .  .  .  .\n  .  .  .  .  .\n   .  . .  .  .\n  .  .  .  .  .\n  .  .  .  .  .\n  .  .  .  .  .\n  .  .  .  .  .\n   .  . .  .  .\n  .  .  .  .  .\n  .  .  .  .  .\n  .  .  .  .  .\n  .  . .  .  .170 S.C. Malik and S.K. Chauhan\n\nTable 8.10 (continued)\nmn λ=.,t= λ=.,t= λ=.,t= λ=.,t= λ=.,t=\n   . . .  . .\n  .  .  .  .  .\n  .  .  .  .  .\n  .  . .  .  .\n  .  .  .  .  .\n   .  .  .  .  .\n  .  .  .  .  .\n  .  .  .  .  .\n  .  .  .  .  .\n  .  .  .  .  .\n    .  .\n  .  .  .  .  .\n  .  .  .  .  .\n  .  .  .  .  .\n  .  .  .  .  .\n050100150200250\n12345123451234512345123451234512345123451234512345\n12 34 56 78 9 1 0MTSF →\nNo. of Subsystems and Components (m,n) →λ=0.01,t=10\nλ=0.02,t=10\nλ=0.03,t=10\nλ=0.04,t=10\nλ=0.05,t=10\nFigure 8.11: MTSF versus no. of components (n ) and subsystems (m ).8 On use of Weibull failure laws for reliability measures of a series –parallel system 171\n\nTable 8.11: Reliability versus no. of subsystems (m ) and components (n ).\nmn λ=.,t=λ=.,t= λ=.,t= λ=.,t= λ=.,t=\n  . . . .  .\n . . . .  .\n .  . . .  .\n .  .  . .  .\n .  .  .  .  .\n  . . . .  .\n . . . .  .\n . . . .  .\n .  . . .  .\n .  .  . .  .\n  . . . .  .\n . . . .  .\n . . . .  .\n .  . . .  .\n .  .  . .  .\n  . . . .  .\n . . . .  .\n . . . .  .\n .  . . .  .\n .  .  . .  .\n  . . . .  .\n . . . .  .\n . . . .  .\n .  . . .  .\n .  .  . .  .172 S.C. Malik and S.K. Chauhan\n\nTable 8.11 (continued)\nmn λ=.,t=λ=.,t= λ=.,t= λ=.,t= λ=.,t=\n  . . . .  .\n . . . .  .\n . . . .  .\n .  . . .  .\n .  .  . .  .\n  . . . .  .\n . . . .  .\n . . . .  .\n .  . . .  .\n .  .  . .  .\n  . . . .  .\n . . . .  .\n . . . .  .\n .  . . .  .\n .  .  . .  .\n  . . . .  .\n . . . .  .\n . . . .  .\n .  . . .  .\n .  .  . .  .\n   . . . .  .\n . . . .  .\n . . . .  .\n .  . . .  .\n .  .  . .  .8 On use of Weibull failure laws for reliability measures of a series –parallel system 173\n\n8.11 Discussion and conclusion\nHere, we discussed a series –parallel system consisting “m”subsystems connected in\nseries and each having “n”components connected in parallel. The effect of number of\ncomponents, failure rates of the components, shape parameter, and operating time of\nthe components on reliability and MTSF has been examined by considering Weibullfailure laws. It is observed that reliability and MTSF keeps on increasing with the in-\ncrease of components (connected in parallel) in subsystems while their value declines\nwith the increase of subsystems (connected in series), failure rate, and operating timeof the components. For a special case of Weibull failure laws, that is, Rayleigh and\nexponential failure laws, the values of reliability, and MTSF have also been observed.\nBut, the reliability and MTSF in case of exponential failure laws is more as comparedto Weibull and Rayleigh failure laws. Further, reliability of a series –parallel system\ngoes on decreasing with the increase of operating time irrespective of the distributionsrelated to failure time of components and subsystems. For Weibull failure laws, thereliability and MTSF keeps on decreasing with the increase in the shape parameter.\nHence, finally we conclude that the performance of a series –parallel system\ncan be improved by increasing the number of components rather than to increasethe number of subsystems. The results are shown numerically and graphically inthe respective tables and figures. It is interesting to note that reliability of this sys-\ntem is more than that of series system while a parallel system has more reliability.00.20.40.60.811.2\n12345123451234512345123451234512345123451234512345\n1234 56 78 9 1 0Reliability →\nNo. of Subsystems and Components (m,n) →λ=0.01,t=5\nλ=0.01,t=10\nλ=0.01,t=15\nλ=0.01,t=20\nλ=0.01,t=25\nFigure 8.12: Reliability versus no. of subsystems (m ) and components (n ).174 S.C. Malik and S.K. Chauhan\n\nReferences\n[1] Barlow, R.E. & Prochan, F. (1975). Statistical Theory of Reliability and Life Testing, Holt,\nRinehart & Winston, Inc, New York.\n[2] E. Balagurusamy (1984). Reliability Engineering, Tata McGraw Hill Publishing Co. Ltd., India.\n[3] L. S. Srinath (1985). Concept in Reliability Engineering, Affiliated East-West Press (P) Ltd.[4] Chauhan, S.K. and Malik, S.C. (2016). Reliability evaluation of a series-parallel and\nparallel-series systems for arbitrary values of the parameters, International Journal ofStatistics and Reliability Engineering, 3(1), 10 –19.\n[5] Elsayed, A. (2012). Reliability Engineering, Wiley Series in Systems Engineering and\nManagement.\n[6] Dao, C.D., Zuo, M.J. and Pandey, M. (2014). Selective maintenance for multi-state\nseries-parallel systems under economic dependence, Reliability Engineering System Safety,121, 240 –249.\n[7] M.A. El-Damcese (2009). Reliability equivalence factors of sereis-parallel system in Weibull\ndistribution, International Mathematical Forum, 4(1), 941 –951.\n[8] Sarhan, A.M. (2002). Reliability equivalence with a basic series-parallel system, Applied\nmathematics and computation, 132(1), 115 –133.\n[9] Misra, K.B. (1972). Reliability optimization of a series-parallel system, IEE Transactions on\nReliability, R-21(4), 230– 238.\n[10] Ei-Heweihi, E., Proschan, F. and Sethuraman, J. (1986). Optimal allocation of components in\nparallel-series and series-parallel system, Journal of Applied Probability, 23, 770 –777.\n[11] Gopalan, M.N. (1975). Availability and reliability of a series-parallel system with a single\nrepair facility, IEEE Transactions on Reliability, R –24(3), 219 –220.8 On use of Weibull failure laws for reliability measures of a series –parallel system 175",45248
14-Index.pdf,14-Index,"Index\nActive monitoring 118\nAI 67\nAlias 111Application ’s logic 95\nAtomic data 99Atomic value 109Attributes 92Automatizing any measurement process 92Autonomous 99Availability(ies) 22 –24, 27, 29, 30, 33 –37, 42,\n130– 131\nAvailability analysis 23Availability models 24, 27\nBasic operations 109\nBig data environment 89\nCollecting point of view 96\nCollectors 105, 106Continuous-time absorbing Markov chain 17Cooperative positional data stream 105\nData collecting strategy(ies) 97, 98\nData joining 104Data pipelines 89Data transmission policy 105Data-driven 80, 118Decision-making 80Deep learning 68Difference operation 113Division operation 114\nExclusive data stream 101\nExclusive positional data stream 100Exclusive temporal data stream 100\nExpected profit 133 –135\nExponential distribution 165\nFacebook ’s ecosystem 82\nFailure rate 45 –51, 53, 54, 57, 58,\n63, 64\nFault big data 68GeoStreams 83Heterogeneous data sensors 95\nHeterogeneous data sources 81\nInfinite server queueing approach 14\nInner join 114Internet-of-Things (IoT) 81Intersection operation 113IoTPy 84\nJump diffusion process model 67Landmark windows 103\nLevel of autonomy 95Level of independence 94Library 118Load-aware shedding 82Logical window 102\nMatching operations 107\nMean 45 –47, 49 –54, 56\n–number of births (failures) 49\n–number of failures 45 –47, 50 –53\n–time between failures (MTBF) 52, 53, 55, 58,\n60, 62\nMean time to failure (MTTF) 133Meaning of data 98Measurement framework 93, 118Measurement process 91Metric 93Mix temporal cooperative data streams 106Morphological point of view 86\nNonhomogeneous Poisson process 14, 16\nOpen-source software (OSS) 67\nOptimality analysis 135 –137\nPatch(es)/Patching 1 –3, 7–9\nPhase-type modeling approach 16Phase-type probability distribution 17Physical window 102, 106Positional data streams 102, 103Projection operation 111, 117\nhttps://doi.org/10.1515/9783110619058-009\n\nQuantification 91\nRayleigh distribution 157\nReal-time 80Reconstruction of sessions 83Reliability 23, 27, 131 –132\nReliability block diagrams(RBD) 22, 42Reliability growth 45 –48, 50 –52, 54, 55, 57, 58,\n61, 63, 64\nReliability measures 143Research Questions 84Restriction operation 113\nSearch string 85\nSecurity patch 2Sensitivity analysis 23, 24, 36 –38, 42, 135 –137\nSensitivity of MTTF 136Sensitivity of reliability 136Sensor 95Sensorizer 82Series –parallel system 142\nSeverity/Severe 1 –9\nSimulation 115Sliding window 102SMP 32, 33SMS 84, 118software effort 68Software patch(es)/Patching 1 –9\nSoftware security 1, 2Spark Structured Streaming 83StreamCE 115Switched telephone exchange\nnetwork 123 –138\nTemporal cooperative data stream 106Temporal data streams 103Transition state diagram 126\nUnbounded 99\nUnion operation 113Updates 1 –4, 6–9\nVehicular ad hoc network 21Vehicular cloud computing (VCC) 22 –24, 27,\n33–35, 37, 42\nVulnerability correction model (VCM) 2, 4 –6\nVulnerability/Vulnerabilities 1 –9\nWeibull distribution 143Windows 108, 109178 Index",3004
15-De Gruyter Series on the Applications of Mathematics in Engineering and Information Sciences.pdf,15-De Gruyter Series on the Applications of Mathematics in Engineering and Information Sciences,"De Gruyter Series on the Applications of\nMathematics in Engineering and InformationSciences\nAlready published in the series\nVolume 3: Computational Intelligence. Theoretical Advances and Advanced Applications\nDinesh C. S. Bisht, Mangey Ram (Eds.)ISBN 978-3-11-065524-7, e-ISBN (PDF) 978-3-11-067135-3e-ISBN (EPUB) 978-3-11-066833-9\nVolume 2: Supply Chain Sustainability. Modeling and Innovative Research Frameworks\nSachin Kumar Mangla, Mangey Ram (Eds.)ISBN 978-3-11-062556-1, e-ISBN (PDF) 978-3-11-062859-3,e-ISBN (EPUB) 978-3-11-062568-4\nVolume 1: Soft Computing. Techniques in Engineering Sciences\nMangey Ram, Suraj B. Singh (Eds.)ISBN 978-3-11-062560-8, e-ISBN (PDF) 978-3-11-062861-6,e-ISBN (EPUB) 978-3-11-062571-4",728
