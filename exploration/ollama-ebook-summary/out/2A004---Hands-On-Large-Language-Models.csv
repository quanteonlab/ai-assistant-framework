filename,title,text,len
01-Table of Contents.pdf,01-Table of Contents,"978-1-098-15096-9\n[LSI]Hands-On Large Language Models\nby Jay Alammar and Maarten Grootendorst\nCopyright © 2024 Jay Alammar and Maarten Pieter Grootendorst. All rights reserved.\nPrinted in the United States of America.\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\nO’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are\nalso available for most titles ( http://oreilly.com ). For more information, contact our corporate/institutional\nsales department: 800-998-9938 or corporate@oreilly.com .\nAcquisitions Editor:  Nicole Butterfield\nDevelopment Editor:  Michele Cronin\nProduction Editor:  Ashley Stussy\nCopyeditor:  Charles Roumeliotis\nProofreader:  Kim CoferIndexer:  BIM Creatives, LLC\nInterior Designer:  David Futato\nCover Designer:  Karen Montgomery\nIllustrator:  Kate Dullea\nSeptember 2024:  First Edition\nRevision History for the First Edition\n2024-09-10: First Release\nSee http://oreilly.com/catalog/errata.csp?isbn=9781098150969  for release details.\nThe O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Hands-On Large Language Models , the\ncover image, and related trade dress are trademarks of O’Reilly Media, Inc.\nThe views expressed in this work are those of the authors and do not represent the publisher’s views.\nWhile the publisher and the authors have used good faith efforts to ensure that the information and\ninstructions contained in this work are accurate, the publisher and the authors disclaim all responsibility\nfor errors or omissions, including without limitation responsibility for damages resulting from the use\nof or reliance on this work. Use of the information and instructions contained in this work is at your\nown risk. If any code samples or other technology this work contains or describes is subject to open\nsource licenses or the intellectual property rights of others, it is your responsibility to ensure that your use\nthereof complies with such licenses and/or rights.\nTable of Contents\nPreface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  xi\nPart I. Understanding Language Models\n1.An Introduction to Large Language Models. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  3\nWhat Is Language AI?                                                                                                     4\nA Recent History of Language AI                                                                                  5\nRepresenting Language as a Bag-of-Words                                                              6\nBetter Representations with Dense Vector Embeddings                                        8\nTypes of Embeddings                                                                                                 10\nEncoding and Decoding Context with Attention                                                  11\nAttention Is All Y ou Need                                                                                         15\nRepresentation Models: Encoder-Only Models                                                     18\nGenerative Models: Decoder-Only Models                                                            20\nThe Y ear of Generative AI                                                                                         23\nThe Moving Definition of a “Large Language Model”                                             25\nThe Training Paradigm of Large Language Models                                                  25\nLarge Language Model Applications: What Makes Them So Useful?                    27\nResponsible LLM Development and Usage                                                               28\nLimited Resources Are All Y ou Need                                                                          28\nInterfacing with Large Language Models                                                                   29\nProprietary, Private Models                                                                                      29\nOpen Models                                                                                                               30\nOpen Source Frameworks                                                                                         31\nGenerating Y our First Text                                                                                           32\nSummary                                                                                                                         34\nv\n2.Tokens and Embeddings. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  37\nLLM Tokenization                                                                                                         38\nHow Tokenizers Prepare the Inputs to the Language Model                               38\nDownloading and Running an LLM                                                                       39\nHow Does the Tokenizer Break Down Text?                                                          43\nWord Versus Subword Versus Character Versus Byte Tokens                            44\nComparing Trained LLM Tokenizers                                                                      46\nTokenizer Properties                                                                                                  55\nToken Embeddings                                                                                                        57\nA Language Model Holds Embeddings for the Vocabulary of Its Tokenizer    57\nCreating Contextualized Word Embeddings with Language Models                58\nText Embeddings (for Sentences and Whole Documents)                                      61\nWord Embeddings Beyond LLMs                                                                               63\nUsing pretrained Word Embeddings                                                                       63\nThe Word2vec Algorithm and Contrastive Training                                            64\nEmbeddings for Recommendation Systems                                                              67\nRecommending Songs by Embeddings                                                                   67\nTraining a Song Embedding Model                                                                         69\nSummary                                                                                                                         71\n3.Looking Inside Large Language Models. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  73\nAn Overview of Transformer Models                                                                         74\nThe Inputs and Outputs of a Trained Transformer LLM                                     74\nThe Components of the Forward Pass                                                                    76\nChoosing a Single Token from the Probability Distribution (Sampling/\nDecoding)                                                                                                                79\nParallel Token Processing and Context Size                                                           81\nSpeeding Up Generation by Caching Keys and Values                                        83\nInside the Transformer Block                                                                                   85\nRecent Improvements to the Transformer Architecture                                         95\nMore Efficient Attention                                                                                           96\nThe Transformer Block                                                                                           101\nPositional Embeddings (RoPE)                                                                              102\nOther Architectural Experiments and Improvements                                       105\nSummary                                                                                                                       106\nPart II. Using Pretrained Language Models\n4.Text Classification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  111\nThe Sentiment of Movie Reviews                                                                              112\nText Classification with Representation Models                                                     113\nvi | Table of Contents\nModel Selection                                                                                                            115\nUsing a Task-Specific Model                                                                                      116\nClassification Tasks That Leverage Embeddings                                                     120\nSupervised Classification                                                                                        121\nWhat If We Do Not Have Labeled Data?                                                              123\nText Classification with Generative Models                                                             127\nUsing the Text-to-Text Transfer Transformer                                                      128\nChatGPT for Classification                                                                                     132\nSummary                                                                                                                       135\n5.Text Clustering and Topic Modeling. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  137\nArXiv’s Articles: Computation and Language                                                         138\nA Common Pipeline for Text Clustering                                                                 139\nEmbedding Documents                                                                                          139\nReducing the Dimensionality of Embeddings                                                     140\nCluster the Reduced Embeddings                                                                          142\nInspecting the Clusters                                                                                            144\nFrom Text Clustering to Topic Modeling                                                                 146\nBERTopic: A Modular Topic Modeling Framework                                           148\nAdding a Special Lego Block                                                                                  156\nThe Text Generation Lego Block                                                                           160\nSummary                                                                                                                       164\n6.Prompt Engineering. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  167\nUsing Text Generation Models                                                                                  167\nChoosing a Text Generation Model                                                                       167\nLoading a Text Generation Model                                                                         168\nControlling Model Output                                                                                     170\nIntro to Prompt Engineering                                                                                     173\nThe Basic Ingredients of a Prompt                                                                        173\nInstruction-Based Prompting                                                                                175\nAdvanced Prompt Engineering                                                                                 177\nThe Potential Complexity of a Prompt                                                                 177\nIn-Context Learning: Providing Examples                                                          180\nChain Prompting: Breaking up the Problem                                                       182\nReasoning with Generative Models                                                                          184\nChain-of-Thought: Think Before Answering                                                      185\nSelf-Consistency: Sampling Outputs                                                                     188\nTree-of-Thought: Exploring Intermediate Steps                                                 189\nOutput Verification                                                                                                     191\nProviding Examples                                                                                                 192\nGrammar: Constrained Sampling                                                                         194\nTable of Contents | vii\nSummary                                                                                                                       198\n7.Advanced Text Generation Techniques and Tools. . . . . . . . . . . . . . . . . . . . . . . . . . . . .  199\nModel I/O: Loading Quantized Models with LangChain                                      200\nChains: Extending the Capabilities of LLMs                                                           202\nA Single Link in the Chain: Prompt Template                                                     203\nA Chain with Multiple Prompts                                                                            206\nMemory: Helping LLMs to Remember Conversations                                          209\nConversation Buffer                                                                                                 210\nWindowed Conversation Buffer                                                                            212\nConversation Summary                                                                                          214\nAgents: Creating a System of LLMs                                                                          218\nThe Driving Power Behind Agents: Step-by-step Reasoning                            219\nReAct in LangChain                                                                                                221\nSummary                                                                                                                       224\n8.Semantic Search and Retrieval-Augmented Generation. . . . . . . . . . . . . . . . . . . . . . . .  225\nOverview of Semantic Search and RAG                                                                   226\nSemantic Search with Language Models                                                                  228\nDense Retrieval                                                                                                         228\nReranking                                                                                                                  240\nRetrieval Evaluation Metrics                                                                                  244\nRetrieval-Augmented Generation (RAG)                                                                249\nFrom Search to RAG                                                                                               250\nExample: Grounded Generation with an LLM API                                            252\nExample: RAG with Local Models                                                                         252\nAdvanced RAG Techniques                                                                                    255\nRAG Evaluation                                                                                                        257\nSummary                                                                                                                       258\n9.Multimodal Large Language Models. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  259\nTransformers for Vision                                                                                             260\nMultimodal Embedding Models                                                                                263\nCLIP: Connecting Text and Images                                                                       265\nHow Can CLIP Generate Multimodal Embeddings?                                          265\nOpenCLIP                                                                                                                 268\nMaking Text Generation Models Multimodal                                                         273\nBLIP-2: Bridging the Modality Gap                                                                       273\nPreprocessing Multimodal Inputs                                                                         278\nUse Case 1: Image Captioning                                                                                280\nUse Case 2: Multimodal Chat-Based Prompting                                                 283\nSummary                                                                                                                       286\nviii | Table of Contents\nPart III. Training and Fine-Tuning Language Models\n10. Creating Text Embedding Models. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  289\nEmbedding Models                                                                                                      289\nWhat Is Contrastive Learning?                                                                                  291\nSBERT                                                                                                                            293\nCreating an Embedding Model                                                                                  296\nGenerating Contrastive Examples                                                                         296\nTrain Model                                                                                                               297\nIn-Depth Evaluation                                                                                                300\nLoss Functions                                                                                                          301\nFine-Tuning an Embedding Model                                                                           309\nSupervised                                                                                                                 309\nAugmented SBERT                                                                                                  311\nUnsupervised Learning                                                                                               316\nTransformer-Based Sequential Denoising Auto-Encoder                                  316\nUsing TSDAE for Domain Adaptation                                                                 320\nSummary                                                                                                                       321\n11. Fine-Tuning Representation Models for Classification . . . . . . . . . . . . . . . . . . . . . . . . .  323\nSupervised Classification                                                                                            323\nFine-Tuning a Pretrained BERT Model                                                                325\nFreezing Layers                                                                                                         328\nFew-Shot Classification                                                                                               333\nSetFit: Efficient Fine-Tuning with Few Training Examples                               333\nFine-Tuning for Few-Shot Classification                                                              337\nContinued Pretraining with Masked Language Modeling                                    340\nNamed-Entity Recognition                                                                                        345\nPreparing Data for Named-Entity Recognition                                                   347\nFine-Tuning for Named-Entity Recognition                                                       352\nSummary                                                                                                                       353\n12. Fine-Tuning Generation Models. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  355\nThe Three LLM Training Steps: Pretraining, Supervised Fine-Tuning, and\nPreference Tuning                                                                                                    355\nSupervised Fine-Tuning (SFT)                                                                                  357\nFull Fine-Tuning                                                                                                       357\nParameter-Efficient Fine-Tuning (PEFT)                                                             359\nInstruction Tuning with QLoRA                                                                               367\nTemplating Instruction Data                                                                                  367\nModel Quantization                                                                                                 369\nLoRA Configuration                                                                                                370\nTable of Contents | ix\nTraining Configuration                                                                                           371\nTraining                                                                                                                     372\nMerge Weights                                                                                                          373\nEvaluating Generative Models                                                                                   373\nWord-Level Metrics                                                                                                 374\nBenchmarks                                                                                                              374\nLeaderboards                                                                                                            376\nAutomated Evaluation                                                                                             376\nHuman Evaluation                                                                                                   376\nPreference-Tuning / Alignment / RLHF                                                                  378\nAutomating Preference Evaluation Using Reward Models                                   379\nThe Inputs and Outputs of a Reward Model                                                       380\nTraining a Reward Model                                                                                       380\nTraining No Reward Model                                                                                    384\nPreference Tuning with DPO                                                                                     385\nTemplating Alignment Data                                                                                   386\nModel Quantization                                                                                                 386\nTraining Configuration                                                                                           387\nTraining                                                                                                                     388\nSummary                                                                                                                       389\nAfterword. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  391\nIndex. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  393\nx | Table of Contents",24696
02-Preface.pdf,02-Preface,,0
03-Prerequisites.pdf,03-Prerequisites,"Preface\nLarge language models (LLMs) have had a profound and far-reaching impact on the\nworld. By enabling machines to better understand and generate human-like language,\nLLMs have opened new possibilities in the field of AI and impacted entire industries.\nThis book provides a comprehensive and highly visual introduction to the world of\nLLMs, covering both the conceptual foundations and practical applications. From\nword representations that preceded deep learning to the cutting-edge (at the time\nof this writing) Transformer architecture, we will explore the history and evolution\nof LLMs. We delve into the inner workings of LLMs, exploring their architectures,\ntraining methods, and fine-tuning techniques. We also examine various applications\nof LLMs in text classification, clustering, topic modeling, chatbots, search engines,\nand more.\nWith its unique blend of intuition-building, applications, and illustrative style, we\nhope that this book provides the ideal foundation for those looking to explore the\nexciting world of LLMs. Whether you are a beginner or an expert, we invite you to\njoin us on this journey to start building with LLMs.\nAn Intuition-First Philosophy\nThe main goal of this book is to provide an intuition  into the field of LLMs. The pace\nof development in the Language AI field is incredibly fast and frustration can build\ntrying to keep up with the latest technologies. Instead, we focus on the fundamentals\nof LLMs and intend to provide a fun and easy learning process.\nTo achieve this intuition-first  philosophy  we liberally make use of visual language.\nIllustrations will help give a visual identity to major concepts and processes involved\nxi",1703
04-Book Structure.pdf,04-Book Structure,,0
05-Part III Training and Fine-Tuning Language Models.pdf,05-Part III Training and Fine-Tuning Language Models,"1J. Alammar. “Machine learning research communication via illustrated and interactive web articles. ” Beyond\nStatic Papers: Rethinking How We Share Scientific  Understanding in ML . ICLR 2021 Workshop (2021).in the learning process of LLMs.1 With our illustrative method of storytelling, we\nwant to take you on a journey to this exciting and potentially world-changing field.\nThroughout the book, we make a clear distinction between representation and gener‐\native language models. Representation models are LLMs that do not generate text but\nare commonly used for task-specific use cases, like classification, whereas generation\nmodels are LLMs that generate text, like GPT models. Although generative models\nare typically the first thing that comes to mind when thinking about LLMs, there is\nstill much use for representation models. We are also loosely using the word “large”\nin large language models  and often elect to simply call them language models as size\ndescriptions are often rather arbitrary and not always indicative of capability.\nPrerequisites\nThis book assumes that you have some experience programming in Python and are\nfamiliar with the fundamentals of machine learning. The focus will be on building a\nstrong intuition rather than deriving mathematical equations. As such, illustrations\ncombined with hands-on examples will drive the examples and learning through this\nbook. This book assumes no prior knowledge of popular deep learning frameworks\nsuch as PyTorch or TensorFlow nor any prior knowledge of generative modeling.\nIf you are not familiar with Python, a great place to start is Learn Python , where you\nwill find many tutorials on the basics of the language. To further ease the learning\nprocess, we made all the code available on Google Colab , a platform where you can\nrun all of the code without the need to install anything locally.\nBook Structure\nThe book is broadly divided into three parts. They are illustrated in Figure P-1  to give\nyou a full view of the book. Note that each chapter can be read independently, so feel\nfree to skim chapters you are already familiar with.\nPart I: Understanding Language Models\nIn Part I of the book, we explore the inner workings of language models both small\nand large. We start with an overview of the field and common techniques (see Chap‐\nter 1 ) before moving over to two central components of these models, tokenization\nand embeddings (see Chapter 2 ). We finish this part of the book with an updated\nand expanded version of Jay’s well-known Illustrated Transformer , which dives into\nxii | Preface\nthe architecture of these models (see Chapter 3 ). Many terms and definitions will be\nintroduced that are used throughout the book.\nFigure P-1. All parts and chapters of the book.\nPart II: Using Pretrained Language Models\nIn Part II of the book, we explore how LLMs can be used through common use cases.\nWe use pretrained models and demonstrate their capabilities without the need to\nfine-tune them.\nY ou learn how to use language models for supervised classification (see Chapter 4 ),\ntext clustering and topic modeling (see Chapter 5 ), leveraging embedding models\nfor semantic search (see Chapter 6 ), generating text (see Chapters 7 and 8), and\nextending the capabilities of text generation to the visual domain (see Chapter 9 ).\nPreface | xiii",3371
06-API Keys.pdf,06-API Keys,"Learning these individual language model capabilities will equip you with the skill\nset to problem-solve with LLMs and build more and more advanced systems and\npipelines.\nPart III: Training and Fine-Tuning Language Models\nIn Part III of the book, we explore advanced concepts through training and fine-\ntuning all kinds of language models. We will explore how to create and fine-tune an\nembedding model (see Chapter 10 ), review how to fine-tune BERT for classification\n(see Chapter 11 ), and end the book with several methods for fine-tuning generation\nmodels (see Chapter 12 ).\nHardware and Software Requirements\nRunning generative models is generally a compute-intensive task that requires a com‐\nputer with a strong GPU. Since those are not available to every reader, all examples\nin this book are made to run using an online platform, namely Google Colaboratory ,\noften shortened to “Google Colab. ” At the time of writing, this platform allows you to\nuse an NVIDIA GPU (T4) for free to run your code. This GPU  has 16 GB of VRAM\n(which is the memory of your GPU), which is the minimum amount of VRAM we\nexpect for the examples throughout the book.\nNot all chapters require a minimum of 16 GB VRAM as\nsome examples, like training and fine-tuning, are more compute-\nintensive than others, such as prompt engineering. In the repos‐\nitory, you will find the minimum GPU requirements for each\nchapter.\nAll code, requirements, and additional tutorials are available in this book’s repository .\nIf you want to run the examples locally, we recommend access to an NVIDIA GPU\nwith a minimum of 16 GB of VRAM. For a local installation, for example with conda,\nyou can follow this setup to create your environment:\nconda create -n thellmbook python=3.10\nconda activate thellmbook\nY ou can install all the necessary dependencies by forking or cloning the repository\nand then running the following in your newly created Python 3.10 environment:\npip install -r requirements.txt\nxiv | Preface",2013
07-Using Code Examples.pdf,07-Using Code Examples,"API Keys\nWe use both open source and proprietary models throughout the examples to\ndemonstrate the advantages and disadvantages of both. For the proprietary models,\nusing OpenAI and Cohere’s offering, you will need to create a free account:\nOpenAI\nClick  “sign up” on the site to create a free account. This account allows you to\ncreate an API key, which can be used to access GPT-3.5. Then, go to “ API keys” to\ncreate a secret key.\nCohere\nRegister  a free account on the website. Then, go to “ API keys” to create a secret\nkey.\nNote that with both accounts, rate limits apply and that these free API keys only allow\nfor a limited number of calls per minute. Throughout all examples, we have taken\nthat into account and provided local alternatives if necessary.\nFor the open source models, you do not need to create an account with the exception\nof the Llama 2 model in Chapter 2 . To use that model, you will need a Hugging Face\naccount:\nHugging Face\nClick  “sign up” on the Hugging Face website to create a free account. Then, in\n“Settings” go to “ Access Tokens” to create a token that you can use to download\ncertain LLMs.\nConventions Used in This Book\nThe following typographical conventions are used in this book:\nItalic\nIndicates new terms, URLs, email addresses, filenames, and file extensions.\nConstant width\nUsed for program listings, as well as within paragraphs to refer to program\nelements such as variable or function names, databases, data types, environment\nvariables, statements, and keywords.\nConstant width bold\nShows commands or other text that should be typed literally by the user.\nConstant width italic\nShows text that should be replaced with user-supplied values or by values deter‐\nmined by context.\nPreface | xv",1772
08-How to Contact Us.pdf,08-How to Contact Us,"This element signifies a tip or suggestion.\nThis element signifies a general note.\nUsing Code Examples\nSupplemental material (code examples, exercises, etc.) is available for download at\nhttps://github.com/HandsOnLLM/Hands-On-Large-Language-Models .\nIf you have a technical question or a problem using the code examples, please send\nemail to support@oreilly.com .\nThis book is here to help you get your job done. In general, if example code is\noffered with this book, you may use it in your programs and documentation. Y ou\ndo not need to contact us for permission unless you’re reproducing a significant\nportion of the code. For example, writing a program that uses several chunks of code\nfrom this book does not require permission. Selling or distributing examples from\nO’Reilly books does require permission. Answering a question by citing this book\nand quoting example code does not require permission. Incorporating a significant\namount of example code from this book into your product’s documentation does\nrequire permission.\nWe appreciate, but generally do not require, attribution. An attribution usually\nincludes the title, author, publisher, and ISBN. For example: “ Hands-On Large Lan‐\nguage Models  by Jay Alammar and Maarten Grootendorst (O’Reilly). Copyright 2024\nJay Alammar and Maarten Pieter Grootendorst, 978-1-098-15096-9. ”\nIf you feel your use of code examples falls outside fair use or the permission given\nabove, feel free to contact us at permissions@oreilly.com .\nO’Reilly Online Learning\nFor more than 40 years, O’Reilly Media  has provided technol‐\nogy and business training, knowledge, and insight to help\ncompanies succeed.\nxvi | Preface",1692
09-Acknowledgments.pdf,09-Acknowledgments,"Our unique network of experts and innovators share their knowledge and expertise\nthrough books, articles, and our online learning platform. O’Reilly’s online learning\nplatform gives you on-demand access to live training courses, in-depth learning\npaths, interactive coding environments, and a vast collection of text and video from\nO’Reilly and 200+ other publishers. For more information, visit https://oreilly.com .\nHow to Contact Us\nPlease address comments and questions concerning this book to the publisher:\nO’Reilly Media, Inc.\n1005 Gravenstein Highway North\nSebastopol, CA 95472\n800-889-8969 (in the United States or Canada)\n707-827-7019 (international or local)\n707-829-0104 (fax)\nsupport@oreilly.com\nhttps://www.oreilly.com/about/contact.html\nWe have a web page for this book, where we list errata, examples, and any additional\ninformation. Y ou can access this page at https://oreil.ly/hands_on_LLMs_1e .\nFor news and information about our books and courses, visit https://oreilly.com .\nFind us on LinkedIn: https://linkedin.com/company/oreilly-media .\nWatch us on Y ouTube: https://youtube.com/oreillymedia .\nAcknowledgments\nWriting this book has been an incredible experience, collaboration, and journey\nfor us.\nThe field of (large) language models is one of the most dynamic areas in technology\ntoday, and within the span of writing this book, we have witnessed extraordinary\nadvancements. Y et, despite the rapid pace of change, the fundamental principles\nremain strikingly consistent which made the writing process particularly intriguing.\nWe are grateful to have had the opportunity to explore this field in-depth at such a\npivotal moment.\nWorking with our O’Reilly team was incredible! Special thanks to Michele Cronin for\nher amazing feedback, support, and enthusiasm for this book from day one. We could\nnot have asked for a better editor—you are amazing! Thank you, Nicole Butterfield,\nfor kicking off this book and helping us maintain a structured approach throughout\nthe writing. Thank you to Karen Montgomery for creating our wonderful cover, we\nPreface | xvii\nlove the kangaroo! Big thanks to Kate Dullea for being so patient with us having to go\nthrough hundreds of illustrations many times over. The timely early releases by Clare\nLaylock helped us see our work grow which was a big motivator, thank you. Thanks\nto Ashley Stussy and Charles Roumeliotis for the development in the final stages of\nthe book and everyone else at O’Reilly who contributed.\nThanks to our amazing crew of technical reviewers. Invaluable feedback was given\nby Harm Buisman, Emir Muñoz, Luba Elliott, Guarav Chawla, Rafael V . Pierre, Luba\nElliott, Tarun Narayanan, Nikhil Buduma, and Patrick Harrison.\nJay\nI’ d love to extend my deepest gratitude to my family for their unwavering support\nand inspiration. I would like to specifically acknowledge my parents, Abdullah and\nMishael, and my aunts, Hussah and Aljoharah.\nI’m grateful to the friends, colleagues, and collaborators who helped me understand\nand explain the tricky concepts covered in this book as well as to the Cohere folks\nwho cultivate a supporting learning and sharing environment. Thank you to Adrien\nMorisot, Aidan Gomez, Andy Toulis, Anfal Alatawi, Arash Ahmadian, Bharat Venki‐\ntesh, Edward Grefenstette, Ivan Zhang, Joao Araújo, Luis Serrano, Matthias Gallé,\nMeor Amer, Nick Frosst, Patrick Lewis, Phil Blunsom, Sara Hooker, and Suhas Pai.\nI couldn’t conceive of this project getting accomplished to the level it has without\nthe extraordinary talent and tireless effort of Maarten, my coauthor. Y our ability\nto repeatedly nail the technical details (from the pinned version of the nth import\ndependency to the latest in LLM quantization) while weaving some of the world’s best\nvisual narratives is absolutely breathtaking.\nLastly, a tip of the hat to the incredible coffee shop scene of Riyadh, Saudi Arabia for\nsupplying me with caffeine and a good place to focus from dawn until midnight. It’s\nwhere I read most of these papers and worked out my understanding (looking at you,\nElixir Bunn).\nMaarten\nI want to begin by expressing my heartfelt appreciation to my coauthor, Jay. Y our\ninsights have made this not only possible but incredibly fulfilling. This journey has\nbeen nothing short of amazing and collaborating with you has been an absolute joy.\nI want to sincerely thank my wonderful colleagues at IKNL for their continued\nsupport throughout this journey. A special mention goes to Harm—our Monday\nmorning coffee breaks discussing this book were a constant source of encouragement.\nThank you to my family and friends for their unwavering support, and to my parents\nin particular. Pap, despite the challenges you faced, you always found a way to be\nxviii | Preface\nthere for me when I needed it most, thank you. Mam, the conversations we had\nas aspiring writers were wonderful and motivated me more than you could ever\nimagine. Thank you both for your endless support and encouragement.\nFinally, I am at a loss for words to adequately express my gratitude to my wonderful\nwife, Ilse. Lieverd, your boundless enthusiasm and patience have been legendary,\nespecially when I droned on about the latest LLM developments for hours on end.\nY ou are my greatest support. My apologies to my amazing daughter, Sarah. At just\ntwo years old, you already have listened to more about large language models than\nanyone should have to endure in a lifetime! I promise we’ll make up for it with\nendless playtime and adventures together.\nPreface | xix",5593
10-Part I. Understanding Language Models.pdf,10-Part I. Understanding Language Models,PART I\nUnderstanding Language Models,37
11-Representing Language as a Bag-of-Words.pdf,11-Representing Language as a Bag-of-Words,"CHAPTER 1\nAn Introduction to Large Language Models\nHumanity is at an inflection point. From 2012 onwards, developments in building AI\nsystems (using deep neural networks) accelerated so that by the end of the decade,\nthey yielded the first software system able to write articles indiscernible from those\nwritten by humans. This system was an AI model called Generative Pre-trained\nTransformer 2, or GPT-2. 2022 marked the release of ChatGPT, which demonstrated\nhow profoundly this technology was poised to revolutionize how we interact with\ntechnology and information. Reaching one million active users in five days and\nthen one hundred million active users in two months, the new breed of AI models\nstarted out as human-like chatbots but quickly evolved into a monumental shift in\nour approach to common tasks, like translation, text generation, summarization, and\nmore. It became an invaluable tool for programmers, educators, and researchers.\nThe success of ChatGPT was unprecedented and popularized more research into the\ntechnology behind it, namely large language models (LLMs). Both proprietary and\npublic models were being released at a steady pace, closing in on, and eventually\ncatching up to the performance of ChatGPT. It is not an exaggeration to state that\nalmost all attention was on LLMs.\nAs a result, 2023 will always be known, at least to us, as the year that drastically\nchanged our field, Language Artificial Intelligence (Language AI), a field character‐\nized by the development of systems capable of understanding and generating human\nlanguage.\nHowever, LLMs have been around for a while now and smaller models are still rele‐\nvant to this day. LLMs are much more than just a single model and there are many\nother techniques and models in the field of language AI that are worth exploring.\nIn this book, we aim to give readers a solid understanding of the fundamentals\nof both LLMs and the field of Language AI in general. This chapter serves as the\n3\n1J. McCarthy (2007). “What is artificial intelligence?” Retrieved from https://oreil.ly/C7sja  and https://oreil.ly/\nn9X8O .scaffolding for the rest of the book and will introduce concepts and terms that we will\nuse throughout the chapters.\nBut mostly, we intend to answer the following questions in this chapter:\n•What is Language AI?•\n•What are large language models?•\n•What are the common use cases and applications of large language models?•\n•How can we use large language models ourselves?•\nWhat Is Language AI?\nThe term artificial  intelligence  (AI) is often used to describe computer systems dedica‐\nted to performing tasks close to human intelligence, such as speech recognition, lan‐\nguage translation, and visual perception. It is the intelligence of software as opposed\nto the intelligence of humans.\nHere is a more formal definition by one of the founders of the artificial intelligence\ndiscipline:\n[Artificial intelligence is] the science and engineering of making intelligent machines,\nespecially intelligent computer programs. It is related to the similar task of using\ncomputers to understand human intelligence, but AI does not have to confine itself to\nmethods that are biologically observable.\n—John McCarthy, 20071\nDue to the ever-evolving nature of AI, the term has been used to describe a wide\nvariety of systems, some of which might not truly embody intelligent behavior. For\ninstance, characters in computer games (NPCs [nonplayable characters]) have often\nbeen referred to as AI even though many are nothing more than if-else  statements.\nLanguage AI refers to a subfield of AI that focuses on developing technologies\ncapable of understanding, processing, and generating human language. The term\nLanguage AI  can often be used interchangeably with natural language processing\n(NLP) with the continued success of machine learning methods in tackling language\nprocessing problems.\n4 | Chapter 1: An Introduction to Large Language Models\nWe use the term Language AI  to encompass technologies that technically might not\nbe LLMs but still have a significant impact on the field, like how retrieval systems can\ngive LLMs superpowers (see Chapter 8 ).\nThroughout this book, we want to focus on the models that have had a major role\nin shaping the field of Language AI. This means exploring more than just LLMs in\nisolation. That, however, brings us to the question: what are large language models?\nTo begin answering this question in this chapter, let’s first explore the history of\nLanguage AI.\nA Recent History of Language AI\nThe history  of Language AI encompasses many developments and models aiming to\nrepresent and generate language, as illustrated in Figure 1-1 .\nFigure 1-1. A peek into the history of Language AI.\nLanguage, however, is a tricky concept for computers. Text is unstructured in nature\nand loses its meaning when represented by zeros and ones (individual characters).\nAs a result, throughout the history of Language AI, there has been a large focus on\nrepresenting language in a structured manner so that it can more easily be used by\ncomputers. Examples of these Language AI tasks are provided in Figure 1-2 .\nA Recent History of Language AI | 5\n2Fabrizio Sebastiani. “Machine learning in automated text categorization. ” ACM Computing Surveys (CSUR)\n34.1 (2002): 1–47.\nFigure 1-2. Language AI is capable of many tasks by processing textual input.\nRepresenting Language as a Bag-of-Words\nOur history of Language AI starts with a technique called bag-of-words, a method for\nrepresenting unstructured text.2 It was first mentioned around the 1950s but became\npopular around the 2000s.\nBag-of-words works as follows: let’s assume that we have two sentences for which we\nwant to create numerical representations. The first step of the bag-of-words model\nis tokenization , the process of splitting up the sentences into individual words or\nsubwords ( tokens ), as illustrated in Figure 1-3 .\nFigure 1-3. Each sentence is split into words (tokens) by splitting on a whitespace.\nThe most common method for tokenization is by splitting on a whitespace to create\nindividual words. However, this has its disadvantages as some languages, like Man‐\ndarin, do not have whitespaces around individual words. In the next chapter, we will\n6 | Chapter 1: An Introduction to Large Language Models\ngo in depth about tokenization and how that technique influences language models.\nAs illustrated in Figure 1-4 , after tokenization, we combine all unique words from\neach sentence to create a vocabulary that we can use to represent the sentences.\nFigure 1-4. A vocabulary is created by retaining all unique words across both sentences.\nUsing our vocabulary, we simply count how often a word in each sentence appears,\nquite literally creating a bag of words. As a result, a bag-of-words model aims to\ncreate representations of text in the form of numbers, also called vectors or vector\nrepresentations, observed in Figure 1-5 . Throughout the book, we refer to these kinds\nof models as representation models .\nFigure 1-5. A bag-of-words is created by counting individual words. These  values are\nreferred to as vector representations.\nAlthough bag-of-words is a classic method, it is by no means completely obsolete.\nIn Chapter 5 , we will explore how it can still be used to complement more recent\nlanguage models.\nA Recent History of Language AI | 7",7451
12-Better Representations with Dense Vector Embeddings.pdf,12-Better Representations with Dense Vector Embeddings,"3Tomas Mikolov et al. “Efficient estimation of word representations in vector space. ” arXiv preprint\narXiv:1301.3781  (2013).Better Representations with Dense Vector Embeddings\nBag-of-words,  although an elegant approach, has a flaw. It considers language to be\nnothing more than an almost literal bag of words and ignores the semantic nature, or\nmeaning, of text.\nReleased in 2013, word2vec was one of the first successful attempts at capturing the\nmeaning of text in embeddings .3 Embeddings are vector representations of data that\nattempt to capture its meaning. To do so, word2vec learns semantic representations of\nwords by training on vast amounts of textual data, like the entirety of Wikipedia.\nTo generate these semantic representations, word2vec leverages neural networks .\nThese networks consist of interconnected layers of nodes that process information.\nAs illustrated in Figure 1-6 , neural networks can have many layers where each\nconnection has a certain weight depending on the input. These weights are often\nreferred to as the parameters  of the model.\nFigure 1-6. A neural network consists of interconnected layers of nodes where each\nconnection is a linear equation.\nUsing these neural networks, word2vec generates word embeddings by looking at\nwhich other words they tend to appear next to in a given sentence. We start by\nassigning every word in our vocabulary with a vector embedding, say of 50 values for\neach word initialized with random values. Then in every training step, as illustrated\nin Figure 1-7 , we take pairs of words from the training data and a model attempts to\npredict whether or not they are likely to be neighbors in a sentence.\n8 | Chapter 1: An Introduction to Large Language Models\nDuring this training process, word2vec learns the relationship between words and\ndistills that information into the embedding. If the two words tend to have the\nsame neighbors, their embeddings will be closer to one another and vice versa. In\nChapter 2 , we will look closer at word2vec’s training procedure.\nFigure 1-7. A neural network is trained to predict if two words are neighbors. During\nthis process, the embeddings are updated to be in line with the ground truth.\nThe resulting embeddings capture the meaning of words but what exactly does that\nmean? To illustrate this phenomenon, let’s somewhat oversimplify and imagine we\nhave embeddings of several words, namely “apple” and “baby. ” Embeddings attempt\nto capture meaning by representing the properties of words. For instance, the word\n“baby” might score high on the properties “newborn” and “human” while the word\n“apple” scores low on these properties.\nAs illustrated in Figure 1-8 , embeddings can have many properties to represent the\nmeaning of a word. Since the size of embeddings is fixed, their properties are chosen\nto create a mental representation of the word.\nFigure 1-8. The values of embeddings represent properties that are used to represent\nwords. We may oversimplify by imagining that dimensions represent concepts (which\nthey don’t), but it helps express the idea.\nIn practice, these properties are often quite obscure and seldom relate to a single\nentity or humanly identifiable concept. However, together, these properties make\nsense to a computer and serve as a good way to translate human language into\ncomputer language.\nA Recent History of Language AI | 9",3414
13-Encoding and Decoding Context with Attention.pdf,13-Encoding and Decoding Context with Attention,"Embeddings are tremendously helpful as they allow us to measure the semantic\nsimilarity between two words. Using various distance metrics, we can judge how close\none word is to another. As illustrated in Figure 1-9 , if we were to compress these\nembeddings into a two-dimensional representation, you would notice that words\nwith similar meaning tend to be closer. In Chapter 5 , we will explore how to compress\nthese embeddings into n-dimensional space.\nFigure 1-9. Embeddings of words that are similar will be close to each other in dimen‐\nsional space.\nTypes of Embeddings\nThere are many types of embeddings, like word embeddings and sentence embed‐\ndings that are used to indicate different levels of abstractions (word versus sentence),\nas illustrated in Figure 1-10 .\nBag-of-words, for instance, creates embeddings at a document level since it repre‐\nsents the entire document. In contrast, word2vec generates embeddings for words\nonly.\nThroughout the book, embeddings will take on a central role as they are utilized in\nmany use cases, such as classification (see Chapter 4 ), clustering (see Chapter 5 ), and\nsemantic search and retrieval-augmented generation (see Chapter 8 ). In Chapter 2 ,\nwe will take our first deep dive into token embeddings.\n10 | Chapter 1: An Introduction to Large Language Models\nFigure 1-10. Embeddings can be created for different  types of input.\nEncoding and Decoding Context with Attention\nThe training  process of word2vec creates static, downloadable representations of\nwords. For instance, the word “bank” will always have the same embedding regardless\nof the context in which it is used. However, “bank” can refer to both a financial bank\nas well as the bank of a river. Its meaning, and therefore its embeddings, should\nchange depending on the context.\nA step in encoding this text was achieved through recurrent neural networks (RNNs).\nThese are variants of neural networks that can model sequences as an additional\ninput.\nTo do so, these RNNs are used for two tasks, encoding  or representing an input\nsentence and decoding  or generating an output sentence. Figure 1-11  illustrates this\nconcept by showing how a sentence like “I love llamas” gets translated to the Dutch\n“Ik hou van lama’s. ”\nA Recent History of Language AI | 11\nFigure 1-11. Two recurrent neural networks (decoder and encoder) translating an input\nsequence from English to Dutch.\nEach step in this architecture is autoregressive . When generating the next word,\nthis architecture needs to consume all previously generated words, as shown in\nFigure 1-12 .\nFigure 1-12. Each previous output token is used as input to generate the next token.\nThe encoding step aims to represent the input as well as possible, generating the\ncontext in the form of an embedding, which serves as the input for the decoder. To\ngenerate this representation, it takes embeddings as its inputs for words, which means\nwe can use word2vec for the initial representations. In Figure 1-13 , we can observe\nthis process. Note how the inputs are processed sequentially, one at a time, as well as\nthe output.\n12 | Chapter 1: An Introduction to Large Language Models\n4Dzmitry Bahdanau, Kyunghyun Cho, and Y oshua Bengio. “Neural machine translation by jointly learning to\nalign and translate. ” arXiv preprint arXiv:1409.0473  (2014).\nFigure 1-13. Using word2vec embeddings, a context embedding is generated that repre‐\nsents the entire sequence.\nThis context embedding, however, makes it difficult to deal with longer sentences\nsince it is merely a single embedding representing the entire input. In 2014, a solution\ncalled attention  was introduced that highly improved upon the original architecture.4\nAttention allows a model to focus on parts of the input sequence that are relevant\nto one another (“attend” to each other) and amplify their signal, as shown in Fig‐\nure 1-14 . Attention selectively determines which words are most important in a given\nsentence.\nFor instance, the output word “lama’s” is Dutch for “llamas, ” which is why the atten‐\ntion between both is high. Similarly, the words “lama’s” and “I” have lower attention\nsince they aren’t as related. In Chapter 3 , we will go more in depth on the attention\nmechanism.\nA Recent History of Language AI | 13\nFigure 1-14. Attention allows a model to “attend” to certain parts of sequences that\nmight relate more or less to one another.\nBy adding these attention mechanisms to the decoder step, the RNN can generate\nsignals for each input word in the sequence related to the potential output. Instead of\npassing only a context embedding to the decoder, the hidden states of all input words\nare passed. This process is demonstrated in Figure 1-15 .\nFigure 1-15. After  generating the words “Ik, ” “hou, ” and “van, ” the attention mechanism\nof the decoder enables it to focus on the word “llamas” before it generates the Dutch\ntranslation (“lama’s”).\nAs a result, during the generation of “Ik hou van lama’s, ” the RNN keeps track of\nthe words it mostly attends to perform the translation. Compared to word2vec, this\narchitecture allows for representing the sequential nature of text and the context\nin which it appears by “attending” to the entire sentence. This sequential nature,\nhowever, precludes parallelization during training of the model.\n14 | Chapter 1: An Introduction to Large Language Models",5428
14-Attention Is All You Need.pdf,14-Attention Is All You Need,"5Ashish Vaswani et al. “ Attention is all you need. ” Advances in Neural Information Processing Systems  30 (2017).Attention Is All You Need\nThe true power of attention, and what drives the amazing abilities of large lan‐\nguage models, was first explored in the well-known “ Attention is all you need”\npaper  released in 2017.5 The authors proposed a network architecture called the\nTransformer , which was solely based on the attention mechanism and removed the\nrecurrence network that we saw previously. Compared to the recurrence network, the\nTransformer could be trained in parallel, which tremendously sped up training.\nIn the Transformer, encoding and decoder components are stacked on top of each\nother, as illustrated in Figure 1-16 . This architecture remains autoregressive, needing\nto consume each generated word before creating a new word.\nFigure 1-16. The Transformer is a combination of stacked encoder and decoder blocks\nwhere the input flows  through each encoder and decoder.\nNow, both the encoder and decoder blocks would revolve around attention instead\nof leveraging an RNN with attention features. The encoder block in the Transformer\nconsists of two parts, self-attention  and a feedforward neural network , which are\nshown in Figure 1-17 .\nA Recent History of Language AI | 15\nFigure 1-17. An encoder block revolves around self-attention to generate intermediate\nrepresentations.\nCompared to previous methods of attention, self-attention can attend to different\npositions within a single sequence, thereby more easily and accurately representing\nthe input sequence as illustrated in Figure 1-18 . Instead of processing one token at a\ntime, it can be used to look at the entire sequence in one go.\nFigure 1-18. Self-attention attends to all parts of the input sequence so that it can “look”\nboth forward and back in a single sequence.\nCompared to the encoder, the decoder has an additional layer that pays attention to\nthe output of the encoder (to find the relevant parts of the input). As demonstrated\nin Figure 1-19 , this process is similar to the RNN attention decoder that we discussed\npreviously.\n16 | Chapter 1: An Introduction to Large Language Models\nFigure 1-19. The decoder has an additional attention layer that attends to the output of\nthe encoder.\nAs shown in Figure 1-20 , the self-attention layer in the decoder masks future posi‐\ntions so it only attends to earlier positions to prevent leaking information when\ngenerating the output.\nFigure 1-20. Only attend to previous tokens to prevent “looking into the future. ”\nTogether, these building blocks create the Transformer architecture and are the foun‐\ndation of many impactful models in Language AI, such as BERT and GPT-1, which\nwe cover later in this chapter. Throughout this book, most models that we will use are\nTransformer-based models.\nA Recent History of Language AI | 17",2911
15-Representation Models Encoder-Only Models.pdf,15-Representation Models Encoder-Only Models,"6Jacob Devlin et al. “BERT: Pre-training of deep bidirectional transformers for language understanding. ” arXiv\npreprint arXiv:1810.04805  (2018).There is much more to the Transformer architecture than what we explored thus\nfar. In Chapters 2 and 3, we will go through the many reasons why Transformer\nmodels work so well, including multi-head attention, positional embeddings, and\nlayer normalization.\nRepresentation Models: Encoder-Only Models\nThe original Transformer model is an encoder-decoder architecture that serves trans‐\nlation tasks well but cannot easily be used for other tasks, like text classification.\nIn 2018, a new architecture called  Bidirectional Encoder Representations from Trans‐\nformers (BERT) was introduced that could be leveraged for a wide variety of tasks\nand would serve as the foundation of Language AI for years to come.6 BERT is\nan encoder-only architecture that focuses on representing language, as illustrated\nin Figure 1-21 . This means that it only uses the encoder and removes the decoder\nentirely.\nFigure 1-21. The architecture of a BERT base model with 12 encoders.\nThese encoder blocks are the same as we saw before: self-attention followed by\nfeedforward neural networks. The input contains an additional token, the [CLS]  or\nclassification token, which is used as the representation for the entire input. Often,\nwe use this [CLS]  token as the input embedding for fine-tuning the model on specific\ntasks, like classification.\n18 | Chapter 1: An Introduction to Large Language Models\nTraining these encoder stacks can be a difficult task that BERT approaches by adopt‐\ning a technique called masked language modeling  (see Chapters 2 and 11). As shown\nin Figure 1-22 , this method masks a part of the input for the model to predict. This\nprediction task is difficult but allows BERT to create more accurate (intermediate)\nrepresentations of the input.\nFigure 1-22. Train a BERT model by using masked language modeling.\nThis architecture and training procedure makes BERT and related architectures\nincredible at representing contextual language. BERT-like models are commonly used\nfor transfer learning,  which involves first pretraining it for language modeling and\nthen fine-tuning it for a specific task. For instance, by training BERT on the entirety\nof Wikipedia, it learns to understand the semantic and contextual nature of text.\nThen, as shown in Figure 1-23 , we can use that pretrained  model to fine-tune  it for a\nspecific task, like text classification.\nFigure 1-23. After  pretraining BERT on masked language model, we fine-tune  it for\nspecific  tasks.\nA huge benefit of pretrained models is that most of the training is already done for\nus. Fine-tuning on specific tasks is generally less compute-intensive and requires less\ndata. Moreover, BERT-like models generate embeddings at almost every step in their\nA Recent History of Language AI | 19",2941
16-Generative Models Decoder-Only Models.pdf,16-Generative Models Decoder-Only Models,"7Alec Radford et al. “Improving language understanding by generative pre-training” , (2018).\n8Alec Radford et al. “Language models are unsupervised multitask learners. ” OpenAI Blog  1.8 (2019): 9.\n9Tom Brown et al. “Language models are few-shot learners. ” Advances in Neural Information Processing Systems\n33 (2020): 1877–1901.architecture. This also makes BERT models feature extraction machines without the\nneed to fine-tune them on a specific task.\nEncoder-only models, like BERT, will be used in many parts of the book. For years,\nthey have been and are still used for common tasks, including classification tasks (see\nChapter 4 ), clustering tasks (see Chapter 5 ), and semantic search (see Chapter 8 ).\nThroughout the book, we will refer to encoder-only models as representation models\nto differentiate them from decoder-only, which we refer to as generative models .\nNote that the main distinction does not lie between the underlying architecture and\nthe way these models work. Representation models mainly focus on representing\nlanguage, for instance, by creating embeddings, and typically do not generate text. In\ncontrast, generative models focus primarily on generating text and typically are not\ntrained to generate embeddings.\nThe distinction between representation and generative models and components will\nalso be shown in most images. Representation models are teal with a small vector\nicon (to indicate its focus on vectors and embeddings) whilst generative models are\npink with a small chat icon (to indicate its generative capabilities).\nGenerative Models: Decoder-Only Models\nSimilar  to the encoder-only architecture of BERT, a decoder-only architecture was\nproposed in 2018 to target generative tasks.7 This architecture was called a Generative\nPre-trained Transformer (GPT) for its generative capabilities (it’s now known as\nGPT-1 to distinguish it from later versions). As shown in Figure 1-24 , it stacks\ndecoder blocks similar to the encoder-stacked architecture of BERT.\nGPT-1 was trained on a corpus of 7,000 books and Common Crawl, a large dataset of\nweb pages. The resulting model consisted of 117 million parameters . Each parameter\nis a numerical value that represents the model’s understanding of language.\nIf everything remains the same, we expect more parameters to greatly influence the\ncapabilities and performance of language models. Keeping this in mind, we saw larger\nand larger models being released at a steady pace. As illustrated in Figure 1-25 , GPT-2\nhad 1.5 billion parameters8 and GPT-3 used 175 billion parameters9 quickly followed.\n20 | Chapter 1: An Introduction to Large Language Models\nFigure 1-24. The architecture of a GPT-1. It uses a decoder-only architecture and\nremoves the encoder-attention block.\nFigure 1-25. GPT models quickly grew in size with each iteration.\nA Recent History of Language AI | 21\nThese generative decoder-only models, especially the “larger” models, are commonly\nreferred to as large language models  (LLMs). As we will discuss later in this chapter,\nthe term LLM is not only reserved for generative models (decoder-only) but also\nrepresentation models (encoder-only).\nGenerative LLMs, as sequence-to-sequence machines, take in some text and attempt\nto autocomplete it. Although a handy feature, their true power shone from being\ntrained as a chatbot. Instead of completing a text, what if they could be trained to\nanswer questions? By fine-tuning these models, we can create instruct  or chat models\nthat can follow directions.\nAs illustrated in Figure 1-26 , the resulting model could take in a user query ( prompt )\nand output a response that would most likely follow that prompt. As such, you will\noften hear that generative models are completion  models.\nFigure 1-26. Generative LLMs take in some input and try to complete it. With instruct\nmodels, this is more than just autocomplete and attempts to answer the question.\nA vital part of these completion models is something called the context length  or\ncontext window . The context length represents the maximum number of tokens the\nmodel can process, as shown in Figure 1-27 . A large context window allows entire\ndocuments to be passed to the LLM. Note that due to the autoregressive nature of\nthese models, the current context length will increase as new tokens are generated.\n22 | Chapter 1: An Introduction to Large Language Models",4432
17-The Year of Generative AI.pdf,17-The Year of Generative AI,"10OpenAI, “Gpt-4 technical report. ” arXiv preprint arXiv:2303.08774  (2023).\nFigure 1-27. The context length is the maximum context an LLM can handle.\nThe Year of Generative AI\nLLMs  had a tremendous impact on the field and led some to call 2023 The Year of\nGenerative AI  with the release, adoption, and media coverage of ChatGPT (GPT-3.5).\nWhen we refer to ChatGPT, we are actually talking about the product and not the\nunderlying model. When it was first released, it was powered by the GPT-3.5 LLM\nand has since then grown to include several more performant variants, such as\nGPT-4.10\nGPT-3.5 was not the only model that made its impact in the Y ear of Generative AI.\nAs illustrated in Figure 1-28 , both open source and proprietary LLMs have made\ntheir way to the people at an incredible pace. These open source base models are\noften referred to as foundation models  and can be fine-tuned for specific tasks, like\nfollowing instructions.\nA Recent History of Language AI | 23\n11Albert Gu and Tri Dao. “Mamba: Linear-time sequence modeling with selective state spaces. ” arXiv preprint\narXiv:2312.00752  (2023).\n12See “ A Visual Guide to Mamba and State Space Models”  for an illustrated and visual guide to Mamba as an\nalternative to the Transformer architecture.\n13Bo Peng et al. “RWKV: Reinventing RNNs for the transformer era. ” arXiv preprint arXiv:2305.13048  (2023).\nFigure 1-28. A comprehensive view into the Year of Generative AI. Note that many\nmodels are still missing from this overview!\nApart from the widely popular Transformer architecture, new promising architec‐\ntures have emerged such as Mamba11,12 and RWKV .13 These novel architectures\nattempt to reach Transformer-level performance with additional advantages, like\nlarger context windows or faster inference.\nThese developments exemplify the evolution of the field and showcase 2023 as a truly\nhectic year for AI. It took all we had to just keep up with the many developments,\nboth within and outside of Language AI.\nAs such, this book explores more than just the latest LLMs. We will explore how\nother models, such as embedding models, encoder-only models, and even bag-of-\nwords can be used to empower LLMs.\n24 | Chapter 1: An Introduction to Large Language Models",2275
18-The Moving Definition of a Large Language Model.pdf,18-The Moving Definition of a Large Language Model,,0
19-The Training Paradigm of Large Language Models.pdf,19-The Training Paradigm of Large Language Models,"The Moving Definition  of a “Large Language Model”\nIn our travels through the recent history of Language AI, we observed that primarily\ngenerative decoder-only (Transformer) models are commonly referred to as large\nlanguage models . Especially if they are considered to be “large. ” In practice, this seems\nlike a rather constrained description!\nWhat if we create a model with the same capabilities as GPT-3 but 10 times smaller?\nWould such a model fall outside the “large” language model categorization?\nSimilarly, what if we released a model as big as GPT-4 that can perform accurate\ntext classification but does not have any generative capabilities? Would it still qualify\nas a large “language model” if its primary function is not language generation, even\nthough it still represents text?\nThe problem with these kinds of definitions is that we exclude capable models. What\nname we give one model or the other does not change how it behaves.\nSince the definition of the term “large language model” tends to evolve with the\nrelease of new models, we want to be explicit in what it means for this book. “Large”\nis arbitrary and what might be considered a large model today could be small tomor‐\nrow. There are currently many names for the same thing and to us, “large language\nmodels” are also models that do not generate text and can be run on consumer\nhardware.\nAs such, aside from covering generative models, this book will also cover models with\nfewer than 1 billion parameters that do not generate text. We will explore how other\nmodels, such as embedding models, representation models, and even bag-of-words\ncan be used to empower LLMs.\nThe Training Paradigm of Large Language Models\nTraditional  machine learning generally involves training a model for a specific task,\nlike classification. As shown in Figure 1-29 , we consider this to be a one-step process.\nFigure 1-29. Traditional machine learning involves a single step: training a model for a\nspecific  target task, like classification  or regression.\nThe Training Paradigm of Large Language Models | 25\n14Hugo Touvron et al. “Llama 2: Open foundation and fine-tuned chat models. ” arXiv preprint arXiv:2307.09288\n(2023).Creating LLMs, in contrast, typically consists of at least two steps:\nLanguage modeling\nThe first step, called pretraining , takes the majority of computation and training\ntime. An LLM is trained on a vast corpus of internet text allowing the model\nto learn grammar, context, and language patterns. This broad training phase is\nnot yet directed toward specific tasks or applications beyond predicting the next\nword. The resulting model is often referred to as a foundation model  or base\nmodel . These models generally do not follow instructions.\nFine-tuning\nThe second step , fine-tuning  or sometimes post-training , involves using the previ‐\nously trained model and further training it on a narrower task. This allows the\nLLM to adapt to specific tasks or to exhibit desired behavior. For example, we\ncould fine-tune a base model to perform well on a classification task or to follow\ninstructions. It saves massive amounts of resources because the pretraining phase\nis quite costly and generally requires data and computing resources that are\nout of the reach of most people and organizations. For instance, Llama 2 has\nbeen trained on a dataset containing 2 trillion tokens.14 Imagine the compute\nnecessary to create that model! In Chapter 12 , we will go over several methods\nfor fine-tuning foundation models on your dataset.\nAny model that goes through the first step, pretraining, we consider a pretrained\nmodel,  which also includes fine-tuned models. This two-step approach of training is\nvisualized in Figure 1-30 .\nFigure 1-30. Compared to traditional machine learning, LLM training takes a multistep\napproach.\nAdditional fine-tuning steps can be added to further align the model with the user’s\npreferences, as we will explore in Chapter 12 .\n26 | Chapter 1: An Introduction to Large Language Models",4054
20-Responsible LLM Development and Usage.pdf,20-Responsible LLM Development and Usage,"Large Language Model Applications: What Makes Them\nSo Useful?\nThe nature of LLMs makes them suitable for a wide range of tasks. With text genera‐\ntion and prompting, it almost seems as if your imagination is the limit. To illustrate,\nlet’s explore some common tasks and techniques:\nDetecting whether a review left by a customer is positive or negative\nThis is (supervised) classification and can be handled with both encoder- and\ndecoder-only models either with pretrained models (see Chapter 4 ) or by fine-\ntuning models (see Chapter 11 ).\nDeveloping a system for finding  common topics in ticket issues\nThis is (unsupervised) classification for which we have no predefined labels.\nWe can leverage encoder-only models to perform the classification itself and\ndecoder-only models for labeling the topics (see Chapter 5 ).\nBuilding a system for retrieval and inspection of relevant documents\nA major component of language model systems is their ability to add external\nresources of information. Using semantic search, we can build systems that allow\nus to easily access and find information for an LLM to use (see Chapter 8 ).\nImprove your system by creating or fine-tuning a custom embedding model (see\nChapter 12 ).\nConstructing an LLM chatbot that can leverage external resources, such as tools and\ndocuments\nThis is a combination of techniques that demonstrates how the true power of\nLLMs can be found through additional components. Methods such as prompt\nengineering (see Chapter 6 ), retrieval-augmented generation (see Chapter 8 ), and\nfine-tuning an LLM (see Chapter 12 ) are all pieces of the LLM puzzle.\nConstructing an LLM capable of writing recipes based on a picture showing the products\nin your fridge\nThis is a multimodal task where the LLM takes in an image and reasons about\nwhat it sees (see Chapter 9 ). LLMs are being adapted to other modalities, such as\nVision, which opens a wide variety of interesting use cases.\nLLM applications are incredibly satisfying to create since they are partially bounded\nby the things you can imagine. As these models grow more accurate, using them in\npractice for creative use cases such as role-playing and writing children’s books simply\nbecomes more and more fun.\nLarge Language Model Applications: What Makes Them So Useful? | 27",2319
21-Interfacing with Large Language Models.pdf,21-Interfacing with Large Language Models,"Responsible LLM Development and Usage\nThe impact of LLMs has been and likely continues to be significant due to their wide‐\nspread adoption. As we explore the incredible capabilities of LLMs it is important to\nkeep their societal and ethical implications in mind. Several key points to consider:\nBias and fairness\nLLMs are trained on large amounts of data that might contain biases. LLMs\nmight learn from these biases, start to reproduce them, and potentially amplify\nthem. Since the data on which LLMs are trained are seldom shared, it remains\nunclear what potential biases they might contain unless you try them out.\nTransparency and accountability\nDue to LLMs’ incredible capabilities, it is not always clear when you are talking\nwith a human or an LLM. As such, the usage of LLMs when interacting with\nhumans can have unintended consequences when there is no human in the\nloop. For instance, LLM-based applications used in the medical field might be\nregulated as medical devices since they could affect a patient’s well-being.\nGenerating harmful content\nAn LLM does not necessarily generate ground-truth content and might confi‐\ndently output incorrect text. Moreover, they can be used to generate fake news,\narticles, and other misleading sources of information.\nIntellectual property\nIs the output of an LLM your intellectual property or that of the LLM’s creator?\nWhen the output is similar to a phrase in the training data, does the intellectual\nproperty belong to the author of that phrase? Without access to the training data\nit remains unclear when copyrighted material is being used by the LLM.\nRegulation\nDue to the enormous impact of LLMs, governments are starting to regulate\ncommercial applications. An example is the European AI Act , which regulates\nthe development and deployment of foundation models including LLMs.\nAs you develop and use LLMs, we want to stress the importance of ethical considera‐\ntions and urge you to learn more about the safe and responsible use of LLMs and AI\nsystems in general.\nLimited Resources Are All You Need\nThe compute resources that we have referenced several times thus far generally relate\nto the GPU(s) you have available on your system. A powerful  GPU (graphics card)\nwill make both training and using LLMs much more efficient and faster.\n28 | Chapter 1: An Introduction to Large Language Models",2387
22-Generating Your First Text.pdf,22-Generating Your First Text,"15The models were trained for 3,311,616 GPU hours , which refers to the amount of time it takes to train a\nmodel on a GPU, multiplied by the number of GPUs available.In choosing a GPU, an important component is the amount of VRAM (video\nrandom-access memory) you have available. This refers to the amount of memory\nyou have available on your GPU. In practice, the more VRAM you have the better.\nThe reason for this is that some models simply cannot be used at all if you do not\nhave sufficient VRAM.\nBecause training and fine-tuning LLMs can be an expensive process, GPU-wise, those\nwithout a powerful GPU have often been referred to as the GPU-poor. This illustrates\nthe battle for computing resources to train these huge models. To create the Llama 2\nfamily of models, for example, Meta used A100-80 GB GPUs. Assuming renting such\na GPU would cost $1.50/hr, the total costs of creating these models would exceed\n$5,000,000!15\nUnfortunately, there is no single rule to determine exactly how much VRAM you\nneed for a specific model. It depends on the model’s architecture and size, compres‐\nsion technique, context size, backend for running the model, etc.\nThis book is for the GPU-poor! We will use models that users can run without the\nmost expensive GPU(s) available or a big budget. To do so, we will make all the code\navailable in Google Colab instances. At the time of writing, a free instance of Google\nColab will net you a T4 GPU with 16 GB VRAM, which is the minimum amount of\nVRAM that we suggest.\nInterfacing with Large Language Models\nInterfacing with LLMs is a vital component of not only using them but also develop‐\ning an understanding of their inner workings. Due to the many developments in\nthe field, there has been an abundance of techniques, methods, and packages for\ncommunicating with LLMs. Throughout the book, we intend to explore the most\ncommon techniques for doing so, including using both proprietary (closed source)\nand publicly available open models.\nProprietary, Private Models\nClosed source LLMs are models that do not have their weights and architecture\nshared with the public. They are developed by specific organizations with their\nunderlying code being kept secret. Examples of such models include OpenAI’s GPT-4\nand Anthropic’s Claude. These proprietary models are generally backed by significant\ncommercial support and have been developed and integrated within their services.\nInterfacing with Large Language Models | 29\nY ou can access these models through an interface that communicates with the LLM,\ncalled an API (application programming interface), as illustrated in Figure 1-31 . For\ninstance, to use ChatGPT in Python you can use OpenAI’s package  to interface with\nthe service without directly accessing it.\nFigure 1-31. Closed source LLMs are accessed by an interface (API). As a result, details\nof the LLM itself, including its code and architecture are not shared with the user.\nA huge benefit of proprietary models is that the user does not need to have a strong\nGPU to use the LLM. The provider takes care of hosting and running the model and\ngenerally has more computing available. There is no expertise necessary concerning\nhosting and using the model, which lowers the barrier to entry significantly. More‐\nover, these models tend to be more performant than their open source counterparts\ndue to the significant investment from these organizations.\nA downside to this is that it can be a costly service. The provider manages the risk\nand costs of hosting the LLM, which often translates to a paid service. Moreover,\nsince there is no direct access to the model, there is no method to fine-tune it\nyourself. Lastly, your data is shared with the provider, which is not desirable in many\ncommon use cases, such as sharing patient data.\nOpen Models\nOpen LLMs are models that share their weights and architecture with the public to\nuse. They are still developed by specific organizations but often share their code for\ncreating or running the model locally—with varying levels of licensing that may or\nmay not allow commercial usage of the model. Cohere’s Command R, the Mistral\nmodels, Microsoft’s Phi, and Meta’s Llama models are all examples of open models.\nThere are ongoing discussions as to what truly represents an open\nsource model. For instance, some publicly shared models have a\npermissive commercial license, which means that the model cannot\nbe used for commercial purposes. For many, this is not the true\ndefinition of open source, which states that using these models\nshould not have any restrictions. Similarly, the data on which a\nmodel is trained as well as its source code are seldom shared.\n30 | Chapter 1: An Introduction to Large Language Models\nY ou can download these models and use them on your device as long as you have a\npowerful GPU that can handle these kinds of models, as shown in Figure 1-32 .\nFigure 1-32. Open source LLMs are directly by the user. As a result, details of the LLM\nitself including its code and architecture are shared with the user.\nA major advantage of these local models is that you, the user, have complete control\nover the model. Y ou can use the model without depending on the API connection,\nfine-tune it, and run sensitive data through it. Y ou are not dependent on any service\nand have complete transparency of the processes that lead to the output of the model.\nThis benefit is enhanced by the large communities that enable these processes, such\nas Hugging Face , demonstrating the possibilities of collaborative efforts.\nA downside is that you need powerful hardware to run these models and even more\nwhen training or fine-tuning them. Moreover, it requires specific knowledge to set up\nand use these models (which we will cover throughout this book).\nWe generally prefer using open source models wherever we can. The freedom this\ngives to play around with options, explore the inner workings, and use the model\nlocally arguably provides more benefits than using proprietary LLMs.\nOpen Source Frameworks\nCompared to closed source LLMs, open source LLMs require you to use certain pack‐\nages to run them. In 2023, many different packages and frameworks were released\nthat, each in their own way, interact with and make use of LLMs. Wading through\nhundreds upon hundreds of potentially worthwhile frameworks is not the most\nenjoyable experience.\nAs a result, you might even miss your favorite framework in this book!\nInstead of attempting to cover every LLM framework in existence (there are too\nmany, and they continue to grow in number), we aim to provide you with a solid\nfoundation for leveraging LLMs. The idea is that after reading this book, you can\neasily pick up most other frameworks as they all work in a very similar manner.\nThe intuition that we attempt to realize is an important component of this. If you\nhave an intuitive understanding of not only LLMs but also using them in practice\nwith common frameworks, branching out to others should be a straightforward task.\nInterfacing with Large Language Models | 31\n16Marah Abdin et al. “Phi-3 technical report: A highly capable language model locally on your phone. ” arXiv\npreprint arXiv:2404.14219  (2024).\nMore specifically, we focus on backend packages. These are packages without a\nGUI (graphical user interface) that are created for efficiently loading and running\nany LLM on your device, such as llama.cpp , LangChain , and the core of many\nframeworks, Hugging Face Transformers .\nWe will mostly cover frameworks for interacting with large lan‐\nguage models through code. Although it helps you learn the\nfundamentals of these frameworks, sometimes you just want a\nChatGPT-like interface with a local LLM. Fortunately, there are\nmany incredible frameworks that allow for this. A few examples\ninclude text-generation-webui , KoboldCpp , and LM Studio .\nGenerating Your First Text\nAn important component of using language models is selecting them. The main\nsource for finding and downloading LLMs is the Hugging Face Hub . Hugging Face\nis the organization behind the well-known Transformers package, which for years\nhas driven the development of language models in general. As the name implies, the\npackage was built on top of the transformers  framework  that we discussed in “ A\nRecent History of Language AI” on page 5 .\nAt the time of writing, you will find more than 800,000 models on Hugging Face’s\nplatform for many different purposes, from LLMs and computer vision models to\nmodels that work with audio and tabular data. Here, you can find almost any open\nsource LLM.\nAlthough we will explore all kinds of models throughout this book, let’s start our first\nlines of code with a generative model. The main generative model we use throughout\nthe book is Phi-3-mini, which is a relatively small (3.8 billion parameters) but quite\nperformant model.16 Due to its small size, the model can be run on devices with less\nthan 8 GB of VRAM. If you perform quantization, a type of compression that we will\nfurther discuss in Chapters 7 and 12, you can use even less than 6 GB of VRAM.\nMoreover, the model is licensed under the MIT license, which allows the model to be\nused for commercial purposes without constraints!\nKeep in mind that new and improved LLMs are frequently released. To ensure this\nbook remains current, most examples are designed to work with any LLM. We’ll also\nhighlight different models in the repository associated with this book for you to try\nout.\nLet’s get started! When you use an LLM, two models are loaded:\n32 | Chapter 1: An Introduction to Large Language Models\n•The generative model itself•\n•Its underlying tokenizer•\nThe tokenizer is in charge of splitting the input text into tokens before feeding it to\nthe generative model. Y ou can find the tokenizer and model on the Hugging Face site\nand only need the corresponding IDs to be passed. In this case, we use “microsoft/\nPhi-3-mini-4k-instruct” as the main path to the model.\nWe can use transformers  to load both the tokenizer and model. Note that we assume\nyou have an NVIDIA GPU ( device_map=""cuda"" ) but you can choose a different\ndevice instead. If you do not have access to a GPU you can use the free Google Colab\nnotebooks we made available in the repository of this book:\nfrom transformers  import AutoModelForCausalLM , AutoTokenizer\n# Load model and tokenizer\nmodel = AutoModelForCausalLM .from_pretrained (\n    ""microsoft/Phi-3-mini-4k-instruct"" ,\n    device_map =""cuda"",\n    torch_dtype =""auto"",\n    trust_remote_code =True,\n)\ntokenizer  = AutoTokenizer .from_pretrained (""microsoft/Phi-3-mini-4k-instruct"" )\nRunning the code will start downloading the model and depending on your internet\nconnection can take a couple of minutes.\nAlthough we now have enough to start generating text, there is a nice trick in trans‐\nformers that simplifies the process, namely transformers.pipeline . It encapsulates\nthe model, tokenizer, and text generation process into a single function:\nfrom transformers  import pipeline\n# Create a pipeline\ngenerator  = pipeline (\n    ""text-generation"" ,\n    model=model,\n    tokenizer =tokenizer ,\n    return_full_text =False,\n    max_new_tokens =500,\n    do_sample =False\n)\nThe following parameters are worth mentioning:\nreturn_full_text\nBy setting this to False , the prompt will not be returned but merely the output of\nthe model.\nGenerating Your First Text | 33",11528
23-Summary.pdf,23-Summary,"max_new_tokens\nThe maximum number of tokens the model will generate. By setting a limit, we\nprevent long and unwieldy output as some models might continue generating\noutput until they reach their context window.\ndo_sample\nWhether the model uses a sampling strategy to choose the next token. By setting\nthis to False , the model will always select the next most probable token. In\nChapter 6 , we explore several sampling parameters that invoke some creativity in\nthe model’s output.\nTo generate our first text, let’s instruct the model to tell a joke about chickens. To do\nso, we format the prompt in a list of dictionaries where each dictionary relates to an\nentity in the conversation. Our role is that of “user” and we use the “content” key to\ndefine our prompt:\n# The prompt (user input / query)\nmessages  = [\n    {""role"": ""user"", ""content"" : ""Create a funny joke about chickens."" }\n]\n# Generate output\noutput = generator (messages )\nprint(output[0][""generated_text"" ])\nWhy don't chickens like to go to the gym? Because they can't crack the egg-\nsistence of it!\nAnd that is it! The first text generated in this book was a decent joke about chickens.\nSummary\nIn this first chapter of the book, we delved into the revolutionary impact LLMs have\nhad on the Language AI field. It has significantly changed our approach to tasks such\nas translation, classification, summarization, and more. Through a recent history of\nLanguage AI, we explored the fundamentals of several types of LLMs, from a simple\nbag-of-words representation to more complex representations using neural networks.\n34 | Chapter 1: An Introduction to Large Language Models\nWe discussed the attention mechanism as a step toward encoding context within\nmodels, a vital component of what makes LLMs so capable. We touched on two\nmain categories of models that use this incredible mechanism: representation models\n(encoder-only) like BERT and generative models (decoder-only) like the GPT family\nof models. Both categories are considered large language models throughout this\nbook.\nOverall, the chapter provided an overview of the landscape of Language AI, including\nits applications, societal and ethical implications, and the resources needed to run\nsuch models. We ended by generating our first text using Phi-3, a model that will be\nused throughout the book.\nIn the next two chapters, you will learn about some underlying processes. We start by\nexploring tokenization and embeddings in Chapter 2 , two often underestimated but\nvital components of the Language AI field. What follows in Chapter 3  is an in-depth\nlook into language models where you will discover the precise methods used for\ngenerating text.\nSummary | 35",2731
24-LLM Tokenization.pdf,24-LLM Tokenization,"CHAPTER 2\nTokens and Embeddings\nTokens and embeddings are two of the central concepts of using large language\nmodels (LLMs). As we’ve seen in the first chapter, they’re not only important to\nunderstanding the history of Language AI, but we cannot have a clear sense of how\nLLMs work, how they’re built, and where they will go in the future without a good\nsense of tokens and embeddings, as we can see in Figure 2-1 .\nFigure 2-1. Language models deal with text in small chunks called tokens. For the lan‐\nguage model to compute language, it needs to turn tokens into numeric representations\ncalled embeddings.\nIn this chapter, we look more closely at what tokens are and the tokenization meth‐\nods used to power LLMs. We will then dive into the famous word2vec embedding\nmethod that preceded modern-day LLMs and see how it’s extending the concept\nof token embeddings to build commercial recommendation systems that power a\nlot of the apps you use. Finally, we go from token embeddings into sentence  or\n37",1019
25-Downloading and Running an LLM.pdf,25-Downloading and Running an LLM,"text embeddings, where a whole sentence or document can have one vector that\nrepresents it—enabling applications like semantic search and topic modeling that we\nsee in Part II of this book.\nLLM Tokenization\nThe way the majority of people interact with language models, at the time of this\nwriting, is through a web playground that presents a chat interface between the user\nand a language model. Y ou may notice that a model does not produce its output\nresponse all at once; it actually generates one token at a time.\nBut tokens aren’t only the output of a model, they’re also the way in which the model\nsees its inputs. A text prompt sent to the model is first broken down into tokens, as\nwe’ll now see.\nHow Tokenizers Prepare the Inputs to the Language Model\nViewed  from the outside, generative LLMs take an input prompt and generate a\nresponse, as we can see in Figure 2-2 .\nFigure 2-2. High-level view of a language model and its input prompt.\nBefore the prompt is presented to the language model, however, it first has to go\nthrough a tokenizer that breaks it into pieces. Y ou can find an example showing the\ntokenizer of GPT-4 on the OpenAI Platform . If we feed it the input text, it shows the\noutput in Figure 2-3 , where each token is shown in a different color.\n38 | Chapter 2: Tokens and Embeddings\nFigure 2-3. A tokenizer breaks down text into words or parts of words before the model\nprocesses the text. It does so according to a specific  method and training procedure (from\nhttps://oreil.ly/ovUWO ).\nLet’s look at a code example and interact with these tokens ourselves. Here we’ll be\ndownloading an LLM and seeing how to tokenize the input before generating text\nwith the LLM.\nDownloading and Running an LLM\nLet’s start by loading our model and its tokenizer as we’ve done in Chapter 1 :\nfrom transformers  import AutoModelForCausalLM , AutoTokenizer\n# Load model and tokenizer\nmodel = AutoModelForCausalLM .from_pretrained (\n    ""microsoft/Phi-3-mini-4k-instruct"" ,\n    device_map =""cuda"",\n    torch_dtype =""auto"",\n    trust_remote_code =True,\n)\ntokenizer  = AutoTokenizer .from_pretrained (""microsoft/Phi-3-mini-4k-instruct"" )\nLLM Tokenization | 39\nWe can then proceed to the actual generation. We first declare our prompt, then\ntokenize it, then pass those tokens to the model, which generates its output. In this\ncase, we’re asking the model to only generate 20 new tokens:\nprompt = ""Write an email apologizing to Sarah for the tragic gardening mishap. \nExplain how it happened.<|assistant|>""\n# Tokenize the input prompt\ninput_ids  = tokenizer (prompt, return_tensors =""pt"").input_ids .to(""cuda"")\n# Generate the text\ngeneration_output  = model.generate (\n  input_ids =input_ids ,\n  max_new_tokens =20\n)\n# Print the output\nprint(tokenizer .decode(generation_output [0]))\nOutput:\n<s> Write an email apologizing to Sarah for the tragic gardening mishap. \nExplain how it happened.<|assistant|> Subject: My Sincere Apologies for the \nGardening Mishap\nDear\nThe text in bold is the 20 tokens generated by the model.\nLooking at the code, we can see that the model does not in fact receive the text\nprompt. Instead, the tokenizers processed the input prompt, and returned the infor‐\nmation the model needed in the variable input_ids , which the model used as its\ninput.\nLet’s print input_ids  to see what it holds inside:\ntensor([[ 1, 14350, 385, 4876, 27746, 5281, 304, 19235, 363, 278, 25305, 293, \n16423, 292, 286, 728, 481, 29889, 12027, 7420, 920, 372, 9559, 29889, 32001]], \ndevice='cuda:0')\nThis reveals the inputs that LLMs respond to, a series of integers as shown in\nFigure 2-4 . Each one is the unique ID for a specific token (character, word, or part of\na word). These IDs reference a table inside the tokenizer containing all the tokens it\nknows.\n40 | Chapter 2: Tokens and Embeddings\nFigure 2-4. A tokenizer processes the input prompt and prepares the actual input into\nthe language model: a list of token IDs. The specific  token IDs in the figure  are just\ndemonstrative.\nIf we want to inspect those IDs, we can use the tokenizer’s decode method to translate\nthe IDs back into text that we can read:\nfor id in input_ids [0]:\n   print(tokenizer .decode(id))\nThis prints (each token is on a separate line):\n<s>\nWrite\nan\nemail\napolog\nizing\nto\nSarah\nfor\nthe\ntrag\nic\ngarden\ning\nm\nish\nLLM Tokenization | 41\nap\n.\nExp\nlain\nhow\nit\nhappened\n.\n<|assistant|>\nThis is how the tokenizer broke down our input prompt. Notice the following:\n•The first token is ID 1 ( <s>), a special token indicating the beginning of the text. •\n•Some tokens are complete words (e.g., Write , an, email ). •\n•Some tokens are parts of words (e.g., apolog , izing , trag, ic). •\n•Punctuation characters are their own token.•\nNotice how the space character does not have its own token. Instead, partial tokens\n(like “izing” and “ic”) have a special hidden character at their beginning that indicates\nthat they’re connected with the token that precedes them in the text. Tokens without\nthat special character are assumed to have a space before them.\nOn the output side, we can also inspect the tokens generated by the model by printing\nthe generation_output  variable. This shows the input tokens as well as the output\ntokens (we’ll highlight the new tokens in bold):\ntensor([[ 1, 14350, 385, 4876, 27746, 5281, 304, 19235, 363, 278,\n25305, 293, 16423, 292, 286, 728, 481, 29889, 12027, 7420,\n920, 372, 9559, 29889, 32001, 3323, 622, 29901, 1619, 317,\n3742, 406, 6225, 11763, 363, 278, 19906, 292, 341, 728,\n481, 13, 13, 29928, 799]], device='cuda:0')\nThis shows us the model generated the token 3323, 'Sub' , followed by token 622,\n'ject' . Together they formed the word 'Subject' . They were then followed by\ntoken 29901, which is the colon ':'...and so on. Just like on the input side, we need\nthe tokenizer on the output side to translate the token ID into the actual text. We do\nthat using the tokenizer’s decode method. We can pass it an individual token ID or a\nlist of them:\n42 | Chapter 2: Tokens and Embeddings",6140
26-Word Versus Subword Versus Character Versus Byte Tokens.pdf,26-Word Versus Subword Versus Character Versus Byte Tokens,"print(tokenizer .decode(3323))\nprint(tokenizer .decode(622))\nprint(tokenizer .decode([3323, 622]))\nprint(tokenizer .decode(29901))\nThis outputs:\nSub\nject\nSubject\n:\nHow Does the Tokenizer Break Down Text?\nThere  are three major factors that dictate how a tokenizer breaks down an input\nprompt.\nFirst, at model design time, the creator of the model chooses a tokenization method.\nPopular methods include byte pair encoding (BPE) (widely used by GPT models) and\nWordPiece (used by BERT). These methods are similar in that they aim to optimize\nan efficient set of tokens to represent a text dataset, but they arrive at it in different\nways.\nSecond, after choosing the method, we need to make a number of tokenizer design\nchoices like vocabulary size and what special tokens to use. More on this in “Compar‐\ning Trained LLM Tokenizers” on page 46 .\nThird, the tokenizer needs to be trained on a specific dataset to establish the best\nvocabulary it can use to represent that dataset. Even if we set the same methods\nand parameters, a tokenizer trained on an English text dataset will be different from\nanother trained on a code dataset or a multilingual text dataset.\nIn addition to being used to process the input text into a language model, tokenizers\nare used on the output of the language model to turn the resulting token ID into the\noutput word or token associated with it, as Figure 2-5  shows.\nLLM Tokenization | 43\nFigure 2-5. Tokenizers are also used to process the output of the model by converting the\noutput token ID into the word or token associated with that ID.\nWord Versus Subword Versus Character Versus Byte Tokens\nThe tokenization scheme we just discussed is called subword tokenization . It’s the\nmost commonly used tokenization scheme but not the only one. The four notable\nways to tokenize are shown in Figure 2-6 . Let’s go over them:\nWord tokens\nThis  approach was common with earlier methods like word2vec but is being\nused less and less in NLP . Its usefulness, however, led it to be used outside of NLP\nfor use cases such as recommendation systems, as we’ll see later in the chapter.\nOne challenge with word tokenization is that the tokenizer may be unable to\ndeal with new words that enter the dataset after the tokenizer was trained. This\nalso results in a vocabulary that has a lot of tokens with minimal differences\nbetween them (e.g., apology, apologize, apologetic, apologist). This latter chal‐\nlenge is resolved by subword tokenization as it has a token for apolog,  and then\nsuffix tokens (e.g., -y, -ize, -etic, -ist) that are common with many other tokens,\nresulting in a more expressive vocabulary.\nSubword tokens\nThis  method contains full and partial words. In addition to the vocabulary\nexpressivity mentioned earlier, another benefit of the approach is its ability to\nrepresent new words by breaking down the new token into smaller characters,\nwhich tend to be a part of the vocabulary.\n44 | Chapter 2: Tokens and Embeddings\nFigure 2-6. There  are multiple methods of tokenization that break down the text to\ndifferent  sizes of components (words, subwords, characters, and bytes).\nCharacter tokens\nThis  is another method that can deal successfully with new words because it\nhas the raw letters to fall back on. While that makes the representation easier\nto tokenize, it makes the modeling more difficult. Where a model with subword\ntokenization can represent “play” as one token, a model using character-level\ntokens needs to model the information to spell out “p-l-a-y” in addition to\nmodeling the rest of the sequence.\nSubword tokens present an advantage over character tokens in the ability to fit\nmore text within the limited context length of a Transformer model. So with a\nmodel with a context length of 1,024, you may be able to fit about three times\nas much text using subword tokenization than using character tokens (subword\ntokens often average three characters per token).\nByte tokens\nOne  additional tokenization method breaks down tokens into the individual\nbytes that are used to represent unicode characters. Papers like “CANINE: Pre-\ntraining an efficient tokenization-free encoder for language representation”  out‐\nline methods like this, which are also called “tokenization-free encoding. ” Other\nworks like “ByT5: Towards a token-free future with pre-trained byte-to-byte\nLLM Tokenization | 45",4425
27-Comparing Trained LLM Tokenizers.pdf,27-Comparing Trained LLM Tokenizers,"models”  show that this can be a competitive method, especially in multilingual\nscenarios.\nOne distinction to highlight here: some subword tokenizers also include bytes as\ntokens in their vocabulary as the final building block to fall back to when they\nencounter characters they can’t otherwise represent. The GPT-2 and RoBERTa token‐\nizers do this, for example. This doesn’t make them tokenization-free byte-level token‐\nizers, because they don’t use these bytes to represent everything, only a subset, as we’ll\nsee in the next section.\nIf you want to go deeper into tokenizers, they are discussed in more detail  in Design‐\ning Large Language Model Applications .\nComparing Trained LLM Tokenizers\nWe’ve  pointed out earlier three major factors that dictate the tokens that appear\nwithin a tokenizer: the tokenization method, the parameters and special tokens we\nuse to initialize the tokenizer, and the dataset the tokenizer is trained on. Let’s\ncompare and contrast a number of actual, trained tokenizers to see how these choices\nchange their behavior. This comparison will show us that newer tokenizers have\nchanged their behavior to improve model performance, and we’ll also see how spe‐\ncialized models (like code generation models, for example) often need specialized\ntokenizers.\nWe’ll use a number of tokenizers to encode the following text:\ntext = """"""\nEnglish and CAPITALIZATION\n🎵鸟\nshow_tokens False None elif == >= else: two tabs:"" "" Three tabs: ""   ""\n12.0*50=600\n""""""\nThis will allow us to see how each tokenizer deals with a number of different kinds of\ntokens:\n•Capitalization.•\n•Languages other than English.•\n•Emojis.•\n•Programming code with keywords and whitespaces often used for indentation•\n(in languages like Python for example).\n46 | Chapter 2: Tokens and Embeddings\n•Numbers and digits.•\n•Special tokens. These are unique tokens that have a role other than representing•\ntext. They include tokens that indicate the beginning of the text, or the end of the\ntext (which is the way the model signals to the system that it has completed this\ngeneration), or other functions as we’ll see.\nLet’s go from older to newer tokenizers to see how they tokenize this text and what\nthat might say about the language model. We’ll tokenize the text, and then print each\ntoken with a color background color using this function:\ncolors_list  = [\n    '102;194;165' , '252;141;98' , '141;160;203' , \n    '231;138;195' , '166;216;84' , '255;217;47'\n]\ndef show_tokens (sentence , tokenizer_name ):\n    tokenizer  = AutoTokenizer .from_pretrained (tokenizer_name )\n    token_ids  = tokenizer (sentence ).input_ids\n    for idx, t in enumerate (token_ids ):\n        print(\n            f '\x1b[0;30;48;2; {colors_list [idx % len(colors_list )]}m' + \n            tokenizer .decode(t) + \n            '\x1b[0m', \n            end=' '\n        )\nBERT base model (uncased) (2018)\nLink to the model on the HuggingFace model hub\nTokenization method: WordPiece, introduced in “Japanese and Korean voice search” :\nVocabulary size: 30,522\nSpecial tokens:\nunk_token [UNK]\nAn unknown token that the tokenizer has no specific encoding for.\nsep_token [SEP]\nA separator that enables certain tasks that require giving the model two texts (in\nthese cases, the model is called a cross-encoder). One example is reranking, as\nwe’ll see in Chapter 8 .\npad_token [PAD]\nA padding token used to pad unused positions in the model’s input (as the model\nexpects a certain length of input, its context-size).\nLLM Tokenization | 47\ncls_token [CLS]\nA special classification token for classification tasks, as we’ll see in Chapter 4 .\nmask_token [MASK]\nA masking token used to hide tokens during the training process.\nTokenized text:\n[CLS]  english  and capital  ##ization  [UNK]  [UNK]  show  _ token  ##s false  none\neli ##f = = > = else  : two tab ##s : "" "" three  tab ##s : "" "" 12 . 0 * 50 = 600 [SEP]\nBERT was released in two major flavors: cased (where the capitalization is kept) and\nuncased (where all capital letters are first turned into small cap letters). With the\nuncased (and more popular) version of the BERT tokenizer, we notice the following:\n•The newline breaks are gone, which makes the model blind to information•\nencoded in newlines (e.g., a chat log when each turn is in a new line).\n•All the text is in lowercase.•\n•The word “capitalization” is encoded as two subtokens: capital  ##ization . The •\n## characters are used to indicate this token is a partial token connected to the\ntoken that precedes it. This is also a method to indicate where the spaces are, as it\nis assumed tokens without ## in front have a space before them.\n•The emoji and Chinese characters are gone and replaced with the [UNK]  special •\ntoken indicating an “unknown token. ”\nBERT base model (cased) (2018)\nLink to the model on the HuggingFace model hub\nTokenization method: WordPiece\nVocabulary size: 28,996\nSpecial tokens: Same as the uncased version\nTokenized text:\n[CLS]  English  and CA ##PI  ##TA  ##L ##I ##Z ##AT  ##ION  [UNK]  [UNK]  show  _ token\n##s F ##als  ##e None  el ##if  = = > = else  : two ta ##bs  : "" "" Three  ta ##bs  : "" ""\n12 . 0 * 50 = 600 [SEP]\nThe cased version of the BERT tokenizer differs mainly in including uppercase\ntokens.\n•Notice how “CAPITALIZATION” is now represented as eight tokens: CA ##PI •\n##TA  ##L ##I ##Z ##AT  ##ION .\n48 | Chapter 2: Tokens and Embeddings\n•Both BERT tokenizers wrap the input within a starting [CLS]  token and a closing •\n[SEP]  token. [CLS]  and [SEP]  are utility tokens used to wrap the input text\nand they serve their own purposes. [CLS]  stands for classification as it’s a token\nused at times for sentence classification. [SEP]  stands for separator, as it’s used to\nseparate sentences in some applications that require passing two sentences to a\nmodel (For example, in Chapter 8 , we will use a [SEP]  token to separate the text\nof the query and a candidate result.)\nGPT-2 (2019)\nLink to the model on the HuggingFace model hub\nTokenization  method: Byte pair encoding (BPE), introduced in “Neural machine\ntranslation of rare words with subword units” .\nVocabulary size: 50,257\nSpecial tokens: <|endoftext|>\nEnglish  and CAP ITAL  IZ ATION\n� � � � � �\nshow  _ t ok ens False  None  el if == >= else  : two tabs  :"" "" Three  tabs  : "" ""\n12 . 0 * 50 = 600\nWith the GPT-2 tokenizer, we notice the following:\n•The newline breaks are represented in the tokenizer.•\n•Capitalization is preserved, and the word “CAPITALIZATION” is represented in•\nfour tokens.\n•The 🎵鸟 characters are now represented by multiple tokens each. While we see •\nthese tokens printed as the � character, they actually stand for different tokens.\nFor example, the 🎵 emoji is broken down into the tokens with token IDs 8582,\n236, and 113. The tokenizer is successful in reconstructing the original character\nfrom these tokens. We can see that by printing tokenizer.decode([8582, 236,\n113]) , which prints out 🎵.\n•The two tabs are represented as two tokens (token number 197 in that vocabu‐•\nlary) and the four spaces are represented as three tokens (number 220) with the\nfinal space being a part of the token for the closing quote character.\n•The two tabs are represented as two tokens (token number 197 in that vocabu‐•\nlary) and the four spaces are represented as three tokens (number 220) with the\nfinal space being a part of the token for the closing quote character.\nLLM Tokenization | 49\nWhat is the significance of whitespace characters? These are\nimportant for models to understand or generate code. A model\nthat uses a single token to represent four consecutive whitespace\ncharacters is more tuned to a Python code dataset. While a model\ncan live with representing it as four different tokens, it does make\nthe modeling more difficult as the model needs to keep track of\nthe indentation level, which often leads to worse performance. This\nis an example of where tokenization choices can help the model\nimprove on a certain task.\nFlan-T5 (2022)\nTokenization method: Flan-T5  uses a tokenizer implementation called SentencePiece,\nintroduced in “SentencePiece: A simple and language independent subword tokenizer\nand detokenizer for neural text processing” , which supports BPE and the unigram\nlanguage model  (described in “Subword regularization: Improving neural network\ntranslation models with multiple subword candidates” ).\nVocabulary size: 32,100\nSpecial tokens:\n•unk_token <unk>•\n•pad_token <pad>•\nTokenized text:\nEnglish  and CA PI TAL IZ ATION  <unk>  <unk>  show  _ to ken s Fal s e None  e l if = = >\n= else  : two tab s : "" "" Three  tab s : "" "" 12. 0 * 50 = 600 </s>\nThe Flan-T5 family of models use  the SentencePiece method. We notice the\nfollowing:\n•No newline or whitespace tokens; this would make it challenging for the model•\nto work with code.\n•The emoji and Chinese characters are both replaced by the <unk>  token, making •\nthe model completely blind to them.\nGPT-4 (2023)\nTokenization method: BPE\nVocabulary size: A little over 100,000\nSpecial tokens:\n50 | Chapter 2: Tokens and Embeddings\n•<|endoftext|>•\n•Fill in the middle tokens. These three tokens enable the LLM to generate a•\ncompletion given not only the text before it but also considering the text after\nit. This method is explained in more detail in the paper “Efficient training of\nlanguage models to fill in the middle” ; its exact details are beyond the scope of\nthis book. These special tokens are:\n—<|fim_prefix|>—\n—<|fim_middle|>—\n—<|fim_suffix|>—\nTokenized text:\nEnglish  and CAPITAL  IZATION\n� � � � � �\nshow  _tokens  False  None  elif  == >= else  : two tabs  :""  "" Three  tabs  : ""  ""\n12 . 0 * 50 = 600\nThe GPT-4 tokenizer behaves similarly to its ancestor, the GPT-2 tokenizer. Some\ndifferences are:\n•The GPT-4 tokenizer represents the four spaces as a single token. In fact, it has a•\nspecific token for every sequence of whitespaces up to a list of 83 whitespaces.\n•The Python keyword elif  has its own token in GPT-4. Both this and the previ‐ •\nous point stem from the model’s focus on code in addition to natural language.\n•The GPT-4 tokenizer uses fewer tokens to represent most words. Examples here•\ninclude “CAPITALIZATION” (two tokens versus four) and “tokens” (one token\nversus three).\n•Refer back to what we said about the GPT-2 tokenizer with regards to the Ł•\ntokens.\nStarCoder2 (2024)\nStarCoder2  is a 15-billion parameter model focused on generating code described in\nthe paper “StarCoder 2 and the stack v2: The next generation” , which continues the\nwork from the original StarCoder described in “StarCoder: May the source be with\nyou!” .\nTokenization method: Byte pair encoding (BPE)\nVocabulary size: 49,152\nExample special tokens:\nLLM Tokenization | 51\n•<|endoftext|>•\n•Fill in the middle tokens:•\n—<fim_prefix>—\n—<fim_middle>—\n—<fim_suffix>—\n—<fim_pad>—\n•When representing code, managing the context is important. One file might•\nmake a function call to a function that is defined in a different file. So the model\nneeds some way of being able to identify code that is in different files in the same\ncode repository, while making a distinction between code in different repos.\nThat’s why StarCoder2 uses special tokens for the name of the repository and the\nfilename:\n—<filename>—\n—<reponame>—\n—<gh_stars>—\nTokenized text:\nEnglish  and CAPITAL  IZATION\n� � � � �\nshow  _ tokens  False  None  elif  == >= else  : two tabs  :""  "" Three  tabs  : ""  ""\n1 2 . 0 * 5 0 = 6 0 0\nThis is an encoder that focuses on code generation:\n•Similar to GPT-4, it encodes the list of whitespaces as a single token.•\n•A major difference here to everything we’ve seen so far is that each digit is•\nassigned its own token (so 600 becomes 6 0 0). The hypothesis here is that this\nwould lead to better representation of numbers and mathematics. In GPT-2, for\nexample, the number 870 is represented as a single token. But 871 is represented\nas two tokens ( 8 and 71). Y ou can intuitively see how that might be confusing to\nthe model and how it represents numbers.\nGalactica\nThe Galactica model  described in “Galactica: A large language model for science”  is\nfocused on scientific knowledge and is trained on many scientific papers, reference\nmaterials, and knowledge bases. It pays extra attention to tokenization that makes it\nmore sensitive to the nuances of the dataset it’s representing. For example, it includes\n52 | Chapter 2: Tokens and Embeddings\nspecial tokens for citations, reasoning, mathematics, amino acid sequences, and DNA\nsequences.\nTokenization method: Byte pair encoding (BPE)\nVocabulary size: 50,000\nSpecial tokens:\n•<s>•\n•<pad>•\n•</s>•\n•<unk>•\n•References: Citations are wrapped within the two special tokens:•\n—[START_REF]—\n—[END_REF]—\n—One example of usage from the paper is: Recurrent neural net —\nworks, long short-term memory [START_REF]Long Short-Term Memory,\nHochreiter[END_REF]\n•Step-by-step reasoning:•\n—<work>  is an interesting token that the model uses for chain-of-thought rea‐ —\nsoning.\nTokenized text:\nEnglish  and CAP ITAL  IZATION\n� � � � � � �\nshow  _ tokens  False  None  elif  == > = else  : two t abs : ""  "" Three  t abs : ""  ""\n1 2 . 0 * 5 0 = 6 0 0\nThe Galactica tokenizer behaves similar to StarCoder2 in that it has code in mind. It\nalso encodes whitespaces in the same way: assigning a single token to sequences of\nwhitespace of different lengths. It differs in that it also does that for tabs, though. So\nfrom all the tokenizers we’ve seen so far, it’s the only one that assigns a single token to\nthe string made up of two tabs ( '\t\t' ).\nPhi-3 (and Llama 2)\nThe Phi-3 model  we look at in this book reuses the tokenizer of Llama 2  yet adds a\nnumber of special tokens.\nLLM Tokenization | 53\nTokenization method: Byte pair encoding (BPE)\nVocabulary size: 32,000\nSpecial tokens:\n•<|endoftext|>•\n•Chat tokens: As chat LLMs rose to popularity in 2023, the conversational nature•\nof LLMs started to be a leading use case. Tokenizers have been adapted to this\ndirection by the addition of tokens that indicate the turns in a conversation and\nthe roles of each speaker. These special tokens include:\n—<|user|>—\n—<|assistant|>—\n—<|system|>—\nWe can now recap our tour by looking at all these examples side by side:\nBERT base\nmodel\n(uncased)[CLS]  english  and  capital  ##ization  [UNK]  [UNK]  show  _ token  ##s  false  none  eli\n##f  = = > = else  : two  tab  ##s  : "" "" three  tab  ##s  : "" "" 12 . 0 * 50 = 600  [SEP]\nBERT base\nmodel (cased)[CLS]  English  and  CA ##PI  ##TA  ##L  ##I  ##Z  ##AT  ##ION  [UNK]  [UNK]  show  _ token\n##s  F ##als  ##e  None  el ##if  = = > = else  : two  ta ##bs  : "" "" Three  ta ##bs  : "" "" 12 .\n0 * 50 = 600  [SEP]\nGPT-2 English  and  CAP  ITAL  IZ ATION\n� � � � � �\nshow  _ t ok ens  False  None  el if == >= else  : two  tabs  :"" "" Three  tabs  : "" ""\n12 . 0 * 50 = 600\nFLAN-T5 English  and  CA PI TAL  IZ ATION  <unk>  <unk>  show  _ to ken  s Fal  s e None  e l if = = >\n= else  : two  tab  s : "" "" Three  tab  s : "" "" 12.  0 * 50 = 600  </s>\nGPT-4 English  and  CAPITAL  IZATION\n� � � � � �\nshow  _tokens  False  None  elif  == >= else  : two  tabs  :""  "" Three  tabs  : ""  ""\n12 . 0 * 50 = 600\nStarCoder English  and  CAPITAL  IZATION\n� � � � �\nshow  _ tokens  False  None  elif  == >= else  : two  tabs  :""  "" Three  tabs  : ""  ""\n1 2 . 0 * 5 0 = 6 0 0\n54 | Chapter 2: Tokens and Embeddings",15570
28-Tokenizer Properties.pdf,28-Tokenizer Properties,"Galactica English  and  CAP  ITAL  IZATION\n� � � � � � �\nshow  _ tokens  False  None  elif  == > = else  : two  t abs  : ""  "" Three  t abs  : ""  ""\n1 2 . 0 * 5 0 = 6 0 0\nPhi-3 and\nLlama 2<s>\nEnglish  and  C AP IT AL IZ ATION\n� � � � � � �\nshow  _ to kens  False  None  elif  == >= else  : two  tabs  :""  "" Three  tabs  : ""  ""\n1 2 . 0 * 5 0 = 6 0 0\nTokenizer Properties\nThe preceding guided tour of trained tokenizers showed a number of ways in which\nactual tokenizers differ from each other. But what determines their tokenization\nbehavior? There are three major groups of design choices that determine how the\ntokenizer will break down text: the tokenization method, the initialization parame‐\nters, and the domain of the data the tokenizer targets.\nTokenization methods\nAs we’ve seen, there are a number of tokenization methods with byte pair encoding\n(BPE)  being the more popular one. Each of these methods outlines an algorithm for\nhow to choose an appropriate set of tokens to represent a dataset. Y ou can find a great\noverview of all these methods on the Hugging Face page that summarizes tokenizers .\nTokenizer parameters\nAfter choosing a tokenization method, an LLM designer needs to make some deci‐\nsions about the parameters of the tokenizer. These include:\nVocabulary size\nHow  many tokens to keep in the tokenizer’s vocabulary? (30K and 50K are often\nused as vocabulary size values, but more and more we’re seeing larger sizes like\n100K.)\nSpecial tokens\nWhat special tokens do we want the model to keep track of? We can add as many\nof these as we want, especially if we want to build an LLM for special use cases.\nCommon choices include:\n•Beginning of text token (e.g., <s>) •\n•End of text token•\n•Padding token•\nLLM Tokenization | 55\n•Unknown token•\n•CLS token•\n•Masking token•\nAside from these, the LLM designer can add tokens that help better model the\ndomain of the problem they’re trying to focus on, as we’ve seen with Galactica’s\n<work>  and [START_REF]  tokens.\nCapitalization\nIn languages such as English, how do we want to deal with capitalization?\nShould we convert everything to lowercase? (Name capitalization often carries\nuseful information, but do we want to waste token vocabulary space on all-caps\nversions of words?)\nThe domain of the data\nEven  if we select the same method and parameters, tokenizer behavior will be differ‐\nent based on the dataset it was trained on (before we even start model training). The\ntokenization methods mentioned previously work by optimizing the vocabulary to\nrepresent a specific dataset. From our guided tour we’ve seen how that has an impact\non datasets like code and multilingual text.\nFor code, for example, we’ve seen that a text-focused tokenizer may tokenize the\nindentation spaces like this (we’ll highlight some tokens in color):\ndef add_numbers(a, b):\n....""""""Add the two numbers `a` and `b`.""""""\n....return a + b\nThis may be suboptimal for a code-focused model. Code-focused models are often\nimproved by making different tokenization choices:\ndef add_numbers(a, b):\n....""""""Add the two numbers `a` and `b`. """"""\n....return  a + b\nThese tokenization choices make the model’s job easier and thus its performance has\na higher probability of improving.\nY ou can find a more detailed tutorial on training tokenizers in the Tokenizers section\nof the Hugging Face course  and in Natural Language Processing with Transformers,\nRevised Edition .\n56 | Chapter 2: Tokens and Embeddings",3513
29-Token Embeddings.pdf,29-Token Embeddings,,0
30-Creating Contextualized Word Embeddings with Language Models.pdf,30-Creating Contextualized Word Embeddings with Language Models,"Token Embeddings\nNow  that we understand tokenization, we have solved one part of the problem of\nrepresenting language to a language model. In this sense, language is a sequence of\ntokens. And if we train a good-enough model on a large-enough set of tokens, it\nstarts to capture the complex patterns that appear in its training dataset:\n•If the training data contains a lot of English text, that pattern reveals itself as a•\nmodel capable of representing and generating the English language.\n•If the training data contains factual information (Wikipedia, for example), the•\nmodel would have the ability to generate some factual information (see the\nfollowing note).\nThe next piece of the puzzle is finding the best numerical representation for these\ntokens that the model can use to calculate and properly model the patterns in the\ntext. These patterns reveal themselves to us as a model’s coherence in a specific\nlanguage, or capability to code, or any of the growing list of capabilities we expect\nfrom language models.\nAs we’ve seen in Chapter 1 , that is what embeddings are. They are the numeric\nrepresentation space utilized to capture the meanings and patterns in language.\nOops: Achieving a good threshold of language coherence and\nbetter-than-average factual generation, however, starts to present a\nnew problem. Some users start to trust the model’s fact generation\nability (e.g., at the beginning of 2023 some language models were\nbeing dubbed “Google killers” ). It didn’t take long for advanced\nusers to recognize that generation models alone aren’t reliable\nsearch engines. This led to the rise of retrieval-augmented genera‐\ntion (RAG), which combines search and LLMs. We cover RAG in\nmore detail in Chapter 8 .\nA Language Model Holds Embeddings for the Vocabulary of Its\nTokenizer\nAfter a tokenizer is initialized and trained, it is then used in the training process of its\nassociated language model. This is why a pretrained language model is linked with its\ntokenizer and can’t use a different tokenizer without training.\nThe language model holds an embedding vector for each token in the tokenizer’s\nvocabulary, as we can see in Figure 2-7 . When we download a pretrained language\nmodel, a portion of the model is this embeddings matrix holding all of these vectors.\nToken Embeddings | 57\nBefore the beginning of the training process, these vectors are randomly initialized\nlike the rest of the model’s weights, but the training process assigns them the values\nthat enable the useful behavior they’re trained to perform.\nFigure 2-7. A language model holds an embedding vector associated with each token in\nits tokenizer.\nCreating Contextualized Word Embeddings with Language Models\nNow  that we’ve covered token embeddings as the input to a language model, let’s\nlook at how language models can create  better token embeddings. This is one of\nthe primary ways to use language models for text representation. This empowers\napplications like named-entity recognition or extractive text summarization (which\nsummarizes a long text by highlighting the most important parts of it, instead of\ngenerating new text as a summary).\nInstead of representing each token or word with a static vector, language models\ncreate contextualized word embeddings (shown in Figure 2-8 ) that represent a word\nwith a different token based on its context. These vectors can then be used by other\nsystems for a variety of tasks. In addition to the text applications we mentioned in\nthe previous paragraph, these contextualized vectors, for example, are what powers\nAI image generation systems like DALL·E, Midjourney, and Stable Diffusion, for\nexample.\n58 | Chapter 2: Tokens and Embeddings\nFigure 2-8. Language models produce contextualized token embeddings that improve on\nraw, static token embeddings.\nLet’s look at how we can generate contextualized word embeddings; the majority of\nthis code should be familiar to you by now:\nfrom transformers  import AutoModel , AutoTokenizer\n# Load a tokenizer\ntokenizer  = AutoTokenizer .from_pretrained (""microsoft/deberta-base"" )\n# Load a language model\nmodel = AutoModel .from_pretrained (""microsoft/deberta-v3-xsmall"" )\n# Tokenize the sentence\ntokens = tokenizer ('Hello world' , return_tensors ='pt')\n# Process the tokens\noutput = model(**tokens)[0]\nThe model we’re using here is called DeBERTa v3, which at the time of writing is one\nof the best-performing language models for token embeddings while being small and\nhighly efficient. It is described in the paper “DeBERTaV3: Improving DeBERTa using\nELECTRA-style pre-training gradient-disentangled embedding sharing” .\nToken Embeddings | 59\nThis code downloads a pretrained tokenizer and model, then uses them to process\nthe string “Hello world” . The output of the model is then saved in the output variable.\nLet’s inspect that variable by first printing its dimensions (we expect it to be a\nmultidimensional array):\noutput.shape\nThis prints out:\ntorch.Size([1, 4, 384])\nSkipping the first dimension, we can read this as four tokens, each one embedded in\na vector of 384 values. The first dimension is the batch dimension used in cases (like\ntraining) when we want to send multiple input sentences to the model at the same\ntime (they’re processed at the same time, which speeds up the process).\nBut what are these four vectors? Did the tokenizer break the two words into four\ntokens, or is something else happening here? We can use what we’ve learned about\ntokenizers to inspect them:\nfor token in tokens['input_ids' ][0]:\n    print(tokenizer .decode(token))\nThis prints out:\n[CLS]\nHello\nworld\n[SEP]\nThis particular tokenizer and model operate by adding the [CLS]  and [SEP]  tokens to\nthe beginning and end of a string.\nOur language model has now processed the text input. The result of its output is the\nfollowing:\ntensor([[\n[-3.3060, -0.0507, -0.1098, ..., -0.1704, -0.1618, 0.6932],\n[ 0.8918, 0.0740, -0.1583, ..., 0.1869, 1.4760, 0.0751],\n[ 0.0871, 0.6364, -0.3050, ..., 0.4729, -0.1829, 1.0157],\n[-3.1624, -0.1436, -0.0941, ..., -0.0290, -0.1265, 0.7954]\n]], grad_fn=<NativeLayerNormBackward0>)\nThis is the raw output of a language model. The applications of large language models\nbuild on top of outputs like this.\nWe recap the input tokenization and resulting outputs of a language model in Fig‐\nure 2-9 . Technically, the switch from token IDs into raw embeddings is the first step\nthat occurs inside a language model.\n60 | Chapter 2: Tokens and Embeddings",6566
31-Text Embeddings for Sentences and Whole Documents.pdf,31-Text Embeddings for Sentences and Whole Documents,"Figure 2-9. A language model operates on raw, static embeddings as its input and\nproduces contextual text embeddings.\nA visual like this is essential for the next chapter when we start to look at how\nTransformer-based LLMs work.\nText Embeddings (for Sentences and Whole Documents)\nWhile  token embeddings are key to how LLMs operate, a number of LLM applica‐\ntions require operating on entire sentences, paragraphs, or even text documents. This\nhas led to special language models that produce text embeddings—a single vector that\nrepresents a piece of text longer than just one token.\nWe can think of text embedding models as taking a piece of text and ultimately\nproducing a single vector that represents that text and captures its meaning in some\nuseful form. Figure 2-10  shows that process.\nText Embeddings (for Sentences and Whole Documents) | 61\n1Nils Reimers and Iryna Gurevych. “Sentence-BERT: Sentence embeddings using Siamese BERT-networks. ”\narXiv preprint arXiv:1908.10084  (2019).\nFigure 2-10. In step 1, we use the embedding model to extract the features and convert\nthe input text to embeddings.\nThere are multiple ways of producing a text embedding vector. One of the most\ncommon ways is to average the values of all the token  embeddings produced by the\nmodel. Y et high-quality text embedding models tend to be trained specifically for text\nembedding tasks.\nWe can produce text embeddings with sentence-transformers , a popular package\nfor leveraging pretrained embedding models.1 The package, like transformers  in the\nprevious chapter, can be used to load publicly available models. To illustrate creating\nembeddings, we use the all-mpnet-base-v2 model . Note that in Chapter 4 , we will\nfurther explore how you can choose an embedding model for your task.\nfrom sentence_transformers  import SentenceTransformer\n# Load model\nmodel = SentenceTransformer (""sentence-transformers/all-mpnet-base-v2"" )\n# Convert text to text embeddings\nvector = model.encode(""Best movie ever!"" )\nThe number of values, or the dimensions, of the embedding vector depends on the\nunderlying embedding model. Let’s explore that for our model:\nvector.shape\n(768,)\n62 | Chapter 2: Tokens and Embeddings",2229
32-Word Embeddings Beyond LLMs.pdf,32-Word Embeddings Beyond LLMs,,0
33-The Word2vec Algorithm and Contrastive Training.pdf,33-The Word2vec Algorithm and Contrastive Training,"This sentence is now encoded in this one vector with a dimension of 768 numerical\nvalues. In Part II of this book, once we start looking at applications, we’ll start to\nsee the immense usefulness of these text embeddings vectors in powering everything\nfrom categorization to semantic search to RAG.\nWord Embeddings Beyond LLMs\nEmbeddings  are useful even outside of text and language generation. Embeddings,\nor assigning meaningful vector representations to objects, turns out to be useful in\nmany domains, including recommender engines and robotics. In this section, we’ll\nlook at how to use pretrained word2vec embeddings and touch on how the method\ncreates word embeddings. Seeing how word2vec is trained will prime you to learn\nabout contrastive training in Chapter 10 . Then in the following section, we’ll see how\nthose embeddings can be used for recommendation systems.\nUsing pretrained Word Embeddings\nLet’s look at how we can download pretrained word embeddings (like word2vec or\nGloVe) using the Gensim library :\nimport gensim.downloader  as api\n# Download embeddings (66MB, glove, trained on wikipedia, vector size: 50)\n# Other options include ""word2vec-google-news-300""\n# More options at https://github.com/RaRe-Technologies/gensim-data\nmodel = api.load(""glove-wiki-gigaword-50"" )\nHere, we’ve downloaded the embeddings of a large number of words trained on\nWikipedia. We can then explore the embedding space by seeing the  nearest neighbors\nof a specific word, “king” for example:\nmodel.most_similar ([model['king']], topn=11)\nThis outputs:\n[('king', 1.0000001192092896),\n('prince', 0.8236179351806641),\n('queen', 0.7839043140411377),\n('ii', 0.7746230363845825),\n('emperor', 0.7736247777938843),\n('son', 0.766719400882721),\n('uncle', 0.7627150416374207),\n('kingdom', 0.7542161345481873),\n('throne', 0.7539914846420288),\n('brother', 0.7492411136627197),\n('ruler', 0.7434253692626953)]\nWord Embeddings Beyond LLMs | 63\nThe Word2vec Algorithm and Contrastive Training\nThe word2vec algorithm described in the paper “Efficient estimation of word repre‐\nsentations in vector space”  is described in detail in The Illustrated Word2vec . The\ncentral ideas are condensed here as we build on them when discussing one method\nfor creating embeddings for recommendation engines in the following section.\nJust like LLMs, word2vec is trained on examples generated from text. Let’s say, for\nexample, we have the text “Thou shalt not make a machine in the likeness of a human\nmind” from the Dune  novels by Frank Herbert. The algorithm uses a sliding window\nto generate training examples. We can, for example, have a window size two, meaning\nthat we consider two neighbors on each side of a central word.\nThe embeddings are generated from a classification task. This task is used to train\na neural network to predict if words commonly appear in the same context or not\n(context  here means in many sentences in the training dataset we’re modeling). We\ncan think of this as a neural network that takes two words and outputs 1 if they tend\nto appear in the same context, and 0 if they do not.\nIn the first position for the sliding window, we can generate four training examples,\nas we can see in Figure 2-11 .\nFigure 2-11. A sliding window is used to generate training examples for the word2vec\nalgorithm to later predict if two words are neighbors or not.\nIn each of the produced training examples, the word in the center is used as one\ninput, and each of its neighbors is a distinct second input in each training example.\nWe expect the final trained model to be able to classify this neighbor relationship\nand output 1 if the two input words it receives are indeed neighbors. These training\nexamples are visualized in Figure 2-12 .\n64 | Chapter 2: Tokens and Embeddings\nFigure 2-12. Each generated training example shows a pair of neighboring words.\nIf, however, we have a dataset of only a target value of 1, then a model can cheat\nand ace it by outputting 1 all the time. To get around this, we need to enrich our\ntraining dataset with examples of words that are not typically neighbors. These are\ncalled negative examples and are shown in Figure 2-13 .\nFigure 2-13. We need to present our models with negative examples: words that are not\nusually neighbors. A better model is able to better distinguish between the positive and\nnegative examples.\nIt turns out that we don’t have to be too scientific in how we choose the negative\nexamples. A lot of useful models result from the simple ability to detect positive\nexamples from randomly generated examples (inspired by an important idea called\nnoise-contrastive estimation  and described in “Noise-contrastive estimation: A new\nestimation principle for unnormalized statistical models” ). So in this case, we get\nrandom words and add them to the dataset and indicate that they are not neighbors\n(and thus the model should output 0 when it sees them).\nWith this, we’ve seen two of the main concepts of word2vec ( Figure 2-14 ): skip-gram,\nthe method of selecting neighboring words, and negative sampling, adding negative\nexamples by random sampling from the dataset.\nWord Embeddings Beyond LLMs | 65\nFigure 2-14. Skip-gram and negative sampling are two of the main ideas behind the\nword2vec algorithm and are useful in many other problems that can be formulated as\ntoken sequence problems.\nWe can generate millions and even billions of training examples like this from\nrunning text. Before proceeding to train a neural network on this dataset, we need\nto make a couple of tokenization decisions, which, just like we’ve seen with LLM\ntokenizers, include how to deal with capitalization and punctuation and how many\ntokens we want in our vocabulary.\nWe then create an embedding vector for each token, and randomly initialize them, as\ncan be seen in Figure 2-15 . In practice, this is a matrix of dimensions vocab_size x\nembedding_dimensions .\nFigure 2-15. A vocabulary of words and their starting, random, uninitialized embedding\nvectors.\nA model is then trained on each example to take in two embedding vectors and\npredict if they’re related or not. We can see what this looks like in Figure 2-16 .\n66 | Chapter 2: Tokens and Embeddings",6277
34-Embeddings for Recommendation Systems.pdf,34-Embeddings for Recommendation Systems,,0
35-Recommending Songs by Embeddings.pdf,35-Recommending Songs by Embeddings,"Figure 2-16. A neural network is trained to predict if two words are neighbors. It updates\nthe embeddings in the training process to produce the final,  trained embeddings.\nBased on whether its prediction was correct or not, the typical machine learning\ntraining step updates the embeddings so that the next time the model is presented\nwith those two vectors, it has a better chance of being more correct. And by the\nend of the training process, we have better embeddings for all the tokens in our\nvocabulary.\nThis idea of a model that takes two vectors and predicts if they have a certain relation\nis one of the most powerful ideas in machine learning, and time after time has proven\nto work very well with language models. This is why we’re dedicating Chapter 10  to\nthis concept and how it optimizes language models for specific tasks (like sentence\nembeddings and retrieval).\nThe same idea is also central to bridging modalities like text and images, which is key\nto AI image generation models, as we’ll see in Chapter 9  on multimodal models. In\nthat formulation, a model is presented with an image and a caption, and it should\npredict whether that caption describes the image or not.\nEmbeddings for Recommendation Systems\nAs we’ve mentioned, the concept of embeddings is useful in so many other domains.\nIn industry, it’s widely used for recommendation systems, for example.\nRecommending Songs by Embeddings\nIn this section we’ll use the word2vec algorithm to embed songs using human-made\nmusic playlists. Imagine if we treated each song as we would a word or token, and\nwe treated each playlist like a sentence. These embeddings can then be used to\nrecommend similar songs that often appear together in playlists.\nThe dataset  we’ll use was collected by Shuo Chen from Cornell University. It contains\nplaylists from hundreds of radio stations around the US. Figure 2-17  demonstrates\nthis dataset.\nEmbeddings for Recommendation Systems | 67\nFigure 2-17. For song embeddings that capture song similarity we’ll use a dataset made\nup of a collection of playlists, each containing a list of songs.\nLet’s demonstrate the end product before we look at how it’s built. So let’s give it a few\nsongs and see what it recommends in response.\nLet’s start by giving it Michael Jackson’s “Billie Jean, ” the song with ID 3822:\n# We will define and explore this function in detail below\nprint_recommendations (3822)\nid Title artist\n4181 Kiss Prince & The Revolution\n12749 Wanna Be Startin’ Somethin’ Michael Jackson\n1506 The Way You Make Me Feel Michael Jackson\n3396 Holiday Madonna\n500 Don’t Stop ‘Til You Get Enough Michael Jackson\nThat looks reasonable. Madonna, Prince, and other Michael Jackson songs are the\nnearest neighbors.\nLet’s step away from pop and into rap, and see the neighbors of 2Pac’s “California\nLove”:\nprint_recommendations (842)\nid Title artist\n413 If I Ruled the World (Imagine That) (w\/ Lauryn Hill) Nas\n196 I’ll Be Missing You Puff  Daddy & The Family\n330 Hate It or Love It (w\/ 50 Cent) The Game\n211 Hypnotize The Notorious B.I.G.\n5788 Drop It Like It’s Hot (w\/ Pharrell) Snoop Dogg\n68 | Chapter 2: Tokens and Embeddings",3193
36-Training a Song Embedding Model.pdf,36-Training a Song Embedding Model,"Another quite reasonable list! Now that we know it works, let’s see how to build such\na system.\nTraining a Song Embedding Model\nWe’ll start by loading the dataset containing the song playlists as well as each song’s\nmetadata, such as its title and artist:\nimport pandas as pd\nfrom urllib import request\n# Get the playlist dataset file\ndata = request.urlopen('https://storage.googleapis.com/maps-premium/data\nset/yes_complete/train.txt' )\n# Parse the playlist dataset file. Skip the first two lines as\n# they only contain metadata\nlines = data.read().decode(""utf-8"").split('\n')[2:]\n# Remove playlists with only one song\nplaylists  = [s.rstrip().split() for s in lines if len(s.split()) > 1]\n# Load song metadata\nsongs_file  = request.urlopen('https://storage.googleapis.com/maps-premium/data\nset/yes_complete/song_hash.txt' )\nsongs_file  = songs_file .read().decode(""utf-8"").split('\n')\nsongs = [s.rstrip().split('\t') for s in songs_file ]\nsongs_df  = pd.DataFrame (data=songs, columns = ['id', 'title', 'artist' ])\nsongs_df  = songs_df .set_index ('id')\nNow that we’ve saved them, let’s inspect the playlists  list. Each element inside it is a\nplaylist containing a list of song IDs:\nprint( 'Playlist #1: \n ', playlists [0], '\n')\nprint( 'Playlist #2: \n ', playlists [1])\nPlaylist #1: ['0', '1', '2', '3', '4', '5', ..., '43']\nPlaylist #2: ['78', '79', '80', '3', '62', ..., '210']\nLet’s train the model:\nfrom gensim.models  import Word2Vec\n# Train our Word2Vec model\nmodel = Word2Vec (\n    playlists , vector_size =32, window=20, negative =50, min_count =1, workers=4\n)\nEmbeddings for Recommendation Systems | 69\nThat takes a minute or two to train and results in embeddings being calculated for\neach song that we have. Now we can use those embeddings to find similar songs\nexactly as we did earlier with words:\nsong_id = 2172\n# Ask the model for songs similar to song #2172\nmodel.wv.most_similar (positive =str(song_id))\nThis outputs:\n[('2976', 0.9977465271949768),\n ('3167', 0.9977430701255798),\n ('3094', 0.9975950717926025),\n ('2640', 0.9966474175453186),\n ('2849', 0.9963167905807495)]\nThat is the list of the songs whose embeddings are most similar to song 2172.\nIn this case, the song is:\nprint(songs_df .iloc[2172])\ntitle Fade To Black\nartist Metallica\nName: 2172 , dtype: object\nThis results in recommendations that are all in the same heavy metal and hard rock\ngenre:\nimport numpy as np\ndef print_recommendations (song_id):\n    similar_songs  = np.array(\n        model.wv.most_similar (positive =str(song_id),topn=5)\n    )[:,0]\n    return  songs_df .iloc[similar_songs ]\n# Extract recommendations\nprint_recommendations (2172)\nid Title artist\n11473 Little Guitars Van Halen\n3167 Unchained Van Halen\n5586 The Last in Line Dio\n5634 Mr. Brownstone Guns N’ Roses\n3094 Breaking the Law Judas Priest\n70 | Chapter 2: Tokens and Embeddings",2914
37-Summary.pdf,37-Summary,"Summary\nIn this chapter, we have covered LLM tokens, tokenizers, and useful approaches to\nusing token embeddings. This prepares us to start looking closer at language models\nin the next chapter, and also opens the door to learn about how embeddings are used\nbeyond language models.\nWe explored how tokenizers are the first step in processing input to an LLM, trans‐\nforming raw textual input into token IDs. Common tokenization schemes include\nbreaking text down into words, subword tokens, characters, or bytes, depending on\nthe specific requirements of a given application.\nA tour of real-world pretrained tokenizers (from BERT to GPT-2, GPT-4, and other\nmodels) showed us areas where some tokenizers are better (e.g., preserving informa‐\ntion like capitalization, newlines, or tokens in other languages) and other areas where\ntokenizers are just different from each other (e.g., how they break down certain\nwords).\nThree of the major tokenizer design decisions are the tokenizer algorithm (e.g.,\nBPE, WordPiece, SentencePiece), tokenization parameters (including vocabulary size,\nspecial tokens, capitalization, treatment of capitalization and different languages), and\nthe dataset the tokenizer is trained on.\nLanguage models are also creators of high-quality contextualized token embeddings\nthat improve on raw static embeddings. Those contextualized token embeddings\nare what’s used for tasks including named-entity recognition (NER), extractive text\nsummarization, and text classification. In addition to producing token embeddings,\nlanguage models can produce text embeddings that cover entire sentences or even\ndocuments. This empowers plenty of applications that will be shown in Part II of this\nbook covering language model applications\nBefore LLMs, word embedding methods like word2vec, GloVe, and fastText were\npopular. In language processing, this has largely been replaced with contextualized\nword embeddings produced by language models. The word2vec algorithm relies on\ntwo main ideas: skip-gram and negative sampling. It also uses contrastive training\nsimilar to the type we’ll see in Chapter 10 .\nEmbeddings are useful for creating and improving recommender systems as we\ndiscussed in the music recommender we built from curated song playlists.\nIn the next chapter, we will take a deep dive into the process after tokenization: how\ndoes an LLM process these tokens and generate text? We will look at some of the\nmain intuitions of how LLMs that use the Transformer architecture work.\nSummary | 71",2549
38-An Overview of Transformer Models.pdf,38-An Overview of Transformer Models,"CHAPTER 3\nLooking Inside Large Language Models\nNow that we have a sense of tokenization and embeddings, we’re ready to dive deeper\ninto the language model and see how it works. In this chapter, we’ll look at some of\nthe main intuitions of how Transformer language models work. Our focus will be on\ntext generation models so we get a deeper sense for generative LLMs in particular.\nWe’ll be looking at both the concepts and some code examples that demonstrate\nthem. Let’s start by loading a language model and getting it ready for generation\nby declaring a pipeline. In your first read, feel free to skip the code and focus on\ngrasping the concepts involved. Then in a second read, the code will get you to start\napplying these concepts.\nimport torch\nfrom transformers  import AutoModelForCausalLM , AutoTokenizer , pipeline\n# Load model and tokenizer\ntokenizer  = AutoTokenizer .from_pretrained (""microsoft/Phi-3-mini-4k-instruct"" )\nmodel = AutoModelForCausalLM .from_pretrained (\n    ""microsoft/Phi-3-mini-4k-instruct"" ,\n    device_map =""cuda"",\n    torch_dtype =""auto"",\n    trust_remote_code =True,\n)\n# Create a pipeline\ngenerator  = pipeline (\n    ""text-generation"" ,\n    model=model,\n    tokenizer =tokenizer ,\n    return_full_text =False,\n    max_new_tokens =50,\n    do_sample =False,\n)\n73",1323
39-The Inputs and Outputs of a Trained Transformer LLM.pdf,39-The Inputs and Outputs of a Trained Transformer LLM,"An Overview of Transformer Models\nLet’s begin our exploration with a high-level overview of the model, and then we’ll see\nhow later work has improved upon the Transformer model since its introduction in\n2017.\nThe Inputs and Outputs of a Trained Transformer LLM\nThe most common picture of understanding the behavior of a Transformer LLM is\nto think of it as a software system that takes in text and generates text in response.\nOnce a large enough text-in-text-out model is trained on a large enough high-quality\ndataset, it becomes able to generate impressive and useful outputs. Figure 3-1  shows\none such model used to author an email.\nFigure 3-1. At a high level of abstraction, Transformer LLMs take a text prompt and\noutput generated text.\nThe model does not generate the text all in one operation; it actually generates one\ntoken at a time. Figure 3-2  shows four steps of token generation in response to the\ninput prompt. Each token generation step is one forward pass through the model\n(that’s machine-learning speak for the inputs going into the neural network and\nflowing through the computations it needs to produce an output on the other end of\nthe computation graph).\n74 | Chapter 3: Looking Inside Large Language Models\nFigure 3-2. Transformer LLMs generate one token at a time, not the entire text at once.\nAfter each token generation, we tweak the input prompt for the next generation step\nby appending the output token to the end of the input prompt. We can see this in\nFigure 3-3 .\nFigure 3-3. An output token is appended to the prompt, then this new text is presented to\nthe model again for another forward pass to generate the next token.\nAn Overview of Transformer Models | 75",1721
40-The Components of the Forward Pass.pdf,40-The Components of the Forward Pass,"This gives us a more accurate picture of the model as it is simply predicting the next\ntoken based on an input prompt. Software around the neural network basically runs\nit in a loop to sequentially expand the generated text until completion.\nThere’s a specific word used in machine learning to describe models that consume\ntheir earlier predictions to make later predictions (e.g., the model’s first generated\ntoken is used to generate the second token). They’re called autoregressive  models.\nThat is why you’ll hear text generation LLMs being called autoregressive models. This\nis often used to differentiate text generation models from text representation models\nlike BERT, which are not autoregressive.\nThis autoregressive, token-by-token generation is what happens under the hood\nwhen we generate text with the LLM like we see here:\nprompt = ""Write an email apologizing to Sarah for the tragic gardening mishap. \nExplain how it happened.""\noutput = generator (prompt)\nprint(output[0]['generated_text' ])\nThis generates the text:\nSolution 1:\nSubject: My Sincere Apologies for the Gardening Mishap\nDear Sarah,\nI hope this message finds you well. I am writing to express my deep\nWe can see the model begin to write the email starting with the subject. It stopped\nabruptly because it reached the token limit we established by setting max_new_tokens\nto 50 tokens. If we increase that, it will continue until concluding the email.\nThe Components of the Forward Pass\nIn addition to the loop, two key internal components are the tokenizer and the\nlanguage modeling head (LM head). Figure 3-4  shows where these components lie in\nthe system. We saw in the previous chapter how tokenizers break down the text into a\nsequence of token IDs that then become the input to the model.\nThe tokenizer is followed by the neural network: a stack of Transformer blocks that\ndo all of the processing. That stack is then followed by the LM head, which translates\nthe output of the stack into probability scores for what the most likely next token is.\n76 | Chapter 3: Looking Inside Large Language Models\nFigure 3-4. A Transformer LLM is made up of a tokenizer, a stack of Transformer blocks,\nand a language modeling head.\nRecall from Chapter 2  that the tokenizer contains a table of tokens—the tokenizer’s\nvocabulary . The model has a vector representation associated with each of these\ntokens in the vocabulary (token embeddings). Figure 3-5  shows both the vocabulary\nand associated token embeddings for a model with a vocabulary of 50,000 tokens.\nFigure 3-5. The tokenizer has a vocabulary of 50,000 tokens. The model has token\nembeddings associated with those embeddings.\nThe flow of the computation follows the direction of the arrow from top to bottom.\nFor each generated token, the process flows once through each of the Transformer\nblocks in the stack in order, then to the LM head, which finally outputs the probabil‐\nity distribution for the next token, seen in Figure 3-6 .\nAn Overview of Transformer Models | 77\nFigure 3-6. At the end of the forward pass, the model predicts a probability score for each\ntoken in the vocabulary.\nThe LM head is a simple neural network layer itself. It is one of multiple possible\n“heads” to attach to a stack of Transformer blocks to build different kinds of systems.\nOther kinds of Transformer heads include sequence classification heads and token\nclassification heads.\nWe can display the order of the layers by simply printing out the model variable. For\nthis model, we have:\nPhi3ForCausalLM(\n  (model): Phi3Model(\n    (embed_tokens): Embedding(32064, 3072, padding_idx=32000)\n    (embed_dropout): Dropout(p=0.0, inplace= False)\n    (layers): ModuleList(\n      (0-31): 32 x Phi3DecoderLayer(\n        (self_attn): Phi3Attention(\n          (o_proj): Linear(in_features=3072, out_features=3072, bias= False)\n          (qkv_proj): Linear(in_features=3072, out_features=9216, bias= False)\n          (rotary_emb): Phi3RotaryEmbedding()\n        )\n        (mlp): Phi3MLP(\n          (gate_up_proj): Linear(in_features=3072, out_features=16384, \nbias=False)\n          (down_proj): Linear(in_features=8192, out_features=3072, bias= False)\n          (activation_fn): SiLU()\n        )\n        (input_layernorm): Phi3RMSNorm()\n        (resid_attn_dropout): Dropout(p=0.0, inplace= False)\n        (resid_mlp_dropout): Dropout(p=0.0, inplace= False)\n        (post_attention_layernorm): Phi3RMSNorm()\n      )\n    )\n    (norm): Phi3RMSNorm()\n  )\n  (lm_head): Linear(in_features=3072, out_features=32064, bias= False)\n)\n78 | Chapter 3: Looking Inside Large Language Models",4662
41-Choosing a Single Token from the Probability Distribution SamplingDecoding.pdf,41-Choosing a Single Token from the Probability Distribution SamplingDecoding,"Looking at this structure, we can notice the following highlights:\n•This shows us the various nested layers of the model. The majority of the model•\nis labeled model , followed by lm_head .\n•Inside the Phi3Model  model, we see the embeddings matrix embed_tokens  and its •\ndimensions. It has 32,064 tokens each with a vector size of 3,072.\n•Skipping the dropout layer for now, we can see the next major component is•\nthe stack of Transformer decoder layers. It contains 32 blocks of type Phi3Deco\nderLayer .\n•Each of these Transformer blocks includes an attention layer and a feedforward•\nneural network (also known as an mlp or multilevel perceptron). We’ll cover\nthese in more detail later in the chapter.\n•Finally, we see the lm_head  taking a vector of size 3,072 and outputting a vector •\nequivalent to the number of tokens the model knows. That output is the proba‐\nbility score for each token that helps us select the output token.\nChoosing a Single Token from the Probability Distribution (Sampling/\nDecoding)\nAt the end of processing, the output of the model is a probability score for each token\nin the vocabulary, as we saw previously in Figure 3-6 . The method of choosing a sin‐\ngle token from the probability distribution is called the decoding strategy . Figure 3-7\nshows how this leads to picking the token “Dear” in one example.\nThe easiest decoding strategy would be to always pick the token with the highest\nprobability score. In practice, this doesn’t tend to lead to the best outputs for most\nuse cases. A better approach is to add some randomness and sometimes choose the\nsecond or third highest probability token. The idea here is to basically sample  from\nthe probability distribution based on the probability score, as the statisticians would\nsay.\nWhat this means for the example in Figure 3-7  is that if the token “Dear” has a 40%\nprobability of being the next token, then it has a 40% chance of being picked (instead\nof greedy search, which would pick it directly for having the highest score). So with\nthis method, all the other tokens have a chance of being picked according to their\nscore.\nAn Overview of Transformer Models | 79\nFigure 3-7. The tokens with the highest probability after  the model’s forward pass.\nOur decoding strategy decides which of the tokens to output by sampling based on the\nprobabilities.\nChoosing the highest scoring token every time is called greedy decoding . It’s what\nhappens if you set the temperature parameter to zero in an LLM. We cover the\nconcept of temperature in Chapter 6 .\nLet’s look more closely at the code that demonstrates this process. In this code block,\nwe pass the input tokens through the model, and then lm_head :\nprompt = ""The capital of France is""\n# Tokenize the input prompt\ninput_ids  = tokenizer (prompt, return_tensors =""pt"").input_ids\n# Tokenize the input prompt\ninput_ids  = input_ids .to(""cuda"")\n# Get the output of the model before the lm_head\nmodel_output  = model.model(input_ids )\n# Get the output of the lm_head\nlm_head_output  = model.lm_head(model_output [0])\nNow, lm_head_output  is of the shape [1, 6, 32064]. We can access the token proba‐\nbility scores for the last generated token using lm_head_output[0,-1] , which uses\nthe index 0 across the batch dimension; the index –1 gets us the last token in the\nsequence. This is now a list of probability scores for all 32,064 tokens. We can get the\ntop scoring token ID, and then decode it to arrive at the text of the generated output\ntoken:\n80 | Chapter 3: Looking Inside Large Language Models",3596
42-Parallel Token Processing and Context Size.pdf,42-Parallel Token Processing and Context Size,"token_id  = lm_head_output [0,-1].argmax(-1)\ntokenizer .decode(token_id )\nIn this case this turns out to be:\nParis\nParallel Token Processing and Context Size\nOne of the most compelling features of Transformers is that they lend themselves\nbetter to parallel computing than previous neural network architectures in language\nprocessing. In text generation, we get a first glance at this when looking at how each\ntoken is processed. We know from the previous chapter that the tokenizer will break\ndown the text into tokens. Each of these input tokens then flows through its own\ncomputation path (that’s a good first intuition, at least). We can see these individual\nprocessing tracks or streams in Figure 3-8 .\nFigure 3-8. Each token is processed through its own stream of computation (with some\ninteraction between them in attention steps, as we’ll later see).\nCurrent Transformer models have a limit for how many tokens they can process at\nonce. That limit is called the model’s context length. A model with 4K context length\ncan only process 4K tokens and would only have 4K of these streams.\nAn Overview of Transformer Models | 81\nEach of the token streams starts with an input vector (the embedding vector and\nsome positional information; we’ll discuss positional embeddings later in the chap‐\nter). At the end of the stream, another vector emerges as the result of the model’s\nprocessing, as shown in Figure 3-9 .\nFigure 3-9. Each processing stream takes a vector as input and produces a final  resulting\nvector of the same size (often  referred to as the model dimension).\nFor text generation, only the output result of the last stream is used to predict the\nnext token. That output vector is the only input into the LM head as it calculates the\nprobabilities of the next token.\nY ou may wonder why we go through the trouble of calculating all the token streams\nif we’re discarding the outputs of all but the last token. The answer is that the\ncalculations of the previous streams are required and used in calculating the final\nstream. Y es, we’re not using their final output vector, but we use earlier outputs (in\neach Transformer block) in the Transformer block’s attention mechanism.\n82 | Chapter 3: Looking Inside Large Language Models",2276
43-Speeding Up Generation by Caching Keys and Values.pdf,43-Speeding Up Generation by Caching Keys and Values,"If you’re following along with the code examples, recall that the output of lm_head\nwas of the shape [1, 6, 32064]. That was because the input to it was of the shape [1,\n6, 3072], which is a batch of one input string, containing six tokens, each of them\nrepresented by a vector of size 3,072 corresponding to the output vectors after the\nstack of Transformer blocks.\nWe can access these matrices and view their dimensions by printing:\nmodel_output [0].shape\nThis outputs:\ntorch.Size([1, 6, 3072])\nSimilarly, we can print the output of the LM head:\nlm_head_output .shape\nThis outputs:\ntorch.Size([1, 6, 32064])\nSpeeding Up Generation by Caching Keys and Values\nRecall that when generating the second token, we simply append the output token to\nthe input and do another forward pass through the model. If we give the model the\nability to cache the results of the previous calculation (especially some of the specific\nvectors in the attention mechanism), we no longer need to repeat the calculations of\nthe previous streams. This time the only needed calculation is for the last stream. This\nis an optimization technique called the keys and values (kv) cache  and it provides a\nsignificant speedup of the generation process. Keys and values are some of the central\ncomponents of the attention mechanism, as we’ll see later in this chapter.\nFigure 3-10  shows how when generating the second token, only one processing\nstream is active as we cache the results of the previous streams.\nAn Overview of Transformer Models | 83\nFigure 3-10. When generating text, it’s important to cache the computation results of\nprevious tokens instead of repeating the same calculation over and over again.\nIn Hugging Face Transformers, cache is enabled by default. We can disable it by\nsetting use_cache  to False . We can see the difference in speed by asking for a long\ngeneration, and timing the generation with and without caching:\nprompt = ""Write a very long email apologizing to Sarah for the tragic gardening \nmishap. Explain how it happened.""\n# Tokenize the input prompt\ninput_ids  = tokenizer (prompt, return_tensors =""pt"").input_ids\ninput_ids  = input_ids .to(""cuda"")\n84 | Chapter 3: Looking Inside Large Language Models",2243
44-Inside the Transformer Block.pdf,44-Inside the Transformer Block,"Then we time how long it takes to generate 100 tokens with caching. We can use the\n%%timeit  magic command in Jupyter or Colab to time how long the execution takes\n(it runs the command several times and gets the average):\n%%timeit -n 1\n# Generate the text\ngeneration_output  = model.generate (\n  input_ids =input_ids ,\n  max_new_tokens =100,\n  use_cache =True\n)\nOn a Colab with a T4 GPU, this comes to 4.5 seconds. How long would that take if we\ndisable the cache, however?\n%%timeit -n 1\n# Generate the text\ngeneration_output  = model.generate (\n  input_ids =input_ids ,\n  max_new_tokens =100,\n  use_cache =False\n)\nThis comes out to 21.8 seconds. A dramatic difference. In fact, from a user experience\nstandpoint, even the four-second generation time tends to be a long time to wait for\na user that’s staring at a screen and waiting for an output from the model. This is one\nreason why LLM APIs stream the output tokens as the model generates them instead\nof waiting for the entire generation to be completed.\nInside the Transformer Block\nWe can now talk about where the vast majority of processing happens: the Trans‐\nformer blocks. As Figure 3-11  shows, Transformer LLMs are composed of a series\nTransformer blocks (often in the range of six in the original Transformer paper, to\nover a hundred in many large LLMs). Each block processes its inputs, then passes the\nresults of its processing to the next block.\nAn Overview of Transformer Models | 85\nFigure 3-11. The bulk of the Transformer LLM processing happens inside a series of\nTransformer blocks, each handing the result of its processing as input to the subsequent\nblock.\nA Transformer block ( Figure 3-12 ) is made up of two successive components:\n1.The attention layer  is mainly concerned with incorporating relevant information 1.\nfrom other input tokens and positions\n2.The feedforward layer  houses the majority of the model’s processing capacity 2.\nFigure 3-12. A Transformer block is made up of a self-attention layer and a feedforward\nneural network.\n86 | Chapter 3: Looking Inside Large Language Models\nThe feedforward neural network at a glance\nA simple example giving the intuition of the feedforward neural network would be if\nwe pass the simple input “The Shawshank” to a language model, with the expectation\nthat it will generate “Redemption” as the most probable next word (in reference to the\nfilm from 1994).\nThe feedforward neural network (collectively in all the model layers) is the source\nof this information, as Figure 3-13  shows. When the model was successfully trained\nto model a massive text archive (which included many mentions of “The Shawshank\nRedemption”), it learned and stored the information (and behaviors) that make it\nsucceed at this task.\nFigure 3-13. The feedforward neural network component of a Transformer block likely\ndoes the majority of the model’s memorization and interpolation.\nFor an LLM to be successfully trained, it needs to memorize a lot of information.\nBut it is not simply a large database. Memorization is only one ingredient in the\nrecipe of impressive text generation. The model is able to use this same machinery to\ninterpolate between data points and more complex patterns to be able to generalize—\nwhich means doing well on inputs it hadn’t seen in the past and were not in its\ntraining dataset.\nAn Overview of Transformer Models | 87\nWhen you use a modern commercial LLM, the outputs you get\nare not the ones mentioned earlier in the strict meaning of a “lan‐\nguage model. ” Passing “The Shawshank” to a chat LLM like GPT-4\nproduces an output:\n""The Shawshank Redemption"" is a 1994 film directed \nby Frank Darabont and is based on the novella ""Rita \nHayworth and Shawshank Redemption"" written by Stephen \nKing. ...etc.\nThis is because raw language models (like GPT-3) are difficult\nfor people to properly utilize. This is why the language model\nis then trained on instruction-tuning and human preference and\nfeedback fine-tuning to match people’s expectations of what the\nmodel should output.\nThe attention layer at a glance\nContext  is vital in order to properly model language. Simple memorization and\ninterpolation based on the previous token can only take us so far. We know that\nbecause this was one of the leading approaches to build language models before\nneural networks (see Chapter 3, “N-gram Language Models” of Speech and Language\nProcessing  by Daniel Jurafsky and James H. Martin).\nAttention is a mechanism that helps the model incorporate context as it’s processing a\nspecific token. Think of the following prompt:\n“The dog chased the squirrel because it”\nFor the model to predict what comes after “it, ” it needs to know what “it” refers to.\nDoes it refer to the dog or the squirrel?\nIn a trained Transformer LLM, the attention mechanism makes that determination.\nAttention adds information from the context into the representation of the “it” token.\nWe can see a simple version of that in Figure 3-14 .\n88 | Chapter 3: Looking Inside Large Language Models\nFigure 3-14. The self-attention layer incorporates relevant information from previous\npositions that help process the current token.\nThe model does that based on the patterns seen and learned from the training dataset.\nPerhaps previous sentences also give more clues, like, for example, referring to the\ndog as “she” thus making it clear that “it” refers to the squirrel.\nAttention is all you need\nIt is worth diving deeper into the attention mechanism. The most stripped-down\nversion of the mechanism is shown in Figure 3-15 . It shows multiple token positions\ngoing into the attention layer; the final one is the one being currently processed (the\npink arrow). The attention mechanism operates on the input vector at that position.\nIt incorporates relevant information from the context into the vector it produces as\nthe output for that position.\nAn Overview of Transformer Models | 89\nFigure 3-15. A simplified  framing of attention: an input sequence and a current position\nbeing processed. As we’re mainly concerned with this position, the figure  shows an input\nvector and an output vector that incorporates information from the previous elements in\nthe sequence according to the attention mechanism.\nTwo main steps are involved in the attention mechanism:\n1.A way to score how relevant each of the previous input tokens are to the current1.\ntoken being processed (in the pink arrow).\n2.Using those scores, we combine the information from the various positions into a2.\nsingle output vector.\nFigure 3-16  shows these two steps.\nFigure 3-16. Attention is made up of two major steps: relevance scoring for each posi‐\ntion, then a step where we combine the information based on those scores.\n90 | Chapter 3: Looking Inside Large Language Models\nTo give the Transformer more extensive attention capability, the attention mecha‐\nnism is duplicated and executed multiple times in parallel. Each of these parallel\napplications  of attention is conducted into an attention head . This increases the\nmodel’s capacity to model complex patterns in the input sequence that require paying\nattention to different patterns at once.\nFigure 3-17  shows the intuition of how attention heads run in parallel with a preced‐\ning step of splitting information and a later step of combining the results of all the\nheads.\nFigure 3-17. We get better LLMs by doing attention multiple times in parallel, increasing\nthe model’s capacity to attend to different  types of information.\nHow attention is calculated\nLet’s look at how attention is calculated inside a single attention head. Before we start\nthe calculation, let’s observe the following as the starting position:\n•The attention layer (of a generative LLM) is processing attention for a single•\nposition.\n•The inputs to the layer are:•\n—The vector representation of the current position or token—\n—The vector representations of the previous tokens—\n•The goal is to produce a new representation of the current position that incorpo‐•\nrates relevant information from the previous tokens:\n—For example, if we’re processing the last position in the sentence “Sarah fed—\nthe cat because it, ” we want “it” to represent the cat—so attention bakes in “cat\ninformation” from the cat token.\nAn Overview of Transformer Models | 91\n•The training process produces three projection matrices that produce the com‐•\nponents that interact in this calculation:\n—A query projection matrix—\n—A key projection matrix—\n—A value projection matrix—\nFigure 3-18  shows the starting position for all of these components before the atten‐\ntion calculations start. For simplicity, let’s look at only one attention head because the\nother heads have identical calculations but with their individual projection matrices.\nFigure 3-18. Before starting the self-attention calculation, we have the inputs to the layer\nand projection matrices for queries, keys, and values.\nAttention starts by multiplying the inputs by the projection matrices to create three\nnew matrices. These are called the queries, keys, and values matrices. These matrices\ncontain the information of the input tokens projected to three different spaces that\nhelp carry out the two steps of attention:\n1.Relevance scoring1.\n2.Combining information2.\n92 | Chapter 3: Looking Inside Large Language Models\nFigure 3-19  shows these three new matrices, and how the bottom row of all three\nmatrices is associated with the current position while the rows above it are associated\nwith the previous positions.\nFigure 3-19. Attention is carried out by the interaction of the queries, keys, and val‐\nues matrices. Those  are produced by multiplying the layer’s inputs with the projection\nmatrices.\nSelf-attention: Relevance scoring\nIn a generative Transformer, we’re generating one token at a time. This means we’re\nprocessing one position at a time. So the attention mechanism here is only concerned\nwith this one position, and how information from other positions can be pulled in to\ninform this position.\nAn Overview of Transformer Models | 93\nThe relevance scoring step of attention is conducted by multiplying the query vector\nof the current position with the keys matrix. This produces a score stating how\nrelevant each previous token is. Passing that by a softmax operation normalizes these\nscores so they sum up to 1. Figure 3-20  shows the relevance score resulting from this\ncalculation.\nFigure 3-20. Scoring the relevance of previous tokens is accomplished by multiplying the\nquery associated with the current position with the keys matrix.\nSelf-attention: Combining information\nNow that we have the relevance scores, we multiply the value vector associated with\neach token by that token’s score. Summing up those resulting vectors produces the\noutput of this attention step, as we see in Figure 3-21 .\n94 | Chapter 3: Looking Inside Large Language Models",11001
45-More Efficient Attention.pdf,45-More Efficient Attention,"Figure 3-21. Attention combines the relevant information of previous positions by multi‐\nplying their relevance scores by their respective value vectors.\nRecent Improvements to the Transformer Architecture\nSince  the release of the Transformer architecture, much work has been done to\nimprove it and create better models. This spans training on larger datasets and opti‐\nmizations for the training process and learning rates to use, but it also extends to the\narchitecture itself. At the time of writing, a lot of the ideas of the original Transformer\nstand unchanged. There are a few architectural ideas that have proved to be valuable.\nThey contribute to the performance of more recent Transformer models like Llama\n2. In this final section of the chapter, we go over a number of the important recent\ndevelopments of the Transformer architecture.\nRecent Improvements to the Transformer Architecture | 95\nMore Efficient  Attention\nThe area that gets the most focus from the research community is the attention layer\nof the Transformer. This is because the attention calculation is the most computation‐\nally expensive part of the process.\nLocal/sparse attention\nAs Transformers started getting larger, ideas like sparse attention ( “Generating long\nsequences with sparse transformers” ) and sliding window attention ( “Longformer:\nThe long-document transformer” ) provided improvements for the efficiency of the\nattention calculation. Sparse attention limits the context of previous tokens that the\nmodel can attend to, as we can see in Figure 3-22 .\nFigure 3-22. Local attention boosts performance by only paying attention to a small\nnumber of previous positions.\nOne model that incorporates such a mechanism is GPT-3. But it does not use that\nfor all the Transformer blocks—the quality of the generation would vastly degrade\nif the model could only see a small number of previous tokens. The GPT-3 architec‐\nture interweaved full-attention and efficient-attention Transformer blocks. So the\nTransformer blocks alternate between full attention (e.g., blocks 1 and 3) and sparse\nattention (e.g., blocks 2 and 4).\nTo demonstrate different kinds of attention, review Figure 3-23 , which shows how\ndifferent attention mechanisms work. Each figure shows which previous tokens (light\nblue) can be attended to when processing the current token (in dark blue).\n96 | Chapter 3: Looking Inside Large Language Models\nFigure 3-23. Full attention versus sparse attention. Figure 3-24  explains the coloring.\n(Source: “Generating long sequences with sparse transformers” .)\nEach row corresponds to a token being processed. The color coding indicates which\ntokens the model is able to pay attention to while it’s processing the token in the dark\nblue cell. Figure 3-24  describes this with more clarity.\nFigure 3-24. Attention figures  show which token is being processed, and which previous\ntokens an attention mechanism allows it to attend to.\nThis figure also shows the autoregressive nature of decoder Transformer blocks\n(which make up most text generation models); they can only pay attention to previ‐\nous tokens. Contrast this to BERT, which can pay attention to both sides (hence the B\nin BERT stands for bidirectional).\nRecent Improvements to the Transformer Architecture | 97\nMulti-query and grouped-query attention\nA more recent efficient attention tweak to the Transformer is grouped-query atten‐\ntion ( “GQA: Training generalized multi-query transformer models from multi-head\ncheckpoints” ), which is used by models like Llama 2 and 3. Figure 3-25  shows these\ndifferent types of attention, and the next section continues to explain them.\nFigure 3-25. A comparison of different  kinds of attention: the original multi-head,\ngrouped-query attention, and multi-query attention (source: “Fast transformer decod‐\ning: One write-head is all you need” ).\nGrouped-query attention builds on multi-query attention ( “Fast transformer decod‐\ning: One write-head is all you need” ). These methods improve inference scalability of\nlarger models by reducing the size of the matrices involved.\nOptimizing attention: From multi-head to multi-query to grouped query\nEarlier in the chapter we showed how the Transformer paper described multi-headed\nattention. The Illustrated Transformer  discusses in detail how the queries, keys, and\nvalues matrices are used to conduct the attention operation. Figure 3-26  shows how\neach “attention head” has its own distinct query, key, and value matrices calculated for\na given input.\nThe way that multi-query attention optimizes this is to share the keys and values\nmatrices between all the heads. So the only unique matrices for each head would be\nthe queries matrices, as we can see in Figure 3-27 .\n98 | Chapter 3: Looking Inside Large Language Models\nFigure 3-26. Attention is conducted using matrices of queries, keys, and values. In\nmulti-head attention, each head has a distinct version of each of these matrices.\nFigure 3-27. Multi-query attention presents a more efficient  attention mechanism by\nsharing the keys and values matrices across all the attention heads.\nRecent Improvements to the Transformer Architecture | 99\nAs model sizes grow, however, this optimization can be too punishing and we can\nafford to use a little more memory to improve the quality of the models. This is where\ngrouped-query attention comes in. Instead of cutting the number of keys and values\nmatrices to one of each, it allows us to use more (but less than the number of heads).\nFigure 3-28  shows these groups and how each group of attention heads shares keys\nand values matrices.\nFigure 3-28. Grouped-query attention sacrifices  a little bit of the efficiency  of multi-\nquery attention in return for a large improvement in quality by allowing multiple groups\nof shared key/value matrices; each group has its respective set of attention heads.\nFlash Attention\nFlash  Attention is a popular method and implementation that provides significant\nspeedups for both training and inference of Transformer LLMs on  GPUs. It speeds up\nthe attention calculation by optimizing what values are loaded and moved between a\nGPU’s shared memory (SRAM) and high bandwidth memory (HBM). It is described\nin detail in the papers “FlashAttention: Fast and memory-efficient exact attention\nwith IO-awareness”  and the subsequent “FlashAttention-2: Faster attention with bet‐\nter parallelism and work partitioning” .\n100 | Chapter 3: Looking Inside Large Language Models",6552
46-Positional Embeddings RoPE.pdf,46-Positional Embeddings RoPE,"The Transformer Block\nRecall  that the two major components of a Transformer block are an attention layer\nand a feedforward neural network. A more detailed view of the block would also\nreveal the residual connections and  layer-normalization operations that we can see in\nFigure 3-29 .\nFigure 3-29. A Transformer block from the original Transformer paper.\nThe latest Transformer models at the time of this writing still retain the major\ncomponents, yet make a number of tweaks as we can see in Figure 3-30 .\nOne of the differences we see in this version of the Transformer block is that nor‐\nmalization happens prior to attention and the feedforward layers. This has been\nreported to reduce the required training time (read: “On layer normalization in the\nTransformer architecture” ). Another improvement in normalization here is using\nRMSNorm, which is simpler and more efficient than the LayerNorm used in the\noriginal Transformer (read: “Root mean square layer normalization” ). Lastly, instead\nof the original Transformer’s ReLU activation function, newer variants like SwiGLU\n(described in “GLU Variants Improve Transformer” ) are now more common.\nRecent Improvements to the Transformer Architecture | 101\nFigure 3-30. The Transformer block of a 2024-era Transformer like Llama 3 features\nsome tweaks like pre-normalization and an attention optimized with grouped-query\nattention and rotary embeddings.\nPositional Embeddings (RoPE)\nPositional embeddings have been a key component since the original Transformer.\nThey enable the model to keep track of the order of tokens/words in a sequence/\nsentence, which is an indispensable source of information in language. From the\nmany positional encoding schemes proposed in the past years, rotary positional\nembeddings (or “RoPE, ” introduced in “RoFormer: Enhanced Transformer with\nrotary position embedding” ) is especially important to point out.\nThe original Transformer paper and some of the early variants had absolute posi‐\ntional embeddings that, in essence, marked the first token as position 1, the second as\nposition 2...etc. These could either be static methods (where the positional vectors are\ngenerated using geometric functions) or learned (where the model training assigns\nthem their values during the learning process). Some challenges arise from such\nmethods when we scale up models, which requires us to find ways to improve their\nefficiency.\nFor example, one challenge in efficiently training models with large context is that\na lot of documents in the training set are much shorter than that context. It would\n102 | Chapter 3: Looking Inside Large Language Models\nbe inefficient to allocate the entire, say, 4K context to a short 10-word sentence.\nSo during model training, documents are packed together into each context in the\ntraining batch, as Figure 3-31  shows.\nFigure 3-31. Packing is the process of efficiently  organizing short training documents\ninto the context. It includes grouping multiple documents in a single context while\nminimizing the padding at the end of the context.\nLearn more about packing by reading “Efficient sequence packing without cross-\ncontamination: Accelerating large language models without impacting performance”\nand watching the great visuals in “Introducing packed BERT for 2X training speed-up\nin natural language processing” .\nPositional embedding methods have to adapt to this and other practical considera‐\ntions. If Document 50, for example, starts at position 50, then we’ d be misinforming\nthe model if we tell it that that first token is number 50 and that would affect\nits performance (because it would assume there’s previous context while in reality\nthe earlier tokens belong to a different and unrelated document the model should\nignore).\nInstead of the static, absolute embeddings that are added in the beginning of the\nforward pass, rotary embeddings are a method to encode positional information in\na way that captures absolute and relative token position information. It is based on\nthe idea of rotating vectors in their embeddings space. In the forward pass, they are\nadded in the attention step, as Figure 3-32  shows.\nRecent Improvements to the Transformer Architecture | 103\nFigure 3-32. Rotary embeddings are applied in the attention step, not at the start of the\nforward pass.\nDuring the attention process, the positional information is mixed in specifically to the\nqueries and keys matrices just before we multiply them for relevance scoring, as we\ncan see in Figure 3-33 .\n104 | Chapter 3: Looking Inside Large Language Models",4623
47-Summary.pdf,47-Summary,"Figure 3-33. Rotary positional embeddings are added to the representation of tokens just\nbefore the relevance scoring step in self-attention.\nOther Architectural Experiments and Improvements\nMany tweaks of the Transformer are proposed and researched on a continuous basis.\n“ A Survey of Transformers”  highlights a few of the main directions. Transformer\narchitectures are also constantly adapted to domains beyond LLMs. Computer vision\nis an area where a lot of Transformer architecture research is happening (see: “Trans‐\nformers in vision: A survey”  and “ A survey on vision transformer” ). Other domains\ninclude robotics (see “Open X-Embodiment: Robotic learning datasets and RT-X\nmodels” ) and time series (see “Transformers in time series: A survey” ).\nRecent Improvements to the Transformer Architecture | 105\nSummary\nIn this chapter we discussed the main intuitions of Transformers and recent develop‐\nments that enable the latest Transformer LLMs. We went over many new concepts, so\nlet’s break down the key concepts that we discussed in this chapter:\n•A Transformer LLM generates one token at a time. •\n•That output token is appended to the prompt , then this updated prompt is presen‐ •\nted to the model again for another forward pass to generate the next token.\n•The three major components  of the Transformer LLM are the tokenizer, a stack of •\nTransformer blocks, and a language modeling head.\n•The tokenizer contains the token vocabulary  for the model. The model has token •\nembeddings  associated with those tokens. Breaking the text into tokens and then\nusing the embeddings of these tokens is the first step in the token generation\nprocess.\n•The forward pass flows through all the stages once, one by one . •\n•Near the end of the process, the LM head scores the probabilities of the next •\npossible token . Decoding strategies inform which actual token to pick as the\noutput for this generation step (sometimes it’s the most probable next token, but\nnot always).\n•One reason the Transformer excels is its ability to process tokens in parallel. •\nEach of the input tokens flow into their individual tracks or streams of processing .\nThe number of streams is the model’s “context size” and this represents the max\nnumber of tokens the model can operate on.\n•Because Transformer LLMs loop to generate the text one token at a time, it’s a•\ngood idea to cache  the processing results of each step so we don’t duplicate the\nprocessing effort (these results are stored as various matrices within the layers).\n•The majority of processing happens within Transformer blocks . These are made •\nup of two components. One of them is the feedforward neural network,  which is\nable to store information and make predictions and interpolations from data it\nwas trained on.\n•The second major component of a Transformer block is the attention  layer. •\nAttention incorporates contextual information to allow the model to better cap‐\nture the nuance of language.\n•Attention happens in two major steps: (1) scoring relevance and (2) combining•\ninformation.\n106 | Chapter 3: Looking Inside Large Language Models\n•A Transformer attention layer conducts several attention operations in parallel,•\neach occurring inside an attention head , and their outputs are aggregated to make\nup the output of the attention layer.\n•Attention can be accelerated via sharing the keys and values matrices between all•\nheads, or groups of heads ( grouped-query attention ).\n•Methods like Flash Attention  speed up the attention calculation by optimizing •\nhow the operation is done on the different memory systems of a GPU.\nTransformers continue to see new developments and proposed tweaks to improve\nthem in different scenarios, including language models and other domains and\napplications.\nIn Part II of the book, we will cover some of these practical applications of LLMs.\nIn Chapter 4 , we start with text classification, a common task in Language AI. This\nnext chapter serves as an introduction to applying both generative and representation\nmodels.\nSummary | 107",4108
48-Part II. Using Pretrained Language Models.pdf,48-Part II. Using Pretrained Language Models,PART II\nUsing Pretrained Language Models,41
49-Text Classification with Representation Models.pdf,49-Text Classification with Representation Models,"CHAPTER 4\nText Classification\nA common task in natural language processing is classification. The goal of the task\nis to train a model to assign a label or class to some input text (see Figure 4-1 ).\nClassifying text is used across the world for a wide range of applications, from\nsentiment analysis and intent detection to extracting entities and detecting language.\nThe impact of language models, both representative and generative, on classification\ncannot be understated.\nFigure 4-1. Using a language model to classify text.\nIn this chapter, we will discuss several ways to use language models for classifying\ntext. It will serve as an accessible introduction to using language models that already\nhave been trained. Due to the broad field of text classification, we will discuss several\ntechniques and use them to explore the field of language models:\n•“Text Classification with Representation Models” on page 113 demonstrates the •\nflexibility of nongenerative models for classification. We will cover both task-\nspecific models and embedding models.\n•“Text Classification with Generative Models” on page 127 is an introduction to •\ngenerative language models as most of them can be used for classification. We\nwill cover both an open source as well as a closed source language model.\n111\n1Bo Pang and Lillian Lee. “Seeing stars: Exploiting class relationships for sentiment categorization with respect\nto rating scales. ” arXiv preprint cs/0506075  (2005).\nIn this chapter, we will focus on leveraging pretrained language models, models that\nalready have been trained on large amounts of data that can be used for classifying\ntext. As illustrated in Figure 4-2 , we will examine both representation and language\nmodels and explore their differences.\nFigure 4-2. Although both representation and generative models can be used for classifi‐\ncation,  their approaches differ.\nThis chapter serves as an introduction to a variety of language models, both genera‐\ntive and nongenerative. We will encounter common packages for loading and using\nthese models.\nAlthough this book focuses on LLMs, it is highly advised to com‐\npare these examples against classic, but strong baselines such as\nrepresenting text with TF-IDF and training a logistic regression\nclassifier on top of that.\nThe Sentiment of Movie Reviews\nY ou can find the data we use to explore techniques for classifying text on the Hugging\nFace Hub, a platform for hosting models but also data. We will use the well-known\n“rotten_tomatoes” dataset  to train and evaluate our models.1 It contains 5,331 positive\nand 5,331 negative movie reviews from Rotten Tomatoes.\nTo load this data, we make use of the datasets  package, which will be used through‐\nout the book:\n112 | Chapter 4: Text Classification\nfrom datasets  import load_dataset\n# Load our data\ndata = load_dataset (""rotten_tomatoes"" )\ndata\nDatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 8530\n    })\n    validation: Dataset({\n        features: ['text', 'label'],\n        num_rows: 1066\n    })\n    test: Dataset({\n        features: ['text', 'label'],\n        num_rows: 1066\n    })\n})\nThe data is split up into train , test, and validation  splits. Throughout this chapter,\nwe will use the train split when we train a model and the  test split for validating\nthe results. Note that the additional validation split can be used to further validate\ngeneralization if you used the train and test splits to perform hyperparameter tuning.\nLet’s take a look at some examples in our train split:\ndata[""train""][0, -1]\n{'text': ['the rock is destined to be the 21st century\'s new "" conan "" and \nthat he\'s going to make a splash even greater than arnold schwarzenegger , \njean-claud van damme or steven segal .',\n  'things really get weird , though not particularly scary : the movie is all \nportent and no content .'],\n 'label': [1, 0]}\nThese short reviews are either labeled as positive (1) or negative (0). This means that\nwe will focus on binary sentiment classification.\nText Classification  with Representation Models\nClassification with pretrained representation models generally comes in two flavors,\neither using a task-specific model or an embedding model. As we explored in the\nprevious chapter, these models are created by fine-tuning a foundation model, like\nBERT, on a specific downstream task as illustrated in Figure 4-3 .\nText Classification  with Representation Models | 113\nFigure 4-3. A foundation model is fine-tuned  for specific  tasks; for instance, to perform\nclassification  or generate general-purpose embeddings.\nA task-specific model is a representation model, such as BERT, trained for a specific\ntask, like sentiment analysis. As we explored in Chapter 1 , an embedding model\ngenerates general-purpose embeddings that can be used for a variety of tasks not\nlimited to classification, like semantic search (see Chapter 8 ).\nThe process of fine-tuning a BERT model for classification is covered in Chapter 11\nwhile creating an embedding model is covered in Chapter 10 . In this chapter, we keep\nboth models frozen  (nontrainable) and only use their output as shown in Figure 4-4 .\nFigure 4-4. Perform classification  directly with a task-specific  model or indirectly with\ngeneral-purpose embeddings.\nWe will leverage pretrained models that others have already fine-tuned for us and\nexplore how they can be used to classify our selected movie reviews.\n114 | Chapter 4: Text Classification",5562
50-Using a Task-Specific Model.pdf,50-Using a Task-Specific Model,"2Yinhan Liuet et al. “RoBERTa: A robustly optimized BERT pretraining approach. ” arXiv preprint\narXiv:1907.11692  (2019).\n3Victor Sanh et al. “DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter. ” arXiv preprint\narXiv:1910.01108  (2019).\n4Zhenzhong Lan et al. “ ALBERT: A lite BERT for self-supervised learning of language representations. ” arXiv\npreprint arXiv:1909.11942  (2019).\n5Pengcheng He et al. “DeBERTa: Decoding-enhanced BERT with disentangled attention. ” arXiv preprint\narXiv:2006.03654  (2020).Model Selection\nChoosing  the right models is not as straightforward as you might think with over\n60,000 models on the Hugging Face Hub for text classification  and more than 8,000\nmodels that generate embeddings  at the moment of writing. Moreover, it’s crucial\nto select a model that fits your use case and consider its language compatibility, the\nunderlying architecture, size, and performance.\nLet’s start with the underlying architecture. As we explored in Chapter 1 , BERT, a\nwell-known encoder-only architecture, is a popular choice for creating task-specific\nand embedding models. While generative models, like the GPT family, are incredible\nmodels, encoder-only models similarly excel in task-specific use cases and tend to be\nsignificantly smaller in size.\nOver the years, many variations of BERT have been developed, including RoBERTa,2\nDistilBERT,3 ALBERT,4 and DeBERTa,5 each trained in various contexts. Y ou can find\nan overview of some well-known BERT-like models in Figure 4-5 .\nFigure 4-5. A timeline of common BERT-like model releases. These  are considered\nfoundation models and are mostly intended to be fine-tuned  on a downstream task.\nSelecting the right model for the job can be a form of art in itself. Trying thousands\nof pretrained models that can be found on Hugging Face’s Hub is not feasible so we\nneed to be efficient with the models that we choose. Having said that, several models\nModel Selection | 115\nare great starting points and give you an idea of the base performance of these kinds\nof models. Consider them solid baselines:\n•BERT base model (uncased)•\n•RoBERTa base model•\n•DistilBERT base model (uncased)•\n•DeBERTa base model•\n•bert-tiny•\n•ALBERT base v2•\nFor the task-specific model, we are choosing the Twitter-RoBERTa-base for Senti‐\nment Analysis  model. This is a RoBERTa model fine-tuned on tweets for sentiment\nanalysis. Although this was not trained specifically for movie reviews, it is interesting\nto explore how this model generalizes.\nWhen selecting models to generate embeddings from, the MTEB leaderboard  is a\ngreat place to start. It contains open and closed source models benchmarked across\nseveral tasks. Make sure to not only take performance into account. The importance\nof inference speed should not be underestimated in real-life solutions. As such, we\nwill use sentence-transformers/all-mpnet-base-v2  as the embedding throughout this\nsection. It is a small but performant model.\nUsing a Task-Specific  Model\nNow that we have selected our task-specific representation model, let’s start by load‐\ning our model:\nfrom transformers  import pipeline\n# Path to our HF model\nmodel_path  = ""cardiffnlp/twitter-roberta-base-sentiment-latest""\n# Load model into pipeline\npipe = pipeline (\n    model=model_path ,\n    tokenizer =model_path ,\n    return_all_scores =True,\n    device=""cuda:0""\n)\n116 | Chapter 4: Text Classification\nAs we load our model, we also load the tokenizer,  which is responsible for converting\ninput text into individual tokens, as illustrated in Figure 4-6 . Although that parameter\nis not needed as it is loaded automatically, it illustrates what is happening under the\nhood.\nFigure 4-6. An input sentence is first fed to a tokenizer before it can be processed by the\ntask-specific  model.\nThese tokens are at the core of most language models, as explored in depth in\nChapter 2 . A major benefit of these tokens is that they can be combined to generate\nrepresentations even if they were not in the training data, as shown in Figure 4-7 .\nUsing a Task-Specific  Model | 117\nFigure 4-7. By breaking down an unknown word into tokens, word embeddings can still\nbe generated.\nAfter loading all the necessary components, we can go ahead and use our model on\nthe test split of our data:\nimport numpy as np\nfrom tqdm import tqdm\nfrom transformers.pipelines.pt_utils  import KeyDataset\n# Run inference\ny_pred = []\nfor output in tqdm(pipe(KeyDataset (data[""test""], ""text"")), \ntotal=len(data[""test""])):\n    negative_score  = output[0][""score""]\n    positive_score  = output[2][""score""]\n    assignment  = np.argmax([negative_score , positive_score ])\n    y_pred.append(assignment )\nNow that we have generated our predictions, all that is left is evaluation. We create a\nsmall function that we can easily use throughout this chapter:\nfrom sklearn.metrics  import classification_report\ndef evaluate_performance (y_true, y_pred):\n    """"""Create and print the classification report""""""\n    performance  = classification_report (\n        y_true, y_pred,\n        target_names =[""Negative Review"" , ""Positive Review"" ]\n118 | Chapter 4: Text Classification\n    )\n    print(performance )\nNext, let’s create our classification report:\nevaluate_performance (data[""test""][""label""], y_pred)\n                precision    recall  f1-score   support\nNegative Review       0.76      0.88      0.81       533\nPositive Review       0.86      0.72      0.78       533\n       accuracy                           0.80      1066\n      macro avg       0.81      0.80      0.80      1066\n   weighted avg       0.81      0.80      0.80      1066\nTo read the resulting classification report, let’s first start by exploring how we can\nidentify correct and incorrect predictions. There are four combinations depending\non whether we predict something correctly (True) versus incorrectly (False) and\nwhether we predict the correct class (Positive) versus incorrect class (Negative). We\ncan illustrate these combinations as a matrix, commonly referred to as a confusion\nmatrix , in Figure 4-8 .\nFigure 4-8. The confusion matrix describes four types of predictions we can make.\nUsing the confusion matrix, we can derive several formulas to describe the quality\nof the model. In the previously generated classification report we can see four such\nmethods, namely precision , recall , accuracy , and the F1 score:\n•Precision  measures how many of the items found are relevant, which indicates the •\naccuracy of the relevant results.\n•Recall  refers to how many relevant classes were found, which indicates its ability •\nto find all relevant results.\nUsing a Task-Specific  Model | 119",6763
51-Supervised Classification.pdf,51-Supervised Classification,"•Accuracy  refers to how many correct predictions the model makes out of all •\npredictions, which indicates the overall correctness of the model.\n•The F1 score  balances both precision and recall to create a model’s overall •\nperformance.\nThese four metrics are illustrated in Figure 4-9 , which describes them using the\naforementioned classification report.\nFigure 4-9. The classification  report describes several metrics for evaluating a model’s\nperformance.\nWe will consider the weighted average of the F1 score throughout the examples in\nthis book to make sure each class is treated equally. Our pretrained BERT model gives\nus an F1 score of 0.80 (we are reading this from the weighted avg  row and the f1-score\ncolumn), which is great for a model not trained specifically on our domain data!\nTo improve the performance of our selected model, we could do a few different\nthings including selecting a model trained on our domain data, movie reviews in this\ncase, like DistilBERT base uncased finetuned SST-2 . We could also shift our focus to\nanother flavor of representation models, namely embedding models.\nClassification  Tasks That Leverage Embeddings\nIn the previous example, we used a pretrained task-specific model for sentiment\nanalysis. However, what if we cannot find a model that was pretrained for this specific\ntask? Do we need to fine-tune a representation model ourselves? The answer is no!\n120 | Chapter 4: Text Classification\nThere might be times when you want to fine-tune the model yourself if you have\nsufficient computing available (see Chapter 11 ). However, not everyone has access to\nextensive computing. This is where general-purpose embedding models come in.\nSupervised Classification\nUnlike the previous example, we can perform part of the training process ourselves\nby approaching it from a more classical perspective. Instead of directly using the rep‐\nresentation model for classification, we will use an embedding model for generating\nfeatures. Those features can then be fed into a classifier, thereby creating a two-step\napproach as shown in Figure 4-10 .\nFigure 4-10. The feature extraction step and classification  steps are separated.\nA major benefit of this separation is that we do not need to fine-tune our embedding\nmodel, which can be costly. In contrast, we can train a classifier, like a logistic\nregression, on the CPU instead.\nIn the first step, we convert our textual input to embeddings using the embedding\nmodel as shown in Figure 4-11 . Note that this model is similarly kept frozen  and is\nnot updated during the training process.\nFigure 4-11. In step 1, we use the embedding model to extract the features and convert\nthe input text to embeddings.\nClassification  Tasks That Leverage Embeddings | 121\n6Nils Reimers and Iryna Gurevych. “Sentence-BERT: Sentence embeddings using Siamese BERT-networks. ”\narXiv preprint arXiv:1908.10084  (2019).We can perform this step with sentence-transformer , a popular package for lever‐\naging pretrained embedding models.6 Creating the embeddings is straightforward:\nfrom sentence_transformers  import SentenceTransformer\n# Load model\nmodel = SentenceTransformer (""sentence-transformers/all-mpnet-base-v2"" )\n# Convert text to embeddings\ntrain_embeddings  = model.encode(data[""train""][""text""], show_progress_bar =True)\ntest_embeddings  = model.encode(data[""test""][""text""], show_progress_bar =True)\nAs we covered in Chapter 1 , these embeddings are numerical representations of the\ninput text. The number of values, or dimension, of the embedding depends on the\nunderlying embedding model. Let’s explore that for our model:\ntrain_embeddings .shape\n(8530, 768)\nThis shows that each of our 8,530 input documents has an embedding dimension of\n768 and therefore each embedding contains 768 numerical values.\nIn the second step, these embeddings serve as the input features to the classifier illus‐\ntrated in Figure 4-12 . The classifier is trainable and not limited to logistic regression\nand can take on any form as long as it performs classification.\nFigure 4-12. Using the embeddings as our features, we train a logistic regression model\non our training data.\nWe will keep this step straightforward and use a logistic regression as the classifier. To\ntrain it, we only need to use the generated embeddings together with our labels:\n122 | Chapter 4: Text Classification",4419
52-What If We Do Not Have Labeled Data.pdf,52-What If We Do Not Have Labeled Data,"from sklearn.linear_model  import LogisticRegression\n# Train a logistic regression on our train embeddings\nclf = LogisticRegression (random_state =42)\nclf.fit(train_embeddings , data[""train""][""label""])\nNext, let’s evaluate our model:\n# Predict previously unseen instances\ny_pred = clf.predict(test_embeddings )\nevaluate_performance (data[""test""][""label""], y_pred)\n              precision    recall  f1-score   support\nNegative Review       0.85      0.86      0.85       533\nPositive Review       0.86      0.85      0.85       533\n       accuracy                           0.85      1066\n      macro avg       0.85      0.85      0.85      1066\n   weighted avg       0.85      0.85      0.85      1066\nBy training a classifier on top of our embeddings, we managed to get an F1 score\nof 0.85! This demonstrates the possibilities of training a lightweight classifier while\nkeeping the underlying embedding model frozen.\nIn this example, we used sentence-transformers  to extract our\nembeddings, which benefits from a GPU to speed up inference.\nHowever, we can remove this GPU dependency by using an exter‐\nnal API to create the embeddings. Popular choices for generating\nembeddings are Cohere’s and OpenAI’s offerings. As a result, this\nwould allow the pipeline to run entirely on the CPU.\nWhat If We Do Not Have Labeled Data?\nIn our previous example, we had labeled data that we could leverage, but this might\nnot always be the case in practice. Getting labeled data is a resource-intensive task\nthat can require significant human labor. Moreover, is it actually worthwhile to collect\nthese labels?\nTo test this, we can perform zero-shot classification, where we have no labeled data\nto explore whether the task seems feasible. Although we know the definition of\nthe labels (their names), we do not have labeled data to support them. Zero-shot\nclassification attempts to predict the labels of input text even though it was not\ntrained on them, as shown in Figure 4-13 .\nClassification  Tasks That Leverage Embeddings | 123\nFigure 4-13. In zero-shot classification,  we have no labeled data, only the labels them‐\nselves. The zero-shot model decides how the input is related to the candidate labels.\nTo perform zero-shot classification with embeddings, there is a neat trick that we can\nuse. We can describe our labels based on what they should represent. For example, a\nnegative label for movie reviews can be described as “This is a negative movie review. ”\nBy describing and embedding the labels and documents, we have data that we can\nwork with. This process, as illustrated in Figure 4-14 , allows us to generate our own\ntarget labels without the need to actually have any labeled data.\nFigure 4-14. To embed the labels, we first need to give them a description, such as “a\nnegative movie review. ” This can then be embedded through sentence-transformers.\n124 | Chapter 4: Text Classification\nWe can create these label embeddings using the .encode  function as we did earlier:\n# Create embeddings for our labels\nlabel_embeddings  = model.encode([""A negative review"" ,  ""A positive review"" ])\nTo assign labels to documents, we can apply cosine similarity to the document label\npairs. This is the cosine of the angle between vectors, which is calculated through\nthe dot product of the embeddings and divided by the product of their lengths, as\nillustrated in Figure 4-15 .\nFigure 4-15. The cosine similarity is the angle between two vectors or embeddings. In this\nexample, we calculate the similarity between a document and the two possible labels,\npositive and negative.\nWe can use cosine similarity to check how similar a given document is to the descrip‐\ntion of the candidate labels. The label with the highest similarity to the document is\nchosen as illustrated in Figure 4-16 .\nFigure 4-16. After  embedding the label descriptions and the documents, we can use\ncosine similarity for each label document pair.\nClassification  Tasks That Leverage Embeddings | 125\nTo perform cosine similarity on the embeddings, we only need to compare the\ndocument embeddings with the label embeddings and get the best matching pairs:\nfrom sklearn.metrics.pairwise  import cosine_similarity\n# Find the best matching label for each document\nsim_matrix  = cosine_similarity (test_embeddings , label_embeddings )\ny_pred = np.argmax(sim_matrix , axis=1)\nAnd that is it! We only needed to come up with names for our labels to perform our\nclassification tasks. Let’s see how well this method works:\nevaluate_performance (data[""test""][""label""], y_pred)\n                precision    recall  f1-score   support\nNegative Review       0.78      0.77      0.78       533\nPositive Review       0.77      0.79      0.78       533\n       accuracy                           0.78      1066\n      macro avg       0.78      0.78      0.78      1066\n   weighted avg       0.78      0.78      0.78      1066\nIf you are familiar with zero-shot classification  with Transformer-\nbased models, you might wonder why we choose to illustrate this\nwith embeddings instead. Although natural language inference\nmodels are amazing for zero-shot classification, the example here\ndemonstrates the flexibility of embeddings for a variety of tasks.\nAs you will see throughout the book, embeddings can be found in\nmost Language AI use cases and are often an underestimated but\nincredibly vital component.\nAn F1 score of 0.78 is quite impressive considering we did not use any labeled data at\nall! This just shows how versatile and useful embeddings are, especially if you are a bit\ncreative with how they are used.\nLet’s put that creativity to the test. We decided upon “ A nega‐\ntive/positive review” as the name of our labels but that can be\nimproved. Instead, we can make them a bit more concrete and\nspecific toward our data by using “ A very negative/positive movie\nreview” instead. This way, the embedding will capture that it is a\nmovie review and will focus a bit more on the extremes of the two\nlabels. Try it out and explore how it affects the results.\n126 | Chapter 4: Text Classification",6147
53-Using the Text-to-Text Transfer Transformer.pdf,53-Using the Text-to-Text Transfer Transformer,"Text Classification  with Generative Models\nClassification with generative language models, such as OpenAI’s GPT models, works\na bit differently from what we have done thus far. These models take as input some\ntext and generative text and are thereby aptly named sequence-to-sequence models.\nThis is in stark contrast to our task-specific model, which outputs a class instead, as\nillustrated in Figure 4-17 .\nFigure 4-17. A task-specific  model generates numerical values from sequences of tokens\nwhile a generative model generates sequences of tokens from sequences of tokens.\nThese generative models are generally trained on a wide variety of tasks and usually\ndo not perform your use case out of the box. For instance, if we give a generative\nmodel a movie review without any context, it has no idea what to do with it.\nInstead, we need to help it understand the context and guide it toward the answers\nthat we are looking for. As demonstrated in Figure 4-18 , this guiding process is done\nmainly through the instruction, or prompt , that you give such a model. Iteratively\nimproving your prompt to get your preferred output is called prompt engineering .\nText Classification  with Generative Models | 127\n7Colin Raffel et al. “Exploring the limits of transfer learning with a unified text-to-text transformer. ” The\nJournal of Machine Learning Research  21.1 (2020): 5485–5551.\nFigure 4-18. Prompt engineering allows prompts to be updated to improve the output\ngenerated by the model.\nIn this section, we will demonstrate how we can leverage different types of generative\nmodels to perform classification without our Rotten Tomatoes dataset.\nUsing the Text-to-Text Transfer Transformer\nThroughout  this book, we will explore mostly encoder-only (representation) mod‐\nels like BERT and decoder-only (generative) models like ChatGPT. However, as\ndiscussed in Chapter 1 , the original Transformer architecture actually consists of an\nencoder-decoder architecture. Like the decoder-only models, these  encoder-decoder\nmodels are sequence-to-sequence models and generally fall in the category of genera‐\ntive models.\nAn interesting family of models that leverage this architecture is the Text-to-Text\nTransfer Transformer or T5 model. Illustrated in Figure 4-19 , its architecture is\nsimilar to the original Transformer where 12 decoders and 12 encoders are stacked\ntogether.7\n128 | Chapter 4: Text Classification\nFigure 4-19. The T5 architecture is similar to the original Transformer model, a decoder-\nencoder architecture.\nWith this architecture, these models were first pretrained using masked language\nmodeling. In the first step of training, illustrated in Figure 4-20 , instead of masking\nindividual tokens, sets of tokens (or token spans ) were masked during pretraining.\nFigure 4-20. In the first step of training, namely pretraining, the T5 model needs to\npredict masks that could contain multiple tokens.\nThe second step of training, namely fine-tuning the base model, is where the real\nmagic happens. Instead of fine-tuning the model for one specific task, each task is\nconverted to a sequence-to-sequence task and trained simultaneously. As illustrated\nin Figure 4-21 , this allows the model to be trained on a wide variety of tasks.\nText Classification  with Generative Models | 129\n8Hyung Won Chung et al. “Scaling instruction-finetuned language models. ” arXiv preprint arXiv:2210.11416\n(2022).\nFigure 4-21. By converting specific  tasks to textual instructions, the T5 model can be\ntrained on a variety of tasks during fine-tuning.\nThis method of fine-tuning was extended in the paper “Scaling instruction-finetuned\nlanguage models” , which introduced more than a thousand tasks during fine-tuning\nthat more closely follow instructions as we know them from GPT models.8 This\nresulted in the Flan-T5 family of models that benefit from this large variety of tasks.\nTo use this pretrained Flan-T5 model for classification, we will start by loading it\nthrough the ""text2text-generation""  task, which is generally reserved for these\nencoder-decoder models:\n# Load our model\npipe = pipeline (\n    ""text2text-generation"" , \n    model=""google/flan-t5-small"" , \n    device=""cuda:0""\n)\nThe Flan-T5 model comes in various sizes (flan-t5-small/base/large/xl/xxl) and we\nwill use the smallest to speed things up a bit. However, feel free to play around with\nlarger models to see if you can improve the results.\nCompared to our task-specific model, we cannot just give the model some text and\nhope it will output the sentiment. Instead, we will have to instruct the model to do so.\n130 | Chapter 4: Text Classification\nThus, we prefix each document with the prompt “Is the following sentence positive or\nnegative?”:\n# Prepare our data\nprompt = ""Is the following sentence positive or negative? ""\ndata = data.map(lambda example: {""t5"": prompt + example['text']})\ndata\nDatasetDict({\n    train: Dataset({\n        features: ['text', 'label', 't5'],\n        num_rows: 8530\n    })\n    validation: Dataset({\n        features: ['text', 'label', 't5'],\n        num_rows: 1066\n    })\n    test: Dataset({\n        features: ['text', 'label', 't5'],\n        num_rows: 1066\n    })\n})\nAfter creating our updated data, we can run the pipeline similar to the task-specific\nexample:\n# Run inference\ny_pred = []\nfor output in tqdm(pipe(KeyDataset (data[""test""], ""t5"")), \ntotal=len(data[""test""])):\n    text = output[0][""generated_text"" ]\n    y_pred.append(0 if text == ""negative""  else 1)\nSince this model generates text, we did need to convert the textual output to numer‐\nical values. The output word “negative” was mapped to 0 whereas “positive” was\nmapped to 1.\nThese numerical values now allow us to test the quality of the model in the same way\nwe have done before:\nevaluate_performance (data[""test""][""label""], y_pred)\n                precision    recall  f1-score   support\nNegative Review       0.83      0.85      0.84       533\nPositive Review       0.85      0.83      0.84       533\n       accuracy                           0.84      1066\n      macro avg       0.84      0.84      0.84      1066\n   weighted avg       0.84      0.84      0.84      1066\nWith an F1 score of 0.84, it is clear this Flan-T5 model is an amazing first look into\nthe capabilities of generative models.\nText Classification  with Generative Models | 131",6432
54-ChatGPT for Classification.pdf,54-ChatGPT for Classification,"ChatGPT for Classification\nAlthough we focus throughout the book on open source models, another major com‐\nponent of the Language AI field is closed sourced models; in particular, ChatGPT.\nAlthough the underlying architecture of the original ChatGPT model (GPT-3.5)\nis not shared, we can assume from its name that it is based on the decoder-only\narchitecture that we have seen in the GPT models thus far.\nFortunately, OpenAI shared an overview of the training procedure  that involved\nan important component, namely preference tuning. As illustrated in Figure 4-22 ,\nOpenAI first manually created the desired output to an input prompt (instruction\ndata) and used that data to create a first variant of its model.\nFigure 4-22. Manually labeled data consisting of an instruction (prompt) and output\nwas used to perform fine-tuning  (instruction-tuning).\nOpenAI used the resulting model to generate multiple outputs that were manually\nranked from best to worst. As shown in Figure 4-23 , this ranking demonstrates\na preference for certain outputs (preference data) and was used to create its final\nmodel, ChatGPT.\nFigure 4-23. Manually ranked preference data was used to generate the final  model,\nChatGPT.\n132 | Chapter 4: Text Classification\nA major benefit of using preference data over instruction data is the nuance it\nrepresents. By demonstrating the difference between a good and better output the\ngenerative model learns to generate text that resembles human preference. In Chap‐\nter 12 , we will explore how these fine-tuning and preference-tuning methodologies\nwork and how you can perform them yourself.\nThe process of using a closed sourced model is quite different from the open sourced\nexamples we have seen thus far. Instead of loading the model, we can access the\nmodel through OpenAI’s API.\nBefore we go into the classification example, you will first need to create a free\naccount on https://oreil.ly/AEXvA  and create an API key here: https://oreil.ly/lrTXl .\nAfter doing so, you can use your API to communicate with OpenAI’s servers.\nWe can use this key to create a client:\nimport openai\n# Create client\nclient = openai.OpenAI(api_key=""YOUR_KEY_HERE"" )\nUsing this client, we create the chatgpt_generation  function, which allows us to\ngenerate some text based on a specific prompt, input document, and the selected\nmodel:\ndef chatgpt_generation (prompt, document , model=""gpt-3.5-turbo-0125"" ):\n    """"""Generate an output based on a prompt and an input document.""""""\n    messages =[\n        {\n            ""role"": ""system"" ,\n            ""content"" : ""You are a helpful assistant.""\n            },\n        {\n            ""role"": ""user"",\n            ""content"" :   prompt.replace(""[DOCUMENT]"" , document )\n            }\n    ]\n    chat_completion  = client.chat.completions .create(\n      messages =messages ,\n      model=model,\n      temperature =0\n    )\n    return chat_completion .choices[0].message.content\nText Classification  with Generative Models | 133\nNext, we will need to create a template to ask the model to perform the classification:\n# Define a prompt template as a base\nprompt = """"""Predict whether the following document is a positive or negative \nmovie review:\n[DOCUMENT]\nIf it is positive return 1 and if it is negative return 0. Do not give any \nother answers.\n""""""\n# Predict the target using GPT\ndocument  = ""unpretentious , charming , quirky , original""\nchatgpt_generation (prompt, document )\nThis template is merely an example and can be changed however you want. For now,\nwe kept it as simple as possible to illustrate how to use such a template.\nBefore you use this over a potentially large dataset, it is important to always keep\ntrack of your usage. External APIs such as OpenAI’s offering can quickly become\ncostly if you perform many requests. At the time of writing, running our test dataset\nusing the “gpt-3.5-turbo-0125” model costs 3 cents, which is covered by the free\naccount, but this might change in the future.\nWhen dealing with external APIs, you might run into rate limit\nerrors. These appear when you call the API too often as some APIs\nmight limit the rate with which you can use it per minute or hour.\nTo prevent these errors, we can implement several methods for\nretrying the request, including something referred to as exponential\nbackoff . It performs a short sleep each time we hit a rate limit\nerror and then retries the unsuccessful request. Whenever it is\nunsuccessful again, the sleep length is increased until the request is\nsuccessful or we hit a maximum number of retries.\nTo use it with OpenAI, there is a great guide  that can help you get\nstarted.\nNext, we can run this for all reviews in the test dataset to get its predictions. Y ou can\nskip this if you want to save your (free) credits for other tasks.\n# You can skip this if you want to save your (free) credits\npredictions  = [\n    chatgpt_generation (prompt, doc) for doc in tqdm(data[""test""][""text""])\n]\nLike the previous example, we need to convert the output from strings to integers to\nevaluate its performance:\n134 | Chapter 4: Text Classification",5162
55-Summary.pdf,55-Summary,"# Extract predictions\ny_pred = [int(pred) for pred in predictions ]\n# Evaluate performance\nevaluate_performance (data[""test""][""label""], y_pred)\n                precision    recall  f1-score   support\nNegative Review       0.87      0.97      0.92       533\nPositive Review       0.96      0.86      0.91       533\n       accuracy                           0.91      1066\n      macro avg       0.92      0.91      0.91      1066\n   weighted avg       0.92      0.91      0.91      1066\nThe F1 score of 0.91 already gives a glimpse into the performance of the model that\nbrought generative AI to the masses. However, since we do not know what data the\nmodel was trained on, we cannot easily use these kinds of metrics for evaluating the\nmodel. For all we know, it might have actually been trained on our dataset!\nIn Chapter 12 , we will explore how we can evaluate both open source and closed\nsource models on more generalized tasks.\nSummary\nIn this chapter, we discussed many different techniques for performing a wide variety\nof classification tasks, from fine-tuning your entire model to no tuning at all! Classi‐\nfying textual data is not as straightforward as it may seem on the surface and there is\nan incredible amount of creative techniques for doing so.\nIn this chapter, we explored text classification using both generative and representa‐\ntion language models. Our goal was to assign a label or class to input text for the\nclassification of a review’s sentiment.\nWe explored two types of representation models, a task-specific model and an\nembedding model. The task-specific model was pretrained on a large dataset specif‐\nically for sentiment analysis and showed us that pretrained models are a great\ntechnique for classifying documents. The embedding model was used to generate\nmultipurpose embeddings that we used as the input to train a classifier.\nSimilarly, we explored two types of generative models, an open source encoder-\ndecoder model (Flan-T5) and a closed source decoder-only model (GPT-3.5). We\nused these generative models in text classification without requiring specific (addi‐\ntional) training on domain data or labeled datasets.\nIn the next chapter, we will continue with classification but focus instead on unsuper‐\nvised classification. What can we do if we have textual data without any labels? What\ninformation can we extract? We will focus on clustering our data as well as naming\nthe clusters with topic modeling techniques.\nSummary | 135",2509
56-A Common Pipeline for Text Clustering.pdf,56-A Common Pipeline for Text Clustering,"CHAPTER 5\nText Clustering and Topic Modeling\nAlthough  supervised techniques, such as classification, have reigned supreme over\nthe last few years in the industry, the potential of unsupervised techniques such as\ntext clustering cannot be understated.\nText clustering aims to group similar texts based on their semantic content, meaning,\nand relationships. As illustrated in Figure 5-1 , the resulting clusters of semantically\nsimilar documents not only facilitate efficient categorization of large volumes of\nunstructured text but also allow for quick exploratory data analysis.\nFigure 5-1. Clustering unstructured textual data.\nThe recent evolution of language models, which enable contextual and semantic\nrepresentations of text, has enhanced the effectiveness of text clustering. Language\nis more than a bag of words, and recent language models have proved to be quite\n137\ncapable of capturing that notion. Text clustering, unbound by supervision, allows for\ncreative solutions and diverse applications, such as finding outliers, speedup labeling,\nand finding incorrectly labeled data.\nText clustering has also found itself in the realm of topic modeling, where we want to\ndiscover (abstract) topics that appear in large collections of textual data. As shown in\nFigure 5-2 , we generally describe a topic using keywords or keyphrases and, ideally,\nhave a single overarching label.\nFigure 5-2. Topic modeling is a way to give meaning to clusters of textual documents.\nIn this chapter, we will first explore how to perform clustering with embedding\nmodels and then transition to a text-clustering-inspired method of topic modeling,\nnamely BERTopic.\nText clustering and topic modeling have an important role in this book as they\nexplore creative ways to combine a variety of different language models. We will\nexplore how combining encoder-only (embeddings), decoder-only (generative), and\neven classical methods (bag-of-words) can result in amazing new techniques and\npipelines.\nArXiv’s Articles: Computation and Language\nThroughout  this chapter, we will be running clustering and topic modeling algo‐\nrithms on ArXiv articles. ArXiv  is an open-access platform for scholarly articles,\nmostly in the fields of computer science, mathematics, and physics. We will explore\narticles in the field of Computation and Language to keep with the theme of this\nbook. The dataset  contains 44,949 abstracts between 1991 and 2024 from ArXiv’s\ncs.CL  (Computation and Language) section.\nWe load the data and create separate variables for the abstracts, titles, and years of\neach article:\n138 | Chapter 5: Text Clustering and Topic Modeling",2668
57-Reducing the Dimensionality of Embeddings.pdf,57-Reducing the Dimensionality of Embeddings,"# Load data from Hugging Face\nfrom datasets  import load_dataset\ndataset = load_dataset (""maartengr/arxiv_nlp"" )[""train""]\n# Extract metadata\nabstracts  = dataset[""Abstracts"" ]\ntitles = dataset[""Titles"" ]\nA Common Pipeline for Text Clustering\nText  clustering allows for discovering patterns in data that you may or may not be\nfamiliar with. It allows for getting an intuitive understanding of the task, for example,\na classification task, but also of its complexity. As a result, text clustering can become\nmore than just a quick method for exploratory data analysis.\nAlthough there are many methods for text clustering, from graph-based neural net‐\nworks to centroid-based clustering techniques, a common pipeline that has gained\npopularity involves three steps and algorithms:\n1.Convert the input documents to embeddings with an embedding model . 1.\n2.Reduce the dimensionality of embeddings with a dimensionality reduction model . 2.\n3.Find groups of semantically similar documents with a cluster model . 3.\nEmbedding Documents\nThe first step is to convert our textual data to embeddings, as illustrated in Figure 5-3 .\nRecall from previous chapters that embeddings are numerical representations of text\nthat attempt to capture its meaning.\nFigure 5-3. Step 1: We convert documents to embeddings using an embedding model.\nChoosing embedding models optimized for semantic similarity tasks is especially\nimportant for clustering as we attempt to find groups of semantically similar\ndocuments.  Fortunately, most embedding models at the time of writing focus on\njust that, semantic similarity.\nA Common Pipeline for Text Clustering | 139\nAs we did in the previous chapter, we will use the MTEB leaderboard  to select\nan embedding model. We will need an embedding model that has a decent score\non clustering tasks but also is small enough to run quickly. Instead of using the\n“sentence-transformers/all-mpnet-base-v2” model we used in the previous chapter,\nwe use the “thenlper/gte-small”  model instead. It is a more recent model that outper‐\nforms the previous model on clustering tasks and due to its small size is even faster\nfor inference. However, feel free to play around with newer models that have been\nreleased since!\nfrom sentence_transformers  import SentenceTransformer\n# Create an embedding for each abstract\nembedding_model  = SentenceTransformer (""thenlper/gte-small"" )\nembeddings  = embedding_model .encode(abstracts , show_progress_bar =True)\nLet’s check how many values each document embedding contains:\n# Check the dimensions of the resulting embeddings\nembeddings .shape\n(44949, 384)\nEach embedding has 384 values that together represent the semantic representation\nof the document. Y ou can view these embeddings as the features that we want to\ncluster.\nReducing the Dimensionality of Embeddings\nBefore we cluster the embeddings, we will first need to take their high dimensionality\ninto account. As the number of dimensions increases, there is an exponential growth\nin the number of possible values within each dimension. Finding all subspaces within\neach dimension becomes increasingly complex.\nAs a result, high-dimensional data can be troublesome for many clustering tech‐\nniques as it gets more difficult to identify meaningful clusters. Instead, we can make\nuse of dimensionality reduction. As illustrated in Figure 5-4 , this technique allows\nus to reduce the size of the dimensional space and represent the same data with\nfewer dimensions. Dimensionality reduction techniques aim to preserve the global\nstructure of high-dimensional data by finding low-dimensional representations.\n140 | Chapter 5: Text Clustering and Topic Modeling\n1Harold Hotelling. “ Analysis of a complex of statistical variables into principal components. ” Journal of Educa‐\ntional Psychology  24.6 (1933): 417.\n2Leland McInnes, John Healy, and James Melville. “UMAP: Uniform Manifold Approximation and Projection\nfor dimension reduction. ” arXiv preprint arXiv:1802.03426  (2018).\nFigure 5-4. Dimensionality reduction allows data in high-dimensional space to be com‐\npressed to a lower-dimensional representation.\nNote that this is a compression technique and that the underlying algorithm is not\narbitrarily removing dimensions. To help the cluster model create meaningful clus‐\nters, the second step in our clustering pipeline is therefore dimensionality reduction,\nas shown in Figure 5-5 .\nFigure 5-5. Step 2: The embeddings are reduced to a lower-dimensional space using\ndimensionality reduction.\nWell-known methods for dimensionality reduction are Principal Component Analy‐\nsis (PCA)1 and Uniform Manifold Approximation and Projection (UMAP).2 For this\nA Common Pipeline for Text Clustering | 141",4778
58-Cluster the Reduced Embeddings.pdf,58-Cluster the Reduced Embeddings,"pipeline, we are going with UMAP as it tends to handle nonlinear relationships and\nstructures a bit better than PCA.\nDimensionality reduction techniques, however, are not flawless.\nThey do not perfectly capture high-dimensional data in a lower-\ndimensional representation. Information will always be lost with\nthis procedure. There is a balance between reducing dimensionality\nand keeping as much information as possible.\nTo perform dimensionality reduction, we need to instantiate our UMAP class and\npass the generated embeddings to it:\nfrom umap import UMAP\n# We reduce the input embeddings from 384 dimensions to 5 dimensions\numap_model  = UMAP(\n    n_components =5, min_dist =0.0, metric='cosine' , random_state =42\n)\nreduced_embeddings  = umap_model .fit_transform (embeddings )\nWe can use the n_components  parameter  to decide the shape of the lower-\ndimensional space, namely 5 dimensions. Generally, values between 5 and 10 work\nwell to capture high-dimensional global structures.\nThe min_dist  parameter is the minimum distance between embedded points. We are\nsetting this to 0 as that generally results in tighter clusters. We set metric  to 'cosine'\nas Euclidean-based methods have issues dealing with high-dimensional data.\nNote that setting a random_state  in UMAP will make the results reproducible across\nsessions but will disable parallelism and therefore slow down training.\nCluster the Reduced Embeddings\nThe third step is to cluster the reduced embeddings, as illustrated in Figure 5-6 .\nFigure 5-6. Step 3: We cluster the documents using the embeddings with reduced\ndimensionality.\n142 | Chapter 5: Text Clustering and Topic Modeling\n3Leland McInnes, John Healy, and Steve Astels. “hdbscan: Hierarchical density based clustering. ” J. Open\nSource Softw.  2.11 (2017): 205.\n4Martin Ester et al. “ A density-based algorithm for discovering clusters in large spatial databases with noise. ”\nKDD’96,  Aug. 1996: 226–231.Although a common choice is a centroid-based algorithm like  k-means, which\nrequires a set of clusters to be generated, we do not know the number of clusters\nbeforehand. Instead, a density-based algorithm freely calculates the number of clus‐\nters and does not force all data points to be part of a cluster, as illustrated in\nFigure 5-7 .\nFigure 5-7. The clustering algorithm not only impacts how clusters are generated but also\nhow they are viewed.\nA common density-based model is Hierarchical Density-Based Spatial Clustering of\nApplications with Noise (HDBSCAN).3 HDBSCAN is a hierarchical variation of a\nclustering algorithm called DBSCAN that allows for dense (micro)-clusters to be\nfound without having to explicitly specify the number of clusters.4 As a density-based\nmethod, HDBSCAN can also detect outliers  in the data, which are data points that do\nnot belong to any cluster. These outliers will not be assigned or forced to belong to\nany cluster. In other words, they are ignored. Since ArXiv articles might contain some\nniche papers, using a model that detects outliers could be helpful.\nAs with the previous packages, using HDBSCAN is straightforward. We only need to\ninstantiate the model and pass our reduced embeddings to it:\nA Common Pipeline for Text Clustering | 143",3271
59-Inspecting the Clusters.pdf,59-Inspecting the Clusters,"from hdbscan import HDBSCAN\n# We fit the model and extract the clusters\nhdbscan_model  = HDBSCAN(\n    min_cluster_size =50, metric=""euclidean"" , cluster_selection_method =""eom""\n).fit(reduced_embeddings )\nclusters  = hdbscan_model .labels_\n# How many clusters did we generate?\nlen(set(clusters ))\n156\nWith HDBSCAN, we generated 156 clusters in our dataset. To create more clusters,\nwe will need to reduce the value of min_cluster_size  as it represents the minimum\nsize that a cluster can take.\nInspecting the Clusters\nNow  that we have generated our clusters, we can inspect each cluster manually and\nexplore the assigned documents to get an understanding of its content. For example,\nlet us take a few random documents from cluster 0:\nimport numpy as np\n# Print first three documents in cluster 0\ncluster = 0\nfor index in np.where(clusters ==cluster)[0][:3]:\n    print(abstracts [index][:300] + ""... \n"")\nThis works aims to design a statistical machine translation from English text\nto American Sign Language (ASL). The system is based on Moses tool with some\nmodifications and the results are synthesized through a 3D avatar for\ninterpretation. First, we translate the input text to gloss, a written fo...\nResearches on signed languages still strongly dissociate lin- guistic issues\nrelated on phonological and phonetic aspects, and gesture studies for\nrecognition and synthesis purposes. This paper focuses on the imbrication of\nmotion and meaning for the analysis, synthesis and evaluation of sign lang...\nModern computational linguistic software cannot produce important aspects of\nsign language translation. Using some researches we deduce that the majority of\nautomatic sign language translation systems ignore many aspects when they\ngenerate animation; therefore the interpretation lost the truth inf...\nFrom these documents, it seems that this cluster contains documents mostly about\ntranslation from and to sign language, interesting!\nWe can take this one step further and attempt to visualize our results instead of going\nthrough all documents manually. To do so, we will need to reduce our document\nembeddings to two dimensions, as that allows us to plot the documents on an x/y\nplane:\n144 | Chapter 5: Text Clustering and Topic Modeling\nimport pandas as pd\n# Reduce 384-dimensional embeddings to two dimensions for easier visualization\nreduced_embeddings  = UMAP(\n    n_components =2, min_dist =0.0, metric=""cosine"" , random_state =42\n).fit_transform (embeddings )\n# Create dataframe\ndf = pd.DataFrame (reduced_embeddings , columns=[""x"", ""y""])\ndf[""title""] = titles\ndf[""cluster"" ] = [str(c) for c in clusters ]\n# Select outliers and non-outliers (clusters)\nto_plot = df.loc[df.cluster != ""-1"", :]\noutliers  = df.loc[df.cluster == ""-1"", :]\nWe also created a dataframe for our clusters ( clusters_df ) and for the outliers ( out\nliers_df ) separately since we generally want to focus on the clusters and highlight\nthose.\nUsing any dimensionality reduction technique for visualization\npurposes creates information loss. It is merely an approximation\nof what our original embeddings look like. Although it is informa‐\ntive, it might push clusters together and drive them further apart\nthan they actually are. Human evaluation, inspecting the clusters\nourselves, is therefore a key component of cluster analysis!\nTo generate a static plot, we will use the well-known plotting library, matplotlib :\nimport matplotlib.pyplot  as plt\n# Plot outliers and non-outliers separately\nplt.scatter(outliers_df .x, outliers_df .y, alpha=0.05, s=2, c=""grey"")\nplt.scatter(\n    clusters_df .x, clusters_df .y, c=clusters_df .cluster.astype(int),\n    alpha=0.6, s=2, cmap=""tab20b""\n)\nplt.axis(""off"")\nAs we can see in Figure 5-8 , it tends to capture major clusters quite well. Note how\nclusters of points are colored in the same color, indicating that HDBSCAN put them\nin a group together. Since we have a large number of clusters, the plotting library\ncycles the colors between clusters, so don’t think that all green points are one cluster,\nfor example.\nA Common Pipeline for Text Clustering | 145",4164
60-From Text Clustering to Topic Modeling.pdf,60-From Text Clustering to Topic Modeling,"Figure 5-8. The generated clusters (colored) and outliers (gray) are represented as a 2D\nvisualization.\nThis is visually appealing but does not yet allow us to see what is happening inside\nthe clusters. Instead, we can extend this visualization by going from text clustering to\ntopic modeling.\nFrom Text Clustering to Topic Modeling\nText clustering is a powerful tool for finding structure among large collections of\ndocuments. In our previous example, we could manually inspect each cluster and\nidentify them based on their collection of documents. For instance, we explored a\ncluster that contained documents about sign language. We could say that the topic  of\nthat cluster is “sign language. ”\nThis idea of finding themes or latent topics in a collection of textual data is often\nreferred to as topic modeling . Traditionally, it involves finding a set of keywords or\nphrases that best represent and capture the meaning of the topic, as we illustrate in\nFigure 5-9 .\n146 | Chapter 5: Text Clustering and Topic Modeling\n5David M. Blei, Andrew Y . Ng, and Michael I. Jordan. “Latent Dirichlet allocation. ” Journal of Machine Learning\nResearch  3. Jan (2003): 993–1022.\nFigure 5-9. Traditionally, topics are represented by a number of keywords but can take\nother forms.\nInstead of labeling a topic as “sign language, ” these techniques use keywords such as\n“sign, ” “language, ” and “translation” to describe the topic. As such, this does not give a\nsingle label to a topic and instead requires the user to understand the meaning of the\ntopic through those keywords.\nClassic approaches, like latent Dirichlet allocation, assume that each topic is charac‐\nterized by a probability distribution of words in a corpus’s vocabulary.5 Figure 5-10\ndemonstrates how each word in a vocabulary is scored against its relevance to each\ntopic.\nFigure 5-10. Keywords are extracted based on their distribution over a single topic.\nFrom Text Clustering to Topic Modeling | 147",1991
61-BERTopic A Modular Topic Modeling Framework.pdf,61-BERTopic A Modular Topic Modeling Framework,"6Maarten Grootendorst. “BERTopic: Neural topic modeling with a class-based TF-IDF procedure. ” arXiv\npreprint arXiv:2203.05794  (2022).These approaches generally use a bag-of-words technique for the main features of the\ntextual data, which does not take the context nor the meaning of words and phrases\ninto account. In contrast, our text clustering example does take both into account as\nit relies on Transformer-based embeddings that are optimized for semantic similarity\nand contextual meaning through attention.\nIn this section, we will extend text clustering into the realm of topic modeling\nthrough a highly modular text clustering and topic modeling framework, namely\nBERTopic.\nBERTopic: A Modular Topic Modeling Framework\nBERTopic is a topic modeling technique that leverages clusters of semantically similar\ntexts to extract various types of topic representations.6 The underlying algorithm can\nbe thought of in two steps.\nFirst, as illustrated in Figure 5-11 , we follow the same procedure as we did before\nin our text clustering example. We embed documents, reduce their dimensionality,\nand finally cluster the reduced embedding to create groups of semantically similar\ndocuments.\nFigure 5-11. The first part of BERTopic’s pipeline is to create clusters of semantically\nsimilar documents.\nSecond, it models a distribution over words in the corpus’s vocabulary by leveraging\na classic method, namely bag-of-words. The bag-of-words, as we discussed briefly in\nChapter 1  and illustrate in Figure 5-12 , does exactly what its name implies, counting\n148 | Chapter 5: Text Clustering and Topic Modeling\nthe number of times each word appears in a document. The resulting representation\ncould be used to extract the most frequent words inside a document.\nFigure 5-12. A bag-of-words counts the number of times each word appears inside a\ndocument.\nThere are two caveats, however. First, this is a representation on a document level\nand we are interested in a cluster-level perspective. To address this, the frequency\nof words is calculated within the entire cluster instead of only the document, as\nillustrated in Figure 5-13 .\nFigure 5-13. Generating c-TF by counting the frequency of words per cluster instead of\nper document.\nSecond, stop words like “the” and “I” tend to appear often in documents and provide\nlittle meaning to the actual documents. BERTopic uses a class-based variant of term\nfrequency–inverse document frequency (c-TF-IDF) to put more weight on words that\nare more meaningful to a cluster and put less weight on words that are used across all\nclusters.\nEach word in the bag-of-words, the c-TF in c-TF-IDF, is multiplied by the IDF value\nof each word. As shown in Figure 5-14 , the IDF value is calculated by taking the\nFrom Text Clustering to Topic Modeling | 149\nlogarithm of the average frequency of all words across all clusters divided by the total\nfrequency of each word.\nFigure 5-14. Creating a weighting scheme.\nThe result is a weight (“IDF”) for each word that we can multiply with their fre‐\nquency (“c-TF”) to get the weighted values (“c-TF-IDF”).\nThis second part of the procedure, as shown in Figure 5-15 , allows for generating\na distribution over words as we have seen before. We can use scikit-learn’s CountVec\ntorizer  to generate the bag-of-words (or term frequency) representation. Here, each\ncluster is considered a topic that has a specific ranking of the corpus’s vocabulary.\nFigure 5-15. The second part of BERTopic’s pipeline is representing the topics: the calcu‐\nlation of the weight of term *x* in a class *c*.\nPutting the two steps together, clustering and representing topics, results in the full\npipeline of BERTopic, as illustrated in Figure 5-16 . With this pipeline, we can cluster\nsemantically similar documents and from the clusters generate topics represented by\nseveral keywords. The higher a word’s weight in a topic, the more representative it is\nof that topic.\n150 | Chapter 5: Text Clustering and Topic Modeling\nFigure 5-16. The full pipeline of BERTopic, roughly, consists of two steps, clustering and\ntopic representation.\nA major advantage of this pipeline is that the two steps, clustering and topic repre‐\nsentation, are largely independent of one another. For instance, with c-TF-IDF, we\nare not dependent on the models used in clustering the documents. This allows\nfor significant modularity throughout every component of the pipeline. And as we\nwill explore later in this chapter, it is a great starting point to fine-tune the topic\nrepresentations.\nAs illustrated in Figure 5-17 , although sentence-transformers  is used as the default\nembedding model, we can swap it with any other embedding technique. The same\napplies to all other steps. If you do not want outliers generated with HDBSCAN, you\ncan use k-means instead.\nFigure 5-17. The modularity of BERTopic is a key component and allows you to build\nyour own topic model however you want.\nY ou can think of this modularity as building with Lego blocks; each part of the\npipeline is completely replaceable with another, similar algorithm. Through this\nmodularity, newly released models can be integrated within its architecture. As the\nfield of Language AI grows, so does BERTopic!\nFrom Text Clustering to Topic Modeling | 151\nThe Modularity of BERTopic\nThe modularity of BERTopic has another advantage: it allows it to be used and\nadapted to different use cases using the same base model. For instance, BERTopic\nsupports a wide variety of algorithmic variants:\n•Guided topic modeling•\n•(Semi-)supervised topic modeling•\n•Hierarchical topic modeling•\n•Dynamic topic modeling•\n•Multimodal topic modeling•\n•Multi-aspect topic modeling•\n•Online and incremental topic modeling•\n•Zero-shot topic modeling•\n•Etc.•\nThe modularity and algorithmic flexibility are the foundation of the author’s aim to\nmake BERTopic the one-stop-shop for topic modeling. Y ou can find a full overview of\nits capabilities in the documentation  or the repository .\nTo run BERTopic with our ArXiv dataset, we can use our previously defined models\nand embeddings (although it is not mandatory):\nfrom bertopic  import BERTopic\n# Train our model with our previously defined models\ntopic_model  = BERTopic (\n    embedding_model =embedding_model ,\n    umap_model =umap_model ,\n    hdbscan_model =hdbscan_model ,\n    verbose=True\n).fit(abstracts , embeddings )\nLet us start by exploring the topics that were created. The get_topic_info()  method\nis useful to get a quick description of the topics that we found:\ntopic_model .get_topic_info ()\n152 | Chapter 5: Text Clustering and Topic Modeling\nTopic Count Name Representation\n-1 14520 -1_the_of_and_to [the, of, and, to, in, we, that, language, for...\n0 2290 0_speech_asr_recognition_end [speech, asr, recognition, end, acoustic, spea...\n1 1403 1_medical_clinical_biomedical_patient [medical, clinical, biomedical, patient, healt...\n2 1156 2_sentiment_aspect_analysis_reviews [sentiment, aspect, analysis, reviews, opinion...\n3 986 3_translation_nmt_machine_neural [translation, nmt, machine, neural, bleu, engl...\n... ... ... ...\n150 54 150_coherence_discourse_paragraph_text [coherence, discourse, paragraph, text, cohesi...\n151 54 151_prompt_prompts_optimization_prompting [prompt, prompts, optimization, prompting, llm...\n152 53 152_sentence_sts_embeddings_similarity [sentence, sts, embeddings, similarity, embedd...\n153 53 153_counseling_mental_health_therapy [counseling, mental, health, therapy, psychoth...\n154 50 154_backdoor_attacks_attack_triggers [backdoor, attacks, attack, triggers, poisoned...\nEach of these topics is represented by several keywords, which are concatenated with\na “_” in the Name  column. This Name  column allows us to quickly get a feeling of what\nthe topic is about as it shows the four keywords that best represent it.\nY ou might also have noticed that the very first topic is labeled -1.\nThat topic contains all documents that could not be fitted within a\ntopic and are considered outliers. This is a result of the clustering\nalgorithm, HDBSCAN, which does not force all points to be clus‐\ntered. To remove outliers, we could either use a non-outlier algo‐\nrithm like k-means or use BERTopic’s reduce_outliers()  function\nto reassign the outliers to topics.\nWe can inspect individual topics and explore which keywords best represent them\nwith the get_topic  function. For example, topic 0 contains the following keywords:\ntopic_model .get_topic (0)\n[('speech', 0.028177697715245358),\n ('asr', 0.018971184497453525),\n ('recognition', 0.013457745472471012),\n ('end', 0.00980445092749381),\n ('acoustic', 0.009452082794507863),\n ('speaker', 0.0068822647060204885),\n ('audio', 0.006807649923681604),\n ('the', 0.0063343444687017645),\n ('error', 0.006320144717019838),\n ('automatic', 0.006290216996043161)]\nFrom Text Clustering to Topic Modeling | 153\nFor example, topic 0 contains the keywords “speech, ” “asr, ” and “recognition. ” Based\non these keywords, it seems that the topic is about automatic speech recognition\n(ASR).\nWe can use the find_topics()  function  to search for specific topics based on a\nsearch term. Let’s search for a topic about topic modeling:\ntopic_model .find_topics (""topic modeling"" )\n([22, -1, 1, 47, 32],\n [0.95456535, 0.91173744, 0.9074769, 0.9067007, 0.90510106])\nThis returns that topic 22 has a relatively high similarity (0.95) with our search term.\nIf we then inspect the topic, we can see that it is indeed a topic about topic modeling:\ntopic_model .get_topic (22)\n[('topic', 0.06634619076655907),\n ('topics', 0.035308535091932707),\n ('lda', 0.016386314730705634),\n ('latent', 0.013372311924864435),\n ('document', 0.012973600191120576),\n ('documents', 0.012383715497143821),\n ('modeling', 0.011978375291037142),\n ('dirichlet', 0.010078277589545706),\n ('word', 0.008505619415413312),\n ('allocation', 0.007930890698168108)]\nAlthough we know that this topic is about topic modeling, let’s see if the BERTopic\nabstract is also assigned to this topic:\ntopic_model .topics_[titles.index(""BERTopic: Neural topic modeling with a class-\nbased TF-IDF procedure"" )]\n22\nIt is! These functionalities allow us to quickly find the topics that we are interested in.\nThe modularity of BERTopic gives you a lot of choices, which\ncan be overwhelming. For that purpose, the author created a best\npractices guide  that goes through common practices to speed up\ntraining, improve representations, and more.\nTo make exploration of the topics a bit easier, we can look back at our text clustering\nexample. There, we created a static visualization to see the general structure of the\ncreated topic. With BERTopic, we can create an interactive variant that allows us to\nquickly explore which topics exist and which documents they contain.\nDoing so requires us to use the two-dimensional embeddings, reduced_embeddings ,\nthat we created with UMAP . Moreover, when we hover over documents, we will show\n154 | Chapter 5: Text Clustering and Topic Modeling\nthe title instead of the abstract to quickly get an understanding of the documents in a\ntopic:\n# Visualize topics and documents\nfig = topic_model .visualize_documents (\n    titles, \n    reduced_embeddings =reduced_embeddings , \n    width=1200, \n    hide_annotations =True\n)\n# Update fonts of legend for easier visualization\nfig.update_layout (font=dict(size=16))\nAs we can see in Figure 5-18 , this interactive plot quickly gives us a sense of the\ncreated topics. Y ou can zoom in to view individual documents or double-click a topic\non the righthand side to only view it.\nFigure 5-18. The output when we visualize documents and topics.\nThere is a wide range of visualization options in BERTopic. There are three that are\nworthwhile to explore to get an idea of the relationships between topics:\n# Visualize barchart with ranked keywords\ntopic_model .visualize_barchart ()\n# Visualize relationships between topics\nFrom Text Clustering to Topic Modeling | 155",12099
62-Adding a Special Lego Block.pdf,62-Adding a Special Lego Block,"topic_model .visualize_heatmap (n_clusters =30)\n# Visualize the potential hierarchical structure of topics\ntopic_model .visualize_hierarchy ()\nAdding a Special Lego Block\nThe pipeline in BERTopic that we have explored thus far, albeit fast and modular, has\na disadvantage: it still represents a topic through a bag-of-words without taking into\naccount semantic structures.\nThe solution is to leverage the strength of the bag-of-words representation, which is\nits speed to generate a meaningful representation. We can use this first meaningful\nrepresentation and tweak it using more powerful but slower techniques, like embed‐\nding models. As shown in Figure 5-19 , we can rerank the initial distribution of words\nto improve the resulting representation. Note that this idea of reranking an initial set\nof results is a main staple in neural search, a subject that we cover in Chapter 8 .\nFigure 5-19. Fine-tune the topic representations by reranking the original c-TF-IDF\ndistributions.\nAs a result, we can design a new Lego block, as shown in Figure 5-20 , that takes in\nthis first topic representation and spits out an improved representation.\nIn BERTopic, such reranker models are referred to as representation models . A major\nbenefit of this approach is that the optimization of topic representations only needs\nto be done as many times as we have topics. For instance, if we have millions of\ndocuments and a hundred topics, the representation block only needs to be applied\nonce for every topic instead of for every document.\nAs shown in Figure 5-21 , a wide variety of representation blocks have been designed\nfor BERTopic that allows you to fine-tune the representations. The representation\nblock can even be stacked multiple times to fine-tune representations using different\nmethodologies.\n156 | Chapter 5: Text Clustering and Topic Modeling\nFigure 5-20. The reranker (representation) block sits on top of the c-TF-IDF\nrepresentation.\nFigure 5-21. After  applying the c-TF-IDF weighting, topics can be fine-tuned  with a wide\nvariety of representation models, many of which are large language models.\nBefore we explore how we can use these representation blocks, we first need to do\ntwo things. First, we are going to save our original topic representations so that it will\nbe much easier to compare with and without representation models:\n# Save original representations\nfrom copy import deepcopy\noriginal_topics  = deepcopy (topic_model .topic_representations_ )\nFrom Text Clustering to Topic Modeling | 157\n7Maarten Grootendorst. “KeyBERT: Minimal keyword extraction with BERT. ” (2020).Second, let’s create a short wrapper that we can use to quickly visualize the differences\nin topic words to compare with and without representation models:\ndef topic_differences (model, original_topics , nr_topics =5):\n    """"""Show the differences in topic representations between two models """"""\n    df = pd.DataFrame (columns=[""Topic"", ""Original"" , ""Updated"" ])\n    for topic in range(nr_topics ):\n        # Extract top 5 words per topic per model\n        og_words  = "" | "".join(list(zip(*original_topics [topic]))[0][:5])\n        new_words  = "" | "".join(list(zip(*model.get_topic (topic)))[0][:5])\n        df.loc[len(df)] = [topic, og_words , new_words ]\n    \n    return df\nKeyBERTInspired\nThe first representation block that we are going to explore is KeyBERTInspired.\nKeyBERTInspired is, as you might have guessed, a method inspired by the keyword\nextraction package, KeyBERT .7 KeyBERT extracts keywords from texts by comparing\nword and document embeddings through cosine similarity.\nBERTopic uses a similar approach. KeyBERTInspired uses c-TF-IDF to extract the\nmost representative documents per topic by calculating the similarity between a\ndocument’s c-TF-IDF values and those of the topic they correspond to. As shown in\nFigure 5-22 , the average document embedding per topic is calculated and compared\nto the embeddings of candidate keywords to rerank the keywords.\nFigure 5-22. KeyBERTInspired representation model procedure.\nDue to the modular nature of BERTopic, we can update our initial topic representa‐\ntions with KeyBERTInspired without needing to perform the dimensionality reduc‐\ntion and clustering steps:\n158 | Chapter 5: Text Clustering and Topic Modeling\nfrom bertopic.representation  import KeyBERTInspired\n# Update our topic representations using KeyBERTInspired\nrepresentation_model  = KeyBERTInspired ()\ntopic_model .update_topics (abstracts , representation_model =representation_model )\n# Show topic differences\ntopic_differences (topic_model , original_topics )\nTopic Original Updated\n0 speech | asr | recognition | end | acoustic speech | encoder | phonetic | language | trans...\n1 medical | clinical | biomedical | patient | he... nlp | ehr | clinical | biomedical | language\n2 sentiment | aspect | analysis | reviews | opinion aspect | sentiment | aspects | sentiments | cl...\n3 translation | nmt | machine | neural | bleu translation | translating | translate | transl...\n4 summarization | summaries | summary | abstract... summarization | summarizers | summaries | summ...\nThe updated model shows that the topics are easier to read compared to the original\nmodel. It also demonstrates the downside of using embedding-based techniques.\nWords in the original model, like nmt (topic 3), which stands for neural machine\ntranslation, are removed as the model could not properly represent the entity. For\ndomain experts, these abbreviations are highly informative.\nMaximal marginal relevance\nWith c-TF-IDF and the previously shown KeyBERTInspired techniques, we still have\nsignificant redundancy in the resulting topic representations. For instance, having\nboth the words “summaries” and “summary” in a topic representation introduces\nredundancy as they are quite similar.\nWe can use maximal marginal relevance (MMR) to diversify our topic representa‐\ntions. The algorithm attempts to find a set of keywords that are diverse from one\nanother but still relate to the documents they are compared to. It does so by embed‐\nding a set of candidate keywords and iteratively calculating the next best keyword\nto add. Doing so requires setting a diversity parameter, which indicates how diverse\nkeywords need to be.\nIn BERTopic, we use MMR to go from a set of initial keywords, let’s say 30, to a\nsmaller but more diverse set of keywords, let’s say 10. It filters out redundant words\nand only keeps words that contribute something new to the topic representation.\nFrom Text Clustering to Topic Modeling | 159",6624
63-The Text Generation Lego Block.pdf,63-The Text Generation Lego Block,"Doing so is rather straightforward:\nfrom bertopic.representation  import MaximalMarginalRelevance\n# Update our topic representations to MaximalMarginalRelevance\nrepresentation_model  = MaximalMarginalRelevance (diversity =0.2)\ntopic_model .update_topics (abstracts , representation_model =representation_model )\n# Show topic differences\ntopic_differences (topic_model , original_topics )\nTopic Original Updated\n0 speech | asr | recognition | end | acoustic speech | asr | error | model | training\n1 medical | clinical | biomedical | patient | he... clinical | biomedical | patient | healthcare |...\n2 sentiment | aspect | analysis | reviews | opinion sentiment | analysis | reviews | absa | polarity\n3 translation | nmt | machine | neural | bleu translation | nmt | bleu | parallel | multilin...\n4 summarization | summaries | summary | abstract... summarization | document | extractive | rouge ...\nThe resulting topics demonstrate more diversity in their representations. For\ninstance, topic 4 only shows one “summary”-like word and instead adds other words\nthat might contribute more to the overall representation.\nBoth KeyBERTInspired and MMR are amazing techniques for\nimproving the first set of topic representations. KeyBERTInspired\nespecially tends to remove nearly all stop words since it focuses on\nthe semantic relationships between words and documents.\nThe Text Generation Lego Block\nThe representation block in BERTopic has been acting as a reranking block in our\nprevious examples. However, as we already explored in the previous chapter, genera‐\ntive models have great potential for a wide variety of tasks.\nWe can use generative models in BERTopic quite efficiently by following a part of\nthe reranking procedure. Instead of using a generative model to identify the topic of\nall documents, of which there can potentially be millions, we will use the model to\ngenerate a label for our topic. As illustrated in Figure 5-23 , instead of generating or\nreranking keywords, we ask the model to generate a short label based on keywords\nthat were previously generated and a small set of representative documents.\n160 | Chapter 5: Text Clustering and Topic Modeling\nFigure 5-23. Use text generative LLMs and prompt engineering to create labels for topics\nfrom keywords and documents related to each topic.\nThere are two components to the illustrated prompt. First, the documents that are\ninserted using the [DOCUMENTS]  tag are a small subset of documents, typically four,\nthat best represent the topic. The documents with the highest cosine similarity of\ntheir c-TF-IDF values with those of the topic are selected. Second, the keywords that\nmake up a topic are also passed to the prompt and referenced using the [KEYWORDS]\ntag. The keywords could be generated by c-TF-IDF or any of the other representa‐\ntions we discussed thus far.\nAs a result, we only need to use the generative model once for every topic, of which\nthere could be potentially hundreds, instead of once for each document, of which\nthere could potentially be millions. There are many generative models that we can\nchoose from, both open source and proprietary. Let’s start with a model that we have\nexplored in the previous chapter, the Flan-T5 model.\nWe create a prompt that works well with the model and use it in BERTopic through\nthe representation_model  parameter:\nfrom transformers  import pipeline\nfrom bertopic.representation  import TextGeneration\nprompt = """"""I have a topic that contains the following documents: \n[DOCUMENTS]\nThe topic is described by the following keywords: '[KEYWORDS]'.\nBased on the documents and keywords, what is this topic about?""""""\n# Update our topic representations using Flan-T5\ngenerator  = pipeline (""text2text-generation"" , model=""google/flan-t5-small"" )\nrepresentation_model  = TextGeneration (\nFrom Text Clustering to Topic Modeling | 161\n    generator , prompt=prompt, doc_length =50, tokenizer =""whitespace""\n)\ntopic_model .update_topics (abstracts , representation_model =representation_model )\n# Show topic differences\ntopic_differences (topic_model , original_topics )\nTopic Original Updated\n0 speech | asr | recognition | end | acoustic Speech-to-description\n1 medical | clinical | biomedical | patient | he... Science/Tech\n2 sentiment | aspect | analysis | reviews | opinion Review\n3 translation | nmt | machine | neural | bleu Attention-based neural machine translation\n4 summarization | summaries | summary | abstract... Summarization\nSome of these labels, like “Summarization” seem to be logical when comparing\nthem to the original representations. Others, however, like “Science/Tech, ” seem quite\nbroad and do not do the original topic justice. Let’s explore instead how OpenAI’s\nGPT-3.5 would perform considering the model is not only larger but expected to\nhave more linguistic capabilities:\nimport openai\nfrom bertopic.representation  import OpenAI\nprompt = """"""\nI have a topic that contains the following documents:\n[DOCUMENTS]\nThe topic is described by the following keywords: [KEYWORDS]\nBased on the information above, extract a short topic label in the following \nformat:\ntopic: <short topic label>\n""""""\n# Update our topic representations using GPT-3.5\nclient = openai.OpenAI(api_key=""YOUR_KEY_HERE"" )\nrepresentation_model  = OpenAI(\n    client, model=""gpt-3.5-turbo"" , exponential_backoff =True, chat=True, \nprompt=prompt\n)\ntopic_model .update_topics (abstracts , representation_model =representation_model )\n# Show topic differences\ntopic_differences (topic_model , original_topics )\n162 | Chapter 5: Text Clustering and Topic Modeling\nTopic Original Updated\n0 speech | asr | recognition | end | acoustic Leveraging External Data for Improving Low-Res...\n1 medical | clinical | biomedical | patient | he... Improved Representation Learning for Biomedica...\n2 sentiment | aspect | analysis | reviews | opinion Advancements in Aspect-Based Sentiment Analys...\n3 translation | nmt | machine | neural | bleu Neural Machine Translation Enhancements\n4 summarization | summaries | summary | abstract... Document Summarization Techniques\nThe resulting labels are quite impressive! We are not even using GPT-4 and the\nresulting labels seem to be more informative than our previous example. Note that\nBERTopic is not confined to only using OpenAI’s offering but has local backends as\nwell.\nAlthough it seems like we do not need the keywords anymore, they\nare still representative of the input documents. No model is perfect\nand it is generally advised to generate multiple topic representa‐\ntions. BERTopic allows for all topics to be represented by different\nrepresentations . Y ou could, for example, use KeyBERTInspired,\nMMR, and GPT-3.5 side by side to get different perspectives on the\nsame topic.\nWith these GPT-3.5 generated labels, we can create beautiful illustrations using  the\ndatamapplot  package  (Figure 5-24 ):\n# Visualize topics and documents\nfig = topic_model .visualize_document_datamap (\n    titles,\n    topics=list(range(20)),\n    reduced_embeddings =reduced_embeddings ,\n    width=1200,\n    label_font_size =11,\n    label_wrap_width =20,\n    use_medoids =True,\n)\nFrom Text Clustering to Topic Modeling | 163",7273
64-Summary.pdf,64-Summary,"Figure 5-24. The top 20 topics visualized.\nSummary\nIn this chapter, we explored how LLMs, both generative and representative, can\nbe used in the domain of unsupervised learning. Despite supervised methods like\nclassification being prevalent in recent years, unsupervised approaches such as text\nclustering hold immense potential due to their ability to group texts based on seman‐\ntic content without prior labeling.\nWe covered a common pipeline for clustering textual documents that starts with\nconverting input text into numerical representations, which we call embeddings.\nThen, dimensionality reduction is applied to these embeddings to simplify high-\ndimensional data for better clustering outcomes. Finally, a clustering algorithm on\n164 | Chapter 5: Text Clustering and Topic Modeling\nthe dimensionality-reduced embeddings is applied to cluster the input text. Manually\ninspecting the clusters helped us understand which documents they contained and\nhow to interpret these clusters.\nTo transition away from this manual inspection, we explored how BERTopic extends\nthis text clustering pipeline with a method for automatically representing the clusters.\nThis methodology is often referred to as topic modeling, which attempts to uncover\nthemes within large amounts of documents. BERTopic generates these topic repre‐\nsentations through a bag-of-words approach enhanced with c-TF-IDF, which weighs\nwords based on their cluster relevance and frequency across all clusters.\nA major benefit of BERTopic is its modular nature. In BERTopic, you can choose\nany model in the pipeline, which allows for additional representations of topics that\ncreate multiple perspectives of the same topic. We explored maximal marginal rele‐\nvance and KeyBERTInspired as methodologies to fine-tune the topic representations\ngenerated with c-TF-IDF. Additionally, we used the same generative LLMs as in the\nprevious chapter (Flan-T5 and GPT-3.5) to further improve the interpretability of\ntopics by generating highly interpretable labels.\nIn the next chapter, we shift focus and explore a common method for improving the\noutput of generative models, namely prompt engineering.\nSummary | 165",2202
65-Chapter 6. Prompt Engineering.pdf,65-Chapter 6. Prompt Engineering,,0
66-Using Text Generation Models.pdf,66-Using Text Generation Models,,0
67-Loading a Text Generation Model.pdf,67-Loading a Text Generation Model,"CHAPTER 6\nPrompt Engineering\nIn the first chapters of this book, we took our first steps into the world of large\nlanguage models (LLMs). We delved into various applications, such as supervised and\nunsupervised classification, employing models that focus on representing text, like\nBERT and its derivatives.\nAs we progressed, we used models trained primarily for text generation, models that\nare often referred to as generative pre-trained transformers  (GPT). These models have\nthe remarkable ability to generate text in response to prompts  from the user. Through\nprompt engineering , we can design these prompts in a way that enhances the quality of\nthe generated text.\nIn this chapter, we will explore these generative models in more detail and dive into\nthe realm of prompt engineering, reasoning with generative models, verification, and\neven evaluating their output.\nUsing Text Generation Models\nBefore we start with the fundamentals of prompt engineering, it is essential to explore\nthe basics of utilizing a text generation model. How do we select the model to use?\nDo we use a proprietary or open source model? How can we control the generated\noutput? These questions will serve as our stepping stones into using text generation\nmodels.\nChoosing a Text Generation Model\nChoosing  a text generation model starts with choosing between proprietary models\nor open source models. Although proprietary models are generally more performant,\nwe focus in this book more on open source models as they offer more flexibility and\nare free to use.\n167\nFigure 6-1  shows a small selection of impactful foundation models, LLMs that have\nbeen pretrained on vast amounts of text data and are often fine-tuned for specific\napplications.\nFigure 6-1. Foundation models are often  released in several different  sizes.\nFrom those foundation models, hundreds if not thousands of models have been\nfine-tuned, one more suitable for certain tasks than another. Choosing the model to\nuse can be a daunting task!\nWe advise starting with a small foundation model. So let’s continue using Phi-3-mini ,\nwhich has 3.8 billion parameters. This makes it suitable for running with devices\nup to 8 GB of VRAM. Overall, scaling up to larger models tends to be a nicer\nexperience than scaling down. Smaller models provide a great introduction and lay a\nsolid foundation for progressing to larger models.\nLoading a Text Generation Model\nThe most straightforward method of loading a model, as we have done in previous\nchapters, is by leveraging the transformers  library:\nimport torch\nfrom transformers  import AutoModelForCausalLM , AutoTokenizer , pipeline\n# Load model and tokenizer\nmodel = AutoModelForCausalLM .from_pretrained (\n    ""microsoft/Phi-3-mini-4k-instruct"" ,\n    device_map =""cuda"",\n    torch_dtype =""auto"",\n    trust_remote_code =True,\n)\ntokenizer  = AutoTokenizer .from_pretrained (""microsoft/Phi-3-mini-4k-instruct"" )\n# Create a pipeline\npipe = pipeline (\n    ""text-generation"" ,\n    model=model,\n168 | Chapter 6: Prompt Engineering\n    tokenizer =tokenizer ,\n    return_full_text =False,\n    max_new_tokens =500,\n    do_sample =False,\n)\nCompared to previous chapters, we will take a closer look at developing and using the\nprompt template.\nTo illustrate, let’s revisit the example from Chapter 1  where we asked the LLM to\nmake a joke about chickens:\n# Prompt\nmessages  = [\n    {""role"": ""user"", ""content"" : ""Create a funny joke about chickens."" }\n]\n# Generate the output\noutput = pipe(messages )\nprint(output[0][""generated_text"" ])\nWhy don't chickens like to go to the gym? Because they can't crack the egg-\nsistence of it!\nUnder the hood, transformers.pipeline  first converts our messages into a specific\nprompt template. We can explore this process by accessing the underlying tokenizer:\n# Apply prompt template\nprompt = pipe.tokenizer .apply_chat_template (messages , tokenize =False)\nprint(prompt)\n<s><|user|>\nCreate a funny joke about chickens.<|end|>\n<|assistant|>\nY ou may recognize the special tokens <|user|>  and <|assistant|>  from Chapter 2 .\nThis prompt template, further illustrated in Figure 6-2 , was used during the training\nof the model. Not only does it provide information about who said what, but it is also\nused to indicate when the model should stop generating text (see the <|end|>  token).\nThis prompt is passed directly to the LLM and processed all at once.\nIn the next chapter, we will customize parts of this template ourselves. Throughout\nthis chapter, we can use transformers.pipeline  to handle chat template processing\nfor us. Next, let us explore how we can control the output of the model.\nUsing Text Generation Models | 169",4739
68-Controlling Model Output.pdf,68-Controlling Model Output,"Figure 6-2. The template Phi-3 expects when interacting with the model.\nControlling Model Output\nOther than prompt engineering, we can control the kind of output we want by\nadjusting the model parameters. In our previous example, you might have noticed\nthat we used several parameters in the pipe  function, including temperature  and\ntop_p .\nThese parameters control the randomness of the output. A part of what makes LLMs\nexciting technology is that it can generate different responses for the exact same\nprompt. Each time an LLM needs to generate a token, it assigns a likelihood number\nto each possible token.\nAs illustrated in Figure 6-3 , in the sentence “I am driving a… ” the likelihood of that\nsentence being followed by tokens like “car” or “truck” is generally higher than a\ntoken like “elephant. ” However, there is still a possibility of “elephant” being generated\nbut it is much lower.\nFigure 6-3. The model chooses the next token to generate based on their likelihood scores.\nWhen we loaded our model, we purposefully set do_sample=False  to make sure the\noutput is somewhat consistent. This means that no sampling will be done and only\n170 | Chapter 6: Prompt Engineering\nthe most probable next token is selected. However, to use the temperature  and top_p\nparameters, we will set do_sample=True  in order to make use of them.\nTemperature\nThe temperature  controls  the randomness or creativity of the text generated. It\ndefines how likely it is to choose tokens that are less probable. The underlying idea\nis that a temperature  of 0 generates the same response every time because it always\nchooses the most likely word. As illustrated in Figure 6-4 , a higher value allows less\nprobable words to be generated.\nFigure 6-4. A higher temperature  increases the likelihood that less probable tokens are\ngenerated and vice versa.\nAs a result, a higher temperature  (e.g., 0.8) generally results in a more diverse output\nwhile a lower temperature  (e.g., 0.2) creates a more deterministic output.\nY ou can use temperature  in your pipeline as follows:\n# Using a high temperature\noutput = pipe(messages , do_sample =True, temperature =1)\nprint(output[0][""generated_text"" ])\nWhy don't chickens like to go on a rollercoaster? Because they're afraid they \nmight suddenly become chicken-soup!\nNote that every time you rerun this piece of code, the output will change! tempera\nture  introduces stochastic behavior since the model now randomly selects tokens.\ntop_p\ntop_p , also known as nucleus sampling, is a sampling technique that controls which\nsubset of tokens (the nucleus) the LLM can consider. It will consider tokens until it\nreaches their cumulative probability. If we set top_p  to 0.1, it will consider tokens\nuntil it reaches that value. If we set top_p  to 1, it will consider all tokens.\nUsing Text Generation Models | 171\nAs shown in Figure 6-5 , by lowering the value, it will consider fewer tokens and\ngenerally give less “creative” output, while increasing the value allows the LLM to\nchoose from more tokens.\nFigure 6-5. A higher top_p  increases the number of tokens that can be selected to\ngenerate and vice versa.\nSimilarly, the top_k  parameter controls exactly how many tokens the LLM can\nconsider. If you change its value to 100, the LLM will only consider the top 100 most\nprobable tokens.\nY ou can use top_p  in your pipeline as follows:\n# Using a high top_p\noutput = pipe(messages , do_sample =True, top_p=1)\nprint(output[0][""generated_text"" ])\nWhy don't chickens make good comedians? Because their 'jokes' always 'feather' \nthe truth!\nAs shown in Table 6-1 , these parameters allow the user to have a sliding scale\nbetween being creative (high temperature  and top_p ) and being predictable (lower\ntemperature  and top_p ).\nTable 6-1. Use case examples when selecting values for temperature  and top_p .\nExample use case Tempera\nturetop_p Description\nBrainstorming\nsessionHigh High High randomness with large pool of potential tokens. The results will be highly\ndiverse, often leading to very creative and unexpected results.\nEmail generation Low Low Deterministic output with high probable predicted tokens. This results in\npredictable, focused, and conservative outputs.\nCreative writing High Low High randomness with a small pool of potential tokens. This combination\nproduces creative outputs but still remains coherent.\nTranslation Low High Deterministic output with high probable predicted tokens. Produces coherent\noutput with a wider range of vocabulary, leading to outputs with linguistic\nvariety.\n172 | Chapter 6: Prompt Engineering",4652
69-Intro to Prompt Engineering.pdf,69-Intro to Prompt Engineering,,0
70-The Basic Ingredients of a Prompt.pdf,70-The Basic Ingredients of a Prompt,"Intro to Prompt Engineering\nAn essential part of working with text-generative LLMs is prompt engineering. By\ncarefully designing our prompts we can guide the LLM to generate desired responses.\nWhether the prompts are questions, statements, or instructions, the main goal of\nprompt engineering is to elicit a useful response from the model.\nPrompt engineering is more than designing effective prompts. It can be used as a\ntool to evaluate the output of a model as well as to design safeguards and safety\nmitigation methods. This is an iterative process of prompt optimization and requires\nexperimentation. There is not and unlikely will ever be a perfect prompt design.\nIn this section, we will go through common methods for prompt engineering, and\nsmall tips and tricks to understand what the effect is of certain prompts. These\nskills allow us to understand the capabilities of LLMs and lie at the foundation of\ninterfacing with these kinds of models.\nWe begin by answering the question: what should be in a prompt?\nThe Basic Ingredients of a Prompt\nAn LLM is a prediction machine. Based on a certain input, the prompt, it tries to\npredict the words that might follow it. At its core (illustrated in Figure 6-6 ), the\nprompt does not need to be more than a few words to elicit a response from the LLM.\nFigure 6-6. A basic example of a prompt. No instruction is given so the LLM will simply\ntry to complete the sentence.\nHowever, although the illustration works as a basic example, it fails to complete a\nspecific task. Instead, we generally approach prompt engineering by asking a specific\nquestion or task the LLM should complete. To elicit the desired response, we need a\nmore structured prompt.\nFor example, and as shown in Figure 6-7 , we could ask the LLM to classify a sentence\ninto either having positive or negative sentiment. This extends the most basic prompt\nIntro to Prompt Engineering | 173\nto one consisting of two components—the instruction itself and the data that relates\nto the instruction.\nFigure 6-7. Two components of a basic instruction prompt: the instruction itself and the\ndata it refers to.\nMore complex use cases might require more components in a prompt. For instance,\nto make sure the model only outputs “negative” or “positive” we can introduce output\nindicators that help guide the model. In Figure 6-8 , we prefix the sentence with\n“Text:” and add “Sentiment:” to prevent the model from generating a complete sen‐\ntence. Instead, this structure indicates that we expect either “negative” or “positive. ”\nAlthough the model might not have been trained on these components directly, it was\nfed enough instructions to be able to generalize to this structure.\nFigure 6-8. Extending the prompt with an output indicator that allows for a specific\noutput.\n174 | Chapter 6: Prompt Engineering",2857
71-Instruction-Based Prompting.pdf,71-Instruction-Based Prompting,"We can continue adding or updating the elements of a prompt until we elicit the\nresponse we are looking for. We could add additional examples, describe the use case\nin more detail, provide additional context, etc. These components are merely exam‐\nples and not a limited set of possibilities. The creativity that comes with designing\nthese components is key.\nAlthough a prompt is a single piece of text, it is tremendously helpful to think of\nprompts as pieces of a larger puzzle. Have I described the context of my question?\nDoes the prompt have an example of the output?\nInstruction-Based Prompting\nAlthough prompting comes in many flavors, from discussing philosophy with the\nLLM to role-playing with your favorite superhero, prompting is often used to have\nthe LLM answer a specific question or resolve a certain task. This is referred to as\ninstruction-based prompting .\nFigure 6-9  illustrates a number of use cases in which instruction-based prompting\nplays an important role. We already did one of these in the previous example, namely\nsupervised classification.\nFigure 6-9. Use cases for instruction-based prompting.\nEach of these tasks requires different prompting formats and more specifically, asking\ndifferent questions of the LLM. Asking the LLM to summarize a piece of text will not\nsuddenly result in classification. To illustrate, examples of prompts for some of these\nuse cases can be found in Figure 6-10 .\nIntro to Prompt Engineering | 175\nFigure 6-10. Prompt examples of common use cases. Notice how within a use case, the\nstructure and location of the instruction can be changed.\nAlthough these tasks require different instructions, there is actually a lot of overlap\nin the prompting techniques used to improve the quality of the output. A non-\nexhaustive list of these techniques includes:\nSpecificity\nAccurately  describe what you want to achieve. Instead of asking the LLM to\n“Write a description for a product” ask it to “Write a description for a product in\nless than two sentences and use a formal tone. ”\n176 | Chapter 6: Prompt Engineering",2101
72-Advanced Prompt Engineering.pdf,72-Advanced Prompt Engineering,,0
73-The Potential Complexity of a Prompt.pdf,73-The Potential Complexity of a Prompt,"1Nelson F. Liu et al. “Lost in the middle: How language models use long contexts. ” arXiv preprint\narXiv:2307.03172  (2023).Hallucination\nLLMs  may generate incorrect information confidently, which is referred to as\nhallucination. To reduce its impact, we can ask the LLM to only generate an\nanswer if it knows the answer. If it does not know the answer, it can respond with\n“I don’t know. ”\nOrder\nEither begin or end your prompt with the instruction. Especially with long\nprompts, information in the middle is often forgotten.1 LLMs tend to focus on\ninformation either at the beginning of a prompt (primacy effect) or the end of a\nprompt (recency effect).\nHere, specificity is arguably the most important aspect. By restricting and specifying\nwhat the model should generate, there is a smaller chance of having it generate\nsomething not related to your use case. For instance, if we were to skip the instruc‐\ntion “in two to three sentences” it might generate complete paragraphs. Like human\nconversations, without any specific instructions or additional context, it is difficult to\nderive what the task at hand actually is.\nAdvanced Prompt Engineering\nOn the surface, creating a good prompt might seem straightforward. Ask a specific\nquestion, be accurate, add some examples, and you are done! However, prompting\ncan grow complex quite quickly and as a result is an often-underestimated compo‐\nnent of leveraging LLMs.\nHere, we will go through several advanced techniques for building up your prompts,\nstarting with the iterative workflow of building up complex prompts all the way to\nusing LLMs sequentially to get improved results. Eventually, we will even build up to\nadvanced reasoning techniques.\nThe Potential Complexity of a Prompt\nAs we explored in the intro to prompt engineering, a prompt generally consists of\nmultiple components. In our very first example, our prompt consisted of instruction,\ndata, and output indicators. As we mentioned before, no prompt is limited to just\nthese three components and you can build it up to be as complex as you want.\nThese advanced components can quickly make a prompt quite complex. Some com‐\nmon components are:\nAdvanced Prompt Engineering | 177\nPersona\nDescribe  what role the LLM should take on. For example, use “Y ou are an expert\nin astrophysics” if you want to ask a question about astrophysics.\nInstruction\nThe task itself. Make sure this is as specific as possible. We do not want to leave\nmuch room for interpretation.\nContext\nAdditional information describing the context of the problem or task. It answers\nquestions like “What is the reason for the instruction?”\nFormat\nThe format the LLM should use to output the generated text. Without it, the\nLLM will come up with a format itself, which is troublesome in automated\nsystems.\nAudience\nThe target of the generated text. This also describes the level of the generated\noutput. For education purposes, it is often helpful to use ELI5 (“Explain it like\nI’m 5”).\nTone\nThe tone of voice the LLM should use in the generated text. If you are writing a\nformal email to your boss, you might not want to use an informal tone of voice.\nData\nThe main data related to the task itself.\nTo illustrate, let us extend the classification prompt we had earlier and use all of the\npreceding components. This is demonstrated in Figure 6-11 .\nThis complex prompt demonstrates the modular nature of prompting. We can add\nand remove components freely and judge their effect on the output. As illustrated in\nFigure 6-12 , we can slowly build up our prompt and explore the effect of each change.\nThe changes are not limited to simply introducing or removing components. Their\norder, as we saw before with the recency and primacy effects, can affect the quality\nof the LLM’s output. In other words, experimentation is vital when finding the\nbest prompt for your use case. With prompting, we essentially have ourselves in an\niterative cycle of experimentation.\n178 | Chapter 6: Prompt Engineering\nFigure 6-11. An example of a complex prompt with many components.\nFigure 6-12. Iterating over modular components is a vital part of prompt engineering.\nTry it out yourself! Use the complex prompt to add and/or remove parts to observe\nits impact on the generated output. Y ou will quickly notice when pieces of the puzzle\nare worth keeping. Y ou can use your own data by adding it to the data  variable:\nAdvanced Prompt Engineering | 179",4490
74-In-Context Learning Providing Examples.pdf,74-In-Context Learning Providing Examples,"2Cheng Li et al. “EmotionPrompt: Leveraging psychology for large language models enhancement via emo‐\ntional stimulus. ” arXiv preprint arXiv:2307.11760  (2023).\n3Tom Brown et al. “Language models are few-shot learners. ” Advances in Neural Information Processing Systems\n33 (2020): 1877–1901.\n# Prompt components\npersona = ""You are an expert in Large Language models. You excel at breaking \ndown complex papers into digestible summaries. \n""\ninstruction  = ""Summarize the key findings of the paper provided. \n""\ncontext = ""Your summary should extract the most crucial points that can help \nresearchers quickly understand the most vital information of the paper. \n""\ndata_format  = ""Create a bullet-point summary that outlines the method. Follow \nthis up with a concise paragraph that encapsulates the main results. \n""\naudience  = ""The summary is designed for busy researchers that quickly need to \ngrasp the newest trends in Large Language Models. \n""\ntone = ""The tone should be professional and clear. \n""\ntext = ""MY TEXT TO SUMMARIZE""\ndata = f""Text to summarize: {text}""\n# The full prompt - remove and add pieces to view its impact on the generated \noutput\nquery = persona + instruction  + context + data_format  + audience  + tone + data\nThere is all manner of components that we could add and creative\ncomponents like using emotional stimuli (e.g., “This is very impor‐\ntant for my career. ”2). Part of the fun in prompt engineering is that\nyou can be as creative as possible to figure out which combination\nof prompt components contribute to your use case. There are few\nconstraints to developing a format that works for you.\nIn a way, it is an attempt to reverse engineer what the model has\nlearned and how it responds to certain prompts. However, note\nthat some prompts work better for certain models compared to\nothers as their training data might be different or they are trained\nfor different purposes.\nIn-Context Learning: Providing Examples\nIn the previous sections, we tried to accurately describe what the LLM should do.\nAlthough accurate and specific descriptions help the LLM to understand the use case,\nwe can go one step further. Instead of describing the task, why do we not just show\nthe task?\nWe can provide the LLM with examples of exactly the thing that we want to achieve.\nThis is often referred to as in-context learning , where we provide the model with\ncorrect examples.3\n180 | Chapter 6: Prompt Engineering\n4Ibid.As illustrated in Figure 6-13 , this comes in a number of forms depending on how\nmany examples you show the LLM. Zero-shot prompting does not leverage examples,\none-shot prompts use a single example, and few-shot prompts use two or more\nexamples.\nFigure 6-13. An example of a complex prompt with many components.\nAdopting the original phrase, we believe that “an example is worth a thousand\nwords. ” These examples provide a direct example of what and how the LLM should\nachieve.\nWe can illustrate this method with a simple example taken from the original paper\ndescribing this method.4 The goal of the prompt is to generate a sentence with\na made-up word. To improve the quality of the resulting sentence, we can show\nthe generative model an example of what a proper sentence with a made-up word\nwould be.\nTo do so, we will need to differentiate between our question (user)  and the answers\nthat were provided by the model ( assistant ). We additionally showcase how this\ninteraction is processed using the template:\n# Use a single example of using the made-up word in a sentence\none_shot_prompt  = [\n    {\n        ""role"": ""user"",\n        ""content"" : ""A 'Gigamuru' is a type of Japanese musical instrument. An \nexample of a sentence that uses the word Gigamuru is:""\nAdvanced Prompt Engineering | 181",3805
75-Chain Prompting Breaking up the Problem.pdf,75-Chain Prompting Breaking up the Problem,"},\n    {\n        ""role"": ""assistant"" ,\n        ""content"" : ""I have a Gigamuru that my uncle gave me as a gift. I love \nto play it at home.""\n    },\n    {\n        ""role"": ""user"",\n        ""content"" : ""To 'screeg' something is to swing a sword at it. An example \nof a sentence that uses the word screeg is:""\n    }\n]\nprint(tokenizer .apply_chat_template (one_shot_prompt , tokenize =False))\n<s><|user|>\nA 'Gigamuru' is a type of Japanese musical instrument. An example of a sen-\ntence that uses the word Gigamuru is:<|end|>\n<|assistant|>\nI have a Gigamuru that my uncle gave me as a gift. I love to play it at home.<|\nend|>\n<|user|>\nTo 'screeg' something is to swing a sword at it. An example of a sentence that \nuses the word screeg is:<|end|>\n<|assistant|>\nThe prompt illustrates the need to differentiate between the user and the assistant. If\nwe did not, it would seem as if we were talking to ourselves. Using these interactions,\nwe can generate output as follows:\n# Generate the output\noutputs = pipe(one_shot_prompt )\nprint(outputs[0][""generated_text"" ])\nDuring the intense duel, the knight skillfully screeged his opponent's shield, \nforcing him to defend himself.\nIt correctly generated the answer!\nAs with all prompt components, one- or few-shot prompting is not the be all and\nend all of prompt engineering. We can use it as one piece of the puzzle to further\nenhance the descriptions that we gave it. The model can still “choose, ” through\nrandom sampling, to ignore the instructions.\nChain Prompting: Breaking up the Problem\nIn previous examples, we explored splitting up prompts into modular components to\nimprove the performance of LLMs. Although this works well for many use cases, this\nmight not be feasible for highly complex prompts or use cases.\nInstead of breaking the problem within a prompt, we can do so between prompts.\nEssentially, we take the output of one prompt and use it as input for the next, thereby\ncreating a continuous chain of interactions that solves our problem.\n182 | Chapter 6: Prompt Engineering\nTo illustrate, let us say we want to use an LLM to create a product name, slogan, and\nsales pitch for us based on a number of product features. Although we can ask the\nLLM to do this in one go, we can instead break up the problem into pieces.\nAs a result, and as illustrated in Figure 6-14 , we get a sequential pipeline that first\ncreates the product name, uses that with the product features as input to create the\nslogan, and finally, uses the features, product name, and slogan to create the sales\npitch.\nFigure 6-14. Using a description of a product’s features, chain prompts to create a\nsuitable name, slogan, and sales pitch.\nThis technique of chaining prompts allows the LLM to spend more time on each\nindividual question instead of tackling the whole problem. Let us illustrate this with a\nsmall example. We first create a name and slogan for a chatbot:\n# Create name and slogan for a product\nproduct_prompt  = [\n    {""role"": ""user"", ""content"" : ""Create a name and slogan for a chatbot that \nleverages LLMs."" }\n]\noutputs = pipe(product_prompt )\nproduct_description  = outputs[0][""generated_text"" ]\nprint(product_description )\nName: 'MindMeld Messenger'\nSlogan: 'Unleashing Intelligent Conversations, One Response at a Time'\nAdvanced Prompt Engineering | 183",3365
76-Chain-of-Thought Think Before Answering.pdf,76-Chain-of-Thought Think Before Answering,"Then, we can use the generated output as input for the LLM to generate a sales pitch:\n# Based on a name and slogan for a product, generate a sales pitch\nsales_prompt  = [\n    {""role"": ""user"", ""content"" : f""Generate a very short sales pitch for the \nfollowing product: ' {product_description }'""}\n]\noutputs = pipe(sales_prompt )\nsales_pitch  = outputs[0][""generated_text"" ]\nprint(sales_pitch )\nIntroducing MindMeld Messenger - your ultimate communication partner! Unleash \nintelligent conversations with our innovative AI-powered messaging platform. \nWith MindMeld Messenger, every response is thoughtful, personalized, and \ntimely. Say goodbye to generic replies and hello to meaningful interactions. \nElevate your communication game with MindMeld Messenger - where every message \nis a step toward smarter conversations. Try it now and experience the future \nof messaging!\nAlthough we need two calls to the model, a major benefit is that we can give each call\ndifferent parameters. For instance, the number of tokens created was relatively small\nfor the name and slogan whereas the pitch can be much longer.\nThis can be used for a variety of use cases, including:\nResponse validation\nAsk the LLM to double-check previously generated outputs.\nParallel prompts\nCreate  multiple prompts in parallel and do a final pass to merge them. For\nexample, ask multiple LLMs to generate multiple recipes in parallel and use the\ncombined result to create a shopping list.\nWriting stories\nLeverage the LLM to write books or stories by breaking down the problem into\ncomponents. For example, by first writing a summary, developing characters, and\nbuilding the story beats before diving into creating the dialogue.\nIn the next chapter, we will automate this process and go beyond chaining LLMs.\nWe will chain other pieces of technology together, like memory, tool use, and more!\nBefore that, this idea of prompt chaining will be explored further in the following sec‐\ntions, which describe more complex prompt chaining methods like self-consistency,\nchain-of-thought, and tree-of-thought.\nReasoning with Generative Models\nIn the previous sections, we focused mostly on the modular component of prompts,\nbuilding them up through iteration. These advanced prompt engineering techniques,\n184 | Chapter 6: Prompt Engineering\n5Daniel Kahneman. Thinking,  Fast and Slow . Macmillan (2011).\n6Jason Wei et al. “Chain-of-thought prompting elicits reasoning in large language models. ” Advances in Neural\nInformation Processing Systems  35 (2022): 24824–24837.like prompt chaining, proved to be the first step toward enabling complex reasoning\nwith generative models.\nReasoning is a core component of human intelligence and is often compared to the\nemergent behavior of LLMs that often resembles reasoning. We highlight “resemble”\nas these models, at the time of writing, are generally considered to demonstrate this\nbehavior through memorization of training data and pattern matching.\nThe output that they showcase, however, can demonstrate complex behavior and\nalthough it might not be “true” reasoning, they are still referred to as reasoning capa‐\nbilities. In other words, we work together with the LLM through prompt engineering\nso we can mimic reasoning processes in order to improve the output of the LLM.\nTo allow for this reasoning behavior, it is a good moment to step back and explore\nwhat reasoning entails in human behavior. To simplify, our methods of reasoning can\nbe divided into system 1 and 2 thinking processes.\nSystem 1 thinking represents an automatic, intuitive, and near-instantaneous pro‐\ncess. It shares similarities with generative models that automatically generate tokens\nwithout any self-reflective behavior. In contrast, system 2 thinking is a conscious,\nslow, and logical process, akin to brainstorming and self-reflection.5\nIf we could give a generative model the ability to mimic a form of self-reflection, we\nwould essentially be emulating the system 2 way of thinking, which tends to produce\nmore thoughtful responses than system 1 thinking. In this section, we will explore\nseveral techniques that attempt to mimic these kinds of thought processes of human\nreasoners with the aim of improving the output of the model.\nChain-of-Thought: Think Before Answering\nThe first and major step toward complex reasoning in generative models was\nthrough a method called chain-of-thought. Chain-of-thought aims to have the gen‐\nerative model “think” first rather than answering the question directly without any\nreasoning.6\nAs illustrated in Figure 6-15 , it provides examples in a prompt that demonstrate\nthe reasoning the model should do before generating its response. These reasoning\nprocesses are referred to as “thoughts. ” This helps tremendously for tasks that involve\na higher degree of complexity, like mathematical questions. Adding this reasoning\nstep allows the model to distribute more compute over the reasoning process. Instead\nReasoning with Generative Models | 185\nof calculating the entire solution based on a few tokens, each additional token in this\nreasoning process allows the LLM to stabilize its output.\nFigure 6-15. Chain-of-thought prompting uses reasoning examples to persuade the gen‐\nerative model to use reasoning in its answer.\nWe use the example the authors used in their paper to demonstrate this phenomenon:\n# Answering with chain-of-thought\ncot_prompt  = [\n    {""role"": ""user"", ""content"" : ""Roger has 5 tennis balls. He buys 2 more cans \nof tennis balls. Each can has 3 tennis balls. How many tennis balls does he \nhave now?"" },\n    {""role"": ""assistant"" , ""content"" : ""Roger started with 5 balls. 2 cans of 3 \ntennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11."" },\n    {""role"": ""user"", ""content"" : ""The cafeteria had 23 apples. If they used 20 \nto make lunch and bought 6 more, how many apples do they have?"" }\n]\n# Generate the output\noutputs = pipe(cot_prompt )\nprint(outputs[0][""generated_text"" ])\nThe cafeteria started with 23 apples. They used 20 apples, so they had 23 - 20 \n= 3 apples left. Then they bought 6 more apples, so they now have 3 + 6 = 9 \napples. The answer is 9.\n186 | Chapter 6: Prompt Engineering\n7Takeshi Kojima et al. “Large language models are zero-shot reasoners. ” Advances in Neural Information\nProcessing Systems  35 (2022): 22199–22213.Note how the model doesn’t generate only the answer but provides an explanation\nbefore doing so. By doing so, it can leverage the knowledge it has generated thus far\nto compute the final answer.\nAlthough chain-of-thought is a great method for enhancing the output of a genera‐\ntive model, it does require one or more examples of reasoning in the prompt, which\nthe user might not have access to. Instead of providing examples, we can simply ask\nthe generative model to provide the reasoning (zero-shot chain-of-thought). There\nare many different forms that work but a common and effective method is to use the\nphrase “Let’s think step-by-step, ” which is illustrated in Figure 6-16 .7\nFigure 6-16. Chain-of-thought prompting without using examples. Instead, it uses the\nphrase “Let’s think step-by-step” to prime reasoning in its answer.\nUsing the example we used before, we can simply append that phrase to the prompt\nto enable chain-of-thought-like reasoning:\n# Zero-shot chain-of-thought\nzeroshot_cot_prompt  = [\n    {""role"": ""user"", ""content"" : ""The cafeteria had 23 apples. If they used 20 \nto make lunch and bought 6 more, how many apples do they have? Let's think \nstep-by-step."" }\n]\n# Generate the output\noutputs = pipe(zeroshot_cot_prompt )\nprint(outputs[0][""generated_text"" ])\nReasoning with Generative Models | 187",7757
77-Tree-of-Thought Exploring Intermediate Steps.pdf,77-Tree-of-Thought Exploring Intermediate Steps,"8Chengrun Y ang et al. “Large language models as optimizers. ” arXiv preprint arXiv:2309.03409  (2023).\n9Xuezhi Wang et al. “Self-consistency improves chain of thought reasoning in language models. ” arXiv preprint\narXiv:2203.11171  (2022).\nStep 1: Start with the initial number of apples, which is 23.\nStep 2: Subtract the number of apples used to make lunch, which is 20. So, 23 \n- 20 = 3 apples remaining.\nStep 3: Add the number of apples bought, which is 6. So, 3 + 6 = 9 apples.\nThe cafeteria now has 9 apples.\nWithout needing to provide examples, we again got the same reasoning behavior.\nThis is why it is so important to “show your work” when doing calculations. By\naddressing the reasoning process the LLM can use the previously generated informa‐\ntion as a guide through generating the final answer.\nAlthough the prompt “Let’s think step by step” can improve the\noutput, you are not constrained by this exact formulation. Alterna‐\ntives exist like “Take a deep breath and think step-by-step” and\n“Let’s work through this problem step-by-step. ”8\nSelf-Consistency: Sampling Outputs\nUsing  the same prompt multiple times can lead to different results if we allow for a\ndegree of creativity through parameters like temperature  and top_p . As a result, the\nquality of the output might improve or degrade depending on the random selection\nof tokens. In other words, luck!\nTo counteract this degree of randomness and improve the performance of generative\nmodels, self-consistency was introduced. This method asks the generative model the\nsame prompt multiple times and takes the majority result as the final answer.9 During\nthis process, each answer can be affected by different temperature  and top_p  values\nto increase the diversity of sampling.\nAs illustrated in Figure 6-17 , this method can further be improved by adding chain-\nof-thought prompting to improve its reasoning while only using the answer for the\nvoting procedure.\n188 | Chapter 6: Prompt Engineering\nFigure 6-17. By sampling from multiple reasoning paths, we can use majority voting to\nextract the most likely answer.\nHowever, this does require a single question to be asked multiple times. As a result,\nalthough the method can improve performance, it becomes n times slower where n is\nthe number of output samples.\nTree-of-Thought: Exploring Intermediate Steps\nThe ideas of chain-of-thought and self-consistency are meant to enable more com‐\nplex reasoning. By sampling from multiple “thoughts” and making them more\nthoughtful, we aim to improve the output of generative models.\nThese techniques only scratch the surface of what is currently being done to mimic\ncomplex reasoning. An improvement to these approaches can be found in tree-of-\nthought, which allows for an in-depth exploration of several ideas.\nThe method works as follows. When faced with a problem that requires multiple\nreasoning steps, it often helps to break it down into pieces. At each step, and as\nillustrated in Figure 6-18 , the generative model is prompted to explore different\nReasoning with Generative Models | 189\n10Shunyu Y ao et al. “Tree of thoughts: Deliberate problem solving with large language models. ” arXiv preprint\narXiv:2305.10601  (2023).\n11“Using tree-of-thought prompting to boost ChatGPT’s reasoning. ” Available at https://oreil.ly/a_Nos .solutions to the problem at hand. It then votes for the best solution and continues to\nthe next step.10\nFigure 6-18. By leveraging a tree-based structure, generative models can generate inter‐\nmediate thoughts to be rated. The most promising thoughts are kept and the lowest are\npruned.\nThis method is tremendously helpful when needing to consider multiple paths, like\nwhen writing a story or coming up with creative ideas. A disadvantage of this method\nis that it requires many calls to the generative models, which slows the application\nsignificantly. Fortunately, there has been a successful attempt to convert the tree-of-\nthought framework into a simple prompting technique.11\nInstead of calling the generative model multiple times, we ask the model to mimic\nthat behavior by emulating a conversation between multiple experts. These experts\nwill question each other until they reach a consensus. An example of a tree-of-\nthought prompt is:\n190 | Chapter 6: Prompt Engineering",4352
78-Providing Examples.pdf,78-Providing Examples,"# Zero-shot tree-of-thought\nzeroshot_tot_prompt  = [\n    {""role"": ""user"", ""content"" : ""Imagine three different experts are answering \nthis question. All experts will write down 1 step of their thinking, then share \nit with the group. Then all experts will go on to the next step, etc. If any \nexpert realizes they're wrong at any point then they leave. The question is \n'The cafeteria had 23 apples. If they used 20 to make lunch and bought 6 more, \nhow many apples do they have?' Make sure to discuss the results."" }\n]\nWe can use this prompt to explore how an LLM might respond to complex questions:\n# Generate the output\noutputs = pipe(zeroshot_tot_prompt )\nprint(outputs[0][""generated_text"" ])\nExpert 1: Step 1 - Start with the initial number of apples: 23 apples.\nExpert 2: Step 1 - Subtract the apples used for lunch: 23 - 20 = 3 apples \nremaining.\nExpert 3: Step 1 - Add the newly bought apples: 3 + 6 = 9 apples.\nExpert 1: Step 2 - Confirm the final count: The cafeteria has 9 apples.\nExpert 2: Step 2 - Review the calculations: 23 - 20 = 3, then 3 + 6 = 9. The \ncalculations are correct.\nExpert 3: Step 2 - Agree with the result: The cafeteria indeed has 9 apples.\nAll experts agree on the final count: The cafeteria has 9 apples.\nWe again get the correct answer but instead through a “discussion between experts. ”\nIt is interesting to see such a conservation between “experts” that demonstrates the\ncreativity that comes with prompt engineering.\nOutput Verification\nSystems  and applications built with generative models might eventually end up in\nproduction. When that happens, it is important that we verify and control the output\nof the model to prevent breaking the application and to create a robust generative AI\napplication.\nReasons for validating the output might include:\nStructured output\nBy default, most generative models create free-form text without adhering to\nspecific structures other than those defined by natural language. Some use cases\nrequire their output to be structured in certain formats, like JSON.\nOutput Verification  | 191\nValid output\nEven if we allow the model to generate structured output, it still has the capability\nto freely generate its content. For instance, when a model is asked to output\neither one of two choices, it should not come up with a third.\nEthics\nSome open source generative models have no guardrails and will generate out‐\nputs that do not consider safety or ethical considerations. For instance, use\ncases might require the output to be free of profanity, personally identifiable\ninformation (PII), bias, cultural stereotypes, etc.\nAccuracy\nMany use cases require the output to adhere to certain standards or performance.\nThe aim is to double-check whether the generated information is factually accu‐\nrate, coherent, or free from hallucination.\nControlling the output of a generative model, as we explored with parameters like\ntop_p  and temperature , is not an easy feat. These models require help to generate\nconsistent output conforming to certain guidelines.\nGenerally, there are three ways of controlling the output of a generative model:\nExamples\nProvide a number of examples of the expected output.\nGrammar\nControl the token selection process.\nFine-tuning\nTune a model on data that contains the expected output.\nIn this section, we will go through the first two methods. The third, fine-tuning a\nmodel, is left for Chapter 12  where we will go in depth into fine-tuning methods.\nProviding Examples\nA simple and straightforward method to fix the output is to provide the generative\nmodel with examples of what the output should look like. As we explored before,\nfew-shot learning is a helpful technique that guides the output of the generative\nmodel. This method can be generalized to guide the structure of the output as well.\nFor example, let us consider an example where we want the generative model to\ncreate a character profile for an RPG game. We start by using no examples:\n# Zero-shot learning: Providing no examples\nzeroshot_prompt  = [\n    {""role"": ""user"", ""content"" : ""Create a character profile for an RPG game in \nJSON format."" }\n192 | Chapter 6: Prompt Engineering\n]\n# Generate the output\noutputs = pipe(zeroshot_prompt )\nprint(outputs[0][""generated_text"" ])\n ```json\n{\n  ""characterProfile"": {\n    ""name"": ""Eldrin Stormbringer"",\n    ""class"": ""Warlock"",\n    ""race"": ""Half-Elf"",\n    ""age"": 27,\n   ""gender"": ""Male"",\n    ""alignment"": ""Chaotic Good"",\n    ""background"": ""Rogue"",\n    …\n    },\n    ""attributes"": {\n      ""strength"": 10,\n      ""dexterity"": 17,\n      ""constitution"": 12,\n      ""intelligence"": 12,\n      ""wisdom"": 10,\n      ""charisma\nThe preceding truncated output is not valid JSON since the model stopped gener‐\nating tokens after starting the “charisma” attribute. Moreover, we might not want\ncertain attributes. Instead, we can provide the model with a number of examples that\nindicate the expected format:\n# One-shot learning: Providing an example of the output structure\none_shot_template  = """"""Create a short character profile for an RPG game. Make \nsure to only use this format:\n{\n  ""description"": ""A SHORT DESCRIPTION"",\n  ""name"": ""THE CHARACTER'S NAME"",\n  ""armor"": ""ONE PIECE OF ARMOR"",\n  ""weapon"": ""ONE OR MORE WEAPONS""\n}\n""""""\none_shot_prompt  = [\n    {""role"": ""user"", ""content"" : one_shot_template }\n]\n# Generate the output\noutputs = pipe(one_shot_prompt )\nprint(outputs[0][""generated_text"" ])\nOutput Verification  | 193",5537
79-Grammar Constrained Sampling.pdf,79-Grammar Constrained Sampling,"{\n  ""description"": ""A cunning rogue with a mysterious past, skilled in stealth \nand deception."",\n  ""name"": ""Lysandra Shadowstep"",\n  ""armor"": ""Leather Cloak of the Night"",\n  ""weapon"": ""Dagger of Whispers, Throwing Knives""\n}\nThe model perfectly followed the example we gave it, which allows for more consis‐\ntent behavior. This also demonstrates the importance of leveraging few-shot learning\nto improve the structure of the output and not only its content.\nAn important note here is that it is still up to the model whether it will adhere\nto your suggested format or not. Some models are better than others at following\ninstructions.\nGrammar: Constrained Sampling\nFew-shot learning has a big disadvantage: we cannot explicitly prevent certain output\nfrom being generated. Although we guide the model and give it instructions, it might\nstill not follow it entirely.\nInstead, packages have been rapidly developed to constrain and validate the output\nof generative models, like Guidance , Guardrails , and LMQL . In part, they leverage\ngenerative models to validate their own output, as illustrated in Figure 6-19 . The\ngenerative models retrieve the output as new prompts and attempt to validate it based\non a number of predefined guardrails.\nFigure 6-19. Use an LLM to check whether the output correctly follows our rules.\nSimilarly, as illustrated in Figure 6-20 , this validation process can also be used to\ncontrol the formatting of the output by generating parts of its format ourselves as we\nalready know how it should be structured.\n194 | Chapter 6: Prompt Engineering\nFigure 6-20. Use an LLM to generate only the pieces of information we do not know\nbeforehand.\nThis process can be taken one step further and instead of validating the output we\ncan already perform validation during the token sampling process. When sampling\ntokens, we can define a number of grammars or rules that the LLM should adhere\nto when choosing its next token. For instance, if we ask the model to either return\n“positive, ” “negative, ” or “neutral” when performing sentiment classification, it might\nstill return something else. As illustrated in Figure 6-21 , by constraining the sampling\nprocess, we can have the LLM only output what we are interested in. Note that this is\nstill affected by parameters such as top_p  and temperature .\nFigure 6-21. Constrain the token selection to only three possible tokens: “positive, ”\n“neutral, ” and “negative. ”\nLet us illustrate this phenomenon with llama-cpp-python , a library similar to trans\nformers  that we can use to load in our language model. It is generally used to\nefficiently load and use compressed models (through quantization; see Chapter 12 )\nbut we can also use it to apply a JSON grammar.\nWe load the same model we used throughout this chapter but use a different format\ninstead, namely GGUF. llama-cpp-python  expects this format, which is generally\nused for compressed (quantized) models.\nOutput Verification  | 195\nSince we are loading a new model, it is advised to restart the notebook. That will clear\nany previous models and empty the VRAM. Y ou can also run the following to empty\nthe VRAM:\nimport gc\nimport torch\ndel model, tokenizer , pipe\n# Flush memory\ngc.collect()\ntorch.cuda.empty_cache ()\nNow that we have cleared the memory, we can load Phi-3. We set n_gpu_layers  to\n-1 to indicate that we want all layers of the model to be run from the GPU. The\nn_ctx  refers to the context size of the model. The repo_id  and filename  refer to the\nHugging Face repository  where the model resides:\nfrom llama_cpp.llama  import Llama\n# Load Phi-3\nllm = Llama.from_pretrained (\n    repo_id=""microsoft/Phi-3-mini-4k-instruct-gguf"" ,\n    filename =""*fp16.gguf"" ,\n    n_gpu_layers =-1,\n    n_ctx=2048,\n    verbose=False\n)\nTo generate the output using the internal JSON grammar, we only need to specify the\nresponse_format  as a JSON object. Under the hood, it will apply a JSON grammar to\nmake sure the output adheres to that format.\nTo illustrate, let’s ask the model to create an RPG character in JSON format to be used\nin a Dungeons & Dragons session:\n# Generate output\noutput = llm.create_chat_completion (\n    messages =[\n        {""role"": ""user"", ""content"" : ""Create a warrior for an RPG in JSON for\nmat.""},\n    ],\n    response_format ={""type"": ""json_object"" },\n    temperature =0,\n)['choices' ][0]['message' ][""content"" ]\nTo check whether the output actually is JSON, we can attempt to process it as such:\nimport json\n# Format as json\njson_output  = json.dumps(json.loads(output), indent=4)\nprint(json_output )\n196 | Chapter 6: Prompt Engineering\n{\n    ""name"": ""Eldrin Stormbringer"",\n    ""class"": ""Warrior"",\n    ""level"": 10,\n    ""attributes"": {\n        ""strength"": 18,\n        ""dexterity"": 12,\n        ""constitution"": 16,\n        ""intelligence"": 9,\n        ""wisdom"": 14,\n        ""charisma"": 10\n    },\n    ""skills"": {\n        ""melee_combat"": {\n            ""weapon_mastery"": 20,\n            ""armor_class"": 18,\n            ""hit_points"": 35\n        },\n        ""defense"": {\n            ""shield_skill"": 17,\n            ""block_chance"": 90\n        },\n        ""endurance"": {\n            ""health_regeneration"": 2,\n            ""stamina"": 30\n        }\n    },\n    ""equipment"": [\n        {\n            ""name"": ""Ironclad Armor"",\n            ""type"": ""Armor"",\n            ""defense_bonus"": 15\n        },\n        {\n            ""name"": ""Steel Greatsword"",\n            ""type"": ""Weapon"",\n            ""damage"": 8,\n            ""critical_chance"": 20\n        }\n    ],\n    ""background"": ""Eldrin grew up in a small village on the outskirts of a \nwar-torn land. Witnessing the brutality and suffering caused by conflict, he \ndedicated his life to becoming a formidable warrior who could protect those \nunable to defend themselves.""\n}\nThe output is properly formatted as JSON. This allows us to more confidently use\ngenerative models in applications where we expect the output to adhere to certain\nformats.\nOutput Verification  | 197",6088
80-Model IO Loading Quantized Models with LangChain.pdf,80-Model IO Loading Quantized Models with LangChain,"Summary\nIn this chapter, we explored the basics of using generative models through prompt\nengineering and output verification. We focused on the creativity and potential com‐\nplexity that comes with prompt engineering. These components of a prompt are key\nin generating and optimizing output appropriate for different use cases.\nWe further explored advanced prompt engineering techniques such as in-context\nlearning and chain-of-thought. These methods involve guiding generative models to\nreason through complex problems by providing examples or phrases that encourage\nstep-by-step thinking thereby mimicking human reasoning processes.\nOverall, this chapter demonstrated that prompt engineering is a crucial aspect of\nworking with LLMs, as it allows us to effectively communicate our needs and prefer‐\nences to the model. By mastering prompt engineering techniques, we can unlock\nsome of the potential of LLMs and generate high-quality responses that meet our\nrequirements.\nThe next chapter will build upon these concepts by exploring more advanced tech‐\nniques for leveraging generative models. We will go beyond prompt engineering and\nexplore how LLMs can use external memory and tools.\n198 | Chapter 6: Prompt Engineering\nCHAPTER 7\nAdvanced Text Generation\nTechniques and Tools\nIn the previous chapter, we saw how prompt engineering can do wonders for the\naccuracy of your text-generation large language model (LLM). With just a few small\ntweaks, these LLMs are guided toward more purposeful and accurate answers. This\nshowed how much there is to gain using techniques that do not fine-tune the LLM\nbut instead use the LLM more efficiently, such as the relatively straightforward\nprompt engineering.\nIn this chapter, we will continue this train of thought. What can we do to further\nenhance the experience and output that we get from the LLM without needing to\nfine-tune the model itself?\nFortunately, a great deal of methods and techniques allow us to further improve what\nwe started with in the previous chapter. These more advanced techniques lie at the\nfoundation of numerous LLM-focused systems and are, arguably, one of the first\nthings users implement when designing such systems.\nIn this chapter, we will explore several such methods and concepts for improving the\nquality of the generated text:\nModel I/O\nLoading and working with LLMs\nMemory\nHelping LLMs to remember\n199\nAgents\nCombining complex behavior with external tools\nChains\nConnecting methods and modules\nThese methods are all integrated with the LangChain framework  that will help us\neasily use these advanced techniques throughout this chapter. LangChain is one of\nthe earlier frameworks that simplify working with LLMs through useful abstractions.\nNewer frameworks of note are DSPy  and Haystack . Some of these abstractions are\nillustrated in Figure 7-1 . Note that retrieval will be discussed in the next chapter.\nFigure 7-1. LangChain is a complete framework for using LLMs. It has modular compo‐\nnents that can be chained together to allow for complex LLM systems.\nEach of these techniques has significant strengths by themselves but their true value\ndoes not exist in isolation. It is when you combine all of these techniques that you\nget an LLM-based system with incredible performance. The culmination of these\ntechniques is truly where LLMs shine.\nModel I/O: Loading Quantized Models with LangChain\nBefore  we can make use of LangChain’s features to extend the capabilities of LLMs,\nwe need to start by loading our LLM. As in previous chapters, we will be using\nPhi-3 but with a twist; we will use a GGUF model variant instead. A GGUF model\nrepresents a compressed version of its original counterpart through a method called\nquantization, which reduces the number of bits needed to represent the parameters of\nan LLM.\nBits, a series of 0s and 1s, represent values by encoding them in binary form. More\nbits result in a wider range of values but requires more memory to store those values,\nas shown in Figure 7-2 .\n200 | Chapter 7: Advanced Text Generation Techniques and Tools\nFigure 7-2. Attempting to represent pi with float  32-bit and float  16-bit representations.\nNotice the lowered accuracy when we halve the number of bits.\nQuantization reduces the number of bits required to represent the parameters of an\nLLM while attempting to maintain most of the original information. This comes with\nsome loss in precision but often makes up for it as the model is much faster to run,\nrequires less VRAM, and is often almost as accurate as the original.\nTo illustrate quantization, consider this analogy. If asked what the time is, you might\nsay “14:16, ” which is correct but not a fully precise answer. Y ou could have said it\nis “14:16 and 12 seconds” instead, which would have been more accurate. However,\nmentioning seconds is seldom helpful and we often simply put that in discrete\nnumbers, namely full minutes. Quantization is a similar process that reduces the\nprecision of a value (e.g., removing seconds) without removing vital information\n(e.g., retaining hours and minutes).\nIn Chapter 12 , we will further discuss how quantization works under the hood. Y ou\ncan also see a full visual guide to quantization in “ A Visual Guide to Quantization”\nby Maarten Grootendorst. For now, it is important to know that we will use an\n8-bit variant of Phi-3 compared to the original 16-bit variant, cutting the memory\nrequirements almost in half.\nAs a rule of thumb, look for at least 4-bit quantized models. These\nmodels have a good balance between compression and accuracy.\nAlthough it is possible to use 3-bit or even 2-bit quantized mod‐\nels, the performance degradation becomes noticeable and it would\ninstead be preferable to choose a smaller model with a higher\nprecision.\nFirst, we will need to download the model . Note that the link contains multiple files\nwith different bit-variants. FP16, the model we choose, represents the 16-bit variant:\nModel I/O: Loading Quantized Models with LangChain | 201",6083
81-A Single Link in the Chain Prompt Template.pdf,81-A Single Link in the Chain Prompt Template,"!wget https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/\nPhi-3-mini-4k-instruct-fp16.gguf\nWe use llama-cpp-python  together with LangChain to load the GGUF file:\nfrom langchain  import LlamaCpp\n# Make sure the model path is correct for your system!\nllm = LlamaCpp (\n    model_path =""Phi-3-mini-4k-instruct-fp16.gguf"" ,\n    n_gpu_layers =-1,\n    max_tokens =500,\n    n_ctx=2048,\n    seed=42,\n    verbose=False\n)\nIn LangChain, we use the invoke  function to generate output:\nllm.invoke(""Hi! My name is Maarten. What is 1 + 1?"" )\n''\nUnfortunately, we get no output! As we have seen in previous chapters, Phi-3 requires\na specific prompt template. Compared to our examples with transformers , we will\nneed to explicitly use a template ourselves. Instead of copy-pasting this template each\ntime we use Phi-3 in LangChain, we can use one of LangChain’s core functionalities,\nnamely “chains. ”\nAll examples in this chapter can be run with any LLM. This means\nthat you can choose whether to use Phi-3, ChatGPT, Llama 3 or\nanything else when going through the examples. We will use Phi-3\nas a default throughout, but the state-of-the-art changes quickly, so\nconsider using a newer model instead. Y ou can use the Open LLM\nLeaderboard  (a ranking of open source LLMs) to choose whichever\nworks best for your use case.\nIf you do not have access to a device that can run LLMs locally,\nconsider using ChatGPT instead:\nfrom langchain.chat_models  import ChatOpenAI\n# Create a chat-based LLM\nchat_model  = ChatOpenAI (openai_api_key =""MY_KEY"" )\nChains: Extending the Capabilities of LLMs\nLangChain  is named after one of its main methods, chains. Although we can run\nLLMs in isolation, their power is shown when used with additional components or\n202 | Chapter 7: Advanced Text Generation Techniques and Tools\neven when used in conjunction with each other. Chains not only allow for extending\nthe capabilities of LLMs but also for multiple chains to be connected together.\nThe most basic form of a chain in LangChain is a single chain. Although a chain can\ntake many forms, each with a different complexity, it generally connects an LLM with\nsome additional tool, prompt, or feature. This idea of connecting a component to an\nLLM is illustrated in Figure 7-3 .\nFigure 7-3. A single chain connects some modular component, like a prompt template or\nexternal memory, to the LLM.\nIn practice, chains can become complex quite quickly. We can extend the prompt\ntemplate however we want and we can even combine several separate chains together\nto create intricate systems. In order to thoroughly understand what is happening in a\nchain, let’s explore how we can add Phi-3’s prompt template to the LLM.\nA Single Link in the Chain: Prompt Template\nWe start with creating our first chain, namely the prompt template that Phi-3 expects.\nIn the previous chapter, we explored how transformers.pipeline  applies the chat\ntemplate automatically. This is not always the case with other packages and they\nmight need the prompt template to be explicitly defined. With LangChain, we will use\nchains to create and use a default prompt template. It also serves as a nice hands-on\nexperience with using prompt templates.\nThe idea, as illustrated in Figure 7-4 , is that we chain the prompt template together\nwith the LLM to get the output we are looking for. Instead of having to copy-paste the\nprompt template each time we use the LLM, we would only need to define the user\nand system prompts.\nFigure 7-4. By chaining a prompt template with an LLM, we only need to define  the\ninput prompts. The template will be constructed for you.\nChains: Extending the Capabilities of LLMs | 203\nThe template for Phi-3 is comprised of four main components:\n•<s> to indicate when the prompt starts •\n•<|user|>  to indicate the start of the user’s prompt •\n•<|assistant|>  to indicate the start of the model’s output •\n•<|end|>  to indicate the end of either the prompt or the model’s output •\nThese are further illustrated in Figure 7-5  with an example.\nFigure 7-5. The prompt template Phi-3 expects.\nTo generate our simple chain, we first need to create a prompt template that\nadheres to Phi-3’s expected template. Using this template, the model takes in a\nsystem_prompt , which generally describes what we expect from the LLM. Then, we\ncan use the input_prompt  to ask the LLM specific questions:\nfrom langchain  import PromptTemplate\n# Create a prompt template with the ""input_prompt"" variable\ntemplate  = """"""<s><|user|>\n{input_prompt} <|end|>\n<|assistant|>""""""\nprompt = PromptTemplate (\n    template =template ,\n    input_variables =[""input_prompt"" ]\n)\nTo create our first chain, we can use both the prompt that we created and the LLM\nand chain them together:\nbasic_chain  = prompt | llm\nTo use the chain, we need to use the invoke  function and make sure that we use the\ninput_prompt  to insert our question:\n204 | Chapter 7: Advanced Text Generation Techniques and Tools\n# Use the chain\nbasic_chain .invoke(\n    {\n        ""input_prompt"" : ""Hi! My name is Maarten. What is 1 + 1?"" ,\n    }\n)\nThe answer to 1 + 1 is 2. It's a basic arithmetic operation where you add one \nunit to another, resulting in two units altogether.\nThe output gives us the response without any unnecessary tokens. Now that we have\ncreated this chain, we do not have to create the prompt template from scratch each\ntime we use the LLM. Note that we did not disable sampling as before, so your\noutput might differ. To make this pipeline more transparent, Figure 7-6  illustrates the\nconnection between a prompt template and the LLM using a single chain.\nFigure 7-6. An example of a single chain using Phi-3’s template.\nThe example assumes that the LLM needs a specific template. This\nis not always the case. With OpenAI’s GPT-3.5, its API handles the\nunderlying template.\nY ou could also use a prompt template to define other variables that\nmight change in your prompts. For example, if we want to create\nfunny names for businesses, retyping that question over and over\nfor different products can be time-consuming.\nInstead, we can create a prompt that is reusable:\n# Create a Chain that creates our business' name\ntemplate  = ""Create a funny name for a business that \nsells {product} .""\nname_prompt  = PromptTemplate (\n    template =template ,\n    input_variables =[""product"" ]\n)\nAdding a prompt template to the chain is just the very first step you need to enhance\nthe capabilities of your LLM. Throughout this chapter, we will see many ways in\nwhich we can add additional modular components to existing chains, starting with\nmemory.\nChains: Extending the Capabilities of LLMs | 205",6756
82-A Chain with Multiple Prompts.pdf,82-A Chain with Multiple Prompts,"A Chain with Multiple Prompts\nIn our previous example, we created a single chain consisting of a prompt template\nand an LLM. Since our example was quite straightforward, the LLM had no issues\ndealing with the prompt. However, some applications are more involved and require\nlengthy or complex prompts to generate a response that captures those intricate\ndetails.\nInstead, we could break this complex prompt into smaller subtasks that can be run\nsequentially. This would require multiple calls to the LLM but with smaller prompts\nand intermediate outputs as shown in Figure 7-7 .\nFigure 7-7. With sequential chains, the output of a prompt is used as the input for the\nnext prompt.\nThis process of using multiple prompts is an extension of our previous example.\nInstead of using a single chain, we link chains where each link deals with a specific\nsubtask.\nFor instance, consider the process of generating a story. We could ask the LLM to\ngenerate a story along with complex details like the title, a summary, a description\nof the characters, etc. Instead of trying to put all of that information into a single\nprompt, we could dissect this prompt into manageable smaller tasks instead.\nLet’s illustrate with an example. Assume that we want to generate a story that has\nthree components:\n•A title•\n•A description of the main character•\n•A summary of the story•\nInstead of generating everything in one go, we create a chain that only requires a\nsingle input by the user and then sequentially generates the three components. This\nprocess is illustrated in Figure 7-8 .\n206 | Chapter 7: Advanced Text Generation Techniques and Tools\nFigure 7-8. The output of the title prompt is used as the input of the character prompt. To\ngenerate the story, the output of all previous prompts is used.\nTo generate that story, we use LangChain to describe the first component, namely the\ntitle. This first link is the only component that requires some input from the user. We\ndefine the template and use the ""summary""  variable as the input variable and ""title""\nas the output.\nWe ask the LLM to “Create a title for a story about {summary}” where “{summary}”\nwill be our input:\nfrom langchain  import LLMChain\n# Create a chain for the title of our story\ntemplate  = """"""<s><|user|>\nCreate a title for a story about {summary} . Only return the title.<|end|>\n<|assistant|>""""""\ntitle_prompt  = PromptTemplate (template =template , input_variables =[""summary"" ])\ntitle = LLMChain (llm=llm, prompt=title_prompt , output_key =""title"")\nLet’s run an example to showcase these variables:\ntitle.invoke({""summary"" : ""a girl that lost her mother"" })\n{'summary': 'a girl that lost her mother',\n 'title': ' ""Whispers of Loss: A Journey Through Grief""'}\nThis already gives us a great title for the story! Note that we can see both the input\n(""summary"" ) as well as the output ( ""title"" ).\nLet’s generate the next component, namely the description of the character. We\ngenerate this component using both the summary as well as the previously generated\ntitle. Making sure that the chain uses those components, we create a new prompt with\nthe {summary}  and {title}  tags:\nChains: Extending the Capabilities of LLMs | 207\n# Create a chain for the character description using the summary and title\ntemplate  = """"""<s><|user|>\nDescribe the main character of a story about {summary}  with the title {title}. \nUse only two sentences.<|end|>\n<|assistant|>""""""\ncharacter_prompt  = PromptTemplate (\n    template =template , input_variables =[""summary"" , ""title""]\n)\ncharacter  = LLMChain (llm=llm, prompt=character_prompt , output_key =""character"" )\nAlthough we could now use the character variable to generate our character descrip‐\ntion manually, it will be used as part of the automated chain instead.\nLet’s create the final component, which uses the summary, title, and character\ndescription to generate a short description of the story:\n# Create a chain for the story using the summary, title, and character descrip\ntion\ntemplate  = """"""<s><|user|>\nCreate a story about {summary}  with the title {title}. The main character is: \n{character} . Only return the story and it cannot be longer than one paragraph. \n<|end|>\n<|assistant|>""""""\nstory_prompt  = PromptTemplate (\n    template =template , input_variables =[""summary"" , ""title"", ""character"" ]\n)\nstory = LLMChain (llm=llm, prompt=story_prompt , output_key =""story"")\nNow that we have generated all three components, we can link them together to\ncreate our full chain:\n# Combine all three components to create the full chain\nllm_chain  = title | character  | story\nWe can run this newly created chain using the same example we used before:\nllm_chain .invoke(""a girl that lost her mother"" )\n{'summary': 'a girl that lost her mother',\n 'title': ' ""In Loving Memory: A Journey Through Grief""',\n 'character': ' The protagonist, Emily, is a resilient young girl who strug-\ngles to cope with her overwhelming grief after losing her beloved and caring \nmother at an early age. As she embarks on a journey of self-discovery and \nhealing, she learns valuable life lessons from the memories and wisdom shared \nby those around her.',\n 'story': "" In Loving Memory: A Journey Through Grief revolves around Emily, a \nresilient young girl who loses her beloved mother at an early age. Struggling \nto cope with overwhelming grief, she embarks on a journey of self-discovery \nand healing, drawing strength from the cherished memories and wisdom shared \nby those around her. Through this transformative process, Emily learns valua-\nble life lessons about resilience, love, and the power of human connection, \nultimately finding solace in honoring her mother's legacy while embracing a \nnewfound sense of inner peace amidst the painful loss.""}\n208 | Chapter 7: Advanced Text Generation Techniques and Tools",5889
83-Conversation Buffer.pdf,83-Conversation Buffer,"Running this chain gives us all three components. This only required us to input a\nsingle short prompt, the summary. Another advantage of dividing the problem into\nsmaller tasks is that we now have access to these individual components. We can\neasily extract the title; that might not have been the case if we were to use a single\nprompt.\nMemory: Helping LLMs to Remember Conversations\nWhen we are using LLMs out of the box, they will not remember what was being said\nin a conversation. Y ou can share your name in one prompt but it will have forgotten\nit by the next prompt.\nLet’s illustrate this phenomenon with an example using the basic_chain  we created\nbefore. First, we tell the LLM our name:\n# Let's give the LLM our name\nbasic_chain .invoke({""input_prompt"" : ""Hi! My name is Maarten. What is 1 + 1?"" })\nHello Maarten! The answer to 1 + 1 is 2.\nNext, we ask it to reproduce the name we have given it:\n# Next, we ask the LLM to reproduce the name\nbasic_chain .invoke({""input_prompt"" : ""What is my name?"" })\nI'm sorry, but as a language model, I don't have the ability to know personal \ninformation about individuals. You can provide the name you'd like to know \nmore about, and I can help you with information or general inquiries related \nto it.\nUnfortunately, the LLM does not know the name we gave it. The reason for this\nforgetful behavior is that these models are stateless—they have no memory of any\nprevious conversation!\nAs illustrated in Figure 7-9 , conversing with an LLM that does not have any memory\nis not the greatest experience.\nTo make these models stateful, we can add specific types of memory to the chain\nthat we created earlier. In this section, we will go through two common methods for\nhelping LLMs to remember conservations:\n•Conversation buffer•\n•Conversation summary•\nMemory: Helping LLMs to Remember Conversations | 209\nFigure 7-9. An example of a conversation between an LLM with memory and without\nmemory.\nConversation Buffer\nOne  of the most intuitive forms of giving LLMs memory is simply reminding them\nexactly what has happened in the past. As illustrated in Figure 7-10 , we can achieve\nthis by copying the full conversation history and pasting that into our prompt.\nFigure 7-10. We can remind an LLM of what previously happened by simply appending\nthe entire conversation history to the input prompt.\nIn LangChain, this form of memory is called a ConversationBufferMemory . Its\nimplementation requires us to update our previous prompt to hold the history of\nthe chat.\n210 | Chapter 7: Advanced Text Generation Techniques and Tools\nWe’ll start by creating this prompt:\n# Create an updated prompt template to include a chat history\ntemplate  = """"""<s><|user|>Current conversation: {chat_history}\n{input_prompt} <|end|>\n<|assistant|>""""""\nprompt = PromptTemplate (\n    template =template ,\n    input_variables =[""input_prompt"" , ""chat_history"" ]\n)\nNotice that we added an additional input variable, namely chat_history . This is\nwhere the conversation history will be given before we ask the LLM our question.\nNext, we can create LangChain’s ConversationBufferMemory  and assign it to the\nchat_history  input variable. ConversationBufferMemory  will store all the conversa‐\ntions we have had with the LLM thus far.\nWe put everything together and chain the LLM, memory, and prompt template:\nfrom langchain.memory  import ConversationBufferMemory\n# Define the type of memory we will use\nmemory = ConversationBufferMemory (memory_key =""chat_history"" )\n# Chain the LLM, prompt, and memory together\nllm_chain  = LLMChain (\n    prompt=prompt,\n    llm=llm,\n    memory=memory\n)\nTo explore whether we did this correctly, let’s create a conversation history with the\nLLM by asking it a simple question:\n# Generate a conversation and ask a basic question\nllm_chain .invoke({""input_prompt"" : ""Hi! My name is Maarten. What is 1 + 1?"" })\n{'input_prompt': 'Hi! My name is Maarten. What is 1 + 1?',\n 'chat_history': ',\n 'text': "" Hello Maarten! The answer to 1 + 1 is 2. Hope you're having a great \nday!""}\nY ou can find the generated text in the 'text'  key, the input prompt in 'in\nput_prompt' , and the chat history in 'chat_history' . Note that since this is the\nfirst time we used this specific chain, there is no chat history.\nNext, let’s follow up by asking the LLM if it remembers the name we used:\n# Does the LLM remember the name we gave it?\nllm_chain .invoke({""input_prompt"" : ""What is my name?"" })\nMemory: Helping LLMs to Remember Conversations | 211",4563
84-Windowed Conversation Buffer.pdf,84-Windowed Conversation Buffer,"{'input_prompt': 'What is my name?',\n 'chat_history': ""Human: Hi! My name is Maarten. What is 1 + 1?\nAI:  Hello \nMaarten! The answer to 1 + 1 is 2. Hope you're having a great day!"",\n 'text': ' Your name is Maarten.'}\nBy extending the chain with memory, the LLM was able to use the chat history to find\nthe name we gave it previously. This more complex chain is illustrated in Figure 7-11\nto give an overview of this additional functionality.\nFigure 7-11. We extend the LLM chain with memory by appending the entire conversa‐\ntion history to the input prompt.\nWindowed Conversation Buffer\nIn our previous example, we essentially created a chatbot. Y ou could talk to it and\nit remembers the conversation you had thus far. However, as the size of the conversa‐\ntion grows, so does the size of the input prompt until it exceeds the token limit.\nOne method of minimizing the context window is to use the last k conversations\ninstead of maintaining the full chat history. In LangChain, we can use Conversation\nBufferWindowMemory  to decide how many conversations are passed to the input\nprompt:\nfrom langchain.memory  import ConversationBufferWindowMemory\n# Retain only the last 2 conversations in memory\nmemory = ConversationBufferWindowMemory (k=2, memory_key =""chat_history"" )\n# Chain the LLM, prompt, and memory together\nllm_chain  = LLMChain (\n    prompt=prompt,\n    llm=llm,\n    memory=memory\n)\nUsing this memory, we can try out a sequence of questions to illustrate what will be\nremembered. We start with two conversations:\n# Ask two questions and generate two conversations in its memory\nllm_chain .predict(input_prompt =""Hi! My name is Maarten and I am 33 years old. \n212 | Chapter 7: Advanced Text Generation Techniques and Tools\nWhat is 1 + 1?"" )\nllm_chain .predict(input_prompt =""What is 3 + 3?"" )\n{'input_prompt': 'What is 3 + 3?',\n'chat_history': ""Human: Hi! My name is Maarten and I am 33 years old. What is \n1 + 1?\nAI: Hello Maarten! It's nice to meet you. Regarding your question, 1 + \n1 equals 2. If you have any other questions or need further assistance, feel \nfree to ask!\n\n(Note: This response answers the provided mathematical query \nwhile maintaining politeness and openness for additional inquiries.)"",\n'text': "" Hello Maarten! It's nice to meet you as well. Regarding your new \nquestion, 3 + 3 equals 6. If there's anything else you need help with or more \nquestions you have, I'm here for you!""}\nThe interaction we had thus far is shown in ""chat_history"" . Note that under the\nhood, LangChain saves it as an interaction between you (indicated with Human) and\nthe LLM (indicated with AI).\nNext, we can check whether the model indeed knows the name we gave it:\n# Check whether it knows the name we gave it\nllm_chain .invoke({""input_prompt"" :""What is my name?"" })\n{'input_prompt': 'What is my name?',\n'chat_history': ""Human: Hi! My name is Maarten and I am 33 years old. What is \n1 + 1?\nAI: Hello Maarten! It's nice to meet you. Regarding your question, 1 + \n1 equals 2. If you have any other questions or need further assistance, feel \nfree to ask!\n\n(Note: This response answers the provided mathematical query \nwhile maintaining politeness and openness for additional inquiries.)\nHuman: \nWhat is 3 + 3?\nAI: Hello Maarten! It's nice to meet you as well. Regarding \nyour new question, 3 + 3 equals 6. If there's anything else you need help with \nor more questions you have, I'm here for you!"",\n'text': ' Your name is Maarten, as mentioned at the beginning of our conversa-\ntion. Is there anything else you would like to know or discuss?'}\nBased on the output in 'text'  it correctly remembers the name we gave it. Note that\nthe chat history is updated with the previous question.\nNow that we have added another conversation we are up to three conversations. Con‐\nsidering the memory only retains the last two conversations, our very first question is\nnot remembered.\nSince we provided an age in our first interaction, we check whether the LLM indeed\ndoes not know the age anymore:\n# Check whether it knows the age we gave it\nllm_chain .invoke({""input_prompt"" :""What is my age?"" })\n{'input_prompt': 'What is my age?',\n'chat_history': ""Human: What is 3 + 3?\nAI: Hello again! 3 + 3 equals 6. If \nthere's anything else I can help you with, just let me know!\nHuman: What is \nmy name?\nAI: Your name is Maarten."",\n'text': "" I'm unable to determine your age as I don't have access to personal \ninformation. Age isn't something that can be inferred from our current con-\nversation unless you choose to share it with me. How else may I assist you \ntoday?""}\nMemory: Helping LLMs to Remember Conversations | 213",4706
85-Conversation Summary.pdf,85-Conversation Summary,"The LLM indeed has no access to our age since that was not retained in the chat\nhistory.\nAlthough this method reduces the size of the chat history, it can only retain the last\nfew conversations, which is not ideal for lengthy conversations. Let’s explore how we\ncan summarize the chat history instead.\nConversation Summary\nAs we have discussed previously, giving your LLM the ability to remember conver‐\nsations is vital for a good interactive experience. However, when using Conversation\nBufferMemory , the conversation starts to increase in size and will slowly approach\nyour token limit. Although ConversationBufferWindowMemory  resolves the issue of\ntoken limits to an extent, only the last k conversations are retained.\nAlthough a solution would be to use an LLM with a larger context window, these\ntokens still need to be processed before generation tokens, which can increase\ncompute time. Instead, let’s look toward a more sophisticated technique, Conversa\ntionSummaryMemory . As the name implies, this technique summarizes an entire con‐\nversation history to distill it into the main points.\nThis summarization process is enabled by another LLM that is given the conversation\nhistory as input and asked to create a concise summary. A nice advantage of using an\nexternal LLM is that we are not confined to using the same LLM during conversation.\nThe summarization process is illustrated in Figure 7-12 .\nFigure 7-12. Instead of passing the conversation history directly to the prompt, we use\nanother LLM to summarize it first.\n214 | Chapter 7: Advanced Text Generation Techniques and Tools\nThis means that whenever we ask the LLM a question, there are two calls:\n•The user prompt•\n•The summarization prompt•\nTo use this in LangChain, we first need to prepare a summarization template that we\nwill use as the summarization prompt:\n# Create a summary prompt template\nsummary_prompt_template  = """"""<s><|user|>Summarize the conversations and update \nwith the new lines.\nCurrent summary:\n{summary}\nnew lines of conversation:\n{new_lines}\nNew summary:<|end|>\n<|assistant|>""""""\nsummary_prompt  = PromptTemplate (\n    input_variables =[""new_lines"" , ""summary"" ],\n    template =summary_prompt_template\n)\nUsing ConversationSummaryMemory  in LangChain is similar to what we did with the\nprevious examples. The main difference is that we additionally need to supply it\nwith an LLM that performs the summarization task. Although we use the same LLM\nfor both summarizing and user prompting, you could use a smaller LLM for the\nsummarization task to speed up computation:\nfrom langchain.memory  import ConversationSummaryMemory\n# Define the type of memory we will use\nmemory = ConversationSummaryMemory (\n    llm=llm, \n    memory_key =""chat_history"" , \n    prompt=summary_prompt\n)\n# Chain the LLM, prompt, and memory together\nllm_chain  = LLMChain (\n    prompt=prompt,\n    llm=llm,\n    memory=memory\n)\nMemory: Helping LLMs to Remember Conversations | 215\nHaving created our chain, we can test out its summarization capabilities by creating a\nshort conversation:\n# Generate a conversation and ask for the name\nllm_chain .invoke({""input_prompt"" : ""Hi! My name is Maarten. What is 1 + 1?"" })\nllm_chain .invoke({""input_prompt"" : ""What is my name?"" })\n{'input_prompt': 'What is my name?',\n'chat_history': ' Summary: Human, identified as Maarten, asked the AI about \nthe sum of 1 + 1, which was correctly answered by the AI as 2 and offered \nadditional assistance if needed.',\n'text': ' Your name in this context was referred to as ""Maarten"". However, \nsince our interaction doesn\'t retain personal data beyond a single session \nfor privacy reasons, I don\'t have access to that information. How can I \nassist you further today?'}\nAfter each step, the chain will summarize the conversation up until that point.\nNote how the first conversation was summarized in 'chat_history'  by creating a\ndescription of the conversation.\nWe can continue the conversation and at each step, the conversation will be summar‐\nized and new information will be added as necessary:\n# Check whether it has summarized everything thus far\nllm_chain .invoke({""input_prompt"" : ""What was the first question I asked?"" })\n{'input_prompt': 'What was the first question I asked?',\n'chat_history': ' Summary: Human, identified as Maarten in the context of this \nconversation, first asked about the sum of 1 + 1 and received an answer of \n2 from the AI. Later, Maarten inquired about their name but the AI clarified \nthat personal data is not retained beyond a single session for privacy rea-\nsons. The AI offered further assistance if needed.',\n'text': ' The first question you asked was ""what\'s 1 + 1?""'}\nAfter asking another question, the LLM updated the summary to include the previ‐\nous conversation and correctly inferred the original question.\nTo get the most recent summary, we can access the memory variable we created\npreviously:\n# Check what the summary is thus far\nmemory.load_memory_variables ({})\n{'chat_history': ' Maarten, identified in this conversation, initially asked \nabout the sum of 1+1 which resulted in an answer from the AI being 2. Subse-\nquently, he sought clarification on his name but the AI informed him that no \npersonal data is retained beyond a single session due to privacy reasons. The \nAI then offered further assistance if required. Later, Maarten recalled and \nasked about the first question he inquired which was ""what\'s 1+1?""'}\nThis more complex chain is illustrated in Figure 7-13  to give an overview of this\nadditional functionality.\n216 | Chapter 7: Advanced Text Generation Techniques and Tools\nFigure 7-13. We extend the LLM chain with memory by summarizing the entire conver‐\nsation history before giving it to the input prompt.\nThis summarization helps keep the chat history relatively small without using too\nmany tokens during inference. However, since the original question was not explicitly\nsaved in the chat history, the model needed to infer it based on the context. This is a\ndisadvantage if specific information needs to be stored in the chat history. Moreover,\nmultiple calls to the same LLM are needed, one for the prompt and one for the\nsummarization. This can slow down computing time.\nOften, it is a trade-off between speed, memory, and accuracy. Where Conversation\nBufferMemory  is instant but hogs tokens, ConversationSummaryMemory  is slow but\nfrees up tokens to use. Additional pros and cons of the memory types we have\nexplored thus far are described in Table 7-1 .\nTable 7-1. The pros and cons of different  memory types.\nMemory type Pros Cons\nConversation\nBuffer•Easiest implementation•\n•Ensures no information loss within context•\nwindow•Slower generation speed as more tokens are•\nneeded\n•Only suitable for large-context LLMs•\n•Larger chat histories make information retrieval•\ndifficult\nWindowed\nConversation\nBuffer•Large-context LLMs are not needed unless•\nchat history is large\n•No information loss over the last k •\ninteractions•Only captures the last k interactions •\n•No compression of the last k interactions •\nConversation\nSummary•Captures the full history•\n•Enables long conversations•\n•Reduces tokens needed to capture full history••An additional call is necessary for each interaction•\n•Quality is reliant on the LLM’s summarization•\ncapabilities\nMemory: Helping LLMs to Remember Conversations | 217",7474
86-The Driving Power Behind Agents Step-by-step Reasoning.pdf,86-The Driving Power Behind Agents Step-by-step Reasoning,"Agents: Creating a System of LLMs\nThus  far, we have created systems that follow a user-defined set of steps to take. One\nof the most promising concepts in LLMs is their ability to determine the actions they\ncan take. This idea is often called agents, systems that leverage a language model to\ndetermine which actions they should take and in what order.\nAgents can make use of everything we have seen thus far, such as model I/O, chains,\nand memory, and extend it further with two vital components:\n•Tools  that the agent can use to do things it could not do itself •\n•The agent type , which plans the actions to take or tools to use •\nUnlike the chains we have seen thus far, agents are able to show more advanced\nbehavior like creating and self-correcting a roadmap to achieve a goal. They can\ninteract with the real world through the use of tools. As a result, these agents can\nperform a variety of tasks that go beyond what an LLM is capable of in isolation.\nFor example, LLMs are notoriously bad at mathematical problems and often fail at\nsolving simple math-based tasks but they could do much more if we provide access\nto a calculator. As illustrated in Figure 7-14 , the underlying idea of agents is that they\nutilize LLMs not only to understand our query but also to decide which tool to use\nand when.\nFigure 7-14. Giving LLMs the ability to choose which tools they use for a particular\nproblem results in more complex and accurate behavior.\nIn this example, we would expect the LLM to use the calculator when it faces a math‐\nematical task. Now imagine we extend this with dozens of other tools, like a search\nengine or a weather API. Suddenly, the capabilities of LLMs increase significantly.\n218 | Chapter 7: Advanced Text Generation Techniques and Tools\n1Shunyu Y ao et al. “ReAct: Synergizing reasoning and acting in language models. ” arXiv preprint\narXiv:2210.03629  (2022).In other words, agents that make use of LLMs can be powerful general problem solv‐\ners. Although the tools they use are important, the driving force of many agent-based\nsystems is the use of a framework called Reasoning and Acting (ReAct1).\nThe Driving Power Behind Agents: Step-by-step Reasoning\nReAct  is a powerful framework that combines two important concepts in behavior:\nreasoning and acting. LLMs are exceptionally powerful when it comes to reasoning as\nwe explored in detail in Chapter 5 .\nActing is a bit of a different story. LLMs are not able to act like you and I do. To\ngive them the ability to act, we could tell an LLM that it can use certain tools, like\na weather forecasting API. However, since LLMs can only generate text, they would\nneed to be instructed to use specific queries to trigger the forecasting API.\nReAct  merges these two concepts and allows reasoning to affect acting and actions\nto affect reasoning. In practice, the framework consists of iteratively following these\nthree steps:\n•Thought•\n•Action•\n•Observation•\nIllustrated in Figure 7-15 , the LLM is asked to create a “thought” about the input\nprompt. This is similar to asking the LLM what it thinks it should do next and\nwhy. Then, based on the thought, an “action” is triggered. The action is generally\nan external tool, like a calculator or a search engine. Finally, after the results of the\n“action” are returned to the LLM it “observes” the output, which is often a summary\nof whatever result it retrieved.\nTo illustrate with an example, imagine you are on holiday in the United States and\ninterested in buying a MacBook Pro. Not only do you want to know the price but you\nneed it converted to EUR as you live in Europe and are more comfortable with those\nprices.\nAs illustrated in Figure 7-16 , the agent will first search the web for current prices. It\nmight find one or more prices depending on the search engine. After retrieving the\nprice, it will use a calculator to convert USD to EUR assuming we know the exchange\nrate.\nAgents: Creating a System of LLMs | 219\nFigure 7-15. An example of a ReAct prompt template.\nFigure 7-16. An example of two cycles in a ReAct pipeline.\n220 | Chapter 7: Advanced Text Generation Techniques and Tools",4183
87-ReAct in LangChain.pdf,87-ReAct in LangChain,"During this process, the agent describes its thoughts (what it should do), its actions\n(what it will do), and its observations (the results of the action). It is a cycle of\nthoughts, actions, and observations that results in the agent’s output.\nReAct in LangChain\nTo illustrate how agents work in LangChain, we are going to build a pipeline that\ncan search the web for answers and perform calculations with a calculator. These\nautonomous processes generally require an LLM that is powerful enough to properly\nfollow complex instructions.\nThe LLM that we used thus far is relatively small and not sufficient to run these\nexamples. Instead, we will be using OpenAI’s GPT-3.5 model as it follows these\ncomplex instructions more closely:\nimport os\nfrom langchain_openai  import ChatOpenAI\n# Load OpenAI's LLMs with LangChain\nos.environ[""OPENAI_API_KEY"" ] = ""MY_KEY""\nopenai_llm  = ChatOpenAI (model_name =""gpt-3.5-turbo"" , temperature =0)\nAlthough the LLM we used throughout the chapter is insufficient\nfor this example, it does not mean that only OpenAI’s LLMs are.\nLarger useful LLMs exist but they require significantly more com‐\npute and VRAM. For instance, local LLMs often come in different\nsizes and within a family of models, increasing a model’s size leads\nto better performance. To keep the necessary compute at a mini‐\nmum, we choose a smaller LLM throughout the examples in this\nchapter.\nHowever, as the field of generative models evolves, so do these\nsmaller LLMs. We would be anything but surprised if eventually\nsmaller LLMs, like the one used in this chapter, would be capable\nenough to run this example.\nAfter doing so, we will define the template for our agent. As we have shown before, it\ndescribes the ReAct steps it needs to follow:\n# Create the ReAct template\nreact_template  = """"""Answer the following questions as best you can. You have \naccess to the following tools:\n{tools}\nUse the following format:\nQuestion: the input question you must answer\nThought: you should always think about what to do\nAgents: Creating a System of LLMs | 221\nAction: the action to take, should be one of [ {tool_names} ]\nAction Input: the input to the action\nObservation: the result of the action\n... (this Thought/Action/Action Input/Observation can repeat N times)\nThought: I now know the final answer\nFinal Answer: the final answer to the original input question\nBegin!\nQuestion: {input}\nThought: {agent_scratchpad} """"""\nprompt = PromptTemplate (\n    template =react_template ,\n    input_variables =[""tools"", ""tool_names"" , ""input"", ""agent_scratchpad"" ]\n)\nThis template illustrates the process of starting with a question and generating inter‐\nmediate thoughts, actions, and observations.\nTo have the LLM interact with the outside world, we will describe the tools it can use:\nfrom langchain.agents  import load_tools , Tool\nfrom langchain.tools  import DuckDuckGoSearchResults\n# You can create the tool to pass to an agent\nsearch = DuckDuckGoSearchResults ()\nsearch_tool  = Tool(\n    name=""duckduck"" ,\n    description =""A web search engine. Use this to as a search engine for gen\neral queries."" ,\n    func=search.run,\n)\n# Prepare tools\ntools = load_tools ([""llm-math"" ], llm=openai_llm )\ntools.append(search_tool )\nThe tools include the DuckDuckGo  search engine and a math tool that allows it to\naccess a basic calculator.\nFinally, we create the ReAct agent and pass it to the AgentExecutor , which handles\nexecuting the steps:\nfrom langchain.agents  import AgentExecutor , create_react_agent\n# Construct the ReAct agent\nagent = create_react_agent (openai_llm , tools, prompt)\nagent_executor  = AgentExecutor (\n    agent=agent, tools=tools, verbose=True, handle_parsing_errors =True\n)\n222 | Chapter 7: Advanced Text Generation Techniques and Tools\nTo test whether the agent works, we use the previous example, namely finding the\nprice of a MacBook Pro:\n# What is the price of a MacBook Pro?\nagent_executor .invoke(\n    {\n        ""input"": ""What is the current price of a MacBook Pro in USD? How much \nwould it cost in EUR if the exchange rate is 0.85 EUR for 1 USD.""\n    }\n)\nWhile executing, the model generates multiple intermediate steps similar to the steps\nillustrated in Figure 7-17 .\nFigure 7-17. An example of the ReAct process in LangChain.\nThese intermediate steps illustrate how the model processes the ReAct template and\nwhat tools it accesses. This allows us to debug issues and explore whether the agent\nuses the tools correctly.\nWhen finished, the model gives us an output like this:\n{'input': 'What is the current price of a MacBook Pro in USD? How much would \nit cost in EUR if the exchange rate is 0.85 EUR for 1 USD?',\n 'output': 'The current price of a MacBook Pro in USD is $2,249.00. It would \ncost approximately 1911.65 EUR with an exchange rate of 0.85 EUR for 1 USD.'}\nConsidering the limited tools the agent has, this is quite impressive! Using just a\nsearch engine and a calculator the agent could give us an answer.\nWhether that answer is actually correct should be taken into account. By creating\nthis relatively autonomous behavior, we are not involved in the intermediate steps. As\nsuch, there is no human in the loop to judge the quality of the output or reasoning\nprocess.\nThis double-edged sword requires a careful system design to improve its reliability.\nFor instance, we could have the agent return the website’s URL where it found the\nMacBook Pro’s price or ask whether the output is correct at each step.\nAgents: Creating a System of LLMs | 223",5589
88-Overview of Semantic Search and RAG.pdf,88-Overview of Semantic Search and RAG,"Summary\nIn this chapter, we explored several ways to extend the capabilities of LLMs by\nadding modular components. We began by creating a simple but reusable chain that\nconnected the LLM with a prompt template. We then expanded on this concept by\nadding memory to the chain, which allowed the LLM to remember conversations. We\nexplored three different methods to add memory and discussed their strengths and\nweaknesses.\nWe then delved into the world of agents that leverage LLMs to determine their\nactions and make decisions. We explored the ReAct framework, which uses an\nintuitive prompting framework that allows agents to reason about their thoughts,\ntake actions, and observe the results. This led us to build an agent that is able to\nfreely use the tools at its disposal, such as searching the web and using a calculator,\ndemonstrating the potential power of agents.\nWith this foundation in place, we are now poised to explore ways in which LLMs can\nbe used to improve existing search systems and even become the core of new, more\npowerful search systems, as discussed in the next chapter.\n224 | Chapter 7: Advanced Text Generation Techniques and Tools\nCHAPTER 8\nSemantic Search and\nRetrieval-Augmented Generation\nSearch was one of the first language model applications to see broad industry adop‐\ntion. Months after the release of the seminal “BERT: Pre-training of deep bidirec‐\ntional transformers for language understanding”  (2018) paper, Google announced it\nwas using it to power Google Search and that it represented  “one of the biggest leaps\nforward in the history of Search. ” Not to be outdone, Microsoft Bing also stated  that\n“Starting from April of this year, we used large transformer models to deliver the\nlargest quality improvements to our Bing customers in the past year. ”\nThis is a clear testament to the power and usefulness of these models. Their addi‐\ntion instantly and dramatically improves some of the most mature, well-maintained\nsystems that billions of people around the planet rely on. The ability they add is\ncalled semantic search , which enables searching by meaning, and not simply keyword\nmatching.\nOn a separate track, the fast adoption of text generation models led many users to\nask the models questions and expect factual answers. And while the models were\nable to answer fluently and confidently, their answers were not always correct or\nup-to-date. This problem grew to be known as model “hallucinations, ” and one of the\nleading ways to reduce it is to build systems that can retrieve relevant information\nand provide it to the LLM to aid it in generating more factual answers. This method,\ncalled RAG, is one of the most popular applications of LLMs.\n225\nOverview of Semantic Search and RAG\nThere’s a lot of research on how to best use language models for search. Three\nbroad categories of these models are dense retrieval, reranking, and RAG. Here is\nan overview of these three categories that the rest of the chapter will then explain in\nmore detail:\nDense retrieval\nDense  retrieval systems rely on the concept of embeddings, the same concept\nwe’ve encountered in the previous chapters, and turn the search problem into\nretrieving the nearest neighbors of the search query (after both the query and\nthe documents are converted into embeddings). Figure 8-1  shows how dense\nretrieval takes a search query, consults its archive of texts, and outputs a set of\nrelevant results.\nFigure 8-1. Dense retrieval is one of the key types of semantic search, relying on the\nsimilarity of text embeddings to retrieve relevant results.\nReranking\nSearch systems are often pipelines of multiple steps. A reranking language model\nis one of these steps and is tasked with scoring the relevance of a subset of results\nagainst the query; the order of results is then changed based on these scores.\nFigure 8-2  shows how rerankers are different from dense retrieval in that they\ntake an additional input: a set of search results from a previous step in the search\npipeline.\n226 | Chapter 8: Semantic Search and Retrieval-Augmented Generation\nFigure 8-2. Rerankers, the second key type of semantic search, take a search query\nand a collection of results, and reorder them by relevance, often  resulting in vastly\nimproved results.\nRAG\nThe growing LLM capability of text generation led to a new type of search\nsystems that include a model that generates an answer in response to a query.\nFigure 8-3  shows an example of such a generative search system.\nGenerative search is a subset of a broader type of category of systems better\ncalled RAG systems. These are text generation systems that incorporate search\ncapabilities to reduce hallucinations, increase factuality, and/or ground the gen‐\neration model on a specific dataset.\nFigure 8-3. A RAG system formulates an answer to a question and (preferably) cites its\ninformation sources.\nThe rest of the chapter covers these three types of systems in more detail. While these\nare the major categories, they are not the only LLM applications in the domain of\nsearch.\nOverview of Semantic Search and RAG | 227",5163
89-Semantic Search with Language Models.pdf,89-Semantic Search with Language Models,,0
90-Dense Retrieval.pdf,90-Dense Retrieval,"Semantic Search with Language Models\nLet’s now dive into more detail on the major categories of systems that can upgrade\nthe search capabilities of our language models. We’ll start with dense retrieval and\nthen move on through reranking and RAG.\nDense Retrieval\nRecall that embeddings turn text into numeric representations. Those can be thought\nof as points in space, as we can see in Figure 8-4 . Points that are close together mean\nthat the text they represent is similar. So in this example, text 1 and text 2 are more\nsimilar to each other (because they are near each other) than text 3 (because it’s\nfarther away).\nFigure 8-4. The intuition of embeddings: each text is a point and texts with similar\nmeaning are close to each other.\nThis is the property that is used to build search systems. In this scenario, when a user\nenters a search query, we embed the query, thus projecting it into the same space as\nour text archive. Then we simply find the nearest documents to the query in that\nspace, and those would be the search results ( Figure 8-5 ).\n228 | Chapter 8: Semantic Search and Retrieval-Augmented Generation\nFigure 8-5. Dense retrieval relies on the property that search queries will be close to their\nrelevant results.\nJudging by the distances in Figure 8-5 , “text 2” is the best result for this query,\nfollowed by “text 1. ” Two questions could arise here, however:\n•Should text 3 even be returned as a result? That’s a decision for you, the system•\ndesigner. It’s sometimes desirable to have a max threshold of similarity score to\nfilter out irrelevant results (in case the corpus has no relevant results for the\nquery).\n•Are a query and its best result semantically similar? Not always. This is why•\nlanguage models need to be trained on question-answer pairs to become better at\nretrieval. This process is explained in more detail in Chapter 10 .\nFigure 8-6  shows how we chunk a document before proceeding to embed each chunk.\nThose embedding vectors are then stored in the vector database and are ready for\nretrieval.\nSemantic Search with Language Models | 229\nFigure 8-6. Convert some external knowledge base to a vector database. We can then\nquery this vector database for information about the knowledge base.\nDense retrieval example\nLet’s take a look at a dense retrieval example by using Cohere to search the Wikipedia\npage for the film Interstellar . In this example, we will do the following:\n1.Get the text we want to make searchable and apply some light processing to1.\nchunk it into sentences.\n2.Embed the sentences.2.\n3.Build the search index.3.\n4.Search and see the results.4.\nGet your Cohere API key by signing up at https://oreil.ly/GxrQ1 . Paste it in the\nfollowing code. Y ou will not have to pay anything to run through this example.\nLet’s import the libraries we’ll need:\nimport cohere\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n# Paste your API key here. Remember to not share publicly\napi_key = ''\n# Create and retrieve a Cohere API key from os.cohere.ai\nco = cohere.Client(api_key)\nGetting the text archive and chunking it.    Let’s  use the first section of the Wikipedia arti‐\ncle on the film Interstellar . We’ll get the text, then break it into sentences:\n230 | Chapter 8: Semantic Search and Retrieval-Augmented Generation\ntext = """"""\nInterstellar is a 2014 epic science fiction film co-written, directed, and pro\nduced by Christopher Nolan. \nIt stars Matthew McConaughey, Anne Hathaway, Jessica Chastain, Bill Irwin, \nEllen Burstyn, Matt Damon, and Michael Caine. \nSet in a dystopian future where humanity is struggling to survive, the film \nfollows a group of astronauts who travel through a wormhole near Saturn in \nsearch of a new home for mankind.\nBrothers Christopher and Jonathan Nolan wrote the screenplay, which had its \norigins in a script Jonathan developed in 2007. \nCaltech theoretical physicist and 2017 Nobel laureate in Physics[4] Kip Thorne \nwas an executive producer, acted as a scientific consultant, and wrote a tie-in \nbook, The Science of Interstellar. \nCinematographer Hoyte van Hoytema shot it on 35 mm movie film in the Panavision \nanamorphic format and IMAX 70 mm. \nPrincipal photography began in late 2013 and took place in Alberta, Iceland, \nand Los Angeles. \nInterstellar uses extensive practical and miniature effects and the company \nDouble Negative created additional digital effects.\nInterstellar premiered on October 26, 2014, in Los Angeles. \nIn the United States, it was first released on film stock, expanding to venues \nusing digital projectors. \nThe film had a worldwide gross over $677 million (and $773 million with subse\nquent re-releases), making it the tenth-highest grossing film of 2014. \nIt received acclaim for its performances, direction, screenplay, musical score, \nvisual effects, ambition, themes, and emotional weight. \nIt has also received praise from many astronomers for its scientific accuracy \nand portrayal of theoretical astrophysics. Since its premiere, Interstellar \ngained a cult following,[5] and now is regarded by many sci-fi experts as one \nof the best science-fiction films of all time.\nInterstellar was nominated for five awards at the 87th Academy Awards, winning \nBest Visual Effects, and received numerous other accolades""""""\n# Split into a list of sentences\ntexts = text.split('.')\n# Clean up to remove empty spaces and new lines\ntexts = [t.strip(' \n') for t in texts]\nEmbedding the text chunks.    Let’s now embed the texts. We’ll send them to the Cohere\nAPI, and get back a vector for each text:\n# Get the embeddings\nresponse  = co.embed(\n  texts=texts,\n  input_type =""search_document"" ,\n).embeddings\nembeds = np.array(response )\nprint(embeds.shape)\nSemantic Search with Language Models | 231\nThis outputs (15, 4096) , which indicates that we have 15 vectors, each one of size\n4,096.\nBuilding the search index.    Before we can search, we need to build a search index.\nAn index stores the embeddings and is optimized to quickly retrieve the nearest\nneighbors even if we have a very large number of points:\nimport faiss\ndim = embeds.shape[1]\nindex = faiss.IndexFlatL2 (dim)\nprint(index.is_trained )\nindex.add(np.float32(embeds))\nSearch the index.    We can now search the dataset using any query we want. We simply\nembed the query and present its embedding to the index, which will retrieve the most\nsimilar sentence from the Wikipedia article.\nLet’s define our search function:\ndef search(query, number_of_results =3):\n  \n  # 1. Get the query's embedding\n  query_embed  = co.embed(texts=[query], \n                input_type =""search_query"" ,).embeddings [0]\n  # 2. Retrieve the nearest neighbors\n  distances  , similar_item_ids  = index.search(np.float32([query_embed ]), num\nber_of_results ) \n  # 3. Format the results\n  texts_np  = np.array(texts) # Convert texts list to numpy for easier indexing\n  results = pd.DataFrame (data={'texts': texts_np [similar_item_ids [0]], \n                              'distance' : distances [0]})\n  \n  # 4. Print and return the results\n  print(f""Query:' {query}'\nNearest neighbors:"" )\n  return results\nWe are now ready to write a query and search the texts!\nquery = ""how precise was the science""\nresults = search(query)\nresults\nThis produces the following output:\n232 | Chapter 8: Semantic Search and Retrieval-Augmented Generation\nQuery: 'how precise was the science'\nNearest neighbors:\n texts distance\n0 It has also received praise from many astronomers for its scientific  accuracy and portrayal of\ntheoretical astrophysics10757.379883\n1 Caltech theoretical physicist and 2017 Nobel laureate in Physics[4] Kip Thorne was an executive\nproducer, acted as a scientific  consultant, and wrote a tie-in book, The Science of Interstellar11566.131836\n2 Interstellar uses extensive practical and miniature effects  and the company Double Negative\ncreated additional digital effects11922.833008\nThe first result has the least distance, and so is the most similar to the query. Looking\nat it, it answers the question perfectly. Notice that this wouldn’t have been possible if\nwe were only doing keyword search because the top result did not include the same\nkeywords in the query.\nWe can actually verify that by defining a keyword search function to compare the\ntwo. We’ll use the BM25 algorithm, which is one of the leading lexical search meth‐\nods. See this notebook  for the source of these code snippets:\nfrom rank_bm25  import BM25Okapi\nfrom sklearn.feature_extraction  import _stop_words\nimport string\ndef bm25_tokenizer (text):\n    tokenized_doc  = []\n    for token in text.lower().split():\n        token = token.strip(string.punctuation )\n        if len(token) > 0 and token not in _stop_words .ENGLISH_STOP_WORDS :\n            tokenized_doc .append(token)\n    return tokenized_doc\ntokenized_corpus  = []\nfor passage in tqdm(texts):\n    tokenized_corpus .append(bm25_tokenizer (passage))\nbm25 = BM25Okapi (tokenized_corpus )\ndef keyword_search (query, top_k=3, num_candidates =15):\n    print(""Input question:"" , query)\n    ##### BM25 search (lexical search) #####\n    bm25_scores  = bm25.get_scores (bm25_tokenizer (query))\n    top_n = np.argpartition (bm25_scores , -num_candidates )[-num_candidates :]\n    bm25_hits  = [{'corpus_id' : idx, 'score': bm25_scores [idx]} for idx in top_n]\n    bm25_hits  = sorted(bm25_hits , key=lambda x: x['score'], reverse=True)\nSemantic Search with Language Models | 233\n    \n    print(f""Top-3 lexical search (BM25) hits"" )\n    for hit in bm25_hits [0:top_k]:\n        print(""\t{:.3f}\t{}"".format(hit['score'], texts[hit['cor\npus_id']].replace(""\n"", "" "")))\nNow when we search for the same query, we get a different set of results from the\ndense retrieval search:\nkeyword_search (query = ""how precise was the science"" )\nResults:\nInput question: how precise was the science\nTop-3 lexical search (BM25) hits\n      1.789 Interstellar is a 2014 epic science fiction film co-written, direc-\nted, and produced by Christopher Nolan\n      1.373 Caltech theoretical physicist and 2017 Nobel laureate in Phys-\nics[4] Kip Thorne was an executive producer, acted as a scientific consultant, \nand wrote a tie-in book, The Science of Interstellar\n      0.000 It stars Matthew McConaughey, Anne Hathaway, Jessica Chastain, \nBill Irwin, Ellen Burstyn, Matt Damon, and Michael Caine\nNote that the first result does not really answer the question despite it sharing the\nword “science” with the query. In the next section, we’ll see how adding a reranker\ncan improve this search system. But before that, let’s complete our overview of dense\nretrieval by looking at its caveats and go over some methods of breaking down texts\ninto chunks.\nCaveats of dense retrieval\nIt’s useful to be aware of some of the drawbacks of dense retrieval and how to address\nthem. What happens, for example, if the texts don’t contain the answer? We still get\nresults and their distances. For example:\nQuery:'What is the mass of the moon?'\nNearest neighbors:\n texts distance\n0 The film  had a worldwide gross over $677 million (and $773 million with subsequent re-releases),\nmaking it the tenth-highest grossing film  of 20141.298275\n1 It has also received praise from many astronomers for its scientific  accuracy and portrayal of theoretical\nastrophysics1.324389\n2 Cinematographer Hoyte van Hoytema shot it on 35 mm movie film  in the Panavision anamorphic\nformat and IMAX 70 mm1.328375\nIn cases like this, one possible heuristic is to set a threshold level—a maximum\ndistance for relevance, for example. A lot of search systems present the user with\nthe best info they can get and leave it up to the user to decide if it’s relevant or not.\n234 | Chapter 8: Semantic Search and Retrieval-Augmented Generation\nTracking the information of whether the user clicked on a result (and were satisfied\nby it) can improve future versions of the search system.\nAnother caveat of dense retrieval is when a user wants to find an exact match for a\nspecific phrase. That’s a case that’s perfect for keyword matching. That’s one reason\nwhy hybrid search, which includes both semantic search and keyword search, is\nadvised instead of relying solely on dense retrieval.\nDense retrieval systems also find it challenging to work properly in domains other\nthan the ones that they were trained on. So, for example, if you train a retrieval model\non internet and Wikipedia data, and then deploy it on legal texts (without having\nenough legal data as part of the training set), the model will not work as well in that\nlegal domain.\nThe final thing we’ d like to point out is that this is a case where each sentence\ncontained a piece of information, and we showed queries that specifically ask for\nthat information. What about questions whose answers span multiple sentences? This\nhighlights one of the important design parameters of dense retrieval systems: what is\nthe best way to chunk long texts? And why do we need to chunk them in the first\nplace?\nChunking long texts\nOne  limitation of Transformer language models is that they are limited in context\nsizes, meaning we cannot feed them very long texts that go above the number of\nwords or tokens that the model supports. So how do we embed long texts?\nThere are several possible ways, and two possible approaches shown in Figure 8-7\ninclude indexing one vector per document and indexing multiple vectors per\ndocument.\nFigure 8-7. It’s possible to create one vector representing an entire document, but it’s bet‐\nter for longer documents to be split into smaller chunks that get their own embeddings.\nSemantic Search with Language Models | 235\nOne vector per document.    In this approach, we use a single vector to represent the\nwhole document. The possibilities here include:\n•Embedding only a representative part of the document and ignoring the rest of•\nthe text. This may mean embedding only the title, or only the beginning of the\ndocument. This is useful to get quickly started with building a demo but it leaves\na lot of information unindexed and therefore unsearchable. As an approach, it\nmay work better for documents where the beginning captures the main points of\na document (think: Wikipedia article). But it’s really not the best approach for a\nreal system because a lot of information would be left out of the index and would\nbe unsearchable.\n•Embedding the document in chunks, embedding those chunks, and then aggre‐•\ngating those chunks into a single vector. The usual method of aggregation here is\nto average those vectors. A downside of this approach is that it results in a highly\ncompressed vector that loses a lot of the information in the document.\nThis approach can satisfy some information needs, but not others. A lot of the time,\na search is for a specific piece of information contained in an article, which is better\ncaptured if the concept had its own vector.\nMultiple vectors per document.    In this approach, we chunk the document into smaller\npieces, and embed those chunks. Our search index then becomes that of chunk\nembeddings, not entire document embeddings. Figure 8-8  shows a number of possi‐\nble text chunking approaches.\nFigure 8-8. Several chunking methods and their effects  on the input text. Overlapping\nchunks can be important to prevent the absence of context.\n236 | Chapter 8: Semantic Search and Retrieval-Augmented Generation\nThe chunking approach is better because it has full coverage of the text and because\nthe vectors tend to capture individual concepts inside the text. This leads to a more\nexpressive search index. Figure 8-9  shows a number of possible approaches.\nFigure 8-9. A number of possible options for chunking a document for embedding.\nThe best way of chunking a long text will depend on the types of texts and queries\nyour system anticipates. Approaches include:\n•Each sentence is a chunk. The issue here is this could be too granular and the•\nvectors don’t capture enough of the context.\n•Each paragraph is a chunk. This is great if the text is made up of short para‐•\ngraphs. Otherwise, it may be that every 3–8 sentences is a chunk.\n•Some chunks derive a lot of their meaning from the text around them. So we can•\nincorporate some context via:\n—Adding the title of the document to the chunk.—\n—Adding some of the text before and after them to the chunk. This way, the—\nchunks can overlap so they include some surrounding text that also appears in\nadjacent chunks. This is what we can see in Figure 8-10 .\nExpect more chunking strategies to arise as the field develops—some of which may\neven use LLMs to dynamically split a text into meaningful chunks.\nSemantic Search with Language Models | 237\nFigure 8-10. Chunking the text into overlapping segments is one strategy to retain more\nof the context around different  segments.\nNearest neighbor search versus vector databases\nOnce  the query is embedded, we need to find the nearest vectors to it from our\ntext archive as we can see in Figure 8-11 . The most straightforward way to find the\nnearest neighbors is to calculate the distances between the query and the archive.\nThat can easily be done with NumPy and is a reasonable approach if you have\nthousands or tens of thousands of vectors in your archive.\nFigure 8-11. As we saw in Chapter 3 , we can compare embeddings to quickly find the\nmost similar documents to a query.\n238 | Chapter 8: Semantic Search and Retrieval-Augmented Generation\nAs you scale beyond to the millions of vectors, an optimized approach for retrieval is\nto rely on approximate nearest neighbor search libraries like Annoy or FAISS. These\nallow you to retrieve results from massive indexes in milliseconds and some of them\ncan improve their performance by utilizing GPUs and scaling to clusters of machines\nto serve very large indices.\nAnother class of vector retrieval systems are vector databases like Weaviate or Pine‐\ncone . A vector database allows you to add or delete vectors without having to rebuild\nthe index. They also provide ways to filter your search or customize it in ways beyond\nmerely vector distances.\nFine-tuning embedding models for dense retrieval\nJust as we discussed in Chapter 4  on text classification, we can improve the perfor‐\nmance of an LLM on a task using fine-tuning. As in that case, retrieval needs to\noptimize text embeddings and not simply token embeddings. The process for this\nfine-tuning is to get training data composed of queries and relevant results.\nLet’s look at one example from our dataset, the sentence “Interstellar premiered on\nOctober 26, 2014, in Los Angeles. ” Two possible queries where this is a relevant result\nare:\n•Relevant query 1: “Interstellar release date”•\n•Relevant query 2: “When did Interstellar premier”•\nThe fine-tuning process aims to make the embeddings of these queries close to the\nembedding of the resulting sentence. It also needs to see negative examples of queries\nthat are not relevant to the sentence, for example:\n•Irrelevant query: “Interstellar cast”•\nWith these examples, we now have three pairs—two positive pairs and one negative\npair. Let’s assume, as we can see in Figure 8-12 , that before fine-tuning, all three\nqueries have the same distance from the result document. That’s not far-fetched\nbecause they all talk about Interstellar .\nSemantic Search with Language Models | 239",19543
91-Reranking.pdf,91-Reranking,"Figure 8-12. Before fine-tuning,  the embeddings of both relevant and irrelevant queries\nmay be close to a particular document.\nThe fine-tuning step works to make the relevant queries closer to the document and\nat the same time make irrelevant queries farther from the document. We can see this\neffect in Figure 8-13 .\nFigure 8-13. After  the fine-tuning  process, the text embedding model becomes better\nat this search task by incorporating how we define  relevance on our dataset using the\nexamples we provided of relevant and irrelevant documents.\nReranking\nA lot of organizations have already built search systems. For those organizations,\nan easier way to incorporate language models is as a final step inside their search\n240 | Chapter 8: Semantic Search and Retrieval-Augmented Generation\npipeline. This step is tasked with changing the order of the search results based on\nrelevance to the search query. This one step can vastly improve search results and\nit’s in fact what Microsoft Bing added to achieve the improvements to search results\nusing BERT-like models. Figure 8-14  shows the structure of a rerank search system\nserving as the second stage in a two-stage search system.\nFigure 8-14. LLM rerankers operate as part of a search pipeline with the goal of\nreordering a number of shortlisted search results by relevance.\nReranking example\nA reranker takes in the search query and a number of search results, and returns\nthe optimal ordering of these documents so the most relevant ones to the query are\nhigher in ranking. Cohere’s Rerank endpoint  is a simple way to start using a first\nreranker. We simply pass it the query and texts and get the results back. We don’t\nneed to train or tune it:\nquery = ""how precise was the science""\nresults = co.rerank(query=query, documents =texts, top_n=3, return_docu\nments=True)\nresults.results\nWe can print these results:\nfor idx, result in enumerate (results.results):\n  print(idx, result.relevance_score  , result.document .text)\nOutput:\n0 0.1698185 It has also received praise from many astronomers for its scien-\ntific accuracy and portrayal of theoretical astrophysics\n1 0.07004896 The film had a worldwide gross over $677 million (and $773 mil-\nlion with subsequent re-releases), making it the tenth-highest grossing film \nof 2014\n2 0.0043994132 Caltech theoretical physicist and 2017 Nobel laureate in Phys-\nics[4] Kip Thorne was an executive producer, acted as a scientific consultant, \nand wrote a tie-in book, The Science of Interstellar\nSemantic Search with Language Models | 241\nThis shows the reranker is much more confident about the first result, assigning it a\nrelevance score of 0.16, while the other results are scored much lower in relevance.\nIn this basic example, we passed our reranker all 15 of our documents. More often,\nhowever, our index would have thousands or millions of entries, and we need to\nshortlist, say one hundred or one thousand results and then present those to the\nreranker. This shortlisting step is called the first stage  of the search pipeline.\nThe first-stage retriever can be keyword search, dense retrieval, or better yet—hybrid\nsearch that uses both of them. We can revisit our previous example to see how adding\na reranker after a keyword search system improves its performance.\nLet’s tweak our keyword search function so it retrieves a list of the top 10 results using\nkeyword search, then use rerank to choose the top 3 results from those 10:\ndef keyword_and_reranking_search (query, top_k=3, num_candidates =10):\n    print(""Input question:"" , query)\n    ##### BM25 search (lexical search) #####\n    bm25_scores  = bm25.get_scores (bm25_tokenizer (query))\n    top_n = np.argpartition (bm25_scores , -num_candidates )[-num_candidates :]\n    bm25_hits  = [{'corpus_id' : idx, 'score': bm25_scores [idx]} for idx in top_n]\n    bm25_hits  = sorted(bm25_hits , key=lambda x: x['score'], reverse=True)\n    \n    print(f""Top-3 lexical search (BM25) hits"" )\n    for hit in bm25_hits [0:top_k]:\n        print(""\t{:.3f}\t{}"".format(hit['score'], texts[hit['cor\npus_id']].replace(""\n"", "" "")))\n   \n    #Add re-ranking\n    docs = [texts[hit['corpus_id' ]] for hit in bm25_hits ]\n    \n    print(f""\nTop-3 hits by rank-API ( {len(bm25_hits )} BM25 hits re-ranked)"" )\n    results = co.rerank(query=query, documents =docs, top_n=top_k, return_docu\nments=True)\n    # print(results.results)\n    for hit in results.results:\n        # print(hit)\n        print(""\t{:.3f}\t{}"".format(hit.relevance_score , hit.docu\nment.text.replace(""\n"", "" "")))\nNow we can send our query and check the results of keyword search and then the\nresult of keyword search shortlisting its top 10 results, then pass them on to the\nreranker:\nkeyword_and_reranking_search (query = ""how precise was the science"" )\n242 | Chapter 8: Semantic Search and Retrieval-Augmented Generation\nResults:\nInput question: how precise was the science\nTop-3 lexical search (BM25) hits\n1.789 Interstellar is a 2014 epic science fiction film co-written, directed, \nand produced by Christopher Nolan\n1.373 Caltech theoretical physicist and 2017 Nobel laureate in Physics[4] Kip \nThorne was an executive producer, acted as a scientific consultant, and wrote \na tie-in book, The Science of Interstellar\n0.000 Interstellar uses extensive practical and miniature effects and the com-\npany Double Negative created additional digital effects\nTop-3 hits by rank-API (10 BM25 hits re-ranked)\n0.004 Caltech theoretical physicist and 2017 Nobel laureate in Physics[4] Kip \nThorne was an executive producer, acted as a scientific consultant, and wrote \na tie-in book, The Science of Interstellar\n0.004 Set in a dystopian future where humanity is struggling to survive, the \nfilm follows a group of astronauts who travel through a wormhole near Saturn \nin search of a new home for mankind\n0.003 Brothers Christopher and Jonathan Nolan wrote the screenplay, which had \nits origins in a script Jonathan developed in 2007\nWe see that keyword search assigns scores to only two results that share some of\nthe keywords. In the second set of results, the reranker elevates the second result\nappropriately as the most relevant result for the query. This is a toy example that\ngives us a glimpse of the effect, but in practice, such a pipeline significantly improves\nsearch quality. On a multilingual benchmark like MIRACL, a reranker can boost\nperformance from 36.5 to 62.8, measured as nDCG@10 (more on evaluation later in\nthis chapter).\nOpen source retrieval and reranking with sentence transformers\nIf you want to locally set up retrieval and reranking on your own machine, then\nyou can use the Sentence Transformers library. Refer to the documentation at https://\noreil.ly/jJOhV  for setup. Check the “Retrieve & Re-Rank” section  for instructions and\ncode examples for how to conduct these steps in the library.\nSemantic Search with Language Models | 243",6987
92-Retrieval Evaluation Metrics.pdf,92-Retrieval Evaluation Metrics,"How reranking models work\nOne popular way of building LLM search rerankers is to present the query and\neach result to an LLM working as a cross-encoder . This means that a query and\npossible result are presented to the model at the same time allowing the model to\nview both these texts before it assigns a relevance score, as we can see in Figure 8-15 .\nAll of the documents are processed simultaneously as a batch yet each document is\nevaluated against the query independently. The scores then determine the new order\nof the results. This method is described in more detail in a paper titled “Multi-stage\ndocument ranking with BERT”  and is sometimes referred to as monoBERT.\nFigure 8-15. A reranker assigns a relevance score to each document by looking at the\ndocument and the query at the same time.\nThis formulation of search as relevance scoring basically boils down to being a\nclassification problem. Given those inputs, the model outputs a score from 0–1 where\n0 is irrelevant and 1 is highly relevant. This should be familiar from our classification\ndiscussions in Chapter 4 .\nTo learn more about the development of using LLMs for search, ""Pretrained trans‐\nformers for text tanking: BERT and beyond "" is a highly recommended look at the\ndevelopments of these models until about 2021.\nRetrieval Evaluation Metrics\nSemantic  search is evaluated using metrics from the Information Retrieval (IR) field.\nLet’s discuss one of these popular metrics: mean average precision (MAP).\nEvaluating search systems needs three major components : a text archive, a set of\nqueries, and relevance judgments indicating which documents are relevant for each\nquery. We see these components in Figure 8-16 .\n244 | Chapter 8: Semantic Search and Retrieval-Augmented Generation\nFigure 8-16. To evaluate search systems, we need a test suite including queries and\nrelevance judgments indicating which documents in our archive are relevant for each\nquery.\nUsing this test suite, we can proceed to explore evaluating search systems. Let’s start\nwith a simple example. Let’s assume we pass query 1 to two different search systems.\nAnd get two sets of results. Say we limit the number of results to three, as we can see\nin Figure 8-17 .\nFigure 8-17. To compare two search systems, we pass the same query from our test suite\nto both systems and look at their top results.\nTo tell which is a better system, we turn to the relevance judgments that we have for\nthe query. Figure 8-18  shows which of the returned results are relevant.\nSemantic Search with Language Models | 245\nFigure 8-18. Looking at the relevance judgments from our test suite, we can see that\nsystem 1 did a better job than system 2.\nThis shows us a clear case where system 1 is better than system 2. Intuitively, we may\njust count how many relevant results each system retrieved. System 1 got two out of\nthree correct, and system 2 got only one out of three correct. But what about a case\nlike Figure 8-19  where both systems only get one relevant result out of three, but\nthey’re in different positions?\nFigure 8-19. We need a scoring system that rewards system 1 for assigning a high\nposition to a relevant result—even though both systems retrieved only one relevant result\nin their top three results.\nIn this case, we can intuit that system 1 did a better job than system 2 because the\nresult in the first position (the most important position) is correct. But how can we\nassign a number or score to how much better that result is? Mean average precision is\na measure that is able to quantify this distinction.\n246 | Chapter 8: Semantic Search and Retrieval-Augmented Generation\nOne common way to assign numeric scores in this scenario is average precision,\nwhich evaluates system 1’s result for the query to be 1 and system 2’s to be 0.3. So let’s\nsee how average precision is calculated to evaluate one set of results, and then how it’s\naggregated to evaluate a system across all the queries in the test suite.\nScoring a single query with average precision\nTo score a search system on this query, we can focus on scoring the relevant docu‐\nments. Let’s start by looking at a query that only has one relevant document in the test\nsuite.\nThe first one is easy: the search system placed the relevant result (the only available\none for this query) at the top. This gets the system the perfect score of 1. Figure 8-20\nshows this calculation: looking at the first position, we have a relevant result leading\nto a precision at position 1 of 1.0 (calculated as the number of relevant results at\nposition 1, divided by the position we’re currently looking at).\nFigure 8-20. To calculate mean average precision, we start by calculating precision at\neach position, starting with position 1.\nSince we’re only scoring relevant documents we can ignore the scores of nonrelevant\ndocuments and stop our calculation here. What if the system actually placed the\nonly relevant result at the third position, however? How would that affect the score?\nFigure 8-21  shows how that results in a penalty.\nSemantic Search with Language Models | 247\nFigure 8-21. If the system places nonrelevant documents ahead of a relevant document,\nits precision score is penalized.\nLet’s now look at a query with more than one relevant document. Figure 8-22  shows\nthat calculation and how averaging now comes into the picture.\nFigure 8-22. Average precision of a document with multiple relevant documents consid‐\ners the precision at k results of all the relevant documents.\nScoring across multiple queries with mean average precision\nNow that we’re familiar with precision at k and average precision, we can extend this\nknowledge to a metric that can score a search system against all the queries in our\ntest suite. That metric is called mean average precision. Figure 8-23  shows how to\ncalculate this metric by taking the mean of the average precisions of each query.\n248 | Chapter 8: Semantic Search and Retrieval-Augmented Generation",6034
93-From Search to RAG.pdf,93-From Search to RAG,"1Patrick Lewis et al. “Retrieval-augmented generation for knowledge-intensive NLP tasks. ” Advances in Neural\nInformation Processing Systems  33 (2020): 9459–9474.\nFigure 8-23. The mean average precision takes into consideration the average precision\nscore of a system for every query in the test suite. By averaging them, it produces a single\nmetric that we can use to compare a search system against another.\nY ou may be wondering why the same operation is called “mean” and “average. ” It’s\nlikely an aesthetic choice because MAP sounds better than average average precision.\nNow we have a single metric that we can use to compare different systems. If you\nwant to learn more about evaluation metrics, see the “Evaluation in Information\nRetrieval” chapter  of Introduction to Information Retrieval  (Cambridge University\nPress) by Christopher D. Manning, Prabhakar Raghavan, and Hinrich Schütze.\nIn addition to mean average precision, another metric commonly used for search\nsystems is normalized discounted cumulative gain (nDCG), which is more nuanced\nin that the relevance of documents is not binary (relevant versus not relevant) and\none document can be labeled as more relevant than another in the test suite and\nscoring mechanism.\nRetrieval-Augmented Generation (RAG)\nThe mass adoption of LLMs quickly led to people asking them questions and\nexpecting factual answers. While the models can answer some questions correctly,\nthey also confidently answer lots of questions incorrectly. The leading method the\nindustry turned to remedy this behavior is RAG, described in the paper “Retrieval-\nAugmented Generation for Knowledge-Intensive NLP Tasks”  (2020)1 and illustrated\nin Figure 8-24 .\nRetrieval-Augmented Generation (RAG) | 249\nFigure 8-24. A basic RAG pipeline is made up of a search step followed by a grounded\ngeneration step where the LLM is prompted with the question and the information\nretrieved from the search step.\nRAG systems incorporate search capabilities in addition to generation capabilities.\nThey can be seen as an improvement to generation systems because they reduce\ntheir hallucinations and improve their factuality. They also enable use cases of “chat\nwith my data” that consumers and companies can use to ground an LLM on internal\ncompany data, or a specific data source of interest (e.g., chatting with a book).\nThis also extends to search systems. More search engines are incorporating an LLM\nto summarize results or answer questions submitted to the search engine. Examples\ninclude Perplexity , Microsoft Bing AI , and Google Gemini .\nFrom Search to RAG\nLet’s now turn our search system into a RAG system. We do that by adding an LLM\nto the end of the search pipeline. We present the question and the top retrieved\ndocuments to the LLM, and ask it to answer the question given the context provided\nby the search results. We can see an example in Figure 8-25 .\nThis generation step is called grounded generation  because the retrieved relevant\ninformation we provide the LLM establishes a certain context that grounds the LLM\nin the domain we’re interested in. Figure 8-26  shows how grounded generation fits\nafter search if we continue our embeddings search example from earlier.\n250 | Chapter 8: Semantic Search and Retrieval-Augmented Generation\nFigure 8-25. Generative search formulates answers and summaries at the end of a search\npipeline while citing its sources (returned by the previous steps in the search system).\nFigure 8-26. Find the most relevant information to an input prompt by comparing the\nsimilarities between embeddings. The most relevant information is added to the prompt\nbefore giving it to the LLM.\nRetrieval-Augmented Generation (RAG) | 251",3751
94-Example Grounded Generation with an LLM API.pdf,94-Example Grounded Generation with an LLM API,,0
95-Example RAG with Local Models.pdf,95-Example RAG with Local Models,"Example: Grounded Generation with an LLM API\nLet’s look at how to add a grounded generation step after the search results to create\nour first RAG system. For this example, we’ll use Cohere’s managed LLM, which\nbuilds on the search systems we’ve seen earlier in the chapter. We’ll use embedding\nsearch to retrieve the top documents, then we’ll pass those to the co.chat  endpoint\nalong with the questions to provide a grounded answer:\nquery = ""income generated""\n# 1- Retrieval\n# We'll use embedding search. But ideally we'd do hybrid\nresults = search(query)\n# 2- Grounded Generation\ndocs_dict  = [{'text': text} for text in results['texts']]\nresponse  = co.chat(\n    message = query,\n    documents =docs_dict\n)\nprint(response .text)\nResult:\nThe film generated a worldwide gross of over $677 million, or $773 million \nwith subsequent re-releases.\nWe are highlighting some of the text because the model indicated the source for these\nspans of text to be the first document we passed in:\ncitations=[ChatCitation(start=21, end=36, text='worldwide gross', docu-\nment_ids=['doc_0']), ChatCitation(start=40, end=57, text='over $677 million', \ndocument_ids=['doc_0']), ChatCitation(start=62, end=103, text='$773 million \nwith subsequent re-releases.', document_ids=['doc_0'])]\ndocuments=[{'id': 'doc_0', 'text': 'The film had a worldwide gross over $677 \nmillion (and $773 million with subsequent re-releases), making it the tenth-\nhighest grossing film of 2014'}]\nExample: RAG with Local Models\nLet us now replicate this basic functionality with local models. We will lose the\nability to do span citations and the smaller local model isn’t going to work as well\nas the larger managed model, but it’s useful to demonstrate the flow. We’ll start by\ndownloading a quantized model.\n252 | Chapter 8: Semantic Search and Retrieval-Augmented Generation\nLoading the generation model\nWe start by downloading our model:\n!wget https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/\nPhi-3-mini-4k-instruct-fp16.gguf\nUsing llama.cpp , llama-cpp-python , and LangChain, we load the text generation\nmodel:\nfrom langchain  import LlamaCpp\n# Make sure the model path is correct for your system!\nllm = LlamaCpp (\n    model_path =""Phi-3-mini-4k-instruct-fp16.gguf"" ,\n    n_gpu_layers =-1,\n    max_tokens =500,\n    n_ctx=2048,\n    seed=42,\n    verbose=False\n)\nLoading the embedding model\nLet’s now load an embedding language model. In this example, we will choose the\nBAAI/bge-small-en-v1.5 model . At the time of writing, it is high on the MTEB\nleaderboard  for embedding models and relatively small:\nfrom langchain.embeddings.huggingface  import HuggingFaceEmbeddings\n# Embedding model for converting text to numerical representations\nembedding_model  = HuggingFaceEmbeddings (\n    model_name ='thenlper/gte-small'\n)\nWe can now use the embedding model to set up our vector database:\nfrom langchain.vectorstores  import FAISS\n# Create a local vector database\ndb = FAISS.from_texts (texts, embedding_model )\nThe RAG prompt\nA prompt template plays a vital part in the RAG pipeline. It is the central place where\nwe communicate the relevant documents to the LLM. To do so, we will create an\nadditional input variable named context  that can provide the LLM with the retrieved\ndocuments:\nRetrieval-Augmented Generation (RAG) | 253\nfrom langchain  import PromptTemplate\n# Create a prompt template\ntemplate  = """"""<|user|>\nRelevant information:\n{context}\nProvide a concise answer the following question using the relevant information \nprovided above:\n{question} <|end|>\n<|assistant|>""""""\nprompt = PromptTemplate (\n    template =template ,\n    input_variables =[""context"" , ""question"" ]\n)\nfrom langchain.chains  import RetrievalQA\n# RAG pipeline\nrag = RetrievalQA .from_chain_type (\n    llm=llm,\n    chain_type ='stuff',\n    retriever =db.as_retriever (),\n    chain_type_kwargs ={\n        ""prompt"" : prompt\n    },\n    verbose=True\n)\nNow we’re ready to call the model and ask it a question:\nrag.invoke('Income generated' )\nResult:\nThe Income generated by the film in 2014 was over $677 million worldwide. \nThis made it the tenth-highest grossing film of that year. It should be noted, \nhowever, this figure includes both initial ticket sales as well as any subse-\nquent re-releases. With these additional releases, total earnings surged to \napproximately $773 million. The release format transitioned from traditional \nfilm stock projection in theaters to digital projectors once it was expanded \nto various venues in the United States. This shift might have contributed \nto wider audience reach and potentially higher grossing figures over time. \nHowever, specific data on how this affected total earnings isn't provided in \nthe information above.\nAs always, we can adjust the prompt to control the model’s generation (e.g., answer\nlength and tone).\n254 | Chapter 8: Semantic Search and Retrieval-Augmented Generation",5010
96-Advanced RAG Techniques.pdf,96-Advanced RAG Techniques,"Advanced RAG Techniques\nThere are several additional techniques to improve the performance of RAG systems.\nSome of them are laid out here.\nQuery rewriting\nIf the RAG system is a chatbot, the preceding simple RAG implementation would\nlikely struggle with the search step if a question is too verbose, or to refer to context\nin previous messages in the conversation. This is why it’s a good idea to use an\nLLM to rewrite the query into one that aids the retrieval step in getting the right\ninformation. An example of this is a message such as:\nUser Question: “We have an essay due tomorrow. We have to write about some animal.\nI love penguins. I could write about them. But I could also write about dolphins. Are\nthey animals? Maybe. Let’s do dolphins. Where do they live for example?”\nThis should actually be rewritten into a query like:\nQuery: “Where do dolphins live”\nThis rewriting behavior can be done through a prompt (or through an API call).\nCohere’s API, for example, has a dedicated query-rewriting mode for co.chat .\nMulti-query RAG\nThe next improvement we can introduce is to extend the query rewriting to be able to\nsearch multiple queries if more than one is needed to answer a specific question. Take\nfor example:\nUser Question: “Compare the financial results of Nvidia in 2020 vs. 2023”\nWe may find one document that contains the results for both years, but more likely,\nwe’re better off making two search queries:\nQuery 1: “Nvidia 2020 financial results”\nQuery 2: “Nvidia 2023 financial results”\nWe then present the top results of both queries to the model for grounded generation.\nAn additional small improvement here is to also give the query rewriter the option to\ndetermine if no search is required and if it can directly generate a confident answer\nwithout searching.\nRetrieval-Augmented Generation (RAG) | 255\nMulti-hop RAG\nA more advanced question may require a series of sequential queries. Take for\nexample a question like:\nUser Question: “Who are the largest car manufacturers in 2023? Do they each make\nEVs or not?”\nTo answer this, the system must first search for:\nStep 1, Query 1: “largest car manufacturers 2023”\nThen after it gets this information (the result being Toyota, Volkswagen, and Hyun‐\ndai), it should ask follow-up questions:\nStep 2, Query 1: “Toyota Motor Corporation electric vehicles”\nStep 2, Query 2: “Volkswagen AG electric vehicles”\nStep 2, Query 3: “Hyundai Motor Company electric vehicles”\nQuery routing\nAn additional enhancement is to give the model the ability to search multiple data\nsources. We can, for example, specify for the model that if it gets a question about\nHR, it should search the company’s HR information system (e.g., Notion) but if\nthe question is about customer data, that it should search the customer relationship\nmanagement (CRM) (e.g., Salesforce).\nAgentic RAG\nY ou may be able to now see that the list of previous enhancements slowly delegates\nmore and more responsibility to the LLM to solve more and more complex problems.\nThis relies on the LLM’s capability to gauge the required information needs as well\nas its ability to utilize multiple data sources. This new nature of the LLM starts to\nbecome closer and closer to an agent that acts on the world. The data sources can also\nnow be abstracted into tools. We saw, for example, that we can search Notion, but by\nthe same token, we should be able to post to Notion as well.\nNot all LLMs will have the RAG capabilities mentioned here. At the time of writing,\nlikely only the largest managed models may be able to attempt this behavior. Thank‐\nfully, Cohere’s Command R+  excels at these tasks and is available as an open-weights\nmodel  as well.\n256 | Chapter 8: Semantic Search and Retrieval-Augmented Generation",3799
97-Transformers for Vision.pdf,97-Transformers for Vision,"2Nelson F. Liu, Tianyi Zhang, and Percy Liang. “Evaluating verifiability in generative search engines. ” arXiv\npreprint arXiv:2304.09848  (2023).RAG Evaluation\nThere  are still ongoing developments in how to evaluate RAG models. A good paper\nto read on this topic is “Evaluating verifiability in generative search engines”  (2023),\nwhich runs human evaluations on different generative search systems.2\nIt evaluates results along four axes:\nFluency\nWhether the generated text is fluent and cohesive.\nPerceived utility\nWhether the generated answer is helpful and informative.\nCitation recall\nThe proportion of generated statements about the external world that are fully\nsupported by their citations.\nCitation precision\nThe proportion of generated citations that support their associated statements.\nWhile human evaluation is always preferred, there are approaches that attempt to\nautomate these evaluations by having a capable LLM act as a judge (called LLM-as-a-\njudge ) and score the different generations along the different axes. Ragas  is a software\nlibrary that does exactly this. It also scores some additional useful metrics like:\nFaithfulness\nWhether the answer is consistent with the provided context\nAnswer relevance\nHow relevant the answer is to the question\nThe Ragas documentation site  provides more details about the formulas to actually\ncalculate these metrics.\nRetrieval-Augmented Generation (RAG) | 257\nSummary\nIn this chapter, we looked at different ways of using language models to improve\nexisting search systems and even be the core of new, more powerful search systems.\nThese include:\n•Dense retrieval, which relies on the similarity of text embeddings. These are•\nsystems that embed a search query and retrieve the documents with the nearest\nembeddings to the query’s embedding.\n•Rerankers, systems (like monoBERT) that look at a query and candidate results•\nand score the relevance of each document to that query. These relevance scores\nare then used to order the shortlisted results according to their relevance to the\nquery, often producing an improved results ranking.\n•RAG, where search systems have a generative LLM at the end of the pipeline to•\nformulate an answer based on retrieved documents while citing sources.\nWe also looked at one of the possible methods of evaluating search systems. Mean\naverage precision allows us to score search systems to be able to compare across a\ntest suite of queries and their known relevance to the test queries. Evaluating RAG\nsystems requires multiple axes, however, like faithfulness, fluency, and others that can\nbe evaluated by humans or by LLM-as-a-judge.\nIn the next chapter, we will explore how language models can be made multimodal\nand reason not just about text but images as well.\n258 | Chapter 8: Semantic Search and Retrieval-Augmented Generation\nCHAPTER 9\nMultimodal Large Language Models\nWhen  you think about large language models (LLMs), multimodality might not be\nthe first thing that comes to mind. After all, they are language  models! But we can\nquickly see that models can be much more useful if they’re able to handle types of\ndata other than text. It’s very useful, for example, if a language model is able to glance\nat a picture and answer questions about it. A model that is able to handle text and\nimages (each of which is called a modality ) is said to be multimodal,  as we can see in\nFigure 9-1 .\nFigure 9-1. Models that are able to deal with different  types (or modalities) of data, such\nas images, audio, video, or sensors, are said to be multimodal. It’s possible for a model to\naccept a modality as input yet not be able to generate in that modality.\n259\n1Jason Wei et al. “Emergent abilities of large language models. ” arXiv preprint arXiv:2206.07682  (2022).\n2Alexey Dosovitskiy et al. “ An image is worth 16x16 words: Transformers for image recognition at scale. ” arXiv\npreprint arXiv:2010.11929  (2020).We have seen all manner of emerging behaviors rising from LLMs, from generaliza‐\ntion capabilities and reasoning to arithmetic and linguistics. As models grow larger\nand smarter, so do their skill sets.1\nThe ability to receive and reason with multimodal input might further increase and\nhelp emerge capabilities that were previously locked. In practice, language does not\nsolely live in a vacuum. As an example, your body language, facial expressions,\nintonation, etc. are all methods of communication that enhance the spoken word.\nThe same thing applies to LLMs; if we can enable them to reason about multimodal\ninformation, their capabilities might increase and we become able to deploy them to\nsolve new kinds of problems.\nIn this chapter, we will explore a number of different LLMs that have multimodal\ncapabilities and what that means for practical use cases. We will start by exploring\nhow images are converted to numerical representations using an adaptation of the\noriginal Transformer technique. Then, we will show how LLMs can be extended to\ninclude vision tasks using this Transformer.\nTransformers for Vision\nThroughout the chapters of this book, we have seen the success of using Transformer-\nbased models for a variety of language modeling tasks, from classification and\nclustering to search and generative modeling. So it might not be surprising that\nresearchers have been looking at a way to generalize some of the Transformer’s\nsuccess to the field of computer vision.\nThe method they came up with is called the Vision Transformer (ViT), which has\nbeen shown to do tremendously well on image recognition tasks compared to the\npreviously default convolutional neural networks (CNNs).2 Like the original Trans‐\nformer, ViT is used to transform unstructured data, an image, into representations\nthat can be used for a variety of tasks, like classification, as illustrated in Figure 9-2 .\nViT relies on an important component of the Transformer architecture, namely the\nencoder. As we saw in Chapter 1 , the encoder is responsible for converting textual\ninput into numerical representations before being passed to the decoder. However,\nbefore the encoder can perform its duties, the textual input needs to be tokenized\nfirst, as is illustrated in Figure 9-3 .\n260 | Chapter 9: Multimodal Large Language Models\nFigure 9-2. Both the original Transformer as well as the Vision Transformer take\nunstructured data, convert it to numerical representations, and finally  use that for tasks\nlike classification.\nFigure 9-3. Text is passed to one or multiple encoders by first tokenizing it using a\ntokenizer.\nSince an image does not consist of words this tokenization process cannot be used for\nvisual data. Instead, the authors of ViT came up with a method for tokenizing images\ninto “words, ” which allowed them to use the original encoder structure.\nImagine that you have an image of a cat. This image is represented by a number\nof pixels, let’s say 512 × 512 pixels. Each individual pixel does not convey much\ninformation but when you combine patches of pixels, you slowly start to see more\ninformation.\nTransformers for Vision | 261\nViT uses a principle much like that. Instead of splitting up text into tokens, it converts\nthe original image into patches of images. In other words, it cuts the image into a\nnumber of pieces horizontally and vertically as illustrated in Figure 9-4 .\nFigure 9-4. The “tokenization” process for image input. It converts an image into patches\nof subimages.\nJust like we are converting text into tokens of text, we are converting an image\ninto patches of images. The flattened input of image patches can be thought of as\nthe tokens in a piece of text. However, unlike tokens, we cannot just assign each\npatch with an ID since these patches will rarely be found in other images, unlike the\nvocabulary of a text.\nInstead, the patches are linearly embedded to create numerical representations,\nnamely embeddings. These can then be used as the input of a Transformer model.\nThat way, the patches of images are treated the same way as tokens. The full process is\nillustrated in Figure 9-5 .\nFor illustrative purposes, the images in the examples were patched into 3 × 3 patches\nbut the original implementation used 16 × 16 patches. After all, the paper is called\n“ An Image is Worth 16x16 Words. ”\nWhat is so interesting about this approach is that the moment the embeddings are\npassed to the encoder, they are treated as if they were textual tokens. From that point\nforward, there is no difference in how a text or image trains.\nDue to these similarities, the ViT is often used to make all kinds of language models\nmultimodal. One of the most straightforward ways to use it is during the training of\nembedding models.\n262 | Chapter 9: Multimodal Large Language Models",8845
98-Multimodal Embedding Models.pdf,98-Multimodal Embedding Models,"Figure 9-5. The main algorithm behind ViT. After  patching the images and linearly\nprojecting them, the patch embeddings are passed to the encoder and treated as if they\nwere textual tokens.\nMultimodal Embedding Models\nIn previous chapters, we used embedding models to capture the semantic content\nof textual representations, such as papers and documents. We saw that we could\nuse these embeddings or numerical representations to find similar documents, apply\nclassification tasks, and even perform topic modeling.\nAs we have seen many times before, embeddings often are an important driver\nbehind LLM applications. They are an efficient method for capturing large-scale\ninformation and searching for the needle in the haystack of information.\nThat said, we have looked at text-only embedding models thus far, which focus\non generating embeddings for textual representations. Although embedding models\nexist for solely embedding imagery, we will look at embedding models that can\ncapture both textual as well as visual representations. We illustrate this in Figure 9-6 .\nMultimodal Embedding Models | 263\nFigure 9-6. Multimodal embedding models can create embeddings for multiple modali‐\nties in the same vector space.\nAn advantage is that this allows for comparing multimodal representations since the\nresulting embeddings lie in the same vector space ( Figure 9-7 ). For instance, using\nsuch a multimodal embedding model, we can find images based on input text. What\nimages would we find if we search for images similar to “pictures of a puppy”? Vice\nversa would also be possible. Which documents are best related to this question?\nFigure 9-7. Despite having coming from different  modalities, embeddings with similar\nmeaning will be close to each other in vector space.\nThere are a number of multimodal embedding models, but the most well-known and\ncurrently most-used model is Contrastive Language-Image Pre-training (CLIP).\n264 | Chapter 9: Multimodal Large Language Models",2005
99-CLIP Connecting Text and Images.pdf,99-CLIP Connecting Text and Images,,0
100-How Can CLIP Generate Multimodal Embeddings.pdf,100-How Can CLIP Generate Multimodal Embeddings,"3Alec Radford et al. “Learning transferable visual models from natural language supervision. ” International\nConference on Machine Learning . PMLR, 2021.\n4Robin Rombach et al. “High-resolution image synthesis with latent diffusion models. ” Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition , 2022.CLIP: Connecting Text and Images\nCLIP is an embedding model that can compute embeddings of both images and texts.\nThe resulting embeddings lie in the same vector space, which means that the embed‐\ndings of images can be compared with the embeddings of text.3 This comparison\ncapability makes CLIP , and similar models, usable for tasks such as:\nZero-shot classification\nWe can compare the embedding of an image with that of the description of its\npossible classes to find which class is most similar.\nClustering\nCluster both images and a collection of keywords to find which keywords belong\nto which sets of images.\nSearch\nAcross billions of texts or images, we can quickly find what relates to an input\ntext or image.\nGeneration\nUse multimodal embeddings to drive the generation of images (e.g., stable\ndiffusion4).\nHow Can CLIP Generate Multimodal Embeddings?\nThe procedure of CLIP is actually quite straightforward. Imagine that you have a\ndataset with millions of images alongside captions as we illustrate in Figure 9-8 .\nFigure 9-8. The type of data that is needed to train a multimodal embedding model.\nMultimodal Embedding Models | 265\nThis dataset can be used to create two representations for each pair, the image and\nits caption. To do so, CLIP uses a text encoder to embed text and an image encoder\nto embed images. As is shown in Figure 9-9 , the result is an embedding for both the\nimage and its corresponding caption.\nFigure 9-9. In the first step of training CLIP , both images and text are embedded using\nan image and text encoder, respectively.\nThe pair of embeddings that are generated are compared through cosine similarity.\nAs we saw in Chapter 4 , cosine similarity is the cosine of the angle between vectors,\nwhich is calculated through the dot product of the embeddings and divided by the\nproduct of their lengths.\nWhen we start training, the similarity between the image embedding and text embed‐\nding will be low as they are not yet optimized to be within the same vector space.\nDuring training, we optimize for the similarity between the embeddings and want\nto maximize them for similar image/caption pairs and minimize them for dissimilar\nimage/caption pairs ( Figure 9-10 ).\nAfter calculating their similarity, the model is updated and the process starts again\nwith new batches of data and updated representations ( Figure 9-11 ). This method\nis called contrastive learning , and we will go in depth into its inner workings in\nChapter 10  where we will create our own embedding model.\n266 | Chapter 9: Multimodal Large Language Models\nFigure 9-10. In the second step of training CLIP , the similarity between the sentence and\nimage embedding is calculated using cosine similarity.\nFigure 9-11. In the third step of training CLIP , the text and image encoders are updated\nto match what the intended similarity should be. This updates the embeddings such that\nthey are closer in vector space if the inputs are similar.\nMultimodal Embedding Models | 267",3359
101-OpenCLIP.pdf,101-OpenCLIP,"Eventually, we expect the embedding of an image of a cat would be similar to the\nembedding of the phrase “a picture of a cat. ” As we will see in Chapter 10 , to make\nsure the representations are as accurate as possible, negative examples of images\nand captions that are not related should also be included in the training process.\nModeling similarity is not only knowing what makes things similar to one another,\nbut also what makes them different and dissimilar.\nOpenCLIP\nFor our next example, we are going to be using models from the open source variant\nof CLIP , namely OpenCLIP . Using OpenCLIP , or any CLIP model, boils down to\ntwo things: processing the textual and image inputs before passing them to the main\nmodel.\nBefore doing so, let’s take a look at a small example where we will be using one\nof the images we have seen before, namely, an AI-generated image (through stable\ndiffusion) of a puppy playing in the snow, as illustrated in Figure 9-12 :\nfrom urllib.request  import urlopen\nfrom PIL import Image\n# Load an AI-generated image of a puppy playing in the snow\npuppy_path  = ""https://raw.githubusercontent.com/HandsOnLLM/Hands-On-Large-\nLanguage-Models/main/chapter09/images/puppy.png""\nimage = Image.open(urlopen(puppy_path )).convert(""RGB"")\ncaption = ""a puppy playing in the snow""\nFigure 9-12. An AI-generated image of a puppy playing in the snow.\nSince we have a caption for this image, we can use OpenCLIP to generate embeddings\nfor both.\n268 | Chapter 9: Multimodal Large Language Models\nTo do so, we load in three models:\n•A tokenizer for tokenizing the textual input•\n•A preprocessor to preprocess and resize the image•\n•The main model that converts the previous outputs to embeddings•\nfrom transformers  import CLIPTokenizerFast , CLIPProcessor , CLIPModel\nmodel_id  = ""openai/clip-vit-base-patch32""\n# Load a tokenizer to preprocess the text\nclip_tokenizer  = CLIPTokenizerFast .from_pretrained (model_id )\n# Load a processor to preprocess the images\nclip_processor  = CLIPProcessor .from_pretrained (model_id )\n# Main model for generating text and image embeddings\nmodel = CLIPModel .from_pretrained (model_id )\nAfter having loaded in the models, preprocessing our input is straightforward. Let’s\nstart with the tokenizer and see what happens if we preprocess our input:\n# Tokenize our input\ninputs = clip_tokenizer (caption, return_tensors =""pt"")\ninputs\nThis outputs a dictionary that contains the IDs of the input:\n{'input_ids': tensor([[49406, 320, 6829, 1629, 530, 518, 2583, 49407]]), 'at-\ntention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1]])}\nTo see what those IDs represent, we can convert them to tokens using the aptly\nnamed convert_ids_to_tokens  function:\n# Convert our input back to tokens\nclip_tokenizer .convert_ids_to_tokens (inputs[""input_ids"" ][0])\nThis gives us the following output:\n['<|startoftext|>',\n 'a</w>',\n 'puppy</w>',\n 'playing</w>',\n 'in</w>',\n 'the</w>',\n 'snow</w>',\n '<|endoftext|>']\nMultimodal Embedding Models | 269\nAs we often have seen before, the text is split up into tokens. Additionally, we now\nalso see that the start and end of the text is indicated to separate it from a potential\nimage embedding. Y ou might also notice that the [CLS]  token is missing. In CLIP , the\n[CLS]  token is actually used to represent the image embedding.\nNow that we have preprocessed our caption, we can create the embedding:\n# Create a text embedding\ntext_embedding  = model.get_text_features (**inputs)\ntext_embedding .shape\nThis results in an embedding that has 512 values for this single string:\n    torch.Size([1, 512])\nBefore we can create our image embedding, like the text embedding, we will need to\npreprocess it as the model expects the input image to have certain characteristics, like\nits size and shape.\nTo do so, we can use the processor  that we created before:\n# Preprocess image\nprocessed_image  = clip_processor (\n    text=None, images=image, return_tensors =""pt""\n)[""pixel_values"" ]\nprocessed_image .shape\nThe original image was 512 × 512 pixels. Notice that the preprocessing of this image\nreduced its size to 224 × 224 pixels as that is its expected size:\n    torch.Size([1, 3, 224, 224])\nLet’s visualize the results of this preprocessing as shown in Figure 9-13 :\nimport torch\nimport numpy as np\nimport matplotlib.pyplot  as plt\n# Prepare image for visualization\nimg = processed_image .squeeze(0)\nimg = img.permute(*torch.arange(img.ndim - 1, -1, -1))\nimg = np.einsum(""ijk->jik"" , img)\n# Visualize preprocessed image\nplt.imshow(img)\nplt.axis(""off"")\n270 | Chapter 9: Multimodal Large Language Models\nFigure 9-13. The preprocessed input image by CLIP .\nTo convert this preprocessed image into embeddings, we can call the model as we did\nbefore and explore what shape it returns:\n# Create the image embedding\nimage_embedding  = model.get_image_features (processed_image )\nimage_embedding .shape\nThis gives us the following shape:\n    torch.Size([1, 512])\nNotice that the shape of the resulting image embedding is the same as that of the text\nembedding. This is important as it allows us to compare their embeddings and see if\nthey are similar.\nWe can use these embeddings to calculate how similar they are. To do so, we normal‐\nize the embeddings first before calculating the dot product to give us a similarity\nscore:\n# Normalize the embeddings\ntext_embedding  /= text_embedding .norm(dim=-1, keepdim=True)\nimage_embedding  /= image_embedding .norm(dim=-1, keepdim=True)\n# Calculate their similarity\ntext_embedding  = text_embedding .detach().cpu().numpy()\nimage_embedding  = image_embedding .detach().cpu().numpy()\nscore = np.dot(text_embedding , image_embedding .T)\nscore\nThis gives us the following score:\narray([[0.33149648]], dtype=float32)\nMultimodal Embedding Models | 271\nWe get a similarity score of 0.33, which is difficult to interpret considering we don’t\nknow what the model considers a low versus a high similarity score. Instead, let’s\nextend the example with more images and captions as illustrated in Figure 9-14 .\nFigure 9-14. The similarity matrix between three images and three captions.\nIt seems that a score of 0.33 is indeed high considering the similarities with other\nimages are quite a bit lower.\nUsing sentence-transformers to Load CLIP\nsentence-transformers  implements  a few CLIP-based models that make it much\neasier to create embeddings. It only takes a few lines of code:\nfrom sentence_transformers  import SentenceTransformer , util\n# Load SBERT-compatible CLIP model\nmodel = SentenceTransformer (""clip-ViT-B-32"" )\n# Encode the images\nimage_embeddings  = model.encode(images)\n# Encode the captions\ntext_embeddings  = model.encode(captions )\n#Compute cosine similarities\nsim_matrix  = util.cos_sim(\n    image_embeddings , text_embeddings\n)\n272 | Chapter 9: Multimodal Large Language Models",6926
102-Making Text Generation Models Multimodal.pdf,102-Making Text Generation Models Multimodal,,0
103-BLIP-2 Bridging the Modality Gap.pdf,103-BLIP-2 Bridging the Modality Gap,"Making Text Generation Models Multimodal\nTraditionally, text generation models have been, as you might expect, models that\ninterpret textual representations. Models like Llama 2 and ChatGPT excel at reason‐\ning about textual information and responding with natural language.\nThey are, however, limited to the modality they were trained in, namely text. As\nwe have seen before with multimodal embedding models, the addition of vision can\nenhance the capabilities of a model.\nIn the case of text generation models, we would like it to reason about certain input\nimages. For example, we could give it an image of a pizza and ask it what ingredients\nit contains. Y ou could show it a picture of the Eiffel Tower and ask when it was built\nor where it is located. This conversational ability is further illustrated in Figure 9-15 .\nFigure 9-15. An example of a multimodal text generation model (BLIP-2) that can\nreason about input images.\nTo bridge the gap between these two domains, attempts have been made to introduce\na form of multimodality to existing models. One such method is called BLIP-2:\nBootstrapping Language-Image Pre-training for Unified  Vision-Language Understand‐\ning and Generation 2 . BLIP-2 is an easy-to-use and modular technique that allows for\nintroducing vision capabilities to existing language models.\nBLIP-2: Bridging the Modality Gap\nCreating a multimodal language model from scratch requires significant computing\npower and data. We would have to use billions of images, text, and image-text pairs to\ncreate such a model. As you can imagine, this is not easily feasible!\nMaking Text Generation Models Multimodal | 273\n5Junnan Li et al. “BLIP-2: Bootstrapping language-image pretraining with frozen image encoders and large\nlanguage models. ” International Conference on Machine Learning . PMLR, 2023.Instead of building the architecture from scratch, BLIP-2 bridges the vision-language\ngap by building a bridge, named the Querying Transformer (Q-Former), that con‐\nnects a pretrained image encoder and a pretrained LLM.5\nBy leveraging pretrained models, BLIP-2 only needs to train the bridge without\nneeding to train the image encoder and LLM from scratch. It makes great use of\nthe technology and models that are already out there! This bridge is illustrated in\nFigure 9-16 .\nFigure 9-16. The Querying Transformer is the bridge between vision (ViT) and text\n(LLM) that is the only trainable component of the pipeline.\nTo connect the two pretrained models, the Q-Former mimics their architectures. It\nhas two modules that share their attention layers:\n•An Image Transformer to interact with the frozen Vision Transformer for feature•\nextraction\n•A Text Transformer that can interact with the LLM•\nThe Q-Former is trained in two stages, one for each modality, as illustrated in\nFigure 9-17 .\nIn step 1, image-document pairs are used to train the Q-Former to represent both\nimages and text. These pairs are generally captions of images, as we have seen before\nwhen training CLIP .\nThe images are fed to the frozen ViT to extract vision embeddings. These embed‐\ndings are used as the input of Q-Former’s ViT. The captions are used as the input of\nQ-Former’s Text Transformer.\n274 | Chapter 9: Multimodal Large Language Models\nFigure 9-17. In step 1, representation learning is applied to learn representations for\nvision and language simultaneously. In step 2, these representations are converted to soft\nvisual prompts to feed the LLM.\nWith these inputs, the Q-Former is then trained on three tasks:\nImage-text contrastive learning\nThis task attempts to align pairs of image and text embeddings such that they\nmaximize their mutual information.\nImage-text matching\nA classification task to predict whether an image and text pair is positive\n(matched) or negative (unmatched).\nImage-grounded text generation\nTrains the model to generate text based on information extracted from the input\nimage.\nThese three objectives are jointly optimized to improve the visual representations that\nare extracted from the frozen ViT. In a way, we are trying to inject textual information\ninto the embeddings of the frozen ViT so that we can use them in the LLM. This first\nstep of BLIP-2 is illustrated in Figure 9-18 .\nMaking Text Generation Models Multimodal | 275\nFigure 9-18. In step 1, the output of the frozen ViT is used together with its caption and\ntrained on three contrastive-like tasks to learn visual-text representations.\nIn step 2, the learnable embeddings derived from step 1 now contain visual informa‐\ntion in the same dimensional space as the corresponding textual information. The\nlearnable embeddings are then passed to the LLM. In a way, these embeddings serve\nas soft visual prompts that condition the LLM on the visual representations that were\nextracted by the Q-Former.\nThere is also a fully connected linear layer in between them to make sure that the\nlearnable embeddings have the same shape as the LLM expects. This second step of\nconverting vision to language is represented in Figure 9-19 .\nFigure 9-19. In step 2, the learned embeddings from the Q-Former are passed to the\nLLM through a projection layer. The projected embeddings serve as a soft visual prompt.\n276 | Chapter 9: Multimodal Large Language Models\n6Haotian Liu et al. “Visual instruction tuning. ” Advances in Neural Information Processing Systems  36 (2024).\n7Hugo Laurençon et al. “What matters when building vision-language models?” arXiv preprint\narXiv:2405.02246  (2024).When we put these steps together, they make it possible for the Q-Former to learn\nvisual and textual representations in the same dimensional space, which can be used\nas a soft prompt to the LLM. As a result, the LLM will be given information about\nthe image in a similar manner to the context you would provide an LLM when\nprompting. The full in-depth process is illustrated in Figure 9-20 .\nFigure 9-20. The full BLIP-2 procedure.\nSince BLIP-2, many other visual LLMs have been released that have similar processes,\nlike LLaV A , a framework for making textual LLMs multimodal6 or Idefics 2 , an\nefficient visual LLM based on the Mistral 7B LLM .7 Both visual LLMs, although\nhaving different architectures, connect pretrained CLIP-like visual encoders with\ntextual LLMs. The goal of these architectures is to project visual features from the\ninput images to language embeddings such that they can be used as the input for an\nLLM. Similar to the Q-Former, they attempt to bridge the gap between images and\ntext.\nMaking Text Generation Models Multimodal | 277",6628
104-Preprocessing Multimodal Inputs.pdf,104-Preprocessing Multimodal Inputs,"Preprocessing Multimodal Inputs\nNow that we know how BLIP-2 is created, there are a number of interesting use cases\nfor such a model, not limited to captioning images, answering visual questions, and\neven performing prompting.\nBefore we go through some use cases, let’s first load the model and explore how you\ncan use it:\nfrom transformers  import AutoProcessor , Blip2ForConditionalGeneration\nimport torch\n# Load processor and main model\nblip_processor  = AutoProcessor .from_pretrained (""Salesforce/blip2-opt-2.7b"" )\nmodel = Blip2ForConditionalGeneration .from_pretrained (\n    ""Salesforce/blip2-opt-2.7b"" ,\n    torch_dtype =torch.float16\n)\n# Send the model to GPU to speed up inference\ndevice = ""cuda"" if torch.cuda.is_available () else ""cpu""\nmodel.to(device)\nUsing model.vision_model  and model.language_model , we can\nsee which ViT and generative model are used, respectively, in the\nBLIP-2 model we loaded.\nWe loaded two components that make up our full pipeline: a processor and a model.\nThe processor can be compared to the tokenizer of language models. It converts\nunstructured input, such as images and text, to representations that the model gener‐\nally expects.\nPreprocessing images\nLet’s start by exploring what the processor does to images. We start by loading the\npicture of a very wide image for illustration purposes:\n# Load image of a supercar\ncar_path  = ""https://raw.githubusercontent.com/HandsOnLLM/Hands-On-Large-\nLanguage-Models/main/chapter09/images/car.png""\nimage = Image.open(urlopen(car_path )).convert(""RGB"")\nimage\n278 | Chapter 9: Multimodal Large Language Models\nThe image has 520 × 492 pixels, which is generally an unusual format. So let’s see\nwhat our processor does to it:\n# Preprocess the image\ninputs = blip_processor (image, return_tensors =""pt"").to(device, torch.float16)\ninputs[""pixel_values"" ].shape\nThis gives us the following shape:\ntorch.Size([1, 3, 224, 224])\nThe result is a 224 × 224-sized image. Quite a bit smaller than we initially had! This\nalso means that all the original different shapes of the image will be processed into\nsquares. So be careful inputting very wide or tall images as they might get distorted.\nPreprocessing text\nLet’s continue this exploration of the processor with text instead. First, we can access\nthe tokenizer used to tokenize the input text:\nblip_processor .tokenizer\nThis gives us the following output:\nGPT2TokenizerFast(name_or_path='Salesforce/blip2-opt-2.7b', vocab_size=50265, \nmodel_max_length=1000000000000000019884624838656, is_fast=True, pad-\nding_side='right', truncation_side='right', special_tokens={'bos_token': '</\ns>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '<pad>'}, \nclean_up_tokenization_spaces=True), added_tokens_decoder={\n1: AddedToken(""<pad>"", rstrip=False, lstrip=False, single_word=False, normal-\nized=True, special=True),\n2: AddedToken(""</s>"", rstrip=False, lstrip=False, single_word=False, normal-\nized=True, special=True),\n}\nMaking Text Generation Models Multimodal | 279",3050
105-Use Case 1 Image Captioning.pdf,105-Use Case 1 Image Captioning,"The BLIP-2 model here uses a GPT2Tokenizer . As we explored in Chapter 2 , how\ntokenizers deal with input text can differ greatly.\nTo explore how GPT2Tokenizer  works, we can try it out with a small sentence. We\nstart by converting the sentence to token IDs before converting them back to tokens:\n# Preprocess the text\ntext = ""Her vocalization was remarkably melodic""\ntoken_ids  = blip_processor (image, text=text, return_tensors =""pt"")\ntoken_ids  = token_ids .to(device, torch.float16)[""input_ids"" ][0]\n# Convert input ids back to tokens\ntokens = blip_processor .tokenizer .convert_ids_to_tokens (token_ids )\ntokens\nThis gives us the following tokens:\n['</s>', 'Her', 'Ġvocal', 'ization', 'Ġwas', 'Ġremarkably', 'Ġmel', 'odic']\nWhen we inspect the tokens, you might notice a strange symbol at the beginning of\nsome tokens, namely, the Ġ symbol. This is actually supposed to be a space. However,\nan internal function takes characters in certain code points and moves them up by\n256 to make them printable. As a result, the space (code point 32) becomes Ġ (code\npoint 288).\nWe will convert them to underscores for illustrative purposes:\n# Replace the space token with an underscore\ntokens = [token.replace(""Ġ"", ""_"") for token in tokens]\ntokens\nThis gives us a nicer output:\n['</s>', 'Her', '_vocal', 'ization', '_was', '_remarkably', '_mel', 'odic']\nThe output shows that the underscore indicates the beginning of a word. That way,\nwords that are made up of multiple tokens can be recognized.\nUse Case 1: Image Captioning\nThe most straightforward usage of a model like BLIP-2 is to create captions of images\nthat you have in your data. Y ou might be a store that wants to create descriptions\nof its clothing or perhaps you are a photographer that does not have the time to\nmanually label the 1,000+ pictures of a wedding.\nThe process of captioning an image closely follows the processing. An image is\nconverted to pixel values that the model can read. These pixel values are passed to\nBLIP-2 to be converted into soft visual prompts that the LLM can use to decide on a\nproper caption.\n280 | Chapter 9: Multimodal Large Language Models\nLet’s take the image of a supercar and use the processor to derive pixels in the\nexpected shape:\n# Load an AI-generated image of a supercar\nimage = Image.open(urlopen(car_path )).convert(""RGB"")\n# Convert an image into inputs and preprocess it\ninputs = blip_processor (image, return_tensors =""pt"").to(device, torch.float16)\nimage\nThe next step is converting the image into token IDs using the BLIP-2 model. After\ndoing so, we can convert the IDs into text (the generated caption):\n# Generate image ids to be passed to the decoder (LLM)\ngenerated_ids  = model.generate (**inputs, max_new_tokens =20)\n# Generate text from the image ids\ngenerated_text  = blip_processor .batch_decode (\n    generated_ids , skip_special_tokens =True\n)\ngenerated_text  = generated_text [0].strip()\ngenerated_text\ngenerated_text  contains the caption:\nan orange supercar driving on the road at sunset\nThis seems like a perfect description for this image!\nImage captioning is a great way to get to learn this model before stepping into more\ncomplex use cases. Try it out with a few images yourself and see where it performs\nwell and where it performs poorly. Domain-specific images, like pictures of specific\nMaking Text Generation Models Multimodal | 281\n8Roy Schafer. Psychoanalytic Interpretation in Rorschach Testing: Theory  and Application  (1954).cartoon characters or imaginary creations, may fail as the model was trained on\nlargely public data.\nLet’s end this use case with a fun example, namely an image from the Rorschach test,\nwhich is illustrated in Figure 9-21 . It is part of an old psychological experiment that\ntests the individual’s perception of inkblots.8 What someone sees in such an inkblot\nsupposedly tells you something about a person’s personality characteristics. It is quite\na subjective test but that just makes it more fun!\nFigure 9-21. An image from the Rorschach test. What do you see in it?\nLet’s take the image illustrated in Figure 9-21  and use that as our input:\n# Load Rorschach image\nurl = ""https://upload.wikimedia.org/wikipedia/commons/7/70/Ror\nschach_blot_01.jpg""\nimage = Image.open(urlopen(url)).convert(""RGB"")\n# Generate caption\ninputs = blip_processor (image, return_tensors =""pt"").to(device, torch.float16)\ngenerated_ids  = model.generate (**inputs, max_new_tokens =20)\ngenerated_text  = blip_processor .batch_decode (\n    generated_ids , skip_special_tokens =True\n)\n282 | Chapter 9: Multimodal Large Language Models",4650
106-Use Case 2 Multimodal Chat-Based Prompting.pdf,106-Use Case 2 Multimodal Chat-Based Prompting,"generated_text  = generated_text [0].strip()\ngenerated_text\nAs before, when we inspect the generated_text  variable, we can take a look at the\ncaption:\na black and white ink drawing of a bat\nI can definitely see how the model would caption this image using such a description.\nSince this is a Rorschach test, what do you think it says about the model?\nUse Case 2: Multimodal Chat-Based Prompting\nAlthough  captioning is an important task, we can extend its use case even further.\nIn the previous example, we showed going from one modality, vision (image), to\nanother, text (caption).\nInstead of following this linear structure, we can try to present both modalities simul‐\ntaneously by performing what is called visual question answering. In this particular\nuse case, we give the model an image along with a question about that specific image\nfor it to answer. The model needs to process both the image as well as the question at\nonce.\nTo demonstrate, let’s start with the picture of a car and ask BLIP-2 to describe the\nimage. To do so, we first need to preprocess the image as we did a few times before:\n# Load an AI-generated image of a supercar\nimage = Image.open(urlopen(car_path )).convert(""RGB"")\nTo perform our visual question answering we need to give BLIP-2 more than just the\nimage, namely the prompt. Without it, the model would generate a caption as it did\nbefore. We will ask the model to describe the image we just processed:\n# Visual question answering\nprompt = ""Question: Write down what you see in this picture. Answer:""\n# Process both the image and the prompt\ninputs = blip_processor (image, text=prompt, return_tensors =""pt"").to(device, \ntorch.float16)\n# Generate text\ngenerated_ids  = model.generate (**inputs, max_new_tokens =30)\ngenerated_text  = blip_processor .batch_decode (\n    generated_ids , skip_special_tokens =True\n)\ngenerated_text  = generated_text [0].strip()\ngenerated_text\nThis gives us the following output:\nMaking Text Generation Models Multimodal | 283\nA sports car driving on the road at sunset\nIt correctly describes the image. However, this is a rather simple example since our\nquestion is essentially asking the model to create a caption. Instead, we can ask\nfollow-up questions in a chat-based manner.\nTo do so, we can give the model our previous conversation, including its answer to\nour question. We then ask it a follow-up question:\n# Chat-like prompting\nprompt = ""Question: Write down what you see in this picture. Answer: A sports \ncar driving on the road at sunset. Question: What would it cost me to drive \nthat car? Answer:""\n# Generate output\ninputs = blip_processor (image, text=prompt, return_tensors =""pt"").to(device, \ntorch.float16)\ngenerated_ids  = model.generate (**inputs, max_new_tokens =30)\ngenerated_text  = blip_processor .batch_decode (\n    generated_ids , skip_special_tokens =True\n)\ngenerated_text  = generated_text [0].strip()\ngenerated_text\nThis gives us the following answer:\n$1,000,000\n$1,000,000 is highly specific! This shows more chat-like behavior from BLIP-2, which\nallows for some interesting conversations.\nFinally, we can make this process a bit smoother by creating an interactive chatbot\nusing ipywidgets , an extension for Jupyter notebooks that allows us to make interac‐\ntive buttons, input text, etc:\nfrom IPython.display  import HTML, display\nimport ipywidgets  as widgets\ndef text_eventhandler (*args):\n  question  = args[0][""new""]\n  if question :\n    args[0][""owner""].value = """"\n    # Create prompt\n    if not memory:\n      prompt = "" Question: ""  + question  + "" Answer:""\n    else:\n      template  = ""Question: {} Answer: {}.""\n      prompt = "" "".join(\n          [\n              template .format(memory[i][0], memory[i][1]) \n284 | Chapter 9: Multimodal Large Language Models\n              for i in range(len(memory))\n          ]\n      ) + "" Question: ""  + question  + "" Answer:""\n    # Generate text\n    inputs = blip_processor (image, text=prompt, return_tensors =""pt"")\n    inputs = inputs.to(device, torch.float16)\n    generated_ids  = model.generate (**inputs, max_new_tokens =100)\n    generated_text  = blip_processor .batch_decode (\n        generated_ids , \n        skip_special_tokens =True\n    )\n    generated_text  = generated_text [0].strip().split(""Question"" )[0]\n    # Update memory\n    memory.append((question , generated_text ))\n    # Assign to output\n    output.append_display_data (HTML(""<b>USER:</b> ""  + question ))\n    output.append_display_data (HTML(""<b>BLIP-2:</b> ""  + generated_text ))\n    output.append_display_data (HTML(""<br>""))\n# Prepare widgets\nin_text = widgets.Text()\nin_text.continuous_update  = False\nin_text.observe(text_eventhandler , ""value"")\noutput = widgets.Output()\nmemory = []\n# Display chat box\ndisplay(\n    widgets.VBox(\n        children =[output, in_text],\n        layout=widgets.Layout(display=""inline-flex"" , flex_flow =""column-\nreverse"" ),\n    )\n)\nMaking Text Generation Models Multimodal | 285",5034
107-Part III. Training and Fine-Tuning Language Models.pdf,107-Part III. Training and Fine-Tuning Language Models,"It seems that we can continue the conversation and ask a bunch of questions. Using\nthis chat-based approach, we essentially created a chatbot that can reason about\nimages!\nSummary\nIn this chapter, we explored various methods for making LLMs multimodal by\nbridging the gap between textual and visual representations. We started by discus‐\nsing Transformers for vision, which are models that convert images into numerical\nrepresentations. This was achieved through the use of image encoders and patch\nembeddings, which allow the model to process images at various scales.\nWe then explored the creation of embedding models that can convert both images\nand text to numerical representations using CLIP . We saw how CLIP uses contrastive\nlearning to align image and text embeddings in a shared space, allowing for tasks like\nzero-shot classification, clustering, and search. The chapter also introduced Open‐\nCLIP , an open source variant of CLIP that is easy to use for multimodal embedding\ntasks.\nFinally, we explored how text generation models could be made multimodal and\ndived into the BLIP-2 model. The core idea of these multimodal text generation\nmodels involves projecting visual features from input images to text embeddings that\ncan be used by LLMs. We saw how this model could be used for image captioning and\nmultimodal chat-based prompting, where both modalities are combined to generate\nresponses. Overall, this chapter highlighted the power of multimodality in LLMs and\ndemonstrated its applications in various areas such as image captioning, search, and\nchat-based prompting.\nIn Part III of the book, we will cover training and fine-tuning techniques. In Chap‐\nter 10 , we will explore how to create and fine-tune a text embedding model, which\nis a core technology that drives many language modeling applications. This next\nchapter serves as an introduction into both training and fine-tuning language models.\n286 | Chapter 9: Multimodal Large Language Models\nPART III\nTraining and Fine-Tuning\nLanguage Models",2051
108-Chapter 10. Creating Text Embedding Models.pdf,108-Chapter 10. Creating Text Embedding Models,,0
109-Embedding Models.pdf,109-Embedding Models,"CHAPTER 10\nCreating Text Embedding Models\nText embedding models lie at the foundation of many powerful natural language\nprocessing applications. They lay the groundwork for empowering already impres‐\nsive technologies such as text generation models. We have already used embedding\nmodels throughout this book in a number of applications, such as supervised classifi‐\ncation, unsupervised classification, semantic search, and even giving memory to text\ngeneration models like ChatGPT.\nIt is nearly impossible to overstate the importance of embedding models in the field\nas they are the driving power behind so many applications. As such, in this chapter,\nwe will discuss a variety of ways that we can create and fine-tune an embedding\nmodel to increase its representative and semantic power.\nLet’s start by discovering what embedding models are and how they generally work.\nEmbedding Models\nEmbeddings  and embedding models have already been discussed in quite a number\nof chapters (Chapters 4, 5, and 8) thereby demonstrating their usefulness. Before\ngoing into training such a model, let’s recap what we have learned with embedding\nmodels.\nUnstructured textual data by itself is often quite hard to process. They are not\nvalues we can directly process, visualize, and create actionable results from. We first\nhave to convert this textual data to something that we can easily process: numeric\nrepresentations. This process is often referred to as embedding  the input to output\nusable vectors, namely embeddings,  as shown in Figure 10-1 .\n289\nFigure 10-1. We use an embedding model to convert textual input, such as documents,\nsentences, and phrases, to numerical representations, called embeddings.\nThis process of embedding the input is typically performed by an LLM, which we\nrefer to as an embedding model . The main purpose of such a model is to be as accurate\nas possible in representing the textual data as an embedding.\nHowever, what does it mean to be accurate in representation? Typically, we want to\ncapture the semantic nature —the meaning—of documents. If we can capture the core\nof what the document communicates, we hope to have captured what the document\nis about. In practice, this means that we expect vectors of documents that are similar\nto one another to be similar, whereas the embeddings of documents that each discuss\nsomething entirely different should be dissimilar. We’ve seen this idea of semantic\nsimilarity several times already in this book, and it is visualized in Figure 10-2 . This\nfigure is a simplified example. While two-dimensional visualization helps illustrate\nthe proximity and similarity of embeddings, these embeddings typically reside in\nhigh-dimensional spaces.\nFigure 10-2. The idea of semantic similarity is that we expect textual data with similar\nmeanings to be closer to each other in n-dimensional space (two dimensions are illustra‐\nted here).\n290 | Chapter 10: Creating Text Embedding Models",2987
110-What Is Contrastive Learning.pdf,110-What Is Contrastive Learning,"An embedding model, however, can be trained for a number of purposes. For\nexample, when we are building a sentiment classifier, we are more interested in the\nsentiment of texts than their semantic similarity. As illustrated in Figure 10-3 , we can\nfine-tune the model such that documents are closer in n-dimensional space based on\ntheir sentiment rather than their semantic nature.\nEither way, an embedding model aims to learn what makes certain documents similar\nto one another and we can guide this process. By presenting the model with enough\nexamples of semantically similar documents, we can steer toward semantics whereas\nusing examples of sentiment would steer it in that direction.\nFigure 10-3. In addition to semantic similarity, an embedding model can be trained\nto focus on sentiment similarity. In this figure,  negative reviews (red) are close to one\nanother and dissimilar to positive reviews (green).\nThere are many ways in which we can train, fine-tune, and guide embedding mod‐\nels, but one of the strongest and most widely used techniques is called contrastive\nlearning.\nWhat Is Contrastive Learning?\nOne  major technique for both training and fine-tuning text embedding models is\ncalled contrastive learning . Contrastive learning is a technique that aims to train\nan embedding model such that similar documents are closer in vector space while\ndissimilar documents are further apart. If this sounds familiar, it’s because it’s very\nWhat Is Contrastive Learning? | 291\n1Alan Garfinkel. Forms of Explanation: Rethinking the Questions in Social Theory . Y ale University Press (1982).\n2Tim Miller. “Contrastive explanation: A structural-model approach. ” The Knowledge Engineering Review  36\n(2021): e14.similar to the word2vec method from Chapter 2 . We have seen this notion previously\nin Figures 10-2  and 10-3 .\nThe underlying idea of contrastive learning is that the best way to learn and model\nsimilarity/dissimilarity between documents is by feeding a model examples of similar\nand dissimilar pairs. In order to accurately capture the semantic nature of a docu‐\nment, it often needs to be contrasted with another document for a model to learn\nwhat makes it different or similar. This contrasting procedure is quite powerful and\nrelates to the context in which documents are written. This high-level procedure is\ndemonstrated in Figure 10-4 .\nFigure 10-4. Contrastive learning aims to teach an embedding model whether docu‐\nments are similar or dissimilar. It does so by presenting groups of documents to a model\nthat are similar or dissimilar to a certain degree.\nAnother way to look at contrastive learning is through the nature of explanations. A\nnice example of this is an anecdotal story of a reporter asking a robber “Why did you\nrob a bank?” to which he answers, “Because that is where the money is. ”1 Although\na factually correct answer, the intent of the question was not why he robs banks\nspecifically but why he robs at all. This is called contrastive explanation  and refers to\nunderstanding a particular case, “Why P?” in contrast to alternatives, “Why P and not\nQ?”2 In the example, the question could be interpreted in a number of ways and may\nbe best modeled by providing an alternative: “Why did you rob a bank (P) instead of\nobeying the law (Q)?”\nThe importance of alternatives to the understanding of a question also applies to how\nan embedding learns through contrastive learning. By showing a model similar and\ndissimilar pairs of documents, it starts to learn what makes something similar/dissim‐\nilar and more importantly, why.\nFor example, you could teach a model to understand what a dog is by letting it\nfind features such as “tail, ” “nose, ” “four legs, ” etc. This learning process can be quite\ndifficult since features are often not well-defined and can be interpreted in a number\nof ways. A being with a “tail, ” “nose, ” and “four legs” can also be a cat. To help the\n292 | Chapter 10: Creating Text Embedding Models",4024
111-SBERT.pdf,111-SBERT,"3Nils Reimers and Iryna Gurevych. “Sentence-BERT: Sentence embeddings using Siamese BERT-networks. ”\narXiv preprint arXiv:1908.10084  (2019).\nmodel steer toward what we are interested in, we essentially ask it, “Why is this a\ndog and not a cat?” By providing the contrast between two concepts, it starts to learn\nthe features that define the concept but also the features that are not related. We get\nmore information when we frame a question as a contrast. We further illustrate this\nconcept of contrastive explanation in Figure 10-5 .\nFigure 10-5. When we feed an embedding model different  contrasts (degrees of similar‐\nity), it starts to learn what makes things different  from one another and thereby the\ndistinctive characteristics of concepts.\nOne of the earliest and most popular examples of contrastive learn‐\ning in NLP  is actually word2vec, as we discussed in Chapters 1 and\n2. The model learns word representations by training on individual\nwords in a sentence. A word close to a target word in a sentence\nwill be constructed as a positive pair whereas randomly sampled\nwords constitute dissimilar pairs. In other words, positive examples\nof neighboring words are contrasted with randomly selected words\nthat are not neighbors. Although not widely known, it is one of\nthe first major breakthroughs in NLP that leverages contrastive\nlearning with neural networks.\nThere are many ways we can apply contrastive learning to create text embed‐\nding models but the most well-known technique and framework is sentence-\ntransformers .\nSBERT\nAlthough  there are many forms of contrastive learning, one framework that has\npopularized the technique within the natural language processing community is\nsentence-transformers .3 Its approach fixes a major problem with the original\nBERT implementation for creating sentence embeddings, namely its computational\nSBERT | 293\n4Jeffrey Pennington, Richard Socher, and Christopher D. Manning. “GloVe: Global vectors for word represen‐\ntation. ” Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP) .\n2014.overhead. Before sentence-transformers , sentence embeddings often used an archi‐\ntectural structure called cross-encoders with BERT.\nA cross-encoder allows two sentences to be passed to the Transformer network\nsimultaneously to predict the extent to which the two sentences are similar. It does so\nby adding a classification head to the original architecture that can output a similarity\nscore. However, the number of computations rises quickly when you want to find\nthe highest pair in a collection of 10,000 sentences. That would require n·(n −1)/2\n= 49,995,000 inference computations and therefore generates significant overhead.\nMoreover, a cross-encoder generally does not generate embeddings, as shown in\nFigure 10-6 . Instead, it outputs a similarity score between the input sentences.\nA solution to this overhead is to generate embeddings from a BERT model by\naveraging its output layer or using the [CLS]  token. This, however, has shown to be\nworse than simply averaging word vectors, like GloVe.4\nFigure 10-6. The architecture of a cross-encoder. Both sentences are concatenated, sepa‐\nrated with a <SEP>  token, and fed to the model simultaneously.\nInstead, the authors of sentence-transformers  approached the problem differently\nand searched for a method that is fast and creates embeddings that can be compared\nsemantically. The result is an elegant alternative to the original cross-encoder archi‐\ntecture. Unlike a cross-encoder, in sentence-transformers  the classification head is\ndropped, and instead mean pooling is used on the final output layer to generate an\nembedding. This pooling layer averages the word embeddings and gives back a fixed\ndimensional output vector. This ensures a fixed-size embedding.\n294 | Chapter 10: Creating Text Embedding Models\nThe training for sentence-transformers  uses a Siamese architecture. In this archi‐\ntecture, as visualized in Figure 10-7 , we have two identical BERT models that share\nthe same weights and neural architecture. These models are fed the sentences from\nwhich embeddings are generated through the pooling of token embeddings. Then,\nmodels are optimized through the similarity of the sentence embeddings. Since the\nweights are identical for both BERT models, we can use a single model and feed it the\nsentences one after the other.\nFigure 10-7. The architecture of the original sentence-transformers  model, which\nleverages a Siamese network, also called a bi-encoder.\nThe optimization process of these pairs of sentences is done through loss functions,\nwhich can have a major impact on the model’s performance. During training, the\nembeddings for each sentence are concatenated together with the difference between\nthe embeddings. Then, this resulting embedding is optimized through a softmax\nclassifier.\nThe resulting architecture is also referred to as a bi-encoder or SBERT for sentence-\nBERT. Although a bi-encoder is quite fast and creates accurate sentence representa‐\ntions, cross-encoders generally achieve better performance than a bi-encoder but do\nnot generate embeddings.\nThe bi-encoder, like a cross-encoder, leverages contrastive learning; by optimizing the\n(dis)similarity between pairs of sentences, the model will eventually learn the things\nthat make the sentences what they are.\nSBERT | 295",5452
112-Creating an Embedding Model.pdf,112-Creating an Embedding Model,,0
113-Train Model.pdf,113-Train Model,"To perform contrastive learning, we need two things. First, we need data that consti‐\ntutes similar/dissimilar pairs. Second, we will need to define how the model defines\nand optimizes similarity.\nCreating an Embedding Model\nThere are many methods through which an embedding model can be created\nbut generally, we look toward contrastive learning. This is an important aspect\nof many embedding models as the process allows it to efficiently learn semantic\nrepresentations.\nHowever, this is not a free process. We will need to understand how to generate\ncontrastive examples, how to train the model, and how to properly evaluate it.\nGenerating Contrastive Examples\nWhen  pretraining your embedding model, you will often see data being used from\nnatural language inference (NLI) datasets. NLI refers to the task of investigating\nwhether, for a given premise, it entails the hypothesis (entailment), contradicts it\n(contradiction), or neither (neutral).\nFor example, when the premise is “He is in the cinema watching Coco ” and the\nhypothesis “He is watching Frozen  at home, ” then these statements are contradictions.\nIn contrast, when the premise is “He is in the cinema watching Coco ” and the\nhypothesis “In the movie theater he is watching the Disney movie Coco , ” then these\nstatements are considered entailment. This principle is illustrated in Figure 10-8 .\nFigure 10-8. We can leverage the structure of NLI datasets to generate negative examples\n(contradiction) and positive examples (entailments) for contrastive learning.\nIf you look closely at entailment and contradiction, then they describe the extent to\nwhich two inputs are similar to one another. As such, we can use NLI datasets to\ngenerate negative examples (contradictions) and positive examples (entailments) for\ncontrastive learning.\nThe data that we are going to be using throughout creating and fine-tuning\nembedding models is derived from the General Language Understanding Evaluation\n296 | Chapter 10: Creating Text Embedding Models\nbenchmark  (GLUE). This GLUE benchmark consists of nine language understanding\ntasks to evaluate and analyze model performance.\nOne of these tasks is the Multi-Genre Natural Language Inference (MNLI) corpus,\nwhich is a collection of 392,702 sentence pairs annotated with entailment (contradic‐\ntion, neutral, entailment). We will be using a subset of the data, 50,000 annotated\nsentence pairs, to create a minimal example that does not need to be trained for hours\non end. Do note, though, that the smaller the dataset, the more unstable training\nor fine-tuning an embedding model is. If possible, larger datasets are preferred\nassuming it is still quality data:\nfrom datasets  import load_dataset\n# Load MNLI dataset from GLUE\n# 0 = entailment, 1 = neutral, 2 = contradiction\ntrain_dataset  = load_dataset (\n    ""glue"", ""mnli"", split=""train""\n).select(range(50_000))\ntrain_dataset  = train_dataset .remove_columns (""idx"")\nNext, we take a look at an example:\ndataset[2]\n{'premise': 'One of our number will carry out your instructions minutely.',\n 'hypothesis': 'A member of my team will execute your orders with immense \nprecision.',\n 'label': 0}\nThis shows an example of an entailment between the premise and the hypothesis as\nthey are positively related and have near identical meanings.\nTrain Model\nNow  that we have our dataset with training examples, we will need to create our\nembedding model. We typically choose an existing sentence-transformers  model\nand fine-tune that model, but in this example, we are going to train an embedding\nfrom scratch.\nThis means that we will have to define two things. First, a pretrained Transformer\nmodel that serves as embedding individual words. We will use the BERT base model\n(uncased)  as it is a great introduction model. However, many others exist that\nalso have been evaluated using sentence-transformers . Most notably, microsoft/\nmpnet-base  often gives good results when used as a word embedding model.\nfrom sentence_transformers  import SentenceTransformer\n# Use a base model\nembedding_model  = SentenceTransformer ('bert-base-uncased' )\nCreating an Embedding Model | 297\nBy default, all layers of an LLM in sentence-transformers  are\ntrainable. Although it is possible to freeze certain layers, it is\ngenerally not advised since the performance is often better when\nunfreezing all layers.\nNext, we will need to define a loss function over which we will optimize the model.\nAs mentioned at the beginning of this section, one of the first instances of sentence-\ntransformers  uses softmax loss. For illustrative purposes, we are going to be using\nthat for now, but we will go into more performant losses later on:\nfrom sentence_transformers  import losses\n# Define the loss function. In softmax loss, we will also need to explicitly \nset the number of labels.\ntrain_loss  = losses.SoftmaxLoss (\n    model=embedding_model ,\n    sentence_embedding_dimension =embedding_model .get_sentence_embedding_dimen\nsion(),\n    num_labels =3\n)\nBefore we train our model, we define an evaluator to evaluate the model’s perfor‐\nmance during training, which also determines the best model to save.\nWe can perform evaluation of the performance of our model using the Semantic\nTextual Similarity Benchmark (STSB). It is a collection of human-labeled sentence\npairs, with similarity scores between 1 and 5.\nWe use this dataset to explore how well our model scores on this semantic similarity\ntask. Moreover, we process the STSB data to make sure all values are between 0 and 1:\nfrom sentence_transformers.evaluation  import EmbeddingSimilarityEvaluator\n# Create an embedding similarity evaluator for STSB\nval_sts = load_dataset (""glue"", ""stsb"", split=""validation"" )\nevaluator  = EmbeddingSimilarityEvaluator (\n    sentences1 =val_sts[""sentence1"" ],\n    sentences2 =val_sts[""sentence2"" ],\n    scores=[score/5 for score in val_sts[""label""]],\n    main_similarity =""cosine"" ,\n)\nNow that we have our evaluator, we create SentenceTransformerTrainingArgu\nments , similar to training with Hugging Face Transformers (as we will explore in\nthe next chapter):\nfrom sentence_transformers.training_args  import SentenceTransformerTrainingArgu\nments\n# Define the training arguments\n298 | Chapter 10: Creating Text Embedding Models\nargs = SentenceTransformerTrainingArguments (\n    output_dir =""base_embedding_model"" ,\n    num_train_epochs =1,\n    per_device_train_batch_size =32,\n    per_device_eval_batch_size =32,\n    warmup_steps =100,\n    fp16=True,\n    eval_steps =100,\n    logging_steps =100,\n)\nOf note are the following arguments:\nnum_train_epochs\nThe number of training rounds. We keep this at 1 for faster training but it is\ngenerally advised to increase this value.\nper_device_train_batch_size\nThe number of samples to process simultaneously on each device (e.g., GPU or\nCPU) during evaluation. Higher values generally means faster training.\nper_device_eval_batch_size\nThe number of samples to process simultaneously on each device (e.g., GPU or\nCPU) during evaluation. Higher values generally means faster evaluation.\nwarmup_steps\nThe number of steps during which the learning rate will be linearly increased\nfrom zero to the initial learning rate defined for the training process. Note that\nwe did not specify a custom learning rate for this training process.\nfp16\nBy enabling this parameter we allow for mixed precision training, where compu‐\ntations are performed using 16-bit floating-point numbers (FP16) instead of the\ndefault 32-bit (FP32). This reduces memory usage and potentially increases the\ntraining speed.\nNow that we have defined our data, embedding model, loss, and evaluator, we can\nstart training our model. We can do that using SentenceTransformerTrainer :\nfrom sentence_transformers.trainer  import SentenceTransformerTrainer\n# Train embedding model\ntrainer = SentenceTransformerTrainer (\n    model=embedding_model ,\n    args=args,\n    train_dataset =train_dataset ,\n    loss=train_loss ,\n    evaluator =evaluator\n)\ntrainer.train()\nCreating an Embedding Model | 299",8170
114-Loss Functions.pdf,114-Loss Functions,"5Niklas Muennighoff et al. “MTEB: Massive Text Embedding Benchmark. ” arXiv preprint arXiv:2210.07316\n(2022).\nAfter training our model, we can use the evaluator to get the performance on this\nsingle task:\n# Evaluate our trained model\nevaluator (embedding_model )\n{'pearson_cosine': 0.5982288436666162,\n 'spearman_cosine': 0.6026682018489217,\n 'pearson_manhattan': 0.6100690915500567,\n 'spearman_manhattan': 0.617732600131989,\n 'pearson_euclidean': 0.6079280934202278,\n 'spearman_euclidean': 0.6158926913905742,\n 'pearson_dot': 0.38364924527804595,\n 'spearman_dot': 0.37008497926991796,\n 'pearson_max': 0.6100690915500567,\n 'spearman_max': 0.617732600131989}\nWe get several different distance measures. The one we are interested in most is\n'pearson_cosine' , which is the cosine similarity between centered vectors. It is a\nvalue between 0 and 1 where a higher value indicates higher degrees of similarity. We\nget a value of 0.59, which we consider a baseline throughout this chapter.\nLarger batch sizes tend to work better with multiple negative rank‐\nings (MNR) loss as a larger batch makes the task more difficult.\nThe reason for this is that the model needs to find the best match‐\ning sentence from a larger set of potential pairs of sentences. Y ou\ncan adapt the code to try out different batch sizes and get a feeling\nof its effects.\nIn-Depth Evaluation\nA good embedding model is more than just a good score on the STSB benchmark! As\nwe observed earlier, the GLUE benchmark has a number of tasks for which we can\nevaluate our embedding model. However, there exist many more benchmarks that\nallow for the evaluation of embedding models. To unify this evaluation procedure,\nthe Massive Text Embedding Benchmark (MTEB) was developed.5 The MTEB spans\n8 embedding tasks that cover 58 datasets and 112 languages.\nTo publicly compare state-of-the-art embedding models, a leaderboard  was created\nwith the scores of each embedding model across all tasks:\nfrom mteb import MTEB\n# Choose evaluation task\nevaluation  = MTEB(tasks=[""Banking77Classification"" ])\n300 | Chapter 10: Creating Text Embedding Models\n# Calculate results\nresults = evaluation .run(model)\n{'Banking77Classification': {'mteb_version': '1.1.2',\n 'dataset_revision': '0fd18e25b25c072e09e0d92ab615fda904d66300',\n 'mteb_dataset_name': 'Banking77Classification',\n 'test': {'accuracy': 0.4926298701298701,\n 'f1': 0.49083335791288685,\n 'accuracy_stderr': 0.010217785746224237,\n 'f1_stderr': 0.010265814957074591,\n 'main_score': 0.4926298701298701,\n 'evaluation_time': 31.83}}}\nThis gives us several evaluation metrics for this specific task that we can use to\nexplore its performance.\nThe great thing about this evaluation benchmark is not only the diversity of the tasks\nand languages but that even the evaluation time is saved. Although many embedding\nmodels exist, we typically want those that are both accurate and have low latency. The\ntasks for which embedding models are used, like semantic search, often benefit from\nand require fast inference.\nSince testing your model on the entire MTEB can take a couple of hours depending\non your GPU, we will use the STSB benchmark throughout this chapter instead for\nillustration purposes.\nWhenever you are done training and evaluating your model, it is\nimportant to restart  the notebook. This will clear your VRAM up\nfor the next training examples throughout this chapter. By restart‐\ning the notebook, we can be sure that all VRAM is cleared.\nLoss Functions\nWe trained our model using softmax loss to illustrate how one of the first sentence-\ntransformers  models was trained. However, not only is there a large variety of loss\nfunctions to choose from, but softmax loss is generally not advised as there are more\nperformant losses .\nInstead of going through every single loss function out there, there are two loss\nfunctions that are typically used and seem to perform generally well, namely:\n•Cosine similarity•\n•Multiple negatives ranking (MNR) loss•\nCreating an Embedding Model | 301\nThere are many more loss functions to choose from than just\nthose discussed here. For example, a loss like MarginMSE works\ngreat for training or fine-tuning a cross-encoder. There are a num‐\nber of interesting loss functions implemented in the sentence-\ntransformers  framework .\nCosine similarity\nThe cosine similarity loss is an intuitive and easy-to-use loss that works across many\ndifferent use cases and datasets. It is typically used in semantic textual similarity\ntasks. In these tasks, a similarity score is assigned to the pairs of texts over which we\noptimize the model.\nInstead of having strictly positive or negative pairs of sentences, we assume pairs of\nsentences that are similar or dissimilar to a certain degree. Typically, this value lies\nbetween 0 and 1 to indicate dissimilarity and similarity, respectively ( Figure 10-9 ).\nFigure 10-9. Cosine similarity loss aims to minimize the cosine distance between seman‐\ntically similar sentences and to maximize the distance between semantically dissimilar\nsentences.\nCosine similarity loss is straightforward—it calculates the cosine similarity between\nthe two embeddings of the two texts and compares that to the labeled similarity score.\nThe model will learn to recognize the degree of similarity between sentences.\nCosine similarity loss intuitively works best using data where you have pairs of sen‐\ntences and labels that indicate their similarity between 0 and 1. To use this loss with\nour NLI dataset, we need to convert the entailment (0), neutral (1), and contradiction\n(2) labels to values between 0 and 1. The entailment represents a high similarity\n302 | Chapter 10: Creating Text Embedding Models\nbetween the sentences, so we give it a similarity score of 1. In contrast, since both\nneutral and contradiction represent dissimilarity, we give these labels a similarity\nscore of 0:\nfrom datasets  import Dataset, load_dataset\n# Load MNLI dataset from GLUE\n# 0 = entailment, 1 = neutral, 2 = contradiction\ntrain_dataset  = load_dataset (\n    ""glue"", ""mnli"", split=""train""\n).select(range(50_000))\ntrain_dataset  = train_dataset .remove_columns (""idx"")\n# (neutral/contradiction)=0 and (entailment)=1\nmapping = {2: 0, 1: 0, 0:1}\ntrain_dataset  = Dataset.from_dict ({\n    ""sentence1"" : train_dataset [""premise"" ],\n    ""sentence2"" : train_dataset [""hypothesis"" ],\n    ""label"": [float(mapping[label]) for label in train_dataset [""label""]]\n})\nAs before, we create our evaluator:\nfrom sentence_transformers.evaluation  import EmbeddingSimilarityEvaluator\n# Create an embedding similarity evaluator for stsb\nval_sts = load_dataset (""glue"", ""stsb"", split=""validation"" )\nevaluator  = EmbeddingSimilarityEvaluator (\n    sentences1 =val_sts[""sentence1"" ],\n    sentences2 =val_sts[""sentence2"" ],\n    scores=[score/5 for score in val_sts[""label""]],\n    main_similarity =""cosine""\n)\nThen, we follow the same steps as before but select a different loss instead:\nfrom sentence_transformers  import losses, SentenceTransformer\nfrom sentence_transformers.trainer  import SentenceTransformerTrainer\nfrom sentence_transformers.training_args  import SentenceTransformerTrainingArgu\nments\n# Define model\nembedding_model  = SentenceTransformer (""bert-base-uncased"" )\n# Loss function\ntrain_loss  = losses.CosineSimilarityLoss (model=embedding_model )\n# Define the training arguments\nargs = SentenceTransformerTrainingArguments (\n    output_dir =""cosineloss_embedding_model"" ,\n    num_train_epochs =1,\n    per_device_train_batch_size =32,\n    per_device_eval_batch_size =32,\nCreating an Embedding Model | 303\n6Matthew Henderson et al. “Efficient natural language response suggestion for smart reply. ” arXiv preprint\narXiv:1705.00652  (2017).\n7Aaron van den Oord, Y azhe Li, and Oriol Vinyals. “Representation learning with contrastive predictive\ncoding. ” arXiv preprint arXiv:1807.03748  (2018).\n8Ting Chen et al. “ A simple framework for contrastive learning of visual representations. ” International Confer‐\nence on Machine Learning . PMLR, 2020.    warmup_steps =100,\n    fp16=True,\n    eval_steps =100,\n    logging_steps =100,\n)\n# Train model\ntrainer = SentenceTransformerTrainer (\n    model=embedding_model ,\n    args=args,\n    train_dataset =train_dataset ,\n    loss=train_loss ,\n    evaluator =evaluator\n)\ntrainer.train()\nEvaluating the model after training gives us the following score:\n# Evaluate our trained model\nevaluator (embedding_model )\n{'pearson_cosine': 0.7222322163831805,\n 'spearman_cosine': 0.7250508271229599,\n 'pearson_manhattan': 0.7338163436711481,\n 'spearman_manhattan': 0.7323479193408869,\n 'pearson_euclidean': 0.7332716434966307,\n 'spearman_euclidean': 0.7316999722750905,\n 'pearson_dot': 0.660366792336156,\n 'spearman_dot': 0.6624167554844425,\n 'pearson_max': 0.7338163436711481,\n 'spearman_max': 0.7323479193408869}\nA Pearson cosine score of 0.72 is a big improvement compared to the softmax loss\nexample, which scored 0.59. This demonstrates the impact the loss function can have\non performance.\nMake sure to restart  your notebook so we can explore a more common and perform‐\nant loss, namely multiple negatives ranking loss.\nMultiple negatives ranking loss\nMultiple  negatives ranking (MNR) loss,6 often referred to as InfoNCE7 or NTXen‐\ntLoss,8 is a loss that uses either positive pairs of sentences or triplets that contain\na pair of positive sentences and an additional unrelated sentence. This unrelated\n304 | Chapter 10: Creating Text Embedding Models\nsentence is called a negative and represents the dissimilarity between the positive\nsentences.\nFor example, you might have pairs of question/answer, image/image caption, paper\ntitle/paper abstract, etc. The great thing about these pairs is that we can be confident\nthey are hard positive pairs. In MNR loss ( Figure 10-10 ), negative pairs are construc‐\nted by mixing a positive pair with another positive pair. In the example of a paper\ntitle and abstract, you would generate a negative pair by combining the title of a paper\nwith a completely different abstract. These negatives are called in-batch negatives  and\ncan also be used to generate the triplets.\nFigure 10-10. Multiple negatives ranking loss aims to minimize the distance between\nrelated pairs of text, such as questions and answers, and maximize the distance between\nunrelated pairs, such as questions and unrelated answers.\nAfter having generated these positive and negative pairs, we calculate their embed‐\ndings and apply cosine similarity. These similarity scores are then used to answer\nthe question, are these pairs negative or positive? In other words, it is treated as a\nclassification task and we can use cross-entropy loss to optimize the model.\nTo make these triplets we start with an anchor sentence (i.e., labeled as the “prem‐\nise”), which is used to compare other sentences. Then, using the MNLI dataset, we\nonly select sentence pairs that are positive (i.e., labeled as “entailment”). To add\nnegative sentences, we randomly sample sentences as the “hypothesis. ”\nimport random\nfrom tqdm import tqdm\nfrom datasets  import Dataset, load_dataset\n# # Load MNLI dataset from GLUE\nmnli = load_dataset (""glue"", ""mnli"", split=""train"").select(range(50_000))\nmnli = mnli.remove_columns (""idx"")\nCreating an Embedding Model | 305\nmnli = mnli.filter(lambda x: True if x[""label""] == 0 else False)\n# Prepare data and add a soft negative\ntrain_dataset  = {""anchor"" : [], ""positive"" : [], ""negative"" : []}\nsoft_negatives  = mnli[""hypothesis"" ]\nrandom.shuffle(soft_negatives )\nfor row, soft_negative  in tqdm(zip(mnli, soft_negatives )):\n    train_dataset [""anchor"" ].append(row[""premise"" ])\n    train_dataset [""positive"" ].append(row[""hypothesis"" ])\n    train_dataset [""negative"" ].append(soft_negative )\ntrain_dataset  = Dataset.from_dict (train_dataset )\nSince we only selected sentences labeled with “entailment, ” the number of rows\nreduced quite a a bit from 50,000 to 16,875 rows.\nLet’s define the evaluator:\nfrom sentence_transformers.evaluation  import EmbeddingSimilarityEvaluator\n# Create an embedding similarity evaluator for stsb\nval_sts = load_dataset (""glue"", ""stsb"", split=""validation"" )\nevaluator  = EmbeddingSimilarityEvaluator (\n    sentences1 =val_sts[""sentence1"" ],\n    sentences2 =val_sts[""sentence2"" ],\n    scores=[score/5 for score in val_sts[""label""]],\n    main_similarity =""cosine""\n)\nWe then train as before but with MNR loss instead:\nfrom sentence_transformers  import losses, SentenceTransformer\nfrom sentence_transformers.trainer  import SentenceTransformerTrainer\nfrom sentence_transformers.training_args  import SentenceTransformerTrainingArgu\nments\n# Define model\nembedding_model  = SentenceTransformer ('bert-base-uncased' )\n# Loss function\ntrain_loss  = losses.MultipleNegativesRankingLoss (model=embedding_model )\n# Define the training arguments\nargs = SentenceTransformerTrainingArguments (\n    output_dir =""mnrloss_embedding_model"" ,\n    num_train_epochs =1,\n    per_device_train_batch_size =32,\n    per_device_eval_batch_size =32,\n    warmup_steps =100,\n    fp16=True,\n    eval_steps =100,\n    logging_steps =100,\n)\n# Train model\n306 | Chapter 10: Creating Text Embedding Models\ntrainer = SentenceTransformerTrainer (\n    model=embedding_model ,\n    args=args,\n    train_dataset =train_dataset ,\n    loss=train_loss ,\n    evaluator =evaluator\n)\ntrainer.train()\nLet’s see how this dataset and loss function compare to our previous examples:\n# Evaluate our trained model\nevaluator (embedding_model )\n{'pearson_cosine': 0.8093892326162132,\n 'spearman_cosine': 0.8121064796503025,\n 'pearson_manhattan': 0.8215001523827565,\n 'spearman_manhattan': 0.8172161486524246,\n 'pearson_euclidean': 0.8210391407846718,\n 'spearman_euclidean': 0.8166537141010816,\n 'pearson_dot': 0.7473360302629125,\n 'spearman_dot': 0.7345184137194012,\n 'pearson_max': 0.8215001523827565,\n 'spearman_max': 0.8172161486524246}\nCompared to our previously trained model with softmax loss (0.72), our model with\nMNR loss (0.80) seems to be much more accurate!\nLarger batch sizes tend to be better with MNR loss as a larger\nbatch makes the task more difficult. The reason for this is that the\nmodel needs to find the best matching sentence from a larger set\nof potential pairs of sentences. Y ou can adapt the code to try out\ndifferent batch sizes and get a feeling of the effects.\nThere is a downside to how we used this loss function. Since negatives are sampled\nfrom other question/answer pairs, these in-batch or “easy” negatives that we used\ncould potentially be completely unrelated to the question. As a result, the embedding\nmodel’s task of then finding the right answer to a question becomes quite easy.\nInstead, we would like to have negatives that are very related to the question but not\nthe right answer. These negatives are called hard negatives . Since this would make\nthe task more difficult for the embedding model as it has to learn more nuanced\nrepresentations, the embedding model’s performance generally improves quite a bit.\nA good example of a hard negative is the following. Let’s assume we have the follow‐\ning question: “How many people live in Amsterdam?” A related answer to this ques‐\ntion would be: “ Almost a million people live in Amsterdam. ” To generate a good hard\nnegative, we ideally want the answer to contain something about Amsterdam and the\nnumber of people living in this city. For example: “More than a million people live in\nUtrecht, which is more than Amsterdam. ” This answer relates to the question but is\nCreating an Embedding Model | 307\nnot the actual answer, so this would be a good hard negative. Figure 10-11  illustrates\nthe differences between easy and hard negatives.\nFigure 10-11. An easy negative is typically unrelated to both the question and answer. A\nsemi-hard negative has some similarities to the topic of the question and answer but is\nsomewhat unrelated. A hard negative is very similar to the question but is generally the\nwrong answer.\nGathering negatives can roughly be divided into the following three processes:\nEasy negatives\nThrough randomly sampling documents as we did before.\nSemi-hard negatives\nUsing  a pretrained embedding model, we can apply cosine similarity on all\nsentence embeddings to find those that are highly related. Generally, this does\nnot lead to hard negatives since this method merely finds similar sentences, not\nquestion/answer pairs.\nHard negatives\nThese often need to be either manually labeled (for instance, by generating semi-\nhard negatives) or you can use a generative model to either judge or generate\nsentence pairs.\nMake sure to restart  your notebook so we can explore the different methods of\nfine-tuning embedding models.\n308 | Chapter 10: Creating Text Embedding Models",16996
115-Fine-Tuning an Embedding Model.pdf,115-Fine-Tuning an Embedding Model,,0
116-Supervised.pdf,116-Supervised,"Fine-Tuning an Embedding Model\nIn the previous section, we went through the basics of training an embedding model\nfrom scratch and saw how we could leverage loss functions to further optimize its\nperformance. This approach, although quite powerful, requires creating an embed‐\nding model from scratch. This process can be quite costly and time-consuming.\nInstead, the sentence-transformers  framework allows nearly all embedding models\nto be used as a base for fine-tuning. We can choose an embedding model that was\nalready trained on a large amount of data and fine-tune it for our specific data or\npurpose.\nThere are several ways to fine-tune your model, depending on the data availability\nand domain. We will go through two such methods and demonstrate the strength of\nleveraging pretrained embedding models.\nSupervised\nThe most straightforward way to fine-tune an embedding model is to repeat the\nprocess of training our model as we did before but replace the 'bert-base-uncased'\nwith a pretrained sentence-transformers  model. There are many to choose from\nbut generally, all-MiniLM-L6-v2  performs  well across many use cases  and due to its\nsmall size is quite fast.\nWe use the same data as we used to train our model in the MNR loss example\nbut instead use a pretrained embedding model to fine-tune. As always, let’s start by\nloading the data and creating the evaluator:\nfrom datasets  import load_dataset\nfrom sentence_transformers.evaluation  import EmbeddingSimilarityEvaluator\n# Load MNLI dataset from GLUE\n# 0 = entailment, 1 = neutral, 2 = contradiction\ntrain_dataset  = load_dataset (\n    ""glue"", ""mnli"", split=""train""\n).select(range(50_000))\ntrain_dataset  = train_dataset .remove_columns (""idx"")\n# Create an embedding similarity evaluator for stsb\nval_sts = load_dataset (""glue"", ""stsb"", split=""validation"" )\nevaluator  = EmbeddingSimilarityEvaluator (\n    sentences1 =val_sts[""sentence1"" ],\n    sentences2 =val_sts[""sentence2"" ],\n    scores=[score/5 for score in val_sts[""label""]],\n    main_similarity =""cosine""\n)\nFine-Tuning an Embedding Model | 309\nThe training steps are similar to our previous examples but instead of using 'bert-\nbase-uncased',  we can use a pretrained embedding model instead:\nfrom sentence_transformers  import losses, SentenceTransformer\nfrom sentence_transformers.trainer  import SentenceTransformerTrainer\nfrom sentence_transformers.training_args  import SentenceTransformerTrainingArgu\nments\n# Define model\nembedding_model  = SentenceTransformer ('sentence-transformers/all-MiniLM-L6-v2' )\n# Loss function\ntrain_loss  = losses.MultipleNegativesRankingLoss (model=embedding_model )\n# Define the training arguments\nargs = SentenceTransformerTrainingArguments (\n    output_dir =""finetuned_embedding_model"" ,\n    num_train_epochs =1,\n    per_device_train_batch_size =32,\n    per_device_eval_batch_size =32,\n    warmup_steps =100,\n    fp16=True,\n    eval_steps =100,\n    logging_steps =100,\n)\n# Train model\ntrainer = SentenceTransformerTrainer (\n    model=embedding_model ,\n    args=args,\n    train_dataset =train_dataset ,\n    loss=train_loss ,\n    evaluator =evaluator\n)\ntrainer.train()\nEvaluating this model gives us the following score:\n# Evaluate our trained model\nevaluator (embedding_model )\n{'pearson_cosine': 0.8509553350510896,\n 'spearman_cosine': 0.8484676559567688,\n 'pearson_manhattan': 0.8503896832470704,\n 'spearman_manhattan': 0.8475760325664419,\n 'pearson_euclidean': 0.8513115442079158,\n 'spearman_euclidean': 0.8484676559567688,\n 'pearson_dot': 0.8489553386816947,\n 'spearman_dot': 0.8484676559567688,\n 'pearson_max': 0.8513115442079158,\n 'spearman_max': 0.8484676559567688}\n310 | Chapter 10: Creating Text Embedding Models",3764
117-Augmented SBERT.pdf,117-Augmented SBERT,"9Nandan Thakur et al. “ Augmented SBERT: Data augmentation method for improving bi-encoders for pairwise\nsentence scoring tasks. ” arXiv preprint arXiv:2010.08240  (2020).\nAlthough a score of 0.85 is the highest we have seen thus far, the pretrained model\nthat we used for fine-tuning was already trained on the full MNLI dataset, whereas we\nonly used 50,000 examples. It might seem redundant but this example demonstrates\nhow to fine-tune a pretrained embedding model on your own data.\nInstead of using a pretrained BERT model like 'bert-base-\nuncased'  or a possible out-of-domain model like 'all-mpnet-\nbase-v2' , you can also perform masked language modeling on the\npretrained BERT model to first adapt it to your domain. Then, you\ncan use this fine-tuned BERT model as the base for training your\nembedding model. This is a form of domain adaptation. In the next\nchapter, we will apply masked language modeling on a pretrained\nmodel.\nNote that the main difficulty of training or fine-tuning your model is finding the\nright data. With these models, we not only want to have very large datasets, but\nthe data in itself needs to be of high quality. Developing positive pairs is generally\nstraightforward but adding hard negative pairs significantly increases the difficulty of\ncreating quality data.\nAs always, restart  your notebook to free up VRAM for the following examples.\nAugmented SBERT\nA disadvantage of training or fine-tuning these embedding models is that they often\nrequire substantial training data. Many of these models are trained with more than a\nbillion sentence pairs. Extracting such a high number of sentence pairs for your use\ncase is generally not possible as in many cases, there are only a couple of thousand\nlabeled data points available.\nFortunately, there is a way to augment your data such that an embedding model\ncan be fine-tuned when there is only a little labeled data available. This procedure is\nreferred to as Augmented SBERT .9\nIn this procedure, we aim to augment the small amount of labeled data such that\nthey can be used for regular training. It makes use of the slow and more accurate\ncross-encoder architecture (BERT) to augment and label a larger set of input pairs.\nThese newly labeled pairs are then used for fine-tuning a bi-encoder (SBERT).\nAs shown in Figure 10-12 , Augmented SBERT involves the following steps:\nFine-Tuning an Embedding Model | 311\n1.Fine-tune a cross-encoder (BERT) using a small, annotated dataset (gold1.\ndataset).\n2.Create new sentence pairs.2.\n3.Label new sentence pairs with the fine-tuned cross-encoder (silver dataset).3.\n4.Train a bi-encoder (SBERT) on the extended dataset (gold + silver dataset).4.\nHere, a gold dataset  is a small but fully annotated dataset that holds the ground truth.\nA silver dataset is also fully annotated but is not necessarily the ground truth as it was\ngenerated through predictions of the cross-encoder.\nFigure 10-12. Augmented SBERT works through training a cross-encoder on a small\ngold dataset, then using that to label an unlabeled dataset to generate a larger silver\ndataset. Finally, both the gold and silver datasets are used to train the bi-encoder.\nBefore we get into the preceding steps, let’s first prepare the data. Instead of our\noriginal 50,000 documents, we take a subset of 10,000 documents to simulate a\nsetting where we have limited annotated data. As we did in our example with cosine\nsimilarity loss, give entailment a score of 1 whereas neutral and contradiction get a\nscore of 0:\nimport pandas as pd\nfrom tqdm import tqdm\nfrom datasets  import load_dataset , Dataset\nfrom sentence_transformers  import InputExample\nfrom sentence_transformers.datasets  import NoDuplicatesDataLoader\n# Prepare a small set of 10000 documents for the cross-encoder\ndataset = load_dataset (""glue"", ""mnli"", split=""train"").select(range(10_000))\nmapping = {2: 0, 1: 0, 0:1}\n# Data loader\ngold_examples  = [\n    InputExample (texts=[row[""premise"" ], row[""hypothesis"" ]], label=map\nping[row[""label""]])\n    for row in tqdm(dataset)\n]\n312 | Chapter 10: Creating Text Embedding Models\ngold_dataloader  = NoDuplicatesDataLoader (gold_examples , batch_size =32)\n# Pandas DataFrame for easier data handling\ngold = pd.DataFrame (\n    {\n    ""sentence1"" : dataset[""premise"" ],\n    ""sentence2"" : dataset[""hypothesis"" ],\n    ""label"": [mapping[label] for label in dataset[""label""]]\n    }\n)\nThis is the gold dataset since it is labeled and represents our ground truth.\nUsing this gold dataset, we train our cross-encoder (step 1):\nfrom sentence_transformers.cross_encoder  import CrossEncoder\n# Train a cross-encoder on the gold dataset\ncross_encoder  = CrossEncoder (""bert-base-uncased"" , num_labels =2)\ncross_encoder .fit(\n    train_dataloader =gold_dataloader ,\n    epochs=1,\n    show_progress_bar =True,\n    warmup_steps =100,\n    use_amp=False\n)\nAfter training our cross-encoder, we use the remaining 400,000 sentence pairs (from\nour original dataset of 50,000 sentence pairs) as our silver dataset (step 2):\n# Prepare the silver dataset by predicting labels with the cross-encoder\nsilver = load_dataset (\n    ""glue"", ""mnli"", split=""train""\n).select(range(10_000, 50_000))\npairs = list(zip(silver[""premise"" ], silver[""hypothesis"" ]))\nIf you do not have any additional unlabeled sentence pairs, you\ncan randomly sample them from your original gold dataset. To\nillustrate, you can create a new sentence pair by taking the premise\nfrom one row and the hypothesis from another. This allows you to\neasily generate 10 times as many sentence pairs that can be labeled\nwith the cross-encoder.\nThis strategy, however, likely generates significantly more dissimi‐\nlar than similar pairs. Instead, we can use a pretrained embedding\nmodel to embed all candidate sentence pairs and retrieve the\ntop-k sentences for each input sentence using semantic search.\nThis rough reranking process allows us to focus on sentence pairs\nthat are likely to be more similar. Although the sentences are still\nchosen based on an approximation since the pretrained embedding\nmodel was not trained on our data, it is much better than random\nsampling.\nFine-Tuning an Embedding Model | 313\nNote that we assume that these sentence pairs are unlabeled in this example. We will\nuse our fine-tuned cross-encoder to label these sentence pairs (step 3):\nimport numpy as np\n# Label the sentence pairs using our fine-tuned cross-encoder\noutput = cross_encoder .predict(\n    pairs, apply_softmax =True, \nshow_progress_bar =True\n)\nsilver = pd.DataFrame (\n    {\n        ""sentence1"" : silver[""premise"" ], \n        ""sentence2"" : silver[""hypothesis"" ],\n        ""label"": np.argmax(output, axis=1)\n    }\n)\nNow that we have a silver and gold dataset, we simply combine them and train our\nembedding model as we did before:\n# Combine gold + silver\ndata = pd.concat([gold, silver], ignore_index =True, axis=0)\ndata = data.drop_duplicates (subset=[""sentence1"" , ""sentence2"" ], keep=""first"")\ntrain_dataset  = Dataset.from_pandas (data, preserve_index =False)\nAs always, we need to define our evaluator:\nfrom sentence_transformers.evaluation  import EmbeddingSimilarityEvaluator\n# Create an embedding similarity evaluator for stsb\nval_sts = load_dataset (""glue"", ""stsb"", split=""validation"" )\nevaluator  = EmbeddingSimilarityEvaluator (\n    sentences1 =val_sts[""sentence1"" ],\n    sentences2 =val_sts[""sentence2"" ],\n    scores=[score/5 for score in val_sts[""label""]],\n    main_similarity =""cosine""\n)\nWe train the model the same as before except now we use the augmented dataset:\nfrom sentence_transformers  import losses, SentenceTransformer\nfrom sentence_transformers.trainer  import SentenceTransformerTrainer\nfrom sentence_transformers.training_args  import SentenceTransformerTrainingArgu\nments\n# Define model\nembedding_model  = SentenceTransformer (""bert-base-uncased"" )\n# Loss function\ntrain_loss  = losses.CosineSimilarityLoss (model=embedding_model )\n# Define the training arguments\nargs = SentenceTransformerTrainingArguments (\n314 | Chapter 10: Creating Text Embedding Models\n    output_dir =""augmented_embedding_model"" ,\n    num_train_epochs =1,\n    per_device_train_batch_size =32,\n    per_device_eval_batch_size =32,\n    warmup_steps =100,\n    fp16=True,\n    eval_steps =100,\n    logging_steps =100,\n)\n# Train model\ntrainer = SentenceTransformerTrainer (\n    model=embedding_model ,\n    args=args,\n    train_dataset =train_dataset ,\n    loss=train_loss ,\n    evaluator =evaluator\n)\ntrainer.train()\nFinally, we evaluate the model:\nevaluator (embedding_model )\n{'pearson_cosine': 0.7101597020018693,\n 'spearman_cosine': 0.7210536464320728,\n 'pearson_manhattan': 0.7296749443525249,\n 'spearman_manhattan': 0.7284184255293913,\n 'pearson_euclidean': 0.7293097297208753,\n 'spearman_euclidean': 0.7282830906742256,\n 'pearson_dot': 0.6746605824703588,\n 'spearman_dot': 0.6754486790570754,\n 'pearson_max': 0.7296749443525249,\n 'spearman_max': 0.7284184255293913}\nThe original cosine similarity loss example had a score of 0.72 with the full dataset.\nUsing only 20% of that data, we managed to get a score of 0.71!\nThis method allows us to increase the size of datasets that you already have available\nwithout the need to manually label hundreds of thousands of sentence pairs. Y ou can\ntest the quality of your silver data by also training your embedding model only on the\ngold dataset. The difference in performance indicates how much your silver dataset\npotentially adds to the quality of the model.\nY ou can restart  your notebook a final time for the last example, namely unsupervised\nlearning.\nFine-Tuning an Embedding Model | 315",9765
118-Unsupervised Learning.pdf,118-Unsupervised Learning,,0
119-Transformer-Based Sequential Denoising Auto-Encoder.pdf,119-Transformer-Based Sequential Denoising Auto-Encoder,"10Tianyu Gao, Xingcheng Y ao, and Danqi Chen. “SimCSE: Simple contrastive learning of sentence embeddings. ”\narXiv preprint arXiv:2104.08821  (2021).\n11Fredrik Carlsson et al. “Semantic re-tuning with Contrastive Tension. ” International Conference on Learning\nRepresentations, 2021 . 2021.\n12Kexin Wang, Nils Reimers, and Iryna Gurevych. “TSDAE: Using Transformer-based Sequential Denoising\nAuto-Encoder for unsupervised sentence embedding learning. ” arXiv preprint arXiv:2104.06979  (2021).\n13Kexin Wang et al. “GPL: Generative Pseudo Labeling for unsupervised domain adaptation of dense retrieval. ”\narXiv preprint arXiv:2112.07577  (2021).Unsupervised Learning\nTo create an embedding model, we typically need labeled data. However, not all\nreal-world datasets come with a nice set of labels that we can use. We instead\nlook for techniques to train the model without any predetermined labels—unsuper‐\nvised learning. Many approaches exist, like Simple Contrastive Learning of Sentence\nEmbeddings (SimCSE) ,10 Contrastive Tension (CT) ,11 Transformer-based Sequential\nDenoising Auto-Encoder (TSDAE) ,12 and Generative Pseudo-Labeling (GPL) .13\nIn this section, we will focus on TSDAE, as it has shown great performance on\nunsupervised tasks as well as domain adaptation.\nTransformer-Based Sequential Denoising Auto-Encoder\nTSDAE  is a very elegant approach to creating an embedding model with unsuper‐\nvised learning. The method assumes that we have no labeled data at all and does not\nrequire us to artificially create labels.\nThe underlying idea of TSDAE is that we add noise to the input sentence by remov‐\ning a certain percentage of words from it. This “damaged” sentence is put through an\nencoder, with a pooling layer on top of it, to map it to a sentence embedding. From\nthis sentence embedding, a decoder tries to reconstruct the original sentence from the\n“damaged” sentence but without the artificial noise. The main concept here is that\nthe more accurate the sentence embedding is, the more accurate the reconstructed\nsentence will be.\nThis method is very similar to masked language modeling, where we try to recon‐\nstruct and learn certain masked words. Here, instead of reconstructing masked\nwords, we try to reconstruct the entire sentence.\nAfter training, we can use the encoder to generate embeddings from text since the\ndecoder is only used for judging whether the embeddings can accurately reconstruct\nthe original sentence ( Figure 10-13 ).\n316 | Chapter 10: Creating Text Embedding Models\nFigure 10-13. TSDAE randomly removes words from an input sentence that is passed\nthrough an encoder to generate a sentence embedding. From this sentence embedding,\nthe original sentence is reconstructed.\nSince we only need a bunch of sentences without any labels, training this model is\nstraightforward. We start by downloading an external tokenizer, which is used for the\ndenoising procedure:\n# Download additional tokenizer\nimport nltk\nnltk.download (""punkt"")\nThen, we create flat sentences from our data and remove any labels that we have to\nmimic an unsupervised setting:\nfrom tqdm import tqdm\nfrom datasets  import Dataset, load_dataset\nfrom sentence_transformers.datasets  import DenoisingAutoEncoderDataset\n# Create a flat list of sentences\nmnli = load_dataset (""glue"", ""mnli"", split=""train"").select(range(25_000))\nflat_sentences  = mnli[""premise"" ] + mnli[""hypothesis"" ]\n# Add noise to our input data\ndamaged_data  = DenoisingAutoEncoderDataset (list(set(flat_sentences )))\n# Create dataset\ntrain_dataset  = {""damaged_sentence"" : [], ""original_sentence"" : []}\nUnsupervised Learning | 317\nfor data in tqdm(damaged_data ):\n    train_dataset [""damaged_sentence"" ].append(data.texts[0])\n    train_dataset [""original_sentence"" ].append(data.texts[1])\ntrain_dataset  = Dataset.from_dict (train_dataset )\nThis creates a dataset of 50,000 sentences. When we inspect the data, notice that the\nfirst sentence is the damaged sentence and the second sentence the original:\ntrain_dataset [0]\n{'damaged_sentence': 'Grim jaws are.',\n 'original_sentence': 'Grim faces and hardened jaws are not people-friendly.'}\nThe first sentence shows the “noisy” data whereas the second shows the original input\nsentence. After creating our data, we define our evaluator as before:\nfrom sentence_transformers.evaluation  import EmbeddingSimilarityEvaluator\n# Create an embedding similarity evaluator for stsb\nval_sts = load_dataset (""glue"", ""stsb"", split=""validation"" )\nevaluator  = EmbeddingSimilarityEvaluator (\n    sentences1 =val_sts[""sentence1"" ],\n    sentences2 =val_sts[""sentence2"" ],\n    scores=[score/5 for score in val_sts[""label""]],\n    main_similarity =""cosine""\n)\nNext, we run the training as before but with the [CLS]  token as the pooling strategy\ninstead of the mean pooling of the token embeddings. In the TSDAE paper, this was\nshown to be more effective since mean pooling loses the position information, which\nis not the case when using the [CLS]  token:\nfrom sentence_transformers  import models, SentenceTransformer\n# Create your embedding model\nword_embedding_model  = models.Transformer (""bert-base-uncased"" )\npooling_model  = models.Pooling(word_embedding_model .get_word_embedding_dimen\nsion(), ""cls"")\nembedding_model  = SentenceTransformer (modules=[word_embedding_model , pool\ning_model ])\nUsing our sentence pairs, we will need a loss function that attempts to reconstruct the\noriginal sentence using the noise sentence, namely DenoisingAutoEncoderLoss . By\ndoing so, it will learn how to accurately represent the data. It is similar to masking but\nwithout knowing where the actual masks are.\nMoreover, we tie the parameters of both models. Instead of having separate weights\nfor the encoder’s embedding layer and the decoder’s output layer, they share the same\nweights. This means that any updates to the weights in one layer will be reflected in\nthe other layer as well:\n318 | Chapter 10: Creating Text Embedding Models\nfrom sentence_transformers  import losses\n# Use the denoising auto-encoder loss\ntrain_loss  = losses.DenoisingAutoEncoderLoss (\n    embedding_model , tie_encoder_decoder =True\n)\ntrain_loss .decoder = train_loss .decoder.to(""cuda"")\nFinally, training our model works the same as we have seen several times before but\nwe lower the batch size as memory increases with this loss function:\nfrom sentence_transformers.trainer  import SentenceTransformerTrainer\nfrom sentence_transformers.training_args  import SentenceTransformerTrainingArgu\nments\n# Define the training arguments\nargs = SentenceTransformerTrainingArguments (\n    output_dir =""tsdae_embedding_model"" ,\n    num_train_epochs =1,\n    per_device_train_batch_size =16,\n    per_device_eval_batch_size =16,\n    warmup_steps =100,\n    fp16=True,\n    eval_steps =100,\n    logging_steps =100,\n)\n# Train model\ntrainer = SentenceTransformerTrainer (\n    model=embedding_model ,\n    args=args,\n    train_dataset =train_dataset ,\n    loss=train_loss ,\n    evaluator =evaluator\n)\ntrainer.train()\nAfter training, we evaluate our model to explore how well such an unsupervised\ntechnique performs:\n# Evaluate our trained model\nevaluator (embedding_model )\n{'pearson_cosine': 0.6991809700971775,\n 'spearman_cosine': 0.713693213167873,\n 'pearson_manhattan': 0.7152343356643568,\n 'spearman_manhattan': 0.7201441944880915,\n 'pearson_euclidean': 0.7151142243297436,\n 'spearman_euclidean': 0.7202291660769805,\n 'pearson_dot': 0.5198066451871277,\n 'spearman_dot': 0.5104025515225046,\n 'pearson_max': 0.7152343356643568,\n 'spearman_max': 0.7202291660769805}\nUnsupervised Learning | 319",7700
120-Summary.pdf,120-Summary,"After fitting our model, we got a score of 0.70, which is quite impressive considering\nwe did all this training with unlabeled data.\nUsing TSDAE for Domain Adaptation\nWhen you have very little or no labeled data available, you typically use unsupervised\nlearning to create your text embedding model. However, unsupervised techniques\nare generally outperformed by supervised techniques and have difficulty learning\ndomain-specific concepts.\nThis is where domain adaptation  comes in. Its goal is to update existing embedding\nmodels to a specific textual domain that contains different subjects from the source\ndomain. Figure 10-14  demonstrates how domains can differ in content. The target\ndomain, or out-domain, generally contains words and subjects that were not found in\nthe source domain or in-domain.\nFigure 10-14. In domain adaptation, the aim is to create and generalize an embedding\nmodel from one domain to another.\nOne method for domain adaptation is called adaptive pretraining . Y ou start by pre‐\ntraining your domain-specific corpus using an unsupervised technique, such as the\npreviously discussed TSDAE or masked language modeling. Then, as illustrated in\nFigure 10-15 , you fine-tune that model using a training dataset that can be either\noutside or in your target domain. Although data from the target domain is preferred,\nout-domain data also works since we started with unsupervised training on the target\ndomain.\n320 | Chapter 10: Creating Text Embedding Models\nFigure 10-15. Domain adaptation can be performed with adaptive pretraining and\nadaptive fine-tuning.\nUsing everything you have learned in this chapter, you should be able to reproduce\nthis pipeline! First, you can start with TSDAE to train an embedding model on\nyour target domain and then fine-tune it using either general supervised training or\nAugmented SBERT.\nSummary\nIn this chapter, we looked at creating and fine-tuning embedding models through\nvarious tasks. We discussed the concept of embeddings and their role in representing\ntextual data in a numerical format. We then explored the foundational technique of\nmany embedding models, namely contrastive learning, which learns primarily from\n(dis)similar pairs of documents.\nUsing a popular embedding framework, sentence-transformers , we then created\nembedding models using a pretrained BERT model while exploring different loss\nfunctions, such as cosine similarity loss and MNR loss. We discussed how the collec‐\ntion of (dis)similar pairs or triples of documents is vital to the performance of the\nresulting model.\nIn the sections that followed, we explored techniques for fine-tuning embedding\nmodels. Both supervised and unsupervised techniques were discussed such as Aug‐\nmented SBERT and TSDAE for domain adaptation. Compared to creating an embed‐\nding model, fine-tuning generally needs less data and is a great way to adapt existing\nembedding models to your domain.\nIn the next chapter, methods for fine-tuning representations for classification will be\ndiscussed. Both BERT models and embedding models will make an appearance as\nwell as a wide range of fine-tuning techniques.\nSummary | 321",3188
121-Chapter 11. Fine-Tuning Representation Models for Classification.pdf,121-Chapter 11. Fine-Tuning Representation Models for Classification,,0
122-Supervised Classification.pdf,122-Supervised Classification,"CHAPTER 11\nFine-Tuning Representation\nModels for Classification\nIn Chapter 4 , we used pretrained models to classify our text. We kept the pretrained\nmodels as they were without any modifications to them. This might make you\nwonder, what happens if we were to fine-tune them?\nIf we have sufficient data, fine-tuning tends to lead to some of the best-performing\nmodels possible. In this chapter, we will go through several methods and applications\nfor fine-tuning BERT models. “Supervised Classification”  on page 323 demonstrates\nthe general process of fine-tuning a classification model. Then, in “Few-Shot Classifi‐\ncation”  on page 333, we look at SetFit, which is a method for efficiently fine-tuning\na high-performing model using a small number of training examples. In “Continued\nPretraining with Masked Language Modeling” on page 340, we will explore how to\ncontinue training a pretrained model. Lastly, classification on a token level is explored\nin “Named-Entity Recognition” on page 345 .\nWe will focus on nongenerative tasks, as generative models will be covered in\nChapter 12 .\nSupervised Classification\nIn Chapter 4 , we explored supervised classification tasks by leveraging pretrained\nrepresentation models that were either trained to predict sentiment (task-specific\nmodel) or to generate embeddings (embedding model), as shown in Figure 11-1 .\n323\nFigure 11-1. In Chapter 4 , we used pretrained models to perform classification  without\nupdating their weight. These  models were kept “frozen. ”\nBoth models were kept frozen (nontrainable) to showcase the potential of leveraging\npretrained models for classification tasks. The embedding model uses a separate\ntrainable classification head (classifier) to predict the sentiment of movie reviews.\nIn this section, we will take a similar approach but allow both the model and the\nclassification head to be updated during training. As shown in Figure 11-2 , instead\nof using an embedding model, we will fine-tune a pretrained BERT model to create\na task-specific model similar to the one we used in Chapter 2 . Compared to the\nembedding model approach, we will fine-tune both the representation model and the\nclassification head as a single architecture.\nFigure 11-2. Compared to the “frozen” architecture, we instead train both the pretrained\nBERT model and the classification  head. A backward pass will start at the classification\nhead and go through BERT.\n324 | Chapter 11: Fine-Tuning Representation Models for Classification",2529
123-Fine-Tuning a Pretrained BERT Model.pdf,123-Fine-Tuning a Pretrained BERT Model,"To do so, instead of freezing the model, we allow it to be trainable and update its\nparameters during training. As illustrated in Figure 11-3 , we will use a pretrained\nBERT model and add a neural network as a classification head, both of which will be\nfine-tuned for classification.\nFigure 11-3. The architecture of a task-specific  model. It contains a pretrained represen‐\ntation model (e.g., BERT) with an additional classification  head for the specific  task.\nIn practice, this means that the pretrained BERT model and the classification head\nare updated jointly. Instead of independent processes, they learn from one another\nand allow for more accurate representations.\nFine-Tuning a Pretrained BERT Model\nWe will be using the same dataset we used in Chapter 4  to fine-tune our model,\nnamely the Rotten Tomatoes dataset, which contains 5,331 positive and 5,331 nega‐\ntive movie reviews from Rotten Tomatoes:\nfrom datasets  import load_dataset\n# Prepare data and splits\ntomatoes  = load_dataset (""rotten_tomatoes"" )\ntrain_data , test_data  = tomatoes [""train""], tomatoes [""test""]\nSupervised Classification  | 325\n1Jacob Devlin et al. “BERT: Pre-training of deep bidirectional transformers for language understanding. ” arXiv\npreprint arXiv:1810.04805  (2018).The first step in our classification task is to select the underlying model we want to\nuse. We use ""bert-base-cased"" , which was pretrained on the English Wikipedia as\nwell as a large dataset consisting of unpublished books.1\nWe define the number of labels that we want to predict beforehand. This is necessary\nto create the feedforward neural network that is applied on top of our pretrained\nmodel:\nfrom transformers  import AutoTokenizer , AutoModelForSequenceClassification\n# Load model and tokenizer\nmodel_id  = ""bert-base-cased""\nmodel = AutoModelForSequenceClassification .from_pretrained (\n    model_id , num_labels =2\n)\ntokenizer  = AutoTokenizer .from_pretrained (model_id )\nNext, we will tokenize our data:\nfrom transformers  import DataCollatorWithPadding\n# Pad to the longest sequence in the batch\ndata_collator  = DataCollatorWithPadding (tokenizer =tokenizer )\ndef preprocess_function (examples ):\n   """"""Tokenize input data""""""\n   return tokenizer (examples [""text""], truncation =True)\n# Tokenize train/test data\ntokenized_train  = train_data .map(preprocess_function , batched=True)\ntokenized_test  = test_data .map(preprocess_function , batched=True)\nBefore creating the Trainer , we will want to prepare a special DataCollator . A\nDataCollator  is a class that helps us build batches of data but also allows us to apply\ndata augmentation.\nDuring this process of tokenization, and as shown in Chapter 9 , we will add padding\nto the input text to create equally sized representations. We use DataCollatorWith\nPadding  for that.\nOf course, an example would not be complete without defining some metrics:\nimport numpy as np\nfrom datasets  import load_metric\ndef compute_metrics (eval_pred ):\n   """"""Calculate F1 score""""""\n   logits, labels = eval_pred\n326 | Chapter 11: Fine-Tuning Representation Models for Classification\n   predictions  = np.argmax(logits, axis=-1)\n   load_f1 = load_metric (""f1"")\n   f1 = load_f1.compute(predictions =predictions , references =labels)[""f1""]\n   return {""f1"": f1}\nWith compute_metrics  we can define any number of metrics that we are interested\nin and that can be printed out or logged during training. This is especially helpful\nduring training as it allows for detecting overfitting behavior.\nNext, we instantiate our Trainer :\nfrom transformers  import TrainingArguments , Trainer\n# Training arguments for parameter tuning\ntraining_args  = TrainingArguments (\n   ""model"",\n   learning_rate =2e-5,\n   per_device_train_batch_size =16,\n   per_device_eval_batch_size =16,\n   num_train_epochs =1,\n   weight_decay =0.01,\n   save_strategy =""epoch"",\n   report_to =""none""\n)\n# Trainer which executes the training process\ntrainer = Trainer(\n   model=model,\n   args=training_args ,\n   train_dataset =tokenized_train ,\n   eval_dataset =tokenized_test ,\n   tokenizer =tokenizer ,\n   data_collator =data_collator ,\n   compute_metrics =compute_metrics ,\n)\nThe TrainingArguments  class defines hyperparameters we want to tune, such as the\nlearning rate and how many epochs (rounds) we want to train. The Trainer  is used\nto execute the training process.\nFinally, we can train our model and evaluate it:\ntrainer.evaluate ()\n{'eval_loss': 0.3663691282272339,\n 'eval_f1': 0.8492366412213741,\n 'eval_runtime': 4.5792,\n 'eval_samples_per_second': 232.791,\n 'eval_steps_per_second': 14.631,\n 'epoch': 1.0}\nSupervised Classification  | 327",4722
124-Freezing Layers.pdf,124-Freezing Layers,"We get an F1 score of 0.85, which is quite a bit higher than the task-specific model\nwe used in Chapter 4 , which resulted in an F1 score of 0.80. It shows that fine-tuning\na model yourself can be more advantageous than using a pretrained model. It only\ncosts us a couple of minutes to train.\nFreezing Layers\nTo further showcase the importance of training the entire network, the next example\nwill demonstrate how you can use Hugging Face Transformers to freeze certain layers\nof your network.\nWe will freeze the main BERT model and allow only updates to pass through the\nclassification head. This will be a great comparison as we will keep everything the\nsame, except for freezing specific layers.\nTo start, let’s reinitialize our model so we can start from scratch:\n# Load model and tokenizer\nmodel = AutoModelForSequenceClassification .from_pretrained (\n    model_id , num_labels =2\n)\ntokenizer  = AutoTokenizer .from_pretrained (model_id )\nOur pretrained BERT model contains a lot of layers that we can potentially freeze.\nInspecting these layers gives insight into the structure of the network and what we\nmight want to freeze:\n# Print layer names\nfor name, param in model.named_parameters ():\n    print(name)\nbert.embeddings.word_embeddings.weight\nbert.embeddings.position_embeddings.weight\nbert.embeddings.token_type_embeddings.weight\nbert.embeddings.LayerNorm.weight\nbert.embeddings.LayerNorm.bias\nbert.encoder.layer.0.attention.self.query.weight\nbert.encoder.layer.0.attention.self.query.bias\n...\nbert.encoder.layer.11.output.LayerNorm.weight\nbert.encoder.layer.11.output.LayerNorm.bias\nbert.pooler.dense.weight\nbert.pooler.dense.bias\nclassifier.weight\nclassifier.bias\n328 | Chapter 11: Fine-Tuning Representation Models for Classification\nThere are 12 (0–11) encoder blocks consisting of attention heads, dense networks,\nand layer normalization. We further illustrate this architecture in Figure 11-4  to\ndemonstrate everything that could be potentially frozen. On top of that, we have our\nclassification head.\nFigure 11-4. The basic architecture of BERT with the additional classification  head.\nWe could choose to only freeze certain layers to speed up computing but still allow\nthe main model to learn from the classification task. Generally, we want frozen layers\nto be followed by trainable layers.\nWe are going to freeze everything except for the classification head as we did in\nChapter 2 :\nfor name, param in model.named_parameters ():\n     # Trainable classification head\n     if name.startswith (""classifier"" ):\n        param.requires_grad  = True\n      # Freeze everything else\n     else:\n        param.requires_grad  = False\nAs shown in Figure 11-5 , we have frozen everything except for the feedforward\nneural network, which is our classification head.\nSupervised Classification  | 329\nFigure 11-5. We fully freeze all encoder blocks and embedding layers such that the BERT\nmodel does not learn new representations during fine-tuning.\nNow that we have successfully frozen everything but the classification head, we can\nmove on to train our model:\nfrom transformers  import TrainingArguments , Trainer\n# Trainer which executes the training process\ntrainer = Trainer(\n   model=model,\n   args=training_args ,\n   train_dataset =tokenized_train ,\n   eval_dataset =tokenized_test ,\n   tokenizer =tokenizer ,\n   data_collator =data_collator ,\n   compute_metrics =compute_metrics ,\n)\ntrainer.train()\nY ou might notice that training has become much faster. That is because we are\nonly training the classification head, which provides us with a significant speedup\ncompared to fine-tuning the entire model:\ntrainer.evaluate ()\n{'eval_loss': 0.6821751594543457,\n 'eval_f1': 0.6331058020477816,\n 'eval_runtime': 4.0175,\n 'eval_samples_per_second': 265.337,\n 'eval_steps_per_second': 16.677,\n 'epoch': 1.0}\n330 | Chapter 11: Fine-Tuning Representation Models for Classification\nWhen we evaluate the model, we only get an F1 score of 0.63, which is quite a bit\nlower compared to our original 0.85 score. Instead of freezing nearly all layers, let’s\nfreeze everything up until encoder block 10 as illustrated in Figure 11-6 , and see\nhow it affects performance. A major benefit is that this reduces computation but still\nallows updates to flow through part of the pretrained model:\nFigure 11-6. We freeze the first 10 encoder blocks of our BERT model. Everything else is\ntrainable and will be fine-tuned.\n# Load model\nmodel_id  = ""bert-base-cased""\nmodel = AutoModelForSequenceClassification .from_pretrained (\n    model_id , num_labels =2\n)\ntokenizer  = AutoTokenizer .from_pretrained (model_id )\n# Encoder block 11 starts at index 165 and\n# we freeze everything before that block\nfor index, (name, param) in enumerate (model.named_parameters ()):    \n    if index < 165:\n        param.requires_grad  = False\n# Trainer which executes the training process\ntrainer = Trainer(\n   model=model,\n   args=training_args ,\n   train_dataset =tokenized_train ,\n   eval_dataset =tokenized_test ,\nSupervised Classification  | 331\n   tokenizer =tokenizer ,\n   data_collator =data_collator ,\n   compute_metrics =compute_metrics ,\n)\ntrainer.train()\nAfter training, we evaluate the results:\ntrainer.evaluate ()\n{'eval_loss' : 0.40812647342681885 ,\n 'eval_f1' : 0.8,\n 'eval_runtime' : 3.7125,\n 'eval_samples_per_second' : 287.137,\n 'eval_steps_per_second' : 18.047,\n 'epoch': 1.0}\nWe got an F1 score of 0.8, which is much higher than our previous score of 0.63 when\nfreezing all layers. It demonstrates that although we generally want to train as many\nlayers as possible, you can get away with training less if you do not have the necessary\ncomputing power.\nTo further illustrate this effect, we tested the effect of iteratively freezing encoder\nblocks and fine-tuning them as we did thus far. As shown in Figure 11-7 , training\nonly the first five encoder blocks (red vertical line) is enough to almost reach the\nperformance of training all encoder blocks.\nFigure 11-7. The effect  of freezing certain encoder blocks on the performance of the\nmodel. Training more blocks leads to improved performance but stabilizes early on.\n332 | Chapter 11: Fine-Tuning Representation Models for Classification",6320
125-Few-Shot Classification.pdf,125-Few-Shot Classification,,0
126-SetFit Efficient Fine-Tuning with Few Training Examples.pdf,126-SetFit Efficient Fine-Tuning with Few Training Examples,"2Lewis Tunstall et al. “Efficient few-shot learning without prompts. ” arXiv preprint arXiv:2209.11055  (2022).\nWhen you are training for multiple epochs, the difference (in\ntraining time and resources) between freezing and not freezing\noften becomes larger. It is therefore advised to play around with a\nbalance that works for you.\nFew-Shot Classification\nFew-shot classification is a technique within supervised classification where you have\na classifier learn target labels based on only a few labeled examples. This technique\nis great when you have a classification task but do not have many labeled data points\nreadily available. In other words, this method allows you to label a few high-quality\ndata points per class on which to train the model. This idea of using a few labeled\ndata points for training your model is shown in Figure 11-8 .\nFigure 11-8. In few-shot classification,  we only use a few labeled data points to learn\nfrom.\nSetFit: Efficient  Fine-Tuning with Few Training Examples\nTo perform few-shot text classification, we use an efficient framework called SetFit .2 It\nis built on top of the architecture of sentence-transformers  to generate high-quality\ntextual representations that are updated during training. Only a few labeled examples\nare needed for this framework to be competitive with fine-tuning a BERT-like model\non a large, labeled dataset as we explored in the previous example.\nThe underlying algorithm of SetFit consists of three steps:\n1. Sampling training data\nBased on in-class and out-class selection of labeled data it generates positive\n(similar) and negative (dissimilar) pairs of sentences\nFew-Shot Classification  | 333\n2. Fine-tuning embeddings\nFine-tuning a pretrained embedding model based on the previously generated\ntraining data\n3. Training a classifier\nCreate a classification head on top of the embedding model and train it using the\npreviously generated training data\nBefore fine-tuning an embedding model, we need to generate training data. The\nmodel assumes the training data to be samples of positive (similar) and negative\n(dissimilar) pairs of sentences. However, when we are dealing with a classification\ntask, our input data is generally not labeled as such.\nSay, for example, we have the training dataset in Figure 11-9  that classifies text into\ntwo categories: text about programming languages, and text about pets.\nFigure 11-9. Data in two classes: text about programming languages and text about pets.\nIn step 1, SetFit handles this problem by generating the necessary data based on\nin-class and out-class selection as we illustrate in Figure 11-10 . For example, when we\nhave 16 sentences about sports, we can create 16 * (16 – 1) / 2 = 120 pairs that we\nlabel as positive  pairs. We can use this process to generate negative  pairs by collecting\npairs from different classes.\nFigure 11-10. Step 1: sampling training data. We assume sentences within a class are\nsimilar and create positive pairs while sentences in different  classes become negative\npairs.\n334 | Chapter 11: Fine-Tuning Representation Models for Classification\nIn step 2, we can use the generated sentence pairs to fine-tune the embedding model.\nThis leverages a method called contrastive learning to fine-tune a pretrained BERT\nmodel. As we reviewed in Chapter 10 , contrastive learning allows accurate sentence\nembeddings to be learned from pairs of similar (positive) and dissimilar (negative)\nsentences.\nSince we generated these pairs in the previous step, we can use them to fine-tune\na SentenceTransformers  model. Although we have discussed contrastive learning\nbefore, we again illustrate the method in Figure 11-11  as a refresher.\nFigure 11-11. Step 2: Fine-tuning a SentenceTransformers  model. Using contrastive\nlearning, embeddings are learned from positive and negative sentence pairs.\nThe goal of fine-tuning this embedding model is that it can create embeddings\nthat are tuned to the classification task. The relevance of the classes, and their rela‐\ntive meaning, are distilled into the embeddings through fine-tuning the embedding\nmodel.\nFew-Shot Classification  | 335\nIn step 3, we generate embeddings for all sentences and use those as the input of a\nclassifier. We can use the fine-tuned SentenceTransformers  model to convert our\nsentences into embeddings that we can use as features. The classifier learns from\nour fine-tuned embeddings to accurately predict unseen sentences. This last step is\nillustrated in Figure 11-12 .\nFigure 11-12. Step 3: Training a classifier.  The classifier  can be any scikit-learn model or\na classification  head.\nWhen we put all the steps together, we get an efficient and elegant pipeline for\nperforming classification when you only have a few labels per class. It cleverly makes\nuse of the idea that we have labeled data, although not in the way that we would like\nit. The three steps together are illustrated in Figure 11-13  to give a single overview of\nthe entire procedure.\nFirst, sentence pairs are generated based on in-class and out-class selection. Second,\nthe sentence pairs are used to fine-tune a pretrained SentenceTransformer  model.\nThird, the sentences are embedded with the fine-tuned model on which a classifier is\ntrained to predict the classes.\n336 | Chapter 11: Fine-Tuning Representation Models for Classification",5405
127-Fine-Tuning for Few-Shot Classification.pdf,127-Fine-Tuning for Few-Shot Classification,"Figure 11-13. The three main steps of SetFit.\nFine-Tuning for Few-Shot Classification\nWe previously trained on a dataset containing roughly 8,500 movie reviews. However,\nsince this is a few-shot setting, we will only sample 16 examples per class. With two\nclasses, we will only have 32 documents to train on compared to the 8,500 movie\nreviews we used before!\nfrom setfit import sample_dataset\n# We simulate a few-shot setting by sampling 16 examples per class\nsampled_train_data  = sample_dataset (tomatoes [""train""], num_samples =16)\nAfter sampling the data, we choose a pretrained SentenceTransformer  model to fine-\ntune. The official documentation contains an overview of pretrained SentenceTrans\nformer  models  from which we are going to be using ""sentence-transformers/\nall-mpnet-base-v2"" . It is one of the best-performing models on the MTEB leader‐\nboard , which shows the performance of embedding models across a variety of tasks:\nfrom setfit import SetFitModel\n# Load a pretrained SentenceTransformer model\nmodel = SetFitModel .from_pretrained (""sentence-transformers/all-mpnet-base-v2"" )\nAfter loading in the pretrained SentenceTransformer  model, we can start defining\nour SetFitTrainer . By default, a logistic regression model is chosen as the classifier\nto train.\nSimilar to what we did with Hugging Face Transformers, we can use the trainer to\ndefine and play around with relevant parameters. For example, we set the num_epochs\nto 3 so that contrastive learning will be performed for three epochs:\nFew-Shot Classification  | 337\nfrom setfit import TrainingArguments  as SetFitTrainingArguments\nfrom setfit import Trainer as SetFitTrainer\n# Define training arguments\nargs = SetFitTrainingArguments (\n    num_epochs =3, # The number of epochs to use for contrastive learning\n    num_iterations =20  # The number of text pairs to generate\n)\nargs.eval_strategy  = args.evaluation_strategy\n# Create trainer\ntrainer = SetFitTrainer (\n    model=model,\n    args=args,\n    train_dataset =sampled_train_data ,\n    eval_dataset =test_data ,\n    metric=""f1""\n)\nWe only need to call train  to start the training loop. When we do, we should get the\nfollowing output:\n# Training loop\ntrainer.train()\n***** Running training *****\n  Num unique pairs = 1280\n  Batch size = 16\n  Num epochs = 3\n  Total optimization steps = 240\nNotice that the output mentions that 1,280 sentence pairs were generated for fine-\ntuning the SentenceTransformer  model. As a default, 20 sentence pair combinations\nare generated for each sample in our data, which would be 20 * 32 = 680 samples.\nWe will have to multiply this value by 2 for each positive and negative pair generated,\n680 * 2 = 1,280 sentence pairs. Generating 1,280 sentence pairs is quite impressive\nconsidering we only had 32 labeled sentences to start with!\n338 | Chapter 11: Fine-Tuning Representation Models for Classification\nWhen we do not specifically define a classification head, by default\na logistic regression is used. If we would like to specify a classifi‐\ncation head ourselves, we can do so by specifying the following\nmodel in SetFitTrainer :\n# Load a SetFit model from Hub\nmodel = SetFitModel .from_pretrained (\n    ""sentence-transformers/all-mpnet-base-v2"" ,\n    use_differentiable_head =True,\n    head_params ={""out_features"" : num_classes },\n)\n# Create trainer\ntrainer = SetFitTrainer (\n    model=model,\n    ...\n)\nHere, num_classes  refers to the number of classes that we want to\npredict.\nNext, let’s evaluate the model to get a feeling of its performance:\n# Evaluate the model on our test data\ntrainer.evaluate ()\n{'f1': 0.8363988383349468}\nWith only 32 labeled documents, we get an F1 score of 0.85. Considering that the\nmodel was trained on a tiny subset of the original data, this is very impressive!\nMoreover, in Chapter 2 , we got the same performance but instead trained a logistic\nregression model on the embeddings of the full data. Thus, this pipeline demonstrates\nthe potential of taking the time to label just a few instances.\nNot only can SetFit perform few-shot classification tasks, but it also\nhas support for when you have no labels at all, also called zero-shot\nclassification. SetFit generates synthetic examples from the label\nnames to resemble the classification task and then trains a SetFit\nmodel on them. For example, if the target labels are “happy” and\n“sad, ” then synthetic data could be “The example is happy” and\n“This example is sad. ”\nFew-Shot Classification  | 339",4552
128-Continued Pretraining with Masked Language Modeling.pdf,128-Continued Pretraining with Masked Language Modeling,"Continued Pretraining with Masked Language Modeling\nIn the examples thus far, we leveraged a pretrained model and fine-tuned it to\nperform classification. This process describes a two-step process: first pretraining a\nmodel (which was already done for us) and then fine-tuning it for a particular task.\nWe illustrate this process in Figure 11-14 .\nFigure 11-14. To fine-tune  the model on a target task—for example, classification—we\neither start with pretraining a BERT model or use a pretrained one.\nThis two-step approach is typically used throughout many applications. It has its\nlimitations when faced with domain-specific data. The pretrained model is often\ntrained on very general data, like Wikipedia pages, and might not be tuned to your\ndomain-specific words.\nInstead of adopting this two-step approach, we can squeeze another step between\nthem, namely continue pretraining an already pretrained BERT model. In other\nwords, we can simply continue training the BERT model using masked language\nmodeling (MLM) but instead use data from our domain. It is like going from a\ngeneral BERT model to a BioBERT model specialized for the medical domain, to a\nfine-tuned BioBERT model to classify medication.\n340 | Chapter 11: Fine-Tuning Representation Models for Classification\n3Chi Sun et al. “How to fine-tune GERT for text classification?” Chinese Computational Linguistics: 18th\nChina National Conference , CCL 2019, Kunming, China, October 18–20, 2019, proceedings 18. Springer\nInternational Publishing, 2019.This will update the subword representations to be more tuned toward words it\nwould not have seen before. This process is illustrated in Figure 11-15  and dem‐\nonstrates how this additional step updates a masked language modeling task. Con‐\ntinuing pretraining on a pretrained BERT model has been shown to improve the\nperformance of models in classification tasks and is a worthwhile addition to the\nfine-tuning pipeline.3\nFigure 11-15. Instead of a two-step approach, we can add another step that continues\nto pretrain the pretrained model before fine-tuning  it on the target task. Notice how the\nmasks were filled  with abstract concepts in 1 while they were filled  with movie-specific\nconcepts in 2.\nInstead of having to pretrain an entire model from scratch, we can simply continue\npretraining before fine-tuning it for classification. This also helps the model to adapt\nto a certain domain or even the lingo of a specific organization. The genealogy of\nmodels a company might want to adopt is further illustrated in Figure 11-16 .\nContinued Pretraining with Masked Language Modeling | 341\nFigure 11-16. The three-step approach illustrated for specific  use cases.\nIn this example, we will demonstrate how to apply step 2 and continue pretraining an\nalready pretrained BERT model. We use the same data that we started with, namely\nthe Rotten Tomatoes reviews.\nWe start by loading the ""bert-base-cased""  model we have used thus far and prepare\nit for MLM:\nfrom transformers  import AutoTokenizer , AutoModelForMaskedLM\n# Load model for masked language modeling (MLM)\nmodel = AutoModelForMaskedLM .from_pretrained (""bert-base-cased"" )\ntokenizer  = AutoTokenizer .from_pretrained (""bert-base-cased"" )\nWe need to tokenize the raw sentences. We will also remove the labels since this is not\na supervised task:\ndef preprocess_function (examples ):\n   return tokenizer (examples [""text""], truncation =True)\n# Tokenize data\ntokenized_train  = train_data .map(preprocess_function , batched=True)\ntokenized_train  = tokenized_train .remove_columns (""label"")\ntokenized_test  = test_data .map(preprocess_function , batched=True)\ntokenized_test  = tokenized_test .remove_columns (""label"")\nPreviously, we used DataCollatorWithPadding , which dynamically pads the input it\nreceives.\nInstead, we will have a DataCollator  that will perform the masking of tokens for\nus. There are two methods that are generally used for this: token and whole-word\nmasking. With token masking, we randomly mask 15% of the tokens in a sentence.\nIt might happen that part of a word will be masked. To enable masking of the entire\nword, we could apply whole-word masking, as illustrated in Figure 11-17 .\n342 | Chapter 11: Fine-Tuning Representation Models for Classification\nFigure 11-17. Different  methods for randomly masking tokens.\nGenerally, predicting whole words tends to be more complicated than tokens, which\nmakes the model perform better as it needs to learn more accurate and precise repre‐\nsentations during training. However, it tends to take a bit more time to converge.\nWe will be going with token masking in this example using DataCollatorForLan\nguageModeling  for faster convergence. However, we can use whole-word masking\nby replacing DataCollatorForLanguageModeling  with DataCollatorForWholeWord\nMask . Lastly, we set the probability that a token is masked in a given sentence to 15%\n(mlm_probability ):\nfrom transformers  import DataCollatorForLanguageModeling\n# Masking Tokens\ndata_collator  = DataCollatorForLanguageModeling (\n    tokenizer =tokenizer , \n    mlm=True, \n    mlm_probability =0.15\n)\nNext, we will create the Trainer  for running the MLM task and specify certain\nparameters:\n# Training arguments for parameter tuning\ntraining_args  = TrainingArguments (\n   ""model"",\n   learning_rate =2e-5,\n   per_device_train_batch_size =16,\n   per_device_eval_batch_size =16,\n   num_train_epochs =10,\n   weight_decay =0.01,\n   save_strategy =""epoch"",\n   report_to =""none""\n)\nContinued Pretraining with Masked Language Modeling | 343\n# Initialize Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args ,\n    train_dataset =tokenized_train ,\n    eval_dataset =tokenized_test ,\n    tokenizer =tokenizer ,\n    data_collator =data_collator\n)\nSeveral parameters are worth noting. We train for 20 epochs and keep the task short.\nY ou can experiment with the learning rate and weight decay to ascertain whether they\nassist in fine-tuning the model.\nBefore we start our training loop we will first save our pretrained tokenizer. The\ntokenizer is not updated during training so there is no need to save it after training.\nWe will, however, save our model after we continue pretraining:\n# Save pre-trained tokenizer\ntokenizer .save_pretrained (""mlm"")\n# Train model\ntrainer.train()\n# Save updated model\nmodel.save_pretrained (""mlm"")\nThis gives us an updated model in the mlm  folder. To evaluate its performance we\nwould normally fine-tune the model on a variety of tasks. For our purposes, however,\nwe can run some masking tasks to see if it has learned from its continued training.\nWe will do so by loading in our pretrained model before we continue pretraining.\nUsing the sentence ""What a horrible [MASK]!""  the model will predict which word\nwould be in place of ""[MASK]"" :\nfrom transformers  import pipeline\n# Load and create predictions\nmask_filler  = pipeline (""fill-mask"" , model=""bert-base-cased"" )\npreds = mask_filler (""What a horrible [MASK]!"" )\n# Print results\nfor pred in preds:\n    print(f"">>> {pred[""sequence"" ]}"")\n>>> What a horrible idea!\n>>> What a horrible dream!\n>>> What a horrible thing!\n>>> What a horrible day!\n>>> What a horrible thought!\n344 | Chapter 11: Fine-Tuning Representation Models for Classification",7372
129-Named-Entity Recognition.pdf,129-Named-Entity Recognition,"The output demonstrates concepts like “idea, ” “dream, ” and “day, ” which definitely\nmake sense. Next, let’s see what our updated model predicts:\n# Load and create predictions\nmask_filler  = pipeline (""fill-mask"" , model=""mlm"")\npreds = mask_filler (""What a horrible [MASK]!"" )\n# Print results\nfor pred in preds:\n    print(f"">>> {pred[""sequence"" ]}"")\n>>> What a horrible movie!\n>>> What a horrible film!\n>>> What a horrible mess!\n>>> What a horrible comedy!\n>>> What a horrible story!\nA horrible movie, film, mess, etc. clearly shows us that the model is more biased\ntoward the data that we fed it compared to the pretrained model.\nThe next step would be to fine-tune this model on the classification task that we did\nat the beginning of this chapter. Simply load the model as follows and you are good\nto go:\nfrom transformers  import AutoModelForSequenceClassification\n# Fine-tune for classification\nmodel = AutoModelForSequenceClassification .from_pretrained (""mlm"", num_labels =2)\ntokenizer  = AutoTokenizer .from_pretrained (""mlm"")\nNamed-Entity Recognition\nIn this section, we will delve into the process of fine-tuning a pretrained BERT\nmodel specifically for NER (named-entity recognition). Instead of classifying entire\ndocuments, this procedure allows for the classification of individual tokens and/or\nwords, including people and locations. This is especially helpful for de-identification\nand anonymization tasks when there is sensitive data.\nNER shares similarities with the classification example we explored at the beginning\nof this chapter. Nevertheless, a key distinction lies in the preprocessing and classifica‐\ntion of data. Given that we are focusing on classifying individual words instead of\nentire documents, we must preprocess the data to consider this granular structure.\nFigure 11-18  provides a visual representation of this word-level approach.\nNamed-Entity Recognition | 345\nFigure 11-18. Fine-tuning a BERT model for NER allows for the detection of named\nentities, such as people or locations.\nFine-tuning the pretrained BERT model follows a similar architecture akin to what\nwe observed with document classification. However, there is a fundamental shift\nin the classification approach. Rather than relying on the aggregation or pooling\nof token embeddings, the model now makes predictions for individual tokens in a\nsequence. It is crucial to emphasize that our word-level classification task does not\nentail classifying entire words, but rather the tokens that collectively constitute those\nwords. Figure 11-19  provides a visual representation of this token-level classification.\nFigure 11-19. During the fine-tuning  process of a BERT model, individual tokens are\nclassified  instead of words or entire documents.\n346 | Chapter 11: Fine-Tuning Representation Models for Classification",2863
130-Preparing Data for Named-Entity Recognition.pdf,130-Preparing Data for Named-Entity Recognition,"4Erik F. Sang and Fien De Meulder. “Introduction to the CoNLL-2003 shared task: Language-independent\nnamed entity recognition. ” arXiv preprint cs/0306050  (2003).\n5Jingjing Liu et al. “ Asgard: A portable architecture for multilingual dialogue systems. ” 2013 IEEE International\nConference on Acoustics, Speech and Signal Processing . IEEE, 2013.\nPreparing Data for Named-Entity Recognition\nIn this example, we will use the English version of the CoNLL-2003  dataset, which\ncontains several different types of named entities (person, organization, location,\nmiscellaneous, and no entity) and has roughly 14,000 training samples.4\n# The CoNLL-2003 dataset for NER\ndataset = load_dataset (""conll2003"" , trust_remote_code =True)\nWhile researching datasets to use for this example, there were a\nfew more that we wanted to share. wnut_17  is a task that focuses\non emerging and rare entities, those that are more difficult to\nspot. Furthermore, the tner/mit_movie_trivia  and tner/mit_res\ntaurant  datasets are quite fun to use. tner/mit_movie_trivia  is\nfor detecting entities like actor, plot, and soundtrack whereas tner/\nmit_restaurant  aims to detect entities such as amenity, dish, and\ncuisine.5\nLet’s inspect the structure of the data with an example:\nexample = dataset[""train""][848]\nexample\n{'id': '848',\n 'tokens': ['Dean',\n  'Palmer',\n  'hit',\n  'his',\n  '30th',\n  'homer',\n  'for',\n  'the',\n  'Rangers',\n  '.'],\n 'pos_tags': [22, 22, 38, 29, 16, 21, 15, 12, 23, 7],\n 'chunk_tags': [11, 12, 21, 11, 12, 12, 13, 11, 12, 0],\n 'ner_tags': [1, 2, 0, 0, 0, 0, 0, 0, 3, 0]}\nThis dataset provides us with labels for each word given in a sentence. These labels\ncan be found in the ner_tags  key, which refers to the following possible entities:\nNamed-Entity Recognition | 347\nlabel2id  = {\n    ""O"": 0, ""B-PER"": 1, ""I-PER"": 2, ""B-ORG"": 3, ""I-ORG"": 4, \n    ""B-LOC"": 5, ""I-LOC"": 6, ""B-MISC"" : 7, ""I-MISC"" : 8\n}\nid2label  = {index: label for label, index in label2id .items()}\nlabel2id\n{'O': 0,\n 'B-PER': 1,\n 'I-PER': 2,\n 'B-ORG': 3,\n 'I-ORG': 4,\n 'B-LOC': 5,\n 'I-LOC': 6,\n 'B-MISC': 7,\n 'I-MISC': 8}\nThese entities correspond to specific categories: a person (PER), organization (ORG),\nlocation (LOC), miscellaneous entities (MISC), and no entity (O). Note that these\nentities are prefixed with either a B (beginning) or an I (inside). If two tokens that\nfollow each other are part of the same phrase, then the start of that phrase is indicated\nwith B, which is followed by an I to show that they belong to each other and are not\nindependent entities.\nThis process is further illustrated in Figure 11-20 . In the figure, since “Dean” is the\nstart of the phrase and “Palmer” is the end, we know that “Dean Palmer” is a person\nand that “Dean” and “Palmer” are not individual people.\nFigure 11-20. By indicating the start and end of the phrase with the same entity, we can\nrecognize entities of entire phrases.\nOur data is preprocessed and split up into words but not yet tokens. To do so, we will\ntokenize it further with the tokenizer of the pretrained model we used throughout\nthis chapter, namely bert-base-cased :\n348 | Chapter 11: Fine-Tuning Representation Models for Classification\nfrom transformers  import AutoModelForTokenClassification\n# Load tokenizer\ntokenizer  = AutoTokenizer .from_pretrained (""bert-base-cased"" )\n# Load model\nmodel = AutoModelForTokenClassification .from_pretrained (\n    ""bert-base-cased"" , \n    num_labels =len(id2label ), \n    id2label =id2label , \n    label2id =label2id\n)\nLet’s explore how the tokenizer would process our example:\n# Split individual tokens into sub-tokens\ntoken_ids  = tokenizer (example[""tokens"" ], is_split_into_words =True)[""input_ids"" ]\nsub_tokens  = tokenizer .convert_ids_to_tokens (token_ids )\nsub_tokens\n['[CLS]',\n 'Dean',\n 'Palmer',\n 'hit',\n 'his',\n '30th',\n 'home',\n '##r',\n 'for',\n 'the',\n 'Rangers',\n '.',\n '[SEP]']\nThe tokenizer added the [CLS]  and [SEP]  tokens as we learned in Chapters 2 and 3.\nNote that the word 'homer'  was further split up into the tokens 'home'  and '##r' .\nThis creates a bit of a problem for us since we have labeled data at the word level but\nnot at the token level. This can be resolved by aligning the labels with their subtoken\ncounterparts during tokenization.\nLet’s consider the word 'Maarten' , which has the label B-PER to signal that this is\na person. If we pass that word through the tokenizer, it splits the word up into the\ntokens 'Ma' , '##arte' , and '##n' . We cannot use the B-PER entity for all tokens\nas that would signal that the three tokens are all independent people. Whenever\nan entity is split into tokens, the first token should have B (for beginning) and the\nfollowing should be I (for inner).\nTherefore, 'Ma'  will get the B-PER to signal the start of a phrase, and '##arte' , and\n'##n'  will get the I-PER to signal they belong to a phrase. This alignment process is\nillustrated in Figure 11-21 .\nNamed-Entity Recognition | 349\nFigure 11-21. The alignment process of labeling tokenized input.\nWe create a function, align_labels , that will tokenize the input and align these\ntokens with their updated labels during tokenization:\ndef align_labels (examples ):\n    token_ids  = tokenizer (\n        examples [""tokens"" ], \n        truncation =True, \n        is_split_into_words =True\n    )\n    labels = examples [""ner_tags"" ]\n    updated_labels  = []\n    for index, label in enumerate (labels):\n        \n        # Map tokens to their respective word\n        word_ids  = token_ids .word_ids (batch_index =index)  \n        previous_word_idx  = None\n        label_ids  = []\n        for word_idx  in word_ids : \n            # The start of a new word\n            if word_idx  != previous_word_idx :\n                \n                previous_word_idx  = word_idx\n                updated_label  = -100 if word_idx  is None else label[word_idx ]\n                label_ids .append(updated_label )\n            # Special token is -100\n            elif word_idx  is None:\n                label_ids .append(-100)\n            # If the label is B-XXX we change it to I-XXX\n            else:\n                updated_label  = label[word_idx ]\n                if updated_label  % 2 == 1:\n350 | Chapter 11: Fine-Tuning Representation Models for Classification\n                    updated_label  += 1\n                label_ids .append(updated_label )\n        updated_labels .append(label_ids )\n    token_ids [""labels"" ] = updated_labels\n    return token_ids\ntokenized  = dataset.map(align_labels , batched=True)\nLooking at our example, note that additional labels ( -100 ) were added for the [CLS]\nand [SEP]  tokens:\n# Difference between original and updated labels\nprint(f""Original: {example[""ner_tags"" ]}"")\nprint(f""Updated: {tokenized [""train""][848][""labels"" ]}"")\nOriginal: [1, 2, 0, 0, 0, 0, 0, 0, 3, 0]\nUpdated: [-100, 1, 2, 0, 0, 0, 0, 0, 0, 0, 3, 0, -100]\nNow that we have tokenized and aligned the labels, we can start thinking about\ndefining our evaluation metrics. This is also different from what we have seen before.\nInstead of a single prediction per document, we now have multiple predictions per\ndocument, namely per token.\nWe will make use of the evaluate  package by Hugging Face to create a compute_met\nrics  function that allows us to evaluate performance on a token level:\nimport evaluate  \n# Load sequential evaluation\nseqeval = evaluate .load(""seqeval"" )\ndef compute_metrics (eval_pred ):\n    # Create predictions\n    logits, labels = eval_pred\n    predictions  = np.argmax(logits, axis=2)\n    true_predictions  = []\n    true_labels  = []\n    # Document-level iteration\n    for prediction , label in zip(predictions , labels):\n      # Token-level iteration\n      for token_prediction , token_label  in zip(prediction , label):\n        # We ignore special tokens\n        if token_label  != -100:\n          true_predictions .append([id2label [token_prediction ]])\n          true_labels .append([id2label [token_label ]])\nNamed-Entity Recognition | 351",8142
131-Summary.pdf,131-Summary,"results = seqeval.compute(\n    predictions =true_predictions , references =true_labels\n)\n    return {""f1"": results[""overall_f1"" ]}\nFine-Tuning for Named-Entity Recognition\nWe are nearly there. Instead of DataCollatorWithPadding , we need a collator that\nworks with classification on a token level, namely DataCollatorForTokenClassifica\ntion :\nfrom transformers  import DataCollatorForTokenClassification\n# Token-classification DataCollator\ndata_collator  = DataCollatorForTokenClassification (tokenizer =tokenizer )\nNow that we have loaded our model, the rest of the steps are similar to previous\ntraining procedures in this chapter. We define a trainer with specific arguments that\nwe can tune and create a Trainer :\n# Training arguments for parameter tuning\ntraining_args  = TrainingArguments (\n   ""model"",\n   learning_rate =2e-5,\n   per_device_train_batch_size =16,\n   per_device_eval_batch_size =16,\n   num_train_epochs =1,\n   weight_decay =0.01,\n   save_strategy =""epoch"",\n   report_to =""none""\n)\n# Initialize Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args ,\n    train_dataset =tokenized [""train""],\n    eval_dataset =tokenized [""test""],\n    tokenizer =tokenizer ,\n    data_collator =data_collator ,\n    compute_metrics =compute_metrics ,\n)\ntrainer.train()\nWe then evaluate the model that we created:\n# Evaluate the model on our test data\ntrainer.evaluate ()\nLastly, let’s save the model and use it in a pipeline for inference. This allows us to\ncheck certain data so we can manually inspect what happens during inference and if\nwe are satisfied with the output:\n352 | Chapter 11: Fine-Tuning Representation Models for Classification\nfrom transformers  import pipeline\n# Save our fine-tuned model\ntrainer.save_model (""ner_model"" )\n# Run inference on the fine-tuned model\ntoken_classifier  = pipeline (\n    ""token-classification"" , \n    model=""ner_model"" , \n)\ntoken_classifier (""My name is Maarten."" )\n[{'entity': 'B-PER',\n  'score': 0.99534035,\n  'index': 4,\n  'word': 'Ma',\n  'start': 11,\n  'end': 13},\n {'entity': 'I-PER',\n  'score': 0.9928328,\n  'index': 5,\n  'word': '##arte',\n  'start': 13,\n  'end': 17},\n {'entity': 'I-PER',\n  'score': 0.9954301,\n  'index': 6,\n  'word': '##n',\n  'start': 17,\n  'end': 18}]\nIn the sentence ""My name is Maarten"" , the word ""Maarten""  and its subtokens were\ncorrectly identified as a person!\nSummary\nIn this chapter, we explored several tasks for fine-tuning pretrained representation\nmodels on specific classification tasks. We started by demonstrating how to fine-tune\na pretrained BERT model and extended the examples by freezing certain layers of its\narchitectures.\nWe experimented with a few-shot classification technique called SetFit, which\ninvolves fine-tuning a pretrained embedding model together with a classification\nhead using limited labeled data. Using only a few labeled data points, this model\ngenerated similar performance to the models we explored in earlier chapters.\nNext, we delved into the concept of continued pretraining, where we used a pre‐\ntrained BERT model as a starting point and continued training it using different data.\nThe underlying process, masked language modeling, is not only used for creating a\nrepresentation model but can also be used to continue pretraining models.\nSummary | 353\nFinally, we looked at named-entity recognition, a task that involves identifying spe‐\ncific entities such as people and places in unstructured text. Compared to previous\nexamples, this classification was done on a word level rather than on a document\nlevel.\nIn the next chapter, we continue with the field of fine-tuning language models but\nfocus on generative models instead. Using a two-step process, we will explore how to\nfine-tune a generative model to properly follow instructions and then fine-tune it for\nhuman preference.\n354 | Chapter 11: Fine-Tuning Representation Models for Classification",3980
132-Chapter 12. Fine-Tuning Generation Models.pdf,132-Chapter 12. Fine-Tuning Generation Models,,0
133-The Three LLM Training Steps Pretraining Supervised Fine-Tuning and Preference Tuning.pdf,133-The Three LLM Training Steps Pretraining Supervised Fine-Tuning and Preference Tuning,"CHAPTER 12\nFine-Tuning Generation Models\nIn this chapter, we will take a pretrained text generation model and go over the pro‐\ncess of fine-tuning  it. This fine-tuning step is key in producing high-quality models\nand an important tool in our toolbox to adapt a model to a specific desired behavior.\nFine-tuning allows us to adapt a model to a specific dataset or domain.\nThroughout this chapter, we will guide you among the two most common methods\nfor fine-tuning text generation models, supervised fine-tuning  and preference tuning .\nWe will explore the transformative potential of fine-tuning pretrained text generation\nmodels to make them more effective tools for your application.\nThe Three LLM Training Steps: Pretraining, Supervised\nFine-Tuning, and Preference Tuning\nThere are three common steps that lead to creating a high-quality LLM:\n1. Language modeling\nThe first step in creating a high-quality LLM is to pretrain it on one or more\nmassive text datasets ( Figure 12-1 ). During training, it attempts to predict the\nnext token to accurately learn linguistic and semantic representations found in\nthe text. As we saw before in Chapters 3 and 11, this is called language modeling\nand is a self-supervised method.\nThis produces a base  model, also commonly referred to as a pretrained  or founda‐\ntion model. Base models are a key artifact of the training process but are harder\nfor the end user to deal with. This is why the next step is important.\n355\nFigure 12-1. During language modeling, the LLM aims to predict the next token\nbased on an input. This is a process without labels.\n2. Fine-tuning 1 (supervised fine-tuning)\nLLMs  are more useful if they respond well to instructions and try to follow\nthem. When humans ask the model to write an article, they expect the model to\ngenerate the article and not list other instructions for example (which is what a\nbase model might do).\nWith supervised fine-tuning (SFT), we can adapt the base model to follow\ninstructions. During this fine-tuning process, the parameters of the base model\nare updated to be more in line with our target task, like following instructions.\nLike a pretrained model, it is trained using next-token prediction but instead of\nonly predicting the next token, it does so based on a user input ( Figure 12-2 ).\nFigure 12-2. During supervised fine-tuning,  the LLM aims to predict the next token\nbased on an input that has additional labels. In a sense, the label is the user’s input.\nSFT can also be used for other tasks, like classification, but is often used to go\nfrom a base generative model to an instruction (or chat) generative model.\n3. Fine-tuning 2 (preference tuning)\nThe final step further improves the quality of the model and makes it more\naligned with the expected behavior of AI safety or human preferences. This\nis called preference tuning. Preference tuning is a form of fine-tuning and, as\nthe name implies, aligns the output of the model to our preferences, which are\ndefined by the data that we give it. Like SFT, it can improve upon the original\nmodel but has the added benefit of distilling preference of output in its training\nprocess. These three steps are illustrated in Figure 12-3  and demonstrate the\nprocess of starting from an untrained architecture and ending with a preference-\ntuned LLM.\n356 | Chapter 12: Fine-Tuning Generation Models",3403
134-Supervised Fine-Tuning SFT.pdf,134-Supervised Fine-Tuning SFT,,0
135-Full Fine-Tuning.pdf,135-Full Fine-Tuning,"Figure 12-3. The three steps of creating a high-quality LLM.\nIn this chapter, we use a base model that was already trained on massive datasets and\nexplore how we can fine-tune it using both fine-tuning strategies. For each method,\nwe start with the theoretical underpinnings before using them in practice.\nSupervised Fine-Tuning (SFT)\nThe purpose of pretraining a model on large datasets is that it is able to reproduce\nlanguage and its meaning. During this process, the model learns to complete input\nphrases as shown in Figure 12-4 .\nFigure 12-4. A base or pretrained LLM was trained to predict the next word(s).\nThis example also illustrates that the model was not trained to follow instructions and\ninstead will attempt to complete a question rather than answer it ( Figure 12-5 ).\nFigure 12-5. A base LLM will not follow instructions but instead attempts to predict\neach next word. It may even create new questions.\nWe can use this base model and adapt it to certain use cases, such as following\ninstructions, by fine-tuning it.\nFull Fine-Tuning\nThe most common fine-tuning process is full fine-tuning. Like pretraining an LLM,\nthis process involves updating all parameters of a model to be in line with your target\nSupervised Fine-Tuning (SFT) | 357\ntask. The main difference is that we now use a smaller but labeled dataset whereas the\npretraining process was done on a large dataset without any labels ( Figure 12-6 ).\nFigure 12-6. Compared to language modeling (pretraining), full fine-tuning  uses a\nsmaller but labeled dataset.\nY ou can use any labeled data for full fine-tuning, making it also a great technique for\nlearning domain-specific representations. To make our LLM follow instructions, we\nwill need question-response data. This data, as shown in Figure 12-7 , is queries by the\nuser with corresponding answers.\nFigure 12-7. Instruction data with instructions by a user and corresponding answers.\nThe instructions can contain many different  tasks.\nDuring full fine-tuning, the model takes the input (instructions) and applies next-\ntoken prediction on the output (response). In turn, instead of generating new ques‐\ntions, it will follow instructions.\n358 | Chapter 12: Fine-Tuning Generation Models",2252
136-Parameter-Efficient Fine-Tuning PEFT.pdf,136-Parameter-Efficient Fine-Tuning PEFT,"1Neil Houlsby et al. “Parameter-efficient transfer learning for NLP . ” International Conference on Machine\nLearning . PMLR, 2019.Parameter-Efficient  Fine-Tuning (PEFT)\nUpdating all parameters of a model has a large potential of increasing its performance\nbut comes with several disadvantages. It is costly to train, has slow training times,\nand requires significant storage. To resolve these issues, attention has been given\nto parameter-efficient fine-tuning (PEFT) alternatives that focus on fine-tuning pre‐\ntrained models at higher computational efficiency.\nAdapters\nAdapters  are a core component of many PEFT-based techniques. The method pro‐\nposes a set of additional modular components inside the Transformer that can be\nfine-tuned to improve the model’s performance on a specific task without having to\nfine-tune all the model weights. This saves a lot of time and compute.\nAdapters are described in the paper “Parameter-efficient transfer learning for NLP” ,\nwhich showed that fine-tuning 3.6% of the parameters of BERT for a task can\nyield comparable performance to fine-tuning all the model’s weights.1 On the GLUE\nbenchmark, the authors show they reach within 0.4% of the performance of full\nfine-tuning. In a single Transformer block, the paper’s proposed architecture places\nadapters after the attention layer and the feedforward neural network as illustrated in\nFigure 12-8 .\nFigure 12-8. Adapters add a small number of weights in certain places in the network\nthat can be fine-tuned  efficiently  while leaving the majority of model weights frozen.\nSupervised Fine-Tuning (SFT) | 359\nIt’s not enough to only alter one Transformer block, however, so these components\nare part of every block in the model, as Figure 12-9  shows.\nFigure 12-9. Adapter components span the various Transformer blocks in the model.\nSeeing all the adapter’s components across the model like this enables us to see indi‐\nvidual adapters as shown in Figure 12-10 , which is a collection of these components\nspanning all the blocks of the model. Adapter 1 can be a specialist in, say, medical text\nclassification, while Adapter 2 can specialize in named-entity recognition (NER) . Y ou\ncan download specialized adapters from https://oreil.ly/XraXg .\n360 | Chapter 12: Fine-Tuning Generation Models\n2Jonas Pfeiffer et al. “ AdapterHub: A framework for adapting transformers. ” arXiv preprint arXiv:2007.07779\n(2020).\n3Renrui Zhang et al. “Llama-adapter: Efficient fine-tuning of language models with zero-init attention. ” arXiv\npreprint arXiv:2303.16199  (2023).\nFigure 12-10. Adapters that specialize in specific  tasks can be swapped into the same\narchitecture (if they share the same original model architecture and weights).\nThe paper “ AdapterHub: A framework for adapting transformers”  introduced the\nAdapter Hub  as a central repository for sharing adapters.2 A lot of these earlier adapt‐\ners were more focused on BERT architectures. More recently, the concept has been\napplied to text generation Transformers in papers like “LLaMA-Adapter: Efficient\nfine-tuning of language models with zero-init attention” .3\nLow-Rank Adaptation (LoRA)\nAs an alternative to adapters, low-rank adaptation (LoRA) was introduced and is at\nthe time of writing is a widely used and effective technique for PEFT. LoRA is a\ntechnique that (like adapters) only requires updating a small set of parameters. As\nSupervised Fine-Tuning (SFT) | 361\n4Edward J. Hu et al. “LoR: Low-Rank Adaptation of large language models. ” arXiv preprint arXiv:2106.09685\n(2021).illustrated in Figure 12-11 , it creates a small subset of the base model to fine-tune\ninstead of adding layers to the model.4\nFigure 12-11. LoRA requires only fine-tuning  a small set of parameters that can be kept\nseparately from the base LLM.\nLike adapters, this subset allows for much quicker fine-tuning since we only need\nto update a small part of the base model. We create this subset of parameters by\napproximating large matrices that accompany the original LLM with smaller matri‐\nces. We can then use those smaller matrices as a replacement and fine-tune them\ninstead of the original large matrices. Take for example the 10 × 10 matrix we see in\nFigure 12-12 .\nFigure 12-12. A major bottleneck of LLMs is their massive weight matrices. Only one\nof these may have 150 million parameters and each Transformer block would have its\nversion of these.\n362 | Chapter 12: Fine-Tuning Generation Models\nWe can come up with two smaller matrices, which when multiplied, reconstruct a 10\n× 10 matrix. This is a major efficiency win because instead of using 100 weights (10\ntimes 10) we now only have 20 weights (10 plus 10), as we can see in Figure 12-13 .\nFigure 12-13. Decomposing a large weight matrix into two smaller matrices leads to a\ncompressed, low-rank version of the matrix that can be fine-tuned  more efficiently.\nDuring training, we only need to update these smaller matrices instead of the full\nweight changes. The updated change matrices (smaller matrices) are then combined\nwith the full (frozen) weights as illustrated in Figure 12-14 .\nFigure 12-14. Compared to full fine-tuning,  LoRA aims to update a small representation\nof the original weights during training.\nSupervised Fine-Tuning (SFT) | 363\n5Armen Aghajanyan, Luke Zettlemoyer, and Sonal Gupta. “Intrinsic dimensionality explains the effectiveness\nof language model fine-tuning. ” arXiv preprint arXiv:2012.13255  (2020).But you might suspect that performance would drop. And you would be right. But\nwhere does this trade-off make sense?\nPapers like “Intrinsic dimensionality explains the effectiveness of language model\nfine-tuning”  demonstrate that language models “have a very low intrinsic dimen‐\nsion. ”5 This means that we can find small ranks that approximate even the massive\nmatrices of an LLM. A 175B model like GPT-3, for example, would have a weight\nmatrix of 12,288 × 12,288 inside each of its 96 Transformer blocks. That’s 150 million\nparameters. If we can successfully adapt that matrix into rank 8, that would only\nrequire two 12,288 × 2 matrices resulting in 197K parameters per block. These are\nmajor savings in speed, storage, and compute as explained further in the previously\nreferenced LoRA paper .\nThis smaller representation is quite flexible in that you can select which parts of the\nbase model to fine-tune. For instance, we can only fine-tune the Query and Value\nweight matrices in each Transformer layer.\nCompressing the model for (more) efficient  training\nWe can make LoRA even more efficient by reducing the memory requirements of the\nmodel’s original weights before projecting them into smaller matrices. The weights of\nan LLM are numeric values with a given precision, which can be expressed by the\nnumber of bits like float64 or float32. As illustrated in Figure 12-15 , if we lower the\namount of bits to represent a value, we get a less accurate result. However, if we lower\nthe number of bits we also lower the memory requirements of that model.\nFigure 12-15. Attempting to represent pi with float  32-bit and float  16-bit representa‐\ntions. Notice the lowered accuracy when we halve the number of bits.\n364 | Chapter 12: Fine-Tuning Generation Models\n6Tim Dettmers et al. “QLoRA: Efficient finetuning of quantized LLMs. ” arXiv preprint arXiv:2305.14314  (2023).With quantization, we aim to lower the number of bits while still accurately repre‐\nsenting the original weight values. However, as shown in Figure 12-16 , when directly\nmapping higher precision values to lower precision values, multiple higher precision\nvalues might end up being represented by the same lower precision values.\nFigure 12-16. Quantizing weights that are close to one another results in the same\nreconstructed weights thereby removing any differentiating  factor.\nInstead, the authors of QLoRA, a quantized version of LoRA, found a way to go from\na higher number of bits to a lower value and vice versa without differentiating too\nmuch from the original weights.6\nThey used blockwise quantization to map certain blocks of higher precision values\nto lower precision values. Instead of directly mapping higher precision to lower preci‐\nsion values, additional blocks are created that allow for quantizing similar weights. As\nshown in Figure 12-17 , this results in values that can be accurately represented with\nlower precision.\nSupervised Fine-Tuning (SFT) | 365\nFigure 12-17. Blockwise quantization can accurately represent weights in lower precision\nthrough quantization blocks.\nA nice property of neural networks is that their values are generally normally dis‐\ntributed between –1 and 1. This property allows us to bin the original weights to\nlower bits based on their relative density, as illustrated in Figure 12-18 . The mapping\nbetween weights is more efficient as it takes into account the relative frequency of\nweights. This also reduces issues with outliers.\nFigure 12-18. Using distribution-aware blocks we can prevent values close to one another\nfrom being represented with the same quantized value.\n366 | Chapter 12: Fine-Tuning Generation Models",9210
137-Instruction Tuning with QLoRA.pdf,137-Instruction Tuning with QLoRA,,0
138-Templating Instruction Data.pdf,138-Templating Instruction Data,"Combined with the blockwise quantization, this normalization procedure allows for\naccurate representation of high precision values by low precision values with only\na small decrease in the performance of the LLM. As a result, we can go from a\n16-bit float representation to a measly 4-bit normalized float representation. A 4-bit\nrepresentation significantly reduces the memory requirements of the LLM during\ntraining. Note that the quantization of LLMs in general is also helpful for inference as\nquantized LLMs are smaller in size and therefore require less VRAM.\nThere are more elegant methods to further optimize this like double quantization and\npaged optimizers, which you can read about more in the QLoRA paper discussed\nearlier . For a complete and highly visual guide to quantization, see this blog post .\nInstruction Tuning with QLoRA\nNow that we have explored how QLoRA works, let us put that knowledge into\npractice! In this section, we will fine-tune a completely open source and smaller\nversion of Llama, TinyLlama , to follow instructions using the QLoRA procedure.\nConsider this model a base or pretrained model, one that was trained with language\nmodeling but cannot yet follow instructions.\nTemplating Instruction Data\nTo have the LLM follow instructions, we will need to prepare instruction data that\nfollows a chat template. This chat template, as illustrated in Figure 12-19 , differenti‐\nates between what the LLM has generated and what the user has generated.\nFigure 12-19. The chat template that we use throughout this chapter.\nInstruction Tuning with QLoRA | 367\n7Ning Ding et al. “Enhancing chat language models by scaling high-quality instructional conversations. ” arXiv\npreprint arXiv:2305.14233  (2023).We chose this chat template to use throughout the examples since the chat version of\nTinyLlama  uses the same format. The data that we are using is a small subset of the\nUltraChat dataset .7 This dataset is a filtered version of the original UltraChat dataset\nthat contains almost 200k conversations between a user and an LLM.\nWe create a function, format_prompt , to make sure that the conversations follow this\ntemplate:\nfrom transformers  import AutoTokenizer\nfrom datasets  import load_dataset\n# Load a tokenizer to use its chat template\ntemplate_tokenizer  = AutoTokenizer .from_pretrained (\n    ""TinyLlama/TinyLlama-1.1BChat-v1.0""\n)\ndef format_prompt (example):\n    """"""Format the prompt to using the <|user|> template TinyLLama is using""""""\n    # Format answers\n    chat = example[""messages"" ]\n    prompt = template_tokenizer .apply_chat_template (chat, tokenize =False)\n    return {""text"": prompt}\n# Load and format the data using the template TinyLLama is using\ndataset = (\n    load_dataset (""HuggingFaceH4/ultrachat_200k"" , split=""test_sft"" )\n      .shuffle(seed=42)\n      .select(range(3_000))\n)\ndataset = dataset.map(format_prompt )\nWe select a subset of 3,000 documents to reduce the training time, but you can\nincrease this value to get more accurate results.\nUsing the ""text""  column, we can explore these formatted prompts:\n# Example of formatted prompt\nprint(dataset[""text""][2576])\n<|user|>\nGiven the text: Knock, knock. Who's there? Hike.\nCan you continue the joke based on the given text material ""Knock, knock. \nWho's there? Hike""?</s>\n<|assistant|>\nSure! Knock, knock. Who's there? Hike. Hike who? Hike up your pants, it's cold \noutside!</s>\n<|user|>\n368 | Chapter 12: Fine-Tuning Generation Models",3511
139-Merge Weights.pdf,139-Merge Weights,"Can you tell me another knock-knock joke based on the same text material \n""Knock, knock. Who's there? Hike""?</s>\n<|assistant|>\nOf course! Knock, knock. Who's there? Hike. Hike who? Hike your way over here \nand let's go for a walk!</s>\nModel Quantization\nNow that we have our data, we can start loading in our model. This is where we\napply the Q in QLoRA, namely quantization. We use the bitsandbytes  package  to\ncompress the pretrained model to a 4-bit representation.\nIn BitsAndBytesConfig , you can define the quantization scheme. We follow the steps\nused in the original QLoRA paper and load the model in 4-bit ( load_in_4bit ) with\na normalized float representation ( bnb_4bit_quant_type ) and double quantization\n(bnb_4bit_use_double_quant ):\nimport torch\nfrom transformers  import AutoModelForCausalLM , AutoTokenizer , BitsAndBytesConfig\nmodel_name  = ""TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T""\n# 4-bit quantization configuration - Q in QLoRA\nbnb_config  = BitsAndBytesConfig (\n    load_in_4bit =True,  # Use 4-bit precision model loading\n    bnb_4bit_quant_type =""nf4"",  # Quantization type\n    bnb_4bit_compute_dtype =""float16"" ,  # Compute dtype\n    bnb_4bit_use_double_quant =True,  # Apply nested quantization\n)\n# Load the model to train on the GPU\nmodel = AutoModelForCausalLM .from_pretrained (\n    model_name ,\n    device_map =""auto"",\n    # Leave this out for regular SFT\n    quantization_config =bnb_config ,\n)\nmodel.config.use_cache  = False\nmodel.config.pretraining_tp  = 1\n# Load LLaMA tokenizer\ntokenizer  = AutoTokenizer .from_pretrained (model_name , trust_remote_code =True)\ntokenizer .pad_token  = ""<PAD>""\ntokenizer .padding_side  = ""left""\nInstruction Tuning with QLoRA | 369\nThis quantization procedure allows us to decrease the size of the original model while\nretaining most of the original weights’ precision. Loading the model now only uses\n~1 GB VRAM compared to the ~4 GB of VRAM it would need without quantization.\nNote that during fine-tuning, more VRAM will be necessary so it does not cap out on\nthe ~1 GB VRAM needed to load the model.\nLoRA Configuration\nNext,  we will need to define our LoRA configuration using the peft  library , which\nrepresents hyperparameters of the fine-tuning process:\nfrom peft import LoraConfig , prepare_model_for_kbit_training , get_peft_model\n# Prepare LoRA Configuration\npeft_config  = LoraConfig (\n    lora_alpha =32,  # LoRA Scaling\n    lora_dropout =0.1,  # Dropout for LoRA Layers\n    r=64,  # Rank\n    bias=""none"",\n    task_type =""CAUSAL_LM"" ,\n    target_modules =  # Layers to target\n     [""k_proj"" , ""gate_proj"" , ""v_proj"" , ""up_proj"" , ""q_proj"" , ""o_proj"" , \n""down_proj"" ]\n)\n# Prepare model for training\nmodel = prepare_model_for_kbit_training (model)\nmodel = get_peft_model (model, peft_config )\nThere are several parameters worth mentioning:\nr\nThis  is the rank of the compressed matrices (recall this from Figure 12-13 )\nIncreasing this value will also increase the sizes of compressed matrices leading\nto less compression and thereby improved representative power. Values typically\nrange between 4 and 64.\nlora_alpha\nControls the amount of change that is added to the original weights. In essence, it\nbalances the knowledge of the original model with that of the new task. A rule of\nthumb is to choose a value twice the size of r.\ntarget_modules\nControls which layers to target. The LoRA procedure can choose to ignore\nspecific layers, like specific projection layers. This can speed up training but\nreduce performance and vice versa.\nPlaying around with the parameters is a worthwhile experiment to get an intuitive\nunderstanding of values that work and those that do not. Y ou can find an amazing\n370 | Chapter 12: Fine-Tuning Generation Models\nresource of additional tips on LoRA fine-tuning in the Ahead of AI newsletter  by\nSebastian Raschka.\nThis example demonstrates an efficient form of fine-tuning your\nmodel. If you want to perform full fine-tuning instead, you can\nremove the quantization_config  parameter when loading the\nmodel and skip the creation of peft_config . By removing those,\nwe would go from “Instruction tuning with QLoRA ” to “full\ninstruction tuning. ”\nTraining Configuration\nLastly, we need to configure our training parameters as we did in Chapter 11 :\nfrom transformers  import TrainingArguments\noutput_dir  = ""./results""\n# Training arguments\ntraining_arguments  = TrainingArguments (\n    output_dir =output_dir ,\n    per_device_train_batch_size =2,\n    gradient_accumulation_steps =4,\n    optim=""paged_adamw_32bit"" ,\n    learning_rate =2e-4,\n    lr_scheduler_type =""cosine"" ,\n    num_train_epochs =1,\n    logging_steps =10,\n    fp16=True,\n    gradient_checkpointing =True\n)\nThere are several parameters worth mentioning:\nnum_train_epochs\nThe total number of training rounds. Higher values tend to degrade performance\nso we generally like to keep this low.\nlearning_rate\nDetermines  the step size at each iteration of weight updates. The authors of\nQLoRA found that higher learning rates work better for larger models (>33B\nparameters).\nlr_scheduler_type\nA cosine-based scheduler to adjust the learning rate dynamically. It will linearly\nincrease the learning rate, starting from zero, until it reaches the set value. After\nthat, the learning rate is decayed following the values of a cosine function.\nInstruction Tuning with QLoRA | 371\noptim\nThe paged optimizers used in the original QLoRA paper.\nOptimizing these parameters is a difficult task and there are no set guidelines for\ndoing so. It requires experimentation to figure out what works best for specific\ndatasets, model sizes, and target tasks.\nAlthough this section describes instruction tuning, we could also\nuse QLoRA to fine-tune an instruction model. For instance, we\ncould fine-tune a chat model to generate specific SQL code or\nto create JSON output that adheres to a specific format. As long\nas you have the data available (with appropriate query-response\nitems), QLoRA is a great technique for nudging an existing chat\nmodel to be more appropriate for your use case.\nTraining\nNow that we have prepared all our models and parameters, we can start fine-tuning\nour model. We load in SFTTrainer  and simply run trainer.train() :\nfrom trl import SFTTrainer\n# Set supervised fine-tuning parameters\ntrainer = SFTTrainer (\n    model=model,\n    train_dataset =dataset,\n    dataset_text_field =""text"",\n    tokenizer =tokenizer ,\n    args=training_arguments ,\n    max_seq_length =512,\n    # Leave this out for regular SFT\n    peft_config =peft_config ,\n)\n# Train model\ntrainer.train()\n# Save QLoRA weights\ntrainer.model.save_pretrained (""TinyLlama-1.1B-qlora"" )\nDuring training the loss will be printed every 10 steps according to the log\nging_steps  parameter. If you are using the free GPU provided by Google Colab,\nwhich is the Tesla T4 at the time of writing, then training might take up to an hour. A\ngood time to take a break!\n372 | Chapter 12: Fine-Tuning Generation Models",7110
140-Word-Level Metrics.pdf,140-Word-Level Metrics,"Merge Weights\nAfter we have trained our QLoRA weights, we still need to combine them with the\noriginal weights to use them. We reload the model in 16 bits, instead of the quantized\n4 bits, to merge the weights. Although the tokenizer was not updated during training,\nwe save it to the same folder as the model for easier access:\nfrom peft import AutoPeftModelForCausalLM\nmodel = AutoPeftModelForCausalLM .from_pretrained (\n    ""TinyLlama-1.1B-qlora"" ,\n    low_cpu_mem_usage =True,\n    device_map =""auto"",\n)\n# Merge LoRA and base model\nmerged_model  = model.merge_and_unload ()\nAfter merging the adapter with the base model, we can use it with the prompt\ntemplate that we defined earlier:\nfrom transformers  import pipeline\n# Use our predefined prompt template\nprompt = """"""<|user|>\nTell me something about Large Language Models.</s>\n<|assistant|>\n""""""\n# Run our instruction-tuned model\npipe = pipeline (task=""text-generation"" , model=merged_model , tokenizer =tokenizer )\nprint(pipe(prompt)[0][""generated_text"" ])\nLarge Language Models (LLMs) are artificial intelligence (AI) models that \nlearn language and understand what it means to say things in a particular \nlanguage. They are trained on huge amounts of text…\nThe aggregate output shows that the model now closely follows our instructions,\nwhich is not possible with the base model.\nEvaluating Generative Models\nEvaluating  generative models poses a significant challenge. Generative models are\nused across many diverse use cases, making it a challenge to rely on a singular metric\nfor judgment. Unlike more specialized models, a generative model’s ability to solve\nmathematical questions does not guarantee success in solving coding questions.\nAt the same time, evaluating these models is vital, particularly in production settings\nwhere consistency is important. Given their probabilistic nature, generative models\ndo not necessarily generate consistent outputs; there is a need for robust evaluation.\nEvaluating Generative Models | 373",2029
141-Benchmarks.pdf,141-Benchmarks,"8Fred Jelinek et al. “Perplexity—a measure of the difficulty of speech recognition tasks. ” The Journal of the\nAcoustical Society of America  62.S1 (1977): S63.\n9Chin-Y ew Lin. “ROUGE: A package for automatic evaluation of summaries. ” Text Summarization Branches\nOut, 74–81. 2004.\n10Kishore Papineni, et al. “Bleu: a method for automatic evaluation of machine translation. ” Proceedings of the\n40th Annual Meeting of the Association for Computational Linguistics . 2002.\n11Tianyi Zhang et al. “BERTscore: Evaluating text generation with BERT. ” arXiv preprint arXiv:1904.09675\n(2019).In this section, we will explore a few common evaluation methods, but we want to\nemphasize the current lack of golden standards. No one metric is perfect for all use\ncases.\nWord-Level Metrics\nOne common metrics category for comparing generative models is word-level evalu‐\nation. These classic techniques compare a reference dataset with the generated tokens\non a token(set) level. Common word-level metrics include perplexity,8 ROUGE,9\nBLEU,10 and BERTScore.11\nOf note is perplexity, which measures how well a language model predicts a text.\nGiven input text, the model predicts how likely the next token is. With perplexity, we\nassume a model performs better if it gives the next token a high probability. In other\nwords, the models should not be “perplexed” when presented with a well-written\ndocument.\nAs illustrated in Figure 12-20 , when presented with the input “When a measure\nbecomes a, ” the model is asked how probable the word “target” is as the next word.\nFigure 12-20. Next-word prediction is a central feature of many LLMs.\nAlthough perplexity, and other word-level metrics, are useful metrics to understand\nthe confidence of the model, they are not a perfect measure. They do not account for\nconsistency, fluency, creativity, or even correctness of the generated text.\nBenchmarks\nA common method for evaluating generative models on language generation and\nunderstanding tasks is on well-known and public benchmarks, such as MMLU,12\nGLUE,13 TruthfulQA,14 GSM8k,15 and HellaSwag.16 These benchmarks give us\n374 | Chapter 12: Fine-Tuning Generation Models\n12Dan Hendrycks et al. “Measuring massive multitask language understanding. ” arXiv preprint arXiv:2009.03300\n(2020).\n13Alex Wang et al. “GLUE: A multi-task benchmark and analysis platform for natural language understanding. ”\narXiv preprint arXiv:1804.07461  (2018).\n14Stephanie Lin, Jacob Hilton, and Owain Evans. “TruthfulQA: Measuring how models mimic human false‐\nhoods. ” arXiv preprint arXiv:2109.07958  (2021).\n15Karl Cobbe et al. “Training verifiers to solve math word problems. ” arXiv preprint arXiv:2110.14168  (2021).\n16Roman Zellers et al. “HellaSwag: Can a machine really finish your sentence?” arXiv preprint arXiv:1905.07830\n(2019).\n17Mark Chen et al. “Evaluating large language models trained on code. ” arXiv preprint arXiv:2107.03374  (2021).information  about basic language understanding but also complex analytical answer‐\ning, like math problems.\nAside from natural language tasks, some models specialize in other domains, like\nprogramming. These models tend to be evaluated on different benchmarks, such\nas HumanEval,17 which consists of challenging programming tasks for the model to\nsolve. Table 12-1  gives an overview of common public benchmarks for generative\nmodels.\nTable 12-1. Common public benchmarks for generative models\nBenchmark Description Resources\nMMLU The Massive Multitask Language Understanding (MMLU) benchmark tests the model on 57\ndifferent  tasks, including classification,  question answering, and sentiment analysis.https://oreil.ly/\nnrG_g\nGLUE The General Language Understanding Evaluation (GLUE) benchmark consists of language\nunderstanding tasks covering a wide degree of difficulty.https://oreil.ly/\nLV_fb\nTruthfulQA TruthfulQA measures the truthfulness of a model’s generated text. https://oreil.ly/\ni2Brj\nGSM8k The GSM8k dataset contains grade-school math word problems. It is linguistically diverse\nand created by human problem writers.https://oreil.ly/\noOBXY\nHellaSwag HellaSwag is a challenge dataset for evaluating common-sense inference. It consists of\nmultiple-choice questions that the model needs to answer. It can select one of four answer\nchoices for each question.https://oreil.ly/\naDvBP\nHumanEval The HumanEval benchmark is used for evaluating generated code based on 164\nprogramming problems.https://oreil.ly/\ndlJIX\nBenchmarks are a great way to get a basic understanding on how well a model\nperforms on a wide variety of tasks. A downside to public benchmarks is that models\ncan be overfitted to these benchmarks to generate the best responses. Moreover,\nthese are still broad benchmarks and might not cover very specific use cases. Lastly,\nanother downside is that some benchmarks require strong GPUs with a long running\ntime (over hours) to compute, which makes iteration difficult.\nEvaluating Generative Models | 375",5016
142-Leaderboards.pdf,142-Leaderboards,,0
143-Automated Evaluation.pdf,143-Automated Evaluation,,0
144-Human Evaluation.pdf,144-Human Evaluation,"18Lianmin Zheng et al. “Judging LLM-as-a-judge with MT-Bench and Chatbot Arena. ” Advances in Neural\nInformation Processing Systems  36 (2024).Leaderboards\nWith  so many different benchmarks, it is hard to choose which benchmark best suits\nyour model. Whenever a model is released, you will often see it evaluated on several\nbenchmarks to showcase how it performs across the board.\nAs such, leaderboards were developed containing multiple benchmarks. A common\nleaderboard is the Open LLM Leaderboard , which, at the time of writing, includes six\nbenchmarks, including HellaSwag, MMLU, TruthfulQA, and GSM8k. Models that top\nthe leaderboard, assuming they were not overfitted on the data, are generally regar‐\nded as the “best” model. However, since these leaderboards often contain publicly\navailable benchmarks, there is a risk of overfitting on the leaderboard.\nAutomated Evaluation\nPart of evaluating a generative output is the quality of its text. For instance, even if\ntwo models were to give the same correct answer to a question, the way they derived\nthat answer might be different. It is often not just about the final answer but also the\nconstruction of it. Similarly, although two summaries might be similar, one could be\nsignificantly shorter than another, which is often important for a good summary.\nTo evaluate the quality of the generated text above the correctness of the final answer,\nLLM-as-a-judge was introduced.18 In essence, a separate LLM is asked to judge the\nquality of the LLM to be evaluated. An interesting variant of this method is pairwise\ncomparison. Two different LLMs will generate an answer to a question and a third\nLLM will be the judge to declare which is better.\nAs a result, this methodology allows for automated evaluation of open-ended ques‐\ntions. A major advantage is that as LLMs improve, so do their capabilities to judge the\nquality of output. In other words, this evaluation methodology grows with the field.\nHuman Evaluation\nAlthough  benchmarks are important, the gold standard of evaluation is gener‐\nally considered to be human evaluation. Even if an LLM scores well on broad\nbenchmarks , it still might not score well on domain-specific tasks. Moreover, bench‐\nmarks do not fully capture human preference and all methods discussed before are\nmerely proxies for that.\nA great example of a human-based evaluation technique is the Chatbot Arena .19\nWhen you go to this leaderboard you are shown two (anonymous) LLMs you can\ninteract with. Any question or prompt you ask will be sent to both models and you\n376 | Chapter 12: Fine-Tuning Generation Models\n19Wei-Lin Chiang et al. “Chatbot Arena: An open platform for evaluating LLMs by human preference. ” arXiv\npreprint arXiv:2403.04132  (2024).\n20Mafilyn Strathern. “‘Improving ratings’: audit in the British University system. ” European Review  5.3 (1997):\n305–321.will receive their output. Then, you can decide which output you prefer. This process\nallows for the community to vote on which models they prefer without knowing\nwhich ones are presented. Only after you vote do you see which model generated\nwhich text.\nAt the time of writing, this method has generated over 800,000+ human votes that\nwere used to compute a leaderboard. These votes are used to calculate the relative\nskill level of LLMs based on their win rates. For instance, if a low-ranked LLM beats a\nhigh-ranked LLM, its ranking changes significantly. In chess, this is referred to as the\nElo rating system.\nThis methodology therefore uses crowdsourced votes, which helps us understand the\nquality of the LLM. However, it is still the aggregated opinion of a wide variety of\nusers, which might not relate to your use case.\nAs a result, there is no one perfect method of evaluating LLMs. All mentioned\nmethodologies and benchmarks provide an important, although limited evaluation\nperspective. Our advice is to evaluate your LLM based on the intended use case. For\ncoding, HumanEval would be more logical than GSM8k.\nBut most importantly, we believe that you are the best evaluator. Human evaluation\nremains the gold standard because it is up to you to decide whether the LLM works\nfor your intended use case. As with the examples in this chapter, we highly advise\nthat you also try these models and perhaps develop some questions yourself. For\nexample, the authors of this book are Arabic (Jay Alammar) and Dutch (Maarten\nGrootendorst), and we often ask questions in our native language when approached\nwith new models.\nOne final note on this topic is a quote we hold dear:\nWhen a measure becomes a target, it ceases to be a good measure.\n—Goodhart’s Law20\nIn the context of LLMs, when using a specific benchmark, we tend to optimize for\nthat benchmark regardless of the consequences. For instance, if we focus purely on\noptimizing for generating grammatically correct sentences, the model could learn to\nonly output one sentence: “This is a sentence. ” It is grammatically correct but tells you\nnothing about its language understanding capabilities. Thus, the model may excel at a\nspecific benchmark but potentially at the expense of other useful capabilities.\nEvaluating Generative Models | 377",5238
145-The Inputs and Outputs of a Reward Model.pdf,145-The Inputs and Outputs of a Reward Model,"Preference-Tuning / Alignment / RLHF\nAlthough  our model can now follow instructions, we can further improve its behav‐\nior by a final training phase that aligns it to how we expect it to behave in different\nscenarios. For instance, when asked “What is an LLM?” we might prefer an elaborate\nanswer that describes the internals of an LLM compared to the answer “It is a large\nlanguage model” without further explanations. How exactly do we align our (human)\npreference for one answer over the other with the output of an LLM?\nTo start with, recall that an LLM takes a prompt and outputs a generation as illustra‐\nted in Figure 12-21 .\nFigure 12-21. An LLM takes an input prompt and outputs a generation.\nWe can ask a person (preference evaluator) to evaluate the quality of that model\ngeneration. Say they assign it a certain score, like 4 (see Figure 12-22 ).\nFigure 12-22. Use a preference evaluator (human or otherwise) to evaluate the quality of\nthe generation.\nFigure 12-23  shows a preference tuning step updating the model based on that score:\n•If the score is high, the model is updated to encourage it to generate more like•\nthis type of generation.\n•If the score is low, the model is updated to discourage such generations.•\n378 | Chapter 12: Fine-Tuning Generation Models\nFigure 12-23. Preference tuning methods update the LLM based on the evaluation score.\nAs always, we need many training examples. So can we automate the preference\nevaluation? Y es, we can by training a different model called a reward model.\nAutomating Preference Evaluation Using Reward Models\nTo automate preference evaluation, we need a step before the preference-tuning step,\nnamely to train a reward model, as shown in Figure 12-24 .\nFigure 12-24. We train a reward model before fine-tuning  the LLM.\nFigure 12-25  shows that to create a reward model, we take a copy of the instruction-\ntuned model and slightly change it so that instead of generating text, it now outputs a\nsingle score.\nFigure 12-25. The LLM becomes a reward model by replacing its language modeling\nhead with a quality classification  head.\nAutomating Preference Evaluation Using Reward Models | 379",2186
146-Training a Reward Model.pdf,146-Training a Reward Model,"The Inputs and Outputs of a Reward Model\nThe way we expect this reward model to work is that we give it a prompt and\na generation, and it outputs a single number indicating the preference/quality of\nthat generation in response to that prompt. Figure 12-26  shows the reward model\ngenerating this single number.\nFigure 12-26. Use a reward model trained on human preference to generate the comple‐\ntion quality score.\nTraining a Reward Model\nWe cannot directly use the reward model. It needs to first be trained to properly score\ngenerations. So let’s get a preference dataset that the model can learn from.\nReward model training dataset\nOne common shape for preference datasets is for a training example to have a\nprompt, with one accepted generation and one rejected generation. (Nuance: it’s not\nalways a good versus bad generation; it can be that the two generations are both good,\nbut that one is better than the other). Figure 12-27  shows an example preference\ntraining set with two training examples.\n380 | Chapter 12: Fine-Tuning Generation Models\nFigure 12-27. Preference tuning datasets are often  made up of prompts with accepted\nand rejected generations.\nOne way to generate preference data is to present a prompt to the LLM and have\nit generate two different generations. As shown in Figure 12-28 , we can ask human\nlabelers which of the two they prefer.\nFigure 12-28. Output two generations and ask a human labeler which one they prefer.\nAutomating Preference Evaluation Using Reward Models | 381\nReward model training step\nNow that we have the preference training dataset, we can proceed to train the reward\nmodel.\nA simple step is that we use the reward model to:\n1.Score the accepted generation1.\n2.Score the rejected generation2.\nFigure 12-29  shows the training objective: to ensure the accepted generation has a\nhigher score than the rejected generation.\nFigure 12-29. The reward model aims to evaluate the quality scores of generations in\nresponse to a prompt.\nWhen we combine everything together as shown in Figure 12-30 , we get the three\nstages to preference tuning:\n1.Collect preference data1.\n2.Train a reward model2.\n3.Use the reward model to fine-tune the LLM (operating as the preference3.\nevaluator)\n382 | Chapter 12: Fine-Tuning Generation Models\n21John Schulman et al. “Proximal Policy Optimization algorithms. ” arXiv preprint arXiv:1707.06347  (2017).\nFigure 12-30. The three stages of preference tuning: collecting preference data, training a\nreward model, and finally  fine-tuning  the LLM.\nReward models are an excellent idea that can be further extended and developed.\nLlama 2, for example, trains two reward models: one that scores helpfulness and\nanother that scores safety ( Figure 12-31 ).\nFigure 12-31. We can use multiple reward models to perform the scoring.\nA common method to fine-tune the LLM with the trained reward model is Proximal\nPolicy Optimization (PPO) . PPO is a popular reinforcement technique that optimizes\nthe instruction-tuned LLM by making sure that the LLM does not deviate too much\nfrom the expected rewards.21 It was even used to train the original ChatGPT  released\nin November 2022.\nAutomating Preference Evaluation Using Reward Models | 383",3261
147-Templating Alignment Data.pdf,147-Templating Alignment Data,"22Rafael Rafailov, et al. “Direct Preference Optimization: Y our language model is secretly a reward model. ” arXiv\npreprint arXiv:2305.18290  (2023).Training No Reward Model\nA disadvantage of PPO is that it is a complex method that needs to train at least\ntwo models, the reward model and the LLM, which can be more costly than perhaps\nnecessary.\nDirect Preference Optimization (DPO) is an alternative to PPO and does away with\nthe reinforcement-based learning procedure.22 Instead of using the reward model to\njudge the quality of a generation, we let the LLM itself do that. As illustrated in\nFigure 12-32 , we use a copy of the LLM as the reference model to judge the shift\nbetween the reference and trainable model in the quality of the accepted generation\nand rejected generation.\nFigure 12-32. Use the LLM itself as the reward model by comparing the output of a\nfrozen model with the trainable model.\nBy calculating this shift during training, we can optimize the likelihood of accepted\ngenerations over rejected generations by tracking the difference in the reference\nmodel and the trainable model.\nTo calculate this shift and its related scores, the log probabilities of the rejected\ngenerations and accepted generations are extracted from both models. As illustrated\nin Figure 12-33 , this process is performed at a token level where the probabilities are\ncombined to calculate the shift between the reference and trainable models.\n384 | Chapter 12: Fine-Tuning Generation Models\nFigure 12-33. Scores are calculated by taking the probabilities of generation on a token\nlevel. The shift in probabilities between the reference model and the trainable model is\noptimized. The accepted generation follows the same procedure.\nUsing these scores, we can optimize the parameters of the trainable model to be more\nconfident of generating the accepted generations and less confident of generating the\nrejected generations. Compared to PPO, the authors found DPO to be more stable\nduring training and more accurate. Due to its stability, we will be using it as our\nprimary model for preference tuning our previously instruction-tuned model.\nPreference Tuning with DPO\nWhen we use the Hugging Face stack, preference tuning is eerily similar to the\ninstruction tuning we covered before with some slight differences. We will still be\nusing TinyLlama but this time an instruction-tuned version  that was first trained\nusing full fine-tuning and then further aligned with DPO. Compared to our initial\ninstruction-tuned model, this LLM was trained on much larger datasets.\nIn this section, we will demonstrate how you can further align this model using DPO\nwith reward-based datasets.\nPreference Tuning with DPO | 385",2748
148-Summary.pdf,148-Summary,"Templating Alignment Data\nWe will use a dataset that for each prompt contains an accepted generation and a\nrejected generation. This dataset  was in part generated by ChatGPT with scores on\nwhich output should be accepted and which rejected:\nfrom datasets  import load_dataset\ndef format_prompt (example):\n    """"""Format the prompt to using the <|user|> template TinyLLama is using""""""\n    # Format answers\n    system = ""<|system|> \n"" + example[""system"" ] + ""</s>\n""\n    prompt = ""<|user|> \n"" + example[""input""] + ""</s>\n<|assistant|> \n""\n    chosen = example[""chosen"" ] + ""</s>\n""\n    rejected  = example[""rejected"" ] + ""</s>\n""\n    return {\n        ""prompt"" : system + prompt,\n        ""chosen"" : chosen,\n        ""rejected"" : rejected ,\n    }\n# Apply formatting to the dataset and select relatively short answers\ndpo_dataset  = load_dataset (\n    ""argilla/distilabel-intel-orca-dpo-pairs"" , split=""train""\n)\ndpo_dataset  = dpo_dataset .filter(\n    lambda r: \n        r[""status"" ] != ""tie"" and \n        r[""chosen_score"" ] >= 8 and \n        not r[""in_gsm8k_train"" ]\n)\ndpo_dataset  = dpo_dataset .map(\n    format_prompt ,  remove_columns =dpo_dataset .column_names\n)\ndpo_dataset\nNote that we apply additional filtering to further reduce the size of the data to\nroughly 6,000 examples from the original 13,000 examples.\nModel Quantization\nWe load our base model and load it with the LoRA we created previously. As before,\nwe quantize the model to reduce the necessary VRAM for training:\nfrom peft import AutoPeftModelForCausalLM\nfrom transformers  import BitsAndBytesConfig , AutoTokenizer\n# 4-bit quantization configuration - Q in QLoRA\nbnb_config  = BitsAndBytesConfig (\n    load_in_4bit =True,  # Use 4-bit precision model loading\n386 | Chapter 12: Fine-Tuning Generation Models\n    bnb_4bit_quant_type =""nf4"",  # Quantization type\n    bnb_4bit_compute_dtype =""float16"" ,  # Compute dtype\n    bnb_4bit_use_double_quant =True,  # Apply nested quantization\n)\n# Merge LoRA and base model\nmodel = AutoPeftModelForCausalLM .from_pretrained (\n    ""TinyLlama-1.1B-qlora"" ,\n    low_cpu_mem_usage =True,\n    device_map =""auto"",\n    quantization_config =bnb_config ,\n)\nmerged_model  = model.merge_and_unload ()\n# Load LLaMA tokenizer\nmodel_name  = ""TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T""\ntokenizer  = AutoTokenizer .from_pretrained (model_name , trust_remote_code =True)\ntokenizer .pad_token  = ""<PAD>""\ntokenizer .padding_side  = ""left""\nNext, we use the same LoRA configuration as before to perform the DPO training:\nfrom peft import LoraConfig , prepare_model_for_kbit_training , get_peft_model\n# Prepare LoRA configuration\npeft_config  = LoraConfig (\n    lora_alpha =32,  # LoRA Scaling\n    lora_dropout =0.1,  # Dropout for LoRA Layers\n    r=64,  # Rank\n    bias=""none"",\n    task_type =""CAUSAL_LM"" ,\n    target_modules =  # Layers to target\n     [""k_proj"" , ""gate_proj"" , ""v_proj"" , ""up_proj"" , ""q_proj"" , ""o_proj"" , \n""down_proj"" ]\n)\n# prepare model for training\nmodel = prepare_model_for_kbit_training (model)\nmodel = get_peft_model (model, peft_config )\nTraining Configuration\nFor the sake of simplicity, we will use the same training arguments as we did before\nwith one difference. Instead of running for a single epoch (which can take up to\ntwo hours), we run for 200 steps instead for illustration purposes. Moreover, we\nadded the warmup_ratio  parameter, which increases the learning rate from 0 to the\nlearning_rate  value we set for the first 10% of steps. By maintaining a small learning\nrate at the start (i.e., warmup period), we allow the model to adjust to the data before\napplying larger learning rates, therefore avoiding harmful divergence:\nPreference Tuning with DPO | 387\nfrom trl import DPOConfig\noutput_dir  = ""./results""\n# Training arguments\ntraining_arguments  = DPOConfig (\n    output_dir =output_dir ,\n    per_device_train_batch_size =2,\n    gradient_accumulation_steps =4,\n    optim=""paged_adamw_32bit"" ,\n    learning_rate =1e-5,\n    lr_scheduler_type =""cosine"" ,\n    max_steps =200,\n    logging_steps =10,\n    fp16=True,\n    gradient_checkpointing =True,\n    warmup_ratio =0.1\n)\nTraining\nNow  that we have prepared all our models and parameters, we can start fine-tuning\nour model:\nfrom trl import DPOTrainer\n# Create DPO trainer\ndpo_trainer  = DPOTrainer (\n    model,\n    args=training_arguments ,\n    train_dataset =dpo_dataset ,\n    tokenizer =tokenizer ,\n    peft_config =peft_config ,\n    beta=0.1,\n    max_prompt_length =512,\n    max_length =512,\n)\n# Fine-tune model with DPO\ndpo_trainer .train()\n# Save adapter\ndpo_trainer .model.save_pretrained (""TinyLlama-1.1B-dpo-qlora"" )\nWe have created a second adapter. To merge both adapters, we iteratively merge the\nadapters with the base model:\nfrom peft import PeftModel\n# Merge LoRA and base model\nmodel = AutoPeftModelForCausalLM .from_pretrained (\n    ""TinyLlama-1.1B-qlora"" ,\n388 | Chapter 12: Fine-Tuning Generation Models\n23Jiwoo Hong, Noah Lee, and James Thorne, “ORPO: Monolithic preference optimization without reference\nmodel” . arXiv preprint arXiv:2403.07691  (2024).    low_cpu_mem_usage =True,\n    device_map =""auto"",\n)\nsft_model  = model.merge_and_unload ()\n# Merge DPO LoRA and SFT model\ndpo_model  = PeftModel .from_pretrained (\n    sft_model ,\n    ""TinyLlama-1.1B-dpo-qlora"" ,\n    device_map =""auto"",\n)\ndpo_model  = dpo_model .merge_and_unload ()\nThis combination of SFT+DPO is a great way to first fine-tune your model to\nperform basic chatting and then align its answers with human preference. However,\nit does come at a cost since we need to perform two training loops and potentially\ntweak the parameters in two processes.\nSince the release of DPO, new methods of aligning preferences have been developed.\nOf note is Odds Ratio Preference Optimization (ORPO), a process that combines SFT\nand DPO into a single training process.23 It removes the need to perform two separate\ntraining loops, further simplifying the training process while allowing for the use of\nQLoRA.\nSummary\nIn this chapter, we explored different steps of fine-tuning pretrained LLMs. We per‐\nformed fine-tuning by making use of parameter-efficient fine-tuning (PEFT) through\nthe low-rank adaptation (LoRA) technique. We explained how LoRA can be extended\nthrough quantization, a technique for reducing memory constraints when represent‐\ning the parameters of the model and adapters.\nThe fine-tuning process we explored has two steps. In the first step, we performed\nsupervised fine-tuning using instruction data on a pretrained LLM, often called\ninstruction tuning. This resulted in a model that has chat-like behavior and could\nclosely follow instructions.\nIn the second step, we further improved the model by fine-tuning it on alignment\ndata, data that represents what type of answers are preferred over others. This pro‐\ncess, referred to as preference tuning, distills human preference to the previously\ninstruction-tuned model.\nOverall, this chapter has shown the two major steps of fine-tuning a pretrained LLM\nand how that could lead to more accurate and informative outputs.\nSummary | 389",7234
149-Afterword.pdf,149-Afterword,"Afterword\nThank you to all who joined us on this fascinating journey through the world of\nlarge language models. We are grateful for your dedication to learning about these\npowerful models that have revolutionized language processing.\nThroughout this book, we have seen how LLMs work and how they can be used\nto create a wide range of applications, from simple chatbots to more complex sys‐\ntems like search engines. We have also explored various methods for fine-tuning\npretrained LLMs on specific tasks, including classification, generation, and language\nrepresentation. By mastering these techniques, readers will be able to unlock the\npotential of LLMs and create innovative solutions that can benefit from their capabili‐\nties. This knowledge will enable readers to stay ahead of the curve and adapt to new\ndevelopments in the field.\nAs we come to the end of this book, we want to emphasize that our exploration of\nLLMs is only just the beginning. There are many more exciting developments on\nthe horizon, and we encourage you to continue following the advancements in the\nfield. To help with this process, keep an eye out on the repository of this book as we\ncontinue to add resources.\nWe hope that by reading this book, you gained a deeper understanding of how LLMs\ncan be used in various applications and how they have the potential to transform\nindustries.\nWith this book as your guide, we believe that you will be well-equipped to navigate\nthe exciting landscape of LLMs and make meaningful contributions to this rapidly\nadvancing field.\n391",1574
150-Index.pdf,150-Index,"Index\nA\naccuracy\nconfusion matrices, 120\noutput verification, 192\nadaptive pretraining, 320\nagents, 218-223\nagentic RAG, 256\nReAct in LangChain, 221-223\nstep-by-step reasoning, 219-221\nAI (artificial intelligence)\naccelerated development of, 3\ndefined, 4\nALBERT, 115\nalign_labels function, 350\nall-MiniLM-L6-v2 model, 309\nall-mpnet-base-v2 model, 337\nAnnoy, 239\nAnthropic Claude, 29\nAPIs (application programming interfaces), 30\nCohere, xv, 230\nexternal, 134\ngenerating embeddings, 123\nOpenAI, xv, 133\nartificial intelligence (see AI)\nArXiv, 138\nattention, 11-18\noverview of, 11-14\nTransformer architecture, 15-18\nattention calculation, 91-93\nattention layer, 79, 86, 88, 106\nFlash Attention, 100\ngrouped-query attention, 98-100\nlocal attention, 96\nmulti-query attention, 98-100optimizing attention, 98\nself-attention and relevance scoring,\n93-94\nsparse attention, 96\nattention heads, 91-93, 107\naudience, in text-generation prompts, 178\nAugmented SBERT, 311-315\nautoregressive architecture, 12, 15, 22, 76, 97\nB\nbag-of-words model, 6-7\nembeddings, 10\ntopic modeling, 148-150, 156\nbenchmarks, in generative model evaluation,\n374\nBERT (Bidirectional Encoder Representations\nfrom Transformers), 18-20\nadoption by search engines, 225\nBERT-like models, 115\ncomparing to other trained tokenizers, 47\nfine-tuning pretrained BERT models,\n325-328\nmasked language modeling, 311\nTransformer blocks versus, 97\nBERTopic, 148-155\nalgorithmic variants, 152\nmodularity of, 151\nrepresentation blocks, 156-163\nKeyBERTInspired, 158\nmaximal marginal relevance, 159\ntext generation, 160-163\nBERTScore, 374\nbias and fairness, 28\n393\nBidirectional Encoder Representations from\nTransformers (see BERT)\nbitsandbytes package, 369\nBLEU, 374\nBLIP-2 (Bootstrapping Language-Image Pre-\ntraining for Unified Vision-Language\nUnderstanding and Generation 2)\nchat-based prompting, 283-286\nimage captioning, 280\npreprocessing text, 280\nQ-Former, 273-277\nBM25 algorithm, 233\nBPE (byte pair encoding), 43, 55\nbyte tokens, 45\nC\nc-TF-IDF, 149, 158, 161\ncapitalization, 56\ncaptioning, 280-283\ncentroid-based algorithms, 143\nchains, 182-184, 202-209\nchain-of-thought, 185-188\nchaining single prompt, 203-205\nsequential chaining of multiple prompts,\n206-209\ncharacter tokens, 45\nchat tokens, 54\nchat-based prompting, 283-286\nChatbot Arena, 376\nChatGPT, 202, 273, 383, 386\nrelease of, 3\ntext classification, 132-135\nchatgpt_generation function, 133\nchat_history input variable, 211\nclassification reports, 119-120\nclassification step, embedding model, 121\nCLIP, 265-268\nconnecting text and images, 265\ngenerating multimodal embeddings,\n265-268\nOpenCLIP, 268-272\nclosed-source LLMs, 29\n[CLS] token, 18, 48, 60, 270, 294, 318, 351\ncluster model, 142-144\nclustering (see text clustering)\nCNNs (convolutional neural networks), 260\nCohere, 252\nCommand R+, 30, 256\ncreating accounts, xv, 230generating embeddings, 123\nquery rewriting, 255\nRerank endpoint, 241\ncompletion models, 22\ncompute_metrics function, 351\nconfusion matrices, 119\nCoNLL-2003 dataset, 347\nconstrained sampling, 194-197\ncontext\nattention and, 88\nprompt engineering, 178\ntraining datasets, 64\ncontext length\ncompletion models, 22\ntoken processing limits, 81\ncontext window, in completion models, 22\ncontrastive learning\ntext embedding models, 291-293, 296\nword2vec algorithm and, 64-67\nContrastive Tension (CT), 316\nconversation buffer memory, 217\noverview of, 210-212\nwindowed, 212-214\nconversation summary memory, 214-217\nconvert_ids_to_tokens function, 269\nconvolutional neural networks (CNNs), 260\ncosine similarity, 125, 302-304\nCountVectorizer, 150\ncross-encoders, 244\ncross-entropy loss, 305\nCT (Contrastive Tension), 316\nD\ndata outliers, 143\nDataCollator class, 326, 342\ndatamapplot package, 163\nDBSCAN (Density-Based Spatial Clustering),\n143\nDeBERTa, 59, 115\ndecoder-only models (see generative models)\ndecoding strategy, 79-81, 106\ndense retrieval, 226, 228-240\ncaveats of, 234\nexample of, 230-234\nfine-tuning embedding models for, 239\nnearest neighbor search versus vector data‐\nbases, 238\ntext chunking, 235-237\ndensity-based algorithms, 143\n394 | Index\nDensity-Based Spatial Clustering (DBSCAN),\n143\ndimensionality reduction model, 140-142\nDistilBERT, 115, 120\ndomain adaptation, 320\ndo_sample parameter, 34\nDPO (Direct Preference Optimization),\n384-389\nfine-tuning, 388\nmodel quantization, 386\ntemplating alignment data, 386\ntraining configuration, 387\nDSPy, 200\nE\neasy negatives, 308\nElo rating system, 377\nembeddings, 8-10, 37, 57-70\ndense retrieval, 226, 228-240\nembedding models, defined, 114, 290\nmultimodality, 263-272\nCLIP, 265-268\nOpenCLIP, 268-272\noverview of, 8-10, 289-291\npositional embeddings, 102-104\nrecommendation systems, 67-70\ntext classification tasks that leverage,\n120-126\nsupervised classification, 121-123\nzero-shot classification, 123-126\ntext clustering pipeline, 139-146\ncluster model, 142-144\ndimensionality reduction model,\n140-142\nembedding model, 139\ninspecting clusters, 144-146\ntext embedding models, 61-63, 289-321\ncontrastive learning, 291-293\ncreating, 296-308\nfine-tuning, 309-315\nSBERT, 293-296\nunsupervised learning, 316\ntoken embeddings, 57-61\ncreating contextualized word embed‐\ndings, 58-61\ntokenizer’s vocabulary and, 57\ntypes of, 10\nword embeddings, 63-67\npretrained, 63word2vec algorithm and contrastive\ntraining, 64-67\nencoder-decoder models, 128\nencoder-only models (see representation mod‐\nels)\nethics, validating output, 192\nexponential backoff, 134\nF\nF1 score, confusion matrices, 120\nFAISS, 239\nFalcon, 168\nfeature extraction step, embedding model, 121\nfeedforward layer, 86\nfeedforward neural networks, 15, 87, 106, 326\nfew-shot classification, 333-339\nfine-tuning for classification, 337-339\nSetFit, 333-336\nfew-shot prompting, 181-182, 192-194\nfind_topics() function, BERTopic, 154\nfine-tuning\nembedding models for dense retrieval, 239\ngenerative models, 192, 355-389\nevaluating, 373-377\npreference tuning, 356, 378-389\nsupervised fine-tuning, 356-373\ntraining steps, 355-357\noverview of, 26\nrepresentation models, 323-354\nfew-shot classification, 333-339\nmasked language modeling, 340-345\nnamed-entity recognition, 345-353\nsupervised classification, 323-332\nT5 model, 129\ntext embedding models, 309-315\nAugmented SBERT, 311-315\nsupervised, 309-311\nFlan-T5 model, 50, 130-131\nFlash Attention, 100, 107\nforward pass, 106\ncomponents of, 76-79\ndefined, 74\nfoundation models, 23\nfp16 parameter, 299\nfreezing layers, 298, 328-332\nfrozen (nontrainable) models, 114, 121, 324\nG\nĠ symbol, 280\nIndex | 395\nGalactica, 52\nGeneral Language Understanding Evaluation\n(GLUE) benchmark, 297, 300, 359, 374\ngenerated_text variable, 281, 283\ngeneration_output variable, 42\ngenerative models, 20-22\nevaluating, 373-377\nautomated evaluation, 376\nbenchmarks, 374\nhuman evaluation, 376\nleaderboards, 376\nword-level metrics, 374\nfine-tuning, 355-389\nevaluation, 373-377\npreference tuning, 356, 378-389\nsupervised fine-tuning, 356-373\ntraining steps, 355-357\nreasoning, 184-191\nself-consistency, 188\ntree-of-thought, 189-191\nrepresentation models versus, 20\ntext classification, 127-135\nChatGPT, 132-135\nT5, 128-131\ngenerative pre-trained transformers (see GPTs)\nGenerative Pseudo-Labeling (GPL), 316\nGensim library, 63\nget_topic function, 153\nget_topic_info() method, 152\nGGUF model, 195, 200\nGitHub, xvi\nGloVe, 294\nGLUE (General Language Understanding Eval‐\nuation) benchmark, 297, 300, 359, 374\ngold datasets, 312\nGoodhart’s Law, 377\nGoogle Colab, xii, xiv, 29, 33, 85, 372\nGoogle Gemini, 250\nGoogle Search, 225\nGPL (Generative Pseudo-Labeling), 316\nGPT2Tokenizer, 280\nGPTs (generative pre-trained transformers), 20,\n167\n(see also text generation)\nGPT-1, 20\nGPT-2, 3, 20, 46, 49\nGPT-3, 20, 87, 96, 364\nGPT-3.5, 23, 132, 205, 221\nGPT-4, 23, 29, 38, 50, 87GPUs\nFlash Attention, 100\nrequirements, xiv, 28\nSRAM and HBM, 100\ngrammar, 192, 194-197\ngreedy decoding, 80\ngrounded generation, 250-252\ngrouped-query attention, 98-100, 107\nGSM8k, 374\nGuardrails, 194\nGuidance, 194\nH\nhallucination\navoiding in in instruction-based prompting,\n177\ntext generation models, 225\nhard negatives, 308\nharmful content, generating, 28\nHaystack, 200\nHBM (high bandwidth memory), 100\nHDBSCAN (Hierarchical Density-Based Spatial\nClustering of Applications with Noise), 143,\n153\nHellaSwag, 374\nhigh bandwidth memory (HBM), 100\nHugging Face, 32, 112\ncreating accounts, xv\nevaluate package, 351\ntokenizers, 56\nhuman evaluation, 145, 376\nHumanEval, 375\nhybrid search, 235, 242\nI\nIdefics 2, 277\nimages (see multimodality)\nin-context learning, 180-182\nindexes, 232\nInfoNCE, 304\ninput_ids variable, 40\ninstruction-based prompting, 175-177\nintellectual property, 28\nintuition-first philosophy, xi\ninvoke function, 204\nJ\nJupyter, 85\n396 | Index\nK\nk-means algorithm, 143, 151, 153\nKeyBERTInspired, 158, 160\nkeyword search\nreranking, 242-243\nverifying semantic search with, 233\nkv (keys and values) cache, 83-85\nL\nLangChain, 200\n(see also chains)\nloading quantized models with, 200-202\nReAct in, 221-223\nLanguage AI (Language Artificial Intelligence),\n3-24\ndefining, 4\nrecent history of, 5-24\nattention, 11-18\nbag-of-words model, 6-7\nembeddings, 8-10\ngenerative models, 20-22\nrepresentation models, 18-20\nY ear of Generative AI, 23-24\nlanguage modeling, 355\nlanguage modeling head (LM head), 76-79\nlarge language models (see LLMs)\nlatent Dirichlet allocation, 147\nLayerNorm, 101\nleaderboards, in generative model evaluation,\n376\nlearning_rate parameter, 371\nLlama, 168\nLlama 2, xv, 29, 53, 98, 168, 273\nllama-cpp-python library, 195\nLLaV A, 277\nLLM-as-a-judge, 257\nLLMs (large language models), 3-35\ncode examples and exercises, xvi\nembeddings, 37, 57-70\nrecommendation systems, 67-70\ntext embeddings, 61-63\ntoken embeddings, 57-61\nword embeddings, 63-67\nfine-tuning generative models, 355-389\nevaluation, 373-377\npreference tuning, 356, 378-389\nsupervised fine-tuning, 356-373\ntraining steps, 355-357\nfine-tuning representation models, 323-354few-shot classification, 333-339\nmasked language modeling, 340-345\nnamed-entity recognition, 345-353\nsupervised classification, 323-332\ngenerating text, 32-34\ngenerative models, 20-22\nhardware and software requirements, xiv, 28\nhigh-level view, 38\nhistory of Language AI, 5-24\ninterfacing with, 29-32\nclosed-source models, 29\nopen models, 30-32\nintuition-first philosophy, xi\nmoving definition of, 25\nmultimodality, 259-286\nembedding models, 263-272\ntext generation models, 273-286\nVision Transformer, 260-262\nprompt engineering, 167-198\nchain prompting, 182-184\nin-context learning, 180-182\ninstruction-based prompting, 175-177\noutput verification, 191-197\npotential complexity of prompts,\n177-179\nprompt components, 173-175\nreasoning with generative models,\n184-191\ntext generation models, 167-172\nrepresentation models, 18-20\nresponsible development and usage of, 28\nretrieval-augmented generation, 227,\n249-257\nagentic RAG, 256\nconverting search system to, 250\nevaluating results, 257\ngrounded generation, 252\nmulti-hop RAG, 256\nmulti-query RAG, 255\nquery rewriting, 255\nquery routing, 256\nwith local models, 252, 254\nsemantic search, 225-249\ndense retrieval, 226, 228-240\nreranking, 226, 240-244\nretrieval evaluation metrics, 244-249\ntext classification, 111-135\nwith generative models, 127-135\nmovie reviews, 112, 113\nIndex | 397\nwith representation models, 113-126\ntext clustering, 137-146\ntext embedding models, 289-321\ncontrastive learning, 291-293\ncreating, 296-308\nfine-tuning, 309-315\nSBERT, 293-296\nunsupervised learning, 316\ntext generation, 199-224\nagents, 218-223\nchains, 202-209\nmemory of conversations, 209-217\nmodel I/O, 200-202\ntokens and tokenizers, 37-61\ncomparing trained tokenizers, 46-54\ndownloading and running LLMs, 39-42\ninput preparation, 38\ntext breakdown, 43\ntoken embeddings, 57-61\ntokenization schemes, 44-46\ntokenizer properties, 55-56\ntopic modeling, 138, 146-163\ntraining paradigm of, 25-26\nTransformer architecture, 73-107\ndecoding strategy, 79-81\nforward pass components, 76-79\ninputs and outputs of, 74-76\nkeys and values cache, 83-85\nparallel token processing and context\nsize, 81-83\nrecent improvements to, 95-105\nTransformer blocks, 85-94\nutility of, 27\nLM head (language modeling head), 76-79\nLMQL, 194\nlocal attention, 96\nLoRA (low-rank adaptation), 361-364, 387\n(see also QLoRA)\nlora_alpha parameter, 370\nloss functions, 301-308\ncosine similarity loss, 302-304\nmultiple negatives ranking loss, 304-308\nlr_scheduler_type parameter, 371\nM\nMamba, 24\nMAP (mean average precision), 244-249\nMarginMSE loss, 302\nmasked language modeling (MLM), 340-345mask_token [MASK], 48\nMassive Text Embedding Benchmark (MTEB),\n116, 140, 253, 300\nmatplotlib library, 145\nmaximal marginal relevance (MMR), 159\nmax_new_tokens parameter, 34\nMcCarthy, John, 4\nmean average precision (MAP)\nmemory of conversations, 209-217\nconversation buffer, 210-212\nconversation summary, 214-217\nwindowed conversation buffer, 212-214\nMeta Llama model, 30\nMicrosoft Bing, 225, 241\nMicrosoft Bing AI, 250\nMicrosoft Phi model, 30\nmicrosoft/mpnet-base model, 297\nmin_cluster_size parameter, 144\nmin_dist parameter, 142\nMIRACL, 243\nMistral, 30, 168, 277\nMLM (masked language modeling), 340-345\nMMLU, 374, 376\nMMR (maximal marginal relevance), 159\nMNLI (Multi-Genre Natural Language Infer‐\nence) corpus, 297, 305\nMNR (multiple negatives ranking) loss,\n304-308\nmodel I/O, 200-202\nmonoBERT, 244\nMTEB (Massive Text Embedding Benchmark),\n116, 140, 253, 300\nMulti-Genre Natural Language Inference\n(MNLI) corpus, 297, 305\nmulti-hop RAG, 256\nmulti-query attention, 98-100\nmulti-query RAG, 255\nmultilevel perceptrons, 79\n(see also feedforward neural networks)\nmultimodality, 259-286\ndefined, 259\nembedding models, 263-272\nCLIP, 265-268\nOpenCLIP, 268-272\ntext generation models, 273-286\nBLIP-2, 273-277\nchat-based prompting, 283-286\nimage captioning, 280-283\npreprocessing images, 278\n398 | Index\npreprocessing text, 279\nVision Transformer, 260-262\nmultiple negatives ranking (MNR) loss,\n304-308\nN\nnamed-entity recognition (see NER)\nnatural language inference (NLI), 296\nnatural language processing (NLP), 4, 293\nnDCG (normalized discounted cumulative\ngain), 243, 249\nnearest neighbor search\npretrained word embeddings, 63\nrecommendation system embeddings, 68\nvector databases versus, 238\nnegative sampling, 65\nNER (named-entity recognition), 345-353, 360\nfine-tuning for, 352\npreparing data for, 347-351\nneural networks, 8\nNLI (natural language inference), 296\nNLP (natural language processing), 4, 293\nnoise-contrastive estimation, 65\nnonplayable characters (NPCs), 4\nnontrainable (frozen) models, 114, 121, 324\nnormalization, Transformer block, 101\nnormalized discounted cumulative gain\n(nDCG), 243, 249\nNPCs (nonplayable characters), 4\nNTXentLoss, 304\nnucleus sampling, 171\nNumPy, 238\nnum_train_epochs parameter, 299, 371\nNVIDIA GPUs, xiv, 33\nn_components parameter, 142\nO\nOdds Ratio Preference Optimization (ORPO),\n389\none-shot prompting, 182\nchain-of-thought versus, 186\nin-context learning, 181\nOpen LLM Leaderboard, 202, 376\nopen-source LLMs, 30-32\nOpenAI, 132\n(see also ChatGPT; GPTs)\ncreating accounts, xv, 133\ngenerating embeddings, 123\nOpenCLIP, 268-272optim parameter, 372\nORPO (Odds Ratio Preference Optimization),\n389\noutput verification, 191-197\nconstrained sampling, 194-197\nproviding examples, 192\nP\npad_token [PAD], 47\nparallel processing, 91, 106\nparallel prompts, 184\nPCA (Principal Component Analysis), 141\nPEFT (parameter-efficient fine-tuning),\n359-367\nadapters, 359-361\ncompression, 364-367\nLoRA, 361-364\npeft library, 370\npeft_config parameter, 371\nPerplexity, 250\npersona, in text-generation prompts, 178\nper_device_eval_batch_size argument, 299\nper_device_train_batch_size argument, 299\nPhi-3\ncomparing to other trained tokenizers, 53\nforward pass, 79\nloading quantized models, 202\nprompt template, 204\nquantization, 201\nPhi-3-mini, 32, 168\nPinecone, 239\npositional embeddings, 102-104\nPPO (Proximal Policy Optimization), 383\nprecision predictions, confusion matrices, 119\npredictions, task-specific model, 118\npreference tuning, 132, 356, 378-389\nDirect Preference Optimization, 384-389\nfine-tuning, 388\nmodel quantization, 386\ntemplating alignment data, 386\ntraining configuration, 387\nreward models, 379-383\ninputs and outputs of, 380\ntraining, 380-383\npretraining, defined, 26\nprimacy effect, 177\nPrincipal Component Analysis (PCA), 141\nprojection matrices, 92\nprompt engineering, 127, 167-198\nIndex | 399\nchain prompting, 182-184\nin-context learning, 180-182\ninstruction-based prompting, 175-177\noutput verification, 191-197\nconstrained sampling, 194-197\nproviding examples, 192\npotential complexity of prompts, 177-179\nprompt components, 173-175\nreasoning with generative models, 184-191\ntext generation models, 167-172\nchoosing, 167\ncontrolling output, 170-172\nloading, 168-169\nProximal Policy Optimization (PPO), 383\nPython, learning about, xii\nQ\nQ-Former (Querying Transformer), 274-277\nQ8 model, 201\nQLoRA (quantized low-rank adaptation),\n367-373\nfine-tuning, 372\nLoRA configuration, 370\nmerging weights, 373\nmodel quantization, 369\ntemplating instruction data, 367\ntraining configuration, 371\nquantization, 201, 364-367\nquantization_config parameter, 371\nquantized low-rank adaptation (see QLoRA)\nQuerying Transformer (Q-Former), 274-277\nR\nr parameter, 370\nRAG (retrieval-augmented generation), 57, 63,\n227, 249-257\nagentic RAG, 256\nbasic pipeline, 249\nconverting search system to, 250\nevaluating results, 257\ngrounded generation, 252\nwith local models, 252-254\nmulti-hop RAG, 256\nmulti-query RAG, 255\nquery rewriting, 255\nquery routing, 256\nRagas, 257\nrandom_state parameter, 142\nrate limit errors, 134ReAct\nin LangChain, 221-223\nstep-by-step reasoning, 219\nreasoning\nwith generative models, 184-191\nchain-of-thought, 185-188\nself-consistency, 188\ntree-of-thought, 189-191\nstep-by-step, 219, 221\nrecall predictions, confusion matrices, 119\nrecency effect, 177\nrecommendation systems, 67-70\nrecurrent neural networks (RNNs), 11\nreduce_outliers() function, 153\nregulation, 28\nrelevance scoring, 90, 92-94, 104\nrepository, xiv\nrepresentation models, 18-20\ndefined, 7\nfine-tuning for classification, 323-354\nfew-shot classification, 333-339\nmasked language modeling, 340-345\nnamed-entity recognition, 345-353\nsupervised classification, 323-332\ngenerative models versus, 20\ntext classification, 113-126\nclassification tasks that leverage embed‐\ndings, 120-126\nmodel selection, 115-116\ntask-specific models, 116\nrepresentation_model parameter, 161\nreranking, 226, 240-244\nBERTopic, 156\nexample of, 241-243\nfunction of reranking models, 244\nsentence transformers, 243\nresponse validation, in chain prompting, 184\nretrieval evaluation metrics, 244-249\nscoring multiple queries with mean average\nprecision, 248\nscoring single queries with average preci‐\nsion, 247\nretrieval-augmented generation (see RAG)\nreturn_full_text parameter, 33\nreward models, 379-383\ninputs and outputs of, 380\ntraining, 380-383\nRMSNorm, 101\nRNNs (recurrent neural networks), 11\n400 | Index\nRoBERTa, 46, 115\nRoPE (rotary positional embeddings), 102-104\nRorschach test, 282\nRotten Tomatoes dataset, 112, 325\nROUGE, 374\nRWKV, 24\nS\nSBERT, 293-296, 311-315\nself-attention, 15, 93-94\nself-consistency, 188\nsemantic search, 225-249\ndefined, 225\ndense retrieval, 226, 228-240\ncaveats of, 234\nexample of, 230-234\nfine-tuning embedding models for, 239\nnearest neighbor search versus vector\ndatabases, 238\ntext chunking, 235-237\nreranking, 226, 240-244\nexample of, 241-243\nfunction of reranking models, 244\nsentence transformers, 243\nretrieval evaluation metrics, 244-249\nscoring multiple queries with mean aver‐\nage precision, 248\nscoring single queries with average pre‐\ncision, 247\nSemantic Textual Similarity Benchmark\n(STSB), 298\nsemi-hard negatives, 308\nsentence-transformers, 62, 122, 151, 243, 272,\n293-295, 309, 333\nSentencePiece, 50\n[SEP] token, 47, 60, 351\nsequence-to-sequence models, 127, 128\nSetFit, 323, 333-336\nSFT (supervised fine-tuning), 356-373\nfull fine-tuning, 357\nparameter-efficient fine-tuning, 359-367\nadapters, 359-361\ncompression, 364-367\nLoRA, 361-364\nQLoRA, 367-373\nfine-tuning, 372\nLoRA configuration, 370\nmerging weights, 373\nmodel quantization, 369templating instruction data, 367\ntraining configuration, 371\nshared memory (SRAM), 100\nshortlisting, 242\nsilver datasets, 312\nSimCSE (Simple Contrastive Learning of Sen‐\ntence Embeddings), 316\nskip-gram, 65\nsoftmax loss function, 301\nsong recommendation systems, 67-70\nsparse attention, 96\nspecial tokens, 47, 55\nspecificity, in instruction-based prompting, 176\nSRAM (shared memory), 100\nStableLM, 168\nStarCoder2, 51\nstep-by-step reasoning, 219-221\nstructured output, validating, 191\nSTSB (Semantic Textual Similarity Bench‐\nmark), 298\nsubword tokens, 44\nsupervised classification, 121-123\nfine-tuning representation models for,\n323-332\nfreezing layers, 328-332\npretrained BERT models, 325-328\nsupervised fine-tuning (see SFT)\nsystem 1 and 2 thinking processes, 185\nT\nT5 (Text-to-Text Transfer Transformer),\n128-131\ntarget_modules parameter, 370\ntask-specific models, 113\ntemperature parameter, 171-172, 188\nTesla T4, 372\ntest splits, 113, 118\ntext chunking, 230, 235-237\napproaches for, 237\nmultiple vectors per document, 236\none vector per document, 236\ntext classification, 111-135\nwith generative models, 127-135\nChatGPT, 132-135\nT5, 128-131\nmovie reviews, 112-113\nwith representation models, 113-126\nclassification tasks that leverage embed‐\ndings, 120-126\nIndex | 401\nmodel selection, 115-116\ntask-specific models, 116-120\ntext clustering, 137-146\nCLIP embedding model and, 265\ncommon pipeline for, 139-146\ncluster model, 142-144\ndimensionality reduction model,\n140-142\nembedding model, 139\ninspecting clusters, 144-146\ntext embedding models, 61-63, 289-321\ncontrastive learning, 291-293\ncreating, 296-308\nevaluating, 300\ngenerating contrastive examples, 296\nloss functions, 301-308\ntraining, 297-300\nfine-tuning, 309\nAugmented SBERT, 311-315\nsupervised, 309-311\nSBERT, 293-296\nunsupervised learning, 316\ntext generation, 32-34, 199-224\nagents, 218-223\nReAct in LangChain, 221-223\nstep-by-step reasoning, 219-221\nchains, 202-209\nchaining single prompt, 203-205\nsequential chaining of multiple prompts,\n206-209\nmemory of conversations, 209-217\nconversation buffer, 210-212\nconversation summary, 214-217\nwindowed conversation buffer, 212-214\nmodel I/O, 200-202\nmultimodality, 273-286\nBLIP-2, 273-277\nchat-based prompting, 283-286\nimage captioning, 280-283\npreprocessing images, 278\npreprocessing text, 279\nprompt engineering, 167-172\nchoosing models, 167\ncontrolling output, 170-172\nloading models, 168-169\ntopic modeling, 160-163\ntext-in-text-out model, 74\nText-to-Text Transfer Transformer (T5),\n128-131thenlper/gte-small model, 140\n%%timeit magic command, 85\nTinyLlama, 367, 385\ntokenization-free encoding, 45\ntokens and tokenizers, 33, 37-61\nbag-of-words model, 6\ncomparing trained tokenizers, 46-54\nBERT base model (cased), 48\nBERT base model (uncased), 47\nFlan-T5, 50\nGalactica, 52\nGPT-2, 49\nGPT-4, 50\nPhi-3 and Llama 2, 53\nStarCoder2, 51\ndecoding strategy, 79-81\ndownloading and running LLMs, 39-42\nforward pass, 76-77\ninput preparation, 38\nmasked language modeling, 129\nparallel token processing and context size,\n81-83\nspecial tokens, 47\ntask-specific representation model, 117, 127\ntext breakdown, 43\ntext-focused versus code-focused models, 56\ntoken embeddings, 57-61, 77, 106\ncreating contextualized word embed‐\ndings, 58-61\ntokenizer’s vocabulary and, 57\ntoken spans, 129\ntokenization schemes, 44-46\nbyte tokens, 45\ncharacter tokens, 45\nsubword tokens, 44\nword tokens, 44\ntokenizer properties, 55-56\ndatasets, 56\nmethods, 55\nparameters, 55\nwhite space characters, 50\ntone of voice, in text-generation prompts, 178\ntopic modeling, 138, 146-163\nBERTopic, 148-155\nrepresentation blocks, 156-163\ntop_k parameter, 172\ntop_p parameter, 171, 188\ntrain splits, 113\nTrainingArguments class, 327\n402 | Index\ntransfer learning, 19\nTransformer architecture, 15-18, 73-107\nattention layer, 79, 86\ndecoding strategy, 79-81\nfeedforward layer, 86\nforward pass components, 76-79\ninputs and outputs of, 74-76\nkeys and values cache, 83-85\noptimizing attention, 98\nparallel token processing and context size,\n81-83\nrecent improvements to, 95-105\nmore efficient attention, 96-100\npositional embeddings, 102-104\nTransformer blocks, 101\nTransformer blocks, 85-94\nattention calculation, 91-93\nattention layer, 88\nattention mechanism, 89-91\nfeedforward neural networks, 87\nself-attention and relevance scoring,\n93-94\nVision Transformer, 260-262\ntransparency and accountability, 28\ntree-of-thought, 189-191\nTruthfulQA, 374, 376\nTSDAE (Transformer-Based Sequential Denois‐\ning Auto-Encoder)\nfor domain adaptation, 320\noverview of, 316-320\nU\nUltraChat dataset, 368\nUMAP (Uniform Manifold Approximation and\nProjection), 141\nunigram language model, 50\nunk_token [UNK], 47\nuse_cache parameter, 84\nV\nvalid output, verifying, 192\nvalidation splits, 113\nvector databases\ndense retrieval, 229nearest neighbor search versus, 238\nretrieval-augmented generation, 253\nvideo random-access memory (VRAM), xiv, 29\nvisualization\nBERTopic, 155\ncluster analysis, 144\ndimensionality reduction and, 145\nViT (Vision Transformer), 260-262, 274\nvocabulary, of tokenizers, 55, 57, 77, 106\nVRAM (video random-access memory), xiv, 29\nW\nwarmup_ratio parameter, 387\nwarmup_steps argument, 299\nWeaviate, 239\nwhitespace characters, 50\nwindowed conversation buffer memory,\n212-214, 217\nword embeddings, 63-67\npretrained, 63\nword2vec algorithm and contrastive train‐\ning, 64-67\nword tokens, 44\nword-level metrics, in generative model evalua‐\ntion, 374\nword2vec algorithm, 8, 10-12\ncontrastive training and, 64-67, 293\nembedding songs, 67\nWordPiece, 43\ncased BERT base model, 48\nuncased BERT base model, 47\n<work> token, 53\nY\nY ear of Generative AI, 23-24\nZ\nzero-shot classification, 123-126\nCLIP, 265\nSetFit, 339\nzero-shot prompting\nchain-of-thought, 187\nin-context learning, 181\nIndex | 403",26803
151-Colophon.pdf,151-Colophon,"About the Authors\nJay Alammar  is Director and Engineering Fellow at Cohere (pioneering provider of\nlarge language models as an API). In this role, he advises and educates enterprises\nand the developer community on using language models for practical use cases.\nThrough his popular AI/ML blog , Jay has helped millions of researchers and engi‐\nneers visually understand machine learning tools and concepts from the basic (end‐\ning up in the documentation of packages like NumPy and pandas) to the cutting-edge\n(Transformers, BERT, GPT-3, Stable Diffusion). Jay is also a co-creator of popular\nmachine learning and natural language processing courses on Deeplearning.ai and\nUdacity.\nMaarten Grootendorst  is a Senior Clinical Data Scientist at IKNL (Netherlands\nComprehensive Cancer Organization). He holds master’s degrees in organizational\npsychology, clinical psychology, and data science, which he leverages to communicate\ncomplex machine learning concepts to a wide audience. With his popular blogs ,\nhe has reached millions of readers by explaining the fundamentals of artificial intelli‐\ngence—often from a psychological point of view. He is the author and maintainer\nof several open source packages that rely on the strength of large language models,\nsuch as BERTopic, PolyFuzz, and KeyBERT. His packages are downloaded millions of\ntimes and used by data professionals and organizations worldwide.\nColophon\nThe animal on the cover of Hands-On Large Language Models  is a red kangaroo\n(Osphranter rufus ). They are the largest of all kangaroos, with a body length that can\nget up to a little over 5 feet and a tail as long as 3 feet. They are very fast and can hop\nto speeds over 35 miles per hour. They can jump 6 feet high and leap a distance of 25\nfeet in a single bound. The position of their eyes allows them see up to 300 degrees.\nRed kangaroos are named after the color of their fur. While the name makes sense for\nthe males—they have short, red-brown fur—females are typically more of a blue-grey\ncolor with a tinge of brown throughout. The red color in their fur comes from a red\noil excreted from the glands in their skin. Because of their color, Australians refer\nto male red kangaroos as “big reds. ” However, because females are faster than males,\nthey are often called “blue fliers. ”\nPreferring open, dry areas with some trees for shade, red kangaroos can be found\nacross Australia’s mainland except in the upper north, lower southwest, and east\ncoast regions of the country. Surrounding environmental conditions can affect repro‐\nduction. Because of this, females can pause or postpone pregnancy or birth until\nconditions are better. They often use this ability to delay birth of a new baby (joey)\nuntil the previous one has left their pouch.\nThe cover illustration is by Karen Montgomery, based on an antique line engraving\nfrom Cassell’s Popular Natural History . The series design is by Edie Freedman, Ellie\nVolckhausen, and Karen Montgomery. The cover fonts are Gilroy Semibold and\nGuardian Sans. The text font is Adobe Minion Pro; the heading font is Adobe Myriad\nCondensed; and the code font is Dalton Maag’s Ubuntu Mono.\nLearn from experts.  \nBecome one yourself.\nBooks | Live online courses   \nInstant answers | Virtual events  \nVideos | Interactive learning\nGet started at oreilly.com.  \n©2023 O’Reilly Media, Inc. O’Reilly is a registered trademark of O’Reilly Media, Inc.  175  7x9.1975",3467
