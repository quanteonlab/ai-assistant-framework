filename,title,text,len
01-Learn Generative AI with PyTorch.pdf,01-Learn Generative AI with PyTorch,"MANNINGMark Liu\nForeword by  Sarah Sanders\nInput embeddingEncode r\nInputsPositional\nencodingEncoder blockN blocksAbstract\nrepresentation\nOutput embeddingDecoder\nOutputsPositional\nencodingDecoder blockN blocksLinear layerOutput\nprobabilities\nSoftmax \nactivation\nThe Transformer architecture. The encoder in the Transformer (left side of the diagram), \nwhich consists of N identical encoder layers, learns the meaning of the input sequence \nand converts it into vectors that represent its meaning. It then passes these vectors \nto the decoder (right side of the diagram), which consists of N identical decoder layers. \nThe decoder constructs the output (e.g., the French translation of an English phrase) \nby predicting one token at a time, based on previous tokens in the sequence and vector \nrepresentations from the encoder. The generator on the top right is the head attached \nto the output from the decoder so that the output is the probability distribution over all \ntokens in the target language (e.g., the French vocabulary).\nMANNING\nShelter  ISlandLearn Generative AI  \nwith PyTorch\nMark Liu\nFOREWORD BY SARAH SANDERS\nFor online information and ordering of this and other Manning books, please visit www.manning.com. \nThe publisher offers discounts on this book when ordered in quantity.\nFor more information, please contact\nSpecial Sales Department\nManning Publications Co.\n20 Baldwin Road\nPO Box 761\nShelter Island, NY 11964\nEmail: orders@manning.com\n© 2024 Manning Publications Co. All rights reserved.\nNo part of this publication may be reproduced, stored in a retrieval system, or transmitted, in any form \nor by means electronic, mechanical, photocopying, or otherwise, without prior written permission of the \npublisher.\nMany of the designations used by manufacturers and sellers to distinguish their products are claimed \nas trademarks. Where those designations appear in the book, and Manning Publications was aware of a \ntrademark claim, the designations have been printed in initial caps or all caps.\nRecognizing the importance of preserving what has been written, it is Manning’s policy to have the books \nwe publish printed on acid-  free paper, and we exert our best efforts to that end. Recognizing also our \nresponsibility to conserve the resources of our planet, Manning books are printed on paper that is at \nleast 15 percent recycled and processed without the use of elemental chlorine.∞\n Manning Publications Co. \n20 Baldwin Road\nPO Box 761 \nShelter Island, NY 11964\nISBN 9781633436466\nPrinted in the United States of AmericaThe author and publisher have made every effort to ensure that the information in this book was correct \nat press time. The author and publisher do not assume and hereby disclaim any liability to any party for \nany loss, damage, or disruption caused by errors or omissions, whether such errors or omissions result \nfrom negligence, accident, or any other cause, or from any usage of the information herein.\n Development editor:  Rebecca Johnson\n Technical editors:  Emmanuel Maggiori  \n  and Wee Hyong Tok\n Review editor:  Dunja Nikitovi Ê\n Production editor:  Andy Marinkovich\n Copy editor:  Kari Lucke\n Proofreader:  Jason Everett\n Technical proofreader:  Kostas Passadis\n Typesetter:  Tamara Šveli Ê Sablji Ê\n Cover designer:  Marija Tudor\nTo all AI enthusiasts!",3382
02-contents.pdf,02-contents,"ivcontents\nforeword  xii\npreface  xiv\nacknowledgments  xvi\nabout this book  xvii\nabout the author  xxi\nabout the cover illustration  xxii\nPart 1  Introduction to generative AI  ................... 1\n1What is generative AI and why PyTorch?  3\n1.1 Introducing generative AI and PyTorch  5\nWhat is generative AI?  5  ■  The Python programming \nlanguage  7  ■  Using PyTorch as our AI framework  8\n1.2 GANs  9\nA high-level overview of GANs  9  ■  An illustrating  \nexample: Generating anime faces  11  ■  Why should you \ncare about GANs?  12\n1.3 Transformers  14\nThe attention mechanism  14  ■  The Transformer \narchitecture  15  ■  Multimodal Transformers and \npretrained LLMs  16\n1.4 Why build generative models from scratch?  18\n v contents  v\n 2 Deep learning with PyTorch  21\n 2.1 Data types in PyTorch  22\nCreating PyTorch tensors  23  ■  Index and slice PyTorch \ntensors  26  ■  PyTorch tensor shapes  27  ■  Mathematical \noperations on PyTorch tensors  28\n 2.2 An end-to-end deep learning project with PyTorch  29\nDeep learning in PyTorch: A high-level overview  29 \nPreprocessing data  31\n 2.3 Binary classification  33\nCreating batches  33  ■  Building and training a binary \nclassification model  34  ■  Testing the binary classification \nmodel  36\n 2.4 Multicategory classification  37\nValidation set and early stopping  37  ■  Building and training a \nmulticategory classification model  38\n 3 Generative adversarial networks: Shape and number    \n  generation  43\n 3.1 Steps involved in training GANs  44\n 3.2 Preparing training data  47\nA training dataset that forms an exponential growth curve  47 \nPreparing the training dataset  48\n 3.3 Creating GANs  49\nThe discriminator network  49  ■  The generator network  50 \nLoss functions, optimizers, and early stopping  51\n 3.4 Training and using GANs for shape generation  52\nThe training of GANs  53  ■  Saving and using the trained \ngenerator  58\n 3.5 Generating numbers with patterns  59\nWhat are one-hot variables?  59  ■  GANs to generate numbers \nwith patterns  61  ■  Training the GANs to generate numbers with \npatterns  63  ■  Saving and using the trained model  64\nvi contents vi\nPart 2  Image generation  ....................................... 67\n 4 Image generation with generative adversarial networks  69\n 4.1 GANs to generate grayscale images of clothing items  70\nTraining samples and the discriminator  71  ■  A generator to \ncreate grayscale images  72  ■  Training GANs to generate images \nof clothing items  74\n 4.2 Convolutional layers  78\nHow do convolutional operations work?  78  ■  How do stride and \npadding affect convolutional operations?  82\n 4.3 Transposed convolution and batch normalization  83\nHow do transposed convolutional layers work?  83 \nBatch normalization  85\n 4.4 Color images of anime faces  87\nDownloading anime faces  87  ■  Channels-first color images in \nPyTorch  88\n 4.5 Deep convolutional GAN  89\nBuilding a DCGAN  90  ■  Training and using DCGAN  93\n 5 Selecting characteristics in generated images  97\n 5.1 The eyeglasses dataset  98\nDownloading the eyeglasses dataset  98  ■  Visualizing images in \nthe eyeglasses dataset  99\n 5.2 cGAN and Wasserstein distance  101\nWGAN with gradient penalty  101  ■  cGANs  102\n 5.3 Create a cGAN  104\nA critic in cGAN  104  ■  A generator in cGAN  105 \nWeight initialization and the gradient penalty function  106\n 5.4 Training the cGAN  108\nAdding labels to inputs  108  ■  Training the cGAN  110\n 5.5 Selecting characteristics in generated images  112\nSelecting images with or without eyeglasses  113 \nVector arithmetic in latent space  116  ■  Selecting two \ncharacteristics simultaneously  118\n vii contents  vii\n 6 CycleGAN: Converting blond hair to black hair  123\n 6.1 CycleGAN and cycle consistency loss  124\nWhat is CycleGAN?  125  ■  Cycle consistency loss  126\n 6.2 The celebrity faces dataset  127\nDownloading the celebrity faces dataset  127  ■  Process the black \nand blond hair image data  129\n 6.3 Building a CycleGAN model  130\nCreating two discriminators  131  ■  Creating two \ngenerators  132\n 6.4 Using CycleGAN to translate between black and blond \nhair 135\nTraining a CycleGAN to translate between black and blond \nhair 135  ■  Round-trip conversions of black hair images and \nblond hair images  138\n 7 Image generation with variational autoencoders  142\n 7.1 An overview of AEs  145\nWhat is an AE?  145  ■  Steps in building and training an \nAE 146\n 7.2 Building and training an AE to generate digits  147\nGathering handwritten digits  147  ■  Building and training an \nAE 148  ■  Saving  and using the trained AE  151\n 7.3 What are VAEs?  151\nDifferences between AEs and VAEs  152  ■  The blueprint to train \na VAE to generate human face images  153\n 7.4 A VAE to generate human face images  154\nBuilding a VAE  155  ■  Training the VAE  157  ■  Generating \nimages with the trained VAE  158  ■  Encoding arithmetic with the \ntrained VAE  160\nPart 3  Natural language processing and  ..............  \n  Transformers  ............................................ 167\n 8 Text generation with recurrent neural networks  169\n 8.1 Introduction to RNNs  171\nChallenges in generating text  172  ■  How do RNNs work?  172 \nSteps in training a LSTM model  174\nviii contents viii\n 8.2 Fundamentals of NLP  175\nDifferent tokenization methods  176  ■  Word embedding  177\n 8.3 Preparing data to train the LSTM model  179\nDownloading and cleaning up the text  179  ■  Creating  \nbatches of training data  181\n 8.4 Building and training the LSTM model  182\nBuilding an LSTM model  182  ■  Training the LSTM \nmodel  184\n 8.5 Generating text with the trained LSTM model  185\nGenerating text by predicting the next token  186 \nTemperature and top-K sampling in text generation  188\n 9 A line-by-line implementation of attention and     \n  Transformer  194\n 9.1 Introduction to attention and Transformer  195\nThe attention mechanism  196  ■  The Transformer \narchitecture  200  ■  Different types of Transformers  204\n 9.2 Building an encoder  205\nThe attention mechanism  205  ■  Creating an encoder  208\n 9.3 Building an encoder-decoder Transformer  210\nCreating a decoder layer  210  ■  Creating  an encoder-decoder \nTransformer  212\n 9.4 Putting all the pieces together  213\nDefining a generator  213  ■  Creating a model to translate \nbetween two languages  214\n 10 Training a Transformer to translate English to French  217\n 10.1 Subword tokenization  218\nTokenizing English and French phrases  218  ■  Sequence padding \nand batch creation  223\n 10.2 Word embedding and positional encoding  227\nWord embedding  227  ■  Positional encoding  228\n 10.3 Training the Transformer for English-to-French \ntranslation  230\nLoss function and the optimizer  230  ■  The training loop  233\n 10.4 Translating English to French with the trained model  234\n ix contents  ix\n 11 Building a generative pretrained Transformer from    \n  scratch  238\n 11.1 GPT-2 architecture and causal self-attention  240\nThe architecture of GPT-2  240  ■  Word embedding and positional \nencoding in GPT-2  242  ■  Causal self-attention in GPT-2  243\n 11.2 Building GPT-2XL from scratch  247\nBPE tokenization  248  ■  The Gaussian error linear unit \nactivation function  249  ■  Causal self-attention  251 \nConstructing the GPT-2XL model  252\n 11.3 Loading up pretrained weights and generating text  256\nLoading up pretrained parameters in GPT-2XL  256 \nDefining a generate() function to produce text  258\nText generation with GPT-2XL  260\n 12 Training a Transformer to generate text  264\n 12.1 Building and training a GPT from scratch  266\nThe architecture of a GPT to generate text  267 \nThe training process of the GPT model to generate text  269\n 12.2 Tokenizing text of Hemingway novels  270\nTokenizing the text  271  ■  Creating batches for training  274\n 12.3 Building a GPT to generate text  276\nModel hyperparameters  276  ■  Modeling the causal self-attention \nmechanism  277  ■  Building the GPT model  278\n 12.4 Training the GPT model to generate text  280\nTraining the GPT model  280  ■  A function to generate text  281 \nText generation with different versions of the trained model  283\nPart 4  Applications and new developments  .....289\n 13 Music generation with MuseGAN  291\n 13.1 Digital music representation  293\nMusical notes, octave, and pitch  293  ■  An introduction to \nmultitrack music  294  ■  Digitally represent music: Piano \nrolls 297\n 13.2 A blueprint for music generation  300\nConstructing music with chords, style, melody, and groove  300 \nA blueprint to train a MuseGAN  302\nx contents x\n 13.3 Preparing the training data for MuseGAN  304\nDownloading the training data  304  ■  Converting \nmultidimensional objects to music pieces  305\n 13.4 Building a MuseGAN  307\nA critic in MuseGAN  307  ■  A generator in MuseGAN  308 \nOptimizers and the loss function  311\n 13.5 Training the MuseGAN to generate music  312\nTraining the MuseGAN  313  ■  Generating music with the \ntrained MuseGAN  315\n 14 Building and training a music Transformer  318\n 14.1 Introduction to the music Transformer  320\nPerformance-based music representation  320  ■  The music \nTransformer architecture  324  ■  Training the music \nTransformer  326\n 14.2 Tokenizing music pieces  327\nDownloading training data  328  ■  Tokenizing MIDI files  329 \nPreparing the training data  331\n 14.3 Building a GPT to generate music  333\nHyperparameters in the music Transformer  334  ■  Building a \nmusic Transformer  335\n 14.4 Training and using the music Transformer  336\nTraining the music Transformer  336  ■  Music generation with \nthe trained Transformer  337\n 15 Diffusion models and text-to-image Transformers  341\n 15.1 Introduction to denoising diffusion models  342\nThe forward diffusion process  343  ■  Using the U-Net model to \ndenoise images  345  ■  A blueprint to train the denoising U-Net \nmodel  347\n 15.2 Preparing the training data  348\nFlower images as the training data  348  ■  Visualizing the \nforward diffusion process  350\n 15.3 Building a denoising U-Net model  351\nThe attention mechanism in the denoising U-Net model  352 \nThe denoising U-Net model  353\n xi contents  xi\n 15.4 Training and using the denoising U-Net model  354\nTraining the denoising U-Net model  355  ■  Using the trained \nmodel to generate flower images  357\n 15.5 Text-to-image Transformers  360\nCLIP: A multimodal Transformer  360  ■  Text-to-image \ngeneration with DALL-E 2  362\n 16 Pretrained large language models and the LangChain    \n  library  365\n 16.1 Content generation with the OpenAI API  367\nText generation tasks with OpenAI API  367  ■  Code generation \nwith OpenAI API  370  ■  Image generation with OpenAI \nDALL-E 2  371  ■  Speech generation with OpenAI API  372\n 16.2 Introduction to LangChain  372\nThe need for the LangChain library  373  ■  Using the OpenAI \nAPI in LangChain  374  ■  Zero-shot, one-shot, and few-shot \nprompting  374\n 16.3 A zero-shot know-it-all agent in LangChain  376\nApplying for a Wolfram Alpha API Key  377  ■  Creating an \nagent in LangChain  378  ■  Adding tools by using OpenAI \nGPTs  380  ■  Adding tools to generate code and images  382\n 16.4 Limitations and ethical concerns of LLMs  384\nLimitations of LLMs  384  ■  Ethical concerns for LLMs  385\n appendix A  Installing Python, Jupyter Notebook, and PyTorch  388\n appendix B  Minimally qualified readers and deep learning basics  395\n  index  401",11616
03-foreword.pdf,03-foreword,"xiiforeword\nI first met Mark at the PNC Innovation Summit at the University of Kentucky, at which \nwe were both presenters. His topic was How Machines Learn . From our very first encoun -\nter, I was struck by Mark’s ability to explain complex concepts in an engaging and \neasy-to-understand manner. His knack for breaking down intricate ideas into digest -\nible, relatable terms was truly impressive, and it’s a gift that he now shares through his \nlatest book, Learn Generative AI with PyTorch .\nAt Native AI, where I am cofounder and chief operating officer, we are tasked with \ngenerating predictive synthetic data that is both highly accurate and robust. Mark’s \nexploration of techniques like temperature and top-K sampling to control the preci -\nsion of AI-generated text is cutting-edge. These methods are essential for tailoring nat -\nural language processing outputs to specific use cases, a topic that will continue to grow \nin importance and business value.\nLearn Generative AI with PyTorch  is a comprehensive guide that not only introduces \nreaders to the fascinating world of generative AI but also equips them with practical \nskills to build and implement their own models. Mark’s use of PyTorch as the framework \nof choice is a testament to its flexibility and power in developing advanced AI models. \nFrom long short-term memory models to variational autoencoders, generative adver -\nsarial networks, and Transformers, this book covers an impressive breadth of topics.\nMark’s book is an invaluable resource for anyone looking to dive into generative AI, \nwhether they are beginners seeking to understand the basics or experienced practi -\ntioners aiming to expand their knowledge and skills. His ability to make complex topics \naccessible and engaging ensures that readers will come away with a solid understanding \nand the confidence to apply what they’ve learned.\n xiii foreword  xiii\nI am honored to write the foreword for this exceptional book and am excited for the \nmany readers who will benefit from Mark’s expertise. Learn Generative AI with PyTorch  is \nsure to inspire and educate, paving the way for future innovations in the field of gener -\native AI.\n—Sarah Sanders, Cofounder and COO, NativeAI",2253
04-preface.pdf,04-preface,"xivpreface\nMy fascination with generative AI began a few years ago when I first saw models con -\nverting horse images into zebra images and Transformers producing lifelike text. This \nbook is born out of my journey in building and understanding these models from \nscratch. It’s the book I wish I had during my experiments with various generative mod -\nels. It begins with simple models, helping readers build foundational deep learning \nskills before advancing to more complex challenges. I chose PyTorch for its dynamic \ncomputational graph and clear syntax after experimenting with TensorFlow.\nAll generative models in this book are deep neural networks. The book starts with \na comprehensive deep learning project in PyTorch, ideal for those new to the field. \nEach chapter is carefully structured to build upon the previous one. You’ll first learn to \ncreate basic content, such as shapes, numbers, and images using generative adversarial \nnetworks with straightforward architectures. As you progress, the complexity increases, \nculminating in building state-of-the-art models such as Transformers to generate text \nand music and diffusion models to generate high-resolution images.\nOn the surface, this book provides an exploration of various generative AI models. At \na deeper level, the technological journey reflects how our mind works and the essence \nof what it means to be human. The prominence of deep neural networks in these gen -\nerative models is a testament to our quest to understand and replicate the complex pro -\ncesses of human learning. Generative AI models, drawing inspiration from the marvels \nof evolutionary biology that shaped our brains, learn from the vast amount of data they \nencounter, much like we humans learn from the stimuli around us.\nThe implications of generative AI extend far beyond its practical applications. As we \nstand at the forefront of this technological revolution, we are compelled to re-evaluate \nour understanding of consciousness, life, and the very nature of human existence. The \nparallels between machine learning and human learning are striking. Just as generative \n xv preface  xv\nAI operates through neural networks inspired by the human brain, our thoughts, emo -\ntions, and behaviors are the outputs of the neural networks within our body. Thus, \nthe study of generative AI transcends technological boundaries, becoming an explo -\nration of the human condition and the mechanisms that underlie our consciousness. \nThe study of generative AI leads us to a profound speculation: Are humans, in essence, \nsophisticated generative AI models?\nIn that sense, generative AI is not just a tool: it is a mirror reflecting our deepest \nexistential questions. As we continue to develop and interact with these technologies, \nwe are not only shaping the future of artificial intelligence  but also deepening our under -\nstanding of human intelligence . Ultimately, the exploration of generative AI is an explo -\nration of ourselves, a journey into the heart of consciousness and the essence of life, \nchallenging us to redefine what it means to be conscious, to be alive, and to be human.",3178
05-about this book.pdf,05-about this book,"xviacknowledgments\nMany people have helped to make this book a reality. Jonathan Gennick, my acquisi -\ntion editor at Manning, played a crucial role in identifying the topics readers are eager \nto learn and in structuring the chapters to facilitate learning. A special thanks goes \nto my developmental editor, Rebecca Johnson, whose relentless pursuit of perfection \nsignificantly improved the book. She encouraged me to explain complex concepts in a \nclear and understandable manner. \nMy gratitude also extends to my technical editor, Emmanuel Maggiori, author of \nSmart Until It’s Dumb (Applied Maths Ltd., 2023). Every time I got carried away in my \nwriting about AI’s wondrous potential, Emmanuel was always quick to point out its \nlimitations. While my favorite quote is, “Any sufficiently advanced technology is indis -\ntinguishable from magic” by Arthur C. Clarke, Emmanuel’s perspective on AI can be \nsummed up by the title of his book. This clash of viewpoints, I believe, provides our \nreaders with a more balanced perspective. \nThank you to all the reviewers: Abhilash Babu, Ankit Virmani, Arpit Singh, Chris -\ntopher Kottmyer, David Cronkite, Eduardo Rienzi, Erim Erturk, Francis Osei Annin, \nGeorg Piwonka, Holger Voges, Ian Long, Japneet Singh, Karrtik Iyer, Kollin Trujillo, \nMichael Petrey, Mirerfan Gheibi, Nathan Crocker, Neeraj Gupta, Neha Shetty, Palak \nMathur, Peter Henstock, Piergiorgio Faraglia, Rajat Kant Goel, Ramaa Vissa, Ravi Kiran \nBamidi, Richard Tobias, Ruud Gijsen, Slavomir Furman, Sumit Pal, Thiago Britto \nBorges, Tony Holdroyd, Ursin Stauss, Vamsi Srinivas Parasa, Viju Kothuvatiparambil, \nand Walter Alexander Mata López, your suggestions helped make this a better book. \nI also wish to thank the production team at Manning Publications for helping me \nbring this project to completion. \nFinally, I want to express my deepest gratitude to my wife, Ivey Zhang, and my son, \nAndrew Liu, for their unwavering support throughout this journey.",2001
06-How this book is organized a roadmap.pdf,06-How this book is organized a roadmap,"xviiabout this book\nLearn Generative AI with PyTorch  aims to guide you through the creation of various con -\ntent (shapes, numbers, images, text, and music) from scratch. It begins with simple \nmodels, helping readers build foundational deep learning skills before advancing to \nmore complex challenges. All generative models in this book are deep neural net -\nworks. The book starts with a comprehensive deep learning project in PyTorch, ideal \nfor those new to the field. Each chapter is carefully structured to build upon the previ -\nous one. You’ll first create basic content like shapes, numbers, and images using gen -\nerative adversarial networks with straightforward architectures. As you progress, the \ncomplexity increases, culminating in building state-of-the-art models like Transformers \nand diffusion models. \nWho should read this book?\nLearn Generative AI with PyTorch  is designed for machine learning enthusiasts and data \nscientists in various business fields who possess intermediate Python programming \nskills. This book aims to teach generative AI techniques for creating novel and inno -\nvative content, such as images, text, patterns, numbers, shapes, and audio, to enhance \nboth their employers’ businesses and their own careers. While many free learning mate -\nrials are available online covering individual topics, this book consolidates everything \ninto a clear, easy-to-follow, and up-to-date format, making it an invaluable resource for \nanyone aspiring to become an expert in generative AI.\nI assume the readers have a solid grasp of Python. You should be familiar with variable \ntypes, Python functions and classes, and the installation of third-party Python libraries \nand packages. If you need to brush up on these skills, the free online Python tutorial \nprovided by W3Schools is a great resource ( https://www.w3schools.com/python/ ). \nxviii about  this book xviii\nYou also should have a basic understanding of machine learning, particularly neural \nnetworks and deep learning. If not, a good book for this purpose is Deep Learning with \nPyTorch  by Stevens, Antiga, and Viehmann (2020), also published by Manning Publica -\ntions.  Appendix B of this book provides a review of key concepts such as loss functions, \nactivation functions, and optimizers, which are essential for developing and training \ndeep neural networks. However, this appendix is not meant to be a comprehensive tuto -\nrial on these topics.\nHow this book is organized: a roadmap\nThis book has 16 chapters, organized into four parts. Part I introduces you to genera -\ntive AI and deep learning with PyTorch.\n¡ Chapter 1 explains what generative AI is and the rationale behind selecting \nPyTorch over other AI frameworks like TensorFlow for building generative mod -\nels in this book.\n¡ Chapter 2 uses PyTorch to create deep neural networks to perform binary and \nmulticategory classifications so that you become well-versed in deep learning \nand classification tasks. The intention is to get you ready for the upcoming chap -\nters, where you use deep neural networks in PyTorch to create various generative \nmodels.\n¡ Chapter 3 introduces you to generative adversarial networks (GANs). You learn \nto use GANs to generate shapes and sequences of numbers with certain patterns.\nPart II covers image generation. \n¡ Chapter 4 discusses how to build and train GANs to generate high-resolution \ncolor images. In particular, you’ll learn to use convolutional neural networks to \ncapture spatial features in images. You’ll also learn to use transposed convolu -\ntional layers to upsample and generate high-resolution feature maps in images. \n¡ Chapter 5 details two ways to select characteristics in generated images. The \nfirst method involves selecting specific vectors in the latent space. The second \nmethod uses a conditional GAN, where you build and train a GAN with labeled \ndata. \n¡ Chapter 6 teaches you how to use a CycleGAN to translate images between two \ndomains such as images with black hair and images with blond hair or horse \nimages and zebra images. \n¡ Chapter 7 explains how to generate high-resolution images using another gener -\native model: autoencoders and their variant, variational autoencoders. \nPart III dives into natural language processing and text generation.\n¡ Chapter 8 discusses text generation with a recurrent neural network. Along the \nway, you learn how tokenization and word embedding work. You’ll also learn to \n xix about  this book  xix\ngenerate text autoregressively with the trained model and how to use tempera -\nture and top-K sampling to control the creativity of the generated text. \n¡ Chapter 9 builds a Transformer from scratch, based on the paper “Attention Is \nAll You Need,” to translate between any two languages. You’ll implement line by \nline the multihead attention mechanism and an encoder-decoder Transformer. \n¡ Chapter 10 trains the Transformer you built in chapter 9 with more than 47,000 \npairs of English-to-French translations. You’ll learn to translate common English \nphrases to French with the trained model. \n¡ Chapter 11 builds GPT-2XL, the largest version of GPT-2, from scratch. After \nthat, you’ll learn how to extract the pretrained weights from Hugging Face and \nload them to your own GPT-2 model to generate text. \n¡ Chapter 12 constructs a scaled-down version of the GPT model with approxi -\nmately 5 million parameters so that you can train it on a regular computer. You’ll \nuse three novels by Ernest Hemingway as the training data. The trained model \ncan generate text in Hemingway style. \nPart IV discusses some practical applications of the generative models in the book and \nthe most recent developments in the field of generative AI.\n¡ Chapter 13 builds and trains a MuseGAN to generate music. MuseGAN treats a \npiece of music as a multidimensional object akin to an image. The generator pro -\nduces a complete piece of music and submits it to the critic for evaluation. The \ngenerator then modifies the music based on the critic’s feedback until it closely \nresembles real music from the training dataset. \n¡ Chapter 14 takes a different approach to AI music creation. Instead of treating a \npiece of music as a multidimensional object, you treat it as a sequence of musical \nevents. You’ll then apply techniques from text generation to predict the next \nelement in a sequence. \n¡ Chapter 15 introduces you to diffusion models, which form the foundation of all \nleading text-to-image Transformers (such as DALL-E 2 or Imagen). You’ll build \nand train a diffusion model to generate high-resolution flower images.\n¡ Chapter 16 ends the book with a project in which you use the LangChain library \nto combine pretrained large language models with Wolfram Alpha and Wikipe -\ndia APIs to create a zero-shot know-it-all personal assistant.\nAppendix A discusses how to install PyTorch on your computer, with or without a \ncompute unified device architecture-enabled GPU . Appendix B provides informa -\ntion on what background you need in order to proceed with projects in this book and \nsome basic concepts in deep learning such as loss functions, activation functions, and \noptimizers.",7261
07-About the code.pdf,07-About the code,,0
08-Part 1.pdf,08-Part 1,"xx about  this book xx\nAbout the code\nThis book contains many examples of source code both in numbered listings and in \nline with normal text. In both cases, source code is formatted in a fixed-width font \nlike this  to separate it from ordinary text. Sometimes code is also in bold  to high -\nlight code that has changed from previous steps in the chapter, such as when a new \nfeature adds to an existing line of code.\nIn many cases, the original source code has been reformatted; we’ve added line \nbreaks and reworked indentation to accommodate the available page space in the book. \nIn rare cases, even this was not enough, and listings include line-continuation markers \n(➥). Additionally, comments in the source code have often been removed from the list -\nings when the code is described in the text. Code annotations accompany many of the \nlistings, highlighting important concepts.\nYou can get executable snippets of code from the liveBook (online) version of this \nbook at https: //livebook.manning.com/book/learn-generative-ai-with-pytorch . All \nPython programs in this book are available for download from the Manning website \nat www.manning.com and from the book’s GitHub repository at https: //github.com/\nmarkhliu/DGAI . The programs are organized by chapters with each chapter in a single \nJupyter Notebook file. See appendix A on how to install Python, PyTorch, and Jupyter \nNotebook on your computer. \nliveBook discussion forum\nPurchase of Learn Generative AI with PyTorch  includes free access to liveBook, Manning’s \nonline reading platform. Using liveBook’s exclusive discussion features, you can \nattach comments to the book globally or to specific sections or paragraphs. It’s a \nsnap to make notes for yourself, ask and answer technical questions, and receive \nhelp from the author and other users. To access the forum, go to https: //livebook  \n.manning.com/book/learn-generative-ai-with-pytorch/discussion . You can also learn \nmore about Manning’s forums and the rules of conduct at https: //livebook.manning  \n.com/discussion . \nManning’s commitment to our readers is to provide a venue where a meaningful dia -\nlogue between individual readers and between readers and the author can take place. It \nis not a commitment to any specific amount of participation on the part of the author, \nwhose contribution to the forum remains voluntary (and unpaid). We suggest you try \nasking the author some challenging questions lest their interest stray! The forum and \nthe archives of previous discussions will be accessible from the publisher’s website for as \nlong as the book is in print.\nxxiabout the author\nDr. Mark Liu  is a tenured finance professor and the \n(founding) director of the Master of Science in Finance \nprogram at the University of Kentucky. He is the author \nof two books: Make Python Talk  (No Starch Press, 2021) \nand Machine Learning, Animated  (CRC Press, 2023). \nMark has more than 20 years of coding experience. He \nobtained his PhD in finance from Boston College. Mark \nhas published his research in top finance journals such \nas the Journal of Financial Economics , the Journal of Finan -\ncial and Quantitative Analysis , and the Journal of Corporate \nFinance .\n\nxxiiabout the cover illustration\nThe figure on the cover of Learn Generative AI with PyTorch , captioned “L’Agent de la \nrue de Jerusalem,” or “The Jerusalem Street Agent,” is taken from a book by Louis Cur -\nmer published in 1841. Each illustration is finely drawn and colored by hand. \nIn those days, it was easy to identify where people lived and what their trade or station \nin life was just by their dress. Manning celebrates the inventiveness and initiative of the \ncomputer business with book covers based on the rich diversity of regional culture cen -\nturies ago, brought back to life by pictures from collections such as this one.\nPart 1\nIntroduction to  \ngenerative AI\nW hat is generative AI? How is it different from its nongenerative counter -\nparts, discriminative models? Why do we choose PyTorch as the AI framework in \nthis book? \nIn this part, we answer these questions. In addition, all generative AI models in \nthis book are deep neural networks. Therefore, you’ll learn how to use PyTorch \nto create deep neural networks to perform binary and multicategory classifica -\ntions so that you become well versed in deep learning and classification tasks. The \nintention is to get you ready for the upcoming chapters, where you use deep neu -\nral networks in PyTorch to create various generative models. You’ll also learn to \nuse PyTorch to build and train generative adversarial networks to generate shapes \nand sequences of numbers.",4729
09-1 What is generative AI and why PyTorch.pdf,09-1 What is generative AI and why PyTorch,"31What is generative AI \nand why PyTorch?\nThis chapter covers\n¡ Generative AI vs. nongenerative AI\n¡ Why PyTorch is ideal for deep learning and  \n generative AI\n¡ The concept of Generative Adversarial Networks \n¡ The benefits of the attention mechanism and   \n Transformers\n¡ Advantages of creating generative AI models from  \n scratch\nGenerative AI has significantly affected the global landscape, capturing widespread \nattention and becoming a focal point since the advent of ChatGPT in November \n2022. This technological advancement has revolutionized numerous aspects of \neveryday life, ushering in a new era in technology and inspiring a host of startups to \nexplore the extensive possibilities offered by various generative models.\nConsider the advancements made by Midjourney, a pioneering company, which \nnow creates high-resolution, realistic images from brief text inputs. Similarly, Fresh -\nworks, a software company, has accelerated application development dramatically, \nreducing the time required from an average of 10 weeks to mere days, a feat achieved \nthrough the capabilities of ChatGPT (see the Forbes  article “10 Amazing Real-World \nExamples of How Companies Are Using ChatGPT in 2023,” by Bernard Barr, 2023, \n4 chapter  1 What is generative AI and why PyTorch?\nhttps: //mng.bz/Bgx0 ). To add a case in point, elements of this very introduction have \nbeen enhanced by generative AI, demonstrating its ability to refine content to be more \nengaging.\nNOTE     What better way to explain generative AI than letting generative AI do \nitself? I asked ChatGPT to rewrite an early draft of this introduction in a “more \nengaging manner” before finalizing it.\nThe repercussions of this technological advancement extend far beyond these exam -\nples. Industries are experiencing significant disruption due to the advanced capabili -\nties of generative AI. This technology now produces essays comparable to those written \nby humans, composes music reminiscent of classical compositions, and rapidly gener -\nates complex legal documents, tasks that typically require considerable human effort \nand time. Following the release of ChatGPT, CheggMate, an educational platform, \nwitnessed a significant decrease in its stock value. Furthermore, the Writers Guild of \nAmerica, during a recent strike, reached a consensus to put guardrails around AI’s \nencroachment on scriptwriting and editing (see the WIRED  article “Hollywood Writers \nReached an AI Deal That Will Rewrite History,” by Will Bedingfield, 2023, https: //\nmng.bz/1ajj ).\nNOTE     CheggMate charges college students to have their questions answered by \nhuman specialists. Many of these jobs can now be done by ChatGPT or similar \ntools at a fraction of the costs.\nThis raises several questions: What is generative AI, and how does it differ from other AI \ntechnologies? Why is it causing such widespread disruption across various sectors? What \nis the underlying mechanism of generative AI, and why is it important to understand?\nThis book offers an in-depth exploration of generative AI, a groundbreaking tech -\nnology reshaping numerous industries through its efficient and rapid content creation \ncapabilities. Specifically, you’ll learn to use state-of-the-art generative models to create \nvarious forms of content: shapes, numbers, images, text, and audio. Further, instead of \ntreating these models as black boxes, you’ll learn to create them from scratch so that \nyou have a deep understanding of the inner workings of generative AI. In the words of \nphysicist Richard Feynman, “What I cannot create, I do not understand.” \nAll these models are based on deep neural networks, and you’ll use Python and \nPyTorch to build, train, and use these models. We chose Python for its user-friendly syn -\ntax, cross-platform compatibility, and wide community support. We also chose PyTorch \nover other frameworks like TensorFlow for its ease of use and adaptability to various \nmodel architectures. Python is widely regarded as the primary tool for machine learn -\ning (ML), and PyTorch has become increasingly popular in the field of AI. Therefore, \nusing Python and PyTorch allows you to follow the new developments in generative AI. \nBecause PyTorch allows for graphics processing unit (GPU) training acceleration, you’ll \ntrain these models in a matter of minutes or hours and witness generative AI in action!",4441
10-1.1 Introducing generative AI and PyTorch.pdf,10-1.1 Introducing generative AI and PyTorch,,0
11-1.1.1 What is generative AI.pdf,11-1.1.1 What is generative AI,"5 Introducing generative AI and PyTorch\n1.1 Introducing generative AI and PyTorch\nThis section explains what generative AI is and how it’s different from its nongenera -\ntive counterparts: discriminative models. Generative AI is a category of technologies \nwith the remarkable capacity to produce diverse forms of new content, including text, \nimages, audio, video, source code, and intricate patterns. Generative AI crafts entirely \nnew worlds of novel and innovative content; ChatGPT is a notable example. In con -\ntrast, discriminative modeling predominantly concerns itself with the task of recogniz -\ning and categorizing pre-existing content. \n1.1.1 What is generative AI?\nGenerative AI is a type of artificial intelligence that creates new content, such as text, \nimages, or music, by learning patterns from existing data. It differs from discriminative \nmodels, which specialize in discerning disparities among distinct data instances and \nlearning the boundary between classes. Figure 1.1 illustrates the difference between \nthese two modeling methods. For instance, when confronted with an array of images \nfeaturing dogs and cats, a discriminative model determines whether each image por -\ntrays a dog or a cat by capturing a few key features that distinguish one from the other \n(e.g., cats have small noses and pointy ears). As the top half of the figure shows, a dis -\ncriminative model takes data as inputs and produces probabilities of different labels, \nwhich we denote by Prob(dog) and Prob(cat). We can then label the inputs based on \nthe highest predicted probabilities.  \nDiscriminative\nmodel \nTask\ndescription \nProb(Cat)\nProb(Dog)\nGenerative\nmodel\nFigure 1.1    A comparison of generative models versus discriminative models. A discriminative model \n(top half of the figure) takes data as inputs and produces probabilities of different labels, which we \ndenote by Prob(dog) and Prob(cat). In contrast, a generative model (bottom half) acquires an in-depth \nunderstanding of the defining characteristics of these images to synthesize new images representing \ndogs and cats.\n6 chapter  1 What is generative AI and why PyTorch?\nIn contrast, generative models exhibit a unique ability to generate novel instances \nof data. In the context of our dog and cat example, a generative model acquires an \nin-depth understanding of the defining characteristics of these images to synthesize \nnew images representing dogs and cats. As the bottom half of figure 1.1 shows, a gener -\native model takes task descriptions (such as varying values in a latent space that result \nin different characteristics in the generated image, which we will discuss in detail in \nchapters 4 to 6) as inputs and produces entirely new images of dogs and cats.   \nFrom a statistical perspective, when presented with data examples with features X, \nwhich describe the input and various corresponding labels Y, discriminative models \nundertake the responsibility of predicting conditional probabilities, specifically the \nprobability prob(Y|X). Conversely, generative models attempt to learn the joint prob -\nability distribution of the input features X and the target variable Y, denoted as prob \n(X, Y). Armed with this knowledge, they sample from the distribution to conjure fresh \ninstances of X.\nThere are different types of generative models depending on the specific forms of \ncontent you want to create. In this book, we focus primarily on two prominent technol -\nogies: Generative Adversarial Networks (GANs) and Transformers (although we’ll also \ncover variational autoencoders and diffusion models). The word “adversarial” in GANs \nrefers to the fact that the two neural networks compete against each other in a zero-sum \ngame framework: the generative network tries to create data instances indistinguish -\nable from real samples, while the discriminative network tries to identify the gener -\nated samples from real ones. The competition between the two networks leads to the \nimprovement of both, eventually enabling the generator to create highly realistic data. \nTransformers are deep neural networks that can efficiently solve sequence-to-sequence \nprediction tasks, and we’ll explain them in more detail later in this chapter.\nGANs, celebrated for their ease of implementation and versatility, empower indi -\nviduals with even rudimentary knowledge of deep learning to construct their genera -\ntive models from the ground up. These versatile models can give rise to a plethora of \ncreations, from geometric shapes and intricate patterns, as exemplified in chapter 3 \nof this book, to high-quality color images like human faces, which you’ll learn to gen -\nerate in chapter 4. Furthermore, GANs exhibit the ability to transform image content, \nseamlessly morphing a human face image with blond hair into one with black hair, \na phenomenon discussed in chapter 6. Notably, they extend their creative prowess to \nthe field of music generation, producing realistic-sounding musical compositions, as \ndemonstrated in chapter 13.\nIn contrast to shape, number, or image generation, the art of text generation poses \nformidable challenges, chiefly due to the sequential nature of textual information, \nwhere the order and arrangement of individual characters and words hold significant \nmeaning. To confront this complexity, we turn to Transformers, deep neural networks \ndesigned to proficiently address sequence-to-sequence prediction tasks. Unlike their \npredecessors, such as recurrent neural networks (RNNs) or convolutional neural net -\nworks (CNNs), Transformers excel in capturing intricate, long-range dependencies",5695
12-1.2 GANs.pdf,12-1.2 GANs,"7 Introducing generative AI and PyTorch\ninherent in both input and output sequences. Notably, their capacity for parallel train -\ning (a distributed training method in which a model is trained on multiple devices \nsimultaneously) has substantially reduced training times, making it possible for us to \ntrain Transformers on vast amounts of data.\nThe revolutionary architecture of Transformers underpins the emergence of large \nlanguage models (LLMs; deep neural networks with a massive number of parameters \nand trained on large datasets), including ChatGPT, BERT, DALL-E, and T5. This trans -\nformative architecture serves as the bedrock of the recent surge in AI advancement, \nushered in by the introduction of ChatGPT and other generative pretrained Trans -\nformer (GPT) models.\nIn the subsequent sections, we dive into the comprehensive inner workings of these \ntwo pioneering technologies: their underlying mechanisms and the myriad possibilities \nthey unlock. \n1.1.2 The Python programming language\nI assume you have a working knowledge of Python. To follow the content in the book, \nyou need to know the Python basics such as functions, classes, lists, dictionaries, and \nso on. If not, there are plenty of free resources online to get you started. Follow the \ninstructions in appendix A to install Python. After that, create a virtual environment \nfor this book and install Jupyter Notebook as the computing environment for projects \nin this book. \nPython has established itself as the leading programming language globally since the \nlatter part of 2018, as documented by The Economist (see the article “Python Is Becoming \nthe World’s Most Popular Coding Language” by the Data Team at The Economist,  2018, \nhttps: //mng.bz/2gj0 ). Python is not only free for everyone to use but also allows other \nusers to create and tweak libraries. Python has a massive community-driven ecosystem, \nso you can easily find resources and assistance from fellow Python enthusiasts. Plus, \nPython programmers love to share their code, so instead of reinventing the wheel, you \ncan import premade libraries and share your own with the Python community.\nNo matter if you’re on Windows, Mac, or Linux, Python’s got you covered. It’s a \ncross-platform language, although the process of installing software and libraries might \nvary a bit depending on your operating system—but don’t worry; I’ll show you how to \ndo it in appendix A. Once everything’s set up, Python code behaves the same across \ndifferent systems. \nPython is an expressive language that’s suitable for general application develop -\nment. Its syntax is easy to grasp, making it straightforward for AI enthusiasts to under -\nstand and work with. If you run into any problems with the Python libraries mentioned \nin this book, you can search Python forums or visit sites like Stack Overflow ( https: //\nstackoverflow.com/questions/tagged/python ) for answers. And if all else fails, don’t \nhesitate to reach out to me for assistance.\nLastly, Python offers a large collection of libraries that make creating generative mod -\nels easy (relative to other languages such as C++ or R). In this journey, we’ll exclusively \n8 chapter  1 What is generative AI and why PyTorch?\nuse PyTorch as our AI framework, and I’ll explain why we pick it over competitors like \nTensorFlow shortly.\n1.1.3 Using PyTorch as our AI framework\nNow that we have settled on using Python as the programming language for this book, \nwe’ll choose a suitable AI framework for generative modeling. The two most popular \nAI frameworks in Python are PyTorch and TensorFlow. In this book, we use PyTorch \nover TensorFlow for its ease of use, and I strongly encourage you to do the same. \nPyTorch is an open-source ML library developed by Meta’s AI Research lab. Built on \nthe Python programming language and the Torch library, PyTorch aims to offer a flex -\nible and intuitive platform for creating and training deep learning models. Torch, the \npredecessor of PyTorch, was an ML library for building deep neural networks in C with \na Lua wrapper, but its development was discontinued. PyTorch was designed to meet \nthe needs of researchers and developers by providing a more user-friendly and adapt -\nable framework for deep learning projects.\nA computational graph is a fundamental concept in deep learning that plays a cru -\ncial role in the efficient computation of complex mathematical operations, especially \nthose involving multidimensional arrays or tensors. A computational graph is a directed \ngraph where the nodes represent mathematical operations, and the edges represent \ndata that flow between these operations. One of the key uses of computational graphs \nis the calculation of partial derivatives when implementing backpropagation and gra -\ndient descent algorithms. The graph structure allows for the efficient calculation of \ngradients required to update the model parameters during training. PyTorch creates \nand modifies the graph on the fly, which is called a dynamic computational graph. This \nmakes it more adaptable to varying model architectures and simplifies debugging. Fur -\nther, just like TensorFlow, PyTorch provides accelerated computation through GPU \ntraining, which can significantly reduce training time compared to central processing \nunit (CPU) training. \nPyTorch’s design aligns well with the Python programming language. Its syntax is \nconcise and easy to understand, making it accessible to both newcomers and experi -\nenced developers. Researchers and developers alike appreciate PyTorch for its flexi -\nbility. It empowers them to experiment with novel ideas quickly, thanks to its dynamic \ncomputational graph and simple interface. This flexibility is crucial in the rapidly evolv -\ning fields of generative AI. PyTorch also has a rapidly growing community that actively \ncontributes to its development. This results in an extensive ecosystem of libraries, tools, \nand resources for developers.\nPyTorch excels in transfer learning, a technique where pretrained models designed \nfor a general task are fine-tuned for specific tasks. Researchers and practitioners can \neasily utilize pretrained models, saving time and computational resources. This feature \nis especially important in the age of pretrained LLMs and allows us to adopt LLMs for \ndownstream tasks such as classification, text summarization, and text generation. \nPyTorch is compatible with other Python libraries, such as NumPy and Matplot -\nlib. This interoperability allows data scientists and engineers to seamlessly integrate",6637
13-1.2.1 A high-level overview of GANs.pdf,13-1.2.1 A high-level overview of GANs,"9 GANs\nPyTorch into their existing workflows, enhancing productivity. PyTorch is also known \nfor its commitment to community-driven development. It evolves rapidly, with regular \nupdates and enhancements based on real-world usage and user feedback, ensuring that \nit remains at the cutting edge of AI research and development.\nAppendix A provides detailed instructions on how to install PyTorch on your com -\nputer. Follow the instructions to install PyTorch in the virtual environment for this book. \nIn case you don’t have a Compute Unified Device Architecture (CUDA)-enabled GPU \ninstalled on your computer, all programs in this book are compatible with CPU train -\ning as well. Better yet, I’ll provide the trained models on the book’s GitHub repository \nhttps: //github.com/markhliu/DGAI  so you can see the trained models in action (in \ncase the trained model is too large, I’ll provide them on my personal website https: //\ngattonweb.uky.edu/faculty/lium/ ). In chapter 2, you’ll dive deep into PyTorch. You’ll \nfirst learn the data structure in PyTorch, Tensor, which holds numbers and matrices \nand provides functions to conduct operations. You’ll then learn to perform an end-to-\nend deep learning project using PyTorch. Specifically, you’ll create a neural network \nin PyTorch and use clothing item images and the corresponding labels to train the \nnetwork. Once done, you use the trained model to classify clothing items into 10 differ -\nent label types. The project will get you ready to use PyTorch to build and train various \ngenerative models in later chapters. \n1.2 GANs\nThis section first provides a high-level overview of how GANs work. We then use the \ngeneration of anime face images as an example to show you the inner workings of \nGANs. Finally, we’ll discuss the practical uses of GANs. \n1.2.1 A high-level overview of GANs\nGANs represent a category of generative models initially proposed by Ian Goodfel -\nlow and his collaborators in 2014 (“Generative Adversarial Nets,” https: //arxiv.org/\nabs/1406.2661 ). GANs have become extremely popular in recent years because they \nare easy to build and train, and they can generate a wide variety of content. As you’ll \nsee from the illustrating example in the next subsection, GANs employ a dual-network \narchitecture comprising a generative model tasked with capturing the underlying data \ndistribution to generate content and a discriminative model that serves to estimate \nthe likelihood that a given sample originates from the authentic training dataset (con -\nsidered as “real”) rather than being a product of the generative model (considered \nas “fake”). The primary objective of the model is to produce new data instances that \nclosely resemble those in the training dataset. The nature of the data generated by \nGANs is contingent upon the composition of the training dataset. For example, if the \ntraining data consists of grayscale images of clothing items, the synthesized images \nwill closely resemble such clothing items. Conversely, if the training dataset comprises \ncolor images of human faces, the generated images will also resemble human faces.\n10 chapter  1 What is generative AI and why PyTorch?\nTake a look at figure 1.2—the architecture of our GAN and its components. To train \nthe model, both real samples from the training dataset (as shown at the top of figure \n1.2) and fake samples created by the generator (left) are presented to the discriminator \n(middle). The principal aim of the generator is to create data instances that are virtu -\nally indistinguishable from the examples found within the training dataset. Conversely, \nthe discriminator strives to distinguish fake samples generated by the generator from \nreal samples. These two networks engage in a continual competitive process similar to a \ncat-and-mouse game, trying to outperform each other iteratively. \nDiscriminatorLabel (ground\ntruth)Step 5: Feedback\nFake image\nReal image\nStep 5: FeedbackGeneratorStep 1\nStep 2 Step 3Step 3\nStep 4Task\ndescription\nPrediction\n(real or\nfake?)\nFigure 1.2    GANs architecture and its components. GANs employ a dual-network architecture comprising \na generative model (left) tasked with capturing the underlying data distribution and a discriminative model \n(center) that serves to estimate the likelihood that a given sample originates from the authentic training dataset \n(considered as “real”) rather than being a product of the generative model (considered as “fake”). \nThe training process of the GAN model involves multiple iterations. In each iteration, \nthe generator takes some form of task description (step 1) and uses it to create fake \nimages (step 2). The fake images, along with real images from the training set, are \npresented to the discriminator (step 3). The discriminator tries to classify each sam -\nple as either real or fake. It then compares the classification with the actual labels, \nthe ground truth (step 4). Both the discriminator and the generator receive feedback \n(step 5) from the classification and improve their capabilities: while the discriminator \nadapts its ability to identify fake samples, the generator learns to enhance its capac -\nity to generate convincing samples to fool the discriminator. As training advances, an \nequilibrium is reached when neither network can further improve. At this point, the \ngenerator becomes capable of producing data instances that are practically indistin -\nguishable from real samples. \nTo understand exactly how GANs work, let’s look at an illustrating example.",5613
14-1.2.3 Why should you care about GANs.pdf,14-1.2.3 Why should you care about GANs,"11 GANs\n1.2.2 An illustrating example: Generating anime faces\nPicture this: you’re a passionate anime enthusiast, and you’re on a thrilling quest to \ncreate your very own anime faces using a powerful tool known as a deep convolutional \nGAN (or DCGAN for short; don’t worry, we’ll dive deeper into this in chapter 4). \nIf you look at the top middle of figure 1.2, you’ll spot a picture that reads “Real \nImage.” We’ll use 63,632 colorful images of anime faces as our training dataset. And if \nyou flip to figure 1.3, you’ll see 32 examples from our training set. These special images \nplay a crucial role as they form half of the inputs to our discriminator network.\nFigure 1.3    Examples from the anime faces training dataset\nThe left of figure 1.2 is the generator network. To generate different images every time, \nthe generator takes as input a vector Z from the latent space. We could think of this \nvector as a “task description.” During training, we draw different Z vectors from the \nlatent space, so the network generates different images every time. These fake images \nare the other half of the inputs to the discriminator network. \nNOTE     By altering the values in the vector Z, we generate different outputs. In \nchapter 5, you’ll learn how to select the vector Z to generate images with certain \ncharacteristics (e.g., male or female features).\nBut here’s the twist: before we teach our two networks the art of creation and detec -\ntion, the images produced by the generator are, well, gibberish! They look nothing \nlike the realistic anime faces you see in figure 1.3. In fact, they resemble nothing more \nthan static on a TV screen (you’ll witness this firsthand in chapter 4).\nWe train the model for multiple iterations. In each iteration, we present a group of \nimages created by the generator, along with a group of anime face images from our \ntraining set to the discriminator. We ask the discriminator to predict whether each \nimage is created by the generator (fake) or from the training set (real).\n12 chapter  1 What is generative AI and why PyTorch?\nYou may wonder: How do the discriminator and the generator learn during each \niteration of training? Once the predictions are made, the discriminator doesn’t just sit \nback; it learns from its prediction blunders for each image. With this newfound knowl -\nedge, it fine-tunes its parameters, shaping itself to make better predictions in the next \nround. The generator isn’t idle either. It takes notes from its image generation process \nand the discriminator’s prediction outcomes. With that knowledge in hand, it adjusts its \nown network parameters, striving to create increasingly lifelike images in the next itera -\ntion. The goal? To reduce the odds of the discriminator sniffing out its fakes.\nAs we journey through these iterations, a remarkable transformation takes place. \nThe generator network evolves, producing anime faces that grow more and more real -\nistic, akin to those in our training collection. Meanwhile, the discriminator network \nhones its skills, becoming a seasoned detective when it comes to spotting fakes. It’s a \ncaptivating dance between creation and detection.\nGradually, a magical moment arrives. An equilibrium, or perfect balance, is achieved. \nThe images created by the generator become so astonishingly real that they are indis -\ntinguishable from the genuine anime faces in our training archives. At this point, the \ndiscriminator is so confused that it assigns a 50% chance of authenticity to every image, \nwhether it’s from our training set or was crafted by the generator. \nFinally, behold some examples of the artwork of the generator, as shown in figure \n1.4: they do look indistinguishable from those in our training set. \nFigure 1.4    Generated anime face images by the trained generator in DCGAN\n1.2.3 Why should you care about GANs?\nGANs are easy to implement and versatile: you’ll learn to generate geometric shapes, \nintricate patterns, high-resolution images, and realistic-sounding music in this book \nalone. \n 13 GANs\nThe practical use of GANs doesn’t stop at generating realistic data. GANs can also \ntranslate attributes in one image domain to another. As you’ll see in chapter 6, you can \ntrain a CycleGAN (a type of generative model in the GAN family) to convert blond hair \nto black hair in human face images. The same trained model can also convert black hair \nto blond hair. Figure 1.5 shows four rows of images. The first row is the original images \nwith blond hair. The trained CycleGAN converts them to images with black hair (sec -\nond row). The last two rows are the original images with black hair and the converted \nimage with blond hair, respectively. \nFigure 1.5    Changing hair color with CycleGAN. If we feed images with blond hair (first row) to a trained \nCycleGAN model, the model converts blond hair to black hair in these images (second row). The same \ntrained model can also convert black hair (third row) to blond hair (bottom row). \nThink about all the amazing skills you’ll pick up from training GANs—they’re not \njust cool; they’re super practical too! Let’s say you run an online clothing store with \na “Make to Order” strategy (which allows users to customize their purchases before \nmanufacturing). Your website showcases tons of unique designs for customers to pick \nfrom, but here’s the catch: you only make the clothes once someone places an order. \nCreating high-quality images of these clothes can be quite expensive since you have to \nproduce the items and then photograph them.\nGANs to the rescue! You don’t need a massive collection of manufactured clothing \nitems and their images; instead, you can use something like CycleGAN to transform",5779
15-1.3 Transformers.pdf,15-1.3 Transformers,,0
16-1.3.3 Multimodal Transformers and pretrained LLMs.pdf,16-1.3.3 Multimodal Transformers and pretrained LLMs,"14 chapter  1 What is generative AI and why PyTorch?\nfeatures from one set of images into another, creating a whole new array of styles. This \nis just one nifty way to use GANs. The possibilities are endless because these models are \nsuper versatile and can handle all sorts of data—making them a game-changer for prac -\ntical applications.\n1.3 Transformers\nTransformers are deep neural networks that excel at sequence-to-sequence prediction \nproblems, such as taking an input sentence and predicting the most likely next words. \nThis section introduces you to the key innovation in Transformers: the self-attention \nmechanism. We’ll then discuss the Transformer architecture and different types of \nTransformers. Finally, we’ll discuss some recent developments in Transformers, such \nas multimodal models (Transformers whose inputs include not only text but also other \ndata types such as audio and images) and pretrained LLMs (models trained on large \ntextual data that can perform various downstream tasks). \nBefore the Transformer architecture was invented in 2017 by a group of Goo -\ngle researchers (Vaswani et al., “Attention Is All You Need,” https: //arxiv.org/\nabs/1706.03762 ), natural language processing (NLP) and other sequence-to-sequence \nprediction tasks were primarily handled by RNNs. However, RNNs struggle with retain -\ning information about earlier elements in a sequence, which hampers their ability to \ncapture long-term dependencies. Even advanced RNN variants like long short-term \nmemory (LSTM) networks, which can handle longer-range dependencies, fall short \nwhen it comes to extremely long-range dependencies.\nMore importantly, RNNs (including LSTMs) process inputs sequentially, which \nmeans these models process one element at a time, in sequence, instead of looking at \nthe entire sequence simultaneously. The fact that RNNs conduct computation along \nthe symbol positions of the input and output sequences prevents parallel training, \nwhich makes training slow. This, in turn, makes it impossible to train the models on \nhuge datasets. \nThe key innovation of Transformers is the self-attention mechanism, which excels at \ncapturing long-term dependencies in a sequence. Further, since the inputs are not han -\ndled sequentially in the model, Transformers can be trained in parallel, which greatly \nreduces the training time. More importantly, parallel training makes it possible to train \nTransformers on large amounts of data, which makes LLMs intelligent and knowledge -\nable (based on their ability to process and generate human-like text, understand con -\ntext, and perform a variety of language tasks). This has led to the rise of LLMs such as \nChatGPT and the recent AI boom.\n1.3.1 The attention mechanism\nThe attention mechanism assigns weights on how an element is related to all elements \nin a sequence (including the element itself). The higher the weight, the more closely \nthe two elements are related. These weights are learned from large sets of training data \nin the training process. Therefore, a trained LLM such as ChatGPT can figure out the \n 15 Transformers\nrelationship between any two words in a sentence, hence making sense of the human \nlanguage. \nYou may wonder: How does the attention mechanism assign scores to elements in a \nsequence to capture the long-term dependencies? The attention weights are calculated \nby first passing the inputs through three neural network layers to obtain query Q, key \nK, and value V (which we’ll explain in detail in chapter 9). The method of using query, \nkey, and value to calculate attention comes from retrieval systems. For example, you \nmay go to a public library to search for a book. You can type in, say, “machine learning \nin finance” in the library’s search engine. In this case, the query Q is “machine learning \nin finance.” The keys K are the book titles, book descriptions, and so on. The library’s \nretrieval system will recommend a list of books (values V) based on the similarities \nbetween the query and the keys. Naturally, books with the phrases “machine learning” \nor “finance” or both in titles or descriptions come up on top while books with neither \nphrase in the title or description will show up at the bottom of the list because these \nbooks will be assigned a low matching score.\nIn chapters 9 and 10, you’ll learn the details of the attention mechanism—better \nyet, you’ll implement the attention mechanism from scratch to build and train a Trans -\nformer to successfully translate English to French. \n1.3.2 The Transformer architecture \nTransformers were first proposed when designing models for machine language \ntranslation (e.g., English to German or English to French). Figure 1.6 is a diagram \nof the Transformer architecture. The left side is the encoder, and the right side is the \ndecoder. In chapters 9 and 10, you’ll learn to construct a Transformer from scratch to \ntrain the model to translate English to French, and we’ll explain figure 1.6 in greater \ndetail then. \nThe encoder in the Transformer “learns” the meaning of the input sequence (e.g., \nthe English phrase “How are you?”) and converts it into vectors that represent this \nmeaning before passing the vectors to the decoder. The decoder constructs the output \n(e.g., the French translation of an English phrase) by predicting one word at a time, \nbased on previous words in the sequence and the output from the encoder. The trained \nmodel can translate common English phrases into French. \nThere are three types of Transformers: encoder-only Transformers, decoder-only \nTransformers, and encoder-decoder Transformers. An encoder-only Transformer has \nno decoder and is capable of converting a sequence into an abstract representation for \nvarious downstream tasks such as sentiment analysis, named entity recognition, and \ntext generation. For example, BERT is an encoder-only Transformer. A decoder-only \nTransformer has only a decoder but no encoder, and it’s well suited for text genera -\ntion, language modeling, and creative writing. GPT-2 (the predecessor of ChatGPT) \nand ChatGPT are both decoder-only Transformers. In chapter 11, you’ll learn to create \nGPT-2 from scratch and then extract the trained model weights from Hugging Face (an \nAI community that hosts and collaborates on ML models, datasets, and applications). \nYou’ll load the weights to your GPT-2 model and start generating coherent text.\n16 chapter  1 What is generative AI and why PyTorch?\nInput embeddingEncoder\nInputs\n(e.g., How are you?) Positional\nencoding Encoder blockN blocksAbstract\nrepresentation\nOutput embeddingDecode r\nOutputs\n(e.g., Comment êtes-vous?)Positional\nencoding Decoder blockN blocksLinear layerOutput\nprobabilities\nSoftmax\nactivation\nFigure 1.6    The Transformer architecture. The encoder in the Transformer (left side of the diagram) \nlearns the meaning of the input sequence (e.g., the English phrase “How are you?”) and converts it \ninto an abstract representation that captures its meaning before passing it to the decoder (right side of \nthe diagram). The decoder constructs the output (e.g., the French translation of the English phrase) by \npredicting one word at a time, based on previous words in the sequence and the abstract representation \nfrom the encoder.\nEncoder-decoder Transformers are needed for complicated tasks such as multimodal \nmodels that can handle text-to-image generation or speech recognition. Encoder-\ndecoder Transformers combine the strengths of both encoders and decoders. Encoders \nare efficient in processing and understanding input data, while decoders excel in \ngenerating output. This combination allows the model to effectively understand \ncomplex inputs (like text or speech) and generate intricate outputs (like images or \ntranscribed text).\n1.3.3 Multimodal Transformers and pretrained LLMs\nRecent developments in generative AI give rise to various multimodal models: Trans -\nformers that can use not only text but also other data types, such as audio and images, \nas inputs. Text-to-image Transformers are one such example. DALL-E 2, Imagen, and \nStable Diffusion are all text-to-image models, and they have garnered much media \n 17 Transformers\nattention due to their ability to generate high-resolution images from textual prompts. \nText-to-image Transformers incorporate the principles of diffusion models, which \ninvolve a series of transformations to gradually increase the complexity of data. There -\nfore, we first need to understand diffusion models before we discuss text-to-image \nTransformers. \nImagine you want to generate high-resolution flower images by using a diffusion-\nbased model. You’ll first obtain a training set of high-quality flower images. You then ask \nthe model to gradually add noise to the flower images (the so-called diffusion process) \nuntil they become completely random noise. You then train the model to progressively \nremove noise from these noisy images to generate new data samples. The diffusion \nprocess is illustrated in figure 1.7. The left column contains four original flower images. \nAs we move to the right, some noise is added to the images in each step, until at the \nright column, the four images are pure random noise.  \nFigure 1.7    The diffusion model adds more and more noise to the images and learns to reconstruct them. \nThe left column contains four original flower images. As we move to the right, some noise is added to \nthe images in each time step, until at the right column, the four images are pure random noise. We then \nuse these images to train a diffusion-based model to progressively remove noise from noisy images to \ngenerate new data samples.\nYou may be wondering: How are text-to-image Transformers related to diffusion mod -\nels? Text-to-image Transformers take a text prompt as input and generate images that \ncorrespond to that textual description. The text prompt serves as a form of condition -\ning, and the model uses a series of neural network layers to transform that textual \ndescription into an image. Like diffusion models, text-to-image Transformers use a \nhierarchical architecture with multiple layers, each progressively adding more detail \nto the generated image. The core concept of iteratively refining the output is similar in \nboth diffusion models and text-to-image Transformers, as we’ll explain in chapter 15.",10486
17-1.4 Why build generative models from scratch.pdf,17-1.4 Why build generative models from scratch,"18 chapter  1 What is generative AI and why PyTorch?\nDiffusion models have now become more popular due to their ability to provide \nstable training and generate high-quality images, and they have outperformed other \ngenerative models such as GANs and variational autoencoders. In chapter 15, you’ll \nfirst learn to train a simple diffusion model using the Oxford Flower dataset. You’ll also \nlearn the basic idea behind multimodal Transformers and write a Python program to \nask OpenAI’s DALL-E 2 to generate images through a text prompt. For example, when \nI entered “an astronaut in a space suit riding a unicorn” as the prompt, DALL-E 2 gener -\nated the image shown in figure 1.8. \nFigure 1.8    Image generated by DALL-E 2 with text prompt “an astronaut in a space suit riding a \nunicorn”\nIn chapter 16, you’ll learn how to access pretrained LLMs such as ChatGPT, GPT4, and \nDALL-E 2. These models are trained on large textual data and have learned general \nknowledge from the data. Hence, they can perform various downstream tasks such \nas text generation, sentiment analysis, question answering, and named entity recogni -\ntion. Since pretrained LLMs were trained on information a few months ago, they can -\nnot provide information on events and developments in the last one or two months, let \nalone real-time information such as weather conditions, flight status, or stock prices. \nWe’ll use LangChain (a Python library designed for building applications with LLMs, \nproviding tools for prompt management, LLM chaining, and output parsing) to chain \ntogether LLMs with the Wolfram Alpha and Wikipedia APIs to create a know-it-all per -\nsonal assistant. \n1.4 Why build generative models from scratch? \nThe goal of this book is to show you how to build and train all generative models from \nscratch. This way, you’ll have a thorough understanding of the inner workings of these \nmodels and can make better use of them. Creating something from scratch is the \n 19 Why build generative models from scratch? \nbest way to understand it. You’ll accomplish this goal for GANs: all models, including \nDCGAN and CycleGAN, are built from the ground up and trained using well-curated \ndata in the public domain. \nFor Transformers, you’ll build and train all models from scratch except for LLMs. \nThis exception is due to the vast amount of data and the supercomputing facilities \nneeded to train certain LLMs. However, you’ll make serious progress in this direction. \nSpecifically, you’ll implement in chapters 9 and 10 the original groundbreaking 2017 \npaper “Attention Is All You Need” line by line with English-to-French translation as an \nexample (the same Transformer can be trained on other datasets such as Chinese to \nEnglish or English to German translations). You’ll also build a small-size decoder-only \nTransformer and train it using several of Ernest Hemingway’s novels, including The Old \nMan and the Sea . The trained model can generate text in Hemingway style. ChatGPT and \nGPT-4 are too large and complicated to build and train from scratch for our purposes, \nbut you’ll peek into their predecessor, GPT-2, and learn to build it from scratch. You’ll \nalso extract the trained weights from Hugging Face and load them up to the GPT-2 \nmodel you built and start to generate realistic text that can pass as human-written. \nIn this sense, the book is taking a more fundamental approach than most books. \nInstead of treating generative AI models as a black box, readers have a chance to look \nunder the hood and examine in detail the inner workings of these models. The goal \nis for you to have a deeper understanding of generative models. This, in turn, can \npotentially help you build better and more responsible generative AI for the following \nreasons. \nFirst, having a deep understanding of the architecture of generative models helps \nreaders make better practical uses of these models. For example, in chapter 5, you’ll \nlearn how to select characteristics in generated images such as male or female features \nand with or without eyeglasses. By building a conditional GAN from the ground up, \nyou understand that certain features of the generated images are determined by the \nrandom noise vector, Z, in the latent space. Therefore, you can choose different values \nof Z as inputs to the trained model to generate the desired characteristics (such as male \nor female features). This type of attribute selection is hard to do without understanding \nthe design of the model.  \nFor Transformers, knowing the architecture (and what encoders and decoders do) \ngives you the ability to create and train Transformers to generate the types of content \nyou are interested in (say, Jane Austin–style novels or Mozart-style music). This under -\nstanding also helps you with pretrained LLMs. For example, while it is hard to train \nGPT-2 from scratch with its 1.5 billion parameters, you can add an additional layer to \nthe model and fine-tune it for other downstream tasks such as text classification, senti -\nment analysis, and question-answering.\nSecond, a deep understanding of generative AI helps readers have an unbiased \nassessment of the dangers of AI. While the extraordinary powers of generative AI have \nbenefitted us in our daily lives and work, it also has the potential to create great harm. \nElon Musk went so far as saying that “there’s some chance that it goes wrong and destroys \n20 chapter  1 What is generative AI and why PyTorch?\nhumanity” (see the article by Julia Mueller in The Hill , 2023, “Musk: There’s a Chance \nAI ‘Goes Wrong and Destroys Humanity,’” https: //mng.bz/Aaxz ). More and more peo -\nple in academics and in the tech industry are worried about the dangers posed by AI \nin general and generative AI in particular. Generative AI, especially LLMs, can lead \nto unintended consequences, as many pioneers in the tech profession have warned \n(see, e.g., Stuart Russell, 2023, “How to Stop Runaway AI,” https: //mng.bz/ZVzP ). It’s \nnot a coincidence that merely five months after the release of ChatGPT, many tech \nindustry experts and entrepreneurs, including Steve Wozniak, Tristan Harris, Yoshua \nBengio, and Sam Altman, signed an open letter calling for a pause in training any AI sys -\ntem that’s more powerful than GPT-4 for at least six months (see the article by Connie \nLoizos in TechCrunch , “1,100+ Notable Signatories Just Signed an Open Letter Asking \n‘All AI Labs to Immediately Pause for at Least 6 Months,’” https: //mng.bz/RNEK ). A \nthorough understanding of the architecture of generative models helps us provide a \ndeep and unbiased evaluation of the benefits and potential dangers of AI. \nSummary\n¡ Generative AI is a type of technology with the capacity to produce diverse forms \nof new content, including texts, images, code, music, audio, and video. \n¡ Discriminative models specialize in assigning labels while generative models gen -\nerate new instances of data. \n¡ PyTorch, with its dynamic computational graphs and the ability for GPU train -\ning, is well suited for deep learning and generative modeling. \n¡ GANs are a type of generative modeling method consisting of two neural net -\nworks: a generator and a discriminator. The goal of the generator is to create \nrealistic data samples to maximize the chance that the discriminator thinks they \nare real. The goal of the discriminator is to correctly identify fake samples from \nreal ones. \n¡ Transformers are deep neural networks that use the attention mechanism to \nidentify long-term dependencies among elements in a sequence. The original \nTransformer has an encoder and a decoder. When it’s used for English-to-French \ntranslation, for example, the encoder converts the English sentence into an \nabstract representation before passing it to the decoder. The decoder generates \nthe French translation one word at a time, based on the encoder’s output and the \npreviously generated words.",8010
18-2.1.1 Creating PyTorch tensors.pdf,18-2.1.1 Creating PyTorch tensors,"212Deep learning with \nPyTorch\nThis chapter covers\n¡ PyTorch tensors and basic operations\n¡ Preparing data for deep learning in PyTorch\n¡ Building and training deep neural networks with  \n PyTorch \n¡ Conducting binary and multicategory  \n classifications with deep learning\n¡ Creating a validation set to decide training stop  \n points\nIn this book, we’ll use deep neural networks to generate a wide range of content, \nincluding text, images, shapes, music, and more. I assume you already have a foun -\ndational understanding of machine learning (ML) and, in particular, artificial neu -\nral networks. In this chapter, I’ll refresh your memory on essential concepts such as \nloss functions, activation functions, optimizers, and learning rates, which are cru -\ncial for developing and training deep neural networks. If you find any gaps in your \nunderstanding of these topics, I strongly encourage you to address them before pro -\nceeding with the projects in this book. Appendix B provides a summary of the basic \nskills and concepts needed, including the architecture and training of artificial neu -\nral networks. \n22 chapter  2 Deep learning with PyTorch\nNOTE    There are plenty of great ML books out there for you to choose from. \nExamples include Hands-on Machine Learning with Scikit-Learn, Keras, and Tensor -\nFlow (2019, O’Reilly) and Machine Learning, Animated  (2023, CRC Press). Both \nbooks use TensorFlow to create neural networks. If you prefer a book that uses \nPyTorch, I recommend Deep Learning with PyTorch  (2020, Manning Publications).\nGenerative AI models are frequently confronted with the task of either binary or mul -\nticategory classification. For instance, in generative adversarial networks (GANs), the \ndiscriminator undertakes the essential role of a binary classifier, its purpose being \nto distinguish between the fake samples created by the generator from real samples \nfrom the training set. Similarly, in the context of text generation models, whether in \nrecurrent neural networks or Transformers, the overarching objective is to predict the \nsubsequent character or word from an extensive array of possibilities (essentially a mul -\nticategory classification task).\nIn this chapter, you’ll learn how to use PyTorch to create deep neural networks to \nperform binary and multicategory classifications so that you become well-versed in \ndeep learning and classification tasks. \nSpecifically, you’ll engage in an end-to-end deep learning project in PyTorch, on \na quest to classify grayscale images of clothing items into different categories such as \ncoats, bags, sneakers, shirts, and so on. The intention is to prepare you for the creation \nof deep neural networks, capable of performing both binary and multicategory classi -\nfication tasks in PyTorch. This, in turn, will get you ready for the upcoming chapters, \nwhere you use deep neural networks in PyTorch to create various generative models.\nTo train generative AI models, we harness a diverse range of data formats such as raw \ntext, audio files, image pixels, and arrays of numbers. Deep neural networks created \nin PyTorch cannot take these forms of data directly as inputs. Instead, we must first \nconvert them into a format that the neural networks understand and accept. Specifi -\ncally, you’ll convert various forms of raw data into PyTorch tensors (fundamental data \nstructures used to represent and manipulate data) before feeding them to generative \nAI models. Therefore, in this chapter, you’ll also learn the basics of data types, how to \ncreate various forms of PyTorch tensors, and how to use them in deep learning.\nKnowing how to perform classification tasks has many practical applications in our \nsociety. Classifications are widely used in healthcare for diagnostic purposes, such as \nidentifying whether a patient has a particular disease (e.g., positive or negative for a \nspecific cancer based on medical imaging or test results). They play a vital role in many \nbusiness tasks (stock recommendations, credit card fraud detection, and so on). Clas -\nsification tasks are also integral to many systems and services that we use daily such as \nspam detection and facial recognition. \n2.1 Data types in PyTorch\nWe’ll use datasets from a wide range of sources and formats in this book, and the first \nstep in deep learning is to transform the inputs into arrays of numbers. \n 23 Data types in PyTorch\nIn this section, you’ll learn how PyTorch converts different formats of data into alge -\nbraic structures known as tensors . Tensors can be represented as multidimensional arrays \nof numbers, similar to NumPy arrays but with several key differences, chief among them \nthe ability of GPU accelerated training. There are different types of tensors depending \non their end use, and you’ll learn how to create different types of tensors and when to \nuse each type. We’ll discuss the data structure in PyTorch in this section by using the \nheights of the 46 U.S. presidents as our running example.\nRefer to the instructions in appendix A to create a virtual environment and install \nPyTorch and Jupyter Notebook on your computer. Open the Jupyter Notebook app \nwithin the virtual environment and run the following line of code in a new cell:\n!pip install matplotlib\nThis command will install the Matplotlib library on your computer, enabling you to \nplot images in Python. \n2.1.1 Creating PyTorch tensors\nWhen training deep neural networks, we feed the models with arrays of numbers as \ninputs. Depending on what a generative model is trying to create, these numbers have \ndifferent types. For example, when generating images, the inputs are raw pixels in the \nform of integers between 0 and 255, but we’ll convert them to floating-point numbers \nbetween –1 and 1; when generating text, there is a “vocabulary” akin to a dictionary, \nand the input is a sequence of integers telling you which entry in the dictionary the \nword corresponds to. \nNOTE     The code for this chapter, as well as other chapters in this book, is avail -\nable at the book’s GitHub repository: https: //github.com/markhliu/DGAI .\nImagine you want to use PyTorch to calculate the average height of the 46 U.S. presi -\ndents. We can first collect the heights of the 46 U.S. presidents in centimeters and store \nthem in a Python list:\nheights = [189, 170, 189, 163, 183, 171, 185,\n           168, 173, 183, 173, 173, 175, 178,\n           183, 193, 178, 173, 174, 183, 183,\n           180, 168, 180, 170, 178, 182, 180,\n           183, 178, 182, 188, 175, 179, 183,\n           193, 182, 183, 177, 185, 188, 188,\n           182, 185, 191, 183]\nThe numbers are in chronological order: the first value in the list, 189, indicates that \nthe first U.S. president, George Washington, was 189 centimeters tall. The last value \nshows that Joe Biden’s height is 183 centimeters. We can convert a Python list into a \nPyTorch tensor by using the tensor()  method in PyTorch:\nimport torch\nheights_tensor = torch.tensor(heights,    \n           dtype=torch.float64)    Converts a Python list \nto a PyTorch tensor\nSpecifies the data type \nin the PyTorch tensor\n24 chapter  2 Deep learning with PyTorch\nWe specify the data type using the dtype argument in the tensor()  method. The \ndefault data type in PyTorch tensors is float32 , a 32-bit floating-point number. In the \npreceding code cell, we converted the data type to float64 , double-precision floating-\npoint numbers. float64  provides more precise results than float32 , but it takes \nlonger to compute. There is a tradeoff between precision and computational costs. \nWhich data type to use depends on the task at hand. \nTable 2.1 lists different data types and the corresponding PyTorch tensor types. \nThese include integers and floating-point numbers with different precisions. Integers \ncan also be either signed or unsigned. \nTable 2.1    Data and tensor types in PyTorch\nPyTorch tensor type dtype argument in  tensor() Data type \nFloatTensor torch.float32 or torch.float 32-bit floating point\nHalfTensor torch.float16 or torch.half 16-bit floating point\nDoubleTensor torch.float64 or torch.double 64-bit floating point\nCharTensor torch.int8 8-bit integer (signed)\nByteTensor torch.uint8 8-bit integer (unsigned)\nShortTensor torch.int16 or torch.short 16-bit integer (signed)\nIntTensor torch.int32 or torch.int 32-bit integer (signed)\nLongTensor torch.int64 or torch.long 64-bit integer (signed)\nYou can create a tensor with a certain data type in one of the two ways. The first way is \nto use the PyTorch class as specified in the first column of table 2.1. The second way is \nto use the torch.tensor()  method and specify the data type using the dtype  argu -\nment (the value of the argument is listed in the second column of table 2.1). For exam -\nple, to convert the Python list [1, 2, 3]  into a PyTorch tensor with 32-bit integers in \nit, you can use two methods in the following listing.\nListing 2.1    Two ways of specifying tensor types\nt1=torch.IntTensor([1, 2, 3])    \nt2=torch.tensor([1, 2, 3],\n             dtype=torch.int)    \nprint(t1)\nprint(t2)\nThis leads to the following output:\ntensor([1, 2, 3], dtype=torch.int32)\ntensor([1, 2, 3], dtype=torch.int32)Uses torch.IntTensor() to \nspecify the tensor type\nUses dtype=torch.int to \nspecify the tensor type\n 25 Data types in PyTorch\nExercise 2.1\nUse two different methods to convert the Python list [5, 8, 10]  into a PyTorch tensor \nwith 64-bit floating-point numbers in it. Consult the third row in table 2.1 for this question. \nMany times, you need to create a PyTorch tensor with values 0 everywhere. For exam -\nple, in GANs, we create a tensor of zeros as the labels for fake samples, as you’ll see in \nchapter 3. The zeros()  method in PyTorch generates a tensor of zeros with a certain \nshape. In PyTorch, a tensor is an n-dimensional array, and its shape is a tuple represent -\ning the size along each of its dimensions. The following lines of code generate a tensor \nof zeros with two rows and three columns:\ntensor1 = torch.zeros(2, 3)\nprint(tensor1)\nThe output is as follows:\ntensor([[0., 0., 0.],\n        [0., 0., 0.]])\nThe tensor has a shape of (2, 3), which means the tensor is a 2D array; there are two \nelements in the first dimension and three elements in the second dimension. Here, we \ndidn’t specify the data type, and the output has the default data type of float32 . \nFrom time to time, you need to create a PyTorch tensor with values 1 everywhere. For \nexample, in GANs, we create a tensor of ones as the labels for real samples. Here we use \nthe ones()  method to create a 3D tensor with values 1 everywhere:\ntensor2 = torch.ones(1,4,5)\nprint(tensor2)\nThe output is \ntensor([[[1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1.]]])\nWe have generated a 3D PyTorch tensor. The shape of the tensor is (1, 4, 5).\nExercise 2.2\nCreate a 3D PyTorch tensor with values 0 in it. Make the shape of the tensor (2, 3, 4).\nYou can also use a NumPy array instead of a Python list in the tensor constructor:\nimport numpy as np\nnparr=np.array(range(10))\npt_tensor=torch.tensor(nparr, dtype=torch.int)\nprint(pt_tensor)",11351
19-2.2 An end-to-end deep learning project with PyTorch.pdf,19-2.2 An end-to-end deep learning project with PyTorch,"26 chapter  2 Deep learning with PyTorch\nThe output is\ntensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=torch.int32)\n2.1.2 Index and slice PyTorch tensors\nWe use square brackets ( [ ]) to index and slice PyTorch tensors, as we do with Python \nlists. Indexing and slicing allow us to operate on one or more elements in a tensor, \ninstead of on all elements. To continue our example of the heights of the 46 U.S. pres -\nidents, if we want to assess the height of the third president, Thomas Jefferson, we can \ndo the following:\nheight = heights_tensor[2]\nprint(height)\nThis leads to an output of\ntensor(189., dtype=torch.float64)\nThe output shows that the height of Thomas Jefferson was 189 centimeters. \nWe can use negative indexing to count from the back of the tensor. For example, to \nfind the height of Donald Trump, who is the second to last president in the list, we use \nindex –2:\nheight = heights_tensor[-2]\nprint(height)\nThe output is\ntensor(191., dtype=torch.float64)\nThe output shows that Trump’s height is 191 centimeters.\nWhat if we want to know the heights of five recent presidents in the tensor heights_\ntensor ? We can obtain a slice of the tensor:\nfive_heights = heights_tensor[-5:]\nprint(five_heights)\nThe colon ( :) is used to separate the starting and end index. If no starting index is pro -\nvided, the default is 0; if no end index is provided, you include the very last element in \nthe tensor (as we did in the preceding code cell). Negative indexing means you count \nfrom the back. The output is\ntensor([188., 182., 185., 191., 183.], dtype=torch.float64)\nThe results show that the five recent presidents in the tensor (Clinton, Bush, Obama, \nTrump, and Biden) are 188, 182, 185, 191, and 183 centimeters tall, respectively. \nExercise 2.3\nUse slicing to obtain the heights of the first five U.S. presidents in the tensor \nheights_tensor .\n 27 Data types in PyTorch\n2.1.3 PyTorch tensor shapes\nPyTorch tensors have an attribute shape , which tells us the dimensions of a tensor. It’s \nimportant to know the shapes of PyTorch tensors because mismatched shapes will lead \nto errors when we operate on them. For example, if we want to find out the shape of \nthe tensor heights_tensor , we can do this:\nprint(heights_tensor.shape)\nThe output is\ntorch.Size([46])\nThis tells us that heights_tensor  is a 1D tensor with 46 values in it.\nYou can also change the shape of a PyTorch tensor. To learn how, let’s first convert \nthe heights from centimeters to feet. Since a foot is about 30.48 centimeters, we can \naccomplish this by dividing the tensor by 30.48:\nheights_in_feet = heights_tensor / 30.48\nprint(heights_in_feet)\nThis leads to the following output (I omitted some values to save space; the complete \noutput is in the book’s GitHub repository):\ntensor([6.2008, 5.5774, 6.2008, 5.3478, 6.0039, 5.6102, 6.0696, …\n        6.0039], dtype=torch.float64)\nThe new tensor, heights_in_feet , stores the heights in feet. For example, the last \nvalue in the tensor shows that Joe Biden is 6.0039 feet tall. \nWe can use the cat()  method in PyTorch to concatenate the two tensors:\nheights_2_measures = torch.cat(\n    [heights_tensor,heights_in_feet], dim=0)\nprint(heights_2_measures.shape)\nThe dim argument is used in various tensor operations to specify the dimension along \nwhich the operation is to be performed. In the preceding code cell, dim=0  means \nwe concatenate the two tensors along the first dimension. This leads to the following \noutput:\ntorch.Size([92])\nThe resulting tensor is 1D with 92 values, with some values in centimeters and others \nin feet. Therefore, we need to reshape it into two rows and 46 columns so that the first \nrow represents heights in centimeters and the second in feet:\nheights_reshaped = heights_2_measures.reshape(2, 46)\nThe new tensor, heights_reshaped , is 2D with a shape of (2, 46). We can index and \nslice multidimensional tensors using square brackets as well. For example, to print out \nthe height of Trump in feet, we can do this:\nprint(heights_reshaped[1,-2])\n28 chapter  2 Deep learning with PyTorch\nThis leads to a result of\ntensor(6.2664, dtype=torch.float64)\nThe command heights_reshaped[1,-2]  tells Python to look for the value in the sec -\nond row and the second to last column, which returns Trump’s height in feet, 6.2664.\nTIP    The number of indexes needed to refer to scalar values within the tensor is \nthe same as the dimensionality of the tensor. That’s why we used only one index \nto locate values in the 1D tensor heights_tensor  but we used two indexes to \nlocate values in the 2D tensor heights_reshaped .  \nExercise 2.4\nUse indexing to obtain the height of Joe Biden in the tensor heights_reshaped  in \ncentimeters.\n2.1.4 Mathematical operations on PyTorch tensors\nWe can conduct mathematical operations on PyTorch tensors by using different meth -\nods such as mean() , median() , sum() , max() , and so on. For example, to find the \nmedian height of the 46 presidents in centimeters, we can do this:\nprint(torch.median(heights_reshaped[0,:]))\nThe code snippet heights_reshaped[0,:]  returns the first row and all columns in \nthe tensor heights_reshaped . The preceding line of code returns the median value \nin the first row, and this leads to an output of\ntensor(182., dtype=torch.float64)\nThis means the median height of U.S. presidents is 182 centimeters.\nTo find the average height in both rows, we can use the dim=1  argument in the \nmean()  method:\nprint(torch.mean(heights_reshaped,dim=1))\nThe dim=1  argument indicates that the averages are calculated by collapsing col -\numns (the dimension indexed 1), effectively obtaining averages along the dimension \nindexed 0 (rows). The output is\ntensor([180.0652,   5.9077], dtype=torch.float64)\nThe results show that the average values in the two rows are 180.0652 centimeters and \n5.9077 feet.\nTo find out the tallest president, we can do this:\nvalues, indices = torch.max(heights_reshaped, dim=1)\nprint(values)\nprint(indices)",6066
20-2.2.1 Deep learning in PyTorch A high-level overview.pdf,20-2.2.1 Deep learning in PyTorch A high-level overview,"29 An end-to-end deep learning project with PyTorch\nThe output is\ntensor([193.0000,   6.3320], dtype=torch.float64)\ntensor([15, 15])\nThe torch.max()  method returns two tensors: a tensor values  with the tallest pres -\nident’s height (in centimeters and in feet), and a tensor indices  with the indexes of \nthe president with the maximum height. The results show that the 16th president (Lin -\ncoln) is the tallest, at 193 centimeters, or 6.332 feet.\nExercise 2.5\nUse the torch.min()  method to find out the index and height of the shortest U.S. \npresident.\n2.2 An end-to-end deep learning project with PyTorch\nIn the next few sections, you’ll work through an example deep learning project with \nPyTorch, learning to classify grayscale images of clothing items into 1 of the 10 types. \nIn this section, we’ll first provide a high-level overview of the steps involved. We then \ndiscuss how to obtain training data for this project and how to preprocess the data.\n2.2.1 Deep learning in PyTorch: A high-level overview \nOur job in this project is to create and train a deep neural network in PyTorch to \nclassify grayscale images of clothing items. Figure 2.1 provides a diagram of the steps \ninvolved. \n...Images\nPyTorch\ntensorsPyTorch\ndeep\nneural\nnetworkPredictions\n...Prob(Coat)\nProb(T-shirt)\nProb(Sandal)Actual\nlabels\nStep 4: Feedback in the form of cross-entropy lossStep 1 Step 2 Step 3\nFigure 2.1    The steps involved in training a deep learning model\n30 chapter  2 Deep learning with PyTorch\nFirst, we’ll obtain a dataset of grayscale clothing images, as shown on the left of figure \n2.1. The images are in raw pixels, and we’ll convert them to PyTorch tensors in the \nform of float numbers (step 1). Each image comes with a label. \nWe’ll then create a deep neural network in PyTorch, as shown in the center of figure \n2.1. Some neural networks in this book involve convolutional neural networks (CNNs). \nFor this simple classification problem, we’ll use dense layers only for the moment. \nWe’ll select a loss function for multicategory classification, and cross-entropy loss is \ncommonly used for this task. Cross-entropy loss measures the difference between the \npredicted probability distribution and the true distribution of the labels. We’ll use the \nAdam optimizer (a variant of the gradient descent algorithm) to update the network’s \nweights during training. We set the learning rate to 0.001. The learning rate controls \nhow much the model’s weights are adjusted with respect to the loss gradient during \ntraining.\nOptimizers in ML\nOptimizers in ML are algorithms that update model parameters based on gradient infor -\nmation to minimize the loss function. Stochastic Gradient Descent (SGD) is the most fun -\ndamental optimizer, utilizing straightforward updates based on the loss gradient. Adam, \nthe most popular optimizer, is known for its efficiency and out-of-the-box performance, as \nit combines the strengths of the Adaptive Gradient Algorithm (AdaGrad) and Root Mean \nSquare Propagation (RMSProp). Despite their differences, all optimizers aim to iteratively \nadjust parameters to minimize the loss function, each creating a unique optimization \npath to reach this goal.\nWe’ll divide the training data into a train set and a validation set. In ML, we usually use \nthe validation set to provide an unbiased evaluation of the model and to select the best \nhyperparameters such as the learning rate, number of epochs of training, and so on. \nThe validation set can also be used to avoid overfitting the model in which the model \nworks well in the training set but poorly on unseen data. An epoch is when all the train -\ning data is used to train the model once and only once. \nDuring training, you’ll iterate through the training data. During forward passes, you \nfeed images through the network to obtain predictions (step 2) and compute the loss \nby comparing the predicted labels with the actual labels (step 3; see the right side of \nfigure 2.1). You’ll then backpropagate the gradient through the network to update the \nweights. This is where the learning happens (step 4), as shown at the bottom of figure \n2.1.\nYou’ll use the validation set to determine when we should stop training. We calcu -\nlate the loss in the validation set. If the model stops improving after a fixed number of \nepochs, we consider the model trained. We then evaluate the trained model on the test \nset to assess its performance in classifying images into different labels. \nNow that you have a high-level overview of how deep learning in PyTorch works, let’s \ndive into the end-to-end project!",4667
21-2.2.2 Preprocessing data.pdf,21-2.2.2 Preprocessing data,"31 An end-to-end deep learning project with PyTorch\n2.2.2 Preprocessing data\nWe’ll be using the Fashion Modified National Institute of Standards and Technology \n(MNIST) dataset in this project. Along the way, you’ll learn how to use the datasets  \nand transforms  packages in the Torchvision library, as well as the Dataloader  pack -\nages in PyTorch that will help you for the rest of the book. You’ll use these tools to \npreprocess data throughout the book. The Torchvision library provides tools for image \nprocessing, including popular datasets, model architectures, and common image \ntransformations for deep learning applications. \nWe first import needed libraries and instantiate a Compose()  class in the transforms \npackage to transform raw images to PyTorch tensors.\nListing 2.2    Transforming raw image data to PyTorch tensors\nimport torch\nimport torch.nn as nn\nimport torchvision\nimport torchvision.transforms as T\ntorch.manual_seed(42)\ntransform=T.Compose([    \n    T.ToTensor(),    \n    T.Normalize([0.5],[0.5])])    \nWe use the manual_seed()  method in PyTorch to fix the random state so that results \nare reproducible. The transforms  package in Torchvision can help create a series of \ntransformations to preprocess images. The ToTensor()  class converts image data (in \neither Python Imaging Library (PIL) image formats or NumPy arrays) into PyTorch \ntensors. In particular, the image data are integers ranging from 0 to 255, and the \nToTensor()  class converts them to float tensors with values in the range of 0.0 and 1.0.\nThe Normalize()  class normalizes tensor images with mean and standard deviation \nfor n channels. The Fashion MNIST data are grayscale images of clothing items so there is \nonly one color channel. Later in this book, we’ll deal with images of three different color \nchannels (red, green, and blue). In the preceding code cell, Normalize([0.5],[0.5])  \nmeans that we subtract 0.5 from the data and divide the difference by 0.5. The resulting \nimage data range from –1 to 1. Normalizing the input data to the range [–1, 1] allows \ngradient descent to operate more efficiently by maintaining more uniform step sizes \nacross dimensions. This helps in faster convergence during training, and you’ll do this \noften in this book. \nNOTE     The code in listing 2.2 only defines the data transformation process. It \ndoesn’t perform the actual transformation, which happens in the next code cell.  \nNext, we use the datasets  package in Torchvision to download the dataset to a folder on \nyour computer and perform the transformation:Composes several \ntransforms together\nConverts image pixels \nto PyTorch tensors\nNormalizes the values \nto the range [– 1, 1]\n32 chapter  2 Deep learning with PyTorch\ntrain_set=torchvision.datasets.FashionMNIST(    \n    root=""."",    \n    train=True,    \n    download=True,    \n    transform=transform)    \ntest_set=torchvision.datasets.FashionMNIST(root=""."",\n    train=False,download=True,transform=transform)\nYou can print out the first sample in the training set:\nprint(train_set[0])\nThe first sample consists of a tensor with 784 values and a label 9. The 784 numbers \nrepresent a 28 by 28 grayscale image (28 × 28 = 784), and the label 9 means it’s an ankle \nboot. You may be wondering: How do you know the label 9 indicates an ankle boot? \nThere are 10 different types of clothing items. The labels in the dataset are numbered \nfrom 0 to 9. You can search online and find the text labels for the 10 categories (for \nexample, I got the text labels here https://github.com/pranay414/Fashion-MNIST  \n-Pytorch ). The list text_labels  contains the 10 text labels corresponding to the \nnumerical labels 0 to 9. For example, if an item has a numerical label of 0 in the dataset, \nthe corresponding text label is “t-shirt.” The list text_labels  is defined as follows:\ntext_labels=['t-shirt', 'trouser', 'pullover', 'dress', 'coat',\n             'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']\nWe can plot the data to visualize the clothing items in the dataset.\nListing 2.3    Visualizing the clothing items \n!pip install matplotlib\nimport matplotlib.pyplot as plt\nplt.figure(dpi=300,figsize=(8,4))\nfor i in range(24):\n    ax=plt.subplot(3, 8, i + 1)    \n    img=train_set[i][0]    \n    img=img/2+0.5    \n    img=img.reshape(28, 28)    \n    plt.imshow(img,\n               cmap=""binary"")\n    plt.axis('off')\n    plt.title(text_labels[train_set[i][1]],    \n        fontsize=8)\nplt.show()\nThe plot in figure 2.2 shows 24 clothing items such as coats, pullovers, sandals, and so \non. Which dataset \nto download\nWhere to save\nthe data\nThe training or \ntest dataset\nWhether or not to \ndownload the data to \nyour computer\nPerforms data \ntransformation\nWhere to place \nthe image\nObtains the i-th image \nfrom the training data\nConverts the values \nfrom [– 1,1] to [0, 1]\nReshapes the image \nto 28 by 28\nAdds text label to \neach image",4995
22-2.3 Binary classification.pdf,22-2.3 Binary classification,,0
23-2.3.2 Building and training a binary classification model.pdf,23-2.3.2 Building and training a binary classification model,"33 Binary classification\nFigure 2.2    Grayscale images of clothing items in the Fashion MNIST dataset. \nYou’ll learn how to create deep neural networks with PyTorch to perform binary and \nmulticategory classification problems in the next two sections.\n2.3 Binary classification\nIn this section, we’ll first create batches of data for training. We then build a deep \nneural network in PyTorch for this purpose and train the model using the data. Finally, \nwe’ll use the trained model to make predictions and test how accurate the predictions \nare. The steps involved with binary and multicategory classifications are similar, with a \nfew notable exceptions that I’ll highlight later. \n2.3.1 Creating batches\nWe’ll create a training set and a test set that contain only two types of clothing items: \nt-shirts and ankle boots. (Later in this chapter when we discuss multicategory classifi -\ncation, you’ll also learn to create a validation set to determine when to stop training.) \nThe following code cell  accomplishes that goal:\nbinary_train_set=[x for x in train_set if x[1] in [0,9]]\nbinary_test_set=[x for x in test_set if x[1] in [0,9]]\nWe only keep samples with numerical labels 0 and 9 to create a binary classification \nproblem with a balanced training set. Next, we create batches for training the deep \nneural network.\nListing 2.4    Creating batches for training and testing \nbatch_size=64\nbinary_train_loader=torch.utils.data.DataLoader(\n    binary_train_set,    Creates batches for the \nbinary training set \n34 chapter  2 Deep learning with PyTorch\n    batch_size=batch_size,    \n    shuffle=True)    \nbinary_test_loader=torch.utils.data.DataLoader(\n    binary_test_set,    \n    batch_size=batch_size,shuffle=True)\nThe DataLoader  class in the PyTorch utils package helps create data iterators in \nbatches. We set the batch size to 64. We created two data loaders in listing 2.4: a train -\ning set and a test set for binary classification. We shuffle the observations when creating \nbatches to avoid correlations among the original dataset: the training is more stable if \ndifferent labels are evenly distributed in the data loader. \n2.3.2 Building and training a binary classification model\nWe’ll first create a binary classification model. We then train the model by using the \nimages of t-shirts and ankle boots. Once it’s trained, we’ll see if the model can tell \nt-shirts from ankle boots. We use PyTorch to create the following neural network by \nusing the Pytorch nn.Sequential  class (in later chapters, you’ll also learn to use the \nnn.Module  class to create PyTorch neural networks).\nListing 2.5    Creating a binary classification model \nimport torch.nn as nn\ndevice=""cuda"" if torch.cuda.is_available() else ""cpu""    \nbinary_model=nn.Sequential(    \n    nn.Linear(28*28,256),    \n    nn.ReLU(),    \n    nn.Linear(256,128),\n    nn.ReLU(),\n    nn.Linear(128,32),\n    nn.ReLU(),\n    nn.Linear(32,1),\n    nn.Dropout(p=0.25),\n    nn.Sigmoid()).to(device)    \nThe Linear()  class in PyTorch creates a linear transformation of the incoming data. \nThis effectively creates a dense layer in the neural network. The input shape is 784 \nbecause we’ll later flatten the 2D image to a 1D vector with 28 × 28 = 784 values in it. \nWe flatten the 2D image into a 1D tensor because dense layers only take 1D inputs. \nIn later chapters, you’ll see that you don’t need to flatten images when you use con -\nvolutional layers. There are three hidden layers in the network, with 256, 128, and 32 \nneurons in them, respectively. The numbers 256, 128, and 32 are chosen somewhat \narbitrarily: changing them to, say, 300, 200, and 50 won’t affect the training process. Number of samples \nin each batch\nShuffles the observations \nwhen batching\nCreates batches for \nthe binary test set\nPyTorch automatically \ndetects if a CUDA-enabled \nGPU is available.\nCreates a sequential neural \nnetwork in PyTorch\nNumbers of input and output \nneurons in a linear layer\nApplies ReLU activation \nto outputs of the layer\nApplies sigmoid activation \nand moves the model to a \nGPU if available\n 35 Binary classification\nWe apply the rectified linear unit (ReLU) activation function on the three hidden \nlayers. The ReLU activation function decides whether a neuron should be turned on \nbased on the weighted sum. These functions introduce nonlinearity to the output of a \nneuron so that the network can learn nonlinear relations between inputs and outputs. \nReLU is your go-to activation function with very few exceptions, and you’ll encounter a \nfew other activation functions in later chapters. \nThe output of the last layer of the model contains a single value, and we use the \nsigmoid activation function to squeeze the number to the range [0, 1] so that it can be \ninterpreted as the probability that the object is an ankle boot. With the complementary \nprobability, the object is a t-shirt.\nHere we set the learning rate and define the optimizer and the loss function:\nlr=0.001\noptimizer=torch.optim.Adam(binary_model.parameters(),lr=lr)\nloss_fn=nn.BCELoss()\nWe set the learning rate to 0.001. What learning rate to set is an empirical question, \nand the answer comes with experience. It can also be determined by using hyperpa -\nrameter tuning using a validation set. Most optimizers in PyTorch use a default learn -\ning rate of 0.001. The Adam optimizer is a variant of the gradient descent algorithm, \nwhich is used to determine how much to adjust the model parameters in each training \nstep. The Adam optimizer was first introduced in 2014 by Diederik Kingma and Jimmy \nBa.1 In the traditional gradient descent algorithm, only gradients in the current itera -\ntion are considered. The Adam optimizer, in contrast, takes into consideration gradi -\nents in previous iterations as well. \nWe use nn.BCELoss() , which is the binary cross-entropy loss function. Loss func -\ntions measure how well an ML model performs. The training of a model involves adjust -\ning parameters to minimize the loss function. The binary cross-entropy loss function is \nwidely used in ML, particularly in binary classification problems. It measures the per -\nformance of a classification model whose output is a probability value between 0 and 1. \nThe cross-entropy loss increases as the predicted probability diverges from the actual \nlabel. \nWe train the neural network we just created as shown in the following listing.\nListing 2.6    Training a binary classification model \nfor i in range(50):    \n    tloss=0\n    for imgs,labels in binary_train_loader:    \n        imgs=imgs.reshape(-1,28*28)    \n        imgs=imgs.to(device)\n        labels=torch.FloatTensor(\\n          [x if x==0 else 1 for x in labels])    \n1 Diederik Kingma and Jimmy Ba, 2014, “Adam: A Method for Stochastic Optimization.” https: //arxiv.org/\nabs/1412.6980 .Trains for 50 epochs\nIterates through \nall batches\nFlattens the image before \nmoving the tensor to GPU\nConverts labels to 0 and 1",7054
24-2.4 Multicategory classification.pdf,24-2.4 Multicategory classification,"36 chapter  2 Deep learning with PyTorch\n        labels=labels.reshape(-1,1).to(device)\n        preds=binary_model(imgs)    \n        loss=loss_fn(preds,labels)    \n        optimizer.zero_grad()\n        loss.backward()    \n        optimizer.step()\n        tloss+=loss.detach()\n    tloss=tloss/n\n    print(f""at epoch {i}, loss is {tloss}"")\nIn training deep learning models in PyTorch, loss.backward()  computes the gra -\ndient of the loss with respect to each model parameter, enabling backpropagation, \nwhile optimizer.step()  updates the model parameters based on these computed \ngradients to minimize the loss. We train the model for 50 epochs for simplicity (an \nepoch is when the training data is used to train the model once). In the next section, \nyou’ll use a validation set and an early stopping class to determine how many epochs to \ntrain. In binary classifications, we label the targets as 0s and 1s. Since we have kept only \nt-shirts and ankle boots with labels 0 and 9, respectively, we converted them to 0 and 1 \nin listing 2.6. As a result, the labels for the two categories of clothing items are 0 and 1, \nrespectively. \nThis training takes a few minutes if you use GPU training. It takes longer if you use \nCPU training, but the training time should be less than an hour. \n2.3.3 Testing the binary classification model\nThe prediction from the trained binary classification model is a number between 0 \nand 1. We’ll use the torch.where()  method to convert the predictions into 0s and 1s: \nif the predicted probability is less than 0.5, we label the prediction as 0; otherwise, we \nlabel the prediction as 1. We then compare these predictions with the actual labels to \ncalculate the accuracy of the predictions. In the following listing, we use the trained \nmodel to make predictions on the test dataset.\nListing 2.7    Calculating the accuracy of the predictions \nimport numpy as np\nresults=[]\nfor imgs,labels in binary_test_loader:    \n    imgs=imgs.reshape(-1,28*28).to(device)\n    labels=(labels/9).reshape(-1,1).to(device)\n    preds=binary_model(imgs)\n    pred10=torch.where(preds>0.5,1,0)    \n    correct=(pred10==labels)    \n    results.append(correct.detach().cpu()\\n      .numpy().mean())    \naccuracy=np.array(results).mean()    \nprint(f""the accuracy of the predictions is {accuracy}"")   \nWe iterate through all batches of data in the test set. The trained model produces a \nprobability that the image is an ankle boot. We then convert the probability into 0 or Calculates the loss\nBackpropagation\nIterates through all \nbatches in the test set\nMakes predictions using \nthe trained model\nCompares predictions \nwith labels\nCalculates accuracy \nin the batch\nCalculates accuracy \nin the test set",2771
25-2.4.2 Building and training a multicategory classification model.pdf,25-2.4.2 Building and training a multicategory classification model,"37 Multicategory classification\n1 based on the cutoff value of 0.5, by using the torch.where()  method. The predic -\ntions are either 0 (i.e., a t-shirt) or 1 (an ankle boot) after the conversion. We compare \nthe predictions with the actual labels and see how many times the model gets it right. \nResults show that the accuracy of the predictions is 87.84% in the test set. \n2.4 Multicategory classification\nIn this section, we’ll build a deep neural network in PyTorch to classify the clothing \nitems into one of the 10 categories. We’ll then train the model with the Fashion MNIST \ndataset. Finally, we’ll use the trained model to make predictions and see how accurate \nthey are. We first create a validation set and define an early stopping class so that we \ncan determine when to stop training.\n2.4.1 Validation set and early stopping\nWhen we build and train a deep neural network, there are many hyperparameters that \nwe can choose (such as the learning rate and the number of epochs to train). These \nhyperparameters affect the performance of the model. To find the best hyperparame -\nters, we can create a validation set to test the performance of the model with different \nhyperparameters. \nTo give you an example, we’ll create a validation set in the multicategory classifica -\ntion to determine the optimal number of epochs to train. The reason we do this in the \nvalidation set instead of the training set is to avoid overfitting, when a model performs \nwell in the training set but poorly in out-of-the-sample tests (i.e., on unseen data). \nHere we divide 60,000 observations of the training dataset into a train set and a vali -\ndation set:\ntrain_set,val_set=torch.utils.data.random_split(\\n    train_set,[50000,10000])\nThe original train set now becomes two sets: the new train set with 50,000 observations \nand a validation set with the remaining 10,000 observations. \nWe use the DataLoader  class in the PyTorch utils package to convert the train, valida -\ntion, and test sets into three data iterators in batches:\ntrain_loader=torch.utils.data.DataLoader(\n    train_set,    \n    batch_size=batch_size,   \n    shuffle=True)   \nval_loader=torch.utils.data.DataLoader(\n    val_set,    \n    batch_size=batch_size,   \n    shuffle=True)\ntest_loader=torch.utils.data.DataLoader(\n    test_set,    \n    batch_size=batch_size,   \n    shuffle=True)\nNext, we define an EarlyStop()  class and create an instance of the class.\n38 chapter  2 Deep learning with PyTorch\nListing 2.8    The EarlyStop()  class to determine when to stop training\nclass EarlyStop:\n    def __init__(self, patience=10):    \n        self.patience = patience\n        self.steps = 0\n        self.min_loss = float('inf')\n    def stop(self, val_loss):    \n        if val_loss < self.min_loss:    \n            self.min_loss = val_loss\n            self.steps = 0\n        elif val_loss >= self.min_loss:    \n            self.steps += 1\n        if self.steps >= self.patience:\n            return True\n        else:\n            return False\nstopper=EarlyStop()\nThe EarlyStop()  class determines if the loss in the validation set has stopped improv -\ning in the last patience=10  epochs. We set the default value of patience  argument \nto 10, but you can choose a different value when you instantiate the class. The value of \npatience  measures how many epochs you want to train since the last time the model \nreached the minimum loss. The stop()  method keeps a record of the minimum loss \nand the number of epochs since the minimum loss and compares the number to the \nvalue of patience . The method returns a value of True  if the number of epochs since \nthe minimum loss is greater than the value of patience .\n2.4.2 Building and training a multicategory classification model\nThe Fashion MNIST dataset contains 10 different categories of clothing items. There -\nfore, we create a multicategory classification model to classify them. Next, you’ll learn \nhow to create such a model and train it. You’ll also learn how to make predictions \nusing the trained model and assess the accuracy of the predictions. We use PyTorch to \ncreate the neural network for multicategory classification in the following listing.\nListing 2.9    Creating a multicategory classification model \nmodel=nn.Sequential(\n    nn.Linear(28*28,256),\n    nn.ReLU(),\n    nn.Linear(256,128),\n    nn.ReLU(),\n    nn.Linear(128,64),\n    nn.ReLU(),\n    nn.Linear(64,10)    \n    ).to(device)    \nCompared to the binary classification model we created in the last section, we have \nmade a few changes here. First, the output now has 10 values in it, representing the 10 \ndifferent types of clothing items in the dataset. Second, we have changed the number Sets the default value \nof patience to 10\nDefines the stop() \nmethod\nIf a new minimum loss \nis reached, updates the \nvalue of min_loss\nCounts how many \nepochs since the last \nminimum loss\nThere are 10 neurons \nin the output layer.\nDoes not apply softmax \nactivation on the output\n 39 Multicategory classification\nof neurons in the last hidden layer from 32 to 64. A rule of thumb in creating deep \nneural networks is to gradually increase or decrease the number of neurons from one \nlayer to the next. Since the number of output neurons has increased from 1 (in binary \nclassification) to 10 (in multicategory classification), we change the number of neu -\nrons from 32 to 64 in the second to last layer to match the increase. However, there is \nnothing special about the number 64: if you use, say, 100 neurons in the second to last \nlayer, you’ll get similar results. \nWe’ll use the PyTorch nn.CrossEntropyLoss() class as our loss function, which \ncombines nn.LogSoftmax()  and nn.NLLLoss() in one single class. See the documen -\ntation here for details: https: //mng.bz/pxd2 . In particular, the documentation states, \n“This criterion computes the cross entropy loss between input logits and target.” This \nexplains why we didn’t apply the softmax activation in the proceeding listing. In the \nbook’s GitHub repository, I have demonstrated that if we use nn.LogSoftmax()  in the \nmodel and use nn.NLLLoss() as the loss function, we obtain identical results.\nAs a result, the nn.CrossEntropyLoss() class will apply the softmax activation \nfunction on the output to squeeze the 10 numbers into the range [0, 1] before the \nlogarithm operation. The preferred activation function on the output is sigmoid in \nbinary classifications and softmax in multicategory classifications. Further, the 10 num -\nbers after softmax activation add up to 1, which can be interpreted as the probabilities \ncorresponding to the 10 types of clothing items. We’ll use the same learning rate and \noptimizer as those in the binary classification in the last section.\nlr=0.001\noptimizer=torch.optim.Adam(model.parameters(),lr=lr)\nloss_fn=nn.CrossEntropyLoss()\nWe define the train_epoch() as follows:\ndef train_epoch():\n    tloss=0\n    for n,(imgs,labels) in enumerate(train_loader):    \n        imgs=imgs.reshape(-1,28*28).to(device)\n        labels=labels.reshape(-1,).to(device)\n        preds=model(imgs)    \n        loss=loss_fn(preds,labels)\n        optimizer.zero_grad()\n        loss.backward()    \n        optimizer.step()\n        tloss+=loss.detach()\n    return tloss/n\nThe function trains the model for one epoch. The code is similar to what we have seen \nin the binary classification, except that the labels are from 0 to 9, instead of two num -\nbers (0 and 1). \nWe also define a val_epoch()  function:\ndef val_epoch():\n    vloss=0\n    for n,(imgs,labels) in enumerate(val_loader):    \n        imgs=imgs.reshape(-1,28*28).to(device)\n40 chapter  2 Deep learning with PyTorch\n        labels=labels.reshape(-1,).to(device)\n        preds=model(imgs)    \n        loss=loss_fn(preds,labels)    \n        vloss+=loss.detach()\n    return vloss/n\nThe function uses the model to make predictions on images in the validation set and \ncalculate the average loss per batch of data. \nWe now train the multicategory classifier:\nfor i in range(1,101):    \n    tloss=train_epoch()\n    vloss=val_epoch()\n    print(f""at epoch {i}, tloss is {tloss}, vloss is {vloss}"")\n    if stopper.stop(vloss)==True:             \n        break  \nWe train a maximum of 100 epochs. In each epoch, we first train the model using the \ntraining set. We then calculate the average loss per batch in the validation set. We use \nthe EarlyStop()  class to determine if the training should stop by looking at the loss in \nthe validation set. The training stops if the loss hasn’t improved in the last 10 epochs. \nAfter 19 epochs, the training stops. \nThe training takes about 5 minutes if you use GPU training, which is longer than the \ntraining process in binary classification since we have more observations in the training \nset now (10 clothing items instead of just 2). \nThe output from the model is a vector of 10 numbers. We use torch.argmax()  to \nassign each observation a label based on the highest probability. We then compare the \npredicted label with the actual label. To illustrate how the prediction works, let’s look at \nthe predictions on the first five images in the test set. \nListing 2.10    Testing the trained model on five images\nplt.figure(dpi=300,figsize=(5,1))\nfor i in range(5):    \n    ax=plt.subplot(1,5, i + 1)    \n    img=test_set[i][0]    \n    label=test_set[i][1]\n    img=img/2+0.5    \n    img=img.reshape(28, 28)    \n    plt.imshow(img, cmap=""binary"")\n    plt.axis('off')\n    plt.title(text_labels[label]+f""; {label}"", fontsize=8)\nplt.show()\nfor i in range(5):\n    img,label = test_set[i]    \n    img=img.reshape(-1,28*28).to(device)\n    pred=model(img)    \n    index_pred=torch.argmax(pred,dim=1)    \n    idx=index_pred.item()\n    print(f""the label is {label}; the prediction is {idx}"")    Plots the first five \nimages in the test set \nwith their labels\nObtains the \ni-th image and \nlabel in the \ntest set\nPredicts using \nthe trained \nmodel\nUses the torch  \n.argmax() \nmethod to \nobtain the \npredicted labelPrints out the actual label \nand the predicted label\n 41 Multicategory classification\nWe plot the first five clothing items in the test set in a 1 × 5 grid. We then use the \ntrained model to make a prediction on each clothing item. The prediction is a tensor \nwith 10 values. The torch.argmax()  method returns the position of the highest prob -\nability in the tensor, and we use it as the predicted label. Finally, we print out both the \nactual label and the predicted label to compare and see if the predictions are correct. \nAfter running the previous code listing, you should see the image in figure 2.3.\nFigure 2.3    The first five clothing items in the test dataset and their respective labels. Each clothing \nitem has a text label and a numerical label between 0 and 9. \nFigure 2.3 shows that the first five clothing items in the test set are ankle boot, pullover, \ntrouser, trouser, and shirt, respectively, with numerical labels 9, 2, 1, 1, and 6. \nThe output after running the code in listing 2.10 is as follows:\nthe label is 9; the prediction is 9\nthe label is 2; the prediction is 2\nthe label is 1; the prediction is 1\nthe label is 1; the prediction is 1\nthe label is 6; the prediction is 6\nThe preceding output shows that the model has made correct predictions on all five \nclothing items. \nFixing the random state in PyTorch\nThe torch.manual_seed()  method fixes the random state so the results are the \nsame when you rerun your programs. However, you may get different results from those \nreported in this chapter even if you use the same random seed. This happens because \ndifferent hardware and different versions of PyTorch handle floating point operations \nslightly differently. See, for example, the explanations at https://mng.bz/RNva . The dif -\nference is generally minor, though, so no need to be alarmed. \nNext, we calculate the accuracy of the predictions on the whole test dataset.\nListing 2.11    Testing the trained multicategory classification model \nresults=[]\nfor imgs,labels in test_loader:    \n    imgs=imgs.reshape(-1,28*28).to(device)\n    labels=(labels).reshape(-1,).to(device)\n    preds=model(imgs)    Iterates through all \nbatches in the test set\nPredicts using the \ntrained model\n42 chapter  2 Deep learning with PyTorch\n    pred10=torch.argmax(preds,dim=1)    \n    correct=(pred10==labels)    \n    results.append(correct.detach().cpu().numpy().mean())\naccuracy=np.array(results).mean()    \nprint(f""the accuracy of the predictions is {accuracy}"") \nThe output is \nthe accuracy of the predictions is 0.8819665605095541\nWe iterate through all clothing items in the test set and use the trained model to make \npredictions. We then compare the predictions with the actual labels. The accuracy is \nabout 88% in the out-of-sample test. Given that a random guess has an accuracy of \nabout 10%, 88% accuracy is fairly high. This indicates that we have built and trained \ntwo successful deep learning models in PyTorch! You’ll use these skills quite often later \nin this book. For example, in chapter 3, the discriminator network you’ll construct \nis essentially a binary classification model, similar to what you have created in this \nchapter. \nSummary\n¡ In PyTorch, we use tensors to hold various forms of input data so we can feed \nthem to deep learning models.  \n¡ You can index and slice PyTorch tensors, reshape them, and conduct mathemat -\nical operations on them. \n¡ Deep learning is a type of ML method that uses deep artificial neural networks to \nlearn the relation between input and output data. \n¡ The ReLU activation function decides whether a neuron should be turned on \nbased on the weighted sum. It introduces nonlinearity to the output of a neuron.\n¡ Loss functions measure how well an ML model performs. The training of a model \ninvolves adjusting parameters to minimize the loss function. \n¡ Binary classification is an ML model to classify observations into one of two \ncategories. \n¡ Multicategory classification is an ML model to classify observations into one of \nmultiple categories. Converts probabilities \nto a predicted label\nCompares the \npredicted label with \nthe actual label\nCalculates accuracy \nin the test set",14421
26-3.1 Steps involved in training GANs.pdf,26-3.1 Steps involved in training GANs,"433Generative adversarial \nnetworks: Shape and \nnumber generation\nThis chapter covers\n¡ Building generator and discriminator networks in  \n generative adversarial networks from scratch \n¡ Using GANs to generate data points to form shapes  \n (e.g., exponential growth curve) \n¡ Generating integer sequences that are all  \n multiples of 5\n¡ Training, saving, loading, and using GANs\n¡ Evaluating GAN performance and determining  \n training stop points\nClose to half of the generative models in this book belong to a category called gener -\native adversarial networks (GANs). The method was first proposed by Ian Goodfel -\nlow and his coauthors in 2014.1 GANs, celebrated for their ease of implementation \nand versatility, empower individuals with even rudimentary knowledge of deep \nlearning to construct their models from the ground up. The word “adversarial” in \n1 Goodfellow et al, 2014, “Generative Adversarial Nets.” https: //arxiv.org/abs/1406.2661 .\n44 chapter  3 Generative adversarial networks: Shape and number generation\nGAN refers to the fact that the two neural networks compete against each other in a \nzero-sum game framework. The generative network tries to create data instances indis -\ntinguishable from real samples. In contrast, the discriminative network tries to identify \nthe generated samples from real ones. These versatile models can generate various \ncontent formats, from geometric shapes and sequences of numbers to high-resolution \ncolor images and even realistic-sounding musical compositions. \nIn this chapter, we’ll briefly review the theory behind GANs. Then, I’ll show you how \nto implement that knowledge in PyTorch. You’ll learn to build your first GAN from \nscratch so that all the details are demystified. To make the example relatable, imagine \nyou put $1 in a savings account that pays 8% a year. You want to find out the balance in \nyour account based on the number of years you have invested. The true relation is an \nexponential growth curve. You’ll learn to use GANs to generate data samples—pairs of \nvalues (x, y) that form such an exponential growth curve, with a mathematical relation \ny = 1.08x. Armed with this skill, you’ll be able to generate data to mimic any shape: sine, \ncosine, quadratic, and so on. \nIn the second project in this chapter, you’ll learn how to use GANs to generate a \nsequence of numbers that are all multiples of 5. But you can change the pattern to mul -\ntiples of 2, 3, 7, or other patterns. Along the way, you’ll learn how to create a generator \nnetwork and a discriminator network from scratch. You’ll learn how to train, save, and \nuse GANs. Further, you’ll also learn to assess the performance of GANs either by visu -\nalizing samples generated by the generator network or by measuring the divergence \nbetween the generated sample distribution and the real data distribution. \nImagine that you need data to train a machine learning (ML) model to predict \nthe relation between pairs of values (x, y). However, the training dataset is costly and \ntime-consuming for human beings to prepare by hand. GANs can be well-suited to gen -\nerate data in such cases: while the generated values of x and y generally conform to a \nmathematical relation, there is also noise in the generated data. The noise can be useful \nfor preventing overfitting when the generated data is used to train the ML model.  \nThe primary goal of this chapter is not necessarily to generate novel content with \nthe most practical use. Instead, my objective is to teach you how to train and use GANs \nto create various formats of content from scratch. Along the way, you will gain a solid \nunderstanding of the inner workings of GANs. This foundation will allow us to concen -\ntrate on other, more advanced, aspects of GANs in later chapters when generating other \ncontent such as high-resolution images or realistic-sounding music (e.g., convolutional \nneural networks or how to represent a piece of music as a multidimensional object).\n3.1 Steps involved in training GANs\nIn chapter 1, you gained a high-level overview of the theories behind GANs. In this \nsection, I’ll provide a summary of the steps involved in training GANs in general and in \ncreating data points to form an exponential growth curve in particular.\n 45 Steps involved in training GANs\nLet’s return to our previous example: you plan to invest in a savings account that \npays 8% annual interest. You put $1 in the account today and want to know how much \nmoney you’ll have in the account in the future. \nThe amount in your account in the future, y, depends on how long you invest in the \nsavings account. Let’s denote the number of years you invest by x, which can be a num -\nber, say, between 0 and 50. For example, if you invest for 1 year, the balance is $1.08; \nif you invest for 2 years, the balance is 1.082 = $1.17. To generalize, the relationship \nbetween x and y is y = 1.08x. The function depicts an exponential growth curve. Note \nhere that x can be a whole number such as 1 or 2, as well as a decimal number such as \n1.14 or 2.35 and the formula still works. \nTraining GANs to generate data points that conform to a specific mathematical \nrelation, like the preceding example, is a multistep process. In your case, you want to \ngenerate data points (x, y) such that y = 1.08x. Figure 3.1 provides a diagram of the archi -\ntecture of GANs and the steps involved in generating an exponential growth curve. \nWhen you generate other content such as a sequence of integers, images, or music, you \nfollow similar steps, as you’ll see in the second project in this chapter, as well as in other \nGAN models later in this book. \nLatent\nvariable Z\n Prediction\n(real/fake?)Label (ground\ntruth) Step 4: FeedbackReal sample\nStep 4: FeedbackStep 1\nStep 3\nFake sample\nGenerator\nnetworkDiscriminator \nnetworkStep 2\nFigure 3.1    The steps involved in training GANs to generate an exponential growth curve and the dual-network \narchitecture in GANs. The generator obtains a random noise vector Z from the latent space (top left) to create a \nfake sample and presents it to the discriminator (middle). The discriminator classifies a sample as real (from the \ntraining set) or fake (created by the generator). The predictions are compared to the ground truth and both the \ndiscriminator and the generator learn from the predictions. After many iterations of training, the generator learns to \ncreate shapes that are indistinguishable from real samples.\nBefore we start, we need to obtain a training dataset to train GANs. In our running \nexample, we’ll generate a dataset of (x, y) pairs using the mathematical relation  \ny = 1.08x. We use the savings account example so that the numbers are relatable. The \n46 chapter  3 Generative adversarial networks: Shape and number generation\ntechniques you learn in this chapter can be applied to other shapes: sine, cosine, \nU-shape, and so on. You can choose a range of x values (say, 0 to 50) and calculate the \ncorresponding y values. Since we usually train models in batches of data in deep learn -\ning, the number of observations in your training dataset is usually set to a multiple of \nthe batch size. A real sample is located at the top of figure 3.1, which has an exponen -\ntial growth curve shape. \nOnce you have the training set ready, you need to create two networks in GANs: \na generator and a discriminator. The generator, located at the bottom left of figure \n3.1, takes a random noise vector Z as the input and generates data points (step 1 of \nour training loop). The random noise vector Z used by the generator is obtained from \nthe latent space, which represents the range of possible outputs the GAN can produce \nand is central to the GAN’s ability to generate diverse data samples. In chapter 5, we’ll \nexplore the latent space to select the attributes of the content created by the generator. \nThe discriminator, located at the center of figure 3.1, evaluates whether a given data \npoint (x, y) is real (from the training dataset) or fake (created by the generator); this is \nstep 2 of our training loop. \nThe meaning of the latent space\nThe latent space in a GAN is a conceptual space where each point can be transformed \ninto a realistic data instance by the generator. This space represents the range of pos -\nsible outputs the GAN can produce and is central to the GAN’s ability to generate var -\nied and complex data. The latent space acquires its significance exclusively when it is \nemployed in conjunction with the generative model. Within this context, one can inter -\npolate between points in the latent space to affect the attributes of output, which we’ll \ndiscuss in chapter 5.\nTo know how to adjust model parameters, we must choose the right loss functions. We \nneed to define the loss functions for both the generator and discriminator. The loss \nfunction encourages the generator to generate data points that resemble data points \nfrom the training dataset, making the discriminator classify them as real. The loss func -\ntion encourages the discriminator  to correctly classify real and generated data points.\nIn each iteration of the training loop, we alternate between training the discrimi -\nnator and the generator. During each training iteration, we sample a batch of real (x, \ny) data points from the training dataset and a batch of fake data points generated by \nthe generator. When training the discriminator, we compare the predictions by the dis -\ncriminative model, which is a probability that the sample is from the training set, with \nthe ground truth, which is 1 if the sample is real and 0 if the sample is fake (shown at \nthe right of figure 3.1); this constitutes half of step 3 in the training loop. We adjust the \nweights in the discriminator network slightly so that in the next iteration, the predicted \nprobability moves closer to the ground truth (half of step 4 in our training loop). \nWhen training the generator, we feed fake samples to the discriminative model and \nobtain a probability that the sample is real (the other half of step 3). We then adjust the",10209
27-3.2 Preparing training data.pdf,27-3.2 Preparing training data,,0
28-3.3 Creating GANs.pdf,28-3.3 Creating GANs,"47 Preparing training data\nweights in the generator network slightly so that in the next iteration, the predicted \nprobability moves closer to 1 (since the generator wants to create samples to fool the \ndiscriminator into thinking they are real); this constitutes the other half of step 4. We \nrepeat this process for many iterations, making the generator network create more real -\nistic data points.\nA natural question is when to stop training the GANs. For that, you evaluate the \nGAN’s performance by generating a set of synthetic data points and comparing them to \nthe real data points from the training dataset. In most cases, we use visualization tech -\nniques to assess how well the generated data conforms to the desired relation. However, \nin our running example, since we know the distribution of the training data, we can \ncalculate the mean squared error (MSE) between the generated data and the true data \ndistribution. We stop training GANs when the generated samples stop improving their \nqualities after a fixed number of rounds of training. \nAt this point, the model is considered trained. We then discard the discriminator and \nkeep the generator. To create an exponential growth curve, we feed a random noise \nvector Z to the trained generator and obtain pairs of (x, y) to form the desired shape.\n3.2 Preparing training data\nIn this section, you’ll create the training dataset so that you can use it to train the GAN \nmodel later in this chapter. Specifically, you’ll create pairs of data points (x, y) that \nconform to the exponential growth shape. You’ll place them in batches so that they are \nready to be fed to deep neural networks.\nNOTE     The code for this chapter, as well as other chapters in this book, is avail -\nable at the book’s GitHub repository: https: //github.com/markhliu/DGAI .\n3.2.1 A training dataset that forms an exponential growth curve\nWe’ll create a dataset that contains many observations of data pairs, (x, y), where x is \nuniformly distributed in the interval [0, 50] and y is related to x based on the formula  \ny = 1.08x, as shown in the following listing. \nListing 3.1    Creating training data to form an exponential growth shape\nimport torch\ntorch.manual_seed(0)    \nobservations = 2048\ntrain_data = torch.zeros((observations, 2))    \ntrain_data[:,0]=50*torch.rand(observations)    \ntrain_data[:,1]=1.08**train_data[:,0]    Fixes the random state so \nresults are reproducible\nCreates a tensor with 2,048 \nrows and 2 columns\nGenerates values of x \nbetween 0 and 50\nGenerates values of y based \non the relation y = 1.08x\n48 chapter  3 Generative adversarial networks: Shape and number generation\nFirst, we create 2,048 values of x between 0 and 50 using the torch.rand()  method. \nWe use the manual_seed()  method in PyTorch to fix the random state so that all \nresults are reproducible. We first create a PyTorch tensor, train_data , with 2,048 rows \nand 2 columns. The values of x are placed in the first column in the tensor train_\ndata . The rand()  method in PyTorch generates random values between 0.0 and 1.0. \nBy multiplying the value by 50, the resulting values of x are between 0.0 and 50.0. We \nthen fill the second column of train_data  with values of y = 1.08x. \nExercise 3.1\nModify listing 3.1 so that the relation between x and y is y = sin(x) by using the torch \n.sin()  function. Set the value of x between –5 and 5 by using this line of code: train_\ndata[:,0]=10*(torch.rand(observations)-0.5) . \nWe plot the relation between x and y by using the Matplotlib library.\nListing 3.2    Visualizing the relation between x and y\nimport matplotlib.pyplot as plt\nfig=plt.figure(dpi=100,figsize=(8,6))\nplt.plot(train_data[:,0],train_data[:,1],""."",c=""r"")    \nplt.xlabel(""values of x"",fontsize=15)\nplt.ylabel(""values of $y=1.08^x$"",fontsize=15)    \nplt.title(""An exponential growth shape"",fontsize=20)    \nplt.show()\nYou will see an exponential growth curve shape after running listing 3.2, which is simi -\nlar to the top graph in figure 3.1.\nExercise 3.2\nModify listing 3.2 to plot the relation between x and y = sin(x) based on your changes in \nexercise 3.1. Make sure you change the y-axis label and the title in the plot to reflect the \nchanges you made. \n3.2.2 Preparing the training dataset\nWe’ll place the data samples you just created into batches so that we can feed them \nto the discriminator network. We use the DataLoader()  class in PyTorch to wrap an \niterable around the training dataset so that we can easily access the samples during \ntraining, like so:Plots the relation \nbetween x and y\nLabels y-axis\nCreates a title \nfor the plot",4688
29-3.4.1 The training of GANs.pdf,29-3.4.1 The training of GANs,"49 Creating GANs\nfrom torch.utils.data import DataLoader\nbatch_size=128\ntrain_loader=DataLoader(\n    train_data,\n    batch_size=batch_size,\n    shuffle=True)\nMake sure you select the total number of observations and the batch size so that all \nbatches have the same number of samples in them. We chose 2,048 observations with \na batch size of 128. As a result, we have 2,048/128 = 16 batches. The shuffle=True  \nargument in DataLoader()  shuffles the observations randomly before dividing them \ninto batches. \nNOTE     Shuffling makes sure that the data samples are evenly distributed and \nobservations within a batch are not correlated, which, in turn, stabilizes training. \nIn this specific example, shuffling  ensures that values of x fall randomly between \n0 and 50, instead of clustering in a certain range, say, between 0 and 5. \nYou can access a batch of data by using the next()  and iter()  methods, like so:\nbatch0=next(iter(train_loader))\nprint(batch0)\nYou will see 128 pairs of numbers (x, y), where the value of x falls randomly between \n0 and 50. Further, the values of x and y in each pair conform to the relation y = 1.08x. \n3.3 Creating GANs\nNow that the training dataset is ready, we’ll create a discriminator network and a gen -\nerator network. The discriminator network is a binary classifier, which is very similar to \nthe binary classifier for clothing items we have created and trained in chapter 2. Here, \nthe discriminator’s job is to classify the samples into either real or fake. The generator \nnetwork, on the other hand, tries to create data points (x, y) that are indistinguishable \nfrom those in the training set so that the discriminator will classify them as real. \n3.3.1 The discriminator network\nWe use PyTorch to create a discriminator neural network. We’ll use fully connected \n(dense) layers with ReLU  activations. We’ll also use dropout layers to prevent overfit -\nting. We create a sequential deep neural network in PyTorch to represent the discrimi -\nnator, as shown in the following listing.\nListing 3.3    Creating a discriminator network\nimport torch.nn as nn\ndevice=""cuda"" if torch.cuda.is_available() else ""cpu""    Automatically checks if \nCUDA-enabled GPU is \navailable.\n50 chapter  3 Generative adversarial networks: Shape and number generation\nD=nn.Sequential(\n    nn.Linear(2,256),    \n    nn.ReLU(),\n    nn.Dropout(0.3),    \n    nn.Linear(256,128),\n    nn.ReLU(),\n    nn.Dropout(0.3),\n    nn.Linear(128,64),\n    nn.ReLU(),\n    nn.Dropout(0.3),\n    nn.Linear(64,1),    \n    nn.Sigmoid()).to(device)\nMake sure that in the first layer, the input shape is 2 because, in our sample, each data \ninstance has two values in it: x and y. The number of inputs in the first layer should \nalways match with the size of the input data. Also, make sure that the number of output \nfeatures is 1 in the last layer: the output of the discriminator network is a single value. \nWe use the sigmoid activation function to squeeze the output to the range [0, 1] so that \nit can be interpreted as the probability, p, that the sample is real. With the complemen -\ntary probability, 1 – p, the sample is fake. This is very similar to what we have done in \nchapter 2 when a binary classifier attempts to identify a piece of clothing item as either \nan ankle boot or a t-shirt. \nThe hidden layers have 256, 128, and 64 neurons in them, respectively. There is \nnothing magical about these numbers, and you can easily change them and have simi -\nlar results as long as they are in a reasonable range. If the number of neurons in hidden \nlayers is too large, it may lead to overfitting of the model; if the number is too small, it \nmay lead to underfitting. The number of neurons can be optimized separately using a \nvalidation set through hyperparameter tuning. \nDropout layers randomly deactivate (or “drop out”) a certain percentage of neurons \nin the layer to which they are applied. This means that these neurons do not participate \nin forward or backward passes during training. Overfitting occurs when a model learns \nnot only the underlying patterns in the training data but also the noise and random \nfluctuations, leading to poor performance on unseen data. Dropout layers are an effec -\ntive way to prevent overfitting.2\n3.3.2 The generator network\nThe generator’s job is to create a pair of numbers (x, y) so that it can pass the screen -\ning of the discriminator. That is, the generator is trying to create a pair of numbers to \nmaximize the probability that the discriminator thinks that the numbers are from the \ntraining dataset (i.e., they conform to the relation y = 1.08x). We create the neural net -\nwork in the following listing to represent the generator.\n2 Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov, 2014, “Dropout: A \nSimple Way to Prevent Neural Networks from Overfitting.” Journal of Machine Learning Research  15 (56): 1929−1958.The number of input features \nin the first layer is 2, matching \nthe number of elements in \neach data instance, which has \ntwo values, x and y.\nThe dropout layer \nprevents overfitting.\nThe number of output features in the \nlast layer is 1 so that we can squeeze \nit into a value between 0 and 1.\n 51 Creating GANs\nListing 3.4    Creating a generator network\nG=nn.Sequential(\n    nn.Linear(2,16),    \n    nn.ReLU(),\n    nn.Linear(16,32),\n    nn.ReLU(),\n    nn.Linear(32,2)).to(device)    \nWe feed a random noise vector from a 2D latent space, (z1, z2), to the generator. The \ngenerator then generates a pair of values (x, y), based on the input from the latent \nspace. Here we use a 2D latent space, but changing the dimension to other numbers \nsuch as 5 or 10 wouldn’t affect our results.  \n3.3.3 Loss functions, optimizers, and early stopping\nSince the discriminator network is essentially performing a binary classification task \n(identifying a data sample as real or fake), we use binary cross-entropy loss, the pre -\nferred loss function in binary classifications, for the discriminator network. The dis -\ncriminator is trying to maximize the accuracy of the binary classification: identify a real \nsample as real and a fake sample as fake. The weights in the discriminator network are \nupdated based on the gradient of the loss function with respect to the weights.\nThe generator is trying to minimize the probability that the fake sample is being \nidentified as fake. Therefore, we’ll also use binary cross-entropy loss for the generator \nnetwork: the generator updates its network weights so that the generated samples will \nbe classified as real by the discriminator in a binary classification problem. \nAs we have done in chapter 2, we use the Adam optimizer as the gradient descent \nalgorithm. We set the learning rate to 0.0005. Let’s code those steps in by using PyTorch:\nloss_fn=nn.BCELoss()\nlr=0.0005\noptimD=torch.optim.Adam(D.parameters(),lr=lr)\noptimG=torch.optim.Adam(G.parameters(),lr=lr)\nOne question remains before we get to the actual training: How many epochs should \nwe train the GANs? How do we know the model is well trained so that the generator \nis ready to create samples that can mimic the exponential growth curve shape? If you \nrecall, in chapter 2, we split the training set further into a train set and a validation set. \nWe then used the loss in the validation set to determine whether the parameters had \nconverged so that we could stop training. However, GANs are trained using a different \napproach compared to traditional supervised learning models (such as the classifica -\ntion models you have seen in chapter 2). Since the quality of the generated samples \nimproves throughout training, the discriminator’s task becomes more and more dif -\nficult (in a way, the discriminator in GANs is making predictions on a moving target). The number of input features in the first layer \nis 2, the same as the dimension of the random \nnoise vector from the latent space.\nThe number of output features in the last \nlayer is 2, the same as the dimension of the \ndata sample, which contains two values (x, y).\n52 chapter  3 Generative adversarial networks: Shape and number generation\nThe loss from the discriminator network is not a good indicator of the quality of the \nmodel. \nOne common method to measure the performance of GANs is through visual inspec -\ntion. Humans can assess the quality and realism of generated data instances by simply \nlooking at them. This is a qualitative approach but can be very informative. But in our \nsimple case, since we know the exact distribution of the training dataset, we’ll look at \nthe   MSE of the generated samples relative to samples in the training set and use it as a \nmeasure of the performance of the generator. Let’s code that in:\nmse=nn.MSELoss()    \ndef performance(fake_samples):\n    real=1.08**fake_samples[:,0]    \n    mseloss=mse(fake_samples[:,1],real)   \n    return mseloss\nWe’ll stop training the model if the performance of the generator doesn’t improve in, \nsay, 1,000 epochs. Therefore, we define an early stopping class, as we did in chapter 2, \nto decide when to stop training the model.\nListing 3.5    An early stopping class to decide when to stop training\nclass EarlyStop:\n    def __init__(self, patience=1000):    \n        self.patience = patience\n        self.steps = 0\n        self.min_gdif = float('inf')\n    def stop(self, gdif):    \n        if gdif < self.min_gdif:    \n            self.min_gdif = gdif\n            self.steps = 0\n        elif gdif >= self.min_gdif:\n            self.steps += 1\n        if self.steps >= self.patience:    \n            return True\n        else:\n            return False\nstopper=EarlyStop()\nWith that, we have all the components we need to train our GANs, which we’ll do in \nthe next section.\n3.4 Training and using GANs for shape generation\nNow that we have the training data and two networks, we’ll train the model. After that, \nwe’ll discard the discriminator and use the generator to generate data points to form \nan exponential growth curve shape.Uses MSE as the criterion \nto measure performance\nFinds out the true distribution\nCompares the generated \ndistribution with the true \ndistribution and calculates MSE\nSets the default value \nof patience to 1000\nDefines the stop() \nmethod\nIf a new minimum difference between \nthe generated distribution and true \ndistribution is reached, updates the \nvalue of min_gdif.\nStops training if the model stops \nimproving for 1,000 epochs\n 53 Training and using GANs for shape generation\n3.4.1 The training of GANs\nWe first create labels for real samples and fake samples, respectively. Specifically, we’ll \nlabel all real samples as 1s and all fake samples as 0s. During the training process, the \ndiscriminator compares its own predictions with the labels to receive feedback so that \nit can adjust model parameters to make better predictions in the next iteration. \nHere, we define two tensors, real_labels  and fake_labels :\nreal_labels=torch.ones((batch_size,1))\nreal_labels=real_labels.to(device)\nfake_labels=torch.zeros((batch_size,1))\nfake_labels=fake_labels.to(device)\nThe tensor real_labels  is 2D with a shape of (batch_size, 1) —that is, 128 rows \nand 1 column. We use 128 rows because we’ll feed a batch of 128 real samples to the \ndiscriminator network to obtain 128 predictions. Similarly, the tensor fake_labels  \nis 2D with a shape of (batch_size, 1) . We’ll feed a batch of 128 fake samples to the \ndiscriminator network to obtain 128 predictions and compare them with the ground \ntruth: 128 labels of 0s. We move the two tensors to the GPU for fast training if your \ncomputer has a CUDA-enabled GPU.\nTo train the GANs, we define a few functions so that the training loop looks orga -\nnized. The first function, train_D_on_real() , trains the discriminator network with a \nbatch of real samples.\nListing 3.6    Defining a train_D_on_real()  function\ndef train_D_on_real(real_samples):\n    real_samples=real_samples.to(device)\n    optimD.zero_grad()\n    out_D=D(real_samples)    \n    loss_D=loss_fn(out_D,real_labels)    \n    loss_D.backward()\n    optimD.step()    \n    return loss_D\nThe function train_D_on_real()  first moves the real samples to GPU if the com -\nputer has a CUDA-enabled GPU. The discriminator network, D, makes predictions \non the batch of samples. The model then compares the discriminator’s predictions, \nout_D , with the ground truth, real_labels , and calculates the loss of the predictions \naccordingly. The backward()  method calculates the gradients of the loss function with \nrespect to model parameters. The step()  method adjusts the model parameters (that \nis, backpropagation). The zero_grad()  method means that we explicitly set the gradi -\nents to 0 before backpropagation. Otherwise, the accumulated gradients instead of the \nincremental gradients are used on every backward()  call.Makes predictions \non real samples\nCalculates loss\nBackpropagation (i.e., updates \nmodel weights in the discriminator \nnetwork so predictions are more \naccurate in the next iteration)\n54 chapter  3 Generative adversarial networks: Shape and number generation\nTIP    We call the zero_grad()  method before updating model weights when \ntraining each batch of data. We explicitly set the gradients to 0 before backprop -\nagation to use incremental gradients instead of the accumulated gradients on \nevery backward()  call.\nThe second function, train_D_on_fake() , trains the discriminator network with a \nbatch of fake samples.\nListing 3.7    Defining the train_D_on_fake()  function\ndef train_D_on_fake():        \n    noise=torch.randn((batch_size,2))\n    noise=noise.to(device)\n    fake_samples=G(noise)     \n    optimD.zero_grad()\n    out_D=D(fake_samples)    \n    loss_D=loss_fn(out_D,fake_labels)    \n    loss_D.backward()\n    optimD.step()    \n    return loss_D\nThe function train_D_on_fake()  first feeds a batch of random noise vectors from the \nlatent space to the generator to obtain a batch of fake samples. The function then pres -\nents the fake samples to the discriminator to obtain predictions. The function com -\npares the discriminator’s predictions, out_D , with the ground truth, fake_labels , \nand calculates the loss of the predictions accordingly. Finally, it adjusts the model \nparameters based on the gradients of the loss function with respect to model weights.\nNOTE     We use the terms weights  and  parameters  interchangeably. Strictly speak -\ning, model parameters also include bias terms, but we use the term model weights  \nloosely to include model biases. Similarly, we use the terms adjusting weights , \nadjusting parameters , and backpropagation  interchangeably.\nThe third function, train_G() , trains the generator network with a batch of fake \nsamples.\nListing 3.8    Defining the train_G() function\ndef train_G(): \n    noise=torch.randn((batch_size,2))\n    noise=noise.to(device)\n    optimG.zero_grad()\n    fake_samples=G(noise)    \n    out_G=D(fake_samples)    \n    loss_G=loss_fn(out_G,real_labels)    \n    loss_G.backward()Generates a batch \nof fake samples\nMakes predictions \non the fake samples\nCalculates loss\nBackpropagation\nCreates a batch \nof fake samples\nPresents the fake samples \nto the discriminator to \nobtain predictions\nCalculates the loss based on \nwhether G has succeeded\n 55 Training and using GANs for shape generation\n    optimG.step()     \n    return loss_G, fake_samples \nTo train the generator, we first feed a batch of random noise vectors from the latent \nspace to the generator to obtain a batch of fake samples. We then present the fake \nsamples to the discriminator network to obtain a batch of predictions. We compare the \ndiscriminator’s predictions with real_labels , a tensor of 1s, and calculate the loss. \nIt’s important that we use a tensor of 1s, not a tensor of 0s, as the labels, because the \nobjective of the generator is to fool the discriminator into thinking that fake samples \nare real. Finally, we adjust the model parameters based on the gradients of the loss \nfunction with respect to model weights so that in the next iteration, the generator can \ncreate more realistic samples.\nNOTE     We use the tensor real_labels  (a tensor of 1s) instead of fake_labels  \n(a tensor of 0s) when calculating loss and assessing the generator network \nbecause the generator wants the discriminator to predict fake samples as real.\nFinally, we define a function, test_epoch() , which prints out the losses for the dis -\ncriminator and the generator periodically. Further, it plots the data points generated \nby the generator and compares them to those in the training set. The function test_\nepoch()  is shown in the following listing.\nListing 3.9    Defining the test_epoch() function\nimport os\nos.makedirs(""files"", exist_ok=True)    \ndef test_epoch(epoch,gloss,dloss,n,fake_samples):\n    if epoch==0 or (epoch+1)%25==0:\n        g=gloss.item()/n\n        d=dloss.item()/n\n        print(f""at epoch {epoch+1}, G loss: {g}, D loss {d}"")    \n        fake=fake_samples.detach().cpu().numpy()\n        plt.figure(dpi=200)\n        plt.plot(fake[:,0],fake[:,1],""*"",c=""g"",\n            label=""generated samples"")    \n        plt.plot(train_data[:,0],train_data[:,1],""."",c=""r"",\n            alpha=0.1,label=""real samples"")    \n        plt.title(f""epoch {epoch+1}"")\n        plt.xlim(0,50)\n        plt.ylim(0,50)\n        plt.legend()\n        plt.savefig(f""files/p{epoch+1}.png"")\n        plt.show()Backpropagation (i.e., updates \nweights in the generator network \nso the generated samples are more \nrealistic in the next iteration)\nCreates a folder to hold files\nPeriodically \nprints out \nlosses\nPlots the generated \npoints as asterisks (*)\nPlots training \ndata as dots (.)\n56 chapter  3 Generative adversarial networks: Shape and number generation\nAfter every 25 epochs, the function prints out the average losses for the generator and \nthe discriminator in the epoch. Further, it plots a batch of fake data points generated \nby the generator (in asterisks) and compares them to the data points in the training set \n(in dots). The plot is saved as an image in your local folder /files/.\nNow we are ready to train the model. We iterate through all batches in the training \ndataset. For each batch of data, we first train the discriminator using the real samples. \nAfter that, the generator creates a batch of fake samples, and we use them to train the \ndiscriminator again. Finally, we let the generator create a batch of fake samples again, \nbut this time, we use them to train the generator instead. We train the model until the \nearly stopping condition is satisfied, as shown in the following listing.\nListing 3.10    Training GANs to generate an exponential growth curve\nfor epoch in range(10000):    \n    gloss=0\n    dloss=0\n    for n, real_samples in enumerate(train_loader):    \n        loss_D=train_D_on_real(real_samples)\n        dloss+=loss_D\n        loss_D=train_D_on_fake()\n        dloss+=loss_D\n        loss_G,fake_samples=train_G()\n        gloss+=loss_G\n    test_epoch(epoch,gloss,dloss,n,fake_samples)    \n    gdif=performance(fake_samples).item()\n    if stopper.stop(gdif)==True:    \n        break\nThe training takes a few minutes if you are using GPU training. Otherwise, it may take \n20 to 30 minutes, depending on the hardware configuration on your computer. Alter -\nnatively, you can download the trained model from the book’s GitHub repository: \nhttps: //github.com/markhliu/DGAI . \nAfter 25 epochs of training, the generated data are scattered around the point (0,0) \nand don’t form any meaningful shape (an epoch is when all training data is used for \ntraining once). After 200 epochs of training, the data points start to form an exponen -\ntial growth curve shape, even though many points are far away from the dotted curve, \nwhich is formed by points from the training set. After 1,025 epochs, the generated \npoints fit closely with the exponential growth curve. Figure 3.2 provides subplots of the \noutput from six different epochs. Our GANs work really well: the generator is able to \ngenerate data points to form the desired shape.Starts training loops\nIterates through all batches \nin the training dataset\nShows generated \nsamples periodically\nDetermines if training \nshould stop\n 57 Training and using GANs for shape generation\nFigure 3.2    Subplots of the comparison of the generated shape (asterisks in the graph) with the true exponential \ngrowth curve shape (dots in the graph) at different stages of the training process. At epoch 25, the generated \nsamples don’t form any meaningful shape. At epoch 200, the samples start to look like an exponential growth \ncurve shape. At epoch 1025, the generated samples align closely with the exponential growth curve.",21068
30-3.5 Generating numbers with patterns.pdf,30-3.5 Generating numbers with patterns,"58 chapter  3 Generative adversarial networks: Shape and number generation\n3.4.2 Saving and using the trained generator\nNow that the GANs are trained, we’ll discard the discriminator network, as we always \ndo in GANs, and save the trained generator network in the local folder, as follows:\nimport os\nos.makedirs(""files"", exist_ok=True)\nscripted = torch.jit.script(G) \nscripted.save('files/exponential.pt') \nThe torch.jit.script()  method scripts a function or a nn.Module  class as Torch -\nScript code using the TorchScript compiler. We use the method to script our trained \ngenerator network and save it as a file, exponential.pt , on your computer.\nTo use the generator, we don’t even need to define the model. We simply load up the \nsaved file and use it to generate data points as follows:\nnew_G=torch.jit.load('files/exponential.pt',\n                     map_location=device)\nnew_G.eval()\nThe trained generator is now loaded to your device, which is either CPU or CUDA  \ndepending on if you have a CUDA-enabled GPU on your computer. The map_\nlocation=device  argument in torch.jit.load()  specifies where to load the \ngenerator. We can now use the trained generator to generate a batch of data points:\nnoise=torch.randn((batch_size,2)).to(device)\nnew_data=new_G(noise) \nHere, we first obtain a batch of random noise vectors from the latent space. We then \nfeed them to the generator to produce the fake data. We can plot the generated data:\nfig=plt.figure(dpi=100)\nplt.plot(new_data.detach().cpu().numpy()[:,0],\n  new_data.detach().cpu().numpy()[:,1],""*"",c=""g"",\n        label=""generated samples"")    \nplt.plot(train_data[:,0],train_data[:,1],""."",c=""r"",\n         alpha=0.1,label=""real samples"")    \nplt.title(""Inverted-U Shape Generated by GANs"")\nplt.xlim(0,50)\nplt.ylim(0,50)\nplt.legend()\nplt.show()\nYou should see a plot similar to the last subplot in figure 3.2: the generated data sam -\nples closely resemble an exponential growth curve. \nCongratulations! You have created and trained your very first GANs. Armed with this \nskill, you can easily change the code so that the generated data matches other shapes \nsuch as sine, cosine, U-shape, and so on. Plots the generated data \nsamples as asterisks\nPlots the training \ndata as dots",2282
31-3.5.1 What are one-hot variables.pdf,31-3.5.1 What are one-hot variables,"59 Generating numbers with patterns\nExercise 3.3\nModify the programs in the first project so that the generator generates data samples \nto form a sine shape between x = –5 and x = 5. When you plot the data samples, set the \nvalue of y between –1.2 and 1.2. \n3.5 Generating numbers with patterns\nIn this second project, you’ll build and train GANs to generate a sequence of 10 inte -\ngers between 0 and 99, all of them multiples of 5. The main steps involved are similar \nto those to generate an exponential growth curve, with the exception that the training \nset is not data points with two values (x, y). Instead, the training dataset is a sequence of \nintegers that are all multiples of 5 between 0 and 99.\nIn this section, you’ll first learn to convert the training data into a format that neural \nnetworks understand: one-hot variables. Further, you’ll convert one-hot variables back \nto an integer between 0 and 99, so it’s easy for human beings to understand. Hence \nyou are essentially translating data between human-readable and model-ready formats. \nAfter that, you’ll create a discriminator and a generator and train the GANs. You’ll also \nuse early stopping to determine when the training is finished. You then discard the \ndiscriminator and use the trained generator to create a sequence of integers with the \npattern you want.\n3.5.1 What are one-hot variables?\nOne-hot encoding is a technique used in ML and data preprocessing to represent cat -\negorical data as binary vectors. Categorical data consists of categories or labels, such as \ncolors, types of animals, or cities, which are not inherently numeric.  ML algorithms \ntypically work with numerical data, so converting categorical data into a numerical for -\nmat is necessary.\nImagine you are working with a categorical feature—for example, the color of a \nhouse that can take values “red,” “green,” and “blue.” With one-hot encoding, each cat -\negory is represented as a binary vector. You’ll create three binary columns, one for each \ncategory. The color “red” is one-hot encoded as [1, 0, 0], “green” as [0, 1, 0], and “blue” \nas [0, 0, 1]. Doing so preserves the categorical information without introducing any \nordinal relationship between the categories. Each category is treated as independent.\nHere we define a onehot_encoder()  function to convert an integer to a one-hot \nvariable:\nimport torch\ndef onehot_encoder(position,depth):\n    onehot=torch.zeros((depth,))\n    onehot[position]=1\n    return onehot\n60 chapter  3 Generative adversarial networks: Shape and number generation\nThe function takes two arguments: the first argument, position , is the index at which \nthe value is turned on as 1, and the second argument, depth , is the length of the one-\nhot variable. For example, if we print out the value of onehot_encoder(1,5) , it will \nlook like this:\nprint(onehot_encoder(1,5))\nThe result is \ntensor([0., 1., 0., 0., 0.])\nThe result shows a five-value tensor with the second place (the index value of which is \n1) turned on as 1 and the rest turned off as 0s.\nNow that you understand how one-hot encoding works, you can convert any integer \nbetween 0 and 99 to a one-hot variable:\ndef int_to_onehot(number):\n    onehot=onehot_encoder(number,100)\n    return onehot\nLet’s use the function to convert the number 75 to a 100-value tensor:\nonehot75=int_to_onehot(75)\nprint(onehot75)\nThe output is\ntensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n0., 0., 0., 0., 0., 0., 0., 0., 0.])\nThe result is a 100-value tensor with the 76th place (the index value of which is 75) \nturned on as 1 and all other positions turned off as 0s.\nTo function int_to_onehot()  converts an integer into a one-hot variable. In a way, \nthe function is translating human-readable language into model-ready language.\nNext, we want to translate model-ready language back to human-readable language. \nSuppose we have a one-hot variable: How can we convert it into an integer that humans \nunderstand? The following function onehot_to_int()  accomplishes that goal:\ndef onehot_to_int(onehot):\n    num=torch.argmax(onehot)\n    return num.item()\nThe function onehot_to_int()  takes the argument onehot  and converts it into an \ninteger based on which position has the highest value.\nLet’s test the function to see what happens if we use the tensor onehot75  we just cre -\nated as the input:\nprint(onehot_to_int(onehot75))",4772
32-3.5.2 GANs to generate numbers with patterns.pdf,32-3.5.2 GANs to generate numbers with patterns,"61 Generating numbers with patterns\nThe output is\n75\nThe result shows that the function converts the one-hot variable to an integer 75, which \nis the right answer. So we know that the functions are defined properly.\nNext, we’ll build and train GANs to generate multiples of 5.\n3.5.2 GANs to generate numbers with patterns\nOur goal is to build and train a model so that the generator can generate a sequence of \n10 integers, all multiples of 5. We first prepare the training data and then convert them \nto model-ready numbers in batches. Finally, we use the trained generator to generate \nthe patterns we want.\nFor simplicity, we’ll generate a sequence of 10 integers between 0 and 99. We’ll then \nconvert the sequence into 10 model-ready numbers. \nThe following function  generates a sequence of 10 integers, all multiples of 5:\ndef gen_sequence():\n    indices = torch.randint(0, 20, (10,))\n    values = indices*5\n    return values   \nWe first use the randint()  method in PyTorch to generate 10 numbers between 0 and \n19. We then multiply them by 5 and convert them to PyTorch tensors. This creates 10 \nintegers that are all multiples of 5.\nLet’s try to generate a sequence of training data:\nsequence=gen_sequence()\nprint(sequence)\nThe output is \ntensor([60, 95, 50, 55, 25, 40, 70,  5,  0, 55])\nThe values in the preceding output are all multiples of 5. \nNext, we convert each number to a one-hot variable so that we can feed them to the \nneural network later:\nimport numpy as np\ndef gen_batch():\n    sequence=gen_sequence()    \n    batch=[int_to_onehot(i).numpy() for i in sequence]    \n    batch=np.array(batch)\n    return torch.tensor(batch)\nbatch=gen_batch()\nThe preceding function gen_batch()  creates a batch of data so that we can feed them \nto the neural network for training purposes.\nWe also define a function data_to_num()  to convert one-hot variables to a sequence \nof integers so that humans can understand the output:Creates a sequence of 10 \nnumbers, all multiples of 5\nConverts each integer \nto a 100-value one-hot \nvariable\n62 chapter  3 Generative adversarial networks: Shape and number generation\ndef data_to_num(data):\n    num=torch.argmax(data,dim=-1)    \n    return num\nnumbers=data_to_num(batch)    \nThe dim=-1  argument in the torch.argmax()  function means we are trying to find \nthe position (i.e., index) of the largest value in the last dimension: that is, among the \n100-value one-hot vector, which position has the highest value.\nNext, we’ll create two neural networks: one for the discriminator D and one for the \ngenerator G. We’ll build GANs to generate the desired pattern of numbers. Similar to \nwhat we did earlier in this chapter, we create a discriminator network, which is a binary \nclassifier that distinguishes fake samples from real samples. We also create a genera -\ntor network to generate a sequence of 10 numbers. Here is the discriminator neural \nnetwork:\nfrom torch import nn\nD=nn.Sequential(\n    nn.Linear(100,1),\n    nn.Sigmoid()).to(device)\nSince we’ll convert integers into 100-value one-hot variables, we use 100 as the input \nsize in the first Linear  layer in the model. The last Linear  layer has just one output \nfeature in it, and we use the sigmoid activation function to squeeze the output to the \nrange [0, 1] so it can be interpreted as the probability, p, that the sample is real. With \nthe complementary probability 1 – p, the sample is fake.\nThe generator’s job is to create a sequence of numbers so that they can pass as real \nin front of the discriminator D. That is, G is trying to create a sequence of numbers to \nmaximize the probability that D thinks that the numbers are from the training dataset.\nWe create the following neural network to represent the generator G:\nG=nn.Sequential(\n    nn.Linear(100,100),\n    nn.ReLU()).to(device)\nWe’ll feed random noise vectors from a 100-dimensional latent space to the generator. \nThe generator then creates a tensor of 100 values based on the input. Note here that \nwe use the ReLU  activation function at the last layer so that the output is nonnegative. \nSince we are trying to generate 100 values of 0 or 1, nonnegative values are appropriate \nhere. \nAs in the first project, we use the Adam optimizer for both the discriminator and the \ngenerator, with a learning rate of 0.0005:\nloss_fn=nn.BCELoss()\nlr=0.0005\noptimD=torch.optim.Adam(D.parameters(),lr=lr)\noptimG=torch.optim.Adam(G.parameters(),lr=lr)Converts vectors to integers based on \nthe largest values in a 100-value vector\nApplies the function on an example",4622
33-3.5.4 Saving and using the trained model.pdf,33-3.5.4 Saving and using the trained model,"63 Generating numbers with patterns\nNow that we have the training data and two networks, we’ll train the model. After that, \nwe’ll discard the discriminator and use the generator to generate a sequence of 10 \nintegers.\n3.5.3 Training the GANs to generate numbers with patterns\nThe training process for this project is very similar to that in our first project in which \nyou generated an exponential growth shape. \nWe define a function train_D_G() , which is a combination of the three functions \ntrain_D_on_real() , train_D_on_fake() , and train_G()  that we have defined for \nthe first project. The function train_D_G()  is in the Jupyter Notebook for this chapter \nin the book’s GitHub repository: https: //github.com/markhliu/DGAI . Take a look at \nthe function train_D_G()  so you can see what minor changes we have made compared \nto the three functions we defined for the first project. \nWe use the same early stopping class that we defined for the first project so we know \nwhen to stop training. However, we have modified the patience  argument to 800 when \nwe instantiate the class, as shown in the following listing. \nListing 3.11    Training GANs to generate multiples of 5 \nstopper=EarlyStop(800)    \nmse=nn.MSELoss()\nreal_labels=torch.ones((10,1)).to(device)\nfake_labels=torch.zeros((10,1)).to(device)\ndef distance(generated_data):    \n    nums=data_to_num(generated_data)\n    remainders=nums%5\n    ten_zeros=torch.zeros((10,1)).to(device)\n    mseloss=mse(remainders,ten_zeros)\n    return mseloss\nfor i in range(10000):\n    gloss=0\n    dloss=0\n    generated_data=train_D_G(D,G,loss_fn,optimD,optimG)    \n    dis=distance(generated_data)\n    if stopper.stop(dis)==True:\n        break   \n    if i % 50 == 0:\n        print(data_to_num(generated_data))    \nWe have also defined a distance()  function to measure the difference between the \ntraining set and the generated data samples: it calculates the MSE of the remainder of \neach generated number when divided by 5. The measure is 0 when all generated num -\nbers are multiples of 5. \nIf you run the preceding code cell, you’ll see the following output:Creates an instance of \nthe early stopping class\nDefines a distance() function \nto calculate the loss in the \ngenerated numbers\nTrains the GANs \nfor one epoch\nPrints out the generated \nsequence of integers after \nevery 50 epochs\n64 chapter  3 Generative adversarial networks: Shape and number generation\ntensor([14, 34, 19, 89, 44,  5, 58,  6, 41, 87], device='cuda:0')\n…\ntensor([ 0, 80, 65,  0,  0, 10, 80, 75, 75, 75], device='cuda:0')\ntensor([25, 30,  0,  0, 65, 20, 80, 20, 80, 20], device='cuda:0')\ntensor([65, 95, 10, 65, 75, 20, 20, 20, 65, 75], device='cuda:0')\nIn each iteration, we generate a batch of 10 numbers. We first train the discriminator \nD using real samples. After that, the generator creates a batch of fake samples, and \nwe use them to train the discriminator D again. Finally, we let the generator create a \nbatch of fake samples again, but we use them to train the generator G instead. We stop \ntraining if the generator network stops improving after 800 epochs since the last time \nthe minimum loss was achieved. After every 50 epochs, we print out the sequence of 10 \nnumbers created by the generator so you can tell if they are indeed all multiples of 5. \nThe output during the training process is as shown previously. In the first few hun -\ndred epochs, the generator still produces numbers that are not multiples of 5. But after \n900 epochs, all the numbers generated are multiples of 5. The training process takes \njust a minute or so with GPU training. It takes less than 10 minutes if you use CPU train -\ning. Alternatively, you can download the trained model from the book’s GitHub reposi -\ntory: https: //github.com/markhliu/DGAI . \n3.5.4 Saving and using the trained model\nWe’ll discard the discriminator and save the trained generator in the local folder:\nimport os\nos.makedirs(""files"", exist_ok=True)\nscripted = torch.jit.script(G) \nscripted.save('files/num_gen.pt') \nWe have now saved the generator to the local folder. To use the generator, we simply \nload up the model and use it to generate a sequence of integers:\nnew_G=torch.jit.load('files/num_gen.pt',\n                     map_location=device)    \nnew_G.eval()\nnoise=torch.randn((10,100)).to(device)    \nnew_data=new_G(noise)    \nprint(data_to_num(new_data))\nThe output is as follows:\ntensor([40, 25, 65, 25, 20, 25, 95, 10, 10, 65], device='cuda:0')\nThe generated numbers are all multiples of 5. \nYou can easily change the code to generate other patterns, such as odd numbers, \neven numbers, multiples of 3, and so on. Loads the saved generator\nObtains random \nnoise vectors\nFeeds the random noise vectors \nto the trained model to generate \na sequence of integers\n 65 Summary\nExercise 3.4\nModify the programs in the second project so that the generator generates a sequence \nof ten integers that are all multiples of 3.\nNow that you know how GANs work, you’ll be able to extend the idea behind GANs \nto other formats in later chapters, including high-resolution images and realistic-\nsounding music.\nSummary\n¡ GANs consist of two networks: a discriminator to distinguish fake samples from \nreal samples and a generator to create samples that are indistinguishable from \nthose in the training set. \n¡ The steps involved in GANs are preparing training data, creating a discriminator \nand a generator, training the model and deciding when to stop training, and \nfinally, discarding the discriminator and using the trained generator to create \nnew samples. \n¡ The content generated by GANs depends on the training data. When the train -\ning dataset contains data pairs (x, y) that form an exponential growth curve, the \ngenerated samples are also data pairs that mimic such a shape. When the training \ndataset has sequences of numbers that are all multiples of 5, the generated sam -\nples are also sequences of numbers, with multiples of 5 in them. \n¡ GANs are versatile and capable of generating many different formats of content.",6150
34-Part 2.pdf,34-Part 2,"Part 2\nImage generation\nPart II dives deep into image generation. \nIn chapter 4, you’ll learn to build and train generative adversarial networks to \ngenerate high-resolution color images. In particular, you’ll learn to use convo -\nlutional neural networks to capture spatial features in images. You’ll also learn \nto use transposed convolutional layers to upsample and generate high-resolution \nfeature maps in images. In chapter 5, you’ll learn two ways to select characteris -\ntics in the generated images. In chapter 6, you’ll learn to build and train a Cycle -\nGAN to translate images between two domains such as images with black hair and \nimages with blond hair or horse images and zebra images. In chapter 7, you’ll \nlearn to create images using another generative model: autoencoders and their \nvariant, variational autoencoders.",849
35-4.1.2 A generator to create grayscale images.pdf,35-4.1.2 A generator to create grayscale images,"694Image generation  \nwith generative \nadversarial networks \nThis chapter covers\n¡ Designing a generator by mirroring steps in the  \n discriminator network \n¡ How a 2D convolutional operation works on an  \n image\n¡ How a 2D transposed convolutional operation  \n inserts gaps between the output values and  \n generates feature maps of a higher resolution\n¡ Building and training generative adversarial  \n networks to generate grayscale and color images\nYou have successfully generated an exponential growth curve and a sequence of \nintegers that are all multiples of 5 in chapter 3. Now that you understand how gen -\nerative adversarial networks (GANs) work, you are ready to apply the same skills to \ngenerate many other forms of content, such as high-resolution color images and \nrealistic-sounding music. However, this may be easier said than done (you know \nwhat they say: the devil is in the details). For example, exactly how can we make \nthe generator conjure up realistic images out of thin air? That’s the question we’re \ngoing to tackle in this chapter.\n70 chapter  4 Image generation with generative adversarial networks  \nA common approach for the generator to create images from scratch is to mirror \nsteps in the discriminator network. In the first project in this chapter, your goal is to cre -\nate grayscale images of clothing items such as coats, shirts, sandals, and so on. You learn \nto mirror the layers in the discriminator network when designing a generator network. \nIn this project, only dense layers are used in both the generator and discriminator net -\nworks. Each neuron in a dense layer is connected to every neuron in the previous and \nnext layer. For this reason, dense layers are also called fully connected layers. \nIn the second project in this chapter, your goal is to create high-resolution color \nimages of anime faces. Like in the first project, the generator mirrors the steps in the \ndiscriminator network to conjure up images. However, high-resolution color images \nin this project contain many more pixels than the low-resolution grayscale images in \nthe first project. If we use dense layers only, the number of parameters in the model \nincreases enormously. This, in turn, makes learning slow and ineffective. We, there -\nfore, turn to convolutional neural networks (CNNs). In CNNs, each neuron in a layer is \nconnected only to a small region of the input. This local connectivity reduces the num -\nber of parameters, making the network more efficient. CNNs require fewer parameters \nthan fully connected networks of similar size, leading to faster training times and lower \ncomputational costs. CNNs are also generally more effective at capturing spatial hier -\narchies in image data because they treat images as multidimensional objects instead of \n1D vectors.\nTo prepare you for the second project, we’ll show you how convolutional operations \nwork and how they downsample the input images and extract spatial features in them. \nYou’ll also learn concepts such as filter size, stride, and zero-padding and how they \naffect the degree of downsampling in CNNs. While the discriminator network uses con -\nvolutional layers, the generator mirrors these layers by using transposed convolutional \nlayers (also known as deconvolution or upsampling layers). You’ll learn how transposed \nconvolutional layers are used for upsampling to generate high-resolution feature maps. \nTo summarize, you’ll learn how to mirror the steps in the discriminator network to \ncreate images from scratch in this chapter. In addition, you’ll learn how convolutional \nlayers and transposed convolutional layers work. After this chapter, you’ll use convolu -\ntional layers and transposed convolutional layers to create high-resolution images in \nother settings later in this book (such as in feature transfers when training a CycleGAN \nto convert blond hair to black hair or in a variational autoencoder [VAE] to generate \nhigh-resolution human face images).\n4.1 GANs to generate grayscale images of clothing items\nOur goal in the first project is to train a model to generate grayscale images of clothing \nitems such as sandals, t-shirts, coats, and bags. \nWhen you use GANs to generate images, you’ll always start by obtaining training \ndata. You’ll then create a discriminator network from scratch. You’ll mirror steps in \nthe discriminator network when creating a generator network. Finally, you’ll train the \nGANs and use the trained model for image generation. Let’s see how that works with a \nsimple project that creates grayscale images of clothing items. \n 71 GANs to generate grayscale images of clothing items\n4.1.1 Training samples and the discriminator\nThe steps involved with preparing the training data are similar to what we have done in \nchapter 2, with a few exceptions that I’ll highlight later. To save time, I’ll skip the steps \nyou have seen before in chapter 2 and refer you to the book’s GitHub repository. Fol -\nlow the steps in the Jupyter Notebook for this chapter in the book’s GitHub repository \n(https: //github.com/markhliu/DGAI ) so that you create a data iterator with batches. \nThere are 60,000 images in the training set. In chapter 2, we split the training set fur -\nther into a train set and a validation set. We used the loss in the validation set to deter -\nmine whether the parameters had converged so that we could stop training. However, \nGANs are trained using a different approach compared to traditional supervised learn -\ning models (such as the classification models you have seen in chapter 2). Since the \nquality of the generated samples improves throughout training, the discriminator’s task \nbecomes more and more difficult. The loss from the discriminator network is not a good \nindicator of the quality of the model. The usual way of measuring the performance of \nGANs is through visual inspection to assess the quality and realism of generated images. \nWe can potentially compare the quality of generated samples with training samples \nand use methods such as the Inception Score to evaluate the performance of GANs \n(See, for example, “Pros and Cons of GAN Evaluation Measures,” by Ali Borji, 2018, \nfor a survey on various GAN evaluation methods; https: //arxiv.org/abs/1802.03446 ). \nHowever, researchers have documented the weaknesses of these measures (“A Note on \nthe Inception Score,” by Shane Barratt and Rishi Sharma, 2018, demonstrates that the \ninception score fails to provide useful guidance when comparing models; https: //arxiv  \n.org/abs/1801.01973 ). In this chapter, we’ll use visual inspections to check the quality \nof generated samples periodically and determine when to stop training.\nThe discriminator network is a binary classifier, which is similar to the binary classi -\nfier for clothing items we discussed in chapter 2. Here the discriminator’s job is to clas -\nsify the samples into either real or fake. \nWe use PyTorch to create the following discriminator neural network D:\nimport torch\nimport torch.nn as nn\ndevice=""cuda"" if torch.cuda.is_available() else ""cpu""\nD=nn.Sequential(\n    nn.Linear(784, 1024),    \n    nn.ReLU(),\n    nn.Dropout(0.3),\n    nn.Linear(1024, 512),\n    nn.ReLU(),\n    nn.Dropout(0.3),\n    nn.Linear(512, 256),\n    nn.ReLU(),\n    nn.Dropout(0.3),\n    nn.Linear(256, 1),    \n    nn.Sigmoid()).to(device)The first fully \nconnected layer has \n784 inputs and 1,024 \noutputs.\nThe last fully connected \nlayer has 256 inputs \nand 1 output.\n72 chapter  4 Image generation with generative adversarial networks  \nThe input size is 784 because each grayscale image has a size of 28 × 28 pixels in the \ntraining set. Because dense layers take only 1D inputs, we flatten the images before \nfeeding them to the model. The output layer has just one neuron in it: the output of \nthe discriminator D is a single value. We use the sigmoid activation function to squeeze \nthe output to the range [0, 1] so that it can be interpreted as the probability, p, that the \nsample is real. With complementary probability 1 – p, the sample is fake.\nExercise 4.1\nModify the discriminator D so that the numbers of outputs in the first three layers are \n1,000, 500, and 200 instead of 1,024, 512, and 256. Make sure the number of outputs \nin a layer matches the number of inputs in the next layer. \n4.1.2 A generator to create grayscale images\nWhile the discriminator network is fairly easy to create, how to create a generator so \nthat it can conjure up realistic images is a different matter. A common approach is to \nmirror the layers used in the discriminator network to create a generator, as shown in \nthe following listing.\nListing 4.1    Designing a generator by mirroring layers in the discriminator\nG=nn.Sequential(\n    nn.Linear(100, 256),    \n    nn.ReLU(),\n    nn.Linear(256, 512),    \n    nn.ReLU(),\n    nn.Linear(512, 1024),    \n    nn.ReLU(),\n    nn.Linear(1024, 784),    \n    nn.Tanh()).to(device)    \nFigure 4.1 provides a diagram of the architecture of generator and discriminator \nnetworks in the GAN to generate grayscale images of clothing items. As shown in \nthe top right corner of figure 4.1, a flattened grayscale image from the training set, \nwhich contains 28 × 28 = 784 pixels, goes through four dense layers sequentially in The first layer in the generator is \nsymmetric to the last layer in the \ndiscriminator.\nThe second layer in the generator is \nsymmetric to the second to last layer in the \ndiscriminator (numbers of inputs and \noutputs have switched positions).\nThe third layer in the generator is \nsymmetric to the third to last \nlayer in the discriminator.\nThe last layer in the generator is \nsymmetric to the first layer in the \ndiscriminator.\nUses Tanh() activation so the \noutput is between – 1 and 1, the \nsame as values in images\n 73 GANs to generate grayscale images of clothing items\nthe discriminator network, and the output is the probability that the image is real. To \ncreate an image, the generator uses the same four dense layers but in reverse order: it \nobtains a 100-value random noise vector from the latent space (bottom left in figure \n4.1) and feeds the vector through the four dense layers. In each layer, the numbers of \ninputs  and outputs  in the discriminator are reversed and used as the numbers of outputs  \nand inputs  in the generator. Finally, the generator comes up with a 784-value tensor, \nwhich can be reshaped into a 28 × 28 grayscale image (top left).  \nDense layer\n(100, 256)\nRandom noise \nvector (100 values)Dense layer\n(256,512)Dense layer\n(512,1024)Dense layer\n(1024,784)Fake\nDense layer\n(256,1)\nProb(real)Dense layer\n(512,256)Dense layer\n(1024,512)Dense layer\n(784,1024)GeneratorDiscriminatorReal samples\nFigure 4.1    Designing a generator network to create clothing items by mirroring the layers in the \ndiscriminator network. The right side of the diagram shows the discriminator network, which contains \nfour dense layers. To design a generator that can conjure up clothing items from thin air, we mirror the \nlayers in the discriminator network. Specifically, as shown on the left half of the figure, the generator has \nfour similar dense layers in it but in reverse order: the first layer in the generator mirrors the last layer in \nthe discriminator, the second layer in the generator mirrors the second to last layer in the discriminator, \nand so on. Further, in each of the top three layers, the numbers of inputs and outputs in the discriminator \nare reversed and used as the numbers of outputs and inputs in the generator. \nThe left side of figure 4.1 is the generator network, while the right side is the discrimi -\nnator network. If you compare the two networks, you’ll notice how the generator mir -\nrors the layers used in the discriminator. Specifically, the generator has four similar \ndense layers in it but in reverse order: the first layer in the generator mirrors the last \nlayer in the discriminator, the second layer in the generator mirrors the second to last \nlayer in the discriminator, and so on. The number of outputs of the generator is 784, \nwith values between -1 and 1 after the Tanh()  activation, and this matches the input to \nthe discriminator network.",12355
36-4.1.3 Training GANs to generate images of clothing items.pdf,36-4.1.3 Training GANs to generate images of clothing items,"74 chapter  4 Image generation with generative adversarial networks  \nExercise 4.2\nModify the generator G so that the numbers of outputs in the first three layers are 1,000, \n500, and 200 instead of 1,024, 512, and 256. Make sure that the modified generator \nmirrors the layers used in the modified discriminator in exercise 4.1.\nAs in GAN models we have seen in chapter 3, the loss function is the binary cross-\nentropy loss since the discriminator D is performing a binary classification problem. \nWe’ll use the Adam optimizer for both the discriminator and the generator, with a \nlearning rate of 0.0001:\nloss_fn=nn.BCELoss()\nlr=0.0001\noptimD=torch.optim.Adam(D.parameters(),lr=lr)\noptimG=torch.optim.Adam(G.parameters(),lr=lr)  \nNext, we’ll train the GANs we just created by using the clothing item images in the \ntraining dataset. \n4.1.3 Training GANs to generate images of clothing items\nThe training process is similar to what we have done in chapter 3 when training GANs \nto generate an exponential growth curve or to generate a sequence of numbers that \nare all multiples of 5. \nUnlike in chapter 3, we’ll solely rely on visual inspections to determine whether the \nmodel is well-trained. For that purpose, we define a see_output()  function to visualize \nthe fake images created by the generator periodically.\nNOTE    Interested readers can check this GitHub repository to learn how to \nimplement the inception score in PyTorch to evaluate GANs: https: //github  \n.com/sbarratt/inception-score-pytorch . However, the repository doesn’t rec -\nommend using the inception score to evaluate generative models due to its \nineffectiveness.\nListing 4.2    Defining a function to visualize the generated clothing items\nimport matplotlib.pyplot as plt\ndef see_output():\n    noise=torch.randn(32,100).to(device=device)\n    fake_samples=G(noise).cpu().detach()    \n    plt.figure(dpi=100,figsize=(20,10))\n    for i in range(32):\n        ax=plt.subplot(4, 8, i + 1)    \n        img=(fake_samples[i]/2+0.5).reshape(28, 28)\n        plt.imshow(img)    Generates 32 fake images\nPlots them in a 4 × 8 grid\nShows the ith image\n 75 GANs to generate grayscale images of clothing items\n        plt.xticks([])\n        plt.yticks([])\n    plt.show()\n    \nsee_output()    \nIf you run the preceding code cell, you’ll see 32 images that look like snowflake stat -\nics on a TV screen, as shown in figure 4.2. They don’t look like clothing items at all \nbecause we haven’t trained the generator yet. \nFigure 4.2    Output from the GAN model to generate clothing items before training. Since the model is \nnot trained, the generated images are nothing like the images in the training set. \nTo train the GAN model, we define a few functions: train_D_on_real() , train_D_\non_fake() , and train_G() . They are similar to those defined in chapter 3. Go to \nthe Jupypter Notebook for this chapter in the book’s GitHub repository and see what \nminor modifications we have made. \nNow we are ready to train the model. We iterate through all batches in the training \ndataset. For each batch of data, we first train the discriminator using the real samples. \nAfter that, the generator creates a batch of fake samples, and we use them to train the \ndiscriminator again. Finally, we let the generator create a batch of fake samples again, \nbut this time, we use them to train the generator instead. We train the model for 50 \nepochs, as shown in the following listing.\nListing 4.3    Training GANs for clothing item generation\nfor i in range(50):    \n    gloss=0\n    dloss=0\n    for n, (real_samples,_) in enumerate(train_loader):Calls the see_output() function \nto visualize the generated \nimages before training\n76 chapter  4 Image generation with generative adversarial networks  \n        loss_D=train_D_on_real(real_samples)    \n        dloss+=loss_D\n        loss_D=train_D_on_fake()    \n        dloss+=loss_D\n        loss_G=train_G()    \n        gloss+=loss_G\n    gloss=gloss/n\n    dloss=dloss/n    \n    if i % 10 == 9:\n        print(f""at epoch {i+1}, dloss: {dloss}, gloss {gloss}"")\n        see_output()    \nThe training takes about 10 minutes if you are using GPU training. Otherwise, it may \ntake an hour or so, depending on the hardware configuration on your computer. Or \nyou can download the trained model from my website: https: //gattonweb.uky.edu/\nfaculty/lium/gai/fashion_gen.zip . Unzip it after downloading.\nAfter every 10 epochs of training, you can visualize the generated clothing items, as \nshown in figure 4.3. After just 10 epochs of training, the model can already generate \nclothing items that clearly can pass as real: you can tell what they are. The first three \nitems in the first row in figure 4.3 are clearly a coat, a dress, and a pair of trousers, for \nexample. As training progresses, the quality of the generated images becomes better \nand better.\nFigure 4.3    Clothing items generated by an image GAN model after 10 epochs of training\nAs we do in all GANs, we discard the discriminator and save the trained generator to \ngenerate samples later:Trains the discriminator \nusing real samples\nTrains the discriminator \nusing fake samples\nTrains the generator\nVisualizes generated samples \nafter every 10 epochs\n 77 GANs to generate grayscale images of clothing items\nscripted = torch.jit.script(G) \nscripted.save('files/fashion_gen.pt') \nWe have now saved the generator in the local folder. To use the generator, we load up \nthe model:\nnew_G=torch.jit.load('files/fashion_gen.pt',\n                     map_location=device)\nnew_G.eval()\nThe generator is now loaded. We can use it to generate clothing items:\nnoise=torch.randn(32,100).to(device=device)\nfake_samples=new_G(noise).cpu().detach()\nfor i in range(32):\n    ax = plt.subplot(4, 8, i + 1)\n    plt.imshow((fake_samples[i]/2+0.5).reshape(28, 28))\n    plt.xticks([])\n    plt.yticks([])\nplt.subplots_adjust(hspace=-0.6)\nplt.show() \nThe generated clothing items are shown in figure 4.4. As you can see, the clothing \nitems are fairly close to those in the training set.\nFigure 4.4    Clothing items generated by a trained image GAN model (after 50 epochs)\nNow that you have learned how to create grayscale images by using GANs, you’ll learn \nhow to generate high-resolution color images by using deep convolutional GAN \n(DCGAN) in the remaining sections of this chapter.",6443
37-4.2 Convolutional layers.pdf,37-4.2 Convolutional layers,,0
38-4.2.1 How do convolutional operations work.pdf,38-4.2.1 How do convolutional operations work,"78 chapter  4 Image generation with generative adversarial networks  \n4.2 Convolutional layers \nTo create high-resolution color images, we need more sophisticated techniques than \nsimple fully connected neural networks. Specifically, we’ll use CNNs, which are partic -\nularly effective for processing data with a grid-like topology, such as images. They are \ndistinct from fully connected (dense) layers in a couple of ways. First, in CNNs, each \nneuron in a layer is connected only to a small region of the input. This is based on the \nunderstanding that in image data, local groups of pixels are more likely to be related \nto each other. This local connectivity reduces the number of parameters, making the \nnetwork more efficient. Second, CNNs use the concept of shared weights—the same \nweights are used across different regions of the input. This is akin to sliding a filter \nacross the entire input space. This filter detects specific features (e.g., edges or tex -\ntures) regardless of their position in the input, leading to the property of translation \ninvariance. \nDue to their structure, CNNs are more efficient for image processing. They require \nfewer parameters than fully connected networks of similar size, leading to faster train -\ning times and lower computational costs. They are also generally more effective at cap -\nturing spatial hierarchies in image data. \nConvolutional layers and transposed convolutional layers are two fundamental \nbuilding blocks in CNNs, commonly used in image processing and computer vision \ntasks. They have different purposes and characteristics: convolutional layers are used \nfor feature extraction. They apply a set of learnable filters (also known as kernels) to \nthe input data to detect patterns and features at different spatial scales. These layers \nare essential for capturing hierarchical representations of the input data. In contrast, \ntransposed convolutional layers are used for upsampling or generating high-resolution \nfeature maps.\nIn this section, you’ll learn how convolutional operations work and how kernel size, \nstride, and zero-padding affect convolutional operations. \n4.2.1 How do convolutional operations work?\nConvolutional layers use filters to extract spatial patterns on the input data. A convolu -\ntional layer is capable of automatically detecting a large number of patterns and asso -\nciating them with the target label. Therefore, convolutional layers are commonly used \nin image classification tasks. \nConvolutional operations involve applying a filter to an input image to produce a \nfeature map. This process involves using element-wise multiplication of the filter with \nthe input image and summing the results. The weights in the filter are the same as the \nfilter moves on the input image to scan different areas. Figure 4.5 shows a numerical \nexample of how convolutional operations work. The left column is the input image, \nand the second column is a filter (a 2 × 2 matrix). Convolutional operations (the third \ncolumn) involve sliding the filter over the input image, multiplying corresponding ele -\nments, and summing them up (the last column).\n 79 Convolutional layers \n12\n34111\n012\n876Input matrix FilterConvolutional\noperation \n1x1+ 1x2+\n0x3+ 1x4Result\n7\n12\n34111\n012\n8761x1+ 1x2+\n1x3+ 2x471 4\n12\n34111\n012\n8760x1+ 1x2+\n8x3+ 7x471 4\n54\n12\n34111\n012\n8761x1+ 2x2+\n7x3+ 6x471 4\n54 50\nFigure 4.5    A numerical example of how convolutional operations work, with stride equal to 1 and no \npadding\nTo gain a deep understanding of exactly how convolutional operations work, let’s \nimplement the convolutional operations in PyTorch in parallel so that you can verify \nthe numbers as shown in figure 4.5. First, let’s create a PyTorch tensor to represent the \ninput image in the figure:\nimg = torch.Tensor([[1,1,1],\n                    [0,1,2],\n                    [8,7,6]]).reshape(1,1,3,3)    \nThe image is reshaped so that it has a dimension of (1, 1, 3, 3), indicating that there \nis just one observation in the batch, and the image has just one color channel. The \nheight and the width of the image are both 3 pixels.The four values in the shape \nof the image, ( 1, 1, 3, 3), are \nthe number of images in the \nbatch, number of color \nchannels, image height, and \nimage width, respectively.\n80 chapter  4 Image generation with generative adversarial networks  \nLet’s represent the 2 × 2 filter, as shown in the second column of figure 4.5, by creat -\ning a 2D convolutional layer in PyTorch:\nconv=nn.Conv2d(in_channels=1,\n            out_channels=1,\n            kernel_size=2, \n            stride=1)    \nsd=conv.state_dict()    \nprint(sd)\nA 2D convolutional layer takes several arguments. The in_channels  argument is the \nnumber of channels in the input image. This value is 1 for grayscale images and 3 \nfor color images since color images have three color channels (red, green, and blue \n[RGB]). The out_channels  is the number of channels after the convolutional layer, \nwhich can take any number based on how many features you want to extract from \nthe image. The kernel_size  argument controls the size of the kernel; for example, \nkernel_size=3  means the filter has a shape of 3 × 3, and kernel_size=4  means the \nfilter has a shape of 4 × 4. We set the kernel size to 2 so the filter has a shape of 2 × 2.  \nA 2D convolutional layer also has several optional arguments. The stride  argument \nspecifies how many pixels to move to the right or down each time the filter moves along \nthe input image. The stride  argument has a default value of 1. A higher value of stride \nleads to more downsampling of the image. The padding  argument means how many \nrows of zeros to add to four sides of the input image, with a default value of 0. The bias  \nargument indicates whether to add a learnable bias as the parameter, with a default \nvalue of True .\nThe preceding 2D convolutional layer has one input channel, one output channel, \nwith a kernel size of 2 × 2, and a stride of 1. When the convolutional layer is created, the \nweights and the bias in it are randomly initialized. You will see the following output as \nthe weights and the bias of this convolutional layer:\nOrderedDict([('weight', tensor([[[[ 0.3823,  0.4150],\n          [-0.1171,  0.4593]]]])), ('bias', tensor([-0.1096]))])\nTo make our example easier to follow, we’ll replace the weights and the bias with whole \nnumbers:\nweights={'weight':torch.tensor([[[[1,2],\n   [3,4]]]]), 'bias':torch.tensor([0])}    \nfor k in sd:\n    with torch.no_grad():\n        sd[k].copy_(weights[k])    \nprint(conv.state_dict())    \nSince we are not learning the parameters in the convolutional layer, torch.no_\ngrad()  is used to disable gradient calculation, which reduces memory consumption Initiates a 2D \nconvolutional layer\nExtracts the randomly initialized \nweights and bias in the layer\nHandpicks weights and bias\nReplaces the weights and bias \nin the convolutional layer \nwith our handpicked numbers\nPrints out the new weights and \nbias in the convolutional layer\n 81 Convolutional layers \nand speeds up computations. Now the convolutional layer has weights and the bias \nthat we have chosen. They also match the numbers in figure 4.5. The output from the \npreceding code cell is:\nOrderedDict([('weight', tensor([[[[1., 2.],\n          [3., 4.]]]])), ('bias', tensor([0.]))])\nIf we apply the preceding convolutional layer on the 3 × 3 image we mentioned, what is \nthe output? Let’s find out:\noutput = conv(img)\nprint(output)\nThe output is\ntensor([[[[ 7., 14.],\n          [54., 50.]]]], grad_fn=<ConvolutionBackward0>)\nThe output has a shape of (1, 1, 2, 2), with four values in it: 7, 14, 54, and 50. These \nnumbers match those in figure 4.5. \nBut how exactly does the convolutional layer generate this output through the filter? \nWe’ll explain in detail next.\nThe input image is a 3 × 3 matrix, and the filter is a 2 × 2 matrix. When the filter scans \nover the image, it first covers the four pixels in the top left corner of the image, which \nhave values [[1, 1], [0, 1]] , as shown in the first row in Figure 4.5. The filter has \nvalues [[1,2],[3,4]] . The convolution operation finds the sum of the element-wise \nmultiplication of the two tensors (in this case, one tensor is the filter and the other is \nthe covered area). In other words, the convolution operation performs element-wise \nmultiplication in each of the four cells and then adds up the values in the four cells. \nTherefore, the output from scanning the top left corner is\n1 × 1 × 1 × 2 + 0 × 3 + 1 × 4 = 7. \nThis explains why the top left corner of the output has a value of 7. Similarly, when the \nfilter is applied to the top right corner of the image, the covered area is [[1,1],[1,2]] . \nThe output is therefore:\n1 × 1 + 1 × 2 + 1 × 3 + 2 × 4 = 14. \nThis explains why the top right corner of the output has a value of 14. \nExercise 4.3\nWhat are the values in the covered area when the filter is applied to the bottom right \ncorner of the image? Explain why the bottom right corner of the output has a value of 50.",9200
39-4.3 Transposed convolution and batch normalization.pdf,39-4.3 Transposed convolution and batch normalization,"82 chapter  4 Image generation with generative adversarial networks  \n4.2.2 How do stride and padding affect convolutional operations?\nStride and zero padding are two important concepts in the context of convolutional \noperations. They play a crucial role in determining the dimensions of the output fea -\nture map and the way the filter interacts with the input data.\nStride refers to the number of pixels by which the filter moves across the input image. \nWhen the stride is 1, the filter moves 1 pixel at a time. A larger stride means the filter \njumps over more pixels as it slides over the image. Increasing the stride reduces the spa -\ntial dimensions of the output feature map.\nZero padding involves adding layers of zeros around the border of the input image \nbefore applying the convolutional operation. Zero padding allows control over the spa -\ntial dimensions of the output feature map. Without padding, the dimensions of the out -\nput will be smaller than the input. By adding padding, you can preserve the dimensions \nof the input.\nLet’s use an example to show how stride and padding work. The following code cell \nredefines the 2D convolutional layer:\nconv=nn.Conv2d(in_channels=1,\n            out_channels=1,\n            kernel_size=2, \n            stride=2,    \n            padding=1)    \nsd=conv.state_dict()\nfor k in sd:\n    with torch.no_grad():\n        sd[k].copy_(weights[k])\noutput = conv(img)\nprint(output)\nThe output is\ntensor([[[[ 4.,  7.],\n          [32., 50.]]]], grad_fn=<ConvolutionBackward0>)\nThe padding=1  argument adds one row of 0s around the input image, so the padded \nimage now has a size of 5 × 5 instead of 3 × 3. \nWhen the filter scans over the padded image, it first covers the top left corner, which \nhas values [[0, 0], [0, 1]] . The filter has values [[1,2],[3,4]] . Therefore, the \noutput from scanning the top left corner is:\n0 × 1+0 × 2+0 × 3+1 × 4=4\nThis explains why the top left corner of the output has a value of 4. Similarly, when the \nfilter slides two pixels down to the bottom left corner of the image, the covered area is \n[[0,0],[0,8]] . The output is therefore:\n0 × 1+0 × 2+0 × 3+8 × 4=32\nThis explains why the bottom left corner of the output has a value of 32. Changes the stride \nfrom 1 to 2\nChanges the padding \nfrom 0 to 1",2330
40-4.3.1 How do transposed convolutional layers work.pdf,40-4.3.1 How do transposed convolutional layers work,"83 Transposed convolution and batch normalization\n4.3 Transposed convolution and batch normalization\nTransposed convolutional layers are also known as deconvolution or upsampling Lay -\ners. They are used for upsampling or generating high-resolution feature maps. They \nare often employed in generative models like GANs and VAEs.\nTransposed convolutional layers apply a filter to the input data, but unlike standard \nconvolution, they increase the spatial dimensions by inserting gaps between the output \nvalues, which effectively “upscales” the feature maps. This process generates feature \nmaps of a higher resolution. Transposed convolutional layers help increase the spatial \nresolution, which is useful in image generation.\nStrides can be used in transposed convolution layers to control the amount of upsam -\npling. The greater the value of the stride, the more upsampling the transposed convolu -\ntion layer has on the input data. \nTwo-dimensional batch normalization is a technique used in neural networks, par -\nticularly CNNs, to stabilize and speed up the training process. It addresses several prob -\nlems, including saturation, vanishing gradients, and exploding gradients, which are \ncommon challenges in deep learning. In this section, you’ll look at some examples so \nyou have a deeper understanding of how it works. You’ll use it when creating GANs to \ngenerate high-resolution color images in the next section.\nVanishing and exploding gradients in deep learning\nThe vanishing gradient problem occurs in deep neural networks when the gradients \nof the loss function with respect to the network parameters become exceedingly small \nduring backpropagation. This results in very slow updates to the parameters, hindering \nthe learning process, especially in the early layers of the network. Conversely, the explod -\ning gradient problem happens when these gradients become excessively large, leading \nto unstable updates and causing the model parameters to oscillate or diverge to very \nlarge values. Both problems impede the effective training of deep neural networks. \n4.3.1 How do transposed convolutional layers work?\nContrary to convolutional layers, transposed convolutional layers upsample and fill in \ngaps in an image to generate features and increase resolution by using kernels (i.e., \nfilters). The output is usually larger than the input in a transposed convolutional layer. \nTherefore, transposed convolutional layers are essential tools when it comes to gener -\nating high-resolution images. To show you exactly how 2D transposed convolutional \noperations work, let’s use a simple example and a figure. Suppose you have a very small \n2 × 2 input image, as shown in the left column in figure 4.6.\n84 chapter  4 Image generation with generative adversarial networks  \n23\n45 23\n451x2 1x3\n1x4 1x5Input matrix FilterTransposed\nconvolutional\noperation \n Result\n10\n23\n2300\n4500\n4669\n8 10 12 1523\n450x2 0x3\n0x4 0x510\n23\n23\n452x2 2x3\n2x4 2x510\n23\n23\n453x2 3x3\n3x4 3x510\n23\nFigure 4.6    A numerical example of how transposed convolutional operations work\nThe input image has the following values in it:\nimg = torch.Tensor([[1,0],\n                    [2,3]]).reshape(1,1,2,2)\nYou want to upsample the image so that it has a higher resolution. You can create a 2D \ntransposed convolutional layer in PyTorch:\ntransconv=nn.ConvTranspose2d(in_channels=1,\n            out_channels=1,\n            kernel_size=2, \n            stride=2)    \nsd=transconv.state_dict()\nweights={'weight':torch.tensor([[[[2,3],\n   [4,5]]]]), 'bias':torch.tensor([0])}\nfor k in sd:\n    with torch.no_grad():\n        sd[k].copy_(weights[k])    \nThis 2D transposed convolutional layer has one input channel, one output channel, \nwith a kernel size of 2 × 2 and a stride of 2. The 2 × 2 filter is shown in the second col -\numn in figure 4.6. We replaced the randomly initialized weights and the bias in the \nlayer with our handpicked whole numbers so it’s easy to follow the calculations. The A transposed convolutional \nlayer with one input channel, \none output channel, a kernel \nsize of 2, and a stride of 2\nReplaces the weights and bias \nin the transposed \nconvolutional layer with \nhandpicked values",4276
41-4.3.2 Batch normalization.pdf,41-4.3.2 Batch normalization,"85 Transposed convolution and batch normalization\nstate_dict()  method in the preceding code listing returns the parameters in a deep \nneural network. \nWhen the transposed convolutional layer is applied to the 2 × 2 image we mentioned \nearlier, what is the output? Let’s find out:\ntransoutput = transconv(img)\nprint(transoutput)\nThe output is\ntensor([[[[ 2.,  3.,  0.,  0.],\n          [ 4.,  5.,  0.,  0.],\n          [ 4.,  6.,  6.,  9.],\n          [ 8., 10., 12., 15.]]]], grad_fn=<ConvolutionBackward0>)\nThe output has a shape of (1, 1, 4, 4), meaning we have upsampled a 2 × 2 image to a \n4 × 4 image. How does the transposed convolutional layer generate the preceding out -\nput through the filter? We’ll explain in detail next.\nThe image is a 2 × 2 matrix, and the filter is also a 2 × 2 matrix. When the filter is \napplied to the image, each element in the image multiplies with the filter and goes to \nthe output. The top left value in the image is 1, and we multiply it with the values in the \nfilter, [[2, 3], [4, 5]] , and this leads to the four values in the top left block of the \noutput matrix transoutput , with values [[2, 3], [4, 5]] , as shown at the top right \ncorner in figure 4.6. Similarly, the bottom left value in the image is 2, and we multiply it \nwith the values in the filter, [[2, 3], [4, 5]] , and this leads to the four values in the \nbottom left block of the output matrix transoutput , [[4, 6], [8, 10]] . \nExercise 4.4\nIf an image has values [[10, 10], [15, 20]] in it, what is the output after you apply \nthe 2D transposed convolutional layer transconv  to the image? Assume transconv  \nhas values [[2, 3], [4, 5]]  in it. Assume a kernel size of 2 and a stride size of 2. \n4.3.2 Batch normalization\nTwo-dimensional batch normalization is a standard technique in modern deep learn -\ning frameworks and has become a crucial component for effectively training deep neu -\nral networks. You’ll see it quite often later in this book.\nIn 2D batch normalization, normalization is performed independently for each fea -\nture channel by adjusting and scaling values in the channel so they have a mean of 0 and \na variance of 1. A feature channel refers to one of the dimensions in a multidimensional \ntensor in CNNs used to represent different aspects or features of the input data. For \nexample, they can represent color channels like red, green, or blue. The normaliza -\ntion ensures that the distribution of the inputs to layers deep in the network remains \nmore stable during training. This stability arises because the normalization process \nreduces the internal covariate shift, which is the change in the distribution of network \n86 chapter  4 Image generation with generative adversarial networks  \nactivations due to the update of weights in lower layers. It also helps to address the van -\nishing or exploding gradient problems by keeping the inputs in an appropriate range \nto prevent gradients from becoming too small (vanishing) or too large (exploding).1 \nHere’s how the 2D batch normalization works: for each feature channel, we first cal -\nculate the mean and variance of all observations within the channel. We then normalize \nthe values for each feature channel using the mean and variance obtained earlier (by \nsubtracting the mean from each observation and then dividing the difference by the \nstandard deviation). This ensures that the values in each channel have a mean of 0 and \na standard deviation of 1 after normalization, which helps stabilize and speed up train -\ning. It also helps maintain stable gradients during backpropagation, which further aids \nin training deep neural networks.\nLet’s use a concrete example to show how the 2D batch normalization works. \nSuppose that you have a three-channel input with a size of 64 × 64. You pass the input \nthrough a 2D convolutional layer with three output channels as follows:\ntorch.manual_seed(42)    \nimg = torch.rand(1,3,64,64)    \nconv = nn.Conv2d(in_channels=3,\n            out_channels=3,\n            kernel_size=3, \n            stride=1,\n            padding=1)    \nout=conv(img)    \nprint(out.shape)\nThe output from the preceding code cell is\ntorch.Size([1, 3, 64, 64])\nWe have created a three-channel input and passed it through a 2D convolutional layer \nwith three output channels. The processed input has three channels with a size of 64 × \n64 pixels. \nLet’s look at the mean and standard deviation of the pixels in each of the three out -\nput channels:\nfor i in range(3):\n    print(f""mean in channel {i} is"", out[:,i,:,:].mean().item())\n    print(f""std in channel {i} is"", out[:,i,:,:].std().item())\nThe output is\nmean in channel 0 is -0.3766776919364929\nstd in channel 0 is 0.17841289937496185\nmean in channel 1 is -0.3910464942455292\nstd in channel 1 is 0.16061744093894958\nmean in channel 2 is 0.39275866746902466\nstd in channel 2 is 0.18207983672618866\n1 Sergey Ioffe, Christian Szegedy, 2015, “Batch Normalization: Accelerating Deep Network Training by Reducing In -\nternal Covariate Shift.” https: //arxiv.org/abs/1502.03167 .Fixes the random state so \nresults are reproducible\nCreates a 3-channel input\nCreates a 2D \nconvolutional layer\nPasses the input through \nthe convolutional layer",5289
42-4.4 Color images of anime faces.pdf,42-4.4 Color images of anime faces,,0
43-4.5.1 Building a DCGAN.pdf,43-4.5.1 Building a DCGAN,"87 Color images of anime faces\nThe average values of the pixels in each output channel are not 0; the standard \ndeviations of pixels in each output channel are not 1. Now, we perform a 2D batch \nnormalization:\nnorm=nn.BatchNorm2d(3)\nout2=norm(out)\nprint(out2.shape)\nfor i in range(3):\n    print(f""mean in channel {i} is"", out2[:,i,:,:].mean().item())\n    print(f""std in channel {i} is"", out2[:,i,:,:].std().item())\nThen we have the following output:\ntorch.Size([1, 3, 64, 64])\nmean in channel 0 is 6.984919309616089e-09\nstd in channel 0 is 0.9999650120735168\nmean in channel 1 is -5.3085386753082275e-08\nstd in channel 1 is 0.9999282956123352\nmean in channel 2 is 9.872019290924072e-08\nstd in channel 2 is 0.9999712705612183\nThe average values of pixels in each output channel are now practically 0 (or a very \nsmall number that is close to 0); the standard deviations of pixels in each output chan -\nnel are now a number close to 1. That’s what batch normalization does: it normalizes \nobservations in each feature channel so that values in each feature channel have 0 \nmean and unit standard deviation. \n4.4 Color images of anime faces\nIn this second project, you’ll learn how to create high-resolution color images. The \ntraining steps in this project are similar to the first project, with the exception that the \ntraining data are color images of anime faces. Further, the discriminator and genera -\ntor neural networks are more sophisticated. We’ll use 2D convolutional and 2D trans -\nposed convolutional layers in the two networks.\n4.4.1 Downloading anime faces\nYou can download the training data from Kaggle https: //mng.bz/1a9R , which contains \n63,632 color images of anime faces. You need to set up a free Kaggle account to log in \nfirst. Extract the data from the zip file and put them in a folder on your computer. For \nexample, I placed everything in the zip file in /files/anime/ on my computer. As a \nresult, all anime face images are in /files/anime/images/. \nDefine the path name so you can use it later to load the images in Pytorch:\nanime_path = r""files/anime""\nChange the name of the path depending on where you have saved the images on your \ncomputer. Note that the ImageFolder() class uses the directory name of the images \nto identify the class the images belong to. As a result, the final /images/ directory is \nnot included in anime_path  that we define earlier. \n88 chapter  4 Image generation with generative adversarial networks  \nNext, we use the ImageFolder()  class in Torchvision datasets  package to load the \ndataset:\nfrom torchvision import transforms as T\nfrom torchvision.datasets import ImageFolder\ntransform = T.Compose([T.Resize((64, 64)),    \n    T.ToTensor(),    \n    T.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])    \ntrain_data = ImageFolder(root=anime_path,\n                         transform=transform)    \nWe perform three different transformations when loading up the images from the \nlocal folder. First, we resize all images to 64 pixels in height and 64 pixels in width. \nSecond, we convert the images to PyTorch tensors with values in the range [0, 1] by \nusing the ToTensor()  class. Finally, we use the Normalize()  class to deduct 0.5 from \nthe value and divide the difference by 0.5. As a result, the image data are now between \n–1 and 1.\nWe can now put the training data in batches:\nfrom torch.utils.data import DataLoader\nbatch_size = 128\ntrain_loader = DataLoader(dataset=train_data, \n               batch_size=batch_size, shuffle=True)\nThe training dataset is now in batches, with a batch size of 128. \n4.4.2 Channels-first color images in PyTorch\nPyTorch uses a so-called channels-first approach when handling color images. This \nmeans the shape of images in PyTorch are (number_channels, height, width). In \ncontrast, in other Python libraries such as TensorFlow or Matplotlib, a channels-last \napproach is used: a color image has a shape of (height, width, number_channels) \ninstead. \nLet’s look at an example image in our dataset and print out the shape of the image:\nimage0, _ = train_data[0]\nprint(image0.shape)\nThe output is\ntorch.Size([3, 64, 64])\nThe shape of the first image is 3 × 64 × 64. This means the image has three color chan -\nnels (RGB). The height and width of the image are both 64 pixels.\nWhen we plot the images in Matplotlib, we need to convert them to channels-last by \nusing the permute()  method in PyTorch:Changes image size \nto 64 × 64\nConverts images to \nPyTorch tensors\nNormalizes image \nvalues to [- 1, 1] in all \nthree color channels\nLoads the data and \ntransforms images\n 89 Deep convolutional GAN\nimport matplotlib.pyplot as plt\nplt.imshow(image0.permute(1,2,0)*0.5+0.5)\nplt.show()\nNote that we need to multiply the PyTorch tensor representing the image by 0.5 and \nthen add 0.5 to it to convert the values from the range [–1, 1] to the range [0, 1]. You’ll \nsee a plot of an anime face after running the preceding code cell. \nNext, we define a function plot_images()  to visualize 32 images in four rows and \neight columns:\ndef plot_images(imgs):    \n    for i in range(32):\n        ax = plt.subplot(4, 8, i + 1)    \n        plt.imshow(imgs[i].permute(1,2,0)/2+0.5)\n        plt.xticks([])\n        plt.yticks([])\n    plt.subplots_adjust(hspace=-0.6)\n    plt.show()    \nimgs, _ = next(iter(train_loader))    \nplot_images(imgs)    \nYou’ll see a plot of 32 anime faces in a 4 × 8 grid after running the preceding code cell, \nas shown in figure 4.7. \nFigure 4.7    Examples from the anime faces training dataset\n4.5 Deep convolutional GAN\nIn this section, you’ll create a DCGAN model so that we can train it to generate anime \nface images. As usual, the GAN model consists of a discriminator network and a gen -\nerator network. However, the networks are more sophisticated than the ones we have Defines a function to \nvisualize 32 images\nPlaces them in a \n4 × 8 grid\nObtains a batch \nof images\nCalls the function to \nvisualize the images\n90 chapter  4 Image generation with generative adversarial networks  \nseen before: we’ll use convolutional layers, transposed convolutional layers, and batch \nnormalization layers in these networks. \nWe’ll start with the discriminator network. After that, I’ll explain how the generator \nnetwork mirrors the layers in the discriminator network to conjure up realistic color \nimages. You’ll then train the model with the data you prepared earlier in this chapter \nand use the trained model to generate novel images of anime face images. \n4.5.1 Building a DCGAN\nAs in previous GAN models we have seen, the discriminator is a binary classifier to \nclassify samples into real or fake. However, different from the networks we have used so \nfar, we’ll use convolutional layers and batch normalizations. The high-resolution color \nimages in this project have too many parameters, and if we use dense layers only, it’s \ndifficult to train the model effectively. The structure of the discriminator neural net -\nwork is shown in the following listing.\nListing 4.4    A discriminator in DCGAN\nimport torch.nn as nn\nimport torch\ndevice = ""cuda"" if torch.cuda.is_available() else ""cpu""\nD = nn.Sequential(\n    nn.Conv2d(3, 64, 4, 2, 1, bias=False),    \n    nn.LeakyReLU(0.2, inplace=True),    \n    nn.Conv2d(64, 128, 4, 2, 1, bias=False),\n    nn.BatchNorm2d(128),    \n    nn.LeakyReLU(0.2, inplace=True),\n    nn.Conv2d(128, 256, 4, 2, 1, bias=False),\n    nn.BatchNorm2d(256),\n    nn.LeakyReLU(0.2, inplace=True),\n    nn.Conv2d(256, 512, 4, 2, 1, bias=False),\n    nn.BatchNorm2d(512),\n    nn.LeakyReLU(0.2, inplace=True),\n    nn.Conv2d(512, 1, 4, 1, 0, bias=False),\n    nn.Sigmoid(),\n    nn.Flatten()).to(device)    \nThe input to the discriminator network is a color image with three color channels. The \nfirst 2D convolutional layer is Conv2d(3, 64, 4, 2, 1, bias=False) : this means the \ninput has three channels and the output has 64 channels; the kernel size is 4; the stride \nis 2; and the padding is 1. Each of the 2D convolutional layers in the network takes an \nimage and applies filters to extract spatial features. \nStarting from the second 2D convolutional layer, we apply 2D batch normalization \n(which I explained in the last section) and LeakyReLU activation (which I’ll explain \nlater) on the output. The LeakyReLU activation function is a modified version of ReLU. \nIt allows the output to have a slope for values below zero. Specifically, the LeakyReLU \nfunction is defined as follows:Passes the image \nthrough a 2D \nconvolutional layer\nApplies the LeakyReLU \nactivation on outputs of the \nfirst convolutional layer\nPerforms 2D batch normalization \non outputs of the second \nconvolutional layer\nThe output is a single value \nbetween 0 and 1, which can be \ninterpreted as the probability \nthat an image is real.\n 91 Deep convolutional GAN\nwhere β is a constant between 0 and 1. The LeakyReLU activation function is com -\nmonly used to address the sparse gradients problem (when most gradients become \nzero or near-zero). Training DCGANs is one such case. When the input to a neuron is \nnegative, the output of ReLU is zero, and the neuron becomes inactive. LeakyReLU \nreturns a small negative value, not zero, for negative inputs. This helps keep the neu -\nrons active and learning, maintaining a better gradient flow and leading to faster con -\nvergence of model parameters.\nWe’ll use the same approach when building the generator for clothing item genera -\ntion. We’ll mirror the layers used in the discriminator in DCGAN to create a generator, \nas shown in the following listing.\nListing 4.5    Designing a generator in DCGAN\nG=nn.Sequential(\n    nn.ConvTranspose2d(100, 512, 4, 1, 0, bias=False),    \n    nn.BatchNorm2d(512),\n    nn.ReLU(inplace=True),\n    nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),    \n    nn.BatchNorm2d(256),\n    nn.ReLU(inplace=True),\n    nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),\n    nn.BatchNorm2d(128),\n    nn.ReLU(inplace=True),\n    nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),\n    nn.BatchNorm2d(64),\n    nn.ReLU(inplace=True),\n    nn.ConvTranspose2d(64, 3, 4, 2, 1, bias=False),    \n    nn.Tanh()).to(device)    \nAs shown in figure 4.8, to create an image, the generator uses five 2D transposed con -\nvolutional layers: they are symmetric to the five 2D convolutional layers in the discrimi -\nnator. For example, the last layer, ConvTranspose2d(64, 3, 4, 2, 1, bias=False) , \nis modeled after the first layer in the discriminator, Conv2d(3, 64, 4, 2, 1, bias=  \nFalse) . The numbers of input and output  channels in Conv2d  are reversed and used as \nthe numbers of output and input  channels in ConvTranspose2d . The first layer in the generator \nis modeled after the last layer in \nthe discriminator.The second layer in \nthe generator is \nsymmetric to the \nsecond to last layer \nin the discriminator \n(numbers of inputs \nand outputs have \nswitched positions).\nThe last layer in the \ngenerator is symmetric \nto the first layer in the \ndiscriminator.\nUses the Tanh() activation to \nsqueeze values in the output \nlayer to the range [– 1, 1] \nbecause the images in the \ntraining set have values \nbetween – 1 and 1\n92 chapter  4 Image generation with generative adversarial networks  \nConvTranspose2d\n(100,512,4,1,0)\nRandom noise \nvector (100 values)ConvTranspose2d\n(512,256,4,2,1)ConvTranspose2d\n(256,128,4,2,1)ConvTranspose2d\n(128,64,4,2,1)\nConv2d\n(512,1,4,1,0)\nProb(Real)Conv2d\n(256,512,4,2,1)Conv2d\n(128,256,4,2,1)Conv2d\n(64,128,4,2,1)ConvTranspose2d\n(64,3,4,2,1)Conv2d\n(3,64,4,2,1)FakeGeneratorDiscriminatorReal samples\nFigure 4.8    Designing a generator network in DCGAN to create anime faces by mirroring the layers in the \ndiscriminator network. The right side of the diagram shows the discriminator network, which contains \nfive 2D convolutional layers. To design a generator that can conjure up anime faces out of thin air, we \nmirror the layers in the discriminator network. Specifically, as shown on the left half of the figure, the \ngenerator has five 2D transposed convolutional layers, symmetric to the 2D convolutional layers in the \ndiscriminator. Further, in each of the top four layers, the numbers of input and output channels in the \ndiscriminator are reversed and used as the numbers of output  and input channels in the generator. \nThe number of input channels in the first 2D transposed convolutional layer is 100. \nThis is because the generator obtains a 100-value random noise vector from the latent \nspace (bottom left of figure 4.8) and feeds it to the generator. The number of output \nchannels in the last 2D transposed convolutional layer in the generator is 3 because \nthe output is an image with three color channels (RGB). We apply the Tanh activa -\ntion function to the output of the generator to squeeze all values to the range [–1, 1] \nbecause the training images all have values between –1 and 1. \nAs usual, the loss function is binary cross-entropy loss. The discriminator is trying to \nmaximize the accuracy of the binary classification: identify a real sample as real and a \nfake sample as fake. The generator, on the other hand, is trying to minimize the proba -\nbility that the fake sample is being identified as fake.\nWe’ll use the Adam optimizer for both the discriminator and the generator and set \nthe learning rate to 0.0002:",13514
44-4.5.2 Training and using DCGAN.pdf,44-4.5.2 Training and using DCGAN,"93 Deep convolutional GAN\nloss_fn=nn.BCELoss()\nlr = 0.0002\noptimG = torch.optim.Adam(G.parameters(), \n                         lr = lr, betas=(0.5, 0.999))\noptimD = torch.optim.Adam(D.parameters(), \n                         lr = lr, betas=(0.5, 0.999))\nYou have seen the Adam optimizer in chapter 2 but with default values of betas. Here, \nwe select betas that are different from the default values. The betas in the Adam opti -\nmizer play crucial roles in stabilizing and speeding up the convergence of the training \nprocess. They do this by controlling how much emphasis is placed on recent versus \npast gradient information (beta1) and by adapting the learning rate based on the cer -\ntainty of the gradient information (beta2). These parameters are typically fine-tuned \nbased on the specific characteristics of the problem being solved.\n4.5.2 Training and using DCGAN\nThe training process for DCGAN is similar to what we have done for other GAN mod -\nels, such as those used in chapter 3 and earlier in this chapter. Since we don’t know the \ntrue distribution of anime face images, we’ll rely on visualization techniques to deter -\nmine when the training is complete. Specifically, we define a test_epoch()  function \nto visualize the anime faces created by the generator after each epoch of training:\ndef test_epoch():\n    noise=torch.randn(32,100,1,1).\\n        to(device=device)    \n    fake_samples=G(noise).cpu().detach()    \n    for i in range(32):    \n        ax = plt.subplot(4, 8, i + 1)\n        img=(fake_samples.cpu().detach()[i]/2+0.5).\\n            permute(1,2,0)\n        plt.imshow(img)\n        plt.xticks([])\n        plt.yticks([])\n    plt.subplots_adjust(hspace=-0.6)\n    plt.show()\ntest_epoch()    \nIf you run the preceding code cell, you’ll see 32 images that look like snowflake statics \non a TV screen. They don’t look like anime faces at all because we haven’t trained the \ngenerator yet. \nWe define three functions, train_D_on_real() , train_D_on_fake() , and \ntrain_G() , similar to those we used to train the GANs to generate grayscale images \nof clothing items earlier in this chapter. Go to the Jupypter Notebook for this chapter \nin the book’s GitHub repository and familiarize yourself with the functions. They train \nthe discriminator with real images. They then train the discriminator with fake images; \nfinally, they train the generator. \nNext, we train the model for 20 epochs:Obtains 32 random noise \nvectors from the latent space\nGenerates 32 anime \nface images\nPlots the generated \nimages in a 4 × 8 grid\nCalls the function to \ngenerate images before \ntraining the model\n94 chapter  4 Image generation with generative adversarial networks  \nfor i in range(20):\n    gloss=0\n    dloss=0\n    for n, (real_samples,_) in enumerate(train_loader):    \n        loss_D=train_D_on_real(real_samples)\n        dloss+=loss_D\n        loss_D=train_D_on_fake()\n        dloss+=loss_D\n        loss_G=train_G()\n        gloss+=loss_G\n    gloss=gloss/n\n    dloss=dloss/n\n    print(f""epoch {i+1}, dloss: {dloss}, gloss {gloss}"")\n    test_epoch()\nThe training takes about 20 minutes if you are using GPU training. Otherwise, it may \ntake 2 to 3 hours, depending on the hardware configuration on your computer. Alter -\nnatively, you can download the trained model from my website: https: //gattonweb  \n.uky.edu/faculty/lium/gai/anime_gen.zip . \nAfter every epoch of training, you can visualize the generated anime faces. After \njust one epoch of training, the model can already generate color images that look like \nanime faces, as shown in figure 4.9. As training progresses, the quality of the generated \nimages becomes better and better.\nFigure 4.9    Generated images in DCGAN after one epoch of training\nWe’ll discard the discriminator and save the trained generator in the local folder:\nscripted = torch.jit.script(G) \nscripted.save('files/anime_gen.pt') \nTo use the trained generator, we load up the model and use it to generate 32 images:\n 95 Summary\nnew_G=torch.jit.load('files/anime_gen.pt',\n                     map_location=device)\nnew_G.eval()\nnoise=torch.randn(32,100,1,1).to(device)\nfake_samples=new_G(noise).cpu().detach()\nfor i in range(32):\n    ax = plt.subplot(4, 8, i + 1)\n    img=(fake_samples.cpu().detach()[i]/2+0.5).permute(1,2,0)\n    plt.imshow(img)\n    plt.xticks([])\n    plt.yticks([])\nplt.subplots_adjust(hspace=-0.6)\nplt.show() \nThe generated anime faces are shown in figure 4.10. The generated images bear a \nclose resemblance to the ones in the training set shown in figure 4.7.\nFigure 4.10    Generated anime face images by the trained generator in DCGAN\nYou may have noticed that the hair colors of the generated images are different: some \nare black, some are red, and some are blond. You may wonder: Can we tell the gener -\nator to create images with a certain characteristic, such as black hair or red hair? The \nanswer is yes. You’ll learn a couple of different methods to select characteristics in gen -\nerated images in GANs in chapter 5. \nSummary\n¡ To conjure up realistic-looking images out of thin air, the generator mirrors lay -\ners used in the discriminator network.\n¡ While it’s feasible to generate grayscale images by using just fully connected lay -\ners, to generate high-resolution color images, we need to use CNNs.\n¡ Two-dimensional convolutional layers are used for feature extraction. They \napply a set of learnable filters (also known as kernels) to the input data to detect \n96 chapter  4 Image generation with generative adversarial networks  \npatterns and features at different spatial scales. These layers are essential for cap -\nturing hierarchical representations of the input data.\n¡ Two-dimensional transposed convolutional layers (also known as deconvolution \nor upsampling layers) are used for upsampling or generating high-resolution \nfeature maps. They apply a filter to the input data. However, unlike standard \nconvolution, they increase the spatial dimensions by inserting gaps between the \noutput values, which effectively “upscales” the feature maps. This process gener -\nates feature maps of a higher resolution. \n¡ Two-dimensional batch normalization is a technique commonly used in deep \nlearning and neural networks to improve the training and performance of CNNs \nand other models that work with 2D data, such as images. It normalizes the values \nfor each feature channel, so they have a mean of 0 and a standard deviation of 1, \nwhich helps stabilize and speed up training.",6590
45-5.1 The eyeglasses dataset.pdf,45-5.1 The eyeglasses dataset,"975Selecting characteristics \nin generated images\nThis chapter covers\n¡ Building a conditional generative adversarial   \n network to generate images with certain attributes  \n (human faces with or without eyeglasses, for   \n example) \n¡ Implementing Wasserstein distance and gradient  \n penalty to improve image quality\n¡ Selecting vectors associated with different   \n features so that the trained GAN model generates  \n images with certain characteristics (male or  \n female faces, for example)\n¡ Combining conditional GAN with vector selection  \n to specify two attributes simultaneously (female  \n faces without glasses or male faces with glasses,  \n for example)\nThe anime faces we generated with deep convolutional GAN (DCGAN) in chapter \n4 look realistic. However, you may have noticed that each generated image has dif -\nferent attributes such as hair color, eye color, and whether the head tilts toward the \nleft or right. You may be wondering if there is a way to tweak the model so that the \ngenerated images have certain characteristics (such as with black hair and tilting \ntoward the left). It turns out you can.",1148
46-5.1.2 Visualizing images in the eyeglasses dataset.pdf,46-5.1.2 Visualizing images in the eyeglasses dataset,"98 chapter  5 Selecting characteristics in generated images\nIn this chapter, you’ll learn two different ways of selecting characteristics in the gen -\nerated images and their respective advantages and disadvantages. The first method \ninvolves selecting specific vectors in the latent space. Different vectors correspond \nto different characteristics—for example, one vector might result in a male face and \nanother in a female face. The second method uses a conditional GAN (cGAN), which \ninvolves training the model on labeled data. This allows us to prompt the model to \ngenerate images with a specified label, each representing a distinct characteristic—like \nfaces with or without eyeglasses.\nIn addition, you’ll learn to combine the two methods so that you can select two inde -\npendent attributes of the images at the same time. As a result, you can generate four dif -\nferent groups of images: males with glasses, males without glasses, females with glasses, \nand females without glasses. To make things more interesting, you can use a weighted \naverage of the labels or a weighted average of the input vectors to generate images \nthat transition from one attribute to another. For example, you can generate a series of \nimages so that the eyeglasses gradually fade out on the same person’s face (label arith -\nmetic). Or you can generate a series of images so that the male features gradually fade \nout and a male face changes to a female face (vector arithmetic). \nBeing able to conduct either vector arithmetic or label arithmetic alone feels like \nscience fiction, let alone performing the two simultaneously. The whole experience \nreminds us of the quote by Arthur C. Clarke (author of 2001: A Space Odyssey ), “Any suf -\nficiently advanced technology is indistinguishable from magic.” \nDespite the realism of the anime faces generated in chapter 4, they were limited by \nlow resolution. Training GAN models can be tricky and is often hampered by prob -\nlems like small sample sizes or low-quality images. These challenges can prevent mod -\nels from converging, resulting in poor image quality. To address this, we’ll discuss and \nimplement an improved training technique using the Wasserstein distance with gradi -\nent penalty in our cGAN. This enhancement results in more realistic human faces and \nnoticeably better image quality compared to the previous chapter.\n5.1 The eyeglasses dataset\nWe’ll use the eyeglasses dataset in this chapter to train a cGAN model. In the next \nchapter, we’ll also use this dataset to train a CycleGAN model in one of the exercises: to \nconvert an image with eyeglasses to an image without eyeglasses and vice versa. In this \nsection, you’ll learn to download the dataset and preprocess images in it. \nThe Python programs in this chapter and the next are adapted from two excellent \nonline open-source projects: the Kaggle project by Yashika Jain https: //mng.bz/JNVQ  \nand a GitHub repository by Aladdin Persson https: //mng.bz/w5yg . I encourage you to \nlook into these two projects while going through this chapter and the next.  \n5.1.1 Downloading the eyeglasses dataset\nThe eyeglasses dataset we use is from Kaggle. Log into Kaggle and go to the link \nhttps: //mng.bz/q0oz  to download the image folder and the two CSV files on the right: \ntrain.csv  and test.csv . There are 5,000 images in the folder /faces-spring-2020/. \n 99 The eyeglasses dataset\nOnce you have the data, place both the image folder and the two CSV files inside the \nfolder /files/ on your computer.\nNext, we’ll sort the photos into two subfolders: one containing only images with eye -\nglasses and another one with images without eyeglasses.\nFirst, let’s look at the file train.csv:\n!pip install pandas\nimport pandas as pd\ntrain=pd.read_csv('files/train.csv')    \ntrain.set_index('id', inplace=True)    \nThe previous code cell imports the file train.csv  and sets the variable id as the index \nof each observation. The column glasses  in the file has two values: 0 or 1, indicat -\ning whether the image has eyeglasses in it or not (0 means no glasses; 1 means with \nglasses).\nNext, we separate the images into two different folders: one containing images with \neyeglasses and one containing images without eyeglasses.\nListing 5.1    Sorting images with and without eyeglasses     \nimport os, shutil\nG='files/glasses/G/'\nNoG='files/glasses/NoG/'\nos.makedirs(G, exist_ok=True)    \nos.makedirs(NoG, exist_ok=True)    \nfolder='files/faces-spring-2020/faces-spring-2020/'\nfor i in range(1,4501):\n    oldpath=f""{folder}face-{i}.png""\n    if train.loc[i]['glasses']==0:    \n        newpath=f""{NoG}face-{i}.png""\n    elif train.loc[i]['glasses']==1:    \n        newpath=f""{G}face-{i}.png""\n    shutil.move(oldpath, newpath)\nIn the preceding code cell, we first use the os library to create two subfolders /\nglasses/G/ and /glasses/NoG/ inside the folder /files/ on your computer. We then \nuse the shutil  library to move images to the two folders based on the label glasses  \nin the file train.csv . Those labeled 1 are moved to folder G and those labeled 0 to \nfolder NoG. \n5.1.2 Visualizing images in the eyeglasses dataset\nThe classification column glasses  in the file train.csv  is not perfect. If you go to the \nsubfolder G on your computer, for example, you’ll see that most images have glasses, \nbut about 10% have no glasses. Similarly, if you go to the subfolder NoG, you’ll see that \nabout 10% actually have glasses. You need to manually correct this by moving images \nfrom one folder to the other. This is important for our training later so you should \nmanually move images in the two folders so that one contains only images with glasses Loads the data in the file train.\ncsv as a pandas DataFrame\nSets the values in the id column \nas the indexes of observations\nCreates a subfolder /files/glasses/G/ \nto contain images with eyeglasses\nCreates a subfolder /files/\nglasses/NoG/ to contain \nimages without eyeglasses\nMoves images labeled \n0 to folder NoG\nMoves images labeled \n1 to folder G\n100 chapter  5 Selecting characteristics in generated images\nand the other images without glasses. Welcome to the life of a data scientist: fixing data \nproblems is part of daily routine! Let’s first visualize some examples of images with \neyeglasses.\nListing 5.2    Visualizing images with eyeglasses \nimport random\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimgs=os.listdir(G)\nrandom.seed(42)\nsamples=random.sample(imgs,16)    \nfig=plt.figure(dpi=200, figsize=(8,2))\nfor i in range(16):    \n    ax = plt.subplot(2, 8, i + 1)\n    img=Image.open(f""{G}{samples[i]}"")\n    plt.imshow(img)\n    plt.xticks([])\n    plt.yticks([])\nplt.subplots_adjust(wspace=-0.01,hspace=-0.01)\nplt.show()\nIf you have manually corrected the mislabeling of images in folder G, you’ll see 16 \nimages with eyeglasses after running the code in listing 5.2. The output is shown in \nfigure 5.1. \nFigure 5.1    Sample images with eyeglasses in the training dataset\nYou can change G to NoG in listing 5.2 to visualize 16 sample images without eyeglasses \nin the dataset. The complete code is in the book’s GitHub repository https: //github  \n.com/markhliu/DGAI . The output is shown in figure 5.2. \nFigure 5.2    Sample images without eyeglasses in the training datasetRandomly selects 16 \nimages from folder G\nDisplays the 16 images \nin a 2 × 8 grid",7470
47-5.2 cGAN and Wasserstein distance.pdf,47-5.2 cGAN and Wasserstein distance,,0
48-5.2.2 cGANs.pdf,48-5.2.2 cGANs,"101 cGAN and Wasserstein distance\n5.2 cGAN and Wasserstein distance\nA cGAN is similar to the GAN models you have seen in chapters 3 and 4, with the \nexception that you attach a label to the input data. The labels correspond to different \ncharacteristics in the input data. Once the trained GAN model “learns” to associate a \ncertain label with a characteristic, you can feed a random noise vector with a label to \nthe model to generate output with the desired characteristic.1 \nGAN models often suffer from problems like mode collapse (the generator finds \na certain type of output that is good at fooling the discriminator and then collapses \nits outputs to these few modes, ignoring other variations), vanishing gradients, and \nslow convergence. Wasserstein GAN (WGAN) introduces the Earth Mover’s (or \nWasserstein-1) distance as the loss function, offering a smoother gradient flow and \nmore stable training. It mitigates problems like mode collapse.2 We’ll implement it in \ncGAN training in this chapter. Note that WGAN is a concept independent of cGAN: \nIt uses the Wasserstein distance to improve the training process and can be applied to \nany GAN model (such as the ones we created in chapters 3 and 4). We’ll combine both \nconcepts in one setting to save space. \nOther ways to stabilize GAN training\nThe problems with training GAN models are most common when generating high-\nresolution images. The model architecture is usually complex, with many neural layers. \nOther than WGAN, progressive GAN is another way to stabilize training. Progressive \nGANs enhance the stability of GAN training by breaking down the complex task of high-\nresolution image generation into manageable steps, allowing for more controlled and \neffective learning. For details, see “Progressive Growing of GANs for Improved Quality, \nStability, and Variation.” by Karas et al., https://arxiv.org/abs/1710.10196 .\n5.2.1 WGAN with gradient penalty\nWGAN is a technique used to improve the training stability and performance of GAN \nmodels. Regular GANs (such as the ones you have seen in Chapters 3 and 4) have two \ncomponents—a generator and a discriminator. The generator creates fake data, while \nthe discriminator evaluates whether the data is real or fake. Training involves a com -\npetitive zero-sum game in which the generator tries to fool the discriminator, and the \ndiscriminator tries to accurately classify real and fake data instances. \nResearchers have proposed to use Wasserstein distance (a measure of dissimilarity \nbetween two distributions) instead of the binary cross-entropy as the loss function to sta -\nbilize training with a gradient penalty term.3 The technique offers a smoother gradient \n1 Mehdi Mirza, Simon Osindero, 2014, “Conditional Generative Adversarial Nets.” https: //arxiv.org/abs/1411.1784 .\n2 Martin Arjovsky, Soumith Chintala, and Léon Bottou, 2017, “Wasserstein GAN.” https: //arxiv.org/abs/1701.07875 .\n3  Martin Arjovsky, Soumith Chintala, and Leon Bottou, 2017, “Wasserstein GAN.” https: //arxiv.org/abs/1701.07875 ; \nand Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville, 2017, “Improved \nTraining of Wasserstein GANs.” https: //arxiv.org/abs/1704.00028 .\n102 chapter  5 Selecting characteristics in generated images\nflow and mitigates problems like mode collapse. Figure 5.3 provides a diagram of \nWGAN. As you can see on the right side of the figure, the losses associated with the real \nand fake images are Wasserstein loss instead of the regular binary cross-entropy loss.\nLatent\nvariable\nScoreTotal\nloss \nStep 1\nStep 2Real image\n Interpolated image\nStep 3Step 3Step 3\nStep 4Wasserstein \nloss \n+ Gradient\npenalty Step 5: Feedback\nGenerator\nStep 5: FeedbackFake image\nCritic\nFigure 5.3    WGAN with gradient penalty. The discriminator network in WGAN (which we call the critic) rates \ninput images: it tries to assign a score of –∞ to a fake image (bottom left) and a score of ∞ to the real image \n(top middle). Further, an interpolated image of the real and fake images (top left) is presented to the critic, and \nthe gradient penalty with respect to the critic’s rating on the interpolated image is added to the total loss in the \ntraining process. \nFurther, for the Wasserstein distance to work correctly, the discriminator (called the \ncritic in WGANs) must be 1-Lipschitz continuous, meaning the gradient norms of the \ncritic’s function must be at most 1 everywhere. The original WGAN paper proposed \nweight clipping to enforce the Lipschitz constraint.\nTo address weight clipping problems, the gradient penalty is added to the loss func -\ntion to enforce the Lipschitz constraint more effectively. To implement WGAN with \ngradient penalty, we first randomly sample points along the straight line between real \nand generated data points (as indicated by the interpolated image in the top left of fig -\nure 5.3). Since both real and fake images have labels attached to them, the interpolated \nimage also has a label attached to it, which is the interpolated value of the two original \nlabels. We then compute the gradient of the critic’s output with respect to these sam -\npled points. Finally, we add a penalty to the loss function proportional to the deviation \nof these gradient norms from 1 (the penalty term is called gradient penalty). That is, \ngradient penalty in WGANs is a technique to improve training stability and sample qual -\nity by enforcing the Lipschitz constraint more effectively, addressing the limitations of \nthe original WGAN model.  \n5.2.2 cGANs\ncGAN is an extension of the basic GAN framework. In a cGAN, both the generator \nand the discriminator (or the critic since we are implementing WGAN and cGAN in \n 103 cGAN and Wasserstein distance\nthe same setting) are conditioned on some additional information. This could be any -\nthing, such as class labels, data from other modalities, or even textual descriptions. \nThis conditioning is typically achieved by feeding this additional information into both \nthe generator and discriminator. In our setting, we’ll add class labels to the inputs to \nboth the generator and the critic: we attach one label to images with eyeglasses and \nanother label to images without eyeglasses. Figure 5.4 provides a diagram of the train -\ning process for cGANs. \nLatent variable and a\nlabel indicating with or\nwithout glasses\nCritic networkGround\ntruthStep 5: FeedbackFake imageReal image\nStep 5: Feedback\nA label\nindicating with\nor without\nglasses\nA label\nindicating with\nor without\nglasses\nStep 1\nStep 2\nStep 3Step 3\nGenerator\nnetworkStep 4\nScore\n \nFigure 5.4    The training process for cGANs \nAs you can see at the top left of figure 5.4, in a cGAN, the generator receives both a \nrandom noise vector and the conditional information (a label indicating whether the \nimage has eyeglasses or not) as input. It uses this information to generate data that not \nonly looks real but also aligns with the conditional input. \nThe critic receives either real data from the training set or fake data generated by \nthe generator, along with the conditional information (a label indicating whether the \nimage has eyeglasses or not in our setting). Its task is to determine whether the given \ndata is real or fake, taking the conditional information into account (does the gener -\nated image have eyeglasses in it?). In figure 5.4, we use the critic network instead of the \ndiscriminator network since we implement both cGAN and WGAN simultaneously, but \nthe concept of cGAN applies to traditional GANs as well. \nThe main advantage of cGANs is their ability to select aspects of the generated data, \nmaking them more versatile and applicable in scenarios where the output needs to \nbe directed or conditioned on certain input parameters. In our setting, we’ll train the",7911
49-5.3 Create a cGAN.pdf,49-5.3 Create a cGAN,,0
50-5.3.3 Weight initialization and the gradient penalty function.pdf,50-5.3.3 Weight initialization and the gradient penalty function,"104 chapter  5 Selecting characteristics in generated images\ncGAN so that we have the ability to select whether the generated images have eyeglasses \nor not. \nIn summary, cGANs are a powerful extension of the basic GAN architecture, enabling \ntargeted generation of synthetic data based on conditional inputs.\n5.3 Create a cGAN\nIn this section, you’ll learn to create a cGAN to generate human faces with or without \neyeglasses. You’ll also learn to implement the WGAN with gradient penalty to stabilize \ntraining.  \nThe generator in cGANs uses not only random noise vectors but also conditional \ninformation such as labels as inputs to create images either with or without eyeglasses. \nFurther, a critic network in WGANs is different from the discriminator network in tradi -\ntional GANs. You’ll also learn how to calculate the Wasserstein distance and the gradi -\nent penalty in this section. \n5.3.1 A critic in cGAN\nIn cGANs, the discriminator is a binary classifier to identify the input as either real or \nfake, conditional on the label. In WGAN, we call the discriminator network the critic. \nThe critic evaluates the input and gives a score between −∞ and ∞. The higher the \nscore, the more likely that the input is from the training set (that is, real). \nListing 5.3 creates the critic network. The architecture is somewhat similar to the dis -\ncriminator network we used in chapter 4 when generating color images of anime faces. \nIn particular, we use seven Conv2d  layers in PyTorch to gradually downsample the input \nso that the output is a single value between −∞ and ∞. \nListing 5.3    A critic network in cGAN with Wasserstein distance\nclass Critic(nn.Module):\n    def __init__(self, img_channels, features):\n        super().__init__()\n        self.net = nn.Sequential(    \n            nn.Conv2d(img_channels, features, \n                      kernel_size=4, stride=2, padding=1),\n            nn.LeakyReLU(0.2),\n            self.block(features, features * 2, 4, 2, 1),   \n            self.block(features * 2, features * 4, 4, 2, 1),\n            self.block(features * 4, features * 8, 4, 2, 1),\n            self.block(features * 8, features * 16, 4, 2, 1),  \n            self.block(features * 16, features * 32, 4, 2, 1),            \n            nn.Conv2d(features * 32, 1, kernel_size=4,\n                      stride=2, padding=0))    \n    def block(self, in_channels, out_channels, \n              kernel_size, stride, padding):\n        return nn.Sequential(    \n            nn.Conv2d(in_channels,out_channels,\n                kernel_size,stride,padding,bias=False,),\n            nn.InstanceNorm2d(out_channels, affine=True),\n            nn.LeakyReLU(0.2))The critic network has two \nConv2d layers plus five blocks.\nThe output has one feature, \nwithout activation.\nEach block contains a \nConv2d layer, an \nInstanceNorm2d \nlayer, with LeakyReLU \nactivation.\n 105 Create a cGAN\n    def forward(self, x):\n        return self.net(x)\nThe input to the critic network is a color image with a shape of 5 × 256 × 256. The first \nthree channels are the color channels (colors red, green, and blue). The last two chan -\nnels (the fourth and fifth channels) are label channels to tell the critic whether the \nimage is with glasses or without glasses. We’ll discuss the exact mechanism to accom -\nplish this in the next section.\nThe critic network consists of seven Conv2d  layers. In chapter 4, we discussed in \ndepth how these layers work. They are used for feature extraction by applying a set \nof learnable filters on the input images to detect patterns and features at different \nspatial scales, effectively capturing hierarchical representations of the input data. The \ncritic then evaluates the input images based on these representations. The five Conv2d  \nlayers in the middle are all followed by an InstanceNorm2d  layer and a LeakyReLU  \nactivation; hence, we define a block()  method to streamline the critic network. The \nInstanceNorm2d  layer is similar to the BatchNorm2d  layer we discussed in chapter 4, \nexcept that we normalize each individual instance in the batch independently. \nAnother key point is that the output is no longer a value between 0 and 1 since we \ndon’t use the sigmoid activation in the last layer in the critic network. Instead, the out -\nput is a value between −∞ and ∞ since we use the Wasserstein distance with gradient \npenalty in our cGAN.\n5.3.2 A generator in cGAN\nIn WGANs, the generator’s job is to create data instances so that they can be evaluated \nat a high score by the critic. In cGANs, the generator must generate data instances \nwith conditional information (with or without eyeglasses in our setting). Since we are \nimplementing a cGAN with Wasserstein distance, we’ll tell the generator what type of \nimages we want to generate by attaching a label to the random noise vector. We’ll dis -\ncuss the exact mechanism in the next section. \nWe create the neural network shown in the following listing to represent the \ngenerator.\nListing 5.4    A generator in cGAN\nclass Generator(nn.Module):\n    def __init__(self, noise_channels, img_channels, features):\n        super(Generator, self).__init__()\n        self.net = nn.Sequential(    \n            self.block(noise_channels, features *64, 4, 1, 0),\n            self.block(features * 64, features * 32, 4, 2, 1),\n            self.block(features * 32, features * 16, 4, 2, 1),\n            self.block(features * 16, features * 8, 4, 2, 1),\n            self.block(features * 8, features * 4, 4, 2, 1),            \n            self.block(features * 4, features * 2, 4, 2, 1),            \n            nn.ConvTranspose2d(\n                features * 2, img_channels, kernel_size=4,\n                stride=2, padding=1),    The generator \nconsists of seven \nConvTranspose2d \nlayers.\n106 chapter  5 Selecting characteristics in generated images\n            nn.Tanh())    \n    def block(self, in_channels, out_channels, \n              kernel_size, stride, padding):\n        return nn.Sequential(    \n            nn.ConvTranspose2d(in_channels,out_channels,\n                kernel_size,stride,padding,bias=False,),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(),)\n    def forward(self, x):\n        return self.net(x)\nWe’ll feed a random noise vector from a 100-dimensional latent space to the generator \nas input. We’ll also feed a 2-value one-hot encoded image label to the generator to tell \nit to generate an image either with or without eyeglasses. We’ll concatenate the two \npieces of information together to form a 102-dimensional input variable to the gener -\nator. The generator then generates a color image based on the input from the latent \nspace and the labeling information. \nThe generator network consists of seven ConvTranspose2d  layers, and the idea is to \nmirror the steps in the critic network to conjure up images, as we discussed in chapter \n4. The first six ConvTranspose2d  layers are all followed by a BatchNorm2d  layer and a \nReLU  activation; hence, we define a block()  method in the generator network to sim -\nplify the architecture. As we have done in chapter 4, we use the Tanh activation function \nat the output layer so the output pixels are all in the range of –1 and 1, the same as the \nimages in the training set.\n5.3.3 Weight initialization and the gradient penalty function\nIn deep learning, the weights in neural networks are randomly initialized. When the \nnetwork architecture is complicated, and there are many hidden layers (which is the \ncase in our setting), how weights are initialized is crucial. \nWe, therefore, define the following weights_init()  function to initialize weights \nin both the generator and the critic networks:\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        nn.init.normal_(m.weight.data, 1.0, 0.02)\n        nn.init.constant_(m.bias.data, 0)   \nThe function initializes weights in Conv2d  and ConvTranspose2d  layers with values \ndrawn from a normal distribution with a mean of 0 and a standard deviation of 0.02. It \nalso initializes weights in BatchNorm2d  layers with values drawn from a normal distri -\nbution with a mean of 1 and a standard deviation of 0.02. We choose a small standard \ndeviation in weight initializations to avoid exploding gradients. Uses Tanh activation \nto squeeze values to \nthe range [– 1, 1], \nthe same as images \nin the training set\nEach block consists of \na ConvTranspose2d \nlayer, a BatchNorm2d \nlayer, and ReLU \nactivation.\n 107 Create a cGAN\nNext, we create a generator and a critic based on the Generator()  and Critic()  \nclasses we defined in the last subsection. We then initialize the weights in them based on \nthe weights_init()  function defined earlier:\nz_dim=100\nimg_channels=3\nfeatures=16\ngen=Generator(z_dim+2,img_channels,features).to(device)\ncritic=Critic(img_channels+2,features).to(device)\nweights_init(gen)\nweights_init(critic)\nAs usual, we’ll use the Adam optimizer for both the critic and the generator:\nlr = 0.0001\nopt_gen = torch.optim.Adam(gen.parameters(), \n                         lr = lr, betas=(0.0, 0.9))\nopt_critic = torch.optim.Adam(critic.parameters(), \n                         lr = lr, betas=(0.0, 0.9))\nThe generator tries to create images that are indistinguishable from those in the train -\ning set with the given label. It presents the images to the critic to obtain high ratings on \nthe generated images. The critic, on the other hand, tries to assign high ratings to real \nimages and low ratings to fake images, conditional on the given label. Specifically, the \nloss function for the critic has three components:\ncritic_value(fake) − critic_value(real) + weight × GradientPenalty\nThe first term, critic_value(fake) , says that if an image is fake, the critic’s objective is to \nidentify it as fake and give it a low evaluation. The second term, − critic_value(real) , indi -\ncates that if the image is real, the critic’s objective is to identify it as real and give it a \nhigh evaluation. Further, the critic wants to minimize the gradient penalty term, weight \n× GradientPenalty , where weight  is a constant to determine how much penalty we want \nto assign to deviations of the gradient norms from the value 1. The gradient penalty is \ncalculated as shown in the following listing.\nListing 5.5    Calculating gradient penalty\ndef GP(critic, real, fake):\n    B, C, H, W = real.shape    \n    alpha=torch.rand((B,1,1,1)).repeat(1,C,H,W).to(device)    \n    interpolated_images = real*alpha+fake*(1-alpha)    \n    critic_scores = critic(interpolated_images)    \n    gradient = torch.autograd.grad(    \n        inputs=interpolated_images,\n        outputs=critic_scores,\n        grad_outputs=torch.ones_like(critic_scores),\n        create_graph=True,\n        retain_graph=True)[0]    \n    gradient = gradient.view(gradient.shape[0], -1)\n    gradient_norm = gradient.norm(2, dim=1)\n    gp = torch.mean((gradient_norm - 1) ** 2)    \n    return gpCreates an \ninterpolated image of \nthe real and the fake\nObtains the critic value \nwith respect to the \ninterpolated image\nCalculates the gradient \nof the critic value \nGradient penalty is the \nsquared deviation of the \ngradient norm from value 1.",11488
51-5.4 Training the cGAN.pdf,51-5.4 Training the cGAN,,0
52-5.4.1 Adding labels to inputs.pdf,52-5.4.1 Adding labels to inputs,"108 chapter  5 Selecting characteristics in generated images\nIn the function GP() , we first create interpolated images of real ones and fake ones. \nThis is done by randomly sampling points along the straight line between real and \ngenerated images. Imagine a slider: at one end is the real image, and at the other is \nthe fake image. As you move the slider, you see a continuous blend from the real to the \nfake, with the interpolated images representing the stages in between.\nWe then present interpolated images to the critic network to obtain ratings on them \nand calculate the gradient of the critic’s output with respect to the interpolated images. \nFinally, the gradient penalty is calculated as the squared deviation of the gradient norms \nfrom the target value of 1.\n5.4 Training the cGAN\nAs we mentioned in the last section, we need to find a way to tell both the critic and the \ngenerator what the image label is so they know if the image has eyeglasses or not.\nIn this section, you’ll first learn how to add labels to the inputs to the critic network \nand the inputs to the generator network so the generator knows what type of images \nto create while the critic can evaluate the images conditional on the labels. After that, \nyou’ll learn how to train the cGAN with Wasserstein distance. \n5.4.1 Adding labels to inputs\nWe first preprocess the data and convert the images to torch tensors:\nimport torchvision.transforms as T\nimport torchvision\nbatch_size=16\nimgsz=256\ntransform=T.Compose([\n    T.Resize((imgsz,imgsz)),\n    T.ToTensor(),\n    T.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5])])      \ndata_set=torchvision.datasets.ImageFolder(\n    root=r""files/glasses"",\n    transform=transform) \nWe set the batch size to 16 and the image size to 256 by 256 pixels. The pixel values are \nchosen so the generated images have higher resolutions than those in the last chapter \n(64 by 64 pixels). We choose a batch size of 16, smaller than the batch size in chapter 3, \ndue to the larger image size. If the batch size is too large, your GPU (or even CPU) will \nrun out of memory. \nTIP    If you are using GPU training and your GPU has a small memory (say, 6GB), \nconsider reducing the batch size to a smaller number than 16, such as 10 or 8, so \nthat your GPU doesn’t run out of memory. Alternatively, you can keep the batch \nsize at 16 but switch to CPU training to address the GPU memory problem. \nNext, we’ll add labels to the training data. Since there are two types of images—images \nwith eyeglasses and images without glasses—we’ll create two one-hot image labels. \n 109 Training the cGAN\nImages with glasses will have a one-hot label of [1, 0], and images without glasses will \nhave a one-hot label of [0, 1]. \nThe input to the generator is a 100-value random noise vector. We concatenate the \none-hot label with the random noise vector and feed the 102-value input to the gener -\nator. The input to the critic network is a three-channel color image with a shape of 3 by \n256 by 256 (PyTorch uses channel-first tensors to represent images). How do we attach \na label with a shape of 1 by 2 to an image with a shape of 3 by 256 by 256? The solution \nis to add two channels to the input image so that the image shape changes from (3, 256, \n256) to (5, 256, 256): the two additional channels are the one-hot labels. Specifically, if \nan image has eyeglasses in it, the fourth channel is filled with 1s and the fifth channel \n0s; if the image has no eyeglasses in it, the fourth channel is filled with 0s and the fifth \nchannel 1s. \nCreating labels if there are more than two values in a characteristic\nYou can easily extend the cGAN model to characteristics with more than two values. For \nexample, if you create a model to generate images with the different hair colors black, \nblond, and white, the image labels you feed to the generator can have values [1, 0, 0], \n[0, 1, 0], and [0, 0, 1], respectively. You can attach three channels to the input image \nbefore you feed it to the discriminator or critic. For example, if an image has black hair, \nthe fourth channel is filled with 1s and the fifth and sixth channels 0s.\nAdditionally, in the eyeglasses example, since there are only two values in the label, you \ncan potentially use values 0 and 1 to indicate images with and without glasses when you \nfeed the label to the generator. You can attach one channel to the input image before you \nfeed it to the critic: if an image has eyeglasses, the fourth channel is filled with 1s; if the \nimage has no eyeglasses, the fourth channel is filled with 0s. I’ll leave that as an exercise \nfor you. The solution is provided in the book’s GitHub repository: https://github.com/\nmarkhliu/DGAI .\nWe implement this change as shown in the following listing.\nListing 5.6    Attaching labels to input images \nnewdata=[]    \nfor i,(img,label) in enumerate(data_set):\n    onehot=torch.zeros((2))\n    onehot[label]=1\n    channels=torch.zeros((2,imgsz,imgsz))    \n    if label==0:\n        channels[0,:,:]=1    \n    else:\n        channels[1,:,:]=1    \n    img_and_label=torch.cat([img,channels],dim=0)    \n    newdata.append((img,label,onehot,img_and_label))Creates two extra channels \nfilled with 0s, each channel \nwith a shape of 256 by 256, the \nsame as the dimension of each \nchannel in the input image\nIf the original image label is 0, \nfills the fourth channel with 1s\nIf the original \nimage label is 1, \nfills the fifth \nchannel with 1sAdds the fourth and fifth channels \nto the original image to form a \nfive-channel labeled image",5619
53-5.4.2 Training the cGAN.pdf,53-5.4.2 Training the cGAN,"110 chapter  5 Selecting characteristics in generated images\nTIP    Earlier when we load the images by using the torchvision.datasets  \n.ImageFolder()  method from the folder /files/glasses, PyTorch assigns labels \nto images in each subfolder in alphabetical order. Therefore, images in /files/\nglasses/G/ are assigned a label of 0, and those in /files/glasses/NoG/, a label \nof 1. \nWe first create an empty list newdata  to hold images with labels. We create a PyTorch \ntensor with a shape (2, 256, 256) to be attached to the original input image to form \na new image with a shape of (5, 256, 256). If the original image label is 0 (this means \nimages are from the folder /files/glasses/G/), we fill the fourth channel with 1s and \nthe fifth channel with 0s so that the critic knows it’s an image with glasses. On the other \nhand, if the original image label is 1 (this means images are from the folder /files/\nglasses/NoG/), we fill the fourth channel with 0s and the fifth channel with 1s so that \nthe critic knows it’s an image without glasses.\nWe create a data iterator with batches (to improve computational efficiency, memory \nusage, and optimization dynamics in the training process) as follows:\ndata_loader=torch.utils.data.DataLoader(\n    newdata,batch_size=batch_size,shuffle=True)\n5.4.2 Training the cGAN\nNow that we have the training data and two networks, we’ll train the cGAN. We’ll use \nvisual inspections to determine when the training should stop.\nOnce the model is trained, we’ll discard the critic network and use the generator to \ncreate images with a certain characteristic (with or without glasses, in our case).\nWe’ll create a function to test periodically what the generated images look like.\nListing 5.7    Inspecting generated images\ndef plot_epoch(epoch):\n    noise = torch.randn(32, z_dim, 1, 1)\n    labels = torch.zeros(32, 2, 1, 1)\n    labels[:,0,:,:]=1    \n    noise_and_labels=torch.cat([noise,labels],dim=1).to(device)\n    fake=gen(noise_and_labels).cpu().detach()    \n    fig=plt.figure(figsize=(20,10),dpi=100)\n    for i in range(32):    \n        ax = plt.subplot(4, 8, i + 1)\n        img=(fake.cpu().detach()[i]/2+0.5).permute(1,2,0)\n        plt.imshow(img)\n        plt.xticks([])\n        plt.yticks([])\n    plt.subplots_adjust(hspace=-0.6)\n    plt.savefig(f""files/glasses/G{epoch}.png"")\n    plt.show() \n    noise = torch.randn(32, z_dim, 1, 1)\n    labels = torch.zeros(32, 2, 1, 1)\n    labels[:,1,:,:]=1    \n    … (code omitted)Creates a one-hot label for \nimages with glasses\nFeeds the \nconcatenated \nnoise vector and \nlabel to the \ngenerator to \ncreate images \nwith glasses\nPlots the generated \nimages with glasses\nCreates a one-hot label for \nimages without glasses\n 111 Training the cGAN\nAfter each epoch of training, we’ll ask the generator to create a set of images with \nglasses and a set of images without glasses. We then plot the images so that we can \ninspect them visually. To create images with glasses, we first create one-hot labels [1, 0] \nand attach them to the random noise vectors before feeding the concatenated vector \nto the generator network. The generator creates images with glasses since the label is \n[1, 0] instead of [0, 1]. We then plot the generated images in four rows and eight col -\numns and save the subplots on your computer. The process of creating images without \nglasses is similar, except that we use the one-hot label [0, 1] instead of [1, 0]. I skipped \npart of the code in listing 5.7, but you can find it in the book’s GitHub repository: \nhttps: //github.com/markhliu/DGAI .\nWe define a train_batch()  function to train the model with a batch of data.\nListing 5.8    Training the model with a batch of data\ndef train_batch(onehots,img_and_labels,epoch):\n    real = img_and_labels.to(device)    \n    B = real.shape[0]\n    for _ in range(5):    \n        noise = torch.randn(B, z_dim, 1, 1)\n        onehots=onehots.reshape(B,2,1,1)\n        noise_and_labels=torch.cat([noise,onehots],dim=1).to(device)\n        fake_img = gen(noise_and_labels).to(device)\n        fakelabels=img_and_labels[:,3:,:,:].to(device)\n        fake=torch.cat([fake_img,fakelabels],dim=1).to(device)    \n        critic_real = critic(real).reshape(-1)\n        critic_fake = critic(fake).reshape(-1)\n        gp = GP(critic, real, fake)    \n        loss_critic=(-(torch.mean(critic_real) - \n           torch.mean(critic_fake)) + 10 * gp)    \n        critic.zero_grad()\n        loss_critic.backward(retain_graph=True)\n        opt_critic.step()\n    gen_fake = critic(fake).reshape(-1)\n    loss_gen = -torch.mean(gen_fake)    \n    gen.zero_grad()\n    loss_gen.backward()\n    opt_gen.step()\n    return loss_critic, loss_gen\nIn the train_batch()  function, we first train the critic with real images. We also ask \nthe generator to create a batch of fake data with the given label. We then train the \ncritic with fake images. In the train_batch()  function, we also train the generator \nwith a batch of fake data.  \nNOTE    The loss for the critic has three components: loss from evaluating real \nimages, loss from evaluating fake images, and the gradient penalty loss. \nWe now train the model for 100 epochs:\nfor epoch in range(1,101):\n    closs=0A batch of real images \nwith labels\nA batch of \ngenerated \nimages with \nlabels\nThe total loss for the critic \nhas three components: loss \nfrom evaluating real images, \nloss from evaluating fake \nimages, and the gradient \npenalty loss.\nTrains the generator with \nthe Wasserstein loss",5586
54-5.5.1 Selecting images with or without eyeglasses.pdf,54-5.5.1 Selecting images with or without eyeglasses,"112 chapter  5 Selecting characteristics in generated images\n    gloss=0\n    for _,_,onehots,img_and_labels in data_loader:    \n        loss_critic, loss_gen = train_batch(onehots,\\n                                img_and_labels,epoch)    \n        closs+=loss_critic.detach()/len(data_loader)\n        gloss+=loss_gen.detach()/len(data_loader)\n    print(f""at epoch {epoch},\\n    critic loss: {closs}, generator loss {gloss}"")\n    plot_epoch(epoch)\ntorch.save(gen.state_dict(),'files/cgan.pth')    \nAfter each epoch of training, we print out the critic loss and the generator loss to \nensure that the losses are in a reasonable range. We also generate 32 images of faces \nwith glasses as well as 32 images without glasses by using the plot_epoch()  function \nwe defined earlier. We  save the weights in the trained generator in the local folder \nafter training is done so that later we can generate images using the trained model.\nThis training takes about 30 minutes if you are using GPU training. Otherwise, it may \ntake several hours, depending on the hardware configuration on your computer. Alter -\nnatively, you can download the trained model from my website:  https: //gattonweb.uky  \n.edu/faculty/lium/gai/cgan.zip . Unzip the file after downloading. \n5.5 Selecting characteristics in generated images\nThere are at least two ways to generate images with a certain characteristic. The first is \nto attach a label to a random noise vector before feeding it to the trained cGAN model. \nDifferent labels lead to different characteristics in the generated image (in our case, \nwhether the image has eyeglasses). The second way is to select the noise vector you \nfeed to the trained model: while one vector leads to an image with a male face, another \nleads to an image with a female face. Note that the second way works even in a tradi -\ntional GAN such as the ones we trained in chapter 4. It works in a cGAN as well. \nBetter yet, in this section, you’ll learn to combine these two methods so you can \nselect two characteristics simultaneously: an image of a male face with eyeglasses or a \nfemale face without eyeglasses, and so on. \nThere are pros and cons for each one of these two methods in selecting a certain \ncharacteristic in generated images. The first way, the cGAN, requires labeled data to \ntrain the model. Sometimes, labeled data is costly to curate. However, once you have \nsuccessfully trained a cGAN, you can generate a wide range of images with a certain \ncharacteristic. In our case, you can generate many different images with eyeglasses (or \nwithout eyeglasses); each one is different from the other. The second way, handpicking \na noise vector, doesn’t need labeled data to train the model. However, each handpicked \nnoise vector can only generate one image. If you want to generate many different \nimages with the same characteristic as the cGAN, you’ll need to handpick many differ -\nent noise vectors ex ante. Iterates through all batches \nin the training dataset\nTrains the model with \na batch of data\nSaves the weights in \nthe trained generator\n 113 Selecting characteristics in generated images\n5.5.1 Selecting images with or without eyeglasses\nBy attaching a label of either [1, 0] or [0, 1] to a random noise vector before you \nfeed it to the trained cGAN model, you can select whether the generated image has \neyeglasses.\nFirst, we’ll use the trained model to generate 32 images with glasses and plot them \nin a 4 × 8 grid. To make results reproducible, we’ll fix the random state in PyTorch. \nFurther, we’ll use the same set of random noise vectors so that we look at the same set \nof faces. \nWe fix the random state at seed 0 and generate 32 images of faces with eyeglasses.\nListing 5.9    Generating images of human faces with eyeglasses\ntorch.manual_seed(0)    \ngenerator=Generator(z_dim+2,img_channels,features).to(device)\ngenerator.load_state_dict(torch.load(""files/cgan.pth"",\n    map_location=device))    \ngenerator.eval()\nnoise_g=torch.randn(32, z_dim, 1, 1)    \nlabels_g=torch.zeros(32, 2, 1, 1)\nlabels_g[:,0,:,:]=1    \nnoise_and_labels=torch.cat([noise_g,labels_g],dim=1).to(device)\nfake=generator(noise_and_labels)\nplt.figure(figsize=(20,10),dpi=50)\nfor i in range(32):\n    ax = plt.subplot(4, 8, i + 1)\n    img=(fake.cpu().detach()[i]/2+0.5).permute(1,2,0)\n    plt.imshow(img.numpy())\n    plt.xticks([])\n    plt.yticks([])\nplt.subplots_adjust(wspace=-0.08,hspace=-0.01)\nplt.show()\nWe create another instance of the Generator()  class and name it generator . We then \nload up the trained weights that we saved in the local folder in the last section (or you \ncan download the weights from my website: https: //mng.bz/75Z4 ). To generate 32 \nimages of human faces with eyeglasses; we first draw 32 random noise vectors in the \nlatent space. We’ll also create a set of labels and name them labels_g , and they tell \nthe generator to produce 32 images with eyeglasses.  \nIf you run the program in listing 5.9, you’ll see 32 images as shown in figure 5.5. Fixes the random state so \nresults are reproducible\nLoads up the \ntrained weights\nGenerates a set of \nrandom noise \nvectors and saves \nit so we can select \ncertain vectors \nfrom it to perform \nvector arithmetic\nCreates a label \nto generate \nimages with \neyeglasses\n114 chapter  5 Selecting characteristics in generated images\nFigure 5.5    Images of human faces with eyeglasses that are generated by the trained cGAN model\nFirst, all 32 images do have eyeglasses in them. This indicates that the trained cGAN \nmodel is able to generate images conditional on the provided labels. You may have \nnoticed that some images have male features while others have female features. To \nprepare us for vector arithmetic in the next subsection, we’ll select one random noise \nvector that leads to an image with male features and one that leads to female features. \nAfter inspecting the 32 images in figure 5.5, we select images with index values 0 and \n14, like so:\nz_male_g=noise_g[0]\nz_female_g=noise_g[14]\nTo generate 32 images without eyeglasses, we first produce another set of random \nnoise vectors and labels:\nnoise_ng = torch.randn(32, z_dim, 1, 1)\nlabels_ng = torch.zeros(32, 2, 1, 1)\nlabels_ng[:,1,:,:]=1\nThe new set of random noise vectors is named noise_ng , and the new set of labels \nlabels_ng . Feed them to the generator and you should see 32 images without eye -\nglasses, as shown in figure 5.6.\nNone of the 32 faces in figure 5.6 has eyeglasses in it: the trained cGAN model can \ngenerate images contingent upon the given label. We select images with indexes 8 \n(male) and 31 (female) to prepare for vector arithmetic in the next subsection:\nz_male_ng=noise_ng[8]\nz_female_ng=noise_ng[31]  \n 115 Selecting characteristics in generated images\nFigure 5.6    Images of human faces without eyeglasses that are generated by the trained cGAN model\nNext, we’ll use label interpolation to perform label arithmetic. Recall that the two \nlabels, noise_g  and noise_ng , instruct the trained cGAN model to create images with \nand without eyeglasses, respectively. What if we feed an interpolated label (a weighted \naverage of the two labels [1, 0] and [0, 1]) to the model? What type of images will the \ntrained generator produce? Let’s find out.\nListing 5.10    Label arithmetic in cGAN\nweights=[0,0.25,0.5,0.75,1]    \nplt.figure(figsize=(20,4),dpi=300)\nfor i in range(5):\n    ax = plt.subplot(1, 5, i + 1)\n    # change the value of z\n    label=weights[i]*labels_ng[0]+(1-weights[i])*labels_g[0]    \n    noise_and_labels=torch.cat(\n        [z_female_g.reshape(1, z_dim, 1, 1),\n         label.reshape(1, 2, 1, 1)],dim=1).to(device)    \n    fake=generator(noise_and_labels).cpu().detach()    \n    img=(fake[0]/2+0.5).permute(1,2,0)\n    plt.imshow(img)\n    plt.xticks([])\n    plt.yticks([])\nplt.subplots_adjust(wspace=-0.08,hspace=-0.01)\nplt.show()\nWe first create five weights (w): 0, 0.25, 0.5, 0.75, and 1, equally spaced between 0 \nand 1. Each of these five values of w is the weight we put on the no eyeglasses label \nlabels_ng . The complementary weight is put on the eyeglasses label labels_g . The \ninterpolated label therefore has a value of w*labels_ng+(1-w)*labels_g . We then \nfeed the interpolated label to the trained model, along with the random noise vector Creates five weights\nCreates a \nweighted \naverage of the \ntwo labels\nGives the new \nlabel to the \ntrained model to \ncreate an image",8552
55-5.5.2 Vector arithmetic in latent space.pdf,55-5.5.2 Vector arithmetic in latent space,"116 chapter  5 Selecting characteristics in generated images\nz_female_g  that we saved earlier. The five generated images, based on the five values \nof w, are plotted in a 1 × 5 grid, as shown in figure 5.7. \nFigure 5.7    Label arithmetic in cGAN. We first create two labels: the no eyeglasses label labels_ng  \nand the eyeglasses label labels_g . These two labels instruct the trained generator to produce images \nwith and without eyeglasses, respectively. We then create five interpolated labels, each as a weighted \naverage of the original two labels: w*labels_ng+(1-w)*labels_g , where the weight w takes five \ndifferent values, 0, 0.25, 0.5, 0.75, and 1. The five generated images based on the five interpolated labels \nare shown in the figure. The image on the far left has eyeglasses. As we move from the left to the right, \nthe eyeglasses gradually fade away, until the image on the far right has no eyeglasses in it.\nWhen you look at the five generated images in figure 5.7 from the left to the right, \nyou’ll notice that the eyeglasses gradually fade away. The image on the left has eye -\nglasses while the image on the right has no eyeglasses. The three images in the middle \nshow some signs of eyeglasses, but the eyeglasses are not as conspicuous as those in the \nfirst image. \nExercise 5.1\nSince we used the random noise vector z_female_g  in listing 5.10, the images in figure \n5.7 have a female face. Change the noise vector to z_male_g  in listing 5.10 and rerun \nthe program; see what the images look like. \n5.5.2 Vector arithmetic in latent space\nYou may have noticed that some generated human face images have male features \nwhile others have female features. You may wonder: Can we select male or female fea -\ntures in generated images? The answer is yes. We can achieve this by selecting noise \nvectors in the latent space.\nIn the last subsection, we have saved two random noise vectors, z_male_ng  and  \nz_female_ng , that lead to images of a male face and a female face, respectively. Next, \nwe feed a weighted average of the two vectors (i.e., an interpolated vector) to the trained \nmodel and see what the generated images look like.\nListing 5.11    Vector arithmetic to select image characteristics\nweights=[0,0.25,0.5,0.75,1]    \nplt.figure(figsize=(20,4),dpi=50)\nfor i in range(5):Creates five weights\n 117 Selecting characteristics in generated images\n    ax = plt.subplot(1, 5, i + 1)\n    # change the value of z\n    z=weights[i]*z_female_ng+(1-weights[i])*z_male_ng    \n    noise_and_labels=torch.cat(\n        [z.reshape(1, z_dim, 1, 1),\n         labels_ng[0].reshape(1, 2, 1, 1)],dim=1).to(device)    \n    fake=generator(noise_and_labels).cpu().detach()    \n    img=(fake[0]/2+0.5).permute(1,2,0)\n    plt.imshow(img)\n    plt.xticks([])\n    plt.yticks([])\nplt.subplots_adjust(wspace=-0.08,hspace=-0.01)\nplt.show()\nWe have created five weights, 0, 0.25, 0.5, 0.75, and 1. We iterate through the \nfive weights and create five weighted averages of the two random noise vectors,  \nw*z_female_ng+(1-w)*z_male_ng . We then feed the five vectors, along with the \nlabel, labels_ng , to the trained model to obtain five images, as shown in figure 5.8. \nFigure 5.8    Vector arithmetic in GAN. We first save two random noise vectors z_female_ng  and  \nz_male_ng . The two vectors lead to images of female and male faces, respectively. We then create five \ninterpolated vectors, each as a weighted average of the original two vectors: w*z_female_ng+  \n(1-w)*z_male_ng , where the weight w takes five different values, 0, 0.25, 0.5, 0.75, and 1. The five \ngenerated images based on the five interpolated vectors are shown in the figure. The image on the far left \nhas male features. As we move from the left to the right, the male features gradually fade away and the \nfemale features gradually appear, until the image on the far right shows a female face.\nVector arithmetic can transition from one instance of an image to another instance. \nSince we happen to have selected a male and a female image, when you look at the five \ngenerated images in figure 5.8 from the left to the right, you’ll notice that male fea -\ntures gradually fade away and female features gradually appear. The first image shows \nan image with a male face while the last image shows an image with a female face. \nExercise 5.2\nSince we used the label labels_ng  in listing 5.11, the images in figure 5.8 have no eye -\nglasses in them. Change the label to labels_g  in listing 5.11 and rerun the program to \nsee what the images look like. Creates a \nweighted \naverage of the \ntwo random \nnoise vectors\nFeeds the new \nrandom noise \nvector to the \ntrained model to \ncreate an image",4747
56-5.5.3 Selecting two characteristics simultaneously.pdf,56-5.5.3 Selecting two characteristics simultaneously,"118 chapter  5 Selecting characteristics in generated images\n5.5.3 Selecting two characteristics simultaneously\nSo far, we have selected one characteristic at a time. By selecting the label, you have \nlearned how to generate images with or without eyeglasses. By selecting a specific noise \nvector, you have learned how to select a specific instance of the generated image. \nWhat if you want to select two characteristics (glasses and gender, for example) at the \nsame time? There are four possible combinations of the two independent characteris -\ntics: male faces with glasses, male faces without glasses, female faces with glasses, and \nfemale faces without glasses. Next we’ll generate an image of each type. \nListing 5.12    Selecting two characteristics simultaneously \nplt.figure(figsize=(20,5),dpi=50)\nfor i in range(4):    \n    ax = plt.subplot(1, 4, i + 1)\n    p=i//2    \n    q=i%2    \n    z=z_female_g*p+z_male_g*(1-p)    \n    label=labels_ng[0]*q+labels_g[0]*(1-q)    \n    noise_and_labels=torch.cat(\n        [z.reshape(1, z_dim, 1, 1),\n         label.reshape(1, 2, 1, 1)],dim=1).to(device)    \n    fake=generator(noise_and_labels)\n    img=(fake.cpu().detach()[0]/2+0.5).permute(1,2,0)\n    plt.imshow(img.numpy())\n    plt.xticks([])\n    plt.yticks([])\nplt.subplots_adjust(wspace=-0.08,hspace=-0.01)\nplt.show()\nTo generate four images to cover the four different cases, we need to use one of the \nnoise vectors as the input: z_female_g or z_male_g . We also need to attach to the \ninput a label, which can be either labels_ng or labels_g.  To use one single pro -\ngram to cover all four cases, we iterate through four values of i, 0 to 3, and create \ntwo values, p and q, which are the integer quotient and the remainder of the value \ni divided by 2. Therefore, the values of p and q can be either 0 or 1. By setting the \nvalue of the random noise vector to z_female_g*p+z_male_g*(1-p) , we can select a \nrandom noise vector to generate either a male or female face. Similarly, by setting the \nvalue of the label to labels_ng[0]*q+labels_g[0]*(1-q) , we can select a label to \ndetermine whether the generated image has eyeglasses in it or not. Once we combine \nthe random noise vector with the label and feed them to the trained model, we can \nselect two characteristics simultaneously. \nIf you run the program in listing 5.12, you’ll see four images as shown in figure 5.9. Iterates through 0 to 3\nThe value of p, which can be either 0 \nor 1, selects the random noise vector \nto generate a male or female face. \nThe value of q, \nwhich can be \neither 0 or 1, \nselects the label \nto determine \nwhether the \ngenerated image \nhas eyeglasses in \nit or not.\nCombines the random \nnoise vector with the \nlabel to select two \ncharacteristics\n 119 Selecting characteristics in generated images\nFigure 5.9    Selecting two characteristics simultaneously in the generated image. We select a noise \nvector from the following two choices: z_female_ng  and z_male_ng . We also select a label from \nthe following two choices: labels_ng  and labels_g . We then feed the noise vector and the label \nto the trained generator to create an image. Based on the values of the noise vector and the label, the \ntrained model can create four types of images. By doing this, we effectively select two independent \ncharacteristics in the generated image: a male or a female face and whether the image has eyeglasses in \nit or not. \nThe four generated images in figure 5.9 have two independent characteristics: a male \nor a female face and whether the image has eyeglasses in it or not. The first image \nshows an image of a male face with glasses; the second image is a male face without \nglasses. The third image is a female face with glasses, while the last image shows a \nfemale face without glasses. \nExercise 5.3\nWe used the two random noise vectors z_female_g and z_male_g  in listing 5.12. \nChange the two random noise vectors to z_female_ng and z_male_ng  instead and \nrerun the program to see what the images look like. \nFinally, we can conduct label arithmetic and vector arithmetic simultaneously. That \nis, we can feed an interpolated noise vector and an interpolated label to the trained \ncGAN model and see what the generated image looks like. You can achieve that by run -\nning the following code block: \nplt.figure(figsize=(20,20),dpi=50)\nfor i in range(36):\n    ax = plt.subplot(6,6, i + 1)\n    p=i//6\n    q=i%6 \n    z=z_female_ng*p/5+z_male_ng*(1-p/5)\n    label=labels_ng[0]*q/5+labels_g[0]*(1-q/5)\n    noise_and_labels=torch.cat(\n        [z.reshape(1, z_dim, 1, 1),\n         label.reshape(1, 2, 1, 1)],dim=1).to(device)\n    fake=generator(noise_and_labels)\n    img=(fake.cpu().detach()[0]/2+0.5).permute(1,2,0)\n    plt.imshow(img.numpy())\n    plt.xticks([])\n    plt.yticks([])\n120 chapter  5 Selecting characteristics in generated images\nplt.subplots_adjust(wspace=-0.08,hspace=-0.01)\nplt.show()\nThe code is similar to that in listing 5.12, except that p and q each can take six dif -\nferent values: 0, 1, 2, 3, 4, and 5. The random noise vector, z_female_ng*p/5+z_\nmale_ng*(1-p/5) , takes six different values based on the value of p. The label, \nlabels_ng[0]*q/5+labels_g[0]*(1-q/5) , takes six different values based on the \nvalue of q. We therefore have 36 different combinations of images based on the inter -\npolated noise vector and the interpolated label. If you run the previous program, you’ll \nsee 36 images as shown in figure 5.10. \nFigure 5.10   Conducting vector arithmetic and label arithmetic simultaneously. The value of i \nchanges from 0 to 35; p and q are the integer quotient and remainder, respectively, when i is divided \nby 6. Therefore, p and q each can take six different values: 0, 1, 2, 3, 4, and 5. The interpolated \nnoise vector, z_female_ng*p/5+z_male_ng*(1-p/5) , and the interpolated label, labels_\nng[0]*q/5+labels_g[0]*(1-q/5) , can each take six different values. In each row, when you go \nfrom left to right, the eyeglasses gradually fade away. In each column, when you go from top to bottom, \nthe image changes gradually from a male face to a female face. \n 121 Summary\nThe are 36 images in figure 5.10. The interpolated noise vector is a weighted aver -\nage of the two random noise vectors, z_female_ng  and z_male_ng , which generate \na female face and a male face, respectively. The label is a weighted average of the two \nlabels, labels_ng  and labels_g , which determine whether the generated image has \neyeglasses in it or not. The trained model generates 36 different images based on the \ninterpolated noise vector and the interpolated label. In each row, when you go from \nthe left to the right, the eyeglasses gradually fade away. That is, we conduct label arith -\nmetic in each row. In each column, when you go from the top to the bottom, the image \nchanges gradually from a male face to a female face. That is, we conduct vector arith -\nmetic in each column. \nExercise 5.4\nIn this project, there are two values in the label: one indicates eyeglasses and one indi -\ncates no eyeglasses. Therefore, we can use a binary value instead of one-hot variables \nas labels. Change the programs in this chapter and use values 1 and 0 (instead of [1, 0] \nand [0, 1]) to represent images with and without glasses. Attach 1 or 0 to the random \nnoise vector so that you feed a 101-value vector to the generator. Attach one channel to \nthe input image before you feed it to the critic: if an image has eyeglasses in it, the fourth \nchannel is filled with 0s; if the image has no eyeglasses in it, the fourth channel is filled \nwith 1s. Then create a generator and a critic; use the training dataset to train them. The \nsolution is provided in the book’s GitHub repository, along with solutions to the other \nthree exercises in this chapter. \nNow that you have witnessed what GAN models are capable of, you’ll explore deeper \nin the next chapter by conducting style transfers with GANs. For example, you’ll learn \nhow to build a CycleGAN model and train it using celebrity face images so that you \ncan convert blond hair to black hair or black hair to blond hair in these images. The \nexact same model can be trained on other datasets: for example, you can train it on the \nhuman face dataset you used in this chapter so that you can add or remove eyeglasses \nin human face images. \nSummary\n¡ By selecting a certain noise vector in the latent space and feeding it to the trained \nGAN model, we can select a certain characteristic in the generated image, such as \nwhether the image has a male or female face in it.   \n¡ A cGAN is different from a traditional GAN. We train the model on labeled data \nand ask the trained model to generate data with a specific attribute. For example, \none label tells the model to generate images of human faces with eyeglasses while \nanother tells the model to create human faces without eyeglasses. \n¡ After a cGAN is trained, we can use a series of weighted averages of the labels to \ngenerate images that transition from an image represented by one label to an \nimage represented by another label—for example, a series of images in which \n122 chapter  5 Selecting characteristics in generated images\nthe eyeglasses gradually fade away on the same person’s face. We call this label \narithmetic. \n¡ We can also use a series of weighted averages of two different noise vectors to cre -\nate images that transition from one attribute to another—for example, a series \nof images in which the male features gradually fade away, and female features \ngradually appear. We call this vector arithmetic. \n¡ Wasserstein GAN (WGAN) is a technique used to improve the training stability \nand performance of GAN models by using Wasserstein distance instead of the \nbinary cross-entropy as the loss function. Further, for the Wasserstein distance to \nwork correctly, the critic in WGANs must be 1-Lipschitz continuous, meaning the \ngradient norms of the critic’s function must be at most 1 everywhere. The gradi -\nent penalty in WGANs adds a regularization term to the loss function to enforce \nthe Lipschitz constraint more effectively.",10250
57-6.2 The celebrity faces dataset.pdf,57-6.2 The celebrity faces dataset,"1236CycleGAN: Converting \nblond hair to black hair\nThis chapter covers\n¡ The idea behind CycleGAN an d cycle consistency  \n loss \n¡ Building a CycleGAN model to translate images  \n from one domain to another \n¡ Training a CycleGAN by using any dataset with two  \n domains of images\n¡ Converting black hair to blond h air and vice versa\nThe generative adversarial networks (GAN) models we have discussed in the last \nthree chapters are all trying to produce images that are indistinguishable from \nthose in the training set. \nYou may be wondering: Can we translate images from one domain to another, \nsuch as transforming horses into zebras, converting black hair to blond hair or blond \nhair to black, adding or removing eyeglasses in images, turning photographs into \npaintings, or converting winter scenes to summer scenes? It turns out you can, and \nyou’ll acquire such skills in this chapter through CycleGAN!\n124 chapter  6 CycleGAN: Converting blond hair to black hair\nCycleGAN was introduced in 2017.1 The key innovation of CycleGAN is its ability to \nlearn to translate between domains without paired examples. CycleGAN has a variety of \ninteresting and useful applications, such as simulating the aging or rejuvenation process \non faces to assist digital identity verification or visualizing clothing in different colors or \npatterns without physically creating each variant to streamline the design process. \nCycleGAN uses a cycle consistency loss function to ensure the original image can \nbe reconstructed from the transformed image, encouraging the preservation of key \nfeatures. The idea behind cycle consistency loss is truly ingenious and deserves to be \nhighlighted here. The CycleGAN in this chapter has two generators: let’s call them the \nblack hair generator and the blond hair generator, respectively. The black hair genera -\ntor takes in an image with blond hair (instead of a random noise vector as you have seen \nbefore) and converts it to one with black hair, while the blond hair generator takes in an \nimage with black hair and converts it to one with blond hair.\nTo train the model, we’ll give a real image with black hair to the blond hair generator \nto produce a fake image with blond hair. We’ll then give the fake blond hair image to \nthe black hair generator to convert it back to an image with black hair. If both genera -\ntors work well, there is little difference between the original image with black hair and \nthe fake one after a round-trip conversion. To train the CycleGAN, we adjust the model \nparameters to minimize the sum of adversarial losses and cycle consistency losses. As in \nchapters 3 and 4, adversarial losses are used to quantify how well the generator can fool \nthe discriminator and how well the discriminator can differentiate between real and \nfake samples. Cycle consistency loss, a unique concept in CycleGANs, measures the dif -\nference between the original image and the fake image after a round-trip conversion. \nThe inclusion of the cycle consistency loss in the total loss function is the key innovation \nin CycleGANs.\nWe’ll use black and blond hair images as examples of two domains when training \nCycleGAN. However, the model can be applied to any two domains of images. To drive \nhome the message, I’ll ask you to train the same CycleGAN model by using images \nwith and without eyeglasses that you used in chapter 5. The solution is provided in the \nbook’s GitHub repository ( https: //github.com/markhliu/DGAI ), and you’ll see that \nthe trained model can indeed add or remove eyeglasses from human face images. \n6.1 CycleGAN and cycle consistency loss\nCycleGAN extends the basic GAN architecture to include two generators and two dis -\ncriminators. Each generator-discriminator pair is responsible for learning the mapping \nbetween two distinct domains. It aims to translate images from one domain to another \n(e.g., horses to zebras, summer to winter scenes, and so on) while retaining the key \ncharacteristics of the original images. It uses a cycle consistency loss that ensures the \noriginal image can be reconstructed from the transformed image, encouraging the \npreservation of key features.\n1 Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexie Efros, 2017, “Unpaired Image-to-Image Translation Using Cycle \nConsistent Adversarial Networks.” https: //arxiv.org/abs/1703.10593 .\n 125 CycleGAN and cycle consistency loss\nIn this section, we’ll first discuss the architecture of CycleGAN. We’ll emphasize the \nkey innovation of CycleGANs: cycle consistency loss. \n6.1.1 What is CycleGAN?\nCycleGAN consists of two generators and two discriminators. The generators translate \nimages from one domain to another, while the discriminators determine the authen -\nticity of the images in their respective domains. These networks are capable of trans -\nforming photographs into artworks mimicking the style of famous painters or specific \nart movements, thereby bridging the gap between art and technology. They can also be \nused in healthcare for tasks like converting MRI images to CT scans or vice versa, which \ncan be helpful in situations where one type of imaging is unavailable or too costly. \nFor our project in this chapter, we’ll convert between images with black hair and \nblond hair. We therefore use them as an example when explaining how CycleGAN \nworks. Figure 6.1 is a diagram of the CycleGAN architecture. \nBlond hair\ndiscriminatorReal or\nfake?Ground\ntruthStep 6: Loss_D_blond\nBlond hair\ngenerator\nStep 6: Loss_G_BlondStep 1 Step 3\nBlack hair\ndiscriminatorReal or\nfake?Ground\ntruthStep 6: Loss_D_Black\nBlack hair \ngenerator\nStep 6: Loss_G_BlackStep 1\nStep 2 Step 3 Step 4 Step 5Step 2 Step 3 Step 4 Step 5\nStep 3\nReal black hair\n Real blond hair\nFake blond hair\nReal blond hair\nFake black hairReal black hair\nFigure 6.1    The architecture of a CycleGAN to convert images with black hair to ones with blond hair and \nto convert images with blond hair to ones with black hair. The diagram also outlines the training steps to \nminimize adversarial losses. How the model minimizes cycle consistency losses is explained in figure 6.2.\nTo train CycleGAN, we use unpaired datasets from the two domains we wish to trans -\nlate between. We’ll use 48,472 celebrity face images with black hair and 29,980 images \nwith blond hair. We adjust the model parameters to minimize the sum of adversarial \n126 chapter  6 CycleGAN: Converting blond hair to black hair\nlosses and cycle consistency losses. For ease of explanation, we’ll explain only adversar -\nial losses in figure 6.1. I’ll explain how the model minimizes cycle consistency losses in \nthe next subsection. \nIn each iteration of training, we feed real black hair images (top left in figure 6.1) to \nthe blond hair generator to obtain fake blond hair images. We then feed the fake blond \nhair images, along with real blond hair images, to the blond hair discriminator (top \nmiddle). The blond hair discriminator produces a probability that each one is a real \nblond hair image. We then compare the predictions with the ground truth (whether \nan image is a true image with blond hair) and calculate the loss to the discriminator \n(Loss_D_Blond ) as well as the loss to the generator ( Loss_G_Blond ). \nAt the same time, in each iteration of training, we feed real blond hair images (mid -\ndle left) to the black hair generator (bottom left) to create fake black hair images. We \npresent the fake black hair images, along with real ones, to the black hair discriminator \n(middle bottom) to obtain predictions that they are real. We compare the predictions \nfrom the black hair discriminator with the ground truth and calculate the loss to the dis -\ncriminator ( Loss_D_Black ) and the loss to the generator ( Loss_G_Black ). We train \nthe generators and discriminators simultaneously. To train the two discriminators, we \nadjust the model parameters to minimize the discriminator loss, which is the sum of \nLoss_D_Black  and Loss_D_Blond . \n6.1.2 Cycle consistency loss\nTo train the two generators, we adjust the model parameters to minimize the sum \nof the adversarial loss and cycle consistency loss. The adversarial loss is the sum of  \nLoss_G_Black  and Loss_G_Blond that we discussed in the previous subsection. To \nexplain cycle consistency loss, let’s look at figure 6.2.  \nBlond hair\ngeneratorStep 1\nStep 2 Step 3\nStep 2 Step 3Step 4Fake black hair Real black hair\nFake blond hair\nBlack hair\ngenerator\nStep 5: Cycle consistency loss\nBlack hair\ngeneratorStep 1Step 4Fake blond hair Real blond hair\nFake black hair\nBlond hair\ngeneratorStep 5: Cycle consistency loss\nFigure 6.2    How \nCycleGAN minimizes \ncycle consistency \nlosses between \noriginal black hair \nimages and fake ones \nafter round trips and \ncycle consistency \nlosses between \noriginal blond hair \nimages and fake ones \nafter round trips",8985
58-6.2.1 Downloading the celebrity faces dataset.pdf,58-6.2.1 Downloading the celebrity faces dataset,"127 The celebrity faces dataset\nThe loss function for the generators in CycleGAN consists of two parts. The first part, \nthe adversarial loss, ensures that generated images are indistinguishable from real \nimages in the target domain. For example, Loss_G_Blond  (defined in the previous \nsubsection) ensures that fake blond images produced by the blond hair generator \nresemble real images with blond hair in the training set. The second part, the cycle \nconsistency loss, ensures that an image translated from one domain to another can be \ntranslated back to the original domain.\nThe cycle consistency loss is a crucial component of CycleGANs, ensuring that the \noriginal input image can be recovered after a round-trip translation. The idea is that if \nyou translate a real black hair image (top left in figure 6.2) to a fake blond hair image \nand convert it back to a fake black hair image (top right), you should end up with an \nimage close to the original black hair image. The cycle consistency loss for black hair \nimages is the mean absolute error, at the pixel level, between the fake image and the \noriginal real one. Let’s call this loss Loss_Cycle_Black . The same applies to trans -\nlating blond hair to black hair and then back to blond hair, and we call this loss Loss_\nCycle_Blond . The total cycle consistency loss is the sum of Loss_Cycle_Black  and \nLoss_Cycle_Blond . \n6.2 The celebrity faces dataset\nWe’ll use celebrity face images with black hair and blond hair as the two domains. \nYou’ll first download the data in this section. You’ll then process the images to get \nthem ready for training later in this chapter. \nYou’ll use two new Python libraries in this chapter: pandas  and albumentations . \nTo install these libraries, execute the following line of code in a new cell in your Jupyter \nNotebook application on your computer:\n!pip install pandas albumentations\nFollow the on-screen instructions to finish the installation. \n6.2.1 Downloading the celebrity faces dataset\nTo download the celebrity faces dataset, log into Kaggle and go to the link https: //\nmng.bz/Ompo . Unzip the dataset after downloading and place all image files inside \nthe folder /files/img_align_celeba/img_align_celeba/ on your computer (note there \nis a subfolder with the same name in the folder itself). There are about 200,000 images \nin the folder. Also download the file list_attr_celeba.csv  from Kaggle and place it \nin the /files/ folder on your computer. The CSV file specifies various attributes of each \nimage.\nThe celebrity faces dataset contains images with many different hair colors: brown, \ngray, black, blond, and so on. We’ll select images with black or blond hair as our train -\ning set because these two types are the most abundant in the celebrity faces dataset. Run \nthe code in the following listing to select all images with black or blond hair.\n128 chapter  6 CycleGAN: Converting blond hair to black hair\nListing 6.1    Selecting images with black or blond hair\nimport pandas as pd\nimport os, shutil\ndf=pd.read_csv(""files/list_attr_celeba.csv"")    \nos.makedirs(""files/black"", exist_ok=True)  \nos.makedirs(""files/blond"", exist_ok=True)    \nfolder=""files/img_align_celeba/img_align_celeba""\nfor i in range(len(df)):\n    dfi=df.iloc[i]\n    if dfi['Black_Hair']==1:    \n        try:\n            oldpath=f""{folder}/{dfi['image_id']}""\n            newpath=f""files/black/{dfi['image_id']}""\n            shutil.move(oldpath, newpath)\n        except:\n            pass\n    elif dfi['Blond_Hair']==1:    \n        try:\n            oldpath=f""{folder}/{dfi['image_id']}""\n            newpath=f""files/blond/{dfi['image_id']}""\n            shutil.move(oldpath, newpath)\n        except:\n            pass\nWe first use the pandas  library to load the file list_attr_celeba.csv  so that we \nknow whether each image has black or blond hair in it. We then create two folders \nlocally, /files/black/ and /files/blond/, to store images with black and blond hair, \nrespectively. Listing 6.1 then iterates through all images in the dataset. If an image’s \nattribute Black_Hair  is 1, we move it to the folder /files/black/; if an image’s attri -\nbute Blond_Hair  is 1, we move it to the folder /files/blond/. You’ll see 48,472 images \nwith black hair and 29,980 images with blond hair. Figure 6.3 shows some examples of \nthe images. \nFigure 6.3    Sample images of celebrity faces with black or blond hair\nImages in the top row of figure 6.3 have black hair while images in the bottom row have \nblond hair. Further, the image quality is high: all faces are front and center, and hair Loads the CSV file that \ncontains image attributes\nCreates two folders to \nstore images with black \nand blond hair\nIf the attribute Black_Hair \nis 1, moves the image to \nthe black folder.\nIf the attribute Blond_Hair \nis 1, moves the image to \nthe blond folder.",4926
59-6.3.2 Creating two generators.pdf,59-6.3.2 Creating two generators,"129 The celebrity faces dataset\ncolors are easy to identify. The quantity and quality of the training data will help the \ntraining of the CycleGAN model.\n6.2.2 Process the black and blond hair image data\nWe’ll generalize the CycleGAN model so that it can be trained on any dataset with \ntwo domains of images. We’ll also define a LoadData()  class to process the training \ndataset for the CycleGAN model. The function can be applied to any dataset with two \ndomains, whether human face images with different hair colors, images with or with -\nout eyeglasses, or images with summer and winter scenes. \nTo that end, we have created a local module ch06util . Download the files \nch06util.py  and __init__.py  from the book’s GitHub repository ( https: //github  \n.com/markhliu/DGAI ) and place them in the folder /utils/ on your computer. In the \nlocal module, we have defined the following LoadData()  class.\nListing 6.2    The LoadData() class to process the training data in CycleGAN \nclass LoadData(Dataset):\n    def __init__(self, root_A, root_B, transform=None):    \n        super().__init__()\n        self.root_A = root_A\n        self.root_B = root_B\n        self.transform = transform\n        self.A_images = []\n        for r in root_A:\n            files=os.listdir(r)\n            self.A_images += [r+i for i in files]\n        self.B_images = []\n        for r in root_B:    \n            files=os.listdir(r)\n            self.B_images += [r+i for i in files]      \n        self.len_data = max(len(self.A_images),\n                            len(self.B_images))\n        self.A_len = len(self.A_images)\n        self.B_len = len(self.B_images)\n    def __len__(self):    \n        return self.len_data\n    def __getitem__(self, index):    \n        A_img = self.A_images[index % self.A_len]\n        B_img = self.B_images[index % self.B_len]\n        A_img = np.array(Image.open(A_img).convert(""RGB""))\n        B_img = np.array(Image.open(B_img).convert(""RGB""))\n        if self.transform:\n            augmentations = self.transform(image=B_img,\n                                           image0=A_img)\n            B_img = augmentations[""image""]\n            A_img = augmentations[""image0""]\n        return A_img, B_img\nThe LoadData()  class is inherited from the Dataset  class in PyTorch. The two lists \nroot_A  and root_B  contain folders of images in domains A and B, respectively. The \nclass loads up images in the two domains and produces a pair of images, one from The two folders \nroot_A and root_B \nare where the \nimages in the two \ndomains are stored\nLoads all images \nin each domain\nDefines a method to \ncount the length of \nthe dataset\nDefines a method to \naccess individual \nelements in each \ndomain\n130 chapter  6 CycleGAN: Converting blond hair to black hair\ndomain A and one from domain B so that we can use the pair to train the CycleGAN \nmodel later.\nAs we did in previous chapters, we create a data iterator with batches to improve \ncomputational efficiency, memory usage, and optimization dynamics in the training \nprocess.\nListing 6.3    Processing the black and blond hair images for training\ntransforms = albumentations.Compose(\n    [albumentations.Resize(width=256, height=256),    \n        albumentations.HorizontalFlip(p=0.5),\n        albumentations.Normalize(mean=[0.5, 0.5, 0.5],\n        std=[0.5, 0.5, 0.5],max_pixel_value=255),    \n        ToTensorV2()],\n    additional_targets={""image0"": ""image""}) \ndataset = LoadData(root_A=[""files/black/""],\n    root_B=[""files/blond/""],\n    transform=transforms)    \nloader=DataLoader(dataset,batch_size=1,\n    shuffle=True, pin_memory=True)    \nWe first define an instance of the Compose()  class in the albumentations  library \n(which is famous for fast and flexible image augmentations) and call it transforms . \nThe class transforms the images in several ways: it resizes images to 256 by 256 pixels \nand normalizes the values to the range –1 to 1. The HorizontalFlip()  argument in \nlisting 6.3 creates a mirror image of the original image in the training set. Horizontal \nflipping is a simple yet powerful augmentation technique that enhances the diversity \nof training data, helping models generalize better and become more robust. The aug -\nmentations and increase in size boost the performance of the CycleGAN model and \nmake the generated images realistic. \nWe then apply the LoadData()  class to the black and blond hair images. We set the \nbatch size to 1 since the images have a large file size, and we use a pair of images to train \nthe model in each iteration. Setting the batch size to more than 1 may result in your \nmachine running out of memory. \n6.3 Building a CycleGAN model\nWe’ll build a CycleGAN model from scratch in this section. We’ll take great care to \nmake our CycleGAN model general so that it can be trained using any dataset with two \ndomains of images. As a result, we’ll use A and B to denote the two domains instead of, \nfor example, black and blond hair images. As an exercise, you’ll train the same Cycle -\nGAN model by using the eyeglasses dataset that you used in chapter 5. This helps you \napply the skills you learned in this chapter to other real-world applications by using a \ndifferent dataset.Resizes the images to \n256 by 256 pixels\nNormalizes the images \nto the range of - 1 to 1\nApplies the LoadData() \nclass on the images\nCreates a data iterator \nfor training\n 131 Building a CycleGAN model\n6.3.1 Creating two discriminators\nEven though CycleGAN has two discriminators, they are identical ex ante. Therefore, \nwe’ll create one single Discriminator()   class and then instantiate the class twice: one \ninstance is discriminator A and the other discriminator B. The two domains in Cycle -\nGAN are symmetric, and it doesn’t matter which domain we call domain A: images with \nblack hair or images with blond hair. \nOpen the file ch06util.py  you just downloaded. In it, I have defined the \nDiscriminator()  class.\nListing 6.4    Defining the Discriminator() class in CycleGAN \nclass Discriminator(nn.Module):\n    def __init__(self, in_channels=3, features=[64,128,256,512]):\n        super().__init__()\n        self.initial = nn.Sequential(\n            nn.Conv2d(in_channels,features[0],    \n                kernel_size=4,stride=2,padding=1,\n                padding_mode=""reflect""),\n            nn.LeakyReLU(0.2, inplace=True))\n        layers = []\n        in_channels = features[0]\n        for feature in features[1:]:    \n            layers.append(Block(in_channels, feature, \n                stride=1 if feature == features[-1] else 2))\n            in_channels = feature\n        layers.append(nn.Conv2d(in_channels,1,kernel_size=4,    \n                stride=1,padding=1,padding_mode=""reflect""))\n        self.model = nn.Sequential(*layers)\n    def forward(self, x):\n        out = self.model(self.initial(x))\n        return torch.sigmoid(out)    \nThe previous code listing defines the discriminator network. The architecture is simi -\nlar to the discriminator network in chapter 4 and the critic network in chapter 5. The \nmain components are five Conv2d  layers. We apply the sigmoid activation function on \nthe last layer because the discriminator performs a binary classification problem. The \ndiscriminator takes a three-channel color image as input and produces a single num -\nber between 0 and 1, which can be interpreted as the probability that the input image \nis a real image in the domain.  \nThe padding_mode=""reflect""  argument we used in listing 6.4 means the padding \nadded to the input tensor is a reflection of the input tensor itself. Reflect padding helps \nin preserving the edge information by not introducing artificial zero values at the bor -\nders. It creates smoother transitions at the boundaries of the input tensor, which is ben -\neficial for differentiating images in different domains in our setting.The first Conv2d layer \nhas 3 input channels and \n64 output channels.\nThree more Conv2d layers \nwith 126, 256, and 5 12 output \nchannels, respectively\nThe last Conv2d \nlayer has 5 12 \ninput channels \nand 1 output \nchannel.\nApplies the sigmoid activation \nfunction on the output so it can \nbe interpreted as a probability\n132 chapter  6 CycleGAN: Converting blond hair to black hair\nWe then create two instances of the class and call them disc_A  and disc_B , \nrespectively:\nfrom utils.ch06util import Discriminator, weights_init    \nimport torch\ndevice = ""cuda"" if torch.cuda.is_available() else ""cpu""\ndisc_A = Discriminator().to(device)\ndisc_B = Discriminator().to(device)    \nweights_init(disc_A)\nweights_init(disc_B)    \nIn the local module ch06util , we also defined a weights_init()  function to initial -\nize model weights. The function is defined similarly to the one in chapter 5. We then \ninitialize weights in the two newly created discriminators, disc_A  and disc_B . \nNow that we have two discriminators, we’ll create two generators next.\n6.3.2 Creating two generators\nSimilarly, we define a single Generator()  class in the local module and instantiate \nthe class twice: one instance is generator A, and the other is generator B. In the file \nch06util.py  you just downloaded, we have defined the Generator()  class.\nListing 6.5    The Generator() class in CycleGAN\nclass Generator(nn.Module):\n    def __init__(self, img_channels, num_features=64,\n                 num_residuals=9):\n        super().__init__()     \n        self.initial = nn.Sequential(\n            nn.Conv2d(img_channels,num_features,kernel_size=7,\n                stride=1,padding=3,padding_mode=""reflect"",),\n            nn.InstanceNorm2d(num_features),\n            nn.ReLU(inplace=True))\n        self.down_blocks = nn.ModuleList(\n            [ConvBlock(num_features,num_features*2,kernel_size=3,\n                       stride=2, padding=1),\n            ConvBlock(num_features*2,num_features*4,kernel_size=3,    \n                stride=2,padding=1)])\n        self.res_blocks = nn.Sequential(    \n            *[ResidualBlock(num_features * 4) \n            for _ in range(num_residuals)])\n        self.up_blocks = nn.ModuleList(\n            [ConvBlock(num_features * 4, num_features * 2,\n                    down=False, kernel_size=3, stride=2,\n                    padding=1, output_padding=1),\n                ConvBlock(num_features * 2, num_features * 1,    \n                    down=False,kernel_size=3, stride=2,\n                    padding=1, output_padding=1)])\n        self.last = nn.Conv2d(num_features * 1, img_channels,\n            kernel_size=7, stride=1,\n            padding=3, padding_mode=""reflect"")\n        Imports the \nDiscriminator \nclass from the \nlocal module\nCreates two instances of \nthe Discriminator class\nInitializes weights\nThree \nConv2d \nlayers\nNine residual blocks\nTwo \nupsampling \nblocks\n 133 Building a CycleGAN model\n    def forward(self, x):\n        x = self.initial(x)\n        for layer in self.down_blocks:\n            x = layer(x)\n        x = self.res_blocks(x)\n        for layer in self.up_blocks:\n            x = layer(x)\n        return torch.tanh(self.last(x))    \nThe generator network consists of several Conv2d  layers, followed by nine residual \nblocks (which I’ll explain in detail later). After that, the network has two upsampling \nblocks that consist of a ConvTranspose2d  layer, an InstanceNorm2d  layer, and a ReLU  \nactivation. As we have done in previous chapters, we use the tanh activation function \nat the output layer, so the output pixels are all in the range of –1 to 1, the same as the \nimages in the training set.\nThe residual block in the generator is defined in the local module as follows:\nclass ConvBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, \n                 down=True, use_act=True, **kwargs):\n        super().__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, \n                      padding_mode=""reflect"", **kwargs)\n            if down\n            else nn.ConvTranspose2d(in_channels, \n                                    out_channels, **kwargs),\n            nn.InstanceNorm2d(out_channels),\n            nn.ReLU(inplace=True) if use_act else nn.Identity())\n    def forward(self, x):\n        return self.conv(x)\nclass ResidualBlock(nn.Module):\n    def __init__(self, channels):\n        super().__init__()\n        self.block = nn.Sequential(\n            ConvBlock(channels,channels,kernel_size=3,padding=1),\n            ConvBlock(channels,channels,\n                      use_act=False, kernel_size=3, padding=1))\n    def forward(self, x):\n        return x + self.block(x)\nA residual connection is a concept in deep learning, particularly in the design of deep \nneural networks. You’ll see it quite often later in this book. It’s a technique used to \naddress the problem of vanishing gradients, which often occurs in very deep networks. \nIn a residual block, which is the basic unit of a network with residual connections, the \ninput is passed through a series of transformations (like convolution, activation, and \nbatch or instance normalization) and then added back to the output of these trans -\nformations. Figure 6.4 provides a diagram of the architecture of the residual block \ndefined previously. Applies tanh activation \non the output\n134 chapter  6 CycleGAN: Converting blond hair to black hair\nConv2d\nInstanceNorm2d\nConv2d\nInstanceNorm2dInput: x\nTransformations\nf(x)\nOutput: x+f(x)The input is\nadded back to\nthe output of\ntransformations\nThe output of the\nresidual block is x+f(x)ReLU\nFigure 6.4    The architecture of a residual block. The input x is passed through a series of \ntransformations (two sets of Conv2d layer and InstanceNorm2d layer and a ReLU activation). The input \nx is then added back to the output of these transformations, f(x). The output of the residual block is \ntherefore x + f(x).\nThe transformations in each residual block are different. In this example, the input \nx is passed through two sets of Conv2d  layer and InstanceNorm2d  layer and a ReLU \nactivation in between. The input x is then added back to the output of these transfor -\nmations, f(x), to form the final output, x+f(x)—hence the name residual connection. \nNext, we create two instances of the Generator() class and call one of them gen_A  \nand the other gen_B :\nfrom utils.ch06util import Generator\ngen_A = Generator(img_channels=3, num_residuals=9).to(device)\ngen_B = Generator(img_channels=3, num_residuals=9).to(device)\nweights_init(gen_A)\nweights_init(gen_B)\nWhen training the model, we’ll use the mean absolute error (i.e., L1 loss) to measure \nthe cycle consistency loss. We’ll use the mean squared error (i.e., L2 loss) to gauge the \nadversarial loss. L1 loss is often used if the data are noisy and have many outliers since \nit punishes extreme values less than the L2 loss. Therefore, we import the following \nloss functions:\nimport torch.nn as nn\nl1 = nn.L1Loss()\nmse = nn.MSELoss()",15141
60-6.4 UsingCycleGAN to translate between black and blond hair.pdf,60-6.4 UsingCycleGAN to translate between black and blond hair,,0
61-6.4.1 Training a CycleGAN to translate between black and blond hair.pdf,61-6.4.1 Training a CycleGAN to translate between black and blond hair,"135 Using CycleGAN to translate between black and blond hair\ng_scaler = torch.cuda.amp.GradScaler()\nd_scaler = torch.cuda.amp.GradScaler()\nBoth L1 and L2 losses are calculated at the pixel level. The original image has a shape \nof (3, 256, 256) and so is the fake image. To calculate the losses, we first calculate the \ndifference (absolute value of this difference for L1 loss and the squared value of this \ndifference for L2 loss) between the corresponding pixel values between two images at \neach of the 3 × 256 × 256 = 196608 positions and average them over the positions. \nWe’ll use PyTorch’s automatic mixed precision package torch.cuda.amp  to speed \nup training. The default data type in PyTorch tensors is float32 , a 32-bit floating-point \nnumber, which takes up twice as much memory as a 16-bit floating number, float16 . \nOperations on the former are slower than those on the latter. There is a trade-off \nbetween precision and computational costs. Which data type to use depends on the task \nat hand. torch.cuda.amp  provides an automatic mixed precision, where some opera -\ntions use float32  and others float16 . Mixed precision tries to match each operation \nto its appropriate data type to speed up training. \nAs we have done in chapter 4, we’ll use the Adam optimizer for both the discrimina -\ntors and the generators:\nlr = 0.00001\nopt_disc = torch.optim.Adam(list(disc_A.parameters()) + \n  list(disc_B.parameters()),lr=lr,betas=(0.5, 0.999))\nopt_gen = torch.optim.Adam(list(gen_A.parameters()) + \n  list(gen_B.parameters()),lr=lr,betas=(0.5, 0.999))\nNext, we’ll train the CycleGAN model by using images with black or blond hair.\n6.4 Using CycleGAN to translate between black and blond hair\nNow that we have the training data and the CycleGAN model, we’ll train the model \nby using images with black or blond hair. As with all GAN models, we’ll discard the \ndiscriminators after training. We’ll use the two trained generators to convert black hair \nimages to blond hair ones and convert blond hair images to black hair ones.\n6.4.1 Training a CycleGAN to translate between black and blond hair\nAs we explained in chapter 4, we’ll use visual inspections to determine when to stop \ntraining. To that end, we create a function to test what the real images look like and \nwhat the corresponding generated images look like so that we can compare the two \nto visually inspect the effectiveness of the model. In the local module ch06util , we \ndefine a test()  function:\ndef test(i,A,B,fake_A,fake_B):\n    save_image(A*0.5+0.5,f""files/A{i}.png"")\n    save_image(B*0.5+0.5,f""files/B{i}.png"")    \n    save_image(fake_A*0.5+0.5,f""files/fakeA{i}.png"")\n    save_image(fake_B*0.5+0.5,f""files/fakeB{i}.png"")    Real images in domains \nA and B, saved in a \nlocal folder\nThe corresponding fake \nimages in domains A \nand B, created by the \ngenerators in batch i\n136 chapter  6 CycleGAN: Converting blond hair to black hair\nWe save four images after every 100 batches of training. We save real images and the \ncorresponding fake images in the two domains in the local folder so we can periodi -\ncally check the generated images and compare them with the real ones to assess the \nprogress of training. We made the function general so that it can be applied to images \nfrom any two domains. \nFurther, we define a train_epoch()  function in the local module ch06util  to train \nthe discriminators and the generators for an epoch. The following listing highlights the \ncode we use to train the two discriminators.\nListing 6.6    Training the two discriminators in CycleGAN \ndef train_epoch(disc_A, disc_B, gen_A, gen_B, loader, opt_disc,\n        opt_gen, l1, mse, d_scaler, g_scaler,device):\n    loop = tqdm(loader, leave=True)\n    for i, (A,B) in enumerate(loop):    \n        A=A.to(device)\n        B=B.to(device)\n        with torch.cuda.amp.autocast():    \n            fake_A = gen_A(B)\n            D_A_real = disc_A(A)\n            D_A_fake = disc_A(fake_A.detach())\n            D_A_real_loss = mse(D_A_real, \n                                torch.ones_like(D_A_real))\n            D_A_fake_loss = mse(D_A_fake,\n                                torch.zeros_like(D_A_fake))\n            D_A_loss = D_A_real_loss + D_A_fake_loss\n            fake_B = gen_B(A)\n            D_B_real = disc_B(B)\n            D_B_fake = disc_B(fake_B.detach())\n            D_B_real_loss = mse(D_B_real,\n                                torch.ones_like(D_B_real))\n            D_B_fake_loss = mse(D_B_fake,\n                                torch.zeros_like(D_B_fake))\n            D_B_loss = D_B_real_loss + D_B_fake_loss\n            D_loss = (D_A_loss + D_B_loss) / 2    \n        opt_disc.zero_grad()\n        d_scaler.scale(D_loss).backward()\n        d_scaler.step(opt_disc)\n        d_scaler.update()\n        …\nWe use the detach() method here to remove gradients in tensors fake_A  and fake_B  \nto reduce memory and speed up computations. The training for the two discriminators \nis similar to what we have done in chapter 4, with a couple of differences. First, instead \nof having just one discriminator, we have two discriminators here: one for images in \ndomain A and one for images in domain B. The total loss for the two discriminators is \nthe simple average of the adversarial losses of the two discriminators. Second, we use \nthe PyTorch automatic mixed precision package to speed up training, reducing the \ntraining time by more than 50%. Iterates through all pairs of \nimages in the two domains\nUses PyTorch automatic \nmixed precision package to \nspeed up training\nThe total loss for the two \ndiscriminators is the simple \naverage of the adversarial \nlosses to the two \ndiscriminators.\n 137 Using CycleGAN to translate between black and blond hair\nWe simultaneously train the two generators in the same iteration. The following list -\ning highlights the code we use to train the two generators.\nListing 6.7    Training the two generators in CycleGAN \ndef train_epoch(disc_A, disc_B, gen_A, gen_B, loader, opt_disc,\n        opt_gen, l1, mse, d_scaler, g_scaler,device):\n        …\n        with torch.cuda.amp.autocast():\n            D_A_fake = disc_A(fake_A)\n            D_B_fake = disc_B(fake_B)\n            loss_G_A = mse(D_A_fake, torch.ones_like(D_A_fake))\n            loss_G_B = mse(D_B_fake, torch.ones_like(D_B_fake))    \n            cycle_B = gen_B(fake_A)\n            cycle_A = gen_A(fake_B)\n            cycle_B_loss = l1(B, cycle_B)\n            cycle_A_loss = l1(A, cycle_A)    \n            G_loss=loss_G_A+loss_G_B+cycle_A_loss*10+cycle_B_loss*10    \n        opt_gen.zero_grad()\n        g_scaler.scale(G_loss).backward()\n        g_scaler.step(opt_gen)\n        g_scaler.update()\n        if i % 100 == 0:\n            test(i,A,B,fake_A,fake_B)    \n        loop.set_postfix(D_loss=D_loss.item(),G_loss=G_loss.item())\nThe training for the two generators is different from what we have done in chapter 4 in \ntwo important ways. First, instead of having just one generator, we train two generators \nsimultaneously here. Second, the total loss for the two generators is the weighted sum \nof adversarial losses and cycle consistency losses, and we weigh the latter 10 times more \nthan the former loss. However, if you change the value 10 to other numbers such as 9 \nor 12, you’ll get similar results. \nThe cycle consistency loss is the mean absolute error between the original image and \nthe fake image that’s translated back to the original domain. \nNow that we have everything ready, we’ll start the training loop:\nfrom utils.ch06util import train_epoch\nfor epoch in range(1):\n    train_epoch(disc_A, disc_B, gen_A, gen_B, loader, opt_disc,\n    opt_gen, l1, mse, d_scaler, g_scaler, device)    \ntorch.save(gen_A.state_dict(), ""files/gen_black.pth"")\ntorch.save(gen_B.state_dict(), ""files/gen_blond.pth"")    Adversarial \nlosses to \nthe two \ngenerators\nCycle consistency losses \nfor the two generators\nThe total loss for the two \ngenerators is the weighted \nsum of adversarial losses \nand cycle consistency losses.\nGenerates images for visual \ninspection after every 100 \nbatches of training\nTrains the \nCycleGAN for \none epoch \nusing the \nblack and \nblond hair \nimages\nSaves the trained \nmodel weights",8336
62-6.4.2 Round-trip conversions of black hair images and blond hair images.pdf,62-6.4.2 Round-trip conversions of black hair images and blond hair images,"138 chapter  6 CycleGAN: Converting blond hair to black hair\nThe preceding training takes a couple of hours if you use GPU training. It may take \na whole day otherwise. If you don’t have the computing resources to train the model, \ndownload the pretrained generators from my website: https: //gattonweb.uky.edu/\nfaculty/lium/ml/hair.zip . Unzip the file and place the files gen_black.pth  and gen_\nblond.pth  in the folder /files/ on your computer. You’ll be able to convert between \nblack hair images and blond hair ones in the next subsection.\nExercise 6.1\nWhen training the CycleGAN model, we assume that domain A contains images with \nblack hair and domain B contains images with blond hair. Modify the code in listing 6.2 \nso that domain A contains images with blond hair and domain B contains images with \nblack hair. \n6.4.2 Round-trip conversions of black hair images and blond hair images\nDue to the high quality and the abundant quantity of the training dataset, we have \ntrained the CycleGAN with great success. We’ll not only convert between images with \nblack hair and images with blond hair, but we’ll also conduct round-trip conversions. \nFor example, we’ll convert images with black hair to images with blond hair and then \nconvert them back to images with black hair. That way, we can compare the original \nimages with the generated images in the same domain after a round trip and see the \ndifference. \nThe following listing performs conversions of images between the two domains as \nwell as round-trip conversions of images in each domain.\nListing 6.8    Round-trip conversions of images with black or blond hair\ngen_A.load_state_dict(torch.load(""files/gen_black.pth"",\n    map_location=device))\ngen_B.load_state_dict(torch.load(""files/gen_blond.pth"",\n    map_location=device))\ni=1\nfor black,blond in loader:\n    fake_blond=gen_B(black.to(device))\n    save_image(black*0.5+0.5,f""files/black{i}.png"")    \n    save_image(fake_blond*0.5+0.5,f""files/fakeblond{i}.png"")   \n    fake2black=gen_A(fake_blond)\n    save_image(fake2black*0.5+0.5,\n        f""files/fake2black{i}.png"")    \n    fake_black=gen_A(blond.to(device))\n    save_image(blond*0.5+0.5,f""files/blond{i}.png"")    \n    save_image(fake_black*0.5+0.5,f""files/fakeblack{i}.png"")\n    fake2blond=gen_B(fake_black)\n    save_image(fake2blond*0.5+0.5,Original image with \nblack hair\nA fake image with black \nhair after a round trip\nOriginal image with \nblond hair\n 139 Using CycleGAN to translate between black and blond hair\n        f""files/fake2blond{i}.png"")    \n    i=i+1\n    if i>10:\n        break\nWe have saved six sets of images in your local folder /files/. The first set is the origi -\nnal images with black hair. The second set is the fake blond images produced by the \ntrained blond hair generator: the images are saved as fakeblond0.png , fakeblond1  \n.png , and so on. The third set is the fake images with black hair after a round trip: we \nfeed the fake images we just created to the trained black hair generator to obtain fake \nimages with black hair. They are saved as fake2black0.png , fake2black1.png , and \nso on. Figure 6.5 shows the three sets of images.\nFigure 6.5    A round-trip conversion of images with black hair. Images in the top row are the original \nimages with black hair from the training set. Images in the middle row are the corresponding fake images \nwith blond hair, produced by the trained blond hair generator. Images in the bottom row are fake images \nwith black hair after a round trip: we feed the images in the middle row to the trained black hair generator \nto create fake images with black hair.\nThere are three rows of images in figure 6.5. The top row displays original images with \nblack hair from the training set. The middle row displays fake blond hair images pro -\nduced by the trained blond hair. The bottom row contains fake black hair images after \na round-trip conversion: the images look almost identical to the ones in the top row! \nOur trained CycleGAN model works extremely well.\nThe fourth set of images in the local folder /files/ are the original images with \nblond hair. The fifth set is the fake image produced by the trained black hair generator. \nFinally, the sixth set contains fake images with blond hair after a round trip. Figure 6.6 \ncompares these three sets of images.  A fake image with blond \nhair after a round trip\n140 chapter  6 CycleGAN: Converting blond hair to black hair\nFigure 6.6    A round-trip conversion of images with blond hair. Images in the top row are the original \nimages with blond hair from the training set. Images in the middle row are the corresponding fake images \nwith black hair, produced by the trained black hair generator. Images in the bottom row are fake images \nwith blond hair after a round-trip conversion: we feed the images in the middle row to the trained blond \nhair generator to create fake images with blond hair.\nIn figure 6.6, fake black hair images produced by the trained black hair generator are \nshown in the middle row: they have black hair on the same human faces as the top \nrow. Fake blond hair images after a round trip are shown in the bottom row: they look \nalmost identical to the original blond hair images in the top row. \nExercise 6.2\nThe CycleGAN model is general and can be applied to any training dataset with two \ndomains of images. Train the CycleGAN model using the eyeglasses images that you \ndownloaded in chapter 5. Use images with glasses as domain A and images without \nglasses as domain B. Then use the trained CycleGAN to add and remove eyeglasses from \nimages (i.e., translating images between the two domains). An example implementation \nand results are in the book’s GitHub repository. \nSo far, we have focused on one type of generative model, GANs. In the next chapter, \nyou’ll learn to use another type of generative model, variational autoencoders (VAEs), \nto generate high-resolution images. You’ll learn the advantages and disadvantages of \nVAEs compared to GANs. More importantly, you’ll learn the encoder-decoder archi -\ntecture in VAEs. The architecture is widely used in generative models, including Trans -\nformers, which we’ll study later in the book. \n 141 Summary\nSummary\n¡ CycleGAN can translate images between two domains without paired examples. \nIt consists of two discriminators and two generators. One generator converts \nimages in domain A to domain B while the other generator converts images in \ndomain B to domain A. The two discriminators classify if a given image is from a \nspecific domain. \n¡ CycleGAN uses a cycle consistency loss function to ensure the original image can \nbe reconstructed from the transformed image, encouraging the preservation of \nkey features.\n¡ A properly constructed CycleGAN model can be applied to any dataset with \nimages from two domains. The same model can be trained with different datasets \nand be used to translate images in different domains. \n¡ When we have abundant high-quality training data, the trained CycleGAN can \nconvert images in one domain to another and convert them back to the original \ndomain. The images after a round-trip conversion can potentially look almost \nidentical to the original images.",7299
63-7 Image generation with variational autoencoders.pdf,63-7 Image generation with variational autoencoders,"1427Image generation \nwith variational \nautoencoders\nThis chapter covers\n¡ Autoencoders vs. variational autoencoders\n¡ Building and training an Autoencoder to  \n reconstruct handwritten digits\n¡ Building and training a variational autoencoder to  \n generate human face images\n¡ Performing encoding arithmetic and interpolation  \n with a trained variational autoencoder\nSo far, you have learned how to generate shapes, numbers, and images, all by using \ngenerative adversarial networks (GANs). In this chapter, you’ll learn to create \nimages by using another generative model: variational autoencoders (VAEs). You’ll \nalso learn the practical uses of VAEs by performing encoding arithmetic and encod -\ning interpolation. \nTo know how VAEs work, we first need to understand autoencoders (AEs). AEs \nhave a dual-component structure: an encoder and a decoder. The encoder com -\npresses the data into an abstract representation in a lower-dimensional space (the \nlatent space), and the decoder decompresses the encoded information and recon -\nstructs the data. The primary goal of an AE is to learn a compressed representation \nof the input data, focusing on minimizing the reconstruction error—the difference \nbetween the original input and its reconstruction (at the pixel level, as we have seen \n 143  \nin chapter 6 when calculating cycle consistency loss). The encoder-decoder architec -\nture is a cornerstone in various generative models, including Transformers, which \nyou’ll explore in detail in the latter half of this book. For example, in chapter 9, you’ll \nbuild a Transformer for machine language translation: the encoder converts an English \nphrase into an abstract representation while the decoder constructs the French transla -\ntion based on the compressed representation generated by the encoder. Text-to-image \nTransformers like DALL-E 2 and Imagen also utilize an AE architecture in their design. \nThis involves first encoding an image into a compact, low-dimensional probability dis -\ntribution. Then, they decode from this distribution. Of course, what constitutes an \nencoder and a decoder is different in different models.    \nYour first project in this chapter involves constructing and training an AE from \nscratch to generate handwritten digits. You’ll use 60,000 grayscale images of handwrit -\nten digits (0 to 9), each with a size of 28 × 28 = 784 pixels, as the training data. The \nencoder in the AE compresses each image into a deterministic vector representation \nwith only 20 values. The decoder in the AE reconstructs the image with the aim of min -\nimizing the difference between the original image and the reconstructed image. This \nis achieved by minimizing the mean absolute error between the two images at the pixel \nlevel. The end result is an AE capable of generating handwritten digits almost identical \nto those in the training set. \nWhile AEs are good at replicating the input data, they often falter in generating new \nsamples that are not present in the training set. More importantly, AEs are not good at \ninput interpolation: they often fail to generate intermediate representations between \ntwo input data points. This leads us to VAEs. VAEs differ from AEs in two critical ways. \nFirst, while an AE encodes each input into a specific point in the latent space, a VAE \nencodes it into a probability distribution within this space. Second, an AE focuses solely \non minimizing the reconstruction error, whereas a VAE learns the parameters of the \nprobability distribution for latent variables, minimizing a loss function that includes both \nreconstruction loss and a regularization term, the Kullback–Liebler (KL) divergence. \nThe KL-divergence encourages the latent space to approximate a certain distribu -\ntion (a normal distribution in our example) and ensures that the latent variables don’t \njust memorize the training data but rather capture the underlying distribution. It helps \nin achieving a well-structured latent space where similar data points are mapped closely \ntogether, making the space continuous and interpretable. As a result, we can manip -\nulate the encodings to achieve new outcomes, which makes encoding arithmetic and \ninput interpolation possible in VAEs.\nIn the second project in this chapter, you’ll build and train a VAE from the ground \nup to generate human face images. Here, your training set comprises eyeglasses images \nthat you downloaded in chapter 5. The VAE’s encoder compresses an image of size 3 × \n256 × 256 = 196,608 pixels into a 100-value probabilistic vector, each following a normal \ndistribution. The decoder then reconstructs the image based on this probabilistic vec -\ntor. The trained VAE can not only replicate human faces from the training set but also \ngenerate novel ones. chapter  7 Image generation with variational autoencoders\n144 chapter  7 Image generation with variational autoencoders\nYou’ll learn how to conduct encoding arithmetic and input interpolation in VAEs. \nYou’ll manipulate the encoded representations (latent vectors) of different inputs to \nachieve specific outcomes (i.e., with or without certain characteristics in images) when \ndecoded. The latent vectors control different characteristics in the decoded images \nsuch as gender, whether there are eyeglasses in an image, and so on. For example, you \ncan first obtain the latent vectors for men with glasses (z1), women with glasses (z2), \nand women without glasses (z3). You then calculate a new latent vector, z4 = z1 – z2 + \nz3. Since both z1 and z2 lead to eyeglasses in images when decoded, z1 – z2 cancels out \nthe eyeglasses feature in the resulting image. Similarly, since both z2 and z3 lead to a \nfemale face, z3 – z2 cancels out the female feature in the resulting image. Therefore, if \nyou decode z4 = z1 – z2 + z3 with the trained VAE, you’ll get an image of a man without \nglasses.\nYou’ll also create a series of images transitioning from a woman with glasses to a \nwoman without glasses by varying the weight assigned to the latent vectors z1 and z2. \nThese exercises exemplify the versatility and creative potential of VAEs in the field of \ngenerative models.\nCompared to GANs, which we studied in the last few chapters, AEs and VAEs have \na simple architecture and are easy to construct. Further, AEs and VAEs are generally \neasier and more stable to train relative to GANs. However, images generated by AEs and \nVAEs tend to be blurrier compared to those generated by GANs. GANs excel in gen -\nerating high-quality, realistic images but suffer from training difficulties and resource \nintensiveness. The choice between GANs and VAEs largely depends on the specific \nrequirements of the task at hand, including the desired quality of the output, computa -\ntional resources available, and the importance of having a stable training process. \nVAEs have a wide range of practical applications in the real world. Consider, for \ninstance, that you run an eyewear store and have successfully marketed a new style of \nmen’s glasses online. Now, you wish to target the female market with the same style but \nlack images of women wearing these glasses, and you face high costs for a professional \nphoto shoot. Here’s where VAEs come into play: you can combine existing images of \nmen wearing the glasses with pictures of both men and women without glasses. This way, \nyou can create realistic images of women sporting the same eyewear style, as illustrated \nin figure 7.1, through encoding arithmetic, a technique you’ll learn in this chapter. \nFigure 7.1    Generating images of women with glasses by performing encoding arithmetic",7708
64-7.1 An overview of AEs.pdf,64-7.1 An overview of AEs,,0
65-7.2 Building and training an AE to generate digits.pdf,65-7.2 Building and training an AE to generate digits,"145 An overview of AEs\nIn another scenario, suppose your store offers eyeglasses with dark and light frames, \nboth of which are popular. You want to introduce a middle option with frames of an \nintermediate shade. With VAEs, through a method called encoding interpolation, you \ncan effortlessly generate a smooth transition series of images, as shown in figure 7.2. \nThese images would vary from dark to light-framed glasses, offering customers a visual \nspectrum of choices. \nFigure 7.2    Generating a series of images that transition from glasses with dark frames to those with \nlight frames\nThe use of VAEs is not limited to eyeglasses; it extends to virtually any product category, \nbe it clothing, furniture, or food. The technology provides a creative and cost-effective \nsolution for visualizing and marketing a wide range of products. Furthermore, although \nimage generation is a prominent example, VAEs can be applied to many other types of \ndata, including music and text. Their versatility opens up endless possibilities in terms \nof practical use!\n7.1 An overview of AEs\nThis section discusses what an AE is and its basic structure. For you to have a deep \nunderstanding of the inner workings of AEs, you’ll build and train an AE to generate \nhandwritten digits as your first project in this chapter. This section provides an over -\nview of an AE’s architecture and a blueprint for completing the first project.  \n7.1.1 What is an AE?\nAEs are a type of neural network used in unsupervised learning that are particularly \neffective for tasks like image generation, compression, and denoising. An AE consists \nof two main parts: an encoder and a decoder. The encoder compresses the input into \na lower-dimensional representation (latent space), and the decoder reconstructs the \ninput from this representation.\nThe compressed representation, or latent space, captures the most important fea -\ntures of the input data. In image generation, this space encodes crucial aspects of the \nimages that the network has been trained on. AEs are useful for their efficiency in learn -\ning data representations and their ability to work with unlabeled data, making them \nsuitable for tasks like dimensionality reduction and feature learning. One challenge \nwith AEs is the risk of losing information in the encoding process, which can lead to \nless accurate reconstructions. Using deeper architectures with multiple hidden layers \ncan help in learning more complex and abstract representations, potentially mitigating \n146 chapter  7 Image generation with variational autoencoders\ninformation loss in AEs. Also, training AEs to generate high-quality images can be com -\nputationally intensive and requires large datasets.\nAs we mentioned in chapter 1, the best way to learn something is to create it from \nscratch. To that end, you’ll learn to create an AE to generate handwritten digits in the \nfirst project in this chapter. The next subsection provides a blueprint for how to do that. \n7.1.2 Steps in building and training an AE\nImagine that you must build and train an AE from the ground up to generate grayscale \nimages of handwritten digits so that you acquire the skills needed to use AEs for more \ncomplicated tasks such as color image generation or dimensionality reduction. How \nshould you go about this task?\nFigure 7.3 provides a diagram of the architecture of an AE and the steps involved in \ntraining an AE to generate handwritten digits. \n...Original \nimages \nEncoded data\n(z1, z2, ..., z 20)Step 5: Feedback as measured by reconstruction loss\n(mean squared error at the pixel level)\nStep 1 Step 2 Step 3 Step 4\nDecoder\n...Reconstructed \nimages\nShape (28, 28) Shape (28, 28)Encoder\nLatent space\nFigure 7.3    The architecture of an AE and the steps to train one to generate handwritten digits. An \nAE consists of an encoder (middle left) and a decoder (middle right). In each iteration of training, \nimages of handwritten digits are fed to the encoder (step 1). The encoder compresses the images to \ndeterministic points in the latent space (step 2). The decoder takes the encoded vectors (step 3) from \nthe latent space and reconstructs the images (step 4). The AE adjusts its parameters to minimize the \nreconstruction loss, the difference between the originals and the reconstructions (step 5).  \nAs you can see from the figure, the AE has two main parts: an encoder (middle left) that \ncompresses images of handwritten digits into vectors in the latent space and a decoder \n(middle right) that reconstructs these images based on the encoded vectors. Both the \nencoder and decoder are deep neural networks that can potentially include different \ntypes of layers such as dense layers, convolutional layers, transposed convolutional \nlayers, and so on. Since our example involves grayscale images of handwritten \ndigits, we’ll use only dense layers. However, AEs can also be used to generate higher-\nresolution color images; for those tasks, convolutional neural networks (CNNs) are",5068
66-7.2.2 Building and training an AE.pdf,66-7.2.2 Building and training an AE,"147 Building and training an AE to generate digits\nusually included in encoders and decoders. Whether to use CNNs in AEs depends on \nthe resolution of the images you want to generate. \nWhen an AE is built, the parameters in it are randomly initialized. We need to obtain \na training set to train the model: PyTorch provides 60,000 grayscale images of handwrit -\nten digits, evenly distributed among the 10 digits 0 to 9. The left side of figure 7.3 shows \nthree examples, and they are images of digits 0, 1, and 9, respectively. In the first step \nin the training loop, we feed images in the training set to the encoder. The encoder \ncompresses the images to 20-value vectors in the latent space (step 2). There is nothing \nmagical about the number 20. If you use 25-value vectors in the latent space, you’ll get \nsimilar results. We then feed the vector representations to the decoder (step 3) and \nask it to reconstruct the images (step 4). We calculate the reconstruct loss, which is \nthe mean squared error, over all the pixels, between the original image and the recon -\nstructed image. We then propagate this loss back through the network to update the \nparameters in the encoder and decoder to minimize the reconstruction loss (step 5) \nso that in the next iteration, the AE can reconstruct images closer to the original ones. \nThis process is repeated for many epochs over the dataset. \nAfter the model is trained, you’ll feed unseen images of handwritten digits to the \nencoder and obtain encodings. You then feed the encodings to the decoder to obtain \nreconstructed images. You’ll notice that the reconstructed images look almost identical \nto the originals. The right side of figure 7.3 shows three examples of reconstructed \nimages: they do look similar to the corresponding originals on the left side of the figure. \n7.2 Building and training an AE to generate digits\nNow that you have a blueprint to build and train an AE to generate handwritten digits, \nlet’s dive into the project and implement the steps outlined in the last section.\nSpecifically, in this section, you’ll learn first how to obtain a training set and a test \nset of images of handwritten digits. You’ll then build an encoder and decoder with \ndense layers. You’ll train the AE with the training dataset and use the trained encoder \nto encode images in the test set. Finally, you’ll learn to use the trained decoder to recon -\nstruct images and compare them to the originals. \n7.2.1 Gathering handwritten digits\nYou can download grayscale images of handwritten images using the datasets  package \nin the Torchvision library, similar to how you downloaded images of clothing items in \nchapter 2.  \nFirst, let’s download a training set and a test set:\nimport torchvision\nimport torchvision.transforms as T\ntransform=T.Compose([\n    T.ToTensor()])\ntrain_set=torchvision.datasets.MNIST(root=""."",    Downloads handwritten \ndigits by using the MNIST() \nclass in torchvision.datasets\n148 chapter  7 Image generation with variational autoencoders\n    train=True,download=True,transform=transform)    \ntest_set=torchvision.datasets.MNIST(root=""."",\n    train=False,download=True,transform=transform)    \nInstead of using the FashionMNIST()  class as we did in chapter 2, we use the MNIST()  \nclass here. The train  argument in the class tells PyTorch whether to download the \ntraining set (when the argument is set to True ) or the test set (when the argument \nis set to False ). Before transformation, the image pixels are integers ranging from 0 \nto 255. The ToTensor()  class in the preceding code block converts them to PyTorch \nfloat tensors with values between 0 to 1. There are 60,000 images in the training set and \n10,000 in the test set, evenly distributed among 10 digits, 0 to 9, in each set. \nWe’ll create batches of data for training and testing, with 32 images in each batch:\nimport torch\nbatch_size=32\ntrain_loader=torch.utils.data.DataLoader(\n    train_set,batch_size=batch_size,shuffle=True)\ntest_loader=torch.utils.data.DataLoader(\n    test_set,batch_size=batch_size,shuffle=True)\nNow that we have the data ready, we’ll build and train an AE next. \n7.2.2 Building and training an AE\nAn AE consists of two parts: the encoder and the decoder. We’ll define an AE()  class, as \nshown in the following listing, to represent the AE.\nListing 7.1    Creating an AE to generate handwritten digits\nimport torch.nn.functional as F\nfrom torch import nn\ndevice=""cuda"" if torch.cuda.is_available() else ""cpu""\ninput_dim = 784    \nz_dim = 20    \nh_dim = 200\nclass AE(nn.Module):\n    def __init__(self,input_dim,z_dim,h_dim):\n        super().__init__()\n        self.common = nn.Linear(input_dim, h_dim)\n        self.encoded = nn.Linear(h_dim, z_dim)\n        self.l1 = nn.Linear(z_dim, h_dim)\n        self.decode = nn.Linear(h_dim, input_dim)                \n    def encoder(self, x):    \n        common = F.relu(self.common(x))\n        mu = self.encoded(common)\n        return mu\n    def decoder(self, z):    The train=True argument \nmeans you download the \ntraining set.\nThe train=False argument \nmeans you download the \ntest set.\nThe input to the AE has \n28 × 28 = 784 values in it.\nThe latent variable (encoding) \nhas 20 values in it.\nThe encoder compresses \nimages to latent variables.\nThe decoder reconstructs \nthe images based on \nencodings.\n 149 Building and training an AE to generate digits\n        out=F.relu(self.l1(z))\n        out=torch.sigmoid(self.decode(out))\n        return out\n    def forward(self, x):    \n        mu=self.encoder(x)\n        out=self.decoder(mu)\n        return out, mu\nThe input size is 784 because the grayscale images of handwritten digits have a size of 28 \nby 28 pixels. We flatten the images to 1D tensors and feed them to the AE. The images \nfirst go through the encoder: they are compressed into encodings in a lower dimen -\nsional space. Each image is now represented by a 20-value latent variable. The decoder \nreconstructs the images based on the latent variables. The output from the AE has two \ntensors: out, the reconstructed images, and mu, latent variables (i.e., encodings).  \nNext, we instantiate the AE()  class we defined earlier to create an AE. We also use the \nAdam optimizer during training, as we did in previous chapters: \nmodel = AE(input_dim,z_dim,h_dim).to(device)\nlr=0.00025\noptimizer = torch.optim.Adam(model.parameters(), lr=lr)\nWe define a function plot_digits()  to visually inspect the reconstructed handwrit -\nten digits after each epoch of training, as shown in the following listing.\nListing 7.2    The plot_digits () function to inspect reconstructed images \nimport matplotlib.pyplot as plt\noriginals = []    \nidx = 0\nfor img,label in test_set:\n    if label == idx:\n        originals.append(img)\n        idx += 1\n    if idx == 10:\n        break\ndef plot_digits():\n    reconstructed=[]\n    for idx in range(10):\n        with torch.no_grad():\n            img = originals[idx].reshape((1,input_dim))\n            out,mu = model(img.to(device))    \n        reconstructed.append(out)    \n    imgs=originals+reconstructed\n    plt.figure(figsize=(10,2),dpi=50)\n    for i in range(20):\n        ax = plt.subplot(2,10, i + 1)\n        img=(imgs[i]).detach().cpu().numpy()\n        plt.imshow(img.reshape(28,28),    \n                   cmap=""binary"")\n        plt.xticks([])\n        plt.yticks([])\n    plt.show()  The encoder and \ndecoder form the AE.\nCollects a sample image of \neach digit in the test set\nFeeds the image to the AE \nto obtain a reconstructed \nimage\nCollects the reconstructed \nimage of each original \nimage\nCompares the originals \nto the reconstructed \ndigits visually\n150 chapter  7 Image generation with variational autoencoders\nWe first collect 10 sample images, one representing a different digit, and place them \nin a list, originals . We feed the images to the AE to obtain the reconstructed images. \nFinally, we plot both the originals and the reconstructed images so that we can com -\npare them and assess the performance of the AE periodically.\nBefore training starts, we call the function plot_digits()  to visualize the output:\nplot_digits()\nYou’ll see the output as shown in figure 7.4.\nFigure 7.4    Comparing reconstructed images by the AE with the originals before training starts. The \ntop row shows 10 original images of handwritten digits in the test set. The bottom row shows the \nreconstructed images by the AE before training. The reconstructions are nothing more than pure noise. \nThough we could divide our data into training and validation sets and train the model \nuntil no further improvements are seen on the validation set (as we have done in chap -\nter 2), our primary aim here is to grasp how AEs work, not necessarily to achieve the \nbest parameter tuning. Therefore, we’ll train the AE for 10 epochs.\nListing 7.3    Training the AE to generate handwritten digits\nfor epoch in range(10):\n    tloss=0\n    for imgs, labels in train_loader:    \n        imgs=imgs.to(device).view(-1, input_dim)\n        out, mu=model(imgs)    \n        loss=((out-imgs)**2).sum()    \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        tloss+=loss.item()\n    print(f""at epoch {epoch} toal loss = {tloss/len(train_loader)}"")\n    plot_digits()    \nIn each epoch of training, we iterate through all batches of data in the training set. We \nfeed the original images to the AE to obtain the reconstructed images. We then cal -\nculate the reconstruction loss, which is the mean squared error between the original \nimages and the reconstructed images. Specifically, the reconstruction loss is obtained \nby first calculating the difference between the two images, pixel by pixel, squaring the \nvalues and averaging the squared difference. We adjust the model parameters to min -\nimize the reconstruction loss, utilizing the Adam optimizer, which is a variation of the \ngradient descent method. Iterates through batches \nin the training set\nUses the AE to reconstruct images\nCalculates reconstruct loss as \nmeasured by mean squared error\nVisually inspects the \nperformance of the AE",10265
67-7.2.3 Saving and using the trained AE.pdf,67-7.2.3 Saving and using the trained AE,,0
68-7.4.1 Building a variational AE.pdf,68-7.4.1 Building a variational AE,"151 What are VAEs? \nThe model takes about 2 minutes to train if you are using GPU training. Alterna -\ntively, you can download the trained model from my website: https: //mng.bz/YV6K . \n7.2.3 Saving and using the trained AE\nWe’ll save the model in the local folder on your computer:\nscripted = torch.jit.script(model) \nscripted.save('files/AEdigits.pt') \nTo use it to reconstruct an image of handwritten digits, we load up the model: \nmodel=torch.jit.load('files/AEdigits.pt',map_location=device)\nmodel.eval()\nWe can use it to generate handwritten digits by calling the plot_digits()  function we \ndefined earlier:\nplot_digits ()\nThe output is shown in figure 7.5. \nFigure 7.5    Comparing reconstructed images by the trained AE with the originals. The top row shows 10 \noriginal images of handwritten digits in the test set. The bottom row shows the reconstructed images by \nthe trained AE. The reconstructed images look similar to the original ones. \nThe reconstructed handwritten digits do resemble the original ones, although the \nreconstruction is not perfect. Some information gets lost during the encoding–\ndecoding process. However, compared to GANs, AEs are easy to construct and take \nless time to train. Further, the encoder–decoder architecture is employed by many \ngenerative models. This project will help your understanding of later chapters, \nespecially when we explore Transformers. \n7.3 What are VAEs? \nWhile AEs are good at reconstructing original images, they fail at generating novel \nimages that are unseen in the training set. Further, AEs tend not to map similar inputs \nto nearby points in the latent space. As a result, the latent space associated with an AE \nis neither continuous nor easily interpretable. For example, you cannot interpolate \ntwo input data points to generate meaningful intermediate representations. For these \nreasons, we’ll study an improvement in AEs: VAEs. \nIn this section, you’ll first learn the key differences between AEs and VAEs and why \nthese differences lead to the ability of the latter to generate realistic images that are \nunseen in the training set. You’ll then learn the steps involved in training VAEs in gen -\neral and training one to generate high-resolution human face images in particular. \n152 chapter  7 Image generation with variational autoencoders\n7.3.1 Differences between AEs and VAEs\nVAEs were first proposed by Diederik Kingma and Max Welling in 2013.1 They are a \nvariant of AEs. Like an AE, a VAE also has two main parts: an encoder and a decoder.\nHowever, there are two key differences between AEs and VAEs. First, the latent space \nin an AE is deterministic. Each input is mapped to a fixed point in the latent space. In \ncontrast, the latent space in a VAE is probabilistic. Instead of encoding an input as a \nsingle vector in the latent space, a VAE encodes an input as a distribution over possible \nvalues. In our second project, for example, we’ll encode a color image into a 100-value \nprobabilistic vector. Additionally, we’ll assume that each element in this vector adheres \nto an independent normal distribution. Since defining a normal distribution requires \njust the mean ( 𝜇) and standard deviation ( 𝜎), each element in our 100-element proba -\nbilistic vector will be characterized by these two parameters. To reconstruct the image, \nwe sample a vector from this distribution and decode it. The uniqueness of VAEs is \nhighlighted by the fact that each sampling from the distribution results in a slightly \nvaried output.   \nIn statistical terms, the encoder in a VAE is trying to learn the true distribution of \nthe training data 𝑥, 𝑝(𝑥|𝜃), where 𝜃 is the parameters defining the distribution. For \ntractability, we usually assume that the distribution of the latent variable is normal. \nBecause we only need the mean, 𝜇, and standard deviation,  𝜎, to define a normal distri -\nbution, we can rewrite the true distribution as 𝑝(𝑥|𝜃)=𝑝(𝑥|𝜇,𝜎). The decoder in the \nVAE generates a sample based on the distribution learned by the encoder. That is, the \ndecoder generates an instance probabilistically from the distribution 𝑝(𝑥|𝜇,𝜎). \nThe second key difference between AEs and VAEs lies in the loss function. When \ntraining an AE, we minimize the reconstruction loss so that the reconstructed images \nare as close to the originals as possible. In contrast, in VAEs, the loss function consists of \ntwo parts: the reconstruction loss and the KL divergence. KL divergence is a measure of \nhow one probability distribution diverges from a second, expected probability distribu -\ntion. In VAEs, KL divergence is used to regularize the encoder by penalizing deviations \nof the learned distribution (the encoder’s output) from a prior distribution (a standard \nnormal distribution). This encourages the encoder to learn meaningful and generaliz -\nable latent representations. By penalizing distributions that are too far from the prior, \nKL divergence helps to avoid overfitting.\nThe KL divergence is calculated as follows in our setting since we assume a normal \ndistribution (the formula is different if a nonnormal distribution is assumed):\n (7.1)\nThe summation is taken over all 100 dimensions of the latent space. When the encoder \ncompresses the images into standard normal distributions in the latent space, such that \n1 Diederik P Kingma and Max Welling, 2013, “Auto-Encoding Variational Bayes.” https: //arxiv.org/abs/1312.6114 .\n 153 What are VAEs? \n𝜇=0 and 𝜎=1, the KL divergence becomes 0. In any other scenario, the value exceeds 0. \nThus, the KL divergence is minimized when the encoder successfully compresses the \nimages into standard normal distributions within the latent space.\n7.3.2 The blueprint to train a VAE to generate human face images\nIn the second project in this chapter, you’ll build and train a VAE from scratch to \ngenerate color images of human faces. The trained model can generate images that \nare unseen in the training set. Further, you can interpolate inputs to generate novel \nimages that are intermediate representations between two input data points. The fol -\nlowing is a blueprint for this second project. \nFigure 7.6 provides a diagram of the architecture of a VAE and the steps in training a \nVAE to generate human face images. \nOriginal\nimages Step 5: Feedback as measured by the sum of \nreconstruction loss and KL divergence\nStep 1Reconstructed \nimages\nStep 4\nEncoded data \nnormally distributed \nwith (mu, std)KL Divergence\nDecoderStep 2\nEncoder\nSample from the \nabove distribution in\nthe latent spaceStep 3\nShape\n(3, 256, 256)Shape\n(3, 256, 256)Latent space\n... ...\nFigure 7.6    The architecture of a VAE and the steps to train one to generate human face images.  \nA VAE consists of an encoder (middle upper left) and a decoder (middle bottom right). In each iteration \nof training, human face images are fed to the encoder (step 1). The encoder compresses the images to \nprobabilistic points in the latent space (step 2; since we assume normal distributions, each probability \npoint is characterized by a vector of means and a vector of standard deviations). We then sample \nencodings from the distribution and present them to the decoder. The decoder takes sampled encodings \n(step 3) and reconstructs images (step 4). The VAE adjusts its parameters to minimize the sum of \nreconstruction loss and the KL divergence. The KL divergence measures the difference between the \nencoder’s output and a standard normal distribution.  \nFigure 7.6 shows that a VAE also has two parts: an encoder (middle top left) and a \ndecoder (middle bottom right). Since the second project involves high-resolution color \nimages, we’ll use CNNs to create the VAE. As we discussed in chapter 4, high-resolution \ncolor images contain many more pixels than low-resolution grayscale images. If we use \nfully connected (dense) layers only, the number of parameters in the model is too \nlarge, making learning slow and ineffective. CNNs require fewer parameters than fully \nconnected networks of similar size, leading to faster and more effective learning.\n154 chapter  7 Image generation with variational autoencoders\nOnce the VAE is created, you’ll use the eyeglasses dataset that you downloaded in \nchapter 5 to train the model. The left side of figure 7.6 shows three examples of the \noriginal human face images in the training set. In the first step in the training loop, \nwe feed images in the training set, with a size of 3 × 256 × 256 = 196,608 pixels, to the \nencoder. The encoder compresses the images to 100-value probabilistic vectors in the \nlatent space (step 2; vectors of means and standard deviations due to the assumption \nof normal distribution). We then sample from the distribution and feed the sampled \nvector representations to the decoder (step 3) and ask it to reconstruct the images (step \n4). We calculate the total loss as the sum of the reconstruction loss at the pixel level and \nthe KL divergence as specified in equation 7.1. We propagate this loss back through the \nnetwork to update the parameters in the encoder and decoder to minimize the total \nloss (step 5). The total loss encourages the VAE to encode the inputs into more mean -\ningful and generalizable latent representations and to reconstruct images closer to the \noriginals. \nAfter the model is trained, you’ll feed human face images to the encoder and obtain \nencodings. You then feed the encodings to the decoder to obtain reconstructed images. \nYou’ll notice that the reconstructed images look close to the originals. The right side of \nfigure 7.6 shows three examples of reconstructed images: they look similar to the corre -\nsponding originals on the left side of the figure, though not perfectly. \nMore importantly, you can discard the encoder and randomly draw encodings from \nthe latent space and feed them to the trained decoder in VAE to generate novel human \nface images that are unseen in the training set. Further, you can manipulate the encoded \nrepresentations of different inputs to achieve specific outcomes when decoded. You \ncan also create a series of images transitioning from one instance to another by varying \nthe weight assigned to any two encodings.\n7.4 A VAE to generate human face images\nThis section creates and trains a VAE from scratch to generate human face images by \nfollowing the steps outlined in the last section.\nCompared to what we have done to build and train AEs, our approach for the sec -\nond project incorporates several modifications. Firstly, we plan to use CNNs in both \nthe encoders and decoders of VAEs, particularly because high-resolution color images \npossess a greater number of pixels. Relying solely on fully connected (dense) layers \nwould result in an excessively large number of parameters, leading to slow and ineffi -\ncient learning. Second, as part of our process to compress images into vectors that fol -\nlow a normal distribution in the latent space, we will generate both a mean vector and \na standard deviation vector during the encoding of each image. This differs from the \nfixed value vector used in AEs. From the encoded normal distribution, we’ll then sam -\nple to obtain encodings, which are subsequently decoded to produce images. Notably, \neach reconstructed image will vary slightly every time we sample from this distribution, \nwhich gives rise to VAEs’ ability to generate novel images.   \n 155 A VAE to generate human face images\n7.4.1 Building a VAE\nIf you recall, the eyeglasses dataset that you downloaded in chapter 5 is saved in the \nfolder /files/glasses/ on your computer after some labels are manually corrected. \nWe’ll resize the images to 256 by 256 pixels with values between 0 and 1. We then create \na batch iterator with 16 images in each batch:\ntransform = T.Compose([\n            T.Resize(256),    \n            T.ToTensor(),    \n            ])\ndata = torchvision.datasets.ImageFolder(\n    root=""files/glasses"",    \n    transform=transform)    \nbatch_size=16\nloader = torch.utils.data.DataLoader(data,    \n     batch_size=batch_size,shuffle=True)\nNext, we’ll create a VAE that includes convolutional and transposed convolutional lay -\ners. We first define an Encoder()  class as follows.\nListing 7.4    The encoder in the VAE\nlatent_dims=100    \nclass Encoder(nn.Module):\n    def __init__(self, latent_dims=100):  \n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 8, 3, stride=2, padding=1)\n        self.conv2 = nn.Conv2d(8, 16, 3, stride=2, padding=1)\n        self.batch2 = nn.BatchNorm2d(16)\n        self.conv3 = nn.Conv2d(16, 32, 3, stride=2, padding=0)  \n        self.linear1 = nn.Linear(31*31*32, 1024)\n        self.linear2 = nn.Linear(1024, latent_dims)\n        self.linear3 = nn.Linear(1024, latent_dims)\n        self.N = torch.distributions.Normal(0, 1)\n        self.N.loc = self.N.loc.cuda() \n        self.N.scale = self.N.scale.cuda()\n    def forward(self, x):\n        x = x.to(device)\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.batch2(self.conv2(x)))\n        x = F.relu(self.conv3(x))\n        x = torch.flatten(x, start_dim=1)\n        x = F.relu(self.linear1(x))\n        mu =  self.linear2(x)    \n        std = torch.exp(self.linear3(x))    \n        z = mu + std*self.N.sample(mu.shape)    \n        return mu, std, z\nThe encoder network consists of several convolutional layers, which extract the spatial \nfeatures of the input images. The encoder compresses the inputs into vector repre -\nsentations, z, which are normally distributed with means, mu, and standard deviations, Resizes images to \n256 by 256 pixels\nConverts images to tensors \nwith values between 0 and 1\nLoads images from the folder \nand apply the transformations\nPlaces the data in a \nbatch iterator\nThe dimension of the latent \nspace is 100.\nThe mean of the distribution \nof the encodings\nThe standard deviation \nof the encodings\nThe encoded vector \nrepresentation\n156 chapter  7 Image generation with variational autoencoders\nstd. The output from the encoder consists of three tensors: mu, std, and z. While the \nmu and std are the mean and standard deviation of the probabilistic vector, respec -\ntively, z is an instance sampled from this distribution. \nSpecifically, the input image, with a size of (3, 256, 256), first goes through a Conv2d \nlayer with a stride value of 2. As we explained in chapter 4, this means the filter skips \ntwo pixels each time it moves on the input image, which leads to downsampling of the \nimage. The output has a size of (8, 128, 128). It then goes through two more Conv2d \nlayers, and the size becomes (32, 31, 31). It is flattened and passed through linear layers \nto obtain values of mu and std. \nWe define a Decoder()  class to represent the decoder in the VAE.\nListing 7.5    The decoder in the VAE\nclass Decoder(nn.Module):   \n    def __init__(self, latent_dims=100):\n        super().__init__()\n        self.decoder_lin = nn.Sequential(    \n            nn.Linear(latent_dims, 1024),\n            nn.ReLU(True),\n            nn.Linear(1024, 31*31*32),    \n            nn.ReLU(True))\n        self.unflatten = nn.Unflatten(dim=1, \n                  unflattened_size=(32,31,31))\n        self.decoder_conv = nn.Sequential(    \n            nn.ConvTranspose2d(32,16,3,stride=2,\n                               output_padding=1),\n            nn.BatchNorm2d(16),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(16, 8, 3, stride=2, \n                               padding=1, output_padding=1),\n            nn.BatchNorm2d(8),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(8, 3, 3, stride=2,\n                               padding=1, output_padding=1))\n        \n    def forward(self, x):\n        x = self.decoder_lin(x)\n        x = self.unflatten(x)\n        x = self.decoder_conv(x)\n        x = torch.sigmoid(x)    \n        return x  \nThe decoder is a mirror image of the encoder: instead of performing convolutional \noperations, it performs transposed convolutional operations on the encodings to \ngenerate feature maps. It gradually converts encodings in the latent space back into \nhigh-resolution color images. \nSpecifically, the encoding first goes through two linear layers. It’s then unflattened \nto a shape (32, 31, 31), mirroring the size of the image after the last Conv2d layer in the \nencoder. It then goes through three ConvTranspose2d layers, mirroring the Conv2d Encodings first go through \ntwo dense layers.\nReshapes encodings into \nmultidimensional objects \nso we can perform \ntransposed convolutional \noperations on them\nPasses the encodings \nthrough three transposed \nconvolutional layers\nSqueezes the output to values \nbetween 0 and 1, the same as \nthe values in the input images",16924
69-7.4.3 Generating images with the trained VAE.pdf,69-7.4.3 Generating images with the trained VAE,"157 A VAE to generate human face images\nlayers in the encoder. The output from the decoder has a shape of (3, 256, 256), the \nsame as that of the training image. \nWe’ll combine the encoder with the decoder to create a VAE:\nclass VAE(nn.Module):\n    def __init__(self, latent_dims=100):\n        super().__init__()\n        self.encoder = Encoder(latent_dims)    \n        self.decoder = Decoder(latent_dims)    \n    def forward(self, x):\n        x = x.to(device)\n        mu, std, z = self.encoder(x)    \n        return mu, std, self.decoder(z)    \nThe VAE consists of an encoder and a decoder, as defined by the Encoder()  and \nDecoder()  classes. When we pass images through the VAE, the output consists of three \ntensors: the mean and standard deviation of the encodings and the reconstructed \nimages. \nNext, we create a VAE by instantiating the VAE()  class and define the optimizer for \nthe model:\nvae=VAE().to(device)\nlr=1e-4 \noptimizer=torch.optim.Adam(vae.parameters(),\n                           lr=lr,weight_decay=1e-5)\nWe’ll manually calculate the reconstruction loss and the KL-divergence loss during \ntraining. Therefore, we don't define a loss function here.\n7.4.2 Training the VAE\nTo train the model, we first define a train_epoch()  function to train the model for \none epoch.\nListing 7.6    Defining the train_epoch() function\ndef train_epoch(epoch):\n    vae.train()\n    epoch_loss = 0.0\n    for imgs, _ in loader: \n        imgs = imgs.to(device)\n        mu, std, out = vae(imgs)    \n        reconstruction_loss = ((imgs-out)**2).sum()    \n        kl = ((std**2)/2 + (mu**2)/2 - torch.log(std) - 0.5).sum()    \n        loss = reconstruction_loss + kl    \n        optimizer.zero_grad()Creates an encoder by \ninstantiating the Encoder() class  \nCreates a decoder by \ninstantiating the Decoder() class \nPasses the input through \nthe encoder to obtain \nthe encoding\nThe output of the VAE is the mean and \nstandard deviation of the encodings, as \nwell as the reconstructed images.\nObtains the \nreconstructed images\nCalculates the \nreconstruction loss\nCalculates the \nKL divergence\nSum of the reconstruction loss \nand the KL divergence.\n158 chapter  7 Image generation with variational autoencoders\n        loss.backward()\n        optimizer.step()\n        epoch_loss+=loss.item()\n    print(f'at epoch {epoch}, loss is {epoch_loss}')  \nWe iterate through all batches in the training set. We pass images through the VAE to \nobtain reconstructed images. The total loss is the sum of the reconstruction loss and \nthe KL divergence. The model parameters are adjusted in each iteration to minimize \nthe total loss. \nWe also define a plot_epoch()  function to visually inspect the generated images by \nthe VAE:\nimport numpy as np\nimport matplotlib.pyplot as plt\ndef plot_epoch():\n    with torch.no_grad():\n        noise = torch.randn(18,latent_dims).to(device)\n        imgs = vae.decoder(noise).cpu()\n        imgs = torchvision.utils.make_grid(imgs,6,3).numpy()\n        fig, ax = plt.subplots(figsize=(6,3),dpi=100)\n        plt.imshow(np.transpose(imgs, (1, 2, 0)))\n        plt.axis(""off"")\n        plt.show()\nA well-trained VAE can map similar inputs to nearby points in the latent space, lead -\ning to a more continuous and interpretable latent space. As a result, we can randomly \ndraw vectors from the latent space, and the VAE can decode the vectors into meaning -\nful outputs. Therefore, in the previous function plot_epoch() , we randomly draw 18 \nvectors from the latent space and use them to generate 18 images after each epoch of \ntraining. We plot them in a 3 × 6 grid and visually inspect them to see how the VAE is \nperforming during the training process.\nNext, we train the VAE for 10 epochs:\nfor epoch in range(1,11):\n    train_epoch(epoch)\n    plot_epoch()\ntorch.save(vae.state_dict(),""files/VAEglasses.pth"")\nThis training takes about half an hour if you use GPU training or several hours other -\nwise. The trained model weights are saved on your computer. Alternatively, you can \ndownload the trained weights from my website: https: //mng.bz/GNRR . Make sure you \nunzip the file after downloading. \n7.4.3 Generating images with the trained VAE\nNow that the VAE is trained, we can use it to generate images. We first load the weights \nof the trained model that we saved in the local folder:\nvae.eval()\nvae.load_state_dict(torch.load('files/VAEglasses.pth',\n    map_location=device))\n 159 A VAE to generate human face images\nWe then check the VAE’s ability to reconstruct images and see how closely they resem -\nble the originals: \nimgs,_=next(iter(loader))\nimgs = imgs.to(device)\nmu, std, out = vae(imgs)\nimages=torch.cat([imgs[:8],out[:8],imgs[8:16],out[8:16]],\n                 dim=0).detach().cpu()\nimages = torchvision.utils.make_grid(images,8,4)\nfig, ax = plt.subplots(figsize=(8,4),dpi=100)\nplt.imshow(np.transpose(images, (1, 2, 0)))\nplt.axis(""off"")\nplt.show()\nIf you run the previous code block, you’ll see an output similar to figure 7.7. \nFigure 7.7    Comparing the reconstructed images by a trained VAE with the originals. The first and the \nthird rows are the original images. We feed them to the trained VAE to obtain the reconstructed images, \nwhich are shown below the original images. \nThe original images are shown in the first and third rows, while the reconstructed \nimages are shown below the originals. The reconstructed images resemble the orig -\ninals, as shown in figure 7.7. However, some information gets lost during the recon -\nstruction process: they don’t look as realistic as the originals. \nNext, we test the VAE’s ability to generate novel images that are unseen in the train -\ning set, by calling the plot_epoch() function we defined before:\nplot_epoch()  \nThe function randomly draws 18 vectors from the latent space and passes them to the \ntrained VAE to generate 18 images. The output is shown in figure 7.8.",5985
70-7.4.4 Encoding arithmetic with the trained VAE.pdf,70-7.4.4 Encoding arithmetic with the trained VAE,"160 chapter  7 Image generation with variational autoencoders\nFigure 7.8    Novel images generated by the trained VAE. We randomly draw vector representations in \nthe latent space and feed them to the decoder in the trained VAE. The decoded images are shown in \nthis figure. Since the vector representations are randomly drawn, the images don’t correspond to any \noriginals in the training set. \nThese images are not present in the training set: the encodings are randomly drawn \nfrom the latent space, not the encoded vectors after passing images in the training set \nthrough the encoder. This is because the latent space in VAEs is continuous and inter -\npretable. New and unseen encodings in the latent space can be meaningfully decoded \ninto images that resemble but differ from those in the training set. \n7.4.4 Encoding arithmetic with the trained VAE\nVAEs include a regularization term (KL divergence) in their loss function, which \nencourages the latent space to approximate a normal distribution. This regulariza -\ntion ensures that the latent variables don’t just memorize the training data but rather \ncapture the underlying distribution. It helps to achieve a well-structured latent space \nwhere similar data points are mapped closely together, making the space continu -\nous and interpretable. As a result, we can manipulate the encodings to achieve new \noutcomes. \nTo make results reproducible, I encourage you to download the trained weights from \nmy website ( https: //mng.bz/GNRR ) and use the same code blocks for the rest of the \nchapter. As we explained in the introduction, encoding arithmetic allows us to generate \nimages with certain features. To illustrate how encoding arithmetic works in VAEs, let’s \nfirst hand-collect three images in each of the following four groups: men with glasses, \nmen without glasses, women with glasses, and women without glasses. \nListing 7.7    Collecting images with different characteristics\ntorch.manual_seed(0)  \nglasses=[]\nfor i in range(25):    Displays 25 images \nwith eyeglasses\n 161 A VAE to generate human face images\n    img,label=data[i]\n    glasses.append(img)\n    plt.subplot(5,5,i+1)\n    plt.imshow(img.numpy().transpose((1,2,0)))\n    plt.axis(""off"")\nplt.show()\nmen_g=[glasses[0],glasses[3],glasses[14]]    \nwomen_g=[glasses[9],glasses[15],glasses[21]]    \nnoglasses=[]\nfor i in range(25):    \n    img,label=data[-i-1]\n    noglasses.append(img)\n    plt.subplot(5,5,i+1)\n    plt.imshow(img.numpy().transpose((1,2,0)))\n    plt.axis(""off"")\nplt.show()\nmen_ng=[noglasses[1],noglasses[7],noglasses[22]]    \nwomen_ng=[noglasses[4],noglasses[9],noglasses[19]])    \nWe select three images in each group instead of just one so that we can calculate the \naverage of multiple encodings in the same group when performing encoding arith -\nmetic later. VAEs are designed to learn the distribution of the input data in the latent \nspace. By averaging multiple encodings, we effectively smooth out the representation \nin this space. This helps us find an average representation that captures common fea -\ntures among different samples within a group.\nNext we feed the three images of men with glasses to the trained VAE to obtain their \nencodings in the latent space. We then calculate the average encoding for the three \nimages and use it to obtain a reconstructed image of a man with glasses. We then repeat \nthis for the other three groups.\nListing 7.8    Encoding and decoding images in four different groups\n# create a batch of images of men with glasses\nmen_g_batch = torch.cat((men_g[0].unsqueeze(0),    \n             men_g[1].unsqueeze(0),\n             men_g[2].unsqueeze(0)), dim=0).to(device)\n# Obtain the three encodings\n_,_,men_g_encodings=vae.encoder(men_g_batch)\n# Average over the three images to obtain the encoding for the group\nmen_g_encoding=men_g_encodings.mean(dim=0)    \n# Decode the average encoding to create an image of a man with glasses \nmen_g_recon=vae.decoder(men_g_encoding.unsqueeze(0))    \n# Do the same for the other three groups\n# group 2, women with glasses\nwomen_g_batch = torch.cat((women_g[0].unsqueeze(0),\n             women_g[1].unsqueeze(0),\n             women_g[2].unsqueeze(0)), dim=0).to(device)\n# group 3, men without glasses\nmen_ng_batch = torch.cat((men_ng[0].unsqueeze(0),\n             men_ng[1].unsqueeze(0),Selects three images of \nmen with glasses\nSelects three images of \nwomen with glasses\nDisplays 25 images \nwithout eyeglasses\nSelects three images of \nmen without glasses\nSelects three images of \nwomen without glasses\nCreates a batch of images \nof men with glasses\nObtains the \naverage \nencoding for \nmen with \nglasses\nDecodes the \naverage encoding \nfor men with glasses\n162 chapter  7 Image generation with variational autoencoders\n             men_ng[2].unsqueeze(0)), dim=0).to(device)\n# group 4, women without glasses\nwomen_ng_batch = torch.cat((women_ng[0].unsqueeze(0),\n             women_ng[1].unsqueeze(0),\n             women_ng[2].unsqueeze(0)), dim=0).to(device)\n# obtain average encoding for each group\n_,_,women_g_encodings=vae.encoder(women_g_batch)\nwomen_g_encoding=women_g_encodings.mean(dim=0)\n_,_,men_ng_encodings=vae.encoder(men_ng_batch)\nmen_ng_encoding=men_ng_encodings.mean(dim=0)\n_,_,women_ng_encodings=vae.encoder(women_ng_batch)\nwomen_ng_encoding=women_ng_encodings.mean(dim=0)    \n# decode for each group\nwomen_g_recon=vae.decoder(women_g_encoding.unsqueeze(0))\nmen_ng_recon=vae.decoder(men_ng_encoding.unsqueeze(0))\nwomen_ng_recon=vae.decoder(women_ng_encoding.unsqueeze(0))    \nThe average encodings for the four groups are men_g_encoding , women_g_encoding , \nmen_ng_encoding , and women_ng_encoding , respectively, where g stands for glasses \nand ng for no glasses. The decoded images for the four groups are men_g_recon , \nwomen_g_recon , men_ng_recon , and women_ng_recon , respectively. We plot the four \nimages:\nimgs=torch.cat((men_g_recon,\n                women_g_recon,\n                men_ng_recon,\n                women_ng_recon),dim=0)\nimgs=torchvision.utils.make_grid(imgs,4,1).cpu().numpy()\nimgs=np.transpose(imgs,(1,2,0))\nfig, ax = plt.subplots(figsize=(8,2),dpi=100)\nplt.imshow(imgs)\nplt.axis(""off"")\nplt.show()\nYou’ll see the output as shown in figure 7.9.\nFigure 7.9    Decoded images based on average encodings. We first obtain three images in each of the \nfollowing four groups: men with glasses, women with glasses, men without glasses, and women without \nglasses. We feed the 12 images to the encoder in the trained VAE to obtain their encodings in the latent \nspace. We then calculate the average encoding of the three images in each group. The four average \nencodings are fed to the decoder in the trained VAE to obtain four images and they are shown in this \nfigure. Obtains the average \nencodings for the other \nthree groups\nDecodes the \naverage \nencodings for \nthe other three \ngroups\n 163 A VAE to generate human face images\nThe four decoded images are shown in figure 7.9. They are the composite images rep -\nresenting the four groups. Notice that they are different from any of the original 12 \nimages. At the same time, they preserve the defining characteristics of each group. \nNext, let’s manipulate the encodings to create a new encoding and then use the \ntrained decoder in the VAE to decode the new encoding and see what happens. For \nexample, we can subtract the average encoding of women with glasses from the average \nencoding of men with glasses and add the average encoding of women without glasses. \nWe then feed the result to the decoder and see the output. \nListing 7.9    An example of encoding arithmetic\nz=men_g_encoding-women_g_encoding+women_ng_encoding    \nout=vae.decoder(z.unsqueeze(0))    \nimgs=torch.cat((men_g_recon,\n                women_g_recon,\n                women_ng_recon,out),dim=0)\nimgs=torchvision.utils.make_grid(imgs,4,1).cpu().numpy()\nimgs=np.transpose(imgs,(1,2,0))\nfig, ax = plt.subplots(figsize=(8,2),dpi=100)\nplt.imshow(imgs)    \nplt.title(""man with glasses - woman \\nwith glasses + woman without \\nglasses = man without glasses "",fontsize=10,c=""r"")    \nplt.axis(""off"")\nplt.show()\nIf you run the code block in listing 7.9, you’ll see an output as shown in figure 7.10.\nFigure 7.10    An example of encoding arithmetic with the trained VAE. We first obtain the average \nencodings for the following three groups: men with glasses (z1), women with glasses (z2), and women \nwithout glasses (z3). We define a new encoding z = z1 – z2 + z3. We then feed z to the decoder in the \ntrained VAE and obtain the decoded image, as shown at the far right of this figure.\nThe first three images in figure 7.10 are the composite images representing the three \ninput groups. The output image, at the far right, is an image of a man without glasses. \nSince both men_g_encoding  and women_g_encoding  lead to eyeglasses in images \nwhen decoded, men_g_encoding  – women_g_encoding  cancels out eyeglasses fea -\ntures in the resulting image. Similarly, since both women_ng_encoding  and women_g_\nencoding  lead to a female face, women_ng_encoding  – women_g_encoding  cancels \nout female features in the resulting image. Therefore, if you decode men_g_encoding  Defines z as the \nencoding of men with \nglasses – women with \nglasses + women \nwithout glasses\nDecodes z to generate \nan image\nDisplays the four images\nDisplays a title on \ntop of the images\n164 chapter  7 Image generation with variational autoencoders\n+ women_g_encoding  –women_ng_encoding  with the trained VAE, you’ll get an image \nof a man without glasses. The encoding arithmetic in this example shows that an encod -\ning for men without glasses can be obtained by manipulating the average encodings in \nthe other three groups.  \nExercise 7.1\nPerform the following encoding arithmetics by modifying code listing 7.9: \n1 Subtract the average encoding of men without glasses from the average encoding \nof men with glasses and add the average encoding of women without glasses. \nFeed the result to the decoder and see what happens. \n2 Subtract the average encoding of women without glasses from the average encod -\ning of men without glasses and add the average encoding of women with glasses. \nFeed the result to the decoder and see what happens. \n3 Subtract the average encoding of men without glasses from the average encod -\ning of women without glasses and add the average encoding of men with glass -\nes. Feed the result to the decoder and see what happens. Make sure you modify \nthe image titles to reflect the changes. The solutions are provided in the book’s \nGitHub repository: https://github.com/markhliu/DGAI .\nFurther, we can interpolate any two encodings in the latent space by assigning different \nweights to them and creating a new encoding. We can then decode the new encoding \nand create a composite image as a result. By choosing different weights, we can create a \nseries of intermediate images that transition from one image to another. \nLet’s use the encodings of women with and without glasses as an example. We’ll \ndefine a new encoding z as w*women_ng_encoding+(1-w)*women_g_encoding , \nwhere w is the weight we put on women_ng_encoding . We’ll change the value of w from \n0 to 1 with an increment of 0.2 in each step. We then decode them and display the \nresulting six images.\nListing 7.10    Interpolating two encodings to create a series of images\nresults=[]\nfor w in [0, 0.2, 0.4, 0.6, 0.8, 1.0]:    \n    z=w*women_ng_encoding+(1-w)*women_g_encoding    \n    out=vae.decoder(z.unsqueeze(0))    \n    results.append(out)\nimgs=torch.cat((results[0],results[1],results[2],\n                results[3],results[4],results[5]),dim=0)\nimgs=torchvision.utils.make_grid(imgs,6,1).cpu().numpy()\nimgs=np.transpose(imgs,(1,2,0))\nfig, ax = plt.subplots(dpi=100)\nplt.imshow(imgs)    \nplt.axis(""off"")\nplt.show()\nAfter running the code in listing 7.10, you’ll see an output as shown in figure 7.11. Iterates through six \ndifferent values of w\nInterpolates between \ntwo encodings\nDecodes the \ninterpolated encoding\nDisplays the six \nresulting images\n 165 Summary\nFigure 7.11    Interpolating encodings to create a series of intermediate images. We first obtain the \naverage encodings for women with glasses ( women_g_encoding ) and women without glasses ( women_\nng_encoding ). The interpolated encoding z is defined as w*women_ng_encoding+(1-w)*women_g_\nencoding , where w is the weight on women_ng_encoding . We change the value of w from 0 to 1 with \nan increment of 0.2 to create six interpolated encodings. We then decode them and display the resulting \nsix images in the figure.\nAs you can see in figure 7.11, as you move from left to right, the image gradually transi -\ntions from a woman with glasses to a woman without glasses. This shows that the encod -\nings in the latent space are continuous, meaningful, and interpolatable. \nExercise 7.2\nModify listing 7.10 to create a series of intermediate images by using the following pairs \nof encodings: (i) men_ng_encoding  and men_g_encoding ; (ii) men_ng_encoding  \nand women_ng_encoding ; (iii) men_g_encoding and women_g_encoding . The \nsolutions are provided in the book’s GitHub repository: https://github.com/markhliu/\nDGAI . \nStarting in the next chapter, you’ll embark on a journey in natural language process -\ning. This will enable you to generate another form of content: text. However, many \ntools you have used so far will be used again in later chapters, such as deep neural net -\nworks and the encoder-decoder architecture. \nSummary\n¡ AEs have a dual-component structure: an encoder and a decoder. The encoder \ncompresses the data into an abstract representation in a lower-dimensional space \n(the latent space), and the decoder decompresses the encoded information and \nreconstructs the data.\n¡ VAEs also consist of an encoder and a decoder. They differ from AEs in two criti -\ncal ways. First, while an AE encodes each input into a specific point in the latent \nspace, a VAE encodes it into a probability distribution within this space. Second, \nan AE focuses solely on minimizing the reconstruction error, whereas a VAE \nlearns the parameters of the probability distribution for latent variables, mini -\nmizing a loss function that includes both reconstruction loss and a regularization \nterm, the KL divergence. \n¡ The KL divergence in the loss function when training VAEs ensures the \ndistribution for latent variables resembles a normal distribution. This \n166 chapter  7 Image generation with variational autoencoders\nencourages the encoder to learn continuous, meaningful, and generalizable \nlatent representations. \n¡ A well-trained VAE can map similar inputs to nearby points in the latent space, \nleading to a more continuous and interpretable latent space. As a result, VAEs \ncan decode random vectors in the latent space into meaningful outputs, leading \nto images that are unseen in the training set.\n¡ The latent space in a VAE is continuous and interpretable, different from that in \nan AE. As a result, we can manipulate the encodings to achieve new outcomes. We \ncan also create a series of intermediate images transitioning from one instance to \nanother by varying weights on two encodings in the latent space.",15440
71-Part 3.pdf,71-Part 3,"Part 3\nNatural language processing  \nand Transformers\nPart III focuses on text generation. \nIn chapter 8, you’ll learn to build and train a recurrent neural network to gen -\nerate text. Along the way, you’ll learn how tokenization and word embedding \nwork. You’ll also learn to generate text autoregressively and how to use tempera -\nture and top-K sampling to control the creativity of the generated text. In chapters \n9 and 10, you’ll build a Transformer from scratch, based on the paper “Attention \nIs All You Need,” to translate English to French. In chapter 11, you’ll learn to \nbuild GPT-2XL, the largest version of GPT-2, from scratch. After that, you’ll learn \nhow to extract the pretrained weights from Hugging Face and load them to your \nown GPT-2 model. You’ll use your GPT-2 to generate text by feeding a prompt to \nthe model. In chapter 12, you’ll build and train a GPT model to generate text in \nHemingway style.",940
72-8 Text generation with recurrent neural networks.pdf,72-8 Text generation with recurrent neural networks,"1698Text generation with \nrecurrent neural \nnetworks\nThis chapter covers\n¡ The idea behind RNNs and why they can handle  \n sequential data \n¡ Character tokenization, word tokenization, and  \n subword tokenization\n¡ How word embedding works\n¡ Building and training an RNN to generate text \n¡ Using temperature and top-K sampling to control  \n the creativeness of text generation\nSo far in this book, we have discussed how to generate shapes, numbers, and images. \nStarting from this chapter, we’ll focus mainly on text generation. Generating text \nis often considered the holy grail of generative AI for several compelling reasons. \nHuman language is incredibly complex and nuanced. It involves understanding not \nonly grammar and vocabulary but also context, tone, and cultural references. Suc -\ncessfully generating coherent and contextually appropriate text is a significant chal -\nlenge that requires deep understanding and processing of language.  \nAs humans, we primarily communicate through language. AI that can generate \nhuman-like text can interact more naturally with users, making technology more \naccessible and user-friendly. Text generation has many applications, from automating \n170 chapter  8 Text generation with recurrent neural networks\ncustomer service responses to creating entire articles, scripting for games and movies, \naiding in creative writing, and even building personal assistants. The potential effect \nacross industries is enormous.\nIn this chapter, we’ll make our first attempt at building and training models to gener -\nate text. You’ll learn to tackle three main challenges in modeling text generation. First, \ntext is sequential data, consisting of data points organized in a specific sequence, where \neach point is successively ordered to reflect the inherent order and interdependencies \nwithin the data. Predicting outcomes for sequences is challenging due to their sensi -\ntive ordering. Altering the sequence of elements changes their meaning. Second, text \nexhibits long-range dependencies: the meaning of a certain part of the text depends \non elements that appeared much earlier in the text (e.g., 100 words ago). Understand -\ning and modeling these long-range dependencies is essential for generating coherent \ntext. Lastly, human language is ambiguous and context dependent. Training a model to \nunderstand nuances, sarcasm, idioms, and cultural references to generate contextually \naccurate text is challenging.\nYou’ll explore a specific neural network designed for handling sequential data, \nsuch as text or time series: the recurrent neural network (RNN). Traditional neural \nnetworks, such as feedforward neural networks or fully connected networks, treat each \ninput independently. This means that the network processes each input separately, \nwithout considering any relationship or order between different inputs. In contrast, \nRNNs are specifically designed to handle sequential data. In an RNN, the output at a \ngiven time step depends not only on the current input but also on previous inputs. This \nallows RNNs to maintain a form of memory, capturing information from previous time \nsteps to influence the processing of the current input. \nThis sequential processing makes RNNs suitable for tasks where the order of the \ninputs matters, such as language modeling, where the goal is to predict the next word in \na sentence based on previous words. We’ll focus on one variant of RNN, long short-term \nmemory (LSTM) networks, which can recognize both short-term and long-term data \npatterns in sequential data like text. LSTM models use a hidden state to capture infor -\nmation in previous time steps. Therefore, a trained LSTM model can produce coherent \ntext based on the context. \nThe style of the generated text depends on the training data. Additionally, as we plan \nto train a model from scratch for text generation, the length of the training text is a \ncrucial factor. It needs to be sufficiently extensive for the model to effectively learn and \nmimic a particular writing style yet concise enough to avoid excessive computational \ndemands during training. As a result, we’ll use the text from the novel Anna Karenina , \nwhich appears to be of the right length for our purposes, to train an LSTM model. Since \nneural networks like an LSTM cannot accept text as input directly, you’ll learn to break \ndown text into tokens (individual words in this chapter but can be parts of words, as \nyou’ll see in later chapters), a process known as tokenization . You’ll then create a dictio -\nnary to map each unique token into an integer (i.e., an index). Based on this dictionary, \nyou’ll convert the text into a long sequence of integers, ready to be fed into a neural \nnetwork.",4799
73-8.1.1 Challenges in generating text.pdf,73-8.1.1 Challenges in generating text,"171 Introduction to RNNs\nYou’ll use sequences of indexes of a certain length as the input to train the LSTM \nmodel. You shift the sequence of inputs by one token to the right and use it as the out -\nput: you are effectively training the model to predict the next token in a sentence. This \nis the so-called sequence-to-sequence  prediction problem in natural language processing \n(NLP), and you’ll see it again in later chapters. \nOnce the LSTM is trained, you’ll use it to generate text one token at a time based on \nprevious tokens in the sequence as follows: you feed a prompt (part of a sentence such \nas “Anna and the”) to the trained model. The model then predicts the most likely next \ntoken and appends the selected token to your prompt. The updated prompt serves \nagain as the input, and the model is used once more to predict the next token. The iter -\native process continues until the prompt reaches a certain length. This approach is sim -\nilar to the mechanism employed by more advanced generative models like ChatGPT \n(though ChatGPT is not an LSTM). You’ll witness the trained LSTM model generat -\ning grammatically correct and coherent text, with a style matching that of the original \nnovel. \nFinally, you also learn how to control the creativeness of the generated text using \ntemperature and top-K sampling. Temperature controls the randomness of the pre -\ndictions of the trained model. A high temperature makes the generated text more cre -\native while a low temperature makes the text more confident and predictable. Top-K \nsampling is a method where you select the next token from the top K most probable \ntokens, rather than selecting from the entire vocabulary. A small value of K leads to the \nselection of highly likely tokens in each step, and this, in turn, makes the generated text \nless creative and more coherent.\nThe primary goal of this chapter is not necessarily to generate the most coherent \ntext possible, which, as mentioned earlier, presents substantial challenges. Instead, our \nobjective is to demonstrate the limitations of RNNs, thereby setting the stage for the \nintroduction of Transformers in subsequent chapters. More importantly, this chapter \nestablishes the basic principles of text generation, including tokenization, word embed -\nding, sequence prediction, temperature settings, and top-K sampling. Consequently, \nin later chapters, you will have a solid understanding of the fundamentals of NLP. This \nfoundation will allow us to concentrate on other, more advanced aspects of NLP, such as \nhow the attention mechanism functions and the architecture of Transformers.\n8.1 Introduction to RNNs\nAt the beginning of this chapter, we touched upon the complexities involved in gener -\nating text, particularly when aiming for coherence and contextual relevance. This sec -\ntion dives deeper into these challenges and explores the architecture of RNNs. We’ll \nexplain why RNNs are suitable for the task and their limitations (which are the reasons \nthey have been overtaken by Transformers). \nRNNs are specifically designed to handle sequential data, making them capable of \ntext generation, a task inherently sequential in nature. They utilize a form of memory, \nknown as hidden states, to capture and retain information from earlier parts of the",3336
74-8.1.2 How do RNNs work.pdf,74-8.1.2 How do RNNs work,"172 chapter  8 Text generation with recurrent neural networks\nsequence. This capability is crucial for maintaining context and understanding depen -\ndencies as the sequence progresses.\nIn this chapter, we will specifically utilize LSTM networks, advanced versions of \nRNNs, for text generation, using their advanced capabilities to tackle the challenges in \nthis task. \n8.1.1 Challenges in generating text\nText represents a quintessential example of sequential data , which is defined as any data -\nset where the order of elements is critical. This structuring implies that the positioning \nof individual elements relative to each other holds significant meaning, often convey -\ning essential information for understanding the data. Examples of sequential data \ninclude time series (like stock prices), textual content (such as sentences), and musical \ncompositions (a succession of notes).\nThis book primarily zeroes in on text generation, although it also ventures into music \ngeneration in chapters 13 and 14. The process of generating text is fraught with com -\nplexities. A primary challenge lies in modeling the sequence of words within sentences, \nwhere altering the order can drastically change the meaning. For instance, in the sen -\ntence “Kentucky defeated Vanderbilt in last night’s football game,” swapping ‘Ken -\ntucky’ and ‘Vanderbilt’ entirely reverses the sentence’s implication, despite using the \nsame words. Furthermore, as mentioned in the introduction, text generation encoun -\nters challenges in handling long-range dependencies and dealing with the problem of \nambiguity. \nIn this chapter, we will explore one approach to tackle these challenges—namely, by \nusing RNNs. While this method isn’t flawless, it lays the groundwork for more advanced \ntechniques you’ll encounter in later chapters. This approach will provide insight into \nmanaging word order, addressing long-range dependencies, and navigating the inher -\nent ambiguity in text, equipping you with fundamental skills in text generation. The \njourney through this chapter serves as a stepping stone to more sophisticated methods \nand deeper understanding in the subsequent parts of the book. Along the way, you’ll \nacquire many valuable skills in NLP, such as text tokenization, word embedding, and \nsequence-to-sequence predictions.     \n8.1.2 How do RNNs work?\nRNNs are a specialized form of artificial neural network designed to recognize patterns \nin sequences of data, such as text, music, or stock prices. Unlike traditional neural net -\nworks, which process inputs independently, RNNs have loops in them, allowing infor -\nmation to persist. \nOne of the challenges in generating text is how to predict the next word based on all \nprevious words so that the prediction captures both the long-range dependencies and \ncontextual meaning. RNNs take input not just as a standalone item but as a sequence \n(like words in a sentence, for example). At each time step, the prediction is based on \nnot only the current input but also all previous inputs in the form of a summary through \n 173 Introduction to RNNs\na hidden state. Let’s consider the phrase “a frog has four legs” as an example. In the first \ntime step, we use the word “a” to predict the second word “frog.” In the second time \nstep, we predict the next word using both “a” and “frog.” By the time we predict the last \nword, we need to use all four previous words “a frog has four.” \nA key feature of RNNs is the so-called hidden state, which captures information in \nall previous elements in a sequence. This feature is crucial for the network’s ability to \nprocess and generate sequential data effectively. The functioning of RNNs and this \nsequential processing is depicted in figure 8.1, which illustrates how a layer of recurrent \nneurons unfolds over time. \nWeights\n& activation\nx(t-1)y(t-1)\nh(t-2)Weights\n& activation\nx(t)y(t)\nh(t-1)\nh(t-1)Weights\n& activation\nx(t+2)y(t+2)\nh(t+1)\nh(t+1)Weights\n& activation\nx(t+1)y(t+1)\nh(t)\nh(t)h(t+2)\nTime\nElements are ordered sequentially over time\nFigure 8.1    How a layer of recurrent neurons unfolds through time. When a recurrent neural network \nmakes a prediction on sequential data, it takes the hidden state from the previous time step, h(t – 1), \nalong with the input at the current time step, x(t), and generates the output, y(t), and the updated hidden \nstate, h(t). The hidden state at time step t captures the information in all previous time steps, x(0), x(1), \n…, x(t). \nThe hidden state in RNNs plays a pivotal role in capturing information across all time \nsteps. This allows RNNs to make predictions that are informed not just by the current \ninput, x(t), but also by the accumulated knowledge from all previous inputs, x(0), x(1), \n…, x(t – 1). This attribute makes RNNs capable of understanding temporal dependen -\ncies. They can grasp the context from an input sequence, which is indispensable for \ntasks like language modeling, where the preceding words in a sentence set the stage \nfor predicting the next word.\nHowever, RNNs are not without their drawbacks. Though standard RNNs are capable \nof handling short-term dependencies, they struggle with longer-range dependencies \nwithin text. This difficulty stems from the vanishing gradient problem, which occurs in \nlong sequences where the gradients (essential for training the network) diminish, hin -\ndering the model’s ability to learn relationships over longer distances. To mitigate this, \nadvanced versions of RNNs, such as LSTM networks, have been developed. \nLSTM networks were introduced by Hochreiter and Schmidhuber in 1997.1 An \nLSTM network is composed of LSTM units (or cells), each of which has a more complex \nstructure than a standard RNN neuron. The cell state is the key innovation of LSTMs: it \nacts as a kind of conveyor belt, running straight down the entire chain of LSTM units. It \n1 Sepp Hochreiter and Jurgen Schmidhuber, 1997, “Long Short-Term Memory,” Neural Computation  9(8): 1735-1780.",6071
75-8.2.2 Word embedding.pdf,75-8.2.2 Word embedding,"174 chapter  8 Text generation with recurrent neural networks\nhas the ability to carry relevant information through the network. The ability to add or \nremove information to the cell state allows LSTMs to capture long-term dependencies \nand remember information for long periods. This makes them more effective for tasks \nlike language modeling and text generation. In this chapter, we will harness the LSTM \nmodel to undertake a project on text generation, aiming to mimic the style of the novel \nAnna Karenina .\nHowever, it’s noteworthy that even advanced RNN variants like LSTMs encounter \nhurdles in capturing extremely long-range dependencies in sequence data. We will dis -\ncuss these challenges and provide solutions in the next chapter, continuing our explo -\nration of sophisticated models for effective sequence data processing and generation.\n8.1.3 Steps in training a LSTM model\nNext, we’ll discuss the steps involved in training an LSTM model to generate text. This \noverview aims to provide a foundational understanding of the training process before \nembarking on the project. \nThe choice of text for training depends on the desired output. A lengthy novel serves \nas a good starting point. Its extensive content enables the model to learn and replicate \na specific writing style effectively. An ample amount of text data enhances the model’s \nproficiency in this style. At the same time, novels are generally not excessively long, \nwhich helps in managing the training time. For our LSTM model training, we’ll uti -\nlize the text from Anna Karenina , aligning with our previously outlined training data \ncriteria. \nSimilar to other deep neural networks, LSTM models cannot process raw text \ndirectly. Instead, we’ll convert the text into numerical form. This begins by breaking \ndown the text into smaller pieces, a process known as tokenization, where each piece \nis a token. Tokens can be entire words, punctuation marks (like an exclamation mark \nor a comma), or special characters (such as & or %). For this chapter, each of these \nelements will be treated as separate tokens. Although this method of tokenization may \nnot be the most efficient, it is easy to implement since all we need is to map words to \ntokens. We will use subword tokenization in subsequent chapters where some infre -\nquent words are broken into smaller pieces such as syllables. Following tokenization, we \nassign a unique integer to each token, creating a numerical representation of the text \nas a sequence of integers.\nTo prepare the training data, we divide this long sequence into shorter sequences of \nequal length. For our project, we’ll use sequences comprising 100 integers each. These \nsequences form the features (the x variable) of our model. We then generate the output \ny by shifting the input sequence one token to the right. This setup enables the LSTM \nmodel to predict the next token in a sequence. The pairs of input and output serve as \nthe training data. Our model includes LSTM layers to understand long-term patterns in \nthe text and an embedding layer to grasp semantic meanings.\nLet’s revisit the example of predicting the sentence “a frog has four legs” that we \nmentioned earlier. Figure 8.2 is a diagram of how the training of the LSTM model \nworks. \n 175 Fundamentals of NLP\nWeights \n& activation\nx0\nafrog\ny0\nInitial h \nﬁlled with 0sWeights \n& activation\nx1\nfroghas\ny1\nh0\nh0Weights \n& activation\nx3\nfourlegs\ny3\nh2\nh2Weights \n& activation\nx2\nhasfour\ny2\nh1\nh1h3\nTime\nElements are ordered sequentially over time\nFigure 8.2    An example of how an LSTM model is trained. We first break down the training text into \ntokens and assign a unique integer to each token, creating a numerical representation of the text as a \nsequence of indexes. We then divide this long sequence into shorter sequences of equal length. These \nsequences form the features (the x variable) of our model. We then generate the output y by shifting \nthe input sequence one token to the right. This setup enables the LSTM model to predict the next token \nbased on previous tokens in the sequence. \nIn the first time step, the model uses the word “a” to predict the word “frog.” Since \nthere’s no preceding word for “a,” we initialize the hidden state with zeros. The LSTM \nmodel receives both the index for “a” and this initial hidden state as input and outputs \nthe predicted next word along with an updated hidden state, h0. In the subsequent \ntime step, the word “frog” and the updated state h0 are used to predict “has” and gen -\nerate a new hidden state, h1. This sequence of predicting the next word and updating \nthe hidden state continues until the model forecasts the final word in the sentence, \n“legs.”\nThe predictions are then compared to the actual next word in the sentence. Since \nthe model is effectively predicting the next token out of all possible tokens in the vocab -\nulary, there is a multicategory classification problem. We tweak the model parameters \nin each iteration to minimize the cross-entropy loss so that in the next iteration, the \nmodel predictions move closer to actual outputs in the training data.  \nOnce the model is trained, generating text begins with a seed sequence input into the \nmodel. The model predicts the next token, which is then appended to your sequence. \nThis iterative process of prediction and sequence updating is repeated to generate text \nfor as long as desired.\n8.2 Fundamentals of NLP\nDeep learning models, including the LSTM models we discussed earlier and Trans -\nformers, which you’ll learn in later chapters, cannot process raw text directly because \nthey are designed to work with numerical data, typically in the form of vectors or \nmatrices. The processing and learning capabilities of neural networks are based on \nmathematical operations like addition, multiplication, and activation functions, which \nrequire numerical input. Consequently, it’s essential first to break down text into \nsmaller, more manageable elements known as tokens. These tokens can range from \nindividual characters and words to subword units. \n176 chapter  8 Text generation with recurrent neural networks\nThe next crucial step in NLP tasks is transforming these tokens into numerical rep -\nresentations. This conversion is necessary for feeding them into deep neural networks, \nwhich is a fundamental part of training our models. \nIn this section, we’ll discuss different tokenization methods, along with their advan -\ntages and drawbacks. Additionally, you’ll gain insights into the process of converting \ntokens into dense vector representations—a method known as word embedding. This \ntechnique is crucial for capturing the meaning of language in a format that deep learn -\ning models can effectively utilize. \n8.2.1 Different tokenization methods\nTokenization involves dividing text into smaller parts, known as tokens, which can be \nin the form of words, characters, symbols, or other significant units. The primary goal \nof tokenization is to streamline the process of text data analysis and processing.\nBroadly speaking, there are three approaches to tokenization. The first is character \ntokenization, where the text is divided into its constituent characters. This method is \nused in languages with complex morphological structures, such as Turkish or Finnish, \nin which the meaning of words can change significantly with slight variations in charac -\nters. Take the English phrase “It is unbelievably good!” as an example; it’s broken down \ninto individual characters as follows: [ 'I', 't', ' ', 'i', 's', ' ', 'u', 'n', \n'b', 'e', 'l', 'i', 'e', 'v', 'a', 'b', 'l', 'y', ' ', 'g', 'o', 'o', \n'd', '!' ]. A key advantage of character tokenization is the limited number of unique \ntokens. This limitation significantly reduces the parameters in deep learning models, \nleading to faster and more efficient training. However, the major drawback is that indi -\nvidual characters often lack significant meaning, making it challenging for machine \nlearning models to derive meaningful insights from a sequence of characters.\nExercise 8.1\nUse character tokenization to divide the phrase “Hi, there!” into individual tokens. \nThe second approach is word tokenization, where the text is split into individual words \nand punctuation marks. It is used often in situations where the number of unique \nwords is not too large. For instance, the same phrase “It is unbelievably good!” becomes \nfive tokens: ['It', 'is', 'unbelievably', 'good', '!'] . The main advantage \nof this method is that each word inherently carries semantic meaning, making it more \nstraightforward for models to interpret the text. The downside, however, lies in the sub -\nstantial increase in unique tokens, which increases the number of parameters in deep \nlearning models. This increase can lead to slower and less efficient training processes.\nExercise 8.2\nUse word tokenization to break down the phrase “Hi, how are you?” into individual \ntokens.\n 177 Fundamentals of NLP\nThe third approach is subword tokenization. This method, a key concept in NLP, \nbreaks text into smaller, meaningful components called subwords. For instance, the \nphrase “It is unbelievably good!” would be divided into tokens like ['It', 'is', \n'un', 'believ', 'ably', 'good', '!'] . Most advanced language models, includ -\ning ChatGPT, use subword tokenization, and you’ll use this method in the next few \nchapters. Subword tokenization strikes a balance between the more traditional token -\nization techniques that typically split text into either individual words or characters. \nWord-based tokenization, while capturing more meaning, leads to a vast vocabulary. \nConversely, character-based tokenization results in a smaller vocabulary, but each \ntoken carries less semantic value. \nSubword tokenization effectively mitigates these problems by keeping frequently \nused words whole in the vocabulary while dividing less common or more complex \nwords into subcomponents. This technique is particularly advantageous for languages \nwith large vocabularies or those exhibiting a high degree of word form variation. By \nadopting subword tokenization, the overall vocabulary size is substantially reduced. \nThis reduction enhances the efficiency and effectiveness of language processing tasks, \nespecially when dealing with a wide range of linguistic structures. \nIn this chapter, we will focus on word tokenization, as it offers a straightforward foun -\ndation for beginners. As we progress to later chapters, our attention will shift to sub -\nword tokenization, utilizing models that have already been trained with this technique. \nThis approach allows us to concentrate on more advanced topics, such as understand -\ning the Transformer architecture and exploring the inner workings of the attention \nmechanism.\n8.2.2 Word embedding\nWord embedding is a method that transforms tokens into compact vector representa -\ntions, capturing their semantic information and interrelationships. This technique is \nvital in NLP, especially since deep neural networks, including models like LSTM and \nTransformers, require numerical input.\nTraditionally, tokens are converted into numbers using one-hot encoding before \nbeing fed into NLP models. In one-hot encoding, each token is represented by a vec -\ntor where only one element is ‘1’, and the rest are ‘0’s. For example, in this chapter, \nthere are 12,778 unique word-based tokens in the text for the novel Anna Karenina . \nEach token is represented by a vector of 12,778 dimensions. Consequently, a phrase like \n“happy families are all alike” is represented as a 5 × 12,778 matrix, where 5 represents \nthe number of tokens. This representation, however, is highly inefficient due to its large \ndimensionality, leading to an increased number of parameters, which can hinder train -\ning speed and efficiency.\nLSTMs, Transformers, and other advanced NLP models address this inefficiency \nthrough word embedding. Instead of bulky one-hot vectors, word embedding uses con -\ntinuous, lower-dimensional vectors (e.g., 128-value vectors we use in this chapter). As \na result, the phrase “happy families are all alike” is represented by a more compact 5 × \n178 chapter  8 Text generation with recurrent neural networks\n128 matrix after word embedding. This streamlined representation drastically reduces \nthe model’s complexity and enhances training efficiency.\nWord embedding not only reduces word complexity by condensing it into a lower-\ndimensional space but also effectively captures the context and the nuanced semantic \nrelationships between words, a feature that simpler representations like one-hot \nencoding lack, for the following reasons. In one-hot encoding, all tokens have the same \ndistance from each other in vector space. However, in word embeddings, tokens with \nsimilar meanings are represented by vectors close to each other in the embedding \nspace. Word embeddings are learned from the text in the training data; the resulting \nvectors capture contextual information. Tokens that appear in similar contexts will \nhave similar embeddings, even if they are not explicitly related.\nWord embedding in NLP\nWord embeddings are a powerful method for representing tokens in NLP that offer sig -\nnificant advantages over traditional one-hot encoding in capturing context and semantic \nrelationships between words. \nOne-hot encoding represents tokens as sparse vectors with a dimension equal to the \nsize of the vocabulary, where each token is represented by a vector with all zeros except \nfor a single one at the index corresponding to the token. In contrast, word embeddings \nrepresent tokens as dense vectors with much lower dimensions (e.g., 128 dimensions \nin this chapter and 256 dimensions in chapter 12). This dense representation is more \nefficient and can capture more information.\nSpecifically, in one-hot encoding, all tokens have the same distance from each other in \nthe vector space, meaning there is no notion of similarity between tokens. However, in \nword embeddings, similar tokens are represented by vectors that are close to each other \nin the embedding space. For example, the words “king” and “queen” would have similar \nembeddings, reflecting their semantic relationship.\nWord embeddings are learned from the text in the training data. The embedding process \nuses the context in which tokens appear to learn their embeddings, meaning that the \nresulting vectors capture contextual information. Tokens that appear in similar contexts \nwill have similar embeddings, even if they are not explicitly related.\nOverall, word embeddings provide a more nuanced and efficient representation of words \nthat captures semantic relationships and contextual information, making them more \nsuitable for NLP tasks compared to one-hot encoding.\nIn practical terms, particularly in frameworks like PyTorch, word embedding is \nimplemented by passing indexes through a linear layer, which compresses them into \na lower-dimensional space. That is, when you pass an index to the  nn.Embedding()  \nlayer, it looks up the corresponding row in the embedding matrix and returns the \nembedding vector for that index, avoiding the need to create potentially very large \none-hot vectors. The weights of this embedding layer are not predefined but are \nlearned during the training process. This learning aspect enables the model to refine",15585
76-8.3 Preparing data to train the LSTM model.pdf,76-8.3 Preparing data to train the LSTM model,,0
77-8.3.1 Downloading and cleaning up the text.pdf,77-8.3.1 Downloading and cleaning up the text,"179 Preparing data to train the LSTM model\nits understanding of word semantics based on the training data, leading to a more \nnuanced and context-aware representation of language in the neural network. This \napproach significantly enhances the model’s ability to process and interpret language \ndata efficiently and meaningfully.\n8.3 Preparing data to train the LSTM model\nIn this section, we’ll process text data and get it ready for training. We’ll first break text \ndown into individual tokens. Our next step involves creating a dictionary that assigns \neach token an index, essentially mapping them to integers. After this setup, we will \norganize these tokens into batches of training data, which will be crucial for training \nan LSTM model in the subsequent section.\nWe’ll walk through the tokenization process in a detailed, step-by-step manner, \nensuring you gain a thorough understanding of how tokenization functions. We’ll use \nword tokenization, owing to its simplicity in dividing text into words, as opposed to the \nmore complex subword tokenization that demands a nuanced grasp of linguistic struc -\nture. In later chapters, we’ll employ pretrained tokenizers for subword tokenization \nusing more sophisticated methods. This will allow us to focus on advanced topics, such \nas the attention mechanism and the Transformer architecture, without getting bogged \ndown in the initial stages of text processing. \n8.3.1 Downloading and cleaning up the text\nWe’ll use the text from the novel Anna Karenina  to train our model. Go to https: //mng  \n.bz/znmX  to download the text file and save it as anna.txt in the folder /files/ on your \ncomputer. After that, open the file and delete everything after line 39888, which says, \n""END OF THIS PROJECT GUTENBERG EBOOK ANNA KARENINA ."" Or you can simply \ndownload the file anna.txt from the book’s GitHub repository: https: //github.com/\nmarkhliu/DGAI . \nFirst, we load up the data and print out some passages to get a feeling about the \ndataset:\nwith open(""files/anna.txt"",""r"") as f:\n    text=f.read()    \nwords=text.split("" "")    \nprint(words[:20]) \nThe output is \n['Chapter', '1\n\n\nHappy', 'families', 'are', 'all', 'alike;', 'every',\n 'unhappy', 'family', 'is', 'unhappy', 'in', 'its',\n'own\nway.\n\nEverything', 'was', 'in', 'confusion', 'in', 'the',\n""Oblonskys'""]\nAs you can see, line breaks (represented by \n) are considered part of the text. There -\nfore, we should replace these line breaks with spaces so they are not in the vocabu -\nlary. Additionally, converting all words to lowercase is helpful in our setting, as it \nensures words like “The” and “the” are recognized as the same token. This step is vital \nfor reducing the variety of unique tokens, thereby making the training process more \n180 chapter  8 Text generation with recurrent neural networks\nefficient. Furthermore, punctuation marks should be spaced apart from the words \nthey follow. Without this separation, combinations like “way.” and “way” would be erro -\nneously treated as different tokens. To address these problems, we’ll clean up the text:\nclean_text=text.lower().replace(""\n"", "" "")    \nclean_text=clean_text.replace(""-"", "" "")    \nfor x in "",.:;?!$()/_&%*@'`"":\n    clean_text=clean_text.replace(f""{x}"", f"" {x} "")\nclean_text=clean_text.replace('""', ' "" ')    \ntext=clean_text.split()\nNext, we obtain unique tokens:\nfrom collections import Counter   \nword_counts = Counter(text)    \nwords=sorted(word_counts, key=word_counts.get,\n                      reverse=True)\nprint(words[:10])\nThe list words  contains all the unique tokens in the text, with the most frequent one \nappearing first, and the least frequent one last. The output from the preceding code \nblock is\n[',', '.', 'the', '""', 'and', 'to', 'of', 'he', ""'"", 'a']\nThe preceding output shows the most frequent 10 tokens. The comma ( ,) and the \nperiod (.) are the most and the second most frequent tokens, respectively. The word \n“the” is the third most frequent token, and so on. \nWe now create two dictionaries: one mapping tokens to indexes and the other map -\nping indexes to tokens.\nListing 8.1    Dictionaries to map tokens to indexes and indexes to tokens\ntext_length=len(text)    \nnum_unique_words=len(words)    \nprint(f""the text contains {text_length} words"")\nprint(f""there are {num_unique_words} unique tokens"")  \nword_to_int={v:k for k,v in enumerate(words)}    \nint_to_word={k:v for k,v in enumerate(words)}    \nprint({k:v for k,v in word_to_int.items() if k in words[:10]})\nprint({k:v for k,v in int_to_word.items() if v in words[:10]})\nThe output from the preceding code block is\nthe text contains 437098 words\nthere are 12778 unique tokens\n{',': 0, '.': 1, 'the': 2, '""': 3, 'and': 4, 'to': 5, 'of': 6, 'he': 7,\n""'"": 8, 'a': 9}\n{0: ',', 1: '.', 2: 'the', 3: '""', 4: 'and', 5: 'to', 6: 'of', 7: 'he',\n 8: ""'"", 9: 'a'}Replaces line break \nwith a space\nReplaces a hyphen \nwith a space\nAdds a space around \npunctuation marks and \nspecial characters\nThe length of text (how \nmany tokens in the text)\nThe length of \nunique tokens\nMaps tokens \nto indexes\nMaps indexes \nto tokens",5183
78-8.4 Building and training the LSTM model.pdf,78-8.4 Building and training the LSTM model,"181 Preparing data to train the LSTM model\nThe text for the novel Anna Karenina  has a total of 437,098 tokens. There are 12,778 \nunique tokens. The dictionary word_to_int  assigns an index to each unique token. \nFor example, the comma ( ,) is assigned an index of 0, and the period ( .) is assigned \nan index of 1. The dictionary int_to_word  translates an index back to a token. For \nexample, index 2 is translated back to the token “ the”. Index 4 is translated back to the \ntoken “and”, and so on. \nFinally, we convert the whole text to indexes:\nprint(text[0:20])\nwordidx=[word_to_int[w] for w in text]  \nprint([word_to_int[w] for w in text[0:20]])  \nThe output is\n['chapter', '1', 'happy', 'families', 'are', 'all', 'alike', ';', 'every',\n 'unhappy', 'family', 'is', 'unhappy', 'in', 'its', 'own', 'way', '.',\n 'everything', 'was']\n[208, 670, 283, 3024, 82, 31, 2461, 35, 202, 690, 365, 38, 690, 10, 234,\n 147, 166, 1, 149, 12]\nWe convert all tokens in the text into the corresponding indexes and save them in a \nlist wordidx . The preceding output shows the first 20 tokens in the text, as well as the \ncorresponding indexes. For example, the first token in the text is chapter , with an \nindex value of 208. \nExercise 8.3\nFind out the index value of the token anna  in the dictionary word_to_int .\n8.3.2 Creating batches of training data\nNext, we create pairs of (x, y) for training purposes. Each x is a sequence with 100 \nindexes. There is nothing magical about the number 100, and you can easily change \nit to 90 or 110 and have similar results. Setting the number too large may slow down \ntraining, while setting the number too small may lead to the model’s failure to capture \nlong-range dependencies. We then slide the window right by one token and use it as the \ntarget y. Shifting the sequence by one token to the right and using it as the output during \nsequence generation is a common technique in training language models, including \nTransformers. The code block in the following listing creates the training data. \nListing 8.2    Creating training data\nimport torch\nseq_len=100    \nxys=[]\nfor n in range(0, len(wordidx)-seq_len-1):    Each input contains \n100 indexes.\nStarting from the first token in text, \nslides to the right one at a time",2300
79-8.4.1 Building an LSTM model.pdf,79-8.4.1 Building an LSTM model,"182 chapter  8 Text generation with recurrent neural networks\n    x = wordidx[n:n+seq_len]    \n    y = wordidx[n+1:n+seq_len+1]    \n    xys.append((torch.tensor(x),(torch.tensor(y))))\nBy shifting the sequence one token to the right and using it as output, the model is \ntrained to predict the next token given the previous tokens. For instance, if the input \nsequence is ""how are you"" , then the shifted sequence would be ""are you today"" . \nDuring training, the model learns to predict 'are' after seeing 'how' , 'you'  after \nseeing 'are' , and so on. This helps the model learn the probability distribution of the \nnext token in a sequence. You’ll see this practice again and again later in this book. \nWe’ll create batches of data for training, with 32 pairs of (x, y) in each batch:\nfrom torch.utils.data import DataLoader\ntorch.manual_seed(42)\nbatch_size=32\nloader = DataLoader(xys, batch_size=batch_size, shuffle=True)\nWe now have the training dataset. Next, we’ll create an LSTM model and train it using \nthe data we just processed. \n8.4 Building and training the LSTM model\nIn this section, you’ll begin by constructing an LSTM model using PyTorch’s built-in \nLSTM layer. This model will start with a word embedding layer, which transforms each \nindex into a dense vector of 128 dimensions. Your training data will pass through this \nembedding layer before being fed into the LSTM layer. This LSTM layer is designed to \nprocess elements of a sequence in a sequential manner. Following the LSTM layer, the \ndata will proceed to a linear layer, which has an output size matching the size of your \nvocabulary. The outputs generated by the LSTM model are essentially logits, serving as \ninputs for the softmax function to compute probabilities.\nOnce you have built the LSTM model, the next step will involve using your training \ndata to train this model. This training phase is crucial to refine the model’s ability to \nunderstand and generate patterns consistent with the data it has been fed. \n8.4.1 Building an LSTM model\nIn listing 8.3, we define a WordLSTM()  class to serve as our LSTM model to be trained \nto generate text in the style of Anna Karenina . The class is defined as shown in the fol -\nlowing listing.\nListing 8.3    Defining the WordLSTM()  class \nfrom torch import nn\ndevice=""cuda"" if torch.cuda.is_available() else ""cpu""\nclass WordLSTM(nn.Module):\n    def __init__(self, input_size=128, n_embed=128,\n             n_layers=3, drop_prob=0.2):Defines the input x\nShifts the input x to the \nright by one token and \nuses it as the output y\n 183 Building and training the LSTM model\n        super().__init__()\n        self.input_size=input_size\n        self.drop_prob = drop_prob\n        self.n_layers = n_layers\n        self.n_embed = n_embed\n        vocab_size=len(word_to_int)\n        self.embedding=nn.Embedding(vocab_size,n_embed)    \n        self.lstm = nn.LSTM(input_size=self.input_size,\n            hidden_size=self.n_embed,\n            num_layers=self.n_layers,\n            dropout=self.drop_prob,batch_first=True)    \n        self.fc = nn.Linear(input_size, vocab_size)    \n    def forward(self, x, hc):\n        embed=self.embedding(x)\n        x, hc = self.lstm(embed, hc)    \n        x = self.fc(x)\n        return x, hc      \n        \n    def init_hidden(self, n_seqs):    \n        weight = next(self.parameters()).data\n        return (weight.new(self.n_layers,\n                           n_seqs, self.n_embed).zero_(),\n                weight.new(self.n_layers,\n                           n_seqs, self.n_embed).zero_())  \nThe WordLSTM()  class defined previously has three layers: the word embedding layer, \nthe LSTM layer, and a final linear layer. We set the value of the argument n_layers  \nto 3, which means the LSTM layer stacks three LSTMs together to form a stacked \nLSTM, with the last two LSTMs taking the output from the previous LSTM as input. \nThe init_hidden()  method fills the hidden state with zeros when the model uses the \nfirst element in the sequence to make predictions. In each time step, the input is the \ncurrent token and the previous hidden state while the output is the next token and the \nnext hidden state. \nHow the torch.nn.Embedding()  class works\nThe torch.nn.Embedding() class in PyTorch is used to create an embedding layer \nin a neural network. An embedding layer is a trainable lookup table that maps integer \nindexes to dense, continuous vector representations (embeddings). \nWhen you create an instance of torch.nn.Embedding() , you need to specify two \nmain parameters: num_embeddings, the size of the vocabulary (total number of unique \ntokens), and embedding_dim, the size of each embedding vector (the dimensionality of \nthe output embeddings).\nInternally, the class creates a matrix (or lookup table) of shape (num_embeddings, \nembedding_dim) where each row corresponds to the embedding vector for a particular \nindex. Initially, these embeddings are randomly initialized but are learned and updated \nduring training through backpropagation.Training data first \ngoes through an \nembedding layer.\nCreates an LSTM layer \nwith the PyTorch \nLSTM() class\nIn each time step, the LSTM layer \nuses the previous token and the \nhidden state to predict the next \ntoken and the next hidden state.\nInitiates the hidden \nstate for the first token \nin the input sequence",5428
80-8.5.1 Generating text by predicting the next token.pdf,80-8.5.1 Generating text by predicting the next token,"184 chapter  8 Text generation with recurrent neural networks\n(continued)\nWhen you pass a tensor of indexes to the embedding layer (during the forward pass of \nthe network), it looks up the corresponding embedding vectors in the lookup table and \nreturns them. More information about the class is provided by PyTorch at https://mng  \n.bz/n0Zd .  \nWe create an instance of the WordLSTM()  class and use it as our LSTM model, as \nfollows:\nmodel=WordLSTM().to(device)\nWhen the LSTM model is created, the weights are randomly initialized. When we use \npairs of (x, y) to train the model, LSTM learns to predict the next token based on all \nprevious tokens in the sequence by adjusting the model parameters. As we have illus -\ntrated in figure 8.2, LSTM learns to predict the next token and the next hidden state \nbased on the current token and the current hidden state, which is a summary of the \ninformation in all previous tokens. \nWe use the Adam optimizer with a learning rate of 0.0001. The loss function is the \ncross-entropy loss since this is essentially a multicategory classification problem: the \nmodel is trying to predict the next token from a dictionary with 12,778 choices:\nlr=0.0001\noptimizer = torch.optim.Adam(model.parameters(), lr=lr)\nloss_func = nn.CrossEntropyLoss()\nNow that the LSTM model is built, we’ll train the model with the batches of training \ndata we prepared before. \n8.4.2 Training the LSTM model\nDuring each training epoch, we go through all data batches of data (x, y) in the train -\ning set. The LSTM model receives the input sequence, x, and generates a predicted \noutput sequence, 𝑦̂. This prediction is compared with the actual output sequence, y, \nto compute the cross-entropy loss since we essentially conduct a multicategory classi -\nfication here. We then tweak the model’s parameters to reduce this loss, as we did in \nchapter 2 when classifying clothing items.\nThough we could divide our data into training and validation sets, training the \nmodel until no further improvements are seen on the validation set (as we have done in \nchapter 2), our primary aim here is to grasp how LSTM models function, not necessar -\nily to achieve the best parameter tuning. Therefore, we’ll train the model for 50 epochs.\nListing 8.4    Training the LSTM model to generate text\nmodel.train()\nfor epoch in range(50):\n    tloss=0\n 185 Generating text with the trained LSTM model\n    sh,sc = model.init_hidden(batch_size)\n    for i, (x,y) in enumerate(loader):    \n        if x.shape[0]==batch_size:\n            inputs, targets = x.to(device), y.to(device)\n            optimizer.zero_grad()\n            output, (sh,sc) = model(inputs, (sh,sc))    \n            loss = loss_func(output.transpose(1,2),targets)    \n            sh,sc=sh.detach(),sc.detach()\n            loss.backward()\n            nn.utils.clip_grad_norm_(model.parameters(), 5)\n            optimizer.step()    \n            tloss+=loss.item()\n        if (i+1)%1000==0:\n            print(f""at epoch {epoch} iteration {i+1}\\n            average loss = {tloss/(i+1)}"")\nIn the preceding code listing, sh and sc together form the hidden state. In particular, \nthe cell state sc acts as a conveyor belt, carrying information over many time steps, with \ninformation added or removed in each time step. The component sh is the output of \nthe LSTM cell at a given time step. It contains information about the current input and \nis used to pass information to the next LSTM cell in the sequence. \nIf you have a CUDA-enabled GPU, this training takes about 6 hours. If you use CPU \nonly, it may take a day or two, depending on your hardware. Or you can download the \npretrained weights from my website: https: //mng.bz/vJZa .\nNext, we save the trained model weights in the local folder:\nimport pickle\ntorch.save(model.state_dict(),""files/wordLSTM.pth"")\nwith open(""files/word_to_int.p"",""wb"") as fb:    \n    pickle.dump(word_to_int, fb)\nThe dictionary word_to_int  is also saved on your computer, which is a practical step \nensuring that you can generate text using the trained model without needing to repeat \nthe tokenization process.\n8.5 Generating text with the trained LSTM model\nNow that you have a trained LSTM model, you’ll learn how to use it to generate text in \nthis section. The goal is to see if the trained model can generate grammatically correct \nand coherent text by iteratively predicting the next token based on previous tokens. \nYou’ll also learn to use temperature and top-K sampling to control the creativeness of \nthe generated text.\nWhen generating text with the trained LSTM model, we start with a prompt as the \ninitial input to the model. We use the trained model to predict the most likely next \ntoken. After appending the next token to the prompt, we feed the new sequence to \nthe trained model to predict the next token again. We repeat this process until the \nsequence reaches a certain length.Iterates through all \nbatches of (x,y) in the \ntraining data\nUses the model to \npredict the output \nsequence\nCompares the \npredictions \nwith the actual \noutput and \ncalculates the \nloss\nTweaks model \nparameters to \nminimize loss\n186 chapter  8 Text generation with recurrent neural networks\n8.5.1 Generating text by predicting the next token\nFirst, we load the trained model weights and the dictionary word_to_int  from the \nlocal folder:\nmodel.load_state_dict(torch.load(""files/wordLSTM.pth"",\n                                    map_location=device))\nwith open(""files/word_to_int.p"",""rb"") as fb:    \n    word_to_int = pickle.load(fb)      \nint_to_word={v:k for k,v in word_to_int.items()}\nThe file word_to_int.p  is also available in the book’s GitHub repository. We switch \nthe positions of keys and values in the dictionary word_to_int  to create the dictionary \nint_to_word . \nTo generate text with the trained LSTM model, we need a prompt as the starting \npoint of the generated text. We’ll set the default prompt to “Anna and the.” An easy \nway to determine when to stop is to limit the generated text to a certain length, say 200 \ntokens: once the desired length is reached, we ask the model to stop generating.  \nThe following listing defines a sample()  function to generate text based on a \nprompt.\nListing 8.5    A sample()  function to generate text\nimport numpy as np\ndef sample(model, prompt, length=200):\n    model.eval()\n    text = prompt.lower().split(' ')\n    hc = model.init_hidden(1)\n    length = length - len(text)    \n    for i in range(0, length):\n        if len(text)<= seq_len:\n            x = torch.tensor([[word_to_int[w] for w in text]])\n        else:\n            x = torch.tensor([[word_to_int[w] for w \\nin text[-seq_len:]]])    \n        inputs = x.to(device)\n        output, hc = model(inputs, hc)    \n        logits = output[0][-1]\n        p = nn.functional.softmax(logits, dim=0).detach().cpu().numpy()\n        idx = np.random.choice(len(logits), p=p)    \n        text.append(int_to_word[idx])    \n    text="" "".join(text)\n    for m in "",.:;?!$()/_&%*@'`"":\n        text=text.replace(f"" {m}"", f""{m} "")\n    text=text.replace('""  ', '""')   \n    text=text.replace(""'  "", ""'"")  \n    text=text.replace('"" ', '""')   \n    text=text.replace(""' "", ""'"")     \n    return text  \nThe function sample()  takes three arguments. The first is the trained LSTM model \nyou will be using. The second is the starting prompt for text generation, which can be a Determines how many \ntokens need to be \ngenerated\nThe input is \nthe current \nsequence; \ntrims it if it’s \nlonger than \n100 tokens\nMakes a prediction using \nthe trained model\nSelects the next token \nbased on predicted \nprobabilities\nAppends the predicted \nnext token to the \nsequence and repeats\n 187 Generating text with the trained LSTM model\nphrase of any length, in quotes. The third parameter specifies the length of the text to \nbe generated, measured in tokens, with a default value of 200 tokens.\nWithin the function, we first deduct the number of tokens in the prompt from the \ntotal desired length to determine the number of tokens that need to be generated. \nWhen generating the next token, we consider the current sequence’s length. If it’s \nunder 100 tokens, we input the entire sequence into the model; if it’s over 100 tokens, \nonly the last 100 tokens of the sequence are used as input. This input is then fed into the \ntrained LSTM model to predict the subsequent token, which we then add to the cur -\nrent sequence. We continue this process until the sequence reaches the desired length.\nWhen generating the next token, the model employs the random.choice(len(logits), \np = p) method from NumPy. Here, the method’s first parameter indicates the range \nof choices, which in this case is len(logits) = 12778. This signifies that the model will \nrandomly select an integer from 0 to 12,777, with each integer corresponding to a \ndifferent token in the vocabulary. The second parameter, p, is an array containing 12,778 \nelements where each element denotes the probability of selecting a corresponding \ntoken from the vocabulary. Tokens with a higher probability in this array are more likely \nto be chosen. \nLet’s generate a passage with the model using “Anna and the prince” as the prompt \n(make sure you put a space before punctuation marks when you use your own prompt):\ntorch.manual_seed(42)\nnp.random.seed(42)\nprint(sample(model, prompt='Anna and the prince'))  \nHere, I fixed the random seed number to 42 in both PyTorch and NumPy in case you \nwant to reproduce results. The generated passage is\nanna and the prince did not forget what he had not spoken. when the \nsoftening barrier was not so long as he had talked to his brother,  all the \nhopelessness of the impression. ""official tail,  a man who had tried him,  \nthough he had been able to get across his charge and locked close,  and the \nlight round the snow was in the light of the altar villa. the article in law \nlevin was first more precious than it was to him so that if it was most easy \nas it would be as the same. this was now perfectly interested. when he had \ngot up close out into the sledge,  but it was locked in the light window with \ntheir one grass,  and in the band of the leaves of his projects,  and all the \nsame stupid woman,  and really,  and i swung his arms round that thinking of \nbed. a little box with the two boys were with the point of a gleam of filling \nthe boy,  noiselessly signed the bottom of his mouth,  and answering them \ntook the red\nYou may have noticed that the text generated is entirely in lowercase. This is because, \nduring the text processing stage, we converted all uppercase letters to lowercase to \nminimize the number of unique tokens.\nThe text generated from 6 hours of training is quite impressive! Most of the sen -\ntences adhere to grammatical norms. While it may not match the level of sophistication \nseen in text generated by advanced systems like ChatGPT, it’s a significant achievement.",11090
81-8.5.2 Temperature and top-K sampling in text generation.pdf,81-8.5.2 Temperature and top-K sampling in text generation,"188 chapter  8 Text generation with recurrent neural networks\nWith skills acquired in this exercise, you are ready to train more advanced text genera -\ntion models in later chapters. \n8.5.2 Temperature and top-K sampling in text generation\nThe creativity of the generated text can be controlled by using techniques like tem -\nperature and top-K sampling.\nTemperature adjusts the distribution of probabilities assigned to each potential \ntoken before selecting the next one. It effectively scales the logits, which are the inputs \nto the softmax function calculating these probabilities, by the value of the tempera -\nture. Logits are the outputs of the LSTM model prior to the application of the softmax \nfunction. \nIn the sample()  function we just defined, we didn’t adjust the logits, implying a \ndefault temperature of 1. A lower temperature (below 1; e.g., 0.8) results in fewer vari -\nations, making the model more deterministic and conservative, favoring more likely \nchoices. Conversely, a higher temperature (above 1; e.g., 1.5) makes it more likely to \nchoose improbable words in text generation, leading to more varied and inventive out -\nputs. However, this could also make the text less coherent or relevant, as the model \nmight opt for less probable words.\nTop-K sampling is another method to influence the output. This approach involves \nselecting the next word from the top K most probable options as predicted by the \nmodel. The probability distribution is truncated to include only the top K words. With a \nsmall K value, such as 5, the model’s choices are limited to a few highly probable words, \nresulting in more predictable and coherent but potentially less diverse and interesting \noutputs. In the sample()  function we defined earlier, we did not apply top-K sampling, \nso the value of K was effectively the size of the vocabulary (12,778 in our case).\nNext, we introduce a new function, generate() , for text generation. This function is \nsimilar to the sample()  function but includes two additional parameters: temperature  \nand top_k , allowing for more control over the creativity and randomness of the \ngenerated text. The function generate()  is defined in the following listing.\nListing 8.6    Generating text with temperature and top-K sampling\ndef generate(model, prompt , top_k=None, \n             length=200, temperature=1):\n    model.eval()\n    text = prompt.lower().split(' ')\n    hc = model.init_hidden(1)\n    length = length - len(text)    \n    for i in range(0, length):\n        if len(text)<= seq_len:\n            x = torch.tensor([[word_to_int[w] for w in text]])\n        else:\n            x = torch.tensor([[word_to_int[w] for w in text[-seq_len:]]])    \n        inputs = x.to(device)\n 189 Generating text with the trained LSTM model\n        output, hc = model(inputs, hc)\n        logits = output[0][-1]\n        logits = logits/temperature    \n        p = nn.functional.softmax(logits, dim=0).detach().cpu()    \n        if top_k is None:\n            idx = np.random.choice(len(logits), p=p.numpy())\n        else:\n            ps, tops = p.topk(top_k)    \n            ps=ps/ps.sum()\n            idx = np.random.choice(tops, p=ps.numpy())    \n        text.append(int_to_word[idx])\n    \n    text="" "".join(text)\n    for m in "",.:;?!$()/_&%*@'`"":\n        text=text.replace(f"" {m}"", f""{m} "")\n    text=text.replace('""  ', '""')   \n    text=text.replace(""'  "", ""'"")  \n    text=text.replace('"" ', '""')   \n    text=text.replace(""' "", ""'"")     \n    return text  \nCompared to the sample()  function, the new function generate()  has two more \noptional arguments: top_k  and temperature . By default, top_k  is set to None,  and \ntemperature  is set to 1. Therefore, if you call the generate()  function without speci -\nfying these two arguments, the output will be the same as what you would get from the \nfunction sample() .\nLet’s illustrate the variations in generated text by focusing on the creation of a \nsingle token. For this purpose, we’ll use “I ’ m not going to see” as the prompt (note \nthe space before the apostrophe, as we previously have done in the chapter). We call \nthe generate() function 10 times, setting its length argument to be one more than \nthe prompt’s length. This approach ensures that the function appends only one extra \ntoken to the prompt:\nprompt=""I ' m not going to see""\ntorch.manual_seed(42)\nnp.random.seed(42)\nfor _ in range(10):\n    print(generate(model, prompt, top_k=None, \n         length=len(prompt.split("" ""))+1, temperature=1))\nThe output is \ni'm not going to see you\ni'm not going to see those\ni'm not going to see me\ni'm not going to see you\ni'm not going to see her\ni'm not going to see her\ni'm not going to see the\ni'm not going to see my\ni'm not going to see you\ni'm not going to see meScales the logits with \ntemperature\nKeeps only the K \nmost probable \ncandidates\nSelects the next \ntoken from the top \nK candidates\n190 chapter  8 Text generation with recurrent neural networks\nWith the default setting of top_k = None and temperature = 1, there is some degree of \nrepetition in the output. For example, the word “you” was repeated three times. There \nare a total of six unique tokens. \nHowever, the functionality of generate()  expands when you adjust these two argu -\nments. For instance, setting a low temperature, like 0.5, and a small top_k  value, such as \n3, results in generated text that is more predictable and less creative. \nLet’s repeat the single token example. This time, we set the temperature to 0.5 and \ntop_k  value to 3:\nprompt=""I ' m not going to see""\ntorch.manual_seed(42)\nnp.random.seed(42)\nfor _ in range(10):\n    print(generate(model, prompt, top_k=3, \n         length=len(prompt.split("" ""))+1, temperature=0.5))\nThe output is\ni'm not going to see you\ni'm not going to see the\ni'm not going to see her\ni'm not going to see you\ni'm not going to see you\ni'm not going to see you\ni'm not going to see you\ni'm not going to see her\ni'm not going to see you\ni'm not going to see her\nThe output has fewer variations: there are only 3 unique tokens from 10 attempts, \n“you,” “the,” and “her.” \nLet’s see this in action by using “Anna and the prince” as our starting prompt when \nwe set the temperature to 0.5 and top_k  value to 3:\ntorch.manual_seed(42)\nnp.random.seed(42)\nprint(generate(model, prompt='Anna and the prince',\n               top_k=3,\n               temperature=0.5))\nThe output is\nanna and the prince had no milk. but,  ""answered levin,  and he stopped. \n""i've been skating to look at you all the harrows,  and i'm glad. . .  """"no,  \ni'm going to the country. """"no,  it's not a nice fellow. """"yes,  sir. \n""""well,  what do you think about it? """"why,  what's the matter? """"yes,  yes,  \n""answered levin,  smiling,  and he went into the hall. ""yes,  i'll come for \nhim and go away,  ""he said,  looking at the crumpled front of his shirt. ""i \nhave not come to see him,  ""she said,  and she went out. ""i'm very glad,  \n""she said,  with a slight bow to the ambassador's hand. ""i'll go to the door. \n""she looked at her watch,  and she did not know what to say \n 191 Generating text with the trained LSTM model\nExercise 8.4\nGenerate text by setting temperature to 0.6 and top_k  to 10 and using “Anna and the \nnurse” as the starting prompt. Set the random seed number to 0 in both PyTorch and \nNumPy.\nConversely, opting for a higher temperature  value, like 1.5, coupled with a higher \ntop_k  value, for instance, None  (enabling selection from the entire pool of 12,778 \ntokens), leads to outputs that are more creative and less predictable. This is demon -\nstrated next, in the single token example. This time, we set the temperature to 2 and \ntop_k  value to None :\nprompt=""I ' m not going to see""\ntorch.manual_seed(42)\nnp.random.seed(42)\nfor _ in range(10):\n    print(generate(model, prompt, top_k=None, \n         length=len(prompt.split("" ""))+1, temperature=2))\nThe output is \ni'm not going to see them\ni'm not going to see scarlatina\ni'm not going to see behind\ni'm not going to see us\ni'm not going to see it\ni'm not going to see it\ni'm not going to see a\ni'm not going to see misery\ni'm not going to see another\ni'm not going to see seryozha\nThe output has almost no repetition: there are 9 unique tokens from 10 attempts; only \nthe word “it” was repeated.\nLet’s again use “Anna and the prince” as the initial prompt but set the temperature \nto 2 and top_k value to None and see what happens:\ntorch.manual_seed(42)\nnp.random.seed(42)\nprint(generate(model, prompt='Anna and the prince',\n               top_k=None,\n               temperature=2))\nThe generated text is \nanna and the prince took sheaves covered suddenly people. ""pyotr marya \nborissovna,  propped mihail though her son will seen how much evening her \nhusband;  if tomorrow she liked great time too. ""adopted heavens details \nfor it women from this terrible,  admitting this touching all everything \nill with flirtation shame consolation altogether:  ivan only all the circle \nwith her honorable carriage in its house dress,  beethoven ashamed had the \nconversations raised mihailov stay of close i taste work? ""on new farming \n192 chapter  8 Text generation with recurrent neural networks\nshow ivan nothing. hat yesterday if interested understand every hundred of \ntwo with six thousand roubles according to women living over a thousand:  \nsnetkov possibly try disagreeable schools with stake old glory mysterious one \nhave people some moral conclusion,  got down and then their wreath. darya \nalexandrovna thought inwardly peaceful with varenka out of the listen from \nand understand presented she was impossible anguish. simply satisfied with \nstaying after presence came where he pushed up his hand as marya her pretty \nhands into their quarters. waltz was about the rider gathered;  sviazhsky \nfurther alone have an hand paused riding towards an exquisite\nThe output generated is not repetitive, although it lacks coherence in many places.\nExercise 8.5\nGenerate text by setting temperature to 2 and top_k  to 10000 and using “Anna and the \nnurse” as the starting prompt. Set the random seed number to 0 in both PyTorch and \nNumPy. \nIn this chapter, you have acquired foundational skills in NLP, including word-level \ntokenization, word embedding, and sequence prediction. Through these exercises, \nyou’ve learned to construct a language model based on word-level tokenization and \nhave trained it using LSTM for text generation. Moving forward, the next few chapters \nwill introduce you to training Transformers, the type of models used in systems like \nChatGPT. This will provide you with a more in-depth understanding of advanced text \ngeneration techniques.\nSummary\n¡ RNNs are a specialized form of artificial neural network designed to recognize \npatterns in sequences of data, such as text, music, or stock prices. Unlike tradi -\ntional neural networks, which process inputs independently, RNNs have loops in \nthem, allowing information to persist. LSTM networks are improved versions of \nRNNs.    \n¡ There are three approaches to tokenization. The first is character tokenization, \nwhere the text is divided into its constituent characters. The second approach \nis word tokenization, where the text is split into individual words. The third \napproach is subword tokenization, which breaks words into smaller, meaningful \ncomponents called subwords. \n¡ Word embedding is a method that transforms words into compact vector rep -\nresentations, capturing their semantic information and interrelationships. This \ntechnique is vital in NLP, especially since deep neural networks, including mod -\nels like LSTM and Transformers, require numerical input. \n¡ Temperature is a parameter that influences the behavior of text generation mod -\nels. It controls the randomness of the predictions by scaling the logits (the inputs \nto the softmax function for probability calculation) before applying softmax. \n 193 Summary\nLow temperature makes the model more conservative in its predictions but also \nmore repetitive. At higher temperatures, the model becomes less repetitive and \nmore innovative, increasing the diversity of the generated text.\n¡ Top-K sampling is another way to influence the behavior of text generation mod -\nels. It involves selecting the next word from the K most likely candidates, as deter -\nmined by the model. The probability distribution is truncated to keep only the \ntop K words. Small values of K make the output more predictable and coherent \nbut potentially less diverse and interesting.",12724
82-9.1.1 The attention mechanism.pdf,82-9.1.1 The attention mechanism,"1949A line-by-line \nimplementation of  \nattention and Transformer\nThis chapter covers\n¡ The architecture and functionalities of encoders  \n and decoders in Transformers\n¡ How the attention mechanism uses query, key, and  \n value to assign weights to elements in a sequence \n¡ Different types of Transformers\n¡ Building a Transformer from scratch for language  \n translation\nTransformers are advanced deep learning models that excel in handling sequence-\nto-sequence prediction challenges, outperforming older models like recurrent \nneural networks (RNNs) and convolutional neural networks (CNNs). Their strength \nlies in effectively understanding the relationships between elements in input and \noutput sequences over long distances, such as two words far apart in the text. \nUnlike RNNs, Transformers are capable of parallel training, significantly cutting \ndown training times and enabling the handling of vast datasets. This transformative \narchitecture has been pivotal in the development of large language models (LLMs) \nlike ChatGPT, BERT, and T5, marking a significant milestone in AI progress.\n 195 Introduction to attention and Transformer\nPrior to the introduction of Transformers in the groundbreaking 2017 paper “Atten -\ntion Is All You Need” by a group of Google researchers,1 natural language processing \n(NLP) and similar tasks primarily relied on RNNs, including long short-term memory \n(LSTM) models. RNNs, however, process information sequentially, limiting their speed \ndue to the inability to train in parallel and struggling with maintaining information \nabout earlier parts of a sequence, thus failing to capture long-term dependencies.\nThe revolutionary aspect of the Transformer architecture is its attention mechanism. \nThis mechanism assesses the relationship between words in a sequence by assigning \nweights, determining the degree of relatedness in meaning among words based on the \ntraining data. This enables models like ChatGPT to comprehend relationships between \nwords, thus understanding human language more effectively. The nonsequential pro -\ncessing of inputs allows for parallel training, reducing training time and facilitating the \nuse of large datasets, thereby powering the rise of knowledgeable LLMs and the current \nsurge in AI advancements.\nIn this chapter, we will implement, line by line, the creation of a Transformer from \nthe ground up, based on the paper “Attention Is All You Need.” The Transformer, once \ntrained, can handle translations between any two languages (such as German to English \nor English to Chinese). In the next chapter, we’ll focus on training the Transformer \ndeveloped here to perform English to French translations. \nTo build the Transformer from scratch, we’ll explore the inner workings of the \nself-attention mechanism, including the roles of query, key, and value vectors, and the \ncomputation of scaled dot product attention (SDPA). We’ll construct an encoder layer \nby integrating layer normalization and residual connection into a multihead attention \nlayer and combining it with a feed-forward layer. We’ll then stack six of these encoder \nlayers to form the encoder. Similarly, we’ll develop a decoder in the Transformer that is \ncapable of generating translation one token at a time, based on previous tokens in the \ntranslation and the encoder’s output.\nThis groundwork will equip you to train the Transformer for translations between \nany two languages. In the next chapter, you’ll learn to train the Transformer using a \ndataset containing more than 47,000 English-to-French translations. You’ll witness the \ntrained model translating common English phrases to French with an accuracy compa -\nrable to using Google Translate.\n9.1 Introduction to attention and Transformer\nTo grasp the concept of Transformers in machine learning, it’s essential to first under -\nstand the attention mechanism. This mechanism allows Transformers to recognize \nlong-range dependencies between sequence elements, a feature that sets them apart \nfrom earlier sequence prediction models like RNNs. With this mechanism, Transform -\ners can simultaneously focus on every element in a sequence, comprehending the con -\ntext of each word.\n1 Vaswani et al., 2017, “Attention Is All You Need.” https: //arxiv.org/abs/1706.03762 .\n196 chapter  9 A line-by-line implementation of attention and Transformer \nConsider the word “bank” to illustrate how the attention mechanism interprets \nwords based on context. In the sentence “I went fishing by the river yesterday, remain -\ning near the bank the whole afternoon,” the word ”bank“ is linked to “fishing” because \nit refers to the area beside a river. Here, a Transformer understands “bank” as part of \nthe river’s terrain.\nBy contrast, in “Kate went to the bank after work yesterday and deposited a check \nthere,” “bank” is connected to “check,” leading the Transformer to identify “bank” as \na financial institution. This example showcases how Transformers discern word mean -\nings based on their surrounding context.\nIn this section, you’ll dive deeper into the attention mechanism, exploring how it \nworks. This process is crucial for determining the importance, or weights, of various \nwords within a sentence. After that, we’ll examine the structure of different Trans -\nformer models, including one that can translate between any two languages.\n9.1.1 The attention mechanism\nThe attention mechanism is a method used to determine the interconnections between \nelements in a sequence. It calculates scores to indicate how one element relates to \nothers in the sequence, with higher scores denoting a stronger relationship. In NLP, \nthis mechanism is instrumental in linking words within a sentence meaningfully. This \nchapter will guide you through implementing the attention mechanism for language \ntranslation.\nWe’ll construct a Transformer composed of an encoder and a decoder for that pur -\npose. We’ll then train the Transformer to translate English to French in the next chap -\nter. The encoder transforms an English sentence, such as “How are you?”, into vector \nrepresentations that capture its meaning. The decoder then uses these vector represen -\ntations to generate the French translation.\nTo transform the phrase “How are you?” into vector representations, the model first \nbreaks it down into tokens [ how, are, you, ? ], a process similar to what you have \ndone in chapter 8. These tokens are each represented by a 256-dimensional vector \nknown as word embeddings, which capture the meaning of each token. The encoder \nalso employs positional encoding, a method to determine the positions of tokens in the \nsequence. This positional encoding is added to the word embeddings to create input \nembeddings, which are then used to calculate self-attention. The input embedding for \n“How are you?” forms a tensor with dimensions (4, 256), where 4 represents the num -\nber of tokens and 256 is the dimensionality of each embedding. \nWhile there are different ways to calculate attention, we’ll use the most common \nmethod, SDPA. This mechanism is also called self-attention because the algorithm cal -\nculates how a word attends to all words in the sequence, including the word itself. Fig -\nure 9.1 provides a diagram of how to calculate SDPA. \n 197 Introduction to attention and Transformer\nWQWKWV\nInput X Input X Input XMatrix \n multiplicationScaleSoftmax \nactivationMatrix multiplication\nAttention score\nK Q VScaled\nattention scoreAttention\nAttention weight\nInput X is passed through three linear \nneural networks, with weights W Q, \nWK, and W V, respectively, to obtain\nquery Q, key K, and value V. \nThe weights W Q, WK, and W V are \nﬁrst randomly initialized and then\nlearned from the training data. \nFigure 9.1    A diagram of the self-attention mechanism. To calculate attention, the input embedding X \nis first passed through three neural layers with weights, WQ, WK, and WV, respectively. The outputs are \nquery Q, key K, and value V. The scaled attention score is the product of Q and K divided by the square \nroot of the dimension of K, dk. We apply the softmax function on the scaled attention score to obtain the \nattention weight. The attention is the product of the attention weight and value V. \nThe utilization of query, key, and value in calculating attention is inspired by retrieval \nsystems. Consider visiting a public library to find a book. If you search for “machine \nlearning in finance” in the library’s search engine, this phrase becomes your query. \nThe book titles and descriptions in the library serve as the keys. Based on the similar -\nity between your query and these keys, the library’s retrieval system suggests a list of \nbooks (values). Books containing “machine learning,” “finance,” or both in their titles \nor descriptions are likely to rank higher. In contrast, books unrelated to these terms \nwill have a lower matching score and thus are less likely to be recommended. \nTo calculate SDPA, the input embedding X is processed through three distinct neu -\nral network layers. The corresponding weights for these layers are WQ, WK, and WV; each \nhas a dimension of 256 × 256. These weights are learned from data during the training \nphase. Thus, we can calculate query Q, key K, and value V as Q = X * WQ, K = X * QK, \nand V = X * WV. The dimensions of Q, K, and V match those of the input embedding X, \nwhich are 4 × 256.\n \n198 chapter  9 A line-by-line implementation of attention and Transformer \nSimilar to the retrieval system example we mentioned earlier, in the attention mech -\nanism, we assess the similarities between the query and key vectors using the SDPA \napproach. SDPA involves calculating the dot product of the query (Q) and key (K) \nvectors. A high dot product indicates a strong similarity between the two vectors and \nvice versa. For instance, in the sentence “How are you?”, the scaled attention score is \ncomputed as follows:\n(9.1)\nwhere dk represents the dimension of the key vector K, which in our case is 256. We \nscale the dot product of Q and K by the square root of dk to stabilize training. This scal -\ning is done to prevent the dot product from growing too large in magnitude. The dot \nproduct between the query and key vectors can become very large when the dimen -\nsion of these vectors (i.e., the depth of the embedding) is high. This is because each \nelement of the query vector is multiplied by each element of the key vector, and these \nproducts are then summed up.\nThe next step is to apply the softmax function to these attention scores, converting \nthem into attention weights. This ensures that the total attention a word gives to all \nwords in the sentence sums to 100%. \nWQ\nsize (256, 256) Embedding of “How are you?” \nwith a size of (4, 256) \nWK\nsize (256, 256) Embedding of “How are you?” \nwith a size of (4, 256) \nQ\nsize (4, 256)K\nsize (4, 256)0.1 0.4 0.4 0.1\n0.4 0.2 0.3 0.1\n0.4 0.3 0.2 0.1\n0.1 0.1 0.1 0.7How\nHoware\nare\nyouyou\n??\nAttention weights \nsize (4, 4) \nFigure 9.2    Steps to calculate attention weights. The input embedding is passed through two neural \nnetworks to obtain query Q and key K. The scaled attention scores are calculated as the dot product of \nQ and K divided by the square root of the dimension of K. Finally, we apply the softmax function on the \nscaled attention scores to obtain attention weights, which demonstrate how each element is related to \nall other elements in the sequence. \nFigure 9.2 shows how this is done. For the sentence “How are you?”, the attention \nweights form a 4 × 4 matrix, which shows how each token in [ ""How"", ""are,"" ""you,"" \n""?""] is related to all other tokens (including itself). The numbers in figure 9.2 are \nmade-up numbers to illustrate the point. For example, the first row in the attention \n 199 Introduction to attention and Transformer\nweights shows that the token ""How""  gives 10% of its attention to itself and 40%, 40%, \nand 10% to the other three tokens, respectively. \nThe final attention is then calculated as the dot product of these attention weights \nand the value vector V (also illustrated in figure 9.3):\n(9.2)\nWQ\nsize (256, 256) WK\nsize (256, 256) \nQ\nsize (4, 256)K \nsize (4, 256)\nWV\nsize (256, 256) Embedding of “How are you?” \nwith a size of (4, 256) \nV\nsize (4, 256)Embedding of “How are you?”\nwith a size of (4, 256) Embedding of “How are you?” \nwith a size of (4, 256) \nFigure 9.3    Use attention weights and the value vector to calculate the attention vector. The input \nembedding is passed through a neural network to obtain value V. The final attention is the dot product of \nthe attention weights that we calculated earlier and the value vector V. \nThis output also maintains a dimension of 4 × 256, consistent with our input \ndimensions. \nTo summarize, the process begins with the input embedding X of the sentence “How \nare you?”, which has a dimension of 4 × 256. This embedding captures the meanings \nof the four individual tokens but lacks contextualized understanding. The attention \nmechanism ends with the output attention(Q,K,V) , which maintains the same \ndimension of 4 × 256. This output can be viewed as a contextually enriched combina -\ntion of the original four tokens. The weighting of the original tokens varies based on \nthe contextual relevance of each token, granting more significance to words that are \nmore important within the sentence’s context. Through this procedure, the attention \nmechanism transforms vectors representing isolated tokens into vectors imbued with \ncontextualized meanings, thereby extracting a richer, more nuanced understanding \nfrom the sentence.\nFurther, instead of using one set of query, key, and value vectors, Transformer models \nuse a concept called multihead attention. For example, the 256-dimensional query, key, \nand value vectors can be split into say, 8, heads, and each head has a set of query, key, \nand value vectors with dimensions of 32 (because 256/8 = 32). Each head pays atten -\ntion to different parts or aspects of the input, enabling the model to capture a broader \nrange of information and form a more detailed and contextual understanding of the \ninput data. Multihead attention is especially useful when a word has multiple meanings",14422
83-9.1.2 The Transformer architecture.pdf,83-9.1.2 The Transformer architecture,"200 chapter  9 A line-by-line implementation of attention and Transformer \nin a sentence, such as in a pun. Let’s continue the “bank” example we mentioned ear -\nlier. Consider the pun joke, “Why is the river so rich? Because it has two banks.” In the \nproject of translating English to French in the next chapter, you’ll implement first-hand \nsplitting Q, K, and V into multiple heads to calculate attention in each head before con -\ncatenating them back into one single attention vector. \n9.1.2 The Transformer architecture\nThe concept of the attention mechanism was introduced by Bahdanau, Cho, and Ben -\ngio in 2014.2 It became widely used after the groundbreaking paper “Attention Is All \nYou Need,” which focused on creating a model for machine language translation. The \narchitecture of this model, known as the Transformer, is depicted in figure 9.4. It fea -\ntures an encoder-decoder structure that relies heavily on the attention mechanism. In \nthis chapter, you’ll build this model from scratch, coding it line by line, intending to \ntrain it for translation between any two languages. \nInput embeddingEncoder\nInputsPositional\nencodingEncoder blockN blocksAbstract\nrepresentation\nOutput embeddingDecoder\nOutputsPositional\nencodingDecoder blockN blocksLinear layerOutput\nprobabilities\nSoftmax \nactivation\nFigure 9.4    The Transformer architecture. The encoder in the Transformer (left side of the diagram), \nwhich consists of N identical encoder layers, learns the meaning of the input sequence and converts it \ninto vectors that represent its meaning. It then passes these vectors to the decoder (right side of the \ndiagram), which consists of N identical decoder layers. The decoder constructs the output (e.g., the \nFrench translation of an English phrase) by predicting one token at a time, based on previous tokens in \nthe sequence and vector representations from the encoder. The generator on the top right is the head \nattached to the output from the decoder so that the output is the probability distribution over all tokens \nin the target language (e.g., the French vocabulary).\n2 Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio, 2014, “Neural Machine Translation by Jointly Learning to \nAlign and Translate.” h ttps://arxiv.org/abs/1409.0473 .\n 201 Introduction to attention and Transformer\nLet’s use English-to-French translation as our example. The Transformer’s encoder \ntransforms an English sentence like “I don’t speak French” into vector representations \nthat store its meaning. The Transformer’s decoder then processes them to produce \nthe French translation “Je ne parle pas français.” The encoder’s role is to capture the \nessence of the original English sentence. For instance, if the encoder is effective, it \nshould translate both “I don’t speak French” and “I do not speak French” into simi -\nlar vector representations. Consequently, the decoder will interpret these vectors and \ngenerate similar translations. Interestingly, when using ChatGPT, these two English \nphrases indeed result in the same French translation.\nThe encoder in the Transformer approaches the task by first tokenizing both the \nEnglish and French sentences. This is similar to the process described in chapter 8 but \nwith a key difference: it employs subword tokenization. Subword tokenization is a tech -\nnique used in NLP to break words into smaller components, or subwords, allowing for \nmore efficient and nuanced processing. For example, as you’ll see in the next chapter, \nthe English phrase “I do not speak French” is divided into six tokens: ( i, do, not, \nspeak, fr, ench ). Similarly, its French counterpart “Je ne parle pas français ” is toke -\nnized into six parts: ( je, ne, parle, pas, franc, ais ). This method of tokenization \nenhances the Transformer’s ability to handle language variations and complexities.\nDeep learning models, including Transformers, can’t directly process text, so tokens \nare indexed using integers before being fed to the model. These tokens are typically \nfirst represented using one-hot encoding, as we discussed in chapter 8. We then pass \nthem through a word embedding layer to compress them into vectors with continuous \nvalues of a much smaller size, such as a length of 256. Thus, after applying word embed -\nding, the sentence “I do not speak French” is represented by a 6 × 256 matrix.\nTransformers process input data such as sentences in parallel, unlike RNNs, which \nhandle data sequentially. This parallelism enhances their efficiency but doesn’t inher -\nently allow them to recognize the sequence order of the input. To address this, Trans -\nformers add positional encodings to the input embeddings. These positional encodings \nare unique vectors assigned to each position in the input sequence and align in dimen -\nsion with the input embeddings. The vector values are determined by a specific posi -\ntional function, particularly involving sine and cosine functions of varying frequencies, \ndefined as\n(9.3)\nIn these equations, vectors are calculated using the sine function for even indexes and \nthe cosine function for odd indexes. The two parameters pos and i represent the posi -\ntion of a token within the sequence and the index within the vector, respectively. As an \nillustration, consider the positional encoding for the phrase “I do not speak French.” \nThis is depicted as a 6 × 256 matrix, the same size as the word embedding for the \n202 chapter  9 A line-by-line implementation of attention and Transformer \nsentence. Here, pos ranges from 0 to 5, and the indexes 2i and 2i + 1 collectively span \n256 distinct values (from 0 to 255). A beneficial aspect of this positional encoding \napproach is that all values are constrained within the range of –1 to 1.\nIt’s important to note that each token position is uniquely identified by a \n256-dimensional vector, and these vector values remain constant throughout training. \nBefore being input to the attention layers, these positional encodings are added to the \nword embeddings of the sequence. In the example of the sentence “I do not speak \nFrench,” the encoder generates both word embedding and positional encoding, each \nhaving dimensions of 6 × 256, before combining them into a single 6 × 256-dimensional \nrepresentation. Subsequently, the encoder applies the attention mechanism to refine \nthis embedding into more sophisticated vector representations that capture the overall \nmeaning of the phrase, before passing them to the decoder.\nThe Transformer’s encoder, as depicted in figure 9.5, is made up of six identical \nlayers (N = 6). Each of these layers comprises two distinct sublayers. The first sublayer is \na multihead self-attention layer, similar to what was discussed earlier. The second sub -\nlayer is a basic, position-wise, fully connected feed-forward network. This network treats \neach position in the sequence independently rather than as sequential elements. In \nthe model’s architecture, each sublayer incorporates layer normalization and a residual \nconnection. Layer normalization normalizes observations to have zero mean and unit \nstandard deviation. Such normalization helps stabilize the training process. After the \nnormalization layer, we perform the residual connection. This means the input to each \nsublayer is added to its output, enhancing the flow of information through the network. \nNormalize & add\nFeed forward\nNormalize & add\nMultihead attentionEncoder, consists of \nN=6 identical encoder \nlayers  \nSecond sublayer of each\nencoder layer:\nFeed-forward network with layer\nnormalization & residual connection  \nFirst sublayer of each\nencoder layer:\nMultihead self attention with layer\nnormalization & residual connectionOutput \n...\nEncoder layer 1  \n Encoder layer 2 \n Encoder layer 6 \n \nInput  \nFigure 9.5    The structure of the encoder in the Transformer. The encoder consists of N = 6 identical \nencoder layers. Each encoder layer contains two sublayers. The first sublayer is a multihead self-\nattention layer and the second is a feed-forward network. Each sublayer uses layer normalization and \nresidual connection. \n 203 Introduction to attention and Transformer\nThe decoder of the Transformer model, as seen in figure 9.6, is comprised of six \nidentical decoder layers (N = 6). Each of these decoder layers features three sublayers: \na multihead self-attention sublayer, a sublayer that performs multihead cross attention \nbetween the output from the first sublayer and the encoder’s output, and a feed-forward \nsublayer. Note that the input to each sublayer is the output from the previous sublayer. \nFurther, the second sublayer in the decoder layer also takes the output from the encoder \nas input. This design is crucial for integrating information from the encoder: this is how \nthe decode generates translations based on the output from the encoder. \nAdd & normalize\nFeed forward\nAdd & normalize\nMultihead attentionDecoder layer 1\n Third sublayer of each\ndecoder layer:\nFeed-forward network with layer \nnormalization & residual connection \nSecond sublayer of each\ndecoder layer:\nMultihead cross attention\nwith layer normalization & residual\nconnection \nAdd & normalize\nMasked \nmultihead attention\nInputFirst sublayer of each\ndecoder layer:\nMasked multihead self attention\nwith layer normalization & residual\nconnection Encoder’s outputDecoder, consists of\nN=6 identical decode r\nlayers  Output\n...\nDecoder layer 2 \n Decoder layer 6 \n \nFigure 9.6    The structure of the decoder in the Transformer. The decoder consists of N = 6 identical \ndecoder layers. Each decoder layer contains three sublayers. The first sublayer is a masked multihead \nself-attention layer. The second is a multihead cross-attention layer to calculate the cross attention \nbetween the output from the first sublayer and the output from the encoder. The third sublayer is a feed-\nforward network. Each sublayer uses layer normalization and residual connection.\nA key aspect of the decoder’s self-attention sublayer is the masking mechanism. This \nmask prevents the model from accessing future positions in the sequence, ensuring \nthat predictions for a particular position can only depend on previously known ele -\nments. This sequential dependency is vital for tasks like language translation or text \ngeneration.",10429
84-9.2 Building an encoder.pdf,84-9.2 Building an encoder,"204 chapter  9 A line-by-line implementation of attention and Transformer \nThe decoding process begins with the decoder receiving an input phrase in French. \nThe decoder transforms the French tokens into word embeddings and positional \nencodings before combining them into a single embedding. This step ensures that the \nmodel not only understands the semantic content of the phrase but also maintains the \nsequential context, which is crucial for accurate translation or generation tasks.\nThe decoder operates in an autoregressive manner, generating the output sequence \none token at a time. At the first time step, it starts with the ""BOS"" token, which indi -\ncates the beginning of a sentence. Using this start token as its initial input, the decoder \nexamines vector representations of the English phrase “I do not speak French” and \nattempts to predict the first token following ""BOS"" . Suppose the decoder’s first predic -\ntion is ""Je"" . In the next time step, it then uses the sequence ""BOS Je""  as its new input \nto predict the following token. This process continues iteratively, with the decoder add -\ning each newly predicted token to its input sequence for the subsequent prediction. \nThe translation process is designed to conclude when the decoder predicts the \n""EOS""  token, signifying the end of the sentence. When preparing for the training data, \nwe add EOS to the end of each phrase, so the model has learned that it means the end \nof a sentence. Upon reaching this token, the decoder recognizes the completion of \nthe translation task and ceases its operation. This autoregressive approach ensures that \neach step in the decoding process is informed by all previously predicted tokens, allow -\ning for coherent and contextually appropriate translations.\n9.1.3 Different types of Transformers\nThere are three types of Transformers: encoder-only Transformers, decoder-only \nTransformers, and encoder-decoder Transformers. We are using an encoder-decoder \nTransformer in this chapter and the next, but you’ll get a chance to explore firsthand \ndecoder-only Transformers later in the book. \nAn encoder-only Transformer consists of N identical encoder layers as shown on the \nleft side of figure 9.4 and is capable of converting a sequence into abstract continuous \nvector representations. For example, BERT is an encoder-only Transformer that con -\ntains 12 encoder layers. An encoder-only Transformer can be used for text classifica -\ntion, for example. If two sentences have similar vector representations, we can classify \nthe two sentences into one category. On the other hand, if two sequences have very \ndifferent vector representations, we can put them in different categories.\nA decoder-only Transformer also consists of N identical layers, and each layer is a \ndecoder layer as shown on the right side of figure 9.4. For example, ChatGPT is a decoder-\nonly Transformer that contains many decoder layers. The decoder-only Transformer can \ngenerate text based on a prompt, for example. It extracts the semantic meaning of the \nwords in the prompt and predicts the most likely next token. It then adds the token to \nthe end of the prompt and repeats the process until the text reaches a certain length.\nThe machine language translation Transformer we discussed earlier is an example \nof an encoder-decoder Transformer. They are needed for handling complicated tasks, \nsuch as text-to-image generation or speech recognition. Encoder-decoder Transform -\ners combine the strengths of both encoders and decoders. Encoders are efficient in",3598
85-9.2.1 The attention mechanism.pdf,85-9.2.1 The attention mechanism,"205 Building an encoder\nprocessing and understanding input data, while decoders excel in generating output. \nThis combination allows the model to effectively understand complex inputs (like text \nor speech) and generate intricate outputs (like images or transcribed text).\n9.2 Building an encoder\nWe’ll develop and train an encoder-decoder Transformer designed for machine \nlanguage translation. The coding in this project is adapted from the work of Chris \nCui in translating Chinese to English ( https: //mng.bz/9o1o ) and Alexander Rush’s \nGerman-to-English translation project ( https: //mng.bz/j0mp ). \nThis section discusses how to construct an encoder in the Transformer. Specifically, \nwe’ll dive into the process of building various sublayers within each encoder layer and \nimplementing the multihead self-attention mechanism.\n9.2.1 The attention mechanism\nWhile there are different attention mechanisms, we’ll use the SDPA because it’s widely \nused and effective. The SDPA attention mechanism uses query, key, and value to calcu -\nlate the relationships among elements in a sequence. It assigns scores to show how an \nelement is related to all elements in a sequence (including the element itself). \nInstead of using one set of query, key, and value vectors, the Transformer model uses \na concept called multihead attention. Our 256-dimensional query, key, and value vec -\ntors are split into 8 heads, and each head has a set of query, key, and value vectors with \ndimensions of 32 (because 256/8 = 32). Each head pays attention to different parts or \naspects of the input, enabling the model to capture a broader range of information and \nform a more detailed and contextual understanding of the input data. For example, \nmultihead attention allows the model to capture the multiple meanings of the word \n“bank” in the pun joke, “Why is the river so rich? Because it has two banks.”\nTo implement this, we define an attention()  function in the local module ch09util. \nDownload the file ch09util.py from the book’s GitHub repository ( https: //github.com/\nmarkhliu/DGAI ) and store it in the /utils/ directory on your computer. The atten -\ntion() function is defined as shown in the following listing.\nListing 9.1    Calculating attention based on query, key, and value\ndef attention(query, key, value, mask=None, dropout=None):\n    d_k = query.size(-1)\n    scores = torch.matmul(query, \n              key.transpose(-2, -1)) / math.sqrt(d_k)    \n    if mask is not None:\n        scores = scores.masked_fill(mask == 0, -1e9)    \n    p_attn = nn.functional.softmax(scores, dim=-1)    \n    if dropout is not None:\n        p_attn = dropout(p_attn)\n    return torch.matmul(p_attn, value), p_attn    Scaled attention score is the dot product of \nquery and key, scaled by the square root of dk.\nIf there is a \nmask, hides \nfuture \nelements in \nthe sequence\nCalculates attention \nweights\nReturns both attention \nand attention weights\n206 chapter  9 A line-by-line implementation of attention and Transformer \nThe attention()  function takes query, key, and value as inputs and calculates attention \nand attention weights as we discussed earlier in this chapter. The scaled attention score \nis the dot product of query and key, scaled by the square root of the dimension of the \nkey, dk. We apply the softmax function on the scaled attention score to obtain attention \nweights. Finally, attention is calculated as the dot product of attention weights and value. \nLet’s use our running example to show how multihead attention works (see figure \n9.7). The embedding for “How are you?” is a tensor with a size of (1, 6, 256), as we \nexplained in the last section (after we add positional encoding to word embedding). \nNote that 1 means there is one sentence in the batch, and there are six tokens in the \nsentence instead of four because we add BOS and EOS to the beginning and the end \nof the sequence. This embedding is passed through three linear layers to obtain query \nQ, key K, and value V, each of the same size (1, 6, 256). These are divided into eight \nheads, resulting in eight distinct sets of Q, K, and V, now sized (1, 6, 256/8 = 32) each. \nThe attention function, as defined earlier, is applied to each of these sets, yielding eight \nattention outputs, each also sized (1, 6, 32). We then concatenate the eight attention \noutputs into one single attention, and the result is a tensor with a size of (1, 6, 32 × 8 = \n256). Finally, this combined attention passes through another linear layer sized 256 × \n256, leading to the output from the MultiHeadAttention()  class. This output main -\ntains the original input’s dimensions, which are (1, 6, 256).\nMatrix\n multiplication\nQ1\nsize (1, 6, 32) ScaleSoftmax\nactivationMatrix multiplication\nAttention scoreScaled\nattention scoreAttention weightAttention1\nsize (1, 6, 32)\nK1\nsize (1, 6, 32) V1\nsize (1, 6, 32) Matrix\n multiplication\nQ8\nsize (1, 6, 32) ScaleSoftmax\nactivationMatrix multiplication\nAttention scoreScaled \nattention scoreAttention weightAttention8\nsize (1, 6, 32)\nK8\nsize (1, 6, 32) V8\nsize (1, 6, 32) Attentions 2 to 7Concatenate 8 attention vectors into one single attention, size (1, 6, 256)Attention\nFigure 9.7    An example of multihead attention. This diagram uses the calculation of the multihead \nself attention for the phrase “How are you?” as an example. We first pass the embedding through three \nneural networks to obtain query Q, key K, and value V, each with a size of (1, 6, 256). We split them into \neight heads, each with a set of Q, k, and V, with a size of (1, 6, 32). We calculate the attention in each \nhead. The attention vectors from the eight heads are then joined back into one single attention vector, \nwith a size of (1, 6, 256). \n 207 Building an encoder\nThis is implemented in the following code listing in the local module. \nListing 9.2    Calculating multihead attention\nfrom copy import deepcopy\nclass MultiHeadedAttention(nn.Module):\n    def __init__(self, h, d_model, dropout=0.1):\n        super().__init__()\n        assert d_model % h == 0\n        self.d_k = d_model // h\n        self.h = h\n        self.linears = nn.ModuleList([deepcopy(\n            nn.Linear(d_model, d_model)) for i in range(4)])\n        self.attn = None\n        self.dropout = nn.Dropout(p=dropout)\n    def forward(self, query, key, value, mask=None):\n        if mask is not None:\n            mask = mask.unsqueeze(1)\n        nbatches = query.size(0)  \n        query, key, value = [l(x).view(nbatches, -1, self.h,\n           self.d_k).transpose(1, 2)    \n         for l, x in zip(self.linears, (query, key, value))]    \n        x, self.attn = attention(\n            query, key, value, mask=mask, dropout=self.dropout)    \n        x = x.transpose(1, 2).contiguous().view(\n            nbatches, -1, self.h * self.d_k)    \n        output = self.linears[-1](x)    \n        return output\nEach encoder layer and decoder layer also contain a feed-forward sublayer, which \nis a two-layer fully connected neural network, with the purpose of enhancing the \nmodel’s ability to capture and learn intricate features in the training dataset. Further, \nthe neural network processes each embedding independently. It doesn’t treat the \nsequence of embeddings as a single vector. Therefore, we often call it a position-wide \nfeed-forward network (or a 1D convolutional network). For that purpose, we define a \nPositionwiseFeedForward()  class in the local module as follows:\nclass PositionwiseFeedForward(nn.Module):\n    def __init__(self, d_model, d_ff, dropout=0.1):\n        super().__init__()\n        self.w_1 = nn.Linear(d_model, d_ff)\n        self.w_2 = nn.Linear(d_ff, d_model)\n        self.dropout = nn.Dropout(dropout)\n    def forward(self, x):\n        h1 = self.w_1(x)\n        h2 = self.dropout(h1)\n        return self.w_2(h2) Passes input \nthrough three \nlinear layers to \nobtain Q, K, V , \nand splits \nthem into \nmultiheads\nCalculates \nattention and \nattention \nweights for \neach head\nConcatenates attention \nvectors from multiheads into \none single attention vector\nPasses the output \nthrough a linear layer",8234
86-9.2.2 Creating an encoder.pdf,86-9.2.2 Creating an encoder,"208 chapter  9 A line-by-line implementation of attention and Transformer \nThe PositionwiseFeedForward()  class is defined with two key parameters: d_ff , \nthe dimensionality of the feed-forward layer, and d_model , representing the model’s \ndimension size. Typically, d_ff  is chosen to be four times the size of d_model . In our \nexample, d_model  is 256, and we therefore set d_ff  to 256 * 4 = 1024. This practice of \nenlarging the hidden layer in comparison to the model size is a standard approach in \nTransformer architectures. It enhances the network’s ability to capture and learn intri -\ncate features in the training dataset.\n9.2.2 Creating an encoder\nTo create an encoder layer, we first define the following EncoderLayer()  class and \nSublayerConnection()  class.\nListing 9.3    A class to define an encoder layer\nclass EncoderLayer(nn.Module):\n    def __init__(self, size, self_attn, feed_forward, dropout):\n        super().__init__()\n        self.self_attn = self_attn\n        self.feed_forward = feed_forward\n        self.sublayer = nn.ModuleList([deepcopy(\n        SublayerConnection(size, dropout)) for i in range(2)])\n        self.size = size  \n    def forward(self, x, mask):\n        x = self.sublayer[0](\n            x, lambda x: self.self_attn(x, x, x, mask))    \n        output = self.sublayer[1](x, self.feed_forward)    \n        return output\nclass SublayerConnection(nn.Module):\n    def __init__(self, size, dropout):\n        super().__init__()\n        self.norm = LayerNorm(size)\n        self.dropout = nn.Dropout(dropout)\n    def forward(self, x, sublayer):\n        output = x + self.dropout(sublayer(self.norm(x)))    \n        return output  \nEach encoder layer is composed of two distinct sublayers: one is a multihead self-\nattention layer, as outlined in the MultiHeadAttention()  class, and the other is a \nstraightforward, position-wise, fully connected feed-forward network, as specified \nin the PositionwiseFeedForward()  class. Additionally, both of these sublayers \nincorporate layer normalization and residual connections. As explained in chapter 6, a \nresidual connection involves passing the input through a sequence of transformations \n(either the attention or the feed-forward layer in this context) and then adding the \ninput back to these transformations’ output. The method of residual connection is The first \nsublayer in \neach encoder \nlayer is a \nmultihead \nself-attention \nnetwork.\nThe second \nsublayer in each \nencoder layer is \na feed-forward \nnetwork.\nEach sublayer \ngoes through \nresidual \nconnection and \nlayer \nnormalization.\n 209 Building an encoder\nemployed to combat the problem of vanishing gradients, which is a common challenge \nin very deep networks. Another benefit of residual connections in Transformers is to \nprovide a passage to pass the positional encodings (which are calculated only before \nthe first layer) to subsequent layers. \nLayer normalization is somewhat similar to the batch normalization we implemented \nin chapter 4. It standardizes the observations in a layer to have a zero mean and a unit \nstandard deviation. To achieve this within the local module, we define the LayerNorm()  \nclass, which executes layer normalization as follows:\nclass LayerNorm(nn.Module):\n    def __init__(self, features, eps=1e-6):\n        super().__init__()\n        self.a_2 = nn.Parameter(torch.ones(features))\n        self.b_2 = nn.Parameter(torch.zeros(features))\n        self.eps = eps\n    def forward(self, x):\n        mean = x.mean(-1, keepdim=True) \n        std = x.std(-1, keepdim=True)\n        x_zscore = (x - mean) / torch.sqrt(std ** 2 + self.eps)\n        output = self.a_2*x_zscore+self.b_2\n        return output \nThe mean  and std values in the preceding LayerNorm() class are the mean and stan -\ndard deviation of the inputs in each layer. The a_2 and b_2 layers in the LayerNorm()  \nclass expand x_zscore  back to the shape of the input x. \nWe can now create an encoder by stacking six encoder layers together. For that pur -\npose, we define the Encoder() class in the local module:\nfrom copy import deepcopy\nclass Encoder(nn.Module):\n    def __init__(self, layer, N):\n        super().__init__()\n        self.layers = nn.ModuleList(\n            [deepcopy(layer) for i in range(N)])\n        self.norm = LayerNorm(layer.size)\n    def forward(self, x, mask):\n        for layer in self.layers:\n            x = layer(x, mask)\n            output = self.norm(x)\n        return output\nHere, the Encoder()  class is defined with two arguments: layer , which is an encoder \nlayer as defined in the EncoderLayer()  class in listing 9.3, and N, the number of \nencoder layers in the encoder. The Encoder()  class takes input x (for example, a \nbatch of English phrases) and the mask (to mask out sequence padding, as I’ll explain \nin chapter 10) to generate output (vector representations that capture the meanings \nof the English phrases).  \nWith that, you have created an encoder. Next, you’ll learn how to create a decoder.",5090
87-9.3 Building an encoder-decoder Transformer.pdf,87-9.3 Building an encoder-decoder Transformer,,0
88-9.3.1 Creating a decoder layer.pdf,88-9.3.1 Creating a decoder layer,"210 chapter  9 A line-by-line implementation of attention and Transformer \n9.3 Building an encoder-decoder Transformer\nNow that you understand how to create an encoder in the Transformer, let’s move on \nto the decoder. You’ll first learn how to create a decoder layer in this section. You’ll \nthen stack N = 6 identical decoder layers to form a decoder. \nWe then create an encoder-decoder transformer with five components: encoder , \ndecoder , src_embed, tgt_embed , and generator , which I’ll explain in this section.\n9.3.1 Creating a decoder layer\nEach decoder layer consists of three sublayers: (1) a multihead self-attention layer, (2) \nthe cross attention between the output from the first sublayer and the encoder’s out -\nput, and (3) a feed-forward network. Each of these three sublayers incorporates a layer \nnormalization and the residual connection, similar to what we have done in encoder \nlayers. Furthermore, the decoder stack’s multihead self-attention sublayer is masked to \nprevent positions from attending to subsequent positions. The mask forces the model \nto use previous elements in a sequence to predict later elements. I’ll explain how \nmasked multihead self-attention works in a moment. To implement this, we define the \nDecoderLayer()  class in the local module.\nListing 9.4    Creating a decoder layer \nclass DecoderLayer(nn.Module):\n    def __init__(self, size, self_attn, src_attn,\n                 feed_forward, dropout):\n        super().__init__()\n        self.size = size\n        self.self_attn = self_attn\n        self.src_attn = src_attn\n        self.feed_forward = feed_forward\n        self.sublayer = nn.ModuleList([deepcopy(\n        SublayerConnection(size, dropout)) for i in range(3)])\n    def forward(self, x, memory, src_mask, tgt_mask):\n        x = self.sublayer[0](x, lambda x: \n                 self.self_attn(x, x, x, tgt_mask))    \n        x = self.sublayer[1](x, lambda x:\n                 self.src_attn(x, memory, memory, src_mask))    \n        output = self.sublayer[2](x, self.feed_forward)    \n        return output\nTo illustrate the operation of a decoder layer, let’s consider our ongoing example. The \ndecoder takes in tokens [ 'BOS', 'comment', 'et', 'es-vous', '?' ], along with \nthe output from the encoder (referred to as memory  in the preceding code block), to The first \nsublayer is a \nmasked \nmultihead \nself-attention \nlayer.\nThe second \nsublayer is a \ncross-attention \nlayer between \nthe target \nlanguage and \nthe source \nlanguage.The third sublayer is a \nfeed-forward network.\n 211 Building an encoder-decoder Transformer\npredict the sequence [ 'comment', 'et', 'es-vous', '?', 'EOS' ]. The embed -\nding of ['BOS', 'comment', 'et', 'es-vous', '?' ] is a tensor of size (1, 5, 256): \n1 is the number of sequences in the batch, 5 is the number of tokens in the sequence, \nand 256 means each token is represented by a 256-value vector. We pass this embed -\nding through the first sublayer, a masked multihead self-attention layer. This process is \nsimilar to the multihead self-attention calculation you saw earlier in the encoder layer. \nHowever, the process utilizes a mask, designated as tgt_mask  in the preceding code \nblock, which is a 5 × 5 tensor with the following values in the ongoing example:\ntensor([[ True, False, False, False, False],\n        [ True,  True, False, False, False],\n        [ True,  True,  True, False, False],\n        [ True,  True,  True,  True, False],\n        [ True,  True,  True,  True,  True]], device='cuda:0')\nAs you may have noticed, the lower half of the mask (values below the main diagonal \nin the tensor) is turned on as True , and the upper half of the mask (values above \nthe main diagonal) is turned off as False . When this mask is applied to the attention \nscores, it results in the first token attending only to itself during the first time step. In \nthe second time step, attention scores are calculated exclusively between the first two \ntokens. As the process continues, for example, in the third time step, the decoder uses \ntokens ['BOS', 'comment', 'et' ] to predict the token 'es-vous' , and the atten -\ntion scores are computed only among these three tokens, effectively hiding the future \ntokens ['es-vous', '?' ]\nFollowing this process, the output generated from the first sublayer, which is a ten -\nsor of size (1, 5, 256), matches the input’s size. This output, which we can refer to as x, \nis then fed into the second sublayer. Here, cross attention is computed between x and \nthe output of the encoder stack, termed memory . You may remember that memory  has \na dimension of (1, 6, 256) since the English phrase “How are you?” is converted to six \ntokens ['BOS', 'how', 'are', 'you', '?', 'EOS' ]. \nFigure 9.8 shows how cross-attention weights are calculated. To calculate the cross \nattention between x and memory , we first pass x through a neural network to obtain \nquery, which has a dimension of (1, 5, 256). We then pass memory  through two neural \nnetworks to obtain key and value, each having a dimension of (1, 6, 256). The scaled \nattention score is calculated using the formula as specified in equation 9.1. This scaled \nattention score has a dimension of (1, 5, 6): the query Q has a dimension of (1, 5, \n256) and the transposed key K has a dimension of (1, 256, 6). Therefore, the scaled \nattention score, which is the dot product of the two, scaled by √ dk, has a size of (1, 5, \n6). After applying the softmax function to the scaled attention score, we obtain atten -\ntion weights, which is a 5 × 6 matrix. This matrix tells us how the five tokens in the \nFrench input [ 'BOS', 'comment', 'et', 'es-vous', '?' ] attend to the six tokens \nin the English phrase [ 'BOS', 'how', 'are', 'you', '?', 'EOS' ]. This is how the \ndecoder captures the meaning of the English phrase when translating.",5920
89-9.4 Putting all the pieces together.pdf,89-9.4 Putting all the pieces together,"212 chapter  9 A line-by-line implementation of attention and Transformer \nWQ\nsize (256, 256) Embedding of\n“BOS How are you? EOS” \nwith a size of (6, 256) \nWK \nsize (256, 256) Embedding of\n“BOS Comment êtes-vous?” \nwith a size of (5, 256) \n Q\nsize (5, 256) K \nsize (6, 256)How\netcommentyou are\nes-vous?\n?\nAttention weights\nsize (5, 6) 0.9 0.02 0.02 0.02 0.02 0.02\n0.02 0.9 0.02 0.02 0.02 0.02\n0.2 0.2 0.2 0.2 0.2 0.2\n0.02 0.02 0.46 0.46 0.02 0.02\n0.01 0.01 0.01 0.01 0.95 0.01BOS\nBOSE OS\nFigure 9.8    An example of how cross-attention weights are calculated between the input to the decoder \nand the output from the encoder. The input to the decoder is passed through a neural network to obtain \nquery Q. The output from the encoder is passed through a different neural network to obtain key K. The \nscaled cross-attention scores are calculated as the dot product of Q and K divided by the square root of \nthe dimension of K. Finally, we apply the softmax function on the scaled cross-attention scores to obtain \nthe cross-attention weights, which demonstrate how each element in Q is related to all elements in K.\nThe final cross attention in the second sublayer is then calculated as the dot product \nof attention weights and the value vector V. The attention weights have a dimension of \n(1, 5, 6) and the value vector has a dimension of (1, 6, 256), so the final cross attention, \nwhich is the dot product of the two, has a size of (1, 5, 256). Therefore, the input and \noutput of the second sublayer have the same dimension of (1, 5, 256). After processing \nthrough this second sublayer, the output is then directed through the third sublayer, \nwhich is a feed-forward network.\n9.3.2 Creating an encoder-decoder Transformer\nThe decoder consists of N = 6 identical decoder layers. \nThe Decoder()  class is defined in the local module as follows:\nclass Decoder(nn.Module):\n    def __init__(self, layer, N):\n        super().__init__()\n        self.layers = nn.ModuleList(\n            [deepcopy(layer) for i in range(N)])\n        self.norm = LayerNorm(layer.size)\n    def forward(self, x, memory, src_mask, tgt_mask):\n        for layer in self.layers:\n            x = layer(x, memory, src_mask, tgt_mask)    \n        output = self.norm(x)\n        return output",2310
90-9.4.2 Creating a model to translate between two languages.pdf,90-9.4.2 Creating a model to translate between two languages,"213 Putting all the pieces together\nTo create an encoder-decoder transformer, we first define a Transformer()  class in \nthe local module. Open the file ch09util.py, and you’ll see the definition of the class as \nshown in the following listing.\nListing 9.5    A class to represent an encoder-decoder Transformer\nclass Transformer(nn.Module):\n    def __init__(self, encoder, decoder,\n                 src_embed, tgt_embed, generator):\n        super().__init__()\n        self.encoder = encoder    \n        self.decoder = decoder    \n        self.src_embed = src_embed\n        self.tgt_embed = tgt_embed\n        self.generator = generator\n    def encode(self, src, src_mask):\n        return self.encoder(self.src_embed(src), src_mask)\n    def decode(self, memory, src_mask, tgt, tgt_mask):\n        return self.decoder(self.tgt_embed(tgt), \n                            memory, src_mask, tgt_mask)\n    def forward(self, src, tgt, src_mask, tgt_mask):\n        memory = self.encode(src, src_mask)    \n        output = self.decode(memory, src_mask, tgt, tgt_mask)    \n        return output\nThe Transformer()  class is constructed with five key components: encoder , decoder , \nsrc_embed , tgt_embed , and generator . The encoder and decoder are represented \nby the Encoder()  and Decoder()  classes defined previously. In the next chapter, you’ll \nlearn to generate the source language embedding: we’ll process numerical represen -\ntations of English phrases using word embedding and positional encoding, combining \nthe results to form the src_embed  component. Similarly, for the target language, we \nprocess numerical representations of French phrases in the same manner, using the \ncombined output as the tgt_embed  component. The generator produces predicted \nprobabilities for each index that corresponds to the tokens in the target language. \nWe’ll define a Generator() class in the next section for this purpose.\n9.4 Putting all the pieces together\nIn this section, we’ll put all the pieces together to create a model that can translate \nbetween any two languages. \n9.4.1 Defining a generator\nFirst, we define a Generator()  class in the local module to generate the probability \ndistribution of the next token (see figure 9.9). The idea is to attach a head to the \ndecoder for downstream tasks. In our example in the next chapter, the downstream \ntask is to predict the next token in the French translation. Defines the encoder in \nthe Transformer\nDefines the decoder \nin the Transformer\nSource language is \nencoded into abstract \nvector representations.\nThe decoder uses these \nvector representations \nto generate translation \nin the target language.\n214 chapter  9 A line-by-line implementation of attention and Transformer \nSoftmax activation\nLinear layerGenerato r\nInput\n(output from the decoder)  Output\n(probability distribution over target\nlanguage’s vocabulary) \nFigure 9.9    The structure of the generator in the Transformer. The generator converts the output \nfrom the decoder stack to a probability distribution over the target language’s vocabulary, so that the \nTransformer can use the distribution to predict the next token in the French translation of an English \nphrase. The generator contains a linear layer so that the number of outputs is the same as the number of \ntokens in the French vocabulary. The generator also applies a softmax activation to the output so that \nthe output is a probability distribution. \nThe class is defined as follows:\nclass Generator(nn.Module):\n    def __init__(self, d_model, vocab):\n        super().__init__()\n        self.proj = nn.Linear(d_model, vocab)\n    def forward(self, x):\n        out = self.proj(x)\n        probs = nn.functional.log_softmax(out, dim=-1)\n        return probs  \nThe Generator()  class produces predicted probabilities for each index that corre -\nsponds to the tokens in the target language. This enables the model to sequentially \npredict tokens in an autoregressive manner, utilizing previously generated tokens and \nthe encoder’s output.\n9.4.2 Creating a model to translate between two languages\nNow we are ready to create a Transformer model to translate between any two lan -\nguages (e.g., English to French or Chinese to English). The create_model()  function \ndefined in the local module accomplishes that.\nListing 9.6    Creating a Transformer to translate between two languages\ndef create_model(src_vocab, tgt_vocab, N, d_model,\n                 d_ff, h, dropout=0.1):\n    attn=MultiHeadedAttention(h, d_model).to(DEVICE)\n    ff=PositionwiseFeedForward(d_model, d_ff, dropout).to(DEVICE)\n    pos=PositionalEncoding(d_model, dropout).to(DEVICE)\n 215 Summary\n    model = Transformer(\n        Encoder(EncoderLayer(d_model,deepcopy(attn),deepcopy(ff),\n                             dropout).to(DEVICE),N).to(DEVICE),    \n        Decoder(DecoderLayer(d_model,deepcopy(attn),\n             deepcopy(attn),deepcopy(ff), dropout).to(DEVICE),\n                N).to(DEVICE),    \n        nn.Sequential(Embeddings(d_model, src_vocab).to(DEVICE),\n                      deepcopy(pos)),    \n        nn.Sequential(Embeddings(d_model, tgt_vocab).to(DEVICE),\n                      deepcopy(pos)),    \n        Generator(d_model, tgt_vocab)).to(DEVICE)    \n    for p in model.parameters():\n        if p.dim() > 1:\n            nn.init.xavier_uniform_(p)    \n    return model.to(DEVICE)\nThe primary element of the create_model()  function is the Transformer()  class, \nwhich was previously defined. Recall that the Transformer()  class is built with five \nessential elements: encoder , decoder , src_embed , tgt_embed , and generator . \nWithin the create_model() function, we sequentially construct these five components, \nusing the recently defined Encoder() , Decoder() , and Generator()  classes. In the \nnext chapter, we’ll discuss in detail how to generate the source and target language \nembeddings, src_embed  and tgt_embed .\nIn the next chapter, you’ll apply the Transformer you created here to English-to-\nFrench translation. You’ll train the model using more than 47,000 pairs of English-to-\nFrench translations. You’ll then use the trained model to translate common English \nphrases into French. \nSummary\n¡ Transformers are advanced deep-learning models that excel in handling \nsequence-to-sequence prediction challenges. Their strength lies in effec -\ntively understanding the relationships between elements in input and output \nsequences over long distances. \n¡ The revolutionary aspect of the Transformer architecture is its attention mecha -\nnism. This mechanism assesses the relationship between words in a sequence by \nassigning weights, determining how closely words are related based on the train -\ning data. This enables Transformer models like ChatGPT to comprehend rela -\ntionships between words, thus understanding human language more effectively.    Creates an encoder by \ninstantiating the Encoder() class\nCreates a \ndecoder by \ninstantiating \nthe Decoder() \nclass\nCreates src_embed \nby passing source \nlanguage through \nword embedding \nand positional \nencoding\nCreates tgt_embed \nby passing target \nlanguage through \nword embedding \nand positional \nencodingCreates a generator by instantiating \nthe Generator() class\n216 chapter  9 A line-by-line implementation of attention and Transformer \n¡ To calculate SDPA, the input embedding X is processed through three distinct \nneural network layers, query (Q), key (K), and value (V). The corresponding \nweights for these layers are WQ, WK, and WV. We can calculate Q, K, and V as Q = X \n* WQ, K = X * QK, and V = X * WV. SDPA is calculated as follows:\nwhere dk represents the dimension of the key vector K. The softmax function \nis applied to the attention scores, converting them into attention weights. This \nensures that the total attention a word gives to all words in the sentence sums to \n100%. The final attention is the dot product of these attention weights and the \nvalue vector V.\n¡ Instead of using one set of query, key, and value vectors, Transformer models use \na concept called multihead attention. The query, key, and value vectors are split \ninto multiple heads. Each head pays attention to different parts or aspects of the \ninput, enabling the model to capture a broader range of information and form a \nmore detailed and contextual understanding of the input data. Multihead atten -\ntion is especially useful when a word has multiple meanings in a sentence.",8568
91-10.1 Subword tokenization.pdf,91-10.1 Subword tokenization,"21710Training a Transformer to \ntranslate English to French\nThis chapter covers\n¡ Tokenizing English and French phrases to   \n subwords\n¡ Understanding word embedding and positional  \n encoding \n¡ Training a Transformer from scratch to translate  \n English to French\n¡ Using the trained Transformer to translate an  \n English phrase into French\nIn the last chapter, we built a Transformer from scratch that can translate between \nany two languages, based on the paper “Attention Is All You Need.”1 Specifically, we \nimplemented the self-attention mechanism, using query, key, and value vectors to \ncalculate scaled dot product attention (SDPA). \nTo have a deeper understanding of self-attention and Transformers, we’ll use \nEnglish-to-French translation as our case study in this chapter. By exploring the \n1 Vaswani et al, 2017, “Attention Is All You Need.” https: //arxiv.org/abs/1706.03762 .",911
92-10.1.1 Tokenizing English and French phrases.pdf,92-10.1.1 Tokenizing English and French phrases,"218 chapter  10 Training a Transformer to translate English to French\nprocess of training a model for converting English sentences into French, you will gain \na deep understanding of the Transformer’s architecture and the functioning of the \nattention mechanism. \nPicture yourself having amassed a collection of more than 47,000 English-to-French \ntranslation pairs. Your objective is to train the encoder-decoder Transformer from \nthe last chapter using this dataset. This chapter will walk you through all phases of the \nproject. You’ll first use subword tokenization to break English and French phrases \ninto tokens. You’ll then build your English and French vocabularies, which contain \nall unique tokens in each language. The vocabularies allow you to represent English \nand French phrases as sequences of indexes. After that, you’ll use word embedding to \ntransform these indexes (essentially one-hot vectors) into compact vector representa -\ntions. We’ll add positional encodings to the word embeddings to form input embed -\ndings. Positional encodings allow the Transformer to know the ordering of tokens in \nthe sequence. \nFinally, you’ll train the encoder-decoder Transformer from chapter 9 to translate \nEnglish to French by using the collection of English-to-French translations as the train -\ning dataset. After training, you’ll learn to translate common English phrases to French \nwith the trained Transformer. Specifically, you’ll use the encoder to capture the mean -\ning of the English phrase. You’ll then use the decoder in the trained Transformer to \ngenerate the French translation in an autoregressive manner, starting with the begin -\nning token ""BOS"" . In each time step, the decoder generates the most likely next token \nbased on previously generated tokens and the encoder’s output, until the predicted \ntoken is ""EOS"" , which signals the end of the sentence. The trained model can translate \ncommon English phrases accurately as if you were using Google Translate for the task. \n10.1 Subword tokenization\nAs we discussed in chapter 8, there are three tokenization methods: character-level \ntokenization, word-level tokenization, and subword tokenization. In this chapter, we’ll \nuse subword tokenization, which strikes a balance between the other two methods. It \nkeeps frequently used words whole in the vocabulary and splits less common or more \ncomplex words into subcomponents.  \nIn this section, you’ll learn to tokenize both English and French phrases into sub -\nwords. You’ll then create dictionaries to map tokens to indexes. The training data are \nthen converted to sequences of indexes and placed in batches for training purposes.\n10.1.1  Tokenizing English and French phrases\nGo to https: //mng.bz/WVAw  to download the zip file that contains the English-to-\nFrench translations I collected from various sources. Unzip the file and place en2fr.csv \nin the folder /files/ on your computer. \nWe’ll load the data and print out an English phrase, along with its French transla -\ntion, as follows:\n 219 Subword tokenization\nimport pandas as pd\ndf=pd.read_csv(""files/en2fr.csv"")    \nnum_examples=len(df)    \nprint(f""there are {num_examples} examples in the training data"")\nprint(df.iloc[30856][""en""])    \nprint(df.iloc[30856][""fr""])    \nThe output from the preceding code snippet is\nthere are 47173 examples in the training data\nHow are you?\nComment êtes-vous?\nThere are 47,173 pairs of English-to-French translations in the training data. We have \nprinted out the English phrase “How are you?” and the corresponding French transla -\ntion “Comment êtes-vous?” as an example.\nRun the following line of code in a new cell in this Jupyter Notebook to install the \ntransformers  library on your computer:\n!pip install transformers\nNext, we’ll tokenize both the English and the French phrases in the dataset. We’ll use \nthe pretrained XLM model from Hugging Face as the tokenizer because it excels at \nhandling multiple languages, including English and French phrases.\nListing 10.1    A pretrained tokenizer\nfrom transformers import XLMTokenizer    \ntokenizer = XLMTokenizer.from_pretrained(""xlm-clm-enfr-1024"")    \ntokenized_en=tokenizer.tokenize(""I don't speak French."")    \nprint(tokenized_en)\ntokenized_fr=tokenizer.tokenize( ""Je ne parle pas français. "")    \nprint(tokenized_fr)\nprint(tokenizer.tokenize(""How are you?""))\nprint(tokenizer.tokenize(""Comment êtes-vous?""))\nThe output from code listing 10.1 is\n['i</w>', 'don</w>', ""'t</w>"", 'speak</w>', 'fr', 'ench</w>', '.</w>']\n['je</w>', 'ne</w>', 'parle</w>', 'pas</w>', 'franc', 'ais</w>', '.</w>']\n['how</w>', 'are</w>', 'you</w>', '?</w>']\n['comment</w>', 'et', 'es-vous</w>', '?</w>']Loads the CSV file\nCounts how many pairs of \nphrases are in the data\nPrints out an example \nof an English phrase\nPrints out the corresponding \nFrench translation\nImports the \npretrained tokenizer\nUses the \ntokenizer to \ntokenize an \nEnglish \nsentence\nTokenizes \na French \nsentence\n220 chapter  10 Training a Transformer to translate English to French\nIn the preceding code block, we use a pretrained tokenizer from the XLM model to \ndivide the English sentence “I don’t speak French.” into a group of tokens. In chap -\nter 8, you developed a custom word-level tokenizer. However, this chapter introduces \nthe use of a more efficient pretrained subword tokenizer, surpassing the word-level \ntokenizer in effectiveness. The sentence “I don’t speak French.” is thus tokenized \ninto ['i', 'don', ""'t"", 'speak', 'fr', 'ench', '.' ]. Similarly, the French \nsentence “Je ne parle pas français.” is split into six tokens: [ 'je', 'ne', 'parle', \n'pas', 'franc', 'ais', '.' ]. We have also tokenized the English phrase “How \nare you?” and its French translation. The results are shown in the last two lines of the \npreceding output. \nNOTE     You may have noticed that the XLM model uses ' </w> ' as a token sep -\narator, except in cases where two tokens are part of the same word. Subword \ntokenization typically results in each token being either a complete word or a \npunctuation mark, but there are occasions when a word is divided into syllables. \nFor example, the word “French” is divided into “fr” and “ench.” It’s noteworthy \nthat the model doesn’t insert </w>  between “fr” and “ench,” as these syllables \njointly constitute the word “French.”\nDeep-learning models such as Transformers cannot process raw text directly; hence \nwe need to convert text into numerical representations before feeding them to the \nmodels. For that purpose, we create a dictionary to map all English tokens to integers.\nListing 10.2    Mapping English tokens to indexes\nfrom collections import Counter\nen=df[""en""].tolist()    \nen_tokens=[[""BOS""]+tokenizer.tokenize(x)+[""EOS""] for x in en]    \nPAD=0\nUNK=1\nword_count=Counter()\nfor sentence in en_tokens:\n    for word in sentence:\n        word_count[word]+=1\nfrequency=word_count.most_common(50000)    \ntotal_en_words=len(frequency)+2\nen_word_dict={w[0]:idx+2 for idx,w in enumerate(frequency)}    \nen_word_dict[""PAD""]=PAD\nen_word_dict[""UNK""]=UNK\nen_idx_dict={v:k for k,v in en_word_dict.items()}    \nWe insert the tokens ""BOS""  (beginning of the sentence) and ""EOS""  (end of the sen -\ntence) at the start and end of each phrase, respectively. The dictionary en_word_dict  Obtains all English sentences \nfrom the training dataset\nTokenizes \nall English \nsentences\nCounts the \nfrequency of tokens\nCreates a \ndictionary to \nmap tokens \nto indexes\nCreates a dictionary to \nmap indexes to tokens\n 221 Subword tokenization\nassigns each token a unique integer value. Further, the ""PAD""  token, used for pad -\nding, is allocated the integer 0, while the ""UNK""  token, representing unknown tokens, \nis given the integer 1. A reverse dictionary, en_idx_dict , maps integers (indexes) \nback to their corresponding tokens. This reverse mapping is essential for converting \na sequence of integers back into a sequence of tokens, enabling us to reconstruct the \noriginal English phrase.\nUsing the dictionary en_word_dict , we can transform the English sentence “I don’t \nspeak French.” into its numerical representation. This process involves looking up each \ntoken in the dictionary to find its corresponding integer value. For instance:\nenidx=[en_word_dict.get(i,UNK) for i in tokenized_en]   \nprint(enidx)\nThe preceding lines of code produce the following output:\n[15, 100, 38, 377, 476, 574, 5]\nThis means that the English sentence “I don’t speak French.” is now represented by a \nsequence of integers [15, 100, 38, 377, 476, 574, 5]. \nWe can also revert the numerical representations into tokens using the dictionary \nen_idx_dict . This process involves mapping each integer in the numerical sequence \nback to its corresponding token as defined in the dictionary. Here’s how it is done:\nentokens=[en_idx_dict.get(i,""UNK"") for i in enidx]    \nprint(entokens)\nen_phrase="""".join(entokens)    \nen_phrase=en_phrase.replace(""</w>"","" "")    \nfor x in '''?:;.,'(""-!&)%''':\n    en_phrase=en_phrase.replace(f"" {x}"",f""{x}"")    \nprint(en_phrase)\nThe output of the preceding code snippet is\n['i</w>', 'don</w>', ""'t</w>"", 'speak</w>', 'fr', 'ench</w>', '.</w>']\ni don't speak french. \nThe dictionary en_idx_dict  is used to translate numbers back into their original \ntokens. Following this, these tokens are transformed into the complete English phrase. \nThis is done by first joining the tokens into a single string and then substituting the \nseparator ''</w>'' with a space. We also remove the space before punctuation marks. \nNotice that the restored English phrase has all lowercase letters because the pretrained \ntokenizer automatically converts uppercase letters into lowercase to reduce the num -\nber of unique tokens. As you’ll see in the next chapter, some models, such as GPT2 and \nChatGPT, don’t do this; hence, they have a larger vocabulary.Converts indexes \nto tokens\nJoins tokens \ninto a string\nReplaces the separator \nwith a space\nRemoves the space \nbefore punctuations\n222 chapter  10 Training a Transformer to translate English to French\nExercise 10.1\nIn listing 10.1, we have split the sentence “How are you?” into tokens [ 'how</w>', \n'are</w>', 'you</w>', '?</w>' ]. Follow the steps in this subsection to (i) convert \nthe tokens into indexes using the dictionary  en_word_dict ; (ii) convert the indexes \nback to tokens using the dictionary en_idx_dict ; (iii) restore the English sentence by \njoining the tokens into a string, changing the separator '</w>'  to a space, and removing \nthe space before punctuation marks. \nWe can apply the same steps to French phrases to map tokens to indexes and vice versa.\nListing 10.3    Mapping French tokens to indexes\nfr=df[""fr""].tolist()       \nfr_tokens=[[""BOS""]+tokenizer.tokenize(x)+[""EOS""] for x in fr]    \nword_count=Counter()\nfor sentence in fr_tokens:\n    for word in sentence:\n        word_count[word]+=1\nfrequency=word_count.most_common(50000)    \ntotal_fr_words=len(frequency)+2\nfr_word_dict={w[0]:idx+2 for idx,w in enumerate(frequency)}    \nfr_word_dict[""PAD""]=PAD\nfr_word_dict[""UNK""]=UNK\nfr_idx_dict={v:k for k,v in fr_word_dict.items()}    \nThe dictionary fr_word_dict  assigns an integer to each French token, while fr_\nidx_dict  maps these integers back to their corresponding French tokens. Next, I’ll \ndemonstrate how to transform the French phrase “Je ne parle pas français.” into its \nnumerical representation:\nfridx=[fr_word_dict.get(i,UNK) for i in tokenized_fr]   \nprint(fridx)\nThe output from the preceding code snippet is\n[28, 40, 231, 32, 726, 370, 4]\nThe tokens for the French phrase “Je ne parle pas français.” are converted into a \nsequence of integers, as shown.\nWe can transform the numerical representations back into French tokens using the \ndictionary fr_idx_dict . This involves translating each number in the sequence back \nto its respective French token in the dictionary. Once the tokens are retrieved, they can \nbe joined to reconstruct the original French phrase. Here’s how it’s done:\nfrtokens=[fr_idx_dict.get(i,""UNK"") for i in fridx] \nprint(frtokens)Tokenizes \nall French \nsentences\nCounts the frequency \nof French tokens\nCreates a \ndictionary to \nmap French \ntokens to \nindexes\nCreates a dictionary to map \nindexes to French tokens",12446
93-10.1.2 Sequence padding and batch creation.pdf,93-10.1.2 Sequence padding and batch creation,"223 Subword tokenization\nfr_phrase= """".join(frtokens)\nfr_phrase=fr_phrase.replace( ""</w>"","" "")\nfor x in '''?:;.,'(""-!&)%''':\n    fr_phrase=fr_phrase.replace(f "" {x}"",f""{x}"")  \nprint(fr_phrase)\nThe output from the preceding code block is\n['je</w>', 'ne</w>', 'parle</w>', 'pas</w>', 'franc', 'ais</w>', '.</w>']\nje ne parle pas francais. \nIt’s important to recognize that the restored French phrase doesn’t exactly match its \noriginal form. This discrepancy is due to the tokenization process, which transforms all \nuppercase letters into lowercase and eliminates accent marks in French.\nExercise 10.2\nIn listing 10.1, we have split the sentence “Comment êtes-vous?” into tokens \n['comment</w>', 'et', 'es-vous</w>', '?</w>' ]. Follow the steps in this \nsubsection to (i) convert the tokens into indexes using the dictionary fr_word_dict ; (ii) \nconvert the indexes back to tokens using the dictionary fr_idx_dict ; (iii) restore the \nFrench phrase by joining the tokens into a string, changing the separator '</w>' to a \nspace, and removing the space before punctuation marks.\nSave the four dictionaries in the folder /files/ on your computer so that you can load \nthem up and start translating later without worrying about first mapping tokens to \nindexes and vice versa:\nimport pickle\nwith open(""files/dict.p"",""wb"") as fb:\n    pickle.dump((en_word_dict,en_idx_dict,\n                 fr_word_dict,fr_idx_dict),fb)\nThe four dictionaries are now saved in a single pickle file dict.p . Alternatively, you \ncan download the file from the book’s GitHub repository.\n10.1.2  Sequence padding and batch creation\nWe’ll divide the training data into batches during training for computational efficiency \nand accelerated convergence, as we have done in previous chapters. \nCreating batches for other data formats such as images is straightforward: simply \ngroup a specific number of inputs to form a batch since they all have the same size. How -\never, in natural language processing, batching can be more complex due to the vary -\ning lengths of sentences. To standardize the length within a batch, we pad the shorter \nsequences. This uniformity is crucial since the numerical representations fed into the \nTransformer need to have the same length. For instance, English phrases in a batch may \nvary in length (this can also happen to French phrases in a batch). To address this, we \nappend zeros to the end of the numerical representations of shorter phrases in a batch, \nensuring that all inputs to the Transformer model are of equal length.\n224 chapter  10 Training a Transformer to translate English to French\nNOTE     Incorporating BOS and EOS tokens at the beginning and end of each sen -\ntence, as well as padding shorter sequences within a batch, is a distinctive feature \nin machine language translation. This distinction arises from the fact that the \ninput consists of entire sentences or phrases. In contrast, as you will see in the \nnext two chapters, training a text generation model does not entail these pro -\ncesses; the model’s input contains a predetermined number of tokens. \nWe start by converting all English phrases into their numerical representations and \nthen apply the same process to the French phrases:\nout_en_ids=[[en_word_dict.get(w,UNK) for w in s] for s in en_tokens]\nout_fr_ids=[[fr_word_dict.get(w,UNK) for w in s] for s in fr_tokens]\nsorted_ids=sorted(range(len(out_en_ids)),\n                  key=lambda x:len(out_en_ids[x]))\nout_en_ids=[out_en_ids[x] for x in sorted_ids]\nout_fr_ids=[out_fr_ids[x] for x in sorted_ids]\nNext, we put the numerical representations into batches for training:\nimport numpy as np\nbatch_size=128\nidx_list=np.arange(0,len(en_tokens),batch_size)\nnp.random.shuffle(idx_list)\nbatch_indexs=[]\nfor idx in idx_list:\n    batch_indexs.append(np.arange(idx,min(len(en_tokens),\n                                          idx+batch_size)))\nNote that we have sorted observations in the training dataset by the length of the \nEnglish phrases before placing them into batches. This method ensures that the obser -\nvations within each batch are of a comparable length, consequently decreasing the \nneed for padding. As a result, this approach not only reduces the overall size of the \ntraining data but also accelerates the training process. \nTo pad sequences in a batch to the same length, we define the following function: \ndef seq_padding(X, padding=0):\n    L = [len(x) for x in X]\n    ML = max(L)    \n    padded_seq = np.array([np.concatenate([x, [padding] * (ML - len(x))])\n        if len(x) < ML else x for x in X])    \n    return padded_seq\nThe function seq_padding()  first identifies the longest sequence within the batch. \nThen it appends zeros to the end of shorter sequences to ensure that every sequence in \nthe batch matches this maximum length.\nTo conserve space, we have created a Batch()  class within the local module ch09util.\npy that you downloaded in the last chapter (see figure 10.1).Find out the length of the \nlongest sequence in the batch.\nIf a batch is shorter than the \nlongest sequence, add 0s to \nthe sequence at the end.\n 225 Subword tokenization\nListing 10.4    Creating a Batch() class in the local module\nimport torch\nDEVICE = ""cuda"" if torch.cuda.is_available() else ""cpu""\nclass Batch:\n    def __init__(self, src, trg=None, pad=0):\n        src = torch.from_numpy(src).to(DEVICE).long()\n        self.src = src\n        self.src_mask = (src != pad).unsqueeze(-2)    \n        if trg is not None:\n            trg = torch.from_numpy(trg).to(DEVICE).long()\n            self.trg = trg[:, :-1]    \n            self.trg_y = trg[:, 1:]    \n            self.trg_mask = make_std_mask(self.trg, pad)    \n            self.ntokens = (self.trg_y != pad).data.sum()\nThe Batch() Classsrc: sequence of indexes for the\nsource language; e.g., those for the\nEnglish phrase ""How are you?""\ntrg: sequence of indexes for the\ntarget language; e.g., those for the\nFrench phrase ""Comment êtes-vous?""src_mask: source mask to conceal\npadding at the end of the source \ntrg: input to the decoder; e.g., indexes \nfor ['BOS', 'comment', 'et',\n'es-vous', '?'] \ntrg_y: output of the decoder;\ne.g., indexes for ['comment', 'et',\n'es-vous', '?', 'EOS']\n \ntrg_mask: target mask to conceal\npadding and to hide future tokens \nFigure 10.1    What does the Batch()  class do? The Batch()  class takes two inputs: src and trg, \nsequences of indexes for the source language and the target language, respectively. It adds several \nattributes to the training data: src_mask , the source mask to conceal padding; modified trg , the \ninput to the decoder; trg_y , the output to the decoder; trg_mask , the target mask to hide padding and \nfuture tokens. \nThe Batch()  class processes a batch of English and French phrases, converting them \ninto a format suitable for training. To make this explanation more tangible, consider \nthe English phrase “How are you?” and its French equivalent “Comment êtes-vous?” \nas our example. The Batch() class receives two inputs: src, which is the sequence of \nindexes representing the tokens in “How are you?”, and trg, the sequence of indexes \nfor the tokens in “Comment êtes-vous?”. This class generates a tensor, src_mask , to \nconceal the padding at the sentence’s end. For instance, the sentence “How are you?” \nis broken down into six tokens: [ 'BOS', 'how', 'are', 'you', '?', 'EOS' ]. If \nthis sequence is part of a batch with a maximum length of eight tokens, two zeros are \nadded to the end. The src_mask  tensor instructs the model to disregard the final two \ntokens in such scenarios.Creates a source mask \nto hide padding at the \nend of the sentence\nCreates input \nto the decoder\nShifts the \ninput one \ntoken to the \nright and uses \nit as output\nCreates a \ntarget mask\n226 chapter  10 Training a Transformer to translate English to French\nThe Batch() class additionally prepares the input and output for the Transformer’s \ndecoder. Consider the French phrase “Comment êtes-vous?”, which is transformed into \nsix tokens: [ 'BOS', 'comment', 'et', 'es-vous', '?', 'EOS' ]. The indexes of \nthese first five tokens serve as the input to the decoder, named trg. Next, we shift this \ninput one token to the right to form the decoder’s output, trg_y . Hence, the input \ncomprises indexes for [ 'BOS', 'comment', 'et', 'es-vous', '?' ], while the \noutput consists of indexes for [ 'comment', 'et', 'es-vous', '?', 'EOS' ]. This \napproach mirrors what we discussed in chapter 8 and is designed to force the model to \npredict the next token based on the previous ones.\nThe Batch()  class also generates a mask, trg_mask , for the decoder’s input. The \naim of this mask is to conceal the subsequent tokens in the input, ensuring that the \nmodel relies solely on previous tokens for making predictions. This mask is produced \nby the make_std_mask()  function, which is defined within the local module ch09util:\nimport numpy as np\ndef subsequent_mask(size):\n    attn_shape = (1, size, size)\n    subsequent_mask = np.triu(np.ones(attn_shape),k=1).astype('uint8')\n    output = torch.from_numpy(subsequent_mask) == 0\n    return output\ndef make_std_mask(tgt, pad):\n    tgt_mask=(tgt != pad).unsqueeze(-2)\n    output=tgt_mask & subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data)\n    return output \nThe subsequent_mask()  function generates a mask specifically for a sequence, \ninstructing the model to focus solely on the actual sequence and disregard the pad -\nded zeros at the end, which are used only to standardize sequence lengths. The make_\nstd_mask() function, on the other hand, constructs a standard mask for the target \nsequence. This standard mask has the dual role of concealing both the padded zeros \nand the future tokens in the target sequence. \nNext, we import the Batch()  class from the local module and use it to create batches \nof training data:\nfrom utils.ch09util import Batch\nclass BatchLoader():\n    def __init__(self):\n        self.idx=0\n    def __iter__(self):\n        return self\n    def __next__(self):\n        self.idx += 1\n        if self.idx<=len(batch_indexs):\n            b=batch_indexs[self.idx-1]\n            batch_en=[out_en_ids[x] for x in b]\n            batch_fr=[out_fr_ids[x] for x in b]\n            batch_en=seq_padding(batch_en)\n            batch_fr=seq_padding(batch_fr)\n            return Batch(batch_en,batch_fr)\n        raise StopIteration",10530
94-10.2 Word embedding and positional encoding.pdf,94-10.2 Word embedding and positional encoding,,0
95-10.2.2 Positional encoding.pdf,95-10.2.2 Positional encoding,"227 Word embedding and positional encoding\nThe BatchLoader()  class creates data batches intended for training. Each batch in \nthis list contains 128 pairs, where each pair contains numerical representations of an \nEnglish phrase and its corresponding French translation.\n10.2 Word embedding and positional encoding\nAfter tokenization in the last section, English and French phrases are represented by \nsequences of indexes. In this section, you’ll use word embedding to transform these \nindexes (essentially one-hot vectors) into compact vector representations. Doing so \ncaptures the semantic information and interrelationship of tokens in a phrase. Word \nembedding also improves training efficiency: instead of bulky one-hot vectors, word \nembedding uses continuous, lower-dimensional vectors to reduce the model’s com -\nplexity and dimensionality.\nThe attention mechanism processes all tokens in a phrase at the same time instead of \nsequentially. This enhances its efficiency but doesn’t inherently allow it to recognize the \nsequence order of the tokens. Therefore, we’ll add positional encodings to the input \nembeddings by using sine and cosine functions of varying frequencies.\n10.2.1  Word embedding\nThe numerical representations of the English and French phrases involve a large num -\nber of indexes. To determine the exact number of distinct indexes required for each \nlanguage, we can count the number of unique elements in the en_word_dict  and \nfr_word_dict  dictionaries. Doing so generates the total number of unique tokens in \neach language’s vocabulary (we’ll use them as inputs to the Transformer later):\nsrc_vocab = len(en_word_dict)\ntgt_vocab = len(fr_word_dict)\nprint(f""there are {src_vocab} distinct English tokens"")\nprint(f""there are {tgt_vocab} distinct French tokens"")\nThe output is\nthere are 11055 distinct English tokens\nthere are 11239 distinct French tokens\nIn our dataset, there are 11,055 unique English tokens and 11,239 unique French \ntokens. Utilizing one-hot encoding for these would result in an excessively high num -\nber of parameters to train. To address this, we will employ word embeddings, which \ncompress the numerical representations into continuous vectors, each with a length \nof d_model = 256 . \nThis is achieved through the use of the Embeddings()  class, which is defined in the \nlocal module ch09util:\nimport math\nclass Embeddings(nn.Module):\n    def __init__(self, d_model, vocab):\n        super().__init__()\n        self.lut = nn.Embedding(vocab, d_model)\n228 chapter  10 Training a Transformer to translate English to French\n        self.d_model = d_model\n    def forward(self, x):\n        out = self.lut(x) * math.sqrt(self.d_model)\n        return out\nThe Embeddings()  class defined previously utilizes PyTorch’s Embedding()  class. It \nalso multiplies the output by the square root of d_model , which is 256. This multi -\nplication is intended to counterbalance the division by the square root of d_model  \nthat occurs later during the computation of attention scores. The Embeddings()  class \ndecreases the dimensionality of the numerical representations of English and French \nphrases. We discussed in detail how PyTorch’s Embedding()  class works in chapter 8. \n10.2.2  Positional encoding\nTo accurately represent the sequence order of elements in both input and output, we \nintroduce the PositionalEncoding()  class in the local module.\nListing 10.5    A class to calculate positional encoding\nclass PositionalEncoding(nn.Module):\n    def __init__(self, d_model, dropout, max_len=5000):    \n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        pe = torch.zeros(max_len, d_model, device=DEVICE)\n        position = torch.arange(0., max_len, \n                                device=DEVICE).unsqueeze(1)\n        div_term = torch.exp(torch.arange(\n            0., d_model, 2, device=DEVICE)\n            * -(math.log(10000.0) / d_model))\n        pe_pos = torch.mul(position, div_term)\n        pe[:, 0::2] = torch.sin(pe_pos)    \n        pe[:, 1::2] = torch.cos(pe_pos)    \n        pe = pe.unsqueeze(0)\n        self.register_buffer('pe', pe)  \n    def forward(self, x):\n        x=x+self.pe[:,:x.size(1)].requires_grad_(False)    \n        out=self.dropout(x)\n        return out\nThe PositionalEncoding() class generates vectors for sequence positions using sine \nfunctions for even indexes and cosine functions for odd indexes. It’s important to note \nthat in the PositionalEncoding()  class, the requires_grad_(False)  argument is \nincluded because there is no need to train these values. They remain constant across \nall inputs, and they don’t change during the training process. \nFor example, the indexes for the six tokens [ 'BOS', 'how', 'are', 'you', \n'?', 'EOS' ] from the English phrase are first processed through a word embedding \nlayer. This step transforms these indexes into a tensor with the dimensions of (1, 6, \n256): 1 means there is only 1 sequence in the batch; 6 means there are 6 tokens in the Initiates the \nclass, allowing a \nmaximum of \n5,000 positions\nApplies sine function \nto even indexes in \nthe vector\nApplies cosine \nfunction to odd \nindexes in the vector\nAdds positional \nencoding to word \nembedding\n 229 Word embedding and positional encoding\nsequence; 256 means each token is represented by a 256-value vector. After this word \nembedding process, the PositionalEncoding() class is employed to calculate the \npositional encodings for the indexes corresponding to the tokens [ 'BOS', 'how', \n'are', 'you', '?', 'EOS' ]. This is done to provide the model with information \nabout the position of each token in the sequence. Better yet, we can tell you the exact \nvalues of the positional encodings for the previous six tokens by using the following \ncode block:\nfrom utils.ch09util import PositionalEncoding\nimport torch\nDEVICE = ""cuda"" if torch.cuda.is_available() else ""cpu""\npe = PositionalEncoding(256, 0.1)    \nx = torch.zeros(1, 8, 256).to(DEVICE)    \ny = pe.forward(x)    \nprint(f""the shape of positional encoding is {y.shape}"")\nprint(y)    \nWe first create an instance, pe, of the PositionalEncoding()  class by setting the \nmodel dimension to 256 and the dropout rate to 0.1. Since the output from this class \nis the sum of word embedding and positional encoding, we create a word embedding \nfilled with zeros and feed it to pe: this way, the output is the same as the positional \nencoding.\nAfter running the preceding code block, you’ll see the following output:\nthe shape of positional encoding is torch.Size([1, 8, 256])\ntensor([[[ 0.0000e+00,  1.1111e+00,  0.0000e+00,  ...,  0.0000e+00,\n           0.0000e+00,  1.1111e+00],\n         [ 9.3497e-01,  6.0034e-01,  8.9107e-01,  ...,  1.1111e+00,\n           1.1940e-04,  1.1111e+00],\n         [ 0.0000e+00, -4.6239e-01,  1.0646e+00,  ...,  1.1111e+00,\n           2.3880e-04,  1.1111e+00],\n         ...,\n         [-1.0655e+00,  3.1518e-01, -1.1091e+00,  ...,  1.1111e+00,\n           5.9700e-04,  1.1111e+00],\n         [-3.1046e-01,  1.0669e+00, -0.0000e+00,  ...,  0.0000e+00,\n           7.1640e-04,  1.1111e+00],\n         [ 7.2999e-01,  8.3767e-01,  2.5419e-01,  ...,  1.1111e+00,\n           8.3581e-04,  1.1111e+00]]], device='cuda:0')\nThe preceding tensor represents the positional encoding for the English phrase “How \nare you?” It’s important to note that this positional encoding also has the dimensions \nof (1, 6, 256), which matches the size of the word embedding for “How are you?”. The Instantiates the \nPositionalEncoding() \nclass and set the model \ndimension to 256\nCreates a word \nembedding and fills \nit with zeros\nCalculates the input \nembedding by adding \npositional encoding to \nthe word embedding\nPrints out the input embedding, which is \nthe same as positional encoding since \nword embedding is set to zero",7938
96-10.3 Training the Transformer for English-to-French translation.pdf,96-10.3 Training the Transformer for English-to-French translation,,0
97-10.3.1 Loss function and the optimizer.pdf,97-10.3.1 Loss function and the optimizer,"230 chapter  10 Training a Transformer to translate English to French\nnext step involves combining the word embedding and positional encoding into a sin -\ngle tensor.\nAn essential characteristic of positional encodings is that their values are the same no \nmatter what the input sequences are. This means that regardless of the specific input \nsequence, the positional encoding for the first token will always be the same 256-value \nvector, [0.0000e+00, 1.1111e+00, ..., 1.1111e+00 ], as shown in the above output. \nSimilarly, the positional encoding for the second token will always be [ 9.3497e-01, \n6.0034e-01, ..., 1.1111e+00 ], and so on. Their values don’t change during the \ntraining process either. \n10.3 Training the Transformer for English-to-French translation\nOur constructed English-to-French translation model can be viewed as a multicate -\ngory classifier. The core objective is to predict the next token in the French vocabulary \nwhen translating an English sentence. This is somewhat similar to the image classifi -\ncation project we discussed in chapter 2, though this model is significantly more com -\nplex. This complexity necessitates careful selection of the loss function, optimizer, and \ntraining loop parameters.\nIn this section, we will detail the process of selecting an appropriate loss function \nand optimizer. We will train the Transformer using batches of English-to-French trans -\nlations as our training dataset. After the model is trained, you’ll learn how to translate \ncommon English phrases into French.\n10.3.1  Loss function and the optimizer\nFirst, we import the create_model()  function from the local module ch09util.py and \nconstruct a Transformer so that we can train it to translate English to French:\nfrom utils.ch09util import create_model\nmodel = create_model(src_vocab, tgt_vocab, N=6,\n    d_model=256, d_ff=1024, h=8, dropout=0.1)\nThe paper “Attention Is All You Need” uses various combinations of hyperparameters \nwhen constructing the model. Here we choose a model dimension of 256 with 8 heads \nbecause we find this combination does a good job translating English to French in our \nsetting. Interested readers could potentially use a validation set to tune hyperparame -\nters to select the best model in their own projects. \nWe’ll follow the original paper “Attention Is All You Need” and use label smoothing \nduring training. Label smoothing is commonly used in training deep neural networks \nto improve the generalization of the model. It is used to address overconfidence prob -\nlems (the predicted probability is greater than the true probability) and overfitting in \nclassifications. Specifically, it modifies the way the model learns by adjusting the target \nlabels, aiming to reduce the model’s confidence in the training data, which can lead to \nbetter performance on unseen data. \nIn a typical classification task, target labels are represented in a one-hot encoding for -\nmat. This representation implies absolute certainty about the correctness of the label \n 231 Training the Transformer for English-to-French translation\nfor each training sample. Training with absolute certainty can lead to two main prob -\nlems. The first is overfitting: the model becomes overly confident in its predictions, \nfitting too closely to the training data, which can harm its performance on new, unseen \ndata. The second problem is poor calibration: models trained this way often output \noverconfident probabilities. For instance, they might output a probability of 99% for a \ncorrect class when, realistically, the confidence should be lower. \nLabel smoothing adjusts the target labels to be less confident. Instead of having a tar -\nget label of [ 1, 0, 0 ] for a three-class problem, you might have something like [ 0.9, \n0.05, 0.05 ]. This approach encourages the model not to be too confident about its \npredictions by penalizing overconfident outputs. The smoothed labels are a mixture \nof the original label and some distribution over the other labels (usually the uniform \ndistribution).\nWe define the following LabelSmoothing() class in the local module ch09util.\nListing 10.6    A class to conduct label smoothing\nclass LabelSmoothing(nn.Module):\n    def __init__(self, size, padding_idx, smoothing=0.1):\n        super().__init__()\n        self.criterion = nn.KLDivLoss(reduction='sum')  \n        self.padding_idx = padding_idx\n        self.confidence = 1.0 - smoothing\n        self.smoothing = smoothing\n        self.size = size\n        self.true_dist = None\n    def forward(self, x, target):\n        assert x.size(1) == self.size\n        true_dist = x.data.clone()    \n        true_dist.fill_(self.smoothing / (self.size - 2))\n        true_dist.scatter_(1, \n               target.data.unsqueeze(1), self.confidence)    \n        true_dist[:, self.padding_idx] = 0\n        mask = torch.nonzero(target.data == self.padding_idx)\n        if mask.dim() > 0:\n            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n        self.true_dist = true_dist\n        output = self.criterion(x, true_dist.clone().detach())    \n        return output\nThe LabelSmoothing()  class first extracts the predictions from the model. It then \nsmoothes the actual labels in the training dataset by adding noise to it. The parameter \nsmoothing  controls how much noise we inject into the actual label. The label [ 1, 0, \n0] is smoothed to [ 0.9, 0.05, 0.05 ] if you set smoothing=0.1 , and it is smoothed to \n[0.95, 0.025, 0.025 ] if you set smoothing=0.05 , for example. The class then calcu -\nlates the loss by comparing the predictions with the smoothed labels.Extracts predictions \nfrom the model\nExtracts \nactual labels \nfrom the \ntraining data \nand adds \nnoise to them\nUses the \nsmoothed \nlabels as \ntargets when \ncalculating \nloss\n232 chapter  10 Training a Transformer to translate English to French\nAs in previous chapters, the optimizer we use is the Adam optimizer. However, \ninstead of using a constant learning rate throughout training, we define the NoamOpt()  \nclass in the local module to change the learning rate during the training process:\nclass NoamOpt:\n    def __init__(self, model_size, factor, warmup, optimizer):\n        self.optimizer = optimizer\n        self._step = 0\n        self.warmup = warmup    \n        self.factor = factor\n        self.model_size = model_size\n        self._rate = 0\n    def step(self):    \n        self._step += 1\n        rate = self.rate()\n        for p in self.optimizer.param_groups:\n            p['lr'] = rate\n        self._rate = rate\n        self.optimizer.step()\n    def rate(self, step=None):\n        if step is None:\n            step = self._step\n        output = self.factor * (self.model_size ** (-0.5) *\n        min(step ** (-0.5), step * self.warmup ** (-1.5)))    \n        return output\nThe NoamOpt()  class, as defined previously, implements a warm-up learning rate \nstrategy. First, it increases the learning rate linearly during the initial warmup steps \nof training. Following this warm-up period, the class then decreases the learning rate, \nadjusting it in proportion to the inverse square root of the training step number.\nNext, we create the optimizer for training:\nfrom utils.ch09util import NoamOpt\noptimizer = NoamOpt(256, 1, 2000, torch.optim.Adam(\n    model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\nTo define the loss function for training, we first create the following SimpleLoss -\nCompute()  class in the local module.\nListing 10.7    A class to compute loss\nclass SimpleLossCompute:\n    def __init__(self, generator, criterion, opt=None):\n        self.generator = generator\n        self.criterion = criterion\n        self.opt = opt\n    def __call__(self, x, y, norm):\n        x = self.generator(x)    \n        loss = self.criterion(x.contiguous().view(-1, x.size(-1)),\n                              y.contiguous().view(-1)) / norm    Defines warm-up steps\nA step() method to apply \nthe optimizer to adjust \nmodel parameters\nCalculates \nthe learning \nrate based \non steps\nUses the model to \nmake predictions\nCompares the predictions \nwith labels to calculate loss, \nutilizing label smoothing",8265
98-10.4 Translating English to French with the trained model.pdf,98-10.4 Translating English to French with the trained model,"233 Training the Transformer for English-to-French translation\n        loss.backward()    \n        if self.opt is not None:\n            self.opt.step()    \n            self.opt.optimizer.zero_grad()\n        return loss.data.item() * norm.float()\nThe SimpleLossCompute()  class is designed with three key elements: generator , \nserving as the prediction model; criterion , which is a function to calculate loss; and \nopt, the optimizer. This class processes a batch of training data, denoted as (x, y), by \nutilizing the generator for predictions. It subsequently evaluates the loss by comparing \nthese predictions with the actual labels y (which is handled by the Label Smoothing() \nclass defined earlier; the actual labels y will be smoothed in the process). The class \ncomputes gradients relative to the model parameters and utilizes the optimizer to \nupdate these parameters accordingly.\nWe are now ready to define the loss function:\nfrom utils.ch09util import (LabelSmoothing,\n       SimpleLossCompute)\ncriterion = LabelSmoothing(tgt_vocab, \n                           padding_idx=0, smoothing=0.1)\nloss_func = SimpleLossCompute(\n            model.generator, criterion, optimizer)\nNext, we’ll train the Transformer by using the data we prepared earlier in the chapter.\n10.3.2  The training loop\nWe could potentially divide the training data into a train set and a validation set and \ntrain the model until the performance of the model doesn’t improve on the validation \nset, similar to what we have done in chapter 2. However, to save space, we’ll train the \nmodel for 100 epochs. We’ll calculate the loss and the number of tokens from each \nbatch. After each epoch, we calculate the average loss in the epoch as the ratio between \nthe total loss and the total number of tokens.\nListing 10.8    Training a Transformer to translate English to French\nfor epoch in range(100):\n    model.train()\n    tloss=0\n    tokens=0\n    for batch in BatchLoader():\n        out = model(batch.src, batch.trg, \n                    batch.src_mask, batch.trg_mask)    \n        loss = loss_func(out, batch.trg_y, batch.ntokens)    \n        tloss += loss\n        tokens += batch.ntokens    \n    print(f""Epoch {epoch}, average loss: {tloss/tokens}"")\ntorch.save(model.state_dict(),""files/en2fr.pth"")    Calculates gradients \nwith respect to model \nparameters\nAdjusts model parameters \n(backpropagate)\nMakes predictions \nusing the Transformer\nCalculates loss and \nadjusts model \nparameters\nCounts the number of \ntokens in the batch\nSaves the weights in the \ntrained model after training\n234 chapter  10 Training a Transformer to translate English to French\nThis training process takes a couple of hours if you are using a CUDA-enabled GPU. It \nmay take a full day if you are using CPU training. Once the training is done, the model \nweights are saved as en2fr.pth on your computer. Alternatively, you can download the \ntrained weights from my website ( https: //gattonweb.uky.edu/faculty/lium/gai/ch9  \n.zip).\n10.4 Translating English to French with the trained model\nNow that you have trained the Transformer, you can use it to translate any English sen -\ntence to French. We define a function translate()  as shown in the following listing.\nListing 10.9    Defining a translate()  function to translate English to French\ndef translate(eng):\n    tokenized_en=tokenizer.tokenize(eng)\n    tokenized_en=[""BOS""]+tokenized_en+[""EOS""]\n    enidx=[en_word_dict.get(i,UNK) for i in tokenized_en]  \n    src=torch.tensor(enidx).long().to(DEVICE).unsqueeze(0)    \n    src_mask=(src!=0).unsqueeze(-2)\n    memory=model.encode(src,src_mask)    \n    start_symbol=fr_word_dict[""BOS""]\n    ys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data)\n    translation=[]\n    for i in range(100):\n        out = model.decode(memory,src_mask,ys,\n        subsequent_mask(ys.size(1)).type_as(src.data))    \n        prob = model.generator(out[:, -1])\n        _, next_word = torch.max(prob, dim=1)\n        next_word = next_word.data[0]    \n        ys = torch.cat([ys, torch.ones(1, 1).type_as(\n            src.data).fill_(next_word)], dim=1)\n        sym = fr_idx_dict[ys[0, -1].item()]\n        if sym != 'EOS':\n            translation.append(sym)\n        else:\n            break    \n    trans="""".join(translation)\n    trans=trans.replace(""</w>"","" "") \n    for x in '''?:;.,'(""-!&)%''':\n        trans=trans.replace(f "" {x}"",f""{x}"")    \n    print(trans) \n    return trans\nTo translate an English phrase to French, we first use the tokenizer to convert the \nEnglish sentence to tokens. We then add ""BOS""  and ""EOS""  at the beginning and the \nend of the phrase. We use the dictionary en_word_dict  we created earlier in the chap -\nter to convert tokens to indexes. We feed the sequence of indexes to the encoder in the \ntrained model. The encoder produces an abstract vector representation and passes it \non to the decoder.   Uses encoder \nto convert the \nEnglish phrase \nto a vector \nrepresentation\nPredicts the next \ntoken using the \ndecoder\nStops translating when the \nnext token is “EOS”\nJoins the predicted tokens \nto form a French sentence\n 235 Translating English to French with the trained model\nBased on the abstract vector representation of the English sentence produced by the \nencoder, the decoder in the trained model starts translating in an autoregressive man -\nner, starting with the beginning token  ""BOS"" . In each time step, the decoder generates \nthe most likely next token based on previously generated tokens, until the predicted \ntoken is ""EOS"" , which signals the end of the sentence. Note this is slightly different from \nthe text generation approach discussed in chapter 8, where the next token is chosen \nrandomly in accordance with its predicted probabilities. Here, the method for selecting \nthe next token is deterministic, meaning the token with the highest probability is cho -\nsen with certainty because we mainly care about accuracy. However, you can switch to \nstochastic prediction as we did in chapter 8 and use top-K  sampling and temperature if \nyou want your translation to be creative. \nFinally, we change the token separator to a space and remove the space before the \npunctuation marks. The output is the French translation in a clean format. \nLet’s try the translate() function with the English phrase “Today is a beautiful \nday!”:\nfrom utils.ch09util import subsequent_mask\nwith open(""files/dict.p"",""rb"") as fb:\n    en_word_dict,en_idx_dict,\\n    fr_word_dict,fr_idx_dict=pickle.load(fb)\ntrained_weights=torch.load(""files/en2fr.pth"",\n                           map_location=DEVICE)\nmodel.load_state_dict(trained_weights)\nmodel.eval()\neng = ""Today is a beautiful day!""\ntranslated_fr = translate(eng)\nThe output is\naujourd'hui est une belle journee!\nYou can verify that the French translation indeed means “Today is a beautiful day!” by \nusing, say, Google Translate. \nLet’s try a longer sentence and see if the trained model can successfully translate:\neng = ""A little boy in jeans climbs a small tree while another child looks on.""\ntranslated_fr = translate(eng)\nThe output is\nun petit garcon en jeans grimpe un petit arbre tandis qu'un autre enfant regarde. \nWhen I translate the preceding output back to English using Google Translate, it says, \n“a little boy in jeans climbs a small tree while another child watches”—not exactly the \nsame as the original English sentence, but the meaning is the same. \nNext, we’ll test if the trained model generates the same translation for the two \nEnglish sentences “I don’t speak French.” and “I do not speak French.” First, let’s try the \nsentence “I don’t speak French.”:\n236 chapter  10 Training a Transformer to translate English to French\neng = ""I don't speak French.""\ntranslated_fr = translate(eng)\nThe output is\nje ne parle pas francais. \nNow let’s try the sentence “I do not speak French.”:\neng = ""I do not speak French.""\ntranslated_fr = translate(eng)\nThe output this time is\nje ne parle pas francais. \nThe results indicate that French translations of the two sentences are exactly the same. \nThis suggests that the encoder component of the Transformer successfully grasps the \nsemantic essence of the two phrases. It then represents them as similar abstract con -\ntinuous vector forms, which are subsequently passed on to the decoder. The decoder \nthen generates translations based on these vectors and produces identical results. \nExercise 10.3\nUse the translate()  function to translate the following two English sentences to \nFrench. Compare the results with those from Google Translate and see if they are the \nsame: (i) I love skiing in the winter! (ii) How are you?  \nIn this chapter, you trained an encoder-decoder Transformer to translate English to \nFrench by using more than 47,000 pairs of English-to-French translations. The trained \nmodel works well, translating common English phrases correctly!\nIn the following chapters, you’ll explore decoder-only Transformers. You’ll learn to \nbuild them from scratch and use them to generate coherent text, better than the text \nyou generated in chapter 8 using long short-term memory. \nSummary\n¡ Transformers process input data such as sentences in parallel, unlike recurrent \nneural networks, which handle data sequentially. This parallelism enhances \ntheir efficiency but doesn’t inherently allow them to recognize the sequence \norder of the input. To address this, Transformers add positional encodings to \nthe input embeddings. These positional encodings are unique vectors assigned \nto each position in the input sequence and align in dimension with the input \nembeddings.    \n¡ Label smoothing is commonly used in training deep neural networks to improve \nthe generalization of the model. It is used to address overconfidence problems \n(the predicted probability is greater than the true probability) and overfitting \nin classifications. Specifically, it modifies the way the model learns by adjusting \n 237 Summary\nthe target labels, aiming to reduce the model’s confidence in the training data, \nwhich can lead to better performance on unseen data.\n¡ Based on the encoder’s output that captures the meaning of the English phrase, \nthe decoder in the trained Transformer starts translating in an autoregressive \nmanner, starting with the beginning token ""BOS"" . In each time step, the decoder \ngenerates the most likely next token based on previously generated tokens, until \nthe predicted token is ""EOS"" , which signals the end of the sentence.",10644
99-11 Building a generative pretrained Transformer.pdf,99-11 Building a generative pretrained Transformer,"23811Building a generative \npretrained Transformer  \nfrom scratch\nThis chapter covers\n¡ Building a generative pretrained Transformer from  \n scratch\n¡ Causal self-attention\n¡ Extracting and loading weights from a pretrained  \n model\n¡ Generating coherent text with GPT-2, the  \n predecessor of ChatGPT and GPT-4\nGenerative Pretrained Transformer 2 (GPT-2) is an advanced large language model \n(LLM) developed by OpenAI and announced in February 2019. It represents a sig -\nnificant milestone in the field of natural language processing (NLP) and has paved \nthe way for the development of even more sophisticated models, including its suc -\ncessors, ChatGPT and GPT-4.  \nGPT-2, an improvement over its predecessor, GPT-1, was designed to generate \ncoherent and contextually relevant text based on a given prompt, demonstrating \na remarkable ability to mimic human-like text generation across various styles and \n 239  \ntopics. Upon its announcement, OpenAI initially decided not to release to the pub -\nlic the most powerful version of GPT-2 (also the one you’ll build from scratch in this \nchapter, with 1.5 billion parameters). The main concern was potential misuse, such as \ngenerating misleading news articles, impersonating individuals online, or automating \nthe production of abusive or fake content. This decision sparked a significant debate \nwithin the AI and tech communities about the ethics of AI development and the bal -\nance between innovation and safety.\nOpenAI later adopted a staggered release strategy, gradually making smaller ver -\nsions of the model available while monitoring the effect and exploring safe deployment \nstrategies. Eventually, in November 2019, OpenAI released the full model, along with \nseveral datasets and a tool to detect model-generated text, contributing to discussions \non responsible AI usage. Because of this release, you’ll learn to extract the pretrained \nweights from GPT-2 and load them to the GPT-2 model that you create. \nGPT-2 is based on the Transformer architecture that we discussed in chapters 9 \nand 10. However, unlike the English-to-French translator you created before, GPT-2 \nis a decoder-only Transformer, meaning there is no encoder stack in the model. When \ntranslating an English phrase into French, the encoder captures the meaning of the \nEnglish phrase and passes it to the decoder to generate the translation. However, in \ntext generation tasks, the model does not need an encoder to understand a different \nlanguage. Instead, it generates text based on the previous tokens in the sentence, using \nonly a decoder-only architecture. Like other Transformer models, GPT-2 uses self-\nattention mechanisms to process input data in parallel, significantly improving the \nefficiency and effectiveness of training LLMs. \nGPT-2 is pretrained on a large corpus of text data, essentially predicting the next \nword in a sentence given the words that precede it. This training enables the model to \nlearn a wide range of language patterns, grammar, and knowledge. \nIn this chapter, you’ll learn to build GPT-2XL, the largest version of GPT-2, from \nscratch. After that, you’ll learn how to extract the pretrained weights from Hugging \nFace (an AI community that hosts and collaborates on machine learning models, data -\nsets, and applications) and load them to your own GPT-2 model. You’ll use your GPT-2 \nto generate text by feeding a prompt to the model. GPT-2 calculates the probabilities of \npossible next tokens and samples from these probabilities. It can produce coherent and \ncontextually relevant paragraphs of text based on the input prompt it receives. Addi -\ntionally, as you did in chapter 8, you can control the creativeness of the generated text \nby using temperature  and top-K  sampling. \nWhile GPT-2 marks a notable advance in NLP, it’s essential to moderate your expec -\ntations and recognize its inherent limitations. It’s crucial not to compare GPT-2 with \nChatGPT or GPT-4 directly, as GPT-2XL has only 1.5 billion parameters compared to \nChatGPT’s 175 billion and GPT-4’s estimated 1.76 trillion parameters. One of the main \nlimitations of GPT-2 is its lack of genuine comprehension of the content it generates. chapter  11 Building a generative pretrained Transformer from scratch",4321
100-11.1 GPT-2 architecture and causal self-attention.pdf,100-11.1 GPT-2 architecture and causal self-attention,,0
101-11.1.1 The architecture of GPT-2.pdf,101-11.1.1 The architecture of GPT-2,"240 chapter  11 Building a generative pretrained Transformer from scratch \nThe model predicts the next word in a sequence based on the probability distribution \nof words in its training data, which can produce syntactically correct and seemingly \nlogical text. However, the model lacks a true understanding of the meaning behind the \nwords, leading to potential inaccuracies, nonsensical statements, or superficial content.\nAnother key factor is GPT-2’s limited contextual awareness. While it can maintain \ncoherence over short spans of text, it struggles with longer passages, potentially result -\ning in a loss of coherence, contradictions, or irrelevant content. We should be cautious \nnot to overestimate the model’s ability to generate long-form content that requires sus -\ntained attention to context and detail. Therefore, while GPT-2 represents a significant \nstep forward in NLP, it’s important to approach its generated text with a healthy dose of \nskepticism and set realistic expectations.\n11.1 GPT-2 architecture and causal self-attention\nGPT-2 operates as a solely decoder-based Transformer (it generates text based on pre -\nvious tokens in the sentence without the need for an encoder to understand a different \nlanguage), mirroring the decoder component of the English-to-French translator dis -\ncussed in chapters 9 and 10. Unlike its bilingual counterpart, GPT-2 lacks an encoder \nand thus does not incorporate encoder-derived inputs in its output generation pro -\ncess. The model relies entirely on preceding tokens within the sequence to produce its \noutput.\nIn this section, we’ll discuss the architecture of GPT-2. We will also dive into the \ncausal self-attention mechanism, which is the core of the GPT-2 model.\n11.1.1  The architecture of GPT-2\nGPT-2 comes in four different sizes: small (S), medium (M), large (L), and extra-large \n(XL), each varying in capability. Our primary focus will be on the most powerful ver -\nsion, GPT-2XL. The smallest GPT-2 model has around 124 million parameters, while \nthe extra-large version has about 1.5 billion parameters. It is the most powerful among \nthe GPT-2 models, with the highest number of parameters. GPT-2XL can understand \ncomplex contexts, generating coherent and nuanced text.\nGPT-2 consists of many identical decoder blocks. The extra-large version has 48 \ndecoder blocks, while the other three versions have 12, 24, and 36 decoder blocks, \nrespectively. Each of these decoder blocks comprises two distinct sublayers. The first \nsublayer is a causal self-attention layer, which I’ll explain in detail soon. The second \nsublayer is a basic, position-wise, fully connected feed-forward network, as we have seen \nin the encoder and decoder blocks in the English-to-French translator. Each sublayer \nincorporates layer normalization and a residual connection to stabilize the training \nprocess.\nFigure 11.1 is a diagram of the architecture of GPT-2. \n 241 GPT-2 architecture and causal self-attention\nAdd & normalize\nFeed forward\nGPT-2 consists \nof N identical \ndecoder layers. Second sublayer of each\ndecoder layer:  \nFeed forward with add & \nnorm \nAdd & normalize\nCausal self-attentionFirst sublayer of each \ndecoder layer:\nCausal self-attention\nwith add & norm GPT-2 is a \ndecoder-only \ntransformer. \nWord embedding\nInputPositional \nencodingNormalizeLinear headOutput\nFigure 11.1    The architecture of the GPT-2 model. GPT-2 is a decoder-only Transformer, consisting of \nN identical decoder layers. Each decoder block contains two sublayers. The first sublayer is a causal \nself-attention layer. The second is a feed-forward network. Each sublayer uses layer normalization and a \nresidual connection. The input is first passed through word embedding and positional encoding, and the \nsum is then passed through the decoder. The output from the decoder goes through layer normalization \nand a linear layer. \nGPT-2 first passes indexes for a sequence of tokens through word embedding and posi -\ntional encoding to obtain input embedding (I’ll explain soon how this process works). \nThe input embedding is passed through N decoder blocks sequentially. After that, the \noutput is passed through layer normalization and a linear layer. The number of out -\nputs in GPT-2 is the number of unique tokens in the vocabulary (50,257 tokens for all \nGPT-2 versions). The model is designed to predict the next token based on all previous \ntokens in the sequence. \nTo train GPT-2, OpenAI used a dataset called WebText, which was collected automat -\nically from the internet. The dataset contained a wide variety of text, including websites \nlike Reddit links that were highly upvoted, aiming to cover a broad spectrum of human \nlanguages and topics. This dataset is estimated to contain about 40GB of text.",4842
102-11.1.3 Causal self-attention in GPT-2.pdf,102-11.1.3 Causal self-attention in GPT-2,"242 chapter  11 Building a generative pretrained Transformer from scratch \nThe training data was broken into sequences of a fixed length (1,024 tokens for all \nGPT-2 versions) and used as inputs. The sequences were shifted to the right by one \ntoken and used as outputs to the model during training. Since the model uses causal \nself-attention, in which future tokens in a sequence are masked (i.e., hidden) during \nthe training process, this is effectively training the model to predict the next token \nbased on all previous tokens in the sequence. \n11.1.2  Word embedding and positional encoding in GPT-2\nGPT-2 uses a subword tokenization method called the Byte Pair Encoder (BPE) to \nbreak text into individual tokens (whole words or punctuation marks in most cases but \nsyllables for uncommon words). These tokens are then mapped into an index between \n0 and 50,256 since the vocabulary size is 50,257. GPT-2 transforms text in the training \ndata into vector representations that capture its meaning through word embedding, \nsimilar to what you’ve done in the previous two chapters. \nTo give you a concrete example, the phrase “this is a prompt” is first converted into \nfour tokens through BPE tokenization, [ 'this', ' is', ' a', ' prompt' ]. Each \ntoken is then represented by a one-hot variable of size 50,257. The GPT-2 model passes \nthem through a word embedding layer to compress them into condensed vectors with \nfloating point values of a much smaller size, such as a length of 1,600 in GPT-2XL (the \nlengths are 768, 1,024, and 1,280, for the other three versions of GPT-2, respectively). \nWith word embedding, the phrase “this is a prompt” is represented by a matrix with size \n4 × 1,600 instead of the original 4 × 50,257. Word embedding significantly reduces the \nnumber of the model’s parameters and makes training more efficient. The left side of \nfigure 11.2 depicts how word embedding works. \nWord embedding layer \nSize (50,257,1,600) Positional encoding layer \nSize (1,024,1,600) \nWord embedding \nSize (4,1,600)Positional encoding\nSize (4,1,600)\nInput embedding \nSize (4,1,600)AddPositional representation of “this \nis a prompt”, size (4, 1,024) Token representation of “this is a \nprompt”, size (4, 50,257) \nFigure 11.2    GPT-2 first represents each token in a sequence with a 50,276-value one-hot vector. \nThe token representation of the sequence goes through a word embedding layer to compress it into \nan embedding with a dimension of 1,600. GPT-2 also represents each position in a sequence with a \n1,024-value one-hot vector. The positional representation of the sequence goes through a positional \nencoding layer to compress it into an embedding also with a dimension of 1,600. The word embedding \nand positional encoding are added together to form the input embedding. \n 243 GPT-2 architecture and causal self-attention\nGPT-2, like other Transformers, processes input data in parallel, and this inherently \ndoesn’t allow it to recognize the sequence order of the input. To address this, we need \nto add positional encodings to the input embeddings. GPT-2 adopts a unique approach \nto positional encoding, diverging from the methodology outlined in the seminal 2017 \n“Attention Is All You Need” paper. Instead, GPT-2’s technique for positional encoding \nparallels that of word embeddings. Given the model’s capacity to handle up to 1,024 \ntokens in an input sequence, each position within the sequence is initially denoted by \na one-hot vector of the same size. For instance, in the sequence “this is a prompt,” the \nfirst token is represented by a one-hot vector where all elements are zero except for the \nfirst, which is set to one. The second token follows suit, represented by a vector where \nall but the second element are zero. Consequently, the positional representation for \nthe phrase “this is a prompt” manifests as a 4 × 1,024 matrix, as illustrated in the upper \nright section of figure 11.2. \nTo generate positional encoding, the sequence’s positional representation under -\ngoes processing through a linear neural network, which is dimensioned at 1,024 × 1,600. \nThe weights within this network are randomly initialized and subsequently refined \nthrough the training process. As a result, the positional encoding for each token in \nthe sequence is a 1,600-value vector, matching the dimension of the word embedding \nvector. A sequence’s input embedding is the sum of its word embedding and positional \nencoding, as depicted at the bottom of figure 11.2. In the context of the phrase “this is a \nprompt,” both the word embedding and positional encoding are structured as 4 × 1,600 \nmatrices. Therefore, the input embedding for “this is a prompt,” which is the sum of \nthese two matrices, maintains a dimensionality of 4 × 1,600.\n11.1.3  Causal self-attention in GPT-2\nCausal self-attention is a crucial mechanism within the GPT-2 model (and broadly in \nthe GPT series of models), enabling the model to generate text by conditioning on the \nsequence of previously generated tokens. It’s similar to the masked self-attention in the \nfirst sublayer of each decoder layer in the English-to-French translator we discussed in \nchapters 9 and 10, though the implementation differs slightly. \nNOTE     The concept of “causal” in this context refers to the model’s ability to \nensure that predictions for a given token can only be influenced by the tokens \nthat precede it in the sequence, respecting the causal (time-forward) direction \nof text generation. This is essential for generating coherent and contextually \nrelevant text outputs.\nSelf-attention is a mechanism that allows each token in the input sequence to attend \nto all other tokens in the same sequence. In the context of Transformer models like \nGPT-2, self-attention enables the model to weigh the importance of other tokens when \nprocessing a specific token, thereby capturing the context and relationships between \nwords in a sentence.\n244 chapter  11 Building a generative pretrained Transformer from scratch \nTo ensure causality, GPT-2’s self-attention mechanism is modified so that any given \ntoken can only attend to itself and the tokens that have come before it in the sequence. \nThis is achieved by masking future tokens (i.e., tokens that come after the current \ntoken in the sequence) in the attention calculation, ensuring that the model cannot \n“see” or be influenced by future tokens when predicting the next token in a sequence. \nFor example, in the phrase “this is a prompt,” the mask hides the last three words in the \nfirst time step when the model uses the word “this” to predict the word “is.” To imple -\nment this, positions corresponding to future tokens are set to minus infinity when we \ncompute the attention scores. After softmax activation, future tokens are allocated zero \nweights, effectively removing them from the attention calculation.\nLet’s use a concrete example to illustrate exactly how causal self-attention works in \ncode. The input embedding for the phrase “this is a prompt” is a 4 × 1,600 matrix after \nword embedding and positional encoding. We then pass this input embedding through \nN decoder layers in GPT-2. In each decoder layer, it first goes through the causal self-\nattention sublayer as follows. The input embedding is passed through three neural \nnetworks to create query Q, key K, and value V, as shown in the following listing.\nListing 11.1    Creating query , key, and value  vectors\nimport torch\nimport torch.nn as nn\ntorch.manual_seed(42)\nx=torch.randn((1,4,1600))    \nc_attn=nn.Linear(1600,1600*3)    \nB,T,C=x.size()\nq,k,v=c_attn(x).split(1600,dim=2)    \nprint(f""the shape of Q vector is {q.size()}"")\nprint(f""the shape of K vector is {k.size()}"")\nprint(f""the shape of V vector is {v.size()}"")    \nWe first create a matrix with size 4 × 1,600, the same size as the input embedding for \n“this is a prompt”. We then pass the input embedding through three neural networks, \neach with a size of 1,600 × 1,600, to obtain query Q, key K, and value V. If you run the \npreceding code block, you’ll see the following output:\nthe shape of Q vector is torch.Size([1, 4, 1600])\nthe shape of K vector is torch.Size([1, 4, 1600])\nthe shape of V vector is torch.Size([1, 4, 1600])\nThe shapes of Q, K, and V are all 4 × 1,600. Next, instead of using one head, we split \nthem into 25 parallel heads. Each head pays attention to different parts or aspects of \nthe input, enabling the model to capture a broader range of information and form a \nmore detailed and contextual understanding of the input data. As a result, we have 25 \nsets of Q, K, and V:Creates an input embedding xCreates three neural networks\nPasses the input embedding \nthe three neural networks to \ncreate Q, K, and V\nPrints out the sizes \nof Q, K, and V\n 245 GPT-2 architecture and causal self-attention\nhs=C//25\nk = k.view(B, T, 25, hs).transpose(1, 2) \nq = q.view(B, T, 25, hs).transpose(1, 2) \nv = v.view(B, T, 25, hs).transpose(1, 2)    \nprint(f""the shape of Q vector is {q.size()}"")\nprint(f""the shape of K vector is {k.size()}"")\nprint(f""the shape of V vector is {v.size()}"")    \nIf you run the preceding code block, you’ll see the following output:\nthe shape of Q vector is torch.Size([1, 25, 4, 64])\nthe shape of K vector is torch.Size([1, 25, 4, 64])\nthe shape of V vector is torch.Size([1, 25, 4, 64])\nThe shapes of Q, K, and V are now 25 × 4 × 64: this means we have 25 heads; each head \nhas a set of query, key, and value, all having a size of 4 × 64. \nNext, we calculate the scaled attention scores in each head:\nimport math\nscaled_att = (q @ k.transpose(-2, -1)) *\\n            (1.0 / math.sqrt(k.size(-1)))\nprint(scaled_att[0,0])\nThe scaled attention scores are the dot product of Q and K in each head, scaled by the \nsquare root of the dimension of K, which is 1,600/25 = 64. The scaled attention scores \nform a 4 × 4 matrix in each head, and we print out those in the first head:\ntensor([[ 0.2334,  0.1385, -0.1305,  0.2664],\n        [ 0.2916,  0.1044,  0.0095,  0.0993],\n        [ 0.8250,  0.2454,  0.0214,  0.8667],\n        [-0.1557,  0.2034,  0.2172, -0.2740]], grad_fn=<SelectBackward0>)\nThe scaled attention scores in the first head are also shown in the bottom left table in \nfigure 11.3. \nExercise 11.1\nThe tensor scaled_att  contains the scaled attention scores in the 25 heads. We have \nprinted out those in the first head previously. How do you print out the scaled attention \nscores in the second head? \nNext, we apply a mask to the scaled attention scores to hide future tokens in the \nsequence:\nmask=torch.tril(torch.ones(4,4))    \nprint(mask)\nmasked_scaled_att=scaled_att.masked_fill(\\n    mask == 0, float('-inf'))    \nprint(masked_scaled_att[0,0])Splits Q, K, and V into \n25 heads\nPrints out the size of the \nmultihead Q, K, and V\nCreates a mask\nApplies the mask on the \nscaled attention scores by \nchanging the values to – ∞ \nfor future tokens\n246 chapter  11 Building a generative pretrained Transformer from scratch \n1000\n0.5467 0.4533 0 0\n0.4980 0.2790 0.2230 0\n0.2095 0.3001 0.3042 0.1862thisis\nis\naa\npromptprompt\n0.2334 0.1385 -0.130 0.2664\n0.2916 0.1044 0.0095 0.0993\n0.8250 0.2454 0.0214 0.8667\n-0.155 0.2034 0.2172 -0.274Scaled attention scores1000\n1100\n1110\n1111Mask =\nMasked scaled\nattention scoresApply maskApply softmax\n0.2334\n0.2916 0.1044\n0.8250 0.2454 0.0214\n-0.155 0.2034 0.2172 -0.274this\nFigure 11.3    How to calculate masked attention weights in causal self-attention. A mask is applied \nto the scaled attention scores so that values corresponding to future tokens (those above the main \ndiagonal in the matrix) become –∞. We then apply the softmax function on the masked scaled attention \nscores and obtain the masked attention weights. The masking ensures that predictions for a given \ntoken can only be influenced by the tokens that precede it in the sequence, not by future tokens. This is \nessential for generating coherent and contextually relevant text outputs.\nIf you run the preceding code, you’ll see the following output:\ntensor([[1., 0., 0., 0.],\n        [1., 1., 0., 0.],\n        [1., 1., 1., 0.],\n        [1., 1., 1., 1.]])\ntensor([[ 0.2334,    -inf,    -inf,    -inf],\n        [ 0.2916,  0.1044,    -inf,    -inf],\n        [ 0.8250,  0.2454,  0.0214,    -inf],\n        [-0.1557,  0.2034,  0.2172, -0.2740]], grad_fn=<SelectBackward0>)\nThe mask is a 4 × 4 matrix as shown at the top of figure 11.3. The lower half of the mask \n(values below the main diagonal) are 1s while the upper half of the mask (values above \nthe main diagonal) are 0s. When this mask is applied to the scaled attention scores, the \nvalues in the upper half of the matrix become –∞ (the middle bottom of figure 11.3). \nThis way, when we apply the softmax function on the scaled attention scores, the upper \nhalf of the attention weights matrix is filled with 0s (bottom right of figure 11.3):\nimport torch.nn.functional as F\natt = F.softmax(masked_scaled_att, dim=-1)\nprint(att[0,0])\nWe print out the attention weights in the first head with the following values:\ntensor([[1.0000, 0.0000, 0.0000, 0.0000],\n        [0.5467, 0.4533, 0.0000, 0.0000],",13315
103-11.2.2 The Gaussian error linear unit activation function.pdf,103-11.2.2 The Gaussian error linear unit activation function,"247 Building GPT-2XL from scratch\n        [0.4980, 0.2790, 0.2230, 0.0000],\n        [0.2095, 0.3001, 0.3042, 0.1862]], grad_fn=<SelectBackward0>)\nThe first row means in the first time step, the token “this” attends only to itself and not \nto any future tokens. Similarly, if you look at the second row, the tokens “this is” attend \nto each other but not to future tokens “a prompt”. \nNOTE     The weights in this numerical example are not trained, so don’t take these \nvalues in attention weights literally. We use them as an example to illustrate how \ncausal self-attention works. \nExercise 11.2\nWe have printed out the attention weights in the first head. How do you print out the \nattention weights in the last (i.e., the 25th) head? \nFinally, we calculate the attention vector in each head as the dot product of attention \nweights and the value vector. The attention vectors in the 25 heads are then joined \ntogether as one single attention vector:\ny=att@v\ny = y.transpose(1, 2).contiguous().view(B, T, C)\nprint(y.shape)\nThe output is  \ntorch.Size([1, 4, 1600])\nThe final output after causal self-attention is a 4 × 1,600 matrix, the same size as the \ninput to the causal self-attention sublayer. The decoder layers are designed in such a \nway that the input and output have the same dimensions, and this allows us to stack \nmany decoder layers together to increase the representation capacity of the model and \nto enable hierarchical feature extraction during training.\n11.2 Building GPT-2XL from scratch\nNow that you understand the architecture of GPT-2 and how its core ingredient, causal \nself-attention, functions, let’s create the largest version of GPT-2 from scratch.\nIn this section, you’ll first learn to use the subword tokenization method in  \nGPT-2, the byte pair encoder (BPE) tokenizer, to break text into individual tokens. \nYou’ll also learn the GELU activation function used in the feed-forward network in \nGPT-2. After that, you’ll code in the causal self-attention mechanism and combine it \nwith a feed-forward network to form a decoder block. Finally, you’ll stack 48 decoder \nblocks to create the GPT-2XL model. The code in this chapter is adapted from the \nexcellent GitHub repository by Andrej Kaparthy ( https: //github.com/karpathy/\nminGPT ). I encourage you to read through the repository if you want to dive deeper \ninto how GPT-2 works. \n248 chapter  11 Building a generative pretrained Transformer from scratch \n11.2.1  BPE tokenization\nGPT-2 uses a subword tokenization method called byte pair encoder (BPE), which is a \ndata compression technique that has been adapted for use in tokenizing text in NLP \ntasks. It’s particularly well-known for its application in training LLMs, such as the GPT \nseries and BERT (Bidirectional Encoder Representations from Transformers). The \nprimary goal of BPE is to encode a piece of text into a sequence of tokens in a way that \nbalances the vocabulary size and the length of the tokenized text.\nBPE operates by iteratively merging the most frequent pair of consecutive charac -\nters in a dataset into a single new token, subject to certain conditions. This process is \nrepeated until a desired vocabulary size is reached or no more merges are beneficial. \nBPE allows for an efficient representation of text, balancing between character-level \nand word-level tokenization. It helps to reduce the vocabulary size without significantly \nincreasing the sequence length, which is crucial for the performance of NLP models.\nWe discussed the pros and cons of the three types of tokenization methods (character-\nlevel, word-level, and subword tokenizations) in chapter 8. Further, you implemented \na word-level tokenizer from scratch in chapter 8 (and will do so again in chapter 12). \nTherefore, in this chapter, we’ll borrow the tokenization method from OpenAI directly. \nThe detailed workings of BPE are beyond the scope of this book. All you need to know \nis that it first converts text into subword tokens and then the corresponding indexes. \nDownload the file bpe.py  from Andrej Karpathy’s GitHub repository, https: //mng  \n.bz/861B , and place the file in the folder /utils/ on your computer. We’ll use the file \nas a local module in this chapter. As Andrej Karpathy explained in his GitHub reposi -\ntory, the module is based on OpenAI’s implementation at https: //mng.bz/EOlj  but was \nmildly modified to make it easier to understand. \nTo see how the module bpe.py  converts text into tokens and then indexes, let’s try \nan example:\nfrom utils.bpe import get_encoder\nexample=""This is the original text.""    \nbpe_encoder=get_encoder()    \nresponse=bpe_encoder.encode_and_show_work(example)\nprint(response[""tokens""])    \nThe output is\n['This', ' is', ' the', ' original', ' text', '.']\nThe BPE tokenizer splits the example text “This is the original text.” into six tokens as \nshown in the preceding output. Note that the BPE tokenizer doesn’t convert upper -\ncase letters to lowercase ones. This leads to more meaningful tokenization but also \na much larger number of unique tokens. In fact, all versions of GPT-2 models have a \nvocabulary size of 50,276, several times larger than the vocabulary size in the previous \nchapters. The text for an example sentence\nInstantiates the get_encoder() \nclass from the bpe.py module\nTokenizes the example text \nand print out the tokens\n 249 Building GPT-2XL from scratch\nWe can also use the module bpe.py  to map tokens to indexes:\nprint(response['bpe_idx'])\nThe output is\n[1212, 318, 262, 2656, 2420, 13]\nThe preceding list contains the six indexes corresponding to the six tokens in the \nexample text “This is the original text.” \nWe can also restore the text based on the indexes:\nfrom utils.bpe import BPETokenizer \ntokenizer = BPETokenizer()    \nout=tokenizer.decode(torch.LongTensor(response['bpe_idx']))    \nprint(out) \nThe output from the preceding code block is\nThis is the original text.\nAs you can see, the BPE tokenizer has restored the example text to its original form. \nExercise 11.3\nUse the BPE tokenizer to split the phrase “this is a prompt” into tokens. After that, map \nthe tokens to indexes. Finally, restore the phrase based on the indexes. \n11.2.2  The Gaussian error linear unit activation function\nThe Gaussian error linear unit (GELU) activation function is used in the feed-forward \nsublayers of each decoder block in GPT-2. GELU provides a blend of linear and non -\nlinear activation properties that have been found to enhance model performance in \ndeep learning tasks, particularly NLP. \nGELU offers a nonlinear, smooth curve that allows for more nuanced adjustments \nduring training compared to other functions like the rectified linear unit (ReLU). This \nsmoothness helps in optimizing the neural network more effectively, as it provides a \nmore continuous gradient for backpropagation. To compare GELU with ReLU, our \ngo-to activation function, let’s first define a GELU() class:\nclass GELU(nn.Module):\n    def forward(self, x):\n        return 0.5*x*(1.0+torch.tanh(math.sqrt(2.0/math.pi)*\\n                       (x + 0.044715 * torch.pow(x, 3.0))))\nThe ReLU function is not differentiable everywhere since it has a kink in it. The GELU \nactivation function, in contrast, is differentiable everywhere and provides a better Instantiates the \nBPETokenizer() class \nfrom the bpe.py module\nUses the \ntokenizer \nto restore text \nbased on indexes\n250 chapter  11 Building a generative pretrained Transformer from scratch \nlearning process. Next we draw a picture of the GELU activation function and com -\npare it to ReLU.\nListing 11.2    Comparing two activation functions: GELU and ReLU \nimport matplotlib.pyplot as plt\nimport numpy as np\ngenu=GELU()\ndef relu(x):    \n    y=torch.zeros(len(x))\n    for i in range(len(x)):\n        if x[i]>0:\n            y[i]=x[i]\n    return y                 \nxs = torch.linspace(-6,6,300)\nys=relu(xs)\ngs=genu(xs)\nfig, ax = plt.subplots(figsize=(6,4),dpi=300)\nplt.xlim(-3,3)\nplt.ylim(-0.5,3.5)\nplt.plot(xs, ys, color='blue', label= ""ReLU"")    \nplt.plot(xs, gs, ""--"", color='red', label=""GELU"")    \nplt.legend(fontsize=15)\nplt.xlabel(""values of x"")\nplt.ylabel(""values of $ReLU(x)$ and $GELU(x)$"")\nplt.title(""The ReLU and GELU Activation Functions"")\nplt.show()\nIf you run the preceding code block, you’ll see a graph as shown in figure 11.4. \nFigure 11.4    Comparing the GELU activation function with ReLU. The solid line is the ReLU activation \nfunction, while the dashed line is the GELU activation function. ReLU is not differentiable everywhere \nsince there is a kink in it. GELU, in contrast, is differentiable everywhere. This smoothness in GELU \nhelps to optimize the neural network more effectively, as it provides a more continuous gradient for \nbackpropagation during the training process.Defines a function to \nrepresent ReLU\nPlots the ReLU \nactivation function in \nsolid lines\nPlots the GELU \nactivation function in \ndashed lines",9073
104-11.2.4 Constructing the GPT-2XL model.pdf,104-11.2.4 Constructing the GPT-2XL model,"251 Building GPT-2XL from scratch\nFurthermore, the formulation of GELU allows it to model input data distributions \nmore effectively. It combines the properties of linear and Gaussian distribution model -\ning, which can be particularly beneficial for the complex, varied data encountered in \nNLP tasks. This capability helps in capturing subtle patterns in language data, improv -\ning the model’s understanding and generation of text.\n11.2.3  Causal self-attention\nAs we explained earlier, causal self-attention is the core element in GPT-2 models. \nNext, we’ll implement this mechanism from scratch in PyTorch.\nWe first specify the hyperparameters in the GPT-2XL model that we’ll build in this \nchapter. To that end, we define a Config() class with the values shown in the following \nlisting.\nListing 11.3    Specifying hyperparameters in GPT-2XL\nclass Config():    \n    def __init__(self):\n        self.n_layer = 48\n        self.n_head = 25\n        self.n_embd = 1600\n        self.vocab_size = 50257\n        self.block_size = 1024 \n        self.embd_pdrop = 0.1 \n        self.resid_pdrop = 0.1 \n        self.attn_pdrop = 0.1    \n        \nconfig=Config()    \nWe define a Config()  class and create several attributes in it to be used as the hyperpa -\nrameters in the GPT-2XL model. The n_layer  attribute means the GPT-2XL model we \nconstruct will have 48 decoder layers (we use the terms “decoder block” and “decoder \nlayer” interchangeably). The n_head  attribute means we’ll split Q, K, and V into 25 \nparallel heads when calculating causal self-attention. The n_embd  attribute means the \nembedding dimension is 1,600: each token will be represented by a 1,600-value vector. \nThe vocab_size  attribute means there are 50,257 unique tokens in the vocabulary. \nThe block_size  attribute means the input sequence to the GPT-2XL model contains \nat most 1,024 tokens. The dropout rates are all set to 0.1. \nIn the last section, I explained in detail how causal self-attention works. Next, we \ndefine a CausalSelfAttention()  class to implement it.\nListing 11.4    Implementing causal self-attention\nclass CausalSelfAttention(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n        self.attn_dropout = nn.Dropout(config.attn_pdrop)Defines a Config() class\nPlaces model \nhyperparameters as \nattributes in the class\nInstantiates the \nConfig() class\n252 chapter  11 Building a generative pretrained Transformer from scratch \n        self.resid_dropout = nn.Dropout(config.resid_pdrop)\n        self.register_buffer(""bias"", torch.tril(torch.ones(\\n                   config.block_size, config.block_size))\n             .view(1, 1, config.block_size, config.block_size))    \n        self.n_head = config.n_head\n        self.n_embd = config.n_embd\n    def forward(self, x):\n        B, T, C = x.size() \n        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)    \n        hs = C // self.n_head\n        k = k.view(B, T, self.n_head, hs).transpose(1, 2) \n        q = q.view(B, T, self.n_head, hs).transpose(1, 2) \n        v = v.view(B, T, self.n_head, hs).transpose(1, 2)    \n        att = (q @ k.transpose(-2, -1)) *\\n            (1.0 / math.sqrt(k.size(-1)))\n        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, \\n                              float(‚-inf'))\n        att = F.softmax(att, dim=-1)    \n        att = self.attn_dropout(att)\n        y = att @ v \n        y = y.transpose(1, 2).contiguous().view(B, T, C)    \n        y = self.resid_dropout(self.c_proj(y))\n        return y\nIn PyTorch, register_buffer  is a method used to register a tensor as a buffer. Vari -\nables in a buffer are not considered learnable parameters of the model; hence they are \nnot updated during backpropagation. In the preceding code block, we have created a \nmask and registered it as a buffer. This has implications for how we extract and load \nmodel weights later: we’ll omit the masks when retrieving weights from GPT-2XL.\nAs we explained in the first section, the input embedding is passed through three \nneural networks to obtain query Q, key K, and value V. We then split them into 25 \nheads and calculate masked self-attention in each head. After that, we join the 25 atten -\ntion vectors back into one single attention vector, which is the output of the previous \nCausalSelfAttention()  class. \n11.2.4  Constructing the GPT-2XL model\nNext, we add a feed-forward network to the causal self-attention sublayer to form a \ndecoder block, as follows.\nListing 11.5    Constructing a decoder block\nclass Block(nn.Module):\n    def __init__(self, config):    \n        super().__init__()\n        self.ln_1 = nn.LayerNorm(config.n_embd)\n        self.attn = CausalSelfAttention(config)Creates a mask \nand registers it \nas a buffer \nsince it doesn’t \nneed to be \nupdated\nPasses input \nembedding \nthrough three \nneural \nnetworks to \nobtain Q, K, \nand V\nSplits Q, K, and \nV into multiple \nheads\nCalculates masked attention \nweights in each head\nConcatenates \nattention vectors \nin all heads into \none single \nattention vector\nInitiates the Block() \nclass\n 253 Building GPT-2XL from scratch\n        self.ln_2 = nn.LayerNorm(config.n_embd)\n        self.mlp = nn.ModuleDict(dict(\n            c_fc   = nn.Linear(config.n_embd, 4 * config.n_embd),\n            c_proj = nn.Linear(4 * config.n_embd, config.n_embd),\n            act    = GELU(),\n            dropout = nn.Dropout(config.resid_pdrop),\n        ))\n        m = self.mlp\n        self.mlpf=lambda x:m.dropout(m.c_proj(m.act(m.c_fc(x)))) \n    def forward(self, x):\n        x = x + self.attn(self.ln_1(x))    \n        x = x + self.mlpf(self.ln_2(x))    \n        return x\nEvery decoder block is composed of two sublayers. The first sublayer is the causal \nself-attention mechanism, with the integration of layer normalization and residual con -\nnection. The second sublayer within the decoder block is the feed-forward network, \nwhich incorporates the GELU activation function, alongside layer normalization and \nresidual connection. \nWe stack 48 decoder layers to form the main body of the GPT-2XL model, as shown \nin the following listing.\nListing 11.6    Building the GPT-2XL model\nclass GPT2XL(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.block_size = config.block_size\n        self.transformer = nn.ModuleDict(dict(\n            wte = nn.Embedding(config.vocab_size, config.n_embd),\n            wpe = nn.Embedding(config.block_size, config.n_embd),\n            drop = nn.Dropout(config.embd_pdrop),\n            h = nn.ModuleList([Block(config) \n                               for _ in range(config.n_layer)]),\n            ln_f = nn.LayerNorm(config.n_embd),))\n        self.lm_head = nn.Linear(config.n_embd,\n                                 config.vocab_size, bias=False)\n    def forward(self, idx, targets=None):\n        b, t = idx.size()\n        pos = torch.arange(0,t,dtype=torch.long).unsqueeze(0)\n        tok_emb = self.transformer.wte(idx)    \n        pos_emb = self.transformer.wpe(pos)    \n        x = self.transformer.drop(tok_emb + pos_emb)    \n        for block in self.transformer.h:\n            x = block(x)    The first sublayer in the block is \nthe causal self-attention sublayer, \nwith layer normalization and \nresidual connection.\nThe second sublayer in the block is a \nfeed-forward network, with GELU \nactivation, layer normalization, and \nresidual connection.\nCalculates input \nembedding as \nthe sum of word \nembedding and \npositional \nencoding\nPasses the input \nembedding through 48 \ndecoder blocks\n254 chapter  11 Building a generative pretrained Transformer from scratch \n        x = self.transformer.ln_f(x)    \n        logits = self.lm_head(x)    \n        loss = None\n        if targets is not None:\n            loss=F.cross_entropy(logits.view(-1,logits.size(-1)),\n                           targets.view(-1), ignore_index=-1)\n        return logits, loss\nWe construct the model in the GPT2XL()  class as we explained in the first section of \nthis chapter. The input to the model consists of sequences of indexes corresponding \nto tokens in the vocabulary. We first pass the input through word embedding and posi -\ntional encoding; we then add the two to form the input embedding. The input embed -\nding goes through 48 decoder blocks. After that, we apply layer normalization to the \noutput and then attach a linear head to it so that the number of outputs is 50,257, the \nsize of the vocabulary. The outputs are the logits corresponding to the 50,257 tokens \nin the vocabulary. Later, we’ll apply the softmax activation on the logits to obtain the \nprobability distribution over the unique tokens in the vocabulary when generating \ntext. \nNOTE     Since the model size is too large, we didn’t move the model to a GPU. This \nleads to a lower speed in text generation later in the chapter. However, if you \nhave access to a CUDA-enabled GPU with large memory (say, above 32GB), you \ncan move the model to a GPU for faster text generation. \nNext, we’ll create the GPT-2XL model by instantiating the GPT2XL()  class we defined \nearlier:\nmodel=GPT2XL(config)\nnum=sum(p.numel() for p in model.transformer.parameters())\nprint(""number of parameters: %.2fM"" % (num/1e6,))\nWe also count the number of parameters in the main body of the model. The output is\nnumber of parameters: 1557.61M\nThe preceding output shows that GPT-2XL has more than 1.5 billion parameters. Note \nthat the number doesn’t include the parameters in the linear head at the end of the \nmodel. Depending on what the downstream task is, we can attach different heads to the \nmodel. Since our focus is on text generation, we have attached a linear head to ensure \nthe number of outputs is equal to the number of unique tokens in the vocabulary. \nNOTE     In LLMs like GPT-2, ChatGPT, or BERT, an output head refers to the final \nlayer of the model that is responsible for producing the actual output based on \nthe processed input. This output can vary depending on the downstream task Applies layer normalization \none more time\nAttaches a linear head to \nthe output so the number \nof outputs equals the \nnumber of unique tokens\n 255 Building GPT-2XL from scratch\nthe model is performing. In text generation, the output head is often a linear \nlayer that transforms the final hidden states into logits for each token in the \nvocabulary. These logits are then passed through a softmax function to generate \na probability distribution over the vocabulary, which is used to predict the next \ntoken in a sequence. For classification tasks, the output head typically consists \nof a linear layer followed by a softmax function. The linear layer transforms the \nfinal hidden states of the model into logits for each class, and the softmax func -\ntion converts these logits into probabilities for each class. The specific architec -\nture of the output head can vary depending on the model and the task, but its \nprimary function is to map the processed input to the desired output format \n(e.g., class probabilities, token probabilities, etc.). \nFinally, you can print out the GPT-2XL model structure:\nprint(model)\nThe output is \nGPT2XL(\n  (transformer): ModuleDict(\n    (wte): Embedding(50257, 1600)\n    (wpe): Embedding(1024, 1600)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-47): 48 x Block(\n        (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n        (attn): CausalSelfAttention(\n          (c_attn): Linear(in_features=1600, out_features=4800, bias=True)\n          (c_proj): Linear(in_features=1600, out_features=1600, bias=True)\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n        (mlp): ModuleDict(\n          (c_fc): Linear(in_features=1600, out_features=6400, bias=True)\n          (c_proj): Linear(in_features=6400, out_features=1600, bias=True)\n          (act): GELU()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=1600, out_features=50257, bias=False)\n)\nIt shows the detailed blocks and layers in the GPT-2XL model.\nAnd just like that, you have created the GPT-2XL model from scratch!",12673
105-11.3 Loading up pretrained weights and generating text.pdf,105-11.3 Loading up pretrained weights and generating text,,0
106-11.3.1 Loading up pretrained parameters in GPT-2XL.pdf,106-11.3.1 Loading up pretrained parameters in GPT-2XL,"256 chapter  11 Building a generative pretrained Transformer from scratch \n11.3 Loading up pretrained weights and generating text\nEven though you have just created the GPT-2XL model, it is not trained. Therefore, \nyou cannot use it to generate any meaningful text.\nGiven the sheer number of the model’s parameters, it’s impossible to train the \nmodel without supercomputing facilities, let alone the amount of data needed to train \nthe model. Luckily, the pretrained weights of GPT-2 models, including the largest one,  \nGPT-2XL, were released by OpenAI to the public on November 5, 2019 (see the \nstatement on the OpenAI website, https: //openai.com/research/gpt-2-1-5b-release , \nas well as a report by an American technology news website, The Verge, https: //mng  \n.bz/NBm7 ). We, therefore, will load up the pretrained weights to generate text in this \nsection. \n11.3.1  Loading up pretrained parameters in GPT-2XL\nWe’ll use the transformers  library developed by the Hugging Face team to extract \npretrained weights in GPT-2XL. \nFirst, run the following line of code in a new cell in this Jupyter Notebook to install \nthe transformers  library on your computer:\n!pip install transformers\nNext, we import the GPT2 model from the transformers  library and extract the pre -\ntrained weights in GPT-2XL:\nfrom transformers import GPT2LMHeadModel\nmodel_hf = GPT2LMHeadModel.from_pretrained('gpt2-xl')    \nsd_hf = model_hf.state_dict()    \nprint(model_hf)    \nThe output from the preceding code block is\nGPT2LMHeadModel(\n  (transformer): GPT2Model(\n    (wte): Embedding(50257, 1600)\n    (wpe): Embedding(1024, 1600)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-47): 48 x GPT2Block(\n        (ln_1): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()    \n          (c_proj): Conv1D()    \n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()    \n          (c_proj): Conv1D()    \n          (act): NewGELUActivation()Loads the pretrained \nGPT-2XL model\nExtracts model weights\nPrints out the model structure of \nthe original OpenAI GTP-2XL model\nOpenAI used a \nConv1d layer \ninstead of a \nlinear layer as \nwe did.\nOpenAI used a Conv1d layer \ninstead of a linear layer as we did.\n 257 Loading up pretrained weights and generating text\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((1600,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=1600, out_features=50257, bias=False)\n)\nIf you compare this model structure with the one from the previous section, you’ll \nnotice that they are the same except that the linear layers are replaced with Conv1d \nlayers. As we explained in chapters 9 and 10, in feed-forward networks, we treat values \nin an input as independent elements rather than a sequence. Therefore, we often call \nit a 1D convolutional network. OpenAI checkpoints use a Conv1d module in places of \nthe model where we use a linear layer. As a result, we need to transpose certain weight \nmatrices when we extract model weights from Hugging Face and place them in our \nmodel. \nTo understand how this works, let’s look at the weights in the first layer of the feed-\nforward network in the first decoder block of the OpenAI GPT-2XL model. We can \nprint out its shape as follows:\nprint(model_hf.transformer.h[0].mlp.c_fc.weight.shape)\nThe output is\ntorch.Size([1600, 6400])\nThe weight matrix in the Conv1d layer is a tensor with size (1,600, 6,400).\nNow, if we look at the same weight matrix in the model we just constructed, its shape is\nprint(model.transformer.h[0].mlp.c_fc.weight.shape)\nThe output this time is\ntorch.Size([6400, 1600])\nThe weight matrix in the linear layer in our model is a tensor with size (6,400, 1,600), \nwhich is a transposed matrix of the weight matrix in OpenAI GPT-2XL. Therefore, \nwe need to transpose the weight matrix in all Conv1d layers in the OpenAI GPT-2XL \nmodel before we place them in our model. \nNext, we name the parameters in the original OpenAI GPT-2XL model as keys :\nkeys = [k for k in sd_hf if not k.endswith('attn.masked_bias')] \nNote that we have excluded parameters ending with attn.masked_bias  in the pre -\nceding line of code. OpenAI GPT-2 uses them to implement future token masking. \nSince we have created our own masking in the CausalSelfAttention()  class and \nregistered it as a buffer in PyTorch, we don’t need to load parameters ending with attn \n.masked_bias  from OpenAI. \nWe name the parameters in the GPT-2XL model we created from scratch as sd:\nsd=model.state_dict()",4878
107-11.3.2 Defining a generate function to produce text.pdf,107-11.3.2 Defining a generate function to produce text,"258 chapter  11 Building a generative pretrained Transformer from scratch \nNext, we’ll extract the pretrained weights in OpenAI GPT-2XL and place them in our \nown model:\ntransposed = ['attn.c_attn.weight', 'attn.c_proj.weight',\n              'mlp.c_fc.weight', 'mlp.c_proj.weight']    \nfor k in keys:\n    if any(k.endswith(w) for w in transposed):\n        with torch.no_grad():\n            sd[k].copy_(sd_hf[k].t())    \n    else:\n        with torch.no_grad():\n            sd[k].copy_(sd_hf[k])    \nWe extract the OpenAI pretrained weights from Hugging Face and place them in our \nown model. In the process, we make sure that we transpose the weight matrix when -\never OpenAI checkpoints use a Conv1d module instead of a plain linear module. \nNow our model is equipped with pre-trained weights from OpenAI. We can use the \nmodel to generate coherent text. \n11.3.2  Defining a generate() function to produce text\nArmed with pretrained weights from the OpenAI GPT-2XL model, we’ll use the GPT2 \nmodel we created from scratch to generate text.\nWhen generating text, we’ll feed a sequence of indexes that correspond to tokens in \na prompt to the model. The model predicts the index corresponding to the next token \nand attaches the prediction to the end of the sequence to form a new sequence. It then \nuses the new sequence to make predictions again. It keeps doing this until the model \nhas generated a fixed number of new tokens or the conversation is over (signified by \nthe special token <|endoftext|> ). \nThe special token <|endoftext|> in GPTs\nGPT models undergo training using text from a diverse range of sources. A unique token, \n<|endoftext|> , is employed during this phase to delineate text from different origins. \nIn the text generation phase, it’s crucial to halt the conversation upon encountering this \nspecial token. Failing to do so may trigger the initiation of an unrelated new topic, result -\ning in subsequent generated text that bears no relevance to the ongoing discussion. \nTo that end, we define a sample()  function to add a certain number of new indexes \nto the current sequence. It takes a sequence of indexes as input to feed to the  \nGPT-2XL model. It predicts one index at a time and adds the new index to the end \nof the running sequence. It stops until the specified number of time steps, max_new_\ntokens , is reached or when the predicted next token is <|endoftext|> , which signals \nthe end of the conversation. If we don’t stop, the model will randomly start an unre -\nlated topic. The sample()  function is defined as shown in the following listing.Finds out layers \nthat OpenAI uses a \nConv1d module \ninstead of a linear \nmodule\nFor those layers, we transpose \nthe weight matrix before \nplacing weights in our model.\nOtherwise, simply copies the \nweights from OpenAI and \nplaces them in our model\n 259 Loading up pretrained weights and generating text\nListing 11.7    Iteratively predicting the next index\nmodel.eval()\ndef sample(idx, max_new_tokens, temperature=1.0, top_k=None):\n    for _ in range(max_new_tokens):    \n        if idx.size(1) <= config.block_size:\n            idx_cond = idx  \n        else:\n            idx_cond = idx[:, -config.block_size:]\n        logits, _ = model(idx_cond)    \n        logits = logits[:, -1, :] / temperature\n        if top_k is not None:\n            v, _ = torch.topk(logits, top_k)\n            logits[logits < v[:, [-1]]] = -float('Inf')    \n        probs = F.softmax(logits, dim=-1)\n        idx_next = torch.multinomial(probs, num_samples=1)\n        if idx_next.item()==tokenizer.encoder.encoder['<|endoftext|>']:\n            break    \n        idx = torch.cat((idx, idx_next), dim=1)    \n    return idx\nThe sample()  function uses GPT-2XL to add new indexes to a running sequence. It \nincorporates two arguments, temperature  and top_k , to modulate the generated out -\nput’s novelty, operating in the same manner as described in chapter 8. The function \nreturns a new sequence of indexes. \nNext, we define a generate()  function to generate text based on a prompt. It first \nconverts the text in the prompt to a sequence of indexes. It then feeds the sequence to \nthe sample()  function we just defined to generate a new sequence of indexes. Finally, \nthe function generate()  converts the new sequence of indexes back to text.\nListing 11.8    A function to generate text with GPT-2XL\ndef generate(prompt, max_new_tokens, temperature=1.0,\n             top_k=None):\n    if prompt == '':\n        x=torch.tensor([[tokenizer.encoder.encoder['<|endoftext|>']]],\n                         dtype=torch.long)    \n    else:\n        x = tokenizer(prompt)    \n    y = sample(x, max_new_tokens, temperature, top_k)    \n    out = tokenizer.decode(y.squeeze())    \n    print(out)Generates a fixed \nnumber of new indexes\nPredicts the next index \nusing GPT-2XL\nIf using top-K \nsampling, sets the \nlogits below the top \nK choices to – ∞\nStops predicting if \nthe next token is \n<|endoftext|>\nAttaches the new prediction to \nthe end of the sequence\nIf the prompt is \nempty, uses \n<|endoftext|> \nas the prompt\nConverts prompt \ninto a sequence of \nindexes\nUses the sample() \nfunction to generate \nnew indexesConverts the new sequence \nof indexes back to text",5325
108-11.3.3 Text generation with GPT-2XL.pdf,108-11.3.3 Text generation with GPT-2XL,"260 chapter  11 Building a generative pretrained Transformer from scratch \nThe generate()  function bears resemblance to the version we introduced in chapter \n8 but with a notable distinction: it employs GPT-2XL for prediction purposes, moving \naway from the LSTM model previously utilized. The function accepts a prompt as its \ninitial input, transforming this prompt into a series of indexes that are then fed into \nthe model to forecast the subsequent index. Upon producing a predetermined num -\nber of new indexes, the function reverts the entire index sequence back into textual \nform.\n11.3.3  Text generation with GPT-2XL\nNow that we have defined the generate()  function, we can use it to generate text.\nIn particular, the generate()  function allows for unconditional text generation, \nwhich means the prompt is empty. The model will generate text randomly. This can be \nbeneficial in creative writing: the generated text can be used as inspiration or a starting \npoint for one’s own creative work. Let’s try that:\nprompt=""""\ntorch.manual_seed(42)\ngenerate(prompt, max_new_tokens=100, temperature=1.0,\n             top_k=None)\nThe output is\n<|endoftext|>Feedback from Ham Radio Recalls\nI discovered a tune sticking in my head -- I'd heard it mentioned on several \noccasions, but hadn't investigated further.\nThe tune sounded familiar to a tune I'd previously heard on the 550 micro. \nDuring that same time period I've heard other people's receipients drone on\nthe idea of the DSH-94013, notably Kim Weaver's instructions in her \nInterview on Radio Ham; and both Scott Mcystem and Steve Simmons' concepts.\nAs you can see, the preceding output is coherent and grammatically correct but may \nnot be factually accurate. I did a quick Google search, and the text doesn’t seem to be \ncopied from any online source.  \nExercise 11.4\nGenerate text unconditionally by setting the prompt as an empty string, temperature to \n0.9, maximum number of new tokens to 100, and top_k  to 40. Set the random seed \nnumber to 42 in PyTorch. See what the output is. \nTo evaluate whether GPT-2XL can produce coherent text based on preceding tokens, \nwe will use the prompt “I went to the kitchen and” and generate 10 additional tokens \nafter the prompt. We will repeat this process five times to determine if the generated \ntext aligns with typical kitchen activities:\n 261 Loading up pretrained weights and generating text\nprompt=""I went to the kitchen and""\nfor i in range(5):\n    torch.manual_seed(i)\n    generate(prompt, max_new_tokens=10, temperature=1.0,\n                 top_k=None)\nThe output is \nI went to the kitchen and said, you're not going to believe this.\nI went to the kitchen and noticed a female producer open a drawer in which was\nI went to the kitchen and asked who was going to be right there and A\nI went to the kitchen and took a small vial of bourbon and a little\nI went to the kitchen and found the bottle of wine, and poured it into\nThese results indicate that the generated text includes activities such as conversing \nwith someone, noticing something, and taking beverages, all of which are typical \nkitchen activities. This demonstrates that GPT-2XL can generate text relevant to the \ngiven context.\nNext, we use “Lexington is the second largest city in the state of Kentucky” as the \nprompt and ask the generate()  function to add up to 100 new tokens:\nprompt=""Lexington is the second largest city in the state of Kentucky""\ntorch.manual_seed(42)\ngenerate(prompt, max_new_tokens=100, temperature=1.0,\n             top_k=None)\nThe output is \nLexington is the second largest city in the state of Kentucky. It caters to\nthose who want to make everything in tune with being with friends and \nenjoying a jaunt through the down to Earth lifestyle. To do so, they are \nblessed with several venues large and small to fill their every need while \nresiding micro- cozy with nature within the landmarks of the city.\nIn a moment we look at ten up and coming suchache music acts from the \nLexington area to draw upon your attention.\nLyrikhop\nThis Lexington-based group\nAgain, this text is coherent. Even though the generated content may not be factually \naccurate. The GPT-2XL model is, fundamentally, trained to predict the next token \nbased on preceding tokens in the sentence. The preceding output shows that the \nmodel has achieved that goal: the generated text is grammatically correct and seem -\ningly logical. It shows the ability to remember the text in the early parts of the sequence \nand generate subsequent words that are relevant to the context. For example, while \nthe first sentence discusses the city of Lexington, about 90 tokens later, the model \nmentions the music acts from the Lexington area. \n262 chapter  11 Building a generative pretrained Transformer from scratch \nAdditionally, as noted in the introduction, GPT-2 has its limitations. It should not \nbe held to the same standard as ChatGPT or GPT-4, given that its size is less than 1% \nof ChatGPT and less than 0.1% of GPT-4. GPT-3 has 175 billion parameters and pro -\nduces more coherent text than GPT-2, but the pretrained weights are not released to \nthe public.  \nNext, we’ll explore how temperature  and top-K  sampling affect the generated text \nfrom GPT-2XL. We’ll set the temperature  to 0.9 and top_k  to 50 and keep other argu -\nments the same. Let’s see what the generated text looks like:\ntorch.manual_seed(42)\ngenerate(prompt, max_new_tokens=100, temperature=0.9,\n             top_k=50)  \nThe output is\nLexington is the second largest city in the state of Kentucky. It is also \nthe state capital. The population of Lexington was 1,731,947 in the 2011 \nCensus. The city is well-known for its many parks, including Arboretum, \nZoo, Aquarium and the Kentucky Science Center, as well as its restaurants, \nsuch as the famous Kentucky Derby Festival.\nIn the United States, there are at least 28 counties in this state with a\npopulation of more than 100,000, according to the 2010 census.\nThe generated text seems more coherent than before. However, the content is not \nfactually accurate. It made up many facts about the city of Lexington, Kentucky, such as \n“The population of Lexington was 1,731,947 in the 2011 Census.” \nExercise 11.5\nGenerate text by setting the temperature to 1.2 and top_k  to None and using “Lex -\nington is the second largest city in the state of Kentucky” as the starting prompt. Set the \nrandom seed number to 42 in PyTorch and the maximum number of new tokens to 100. \nIn this chapter, you have learned how to build GPT-2, the predecessor of ChatGPT \nand GPT-4, from scratch. After that, you extracted the pretrained weights from the  \nGPT-2XL model released by OpenAI and loaded them into your model. You witnessed \nthe coherent text generated by the model. \nDue to the large size of the GPT-2XL model (1.5 billion parameters), it’s impossible \nto train the model without supercomputing facilities. In the next chapter, you’ll cre -\nate a smaller version of a GPT model, with a similar structure as GPT-2 but only about \n5.12 million parameters. You’ll train the model with the text from Ernest Hemingway’s \nnovels. The trained model will generate coherent text with a style matching that of \nHemingway!\n 263 Summary\nSummary\n¡ GPT-2 is an advanced LLM developed by OpenAI and announced in February \n2019. It represents a significant milestone in the field of NLP and has paved the \nway for the development of even more sophisticated models, including its succes -\nsors, ChatGPT and GPT-4.  \n¡ GPT-2 is a decoder-only Transformer, meaning there is no encoder stack in the \nmodel. Like other Transformer models, GPT-2 uses self-attention mechanisms \nto process input data in parallel, significantly improving the efficiency and effec -\ntiveness of training LLMs. \n¡ GPT-2 adopts a different approach to positional encoding than the one used in \nthe seminal 2017 paper “Attention Is All You Need.” Instead, GPT-2’s technique \nfor positional encoding parallels that of word embeddings. \n¡ The GELU activation function is used in the feed-forward sublayers of GPT-2. \nGELU provides a blend of linear and nonlinear activation properties that have \nbeen found to enhance model performance in deep learning tasks, particularly \nin NLPs and in training LLMs. \n¡ We can build a GPT-2 model from scratch and load up the pretrained weights \nreleased by OpenAI. The GPT-2 model you created can generate coherent text \njust as the original OpenAI GPT-2 model does.",8594
109-12 Training a Transformer to generate text.pdf,109-12 Training a Transformer to generate text,"26412Training a Transformer to \ngenerate text\nThis chapter covers\n¡ Building a scaled-down version of the GPT-2XL  \n model tailored to your needs\n¡ Preparing data for training a GPT-style Transformer\n¡ Training a GPT-style Transformer from scratch\n¡ Generating text using the trained GPT model\nIn chapter 11, we developed the GPT-2XL model from scratch but were unable \nto train it due to its vast number of parameters. Training a model with 1.5 billion \nparameters requires supercomputing facilities and an enormous amount of data. \nConsequently, we loaded pretrained weights from OpenAI into our model and then \nused the GPT-2XL model to generate text.\nHowever, learning how to train a Transformer model from scratch is crucial \nfor several reasons. First, while this book doesn’t directly cover fine-tuning a pre -\ntrained model, understanding how to train a Transformer equips you with the skills \nneeded for fine-tuning. Training a model involves initializing parameters randomly, \nwhereas fine-tuning involves loading pretrained weights and further training the \nmodel. Second, training or fine-tuning a Transformer enables you to customize the \nmodel to meet your specific needs and domain, which can significantly enhance its \n 265  \nperformance and relevance for your use case. Finally, training your own Transformer or \nfine-tuning an existing one provides greater control over data and privacy, which is par -\nticularly important for sensitive applications or handling proprietary data. In summary, \nmastering the training and fine-tuning of Transformers is essential for anyone looking \nto harness the power of language models for specific applications while maintaining \nprivacy and control. \nTherefore, in this chapter, we’ll construct a scaled-down version of the GPT model \nwith approximately 5 million parameters. This smaller model follows the architecture \nof the GPT-2XL model; the significant differences are its composition of only 3 decoder \nblocks and an embedding dimension of 256, compared to the original GPT-2XL’s 48 \ndecoder blocks and an embedding dimension of 1,600. By scaling down the GPT model \nto about 5 million parameters, we can train it on a regular computer. \nThe generated text’s style will depend on the training data. When training a model \nfrom scratch for text generation, both text length and variation are crucial. The train -\ning material must be extensive enough for the model to learn and mimic a particular \nwriting style effectively. At the same time, if the training material lacks variation, the \nmodel may simply replicate passages from the training text. On the other hand, if the \nmaterial is too long, training may require excessive computational resources. There -\nfore, we will use three novels by Ernest Hemingway as our training material: The Old \nMan and the Sea , A Farewell to Arms , and For Whom the Bell Tolls . This selection ensures that \nour training data has sufficient length and variation for effective learning without being \nso long that training becomes impractical.\nSince GPT models cannot process raw text directly, we will first tokenize the text into \nwords. We will then create a dictionary to map each unique token to a different index. \nUsing this dictionary, we will convert the text into a long sequence of integers, ready for \ninput into a neural network.\nWe will use sequences of 128 indexes as input to train the GPT model. As in chapters \n8 and 10, we will shift the input sequence by one token to the right and use it as the out -\nput. This approach forces the model to predict the next word in a sentence based on \nthe current token and all previous tokens in the sequence.\nA key challenge is determining the optimal number of epochs for training the \nmodel. Our goal is not merely to minimize the cross-entropy loss in the training set, as \ndoing so could lead to overfitting, where the model simply replicates passages from the \ntraining text. To tackle this problem, we plan to train the model for 40 epochs. We will \nsave the model at 10-epoch intervals and evaluate which version can generate coher -\nent text without merely copying passages from the training material. Alternatively, one \ncould potentially use a validation set to assess the performance of the model and decide \nwhen to stop training, as we did in chapter 2. \nOnce our GPT model is trained, we will use it to generate text autoregressively, as we \ndid in chapter 11. We’ll test different versions of the trained model. The model trained \nfor 40 epochs produces very coherent text, capturing Hemingway’s distinctive style. \nHowever, it may also generate text partly copied from the training material, especially if chapter  12 Training a Transformer to generate text",4791
110-12.1.1 The architecture of a GPT to generate text.pdf,110-12.1.1 The architecture of a GPT to generate text,"266 chapter  12 Training a Transformer to generate text\nthe prompt is similar to passages in the training text. The model trained for 20 epochs \nalso generates coherent text, albeit with occasional grammatical errors, but is less likely \nto directly copy from the training text.\nThe primary goal of this chapter is not necessarily to generate the most coherent text \npossible, which presents significant challenges. Instead, our objective is to teach you \nhow to build a GPT-style model from scratch, tailored to real-world applications and \nyour specific needs. More importantly, this chapter outlines the steps involved in train -\ning a GPT model from scratch. You will learn how to select training text based on your \nobjectives, tokenize the text and convert it to indexes, and prepare batches of training \ndata. You will also learn how to determine the number of epochs for training. Once the \nmodel is trained, you will learn how to generate text using the model and how to avoid \ngenerating text directly copied from the training material.\n12.1 Building and training a GPT from scratch\nOur objective is to master building and training a GPT model from scratch, tailored to \nspecific tasks. This skill is crucial for applying the concepts in this book to real-world \nproblems.\nImagine you are an avid fan of Ernest Hemingway’s work and wish to train a GPT \nmodel to generate text in Hemingway’s style. How would you approach this? This sec -\ntion discusses the steps involved in this task. \nThe first step is to configure a GPT model suitable for training. You’ll create a GPT \nmodel with a structure similar to the GPT-2 model you built in chapter 11 but with \nsignificantly fewer parameters to make training feasible in just a few hours. As a result, \nyou’ll need to determine key hyperparameters of the model, such as sequence length, \nembedding dimension, number of decoder blocks, and dropout rates. These hyperpa -\nrameters are crucial as they influence both the quality of the output from the trained \nmodel and the speed of training.\nFollowing that, you will gather the raw text of several Hemingway novels and clean it \nup to ensure it is suitable for training. You will tokenize the text and assign a different \ninteger to each unique token so that you can feed it to the model. To prepare the train -\ning data, you will break down the text into sequences of integers of a certain length and \nuse them as inputs. You will then shift the inputs one token to the right and use them as \noutputs. This approach forces the model to predict the next token based on the current \ntoken and all previous tokens in the sequence.\nOnce the model is trained, you will use it to generate text based on a prompt. You will \nfirst convert the text in the prompt to a sequence of indexes and feed it to the trained \nmodel. The model uses the sequence to predict the most likely next token iteratively. \nAfter that, you will convert the sequence of tokens generated by the model back to text.\nIn this section, we will first discuss the architecture of the GPT model for the task. \nAfter that, we will discuss the steps involved in training the model.\n 267 Building and training a GPT from scratch\n12.1.1  The architecture of a GPT to generate text\nAlthough GPT-2 is available in various sizes, they all share a similar architecture. The \nGPT model we construct in this chapter follows the same structural design as GPT-2 \nbut is significantly smaller, making it feasible to train without the need for supercom -\nputing facilities. Table 12.1 presents a comparison between our GPT model and the \nfour versions of the GPT-2 models. \nTable 12.1    A comparison of our GPT with different versions of GPT-2 models\nGPT-2S GPT-2M GPT-2L GPT-2XL Our GPT\nEmbedding dimension 768 1,024 1,280 1,600 256\nNumber of decoder layers 12 24 36 48 3\nNumber of heads 12 16 20 25 4\nSequence length 1,024 1,024 1,024 1,024 128\nVocabulary size 50,257 50,257 50,257 50,257 10,600\nNumber of parameters 124 million 350 million 774 million 1,558 million 5.12 million\nIn this chapter, we’ll construct a GPT model with three decoder layers and an embed -\nding dimension of 256 (meaning each token is represented by a 256-value vector after \nword embedding). As we mentioned in chapter 11, GPT models use a different posi -\ntional encoding method than the one used in the 2017 paper “Attention Is All You \nNeed.” Instead, we use embedding layers to learn the positional encodings for differ -\nent positions in a sequence. As a result, each position in a sequence is also represented \nby a 256-value vector. For calculating causal self-attention, we use four parallel atten -\ntion heads to capture different aspects of the meanings of a token in the sequence. \nThus, each attention head has a dimension of 256/4 = 64, similar to that in GPT-2 mod -\nels. For example, in GPT-2XL, each attention head has a dimension of 1,600/25 = 64.\nThe maximum sequence length in our GPT model is 128, which is much shorter \nthan the maximum sequence length of 1,024 in GPT-2 models. This reduction is neces -\nsary to keep the number of parameters in the model manageable. However, even with \n128 elements in a sequence, the model can learn the relationship between tokens in a \nsequence and generate coherent text.\nWhile GPT-2 models have a vocabulary size of 50,257, our model has a much smaller \nvocabulary size of 10,600. It’s important to note that the vocabulary size is mainly deter -\nmined by the training data, rather than being a predefined choice. If you choose to use \nmore text for training, you may end up with a larger vocabulary.\nFigure 12.1 illustrates the architecture of the decoder-only Transformer we will cre -\nate in this chapter. It is similar to the architecture of GPT-2 that you have seen in chap -\nter 11, except that it is smaller in size. As a result, the total number of parameters in our \nmodel is 5.12 million, compared to the 1.558 billion in the GPT-2XL model that we built \nin chapter 11. Figure 12.1 shows the size of the training data at each step of training.\n268 chapter  12 Training a Transformer to generate text\nAdd & normalize\nFeed forward\nSize (256, 256)Three identical \ndecoder layers Second sublayer of each\ndecoder layer:  \nFeed forward with add & \nnorm \nAdd & normalize\nCausal self-attention\nSize (256, 256)First sublayer of each \ndecoder layer:\nCausal self-attention\nwith add & norm \nWord embedding layer \nSize (256, 256) \nToken representation \nSize (32, 128, 256)NormalizeLinear head\nSize (256, 10,600)Output\nSize (32, 128, 10,600)\nPositional encoding layer \nSize (128, 256) \nPositional representation \nSize (32, 128, 128)Word embedding\nSize (32, 128, 256)Positional encoding\nSize (32, 128, 256)Size (32, 128, 256) \nFigure 12.1    The architecture of a decoder-only Transformer, designed to generate text. The text from \nthree Hemingway novels is tokenized and then converted into indexes. We arrange 128 indexes into \na sequence, and each batch contains 32 such sequences. The input first undergoes word embedding \nand positional encoding, with the input embedding being the sum of these two components. This input \nembedding is then processed through three decoder layers. Following this, the output undergoes layer \nnormalization and passes through a linear layer, resulting in an output size of 10,600, which corresponds \nto the number of unique tokens in the vocabulary. \nThe input to the GPT model we create consists of input embeddings, which are illus -\ntrated at the bottom of figure 12.1. We will discuss how to calculate these embeddings \nin detail in the next subsection. Briefly, they are the sum of word embeddings and posi -\ntional encodings from the input sequence.\nThe input embedding is then passed sequentially through three decoder layers. \nSimilar to the GPT-2XL model we built in chapter 11, each decoder layer consists of \ntwo sublayers: a causal self-attention layer and a feed-forward network. Additionally, \nwe apply layer normalization and residual connections to each sublayer. After this, the \noutput goes through a layer normalization and a linear layer. The number of outputs in \nour GPT model corresponds to the number of unique tokens in the vocabulary, which is \n10,600. The output of the model is the logits for the next token. Later, we will apply the",8410
111-12.2.1 Tokenizing the text.pdf,111-12.2.1 Tokenizing the text,"269 Building and training a GPT from scratch\nsoftmax function to these logits to obtain the probability distribution over the vocabu -\nlary. The model is designed to predict the next token based on the current token and all \nprevious tokens in the sequence.\n12.1.2  The training process of the GPT model to generate text\nNow that we know how to construct the GPT model for text generation, let’s explore \nthe steps involved in training the model. We aim to provide an overview of the training \nprocess before diving into the coding aspect of the project.\nThe style of the generated text is influenced by the training text. Since our objective \nis to train the model to generate text in the style of Ernest Hemingway, we’ll use the text \nfrom three of his novels: The Old Man and the Sea , A Farewell to Arms , and For Whom the \nBell Tolls . If we were to choose just one novel, the training data would lack variety, lead -\ning the model to memorize passages from the novel and generate text identical to the \ntraining data. Conversely, using too many novels would increase the number of unique \ntokens, making it challenging to train the model effectively in a short amount of time. \nTherefore, we strike a balance by selecting three novels and combining them as our \ntraining data.\nFigure 12.2 illustrates the steps involved in training the GPT model to generate text. \nShift 1 token\nto the right Input, e.g., indexes \nfor “the old man\nand the”Output, e.g.,\nindexes for “old\nman and the sea”\nGPT\nModelIndex for the next\ntokenCross-entropy loss Step 3\nStep 4\nStep 5Step 6 Step 7\nPredictFeedbackRaw text\n[0, 17, 16, 2, 0, 102, ...]Tokenize & indexStep 1\nCreate inputStep 2\nGround truth\nFigure 12.2    The training process for a decoder-only Transformer to generate text, Hemingway-style. \nAs in the previous three chapters, the first step in the training process is to convert text \ninto a numerical form so that we can feed the training data to the model. Specifically, \nwe first break down the text of the three novels into tokens using word-level tokeniza -\ntion, as we did in chapter 8. In this case, each token is a whole word or a punctuation \nmark (such as a colon, a parenthesis, or a comma). Word-level tokenization is easy to \nimplement, and we can control the number of unique tokens. After tokenization, we \nassign a unique index (i.e., an integer) to each token, converting the training text into \na sequence of integers (see step 1 in figure 12.2).\n270 chapter  12 Training a Transformer to generate text\nNext, we transform the sequence of integers into training data by first dividing this \nsequence into sequences of equal length (step 2 in figure 12.2). We allow a maximum \nlength of 128 indexes in each sequence. The choice of 128 allows us to capture long-\nrange dependencies among tokens in a sentence while keeping the model size man -\nageable. However, the number 128 is not magical: changing the number to, say, 100 or \n150 will lead to similar results. These sequences form the features (the x variable) of \nour model. As we did in previous chapters, we shift the input sequence one token to the \nright and use it as the output in the training data (the y variable; step 3 in figure 12.2). \nThe pairs of input and output serve as the training data (x, y). In the example of the \nsentence “the old man and the sea,” we use indexes corresponding to “the old man and \nthe” as the input x. We shift the input one token to the right and use the indexes for “old \nman and the sea” as the output y. In the first time step, the model uses “the” to predict \n“old.” In the second time step, the model uses “the old” to predict “man,” and so on.\nDuring training, you will iterate through the training data. In the forward passes, \nyou feed the input sequence x through the GPT model (step 4). The GPT then makes \na prediction based on the current parameters in the model (step 5). You compute the \ncross-entropy loss by comparing the predicted next tokens with the output obtained \nfrom step 3. In other words, you compare the model’s prediction with the ground truth \n(step 6). Finally, you will adjust the parameters in the GPT model so that in the next \niteration, the model’s predictions move closer to the actual output, minimizing the \ncross-entropy loss (step 7). Note that the model is essentially performing a multicate -\ngory classification problem: it’s predicting the next token from all unique tokens in the \nvocabulary.\nYou will repeat steps 3 to 7 through many iterations. After each iteration, the model \nparameters are adjusted to improve the prediction of the next token. We will repeat this \nprocess for 40 epochs and save the trained model after every 10 epochs. As you will see \nlater, if we train the model for too long, it becomes overfit, memorizing passages from \nthe training data. The generated text then becomes identical to those in the original \nnovels. We will test ex post which version of the model generates coherent text and, at \nthe same time, does not simply copy from the training data.\n12.2 Tokenizing text of Hemingway novels\nNow that you understand the architecture of the GPT model and the training process, \nlet’s begin with the first step: tokenizing and indexing the text of Hemingway’s novels.\nFirst, we’ll process the text data to prepare it for training. We’ll break down the \ntext into individual tokens, as we did in chapter 8. Since deep neural networks cannot \ndirectly process raw text, we’ll create a dictionary that assigns an index to each token, \neffectively mapping them to integers. After that, we’ll organize these indexes into \nbatches of training data, which will be crucial for training the GPT model in the subse -\nquent steps.\nWe’ll use word-level tokenization for its simplicity in dividing text into words, \nas opposed to the more complex subword tokenization that requires a nuanced \n 271 Tokenizing text of Hemingway novels\nunderstanding of linguistic structure. Additionally, word-level tokenization results in a \nsmaller number of unique tokens than subword tokenization, reducing the number of \nparameters in the GPT model. \n12.2.1  Tokenizing the text\nTo train the GPT model, we’ll use the raw text files of three novels by Ernest Heming -\nway: The Old Man and the Sea , A Farewell to Arms , and For Whom the Bell Tolls . The text \nfiles are downloaded from the Faded Page website: https: //www.fadedpage.com . I have \ncleaned up the text by removing the top and bottom paragraphs that are not part of \nthe original book. When preparing your own training text, it’s crucial to eliminate all \nirrelevant information, such as vendor details, formatting, and license information. \nThis ensures that the model focuses solely on learning the writing style present in the \ntext. I have also removed the text between chapters that are not relevant to the main \ntext. You can download the three files OldManAndSea.txt, FarewellToArms.txt, and \nToWhomTheBellTolls.txt from the book’s GitHub repository: https: //github.com/\nmarkhliu/DGAI . Place them in the /files/ folder on your computer. \nIn the text file for The Old Man and the Sea , both the opening double quote (“) and \nthe closing double quote (”) are represented by straight double quotes ( ""). This is not \nthe case in the text files for the other two novels. Therefore, we load up the text for The \nOld Man and the Sea and change straight quotes to either an opening quote or a closing \nquote. Doing so allows us to differentiate between the opening and closing quotes. This \nwill also aid in formatting the generated text later on: we’ll remove the space after the \nopening quote and the space before the closing quote. This step is implemented as \nshown in the following listing.\nListing 12.1    Changing straight quotes to opening and closing quotes\nwith open(""files/OldManAndSea.txt"",""r"", encoding='utf-8-sig') as f:\n    text=f.read()\ntext=list(text)    \nfor i in range(len(text)):\n    if text[i]=='""':\n        if text[i+1]==' ' or text[i+1]=='\n':\n            text[i]='""'    \n        if text[i+1]!=' ' and text[i+1]!='\n':\n            text[i]='""'    \n    if text[i]==""'"":\n        if text[i-1]!=' ' and text[i-1]!='\n':\n            text[i]='''    \ntext="""".join(text)    Loads up the raw text and breaks \nit into individual characters\nIf a straight double quote is \nfollowed by a space or a line break, \nchanges it to a closing quote\nOtherwise, changes it \nto an opening quote\nConverts a straight single \nquote to an apostrophe\nJoins individual characters \nback to text\n272 chapter  12 Training a Transformer to generate text\nIf a double quote is followed by a space or a line break, we’ll change it to a closing \nquote; otherwise, we’ll change it to an opening quote. The apostrophe was entered as a \nsingle straight quote, and we have changed it to an apostrophe in the form of a closing \nsingle quote in listing 12.1.\nNext, we load the text for the other two novels and combine the three novels into \none single file.\nListing 12.2    Combining the text from three novels\nwith open(""files/ToWhomTheBellTolls.txt"",""r"", encoding='utf-8-sig') as f:\n    text1=f.read()    \nwith open(""files/FarewellToArms.txt"",""r"", encoding='utf-8-sig') as f:\n    text2=f.read()    \ntext=text+"" ""+text1+"" ""+text2    \nwith open(""files/ThreeNovels.txt"",""w"", \n          encoding='utf-8-sig') as f:\n    f.write(text)    \nprint(text[:250])\nWe load the text from the other two novels, A Farewell to Arms  and For Whom the Bell Tolls . \nWe then combine the text from all three novels to use as our training data. Addition -\nally, we save the combined text in a local file named ThreeNovels.txt so that we can \nlater verify if the generated text is directly copied from the original text.\nThe output from the preceding code listing is\nHe was an old man who fished alone in a skiff in the Gulf Stream and he\nhad gone eighty-four days now without taking a fish. In the first\nforty days a boy had been with him. But after forty days without a\nfish the boy's parents had told him that th\nThe output is the first 250 characters in the combined text. \nWe’ll tokenize the text by using a space as the delimiter. As seen in the preceding \noutput, punctuation marks such as periods (.), hyphens (-), and apostrophes (’) are \nattached to the preceding words without a space. Therefore, we need to insert a space \naround all punctuation marks.\nAdditionally, we’ll convert line breaks (\n) into spaces so that they are not included \nin the vocabulary. Converting all words to lowercase is also beneficial in our setting, as \nit ensures that words like “The” and “the” are recognized as the same token. This step \nhelps reduce the number of unique tokens, thereby making the training process more \nefficient. To address these problems, we’ll clean up the text as shown in the following \nlisting.Reads the text from \nthe second novel\nReads the text from \nthe third novel\nCombines the text \nfrom the three novels\nSaves the combined \ntext in the local folder\n 273 Tokenizing text of Hemingway novels\nListing 12.3    Adding spaces around punctuation marks\ntext=text.lower().replace(""\n"", "" "")    \nchars=set(text.lower())\npunctuations=[i for i in chars if i.isalpha()==False\n              and i.isdigit()==False]    \nprint(punctuations)\nfor x in punctuations:\n    text=text.replace(f""{x}"", f"" {x} "")    \ntext_tokenized=text.split()\nunique_tokens=set(text_tokenized)\nprint(len(unique_tokens))    \nWe use the set()  method to obtain all unique characters in the text. We then use the \nisalpha()  and isdigit()  methods to identify and remove letters and numbers from \nthe set of unique characters, leaving us with only punctuation marks.\nIf you execute the preceding code block, the output will be as follows:\n[')', '.', '&', ':', '(', ';', '-', '!', '""', ' ', ''', '""', '?', ',', ''']\n10599\nThis list includes all punctuation marks in the text. We add spaces around them and \nbreak the text into individual tokens using the split()  method. The output indicates \nthat there are 10,599 unique tokens in the text from the three novels by Hemingway, a \nsize that’s much smaller than the 50,257 tokens in GPT-2. This will significantly reduce \nthe model size and training time.\nAdditionally, we’ll add one more token ""UNK""  to represent unknown tokens. This is \nuseful in case we encounter a prompt with unknown tokens, allowing us to convert them \nto an index to feed to the model. Otherwise, we can only use a prompt with the preced -\ning 10,599 tokens. Suppose you include the word “technology” in the prompt. Since \n“technology” is not one of the tokens in the dictionary word_to_int , the program will \ncrash. By including the ""UNK""  token, you can prevent the program from crashing in \nsuch scenarios. When you train your own GPT, you should always include the ""UNK""  \ntoken since it’s impossible to include all tokens in your vocabulary. To that end, we add \n""UNK""  to the list of unique tokens and map them to indexes.\nListing 12.4    Mapping tokens to indexes\nfrom collections import Counter   \nword_counts=Counter(text_tokenized)    \nwords=sorted(word_counts, key=word_counts.get,\n                      reverse=True)     \nwords.append(""UNK"")    \ntext_length=len(text_tokenized)Replaces line breaks \nwith spaces\nIdentifies all \npunctuation marks\nInserts spaces around \npunctuation marks\nCounts the number \nof unique tokens\nAdds “UNK” to the list \nof unique tokens",13586
112-12.2.2 Creating batches for training.pdf,112-12.2.2 Creating batches for training,"274 chapter  12 Training a Transformer to generate text\nntokens=len(words)    \nprint(f""the text contains {text_length} words"")\nprint(f""there are {ntokens} unique tokens"")  \nword_to_int={v:k for k,v in enumerate(words)}    \nint_to_word={v:k for k,v in word_to_int.items()}    \nprint({k:v for k,v in word_to_int.items() if k in words[:10]})\nprint({k:v for k,v in int_to_word.items() if v in words[:10]})\nThe output from the preceding code block is\nthe text contains 698207 words\nthere are 10600 unique tokens\n{'.': 0, 'the': 1, ',': 2, '""': 3, '""': 4, 'and': 5, 'i': 6, 'to': 7, 'he': \n8, 'it': 9}\n{0: '.', 1: 'the', 2: ',', 3: '""', 4: '""', 5: 'and', 6: 'i', 7: 'to', 8: \n'he', 9: 'it'}\nThe text from the three novels contains 698,207 tokens. After including ""UNK""  in the \nvocabulary, the total number of unique  tokens is now 10,600. The dictionary word_to_\nint assigns a different index to each unique token. For example, the most frequent \ntoken, the period (.), is assigned an index of 0, and the word “the” is assigned an index \nof 1. The dictionary int_to_word  translates an index back to a token. For example, \nindex 3 is translated back to the opening quote (“), and index 4 is translated back to \nthe closing quote (”).\nWe print out the first 20 tokens in the text and their corresponding indexes:\nprint(text_tokenized[0:20])\nwordidx=[word_to_int[w] for w in text_tokenized]  \nprint([word_to_int[w] for w in text_tokenized[0:20]])\nThe output is\n['he', 'was', 'an', 'old', 'man', 'who', 'fished', 'alone', 'in', 'a', \n'skiff', 'in', 'the', 'gulf', 'stream', 'and', 'he', 'had', 'gone',\n 'eighty']\n[8, 16, 98, 110, 67, 85, 6052, 314, 14, 11, 1039, 14, 1, 3193, 507, 5, 8,\n25, 223, 3125] \nNext, we’ll break the indexes into sequences of equal length to use as training data.\n12.2.2  Creating batches for training\nWe’ll use a sequence of 128 tokens as the input to the model. We then shift the \nsequence one token to the right and use it as the output. \nSpecifically, we create pairs of (x, y) for training purposes. Each x is a sequence with \n128 indexes. We choose 128 to strike a balance between training speed and the mod -\nel’s ability to capture long-range dependencies. Setting the number too high may slow \ndown training, while setting it too low may prevent the model from capturing long-\nrange dependencies effectively.Counts the size of the vocabulary, \nntokens, which will be a \nhyperparamter in our model\nMaps tokens to indexes\nMaps indexes \nto tokens\n 275 Tokenizing text of Hemingway novels\nOnce we have the sequence x, we slide the sequence window to the right by one \ntoken and use it as the target y. Shifting the sequence by one token to the right and \nusing it as the output during sequence generation is a common technique in training \nlanguage models, including GPTs. We have done this in chapters 8 to 10. The following \ncode block creates the training data:\nimport torch\nseq_len=128    \nxys=[]\nfor n in range(0, len(wordidx)-seq_len-1):\n    x = wordidx[n:n+seq_len]    \n    y = wordidx[n+1:n+seq_len+1]    \n    xys.append((torch.tensor(x),(torch.tensor(y))))    \nWe have created a list xys to contain pairs of (x, y) as our training data. As we did in \nprevious chapters, we organize the training data into batches to stabilize training. We \nchoose a batch size of 32:\nfrom torch.utils.data import DataLoader\ntorch.manual_seed(42)\nbatch_size=32\nloader = DataLoader(xys, batch_size=batch_size, shuffle=True)\nx,y=next(iter(loader))\nprint(x)\nprint(y)\nprint(x.shape,y.shape)\nWe print out a pair of x and y as an example. The output is\ntensor([[   3,  129,    9,  ...,   11,  251,   10],\n        [   5,   41,   32,  ...,  995,   52,   23],\n        [   6,   25,   11,  ...,   15,    0,   24],\n        ...,\n        [1254,    0,    4,  ...,   15,    0,    3],\n        [  17,    8, 1388,  ...,    0,    8,   16],\n        [  55,   20,  156,  ...,   74,   76,   12]])\ntensor([[ 129,    9,   23,  ...,  251,   10,    1],\n        [  41,   32,   34,  ...,   52,   23,    1],\n        [  25,   11,   59,  ...,    0,   24,   25],\n        ...,\n        [   0,    4,    3,  ...,    0,    3,   93],\n        [   8, 1388,    1,  ...,    8,   16, 1437],\n        [  20,  156,  970,  ...,   76,   12,   29]])\ntorch.Size([32, 128]) torch.Size([32, 128])\nEach x and y have a shape of (32, 128). This means that in each batch of training \ndata, there are 32 pairs of sequences, with each sequence containing 128 indexes. \nWhen an index is passed through the nn.Embedding()  layer, PyTorch looks up the Sets the sequence \nlength to 128 indexes\nThe input sequence x \ncontains 128 consecutive \nindexes in the training text.\nShifts x one position \nto the right and uses \nit as output y\nAdds the pair (x, y) to \nthe training data.",4822
113-12.3 Building a GPT to generate text.pdf,113-12.3 Building a GPT to generate text,,0
114-12.3.3 Building the GPT model.pdf,114-12.3.3 Building the GPT model,"276 chapter  12 Training a Transformer to generate text\ncorresponding row in the embedding matrix and returns the embedding vector for \nthat index, avoiding the need to create potentially very large one-hot vectors. There -\nfore, when x is passed through the word embedding layer, it’s as if x is first converted to \na one-hot tensor with a dimension of (32, 128, 256). Similarly, when x is passed through \nthe positional encoding layer (which is implemented by the nn.Embedding()  layer), \nit’s as if x is first converted to a one-hot tensor with a dimension of (32, 128, 128). \n12.3 Building a GPT to generate text\nNow that we have the training data ready, we’ll create a GPT model from scratch to \ngenerate text. The model we’ll build has a similar architecture as the GPT-2XL model \nwe built in chapter 11. However, instead of having 48 decoder layers, we’ll use only 3 \ndecoder layers. The embedding dimensions and the vocabulary size are both much \nsmaller, as I have explained earlier in this chapter. As a result, our GPT model will have \nfar fewer parameters than GPT-2XL.\nWe’ll follow the same steps as those in chapter 11. Along the way, we’ll highlight the \ndifferences between our GPT model and GPT-2XL and explain the reasons for these \nmodifications.\n12.3.1  Model hyperparameters\nThe feed-forward network in the decoder block uses the Gaussian error linear unit \n(GELU) activation function. GELU has been shown to enhance model performance \nin deep learning tasks, particularly in natural language processing. This has become a \nstandard practice in GPT models. Therefore, we define a GELU class as follows, as we \ndid in Chapter 11:\nimport torch\nfrom torch import nn\nimport math\ndevice=""cuda"" if torch.cuda.is_available() else ""cpu""\nclass GELU(nn.Module):\n    def forward(self, x):\n        return 0.5*x*(1.0+torch.tanh(math.sqrt(2.0/math.pi)*\\n                       (x + 0.044715 * torch.pow(x, 3.0))))\nIn chapter 11, we didn’t use a GPU even during the text generation stage, as the model \nwas simply too large and a regular GPU would run out of memory if we loaded the \nmodel onto it.\nIn this chapter, however, our model is significantly smaller. We’ll move the model to \nthe GPU for faster training. We’ll also generate text using the model on the GPU.\nWe use a Config()  class to include all the hyperparameters used in the model:\nclass Config():\n    def __init__(self):\n        self.n_layer = 3\n        self.n_head = 4\n        self.n_embd = 256\n        self.vocab_size = ntokens\n 277 Building a GPT to generate text\n        self.block_size = 128 \n        self.embd_pdrop = 0.1\n        self.resid_pdrop = 0.1\n        self.attn_pdrop = 0.1\nconfig=Config()\nThe attributes in the Config()  class are used as hyperparameters in our GPT model. \nWe set the n_layer  attribute to 3, indicating our GPT model has three decoder layers. \nThe n_head  attribute is set to 4, meaning we’ll split the query Q, key K, and value V \nvectors into 4 parallel heads when calculating causal self-attention. The n_embd  attri -\nbute is set to 256, meaning the embedding dimension is 256: each token will be repre -\nsented by a 256-value vector. The vocab_size  attribute is determined by the number \nof unique tokens in the vocabulary. As explained in the last section, there are 10,600 \nunique tokens in our training text. The block_size  attribute is set to 128, meaning \nthe input sequence contains a maximum of 128 tokens. We set the dropout rates to 0.1, \nas we did in chapter 11. \n12.3.2  Modeling the causal self-attention mechanism\nThe causal self-attention is defined in the same way as in chapter 11:\nimport torch.nn.functional as F\nclass CausalSelfAttention(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n        self.attn_dropout = nn.Dropout(config.attn_pdrop)\n        self.resid_dropout = nn.Dropout(config.resid_pdrop)\n        self.register_buffer(""bias"", torch.tril(torch.ones(\\n                   config.block_size, config.block_size))\n             .view(1, 1, config.block_size, config.block_size))\n        self.n_head = config.n_head\n        self.n_embd = config.n_embd\n    def forward(self, x):\n        B, T, C = x.size() \n        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)\n        hs = C // self.n_head\n        k = k.view(B, T, self.n_head, hs).transpose(1, 2) \n        q = q.view(B, T, self.n_head, hs).transpose(1, 2) \n        v = v.view(B, T, self.n_head, hs).transpose(1, 2) \n        att = (q @ k.transpose(-2, -1)) *\\n            (1.0 / math.sqrt(k.size(-1)))\n        att = att.masked_fill(self.bias[:,:,:T,:T] == 0, \\n                              float(‚-inf'))\n        att = F.softmax(att, dim=-1)\n        att = self.attn_dropout(att)\n        y = att @ v \n        y = y.transpose(1, 2).contiguous().view(B, T, C)\n        y = self.resid_dropout(self.c_proj(y))\n        return y\n278 chapter  12 Training a Transformer to generate text\nWhen calculating causal self-attention, the input embedding is passed through three \nneural networks to obtain the query Q, key K, and value V. We then split each of them \ninto four parallel heads and calculate masked self-attention within each head. After \nthat, we concatenate the four attention vectors back into a single attention vector, \nwhich is then used as the output of the CausalSelfAttention()  class. \n12.3.3  Building the GPT model\nWe combine a feed-forward network with the causal self-attention sublayer to form a \ndecoder block. The feed-forward network injects nonlinearity into the model. With -\nout it, the Transformer would simply be a series of linear operations, constraining its \ncapacity to capture complex data relationships. Moreover, the feed-forward network \nprocesses each position independently and uniformly, enabling the transformation \nof features identified by the self-attention mechanism. This facilitates the capture of \ndiverse aspects of the input data, thereby augmenting the model’s ability to represent \ninformation. A decoder block is defined as follows:\nclass Block(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.ln_1 = nn.LayerNorm(config.n_embd)\n        self.attn = CausalSelfAttention(config)\n        self.ln_2 = nn.LayerNorm(config.n_embd)\n        self.mlp = nn.ModuleDict(dict(\n            c_fc   = nn.Linear(config.n_embd, 4 * config.n_embd),\n            c_proj = nn.Linear(4 * config.n_embd, config.n_embd),\n            act    = GELU(),\n            dropout = nn.Dropout(config.resid_pdrop),\n        ))\n        m = self.mlp\n        self.mlpf=lambda x:m.dropout(m.c_proj(m.act(m.c_fc(x)))) \n    def forward(self, x):\n        x = x + self.attn(self.ln_1(x))\n        x = x + self.mlpf(self.ln_2(x))\n        return x\nEach decoder block in our GPT model consists of two sublayers: a causal self-attention \nsublayer and a feed-forward network. We apply layer normalization and a residual con -\nnection to each sublayer for improved stability and performance. We then stack three \ndecoder layers on top of each other to form the main body of our GPT model.\nListing 12.5    Building a GPT model\nclass Model(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.block_size = config.block_size\n        self.transformer = nn.ModuleDict(dict(\n            wte = nn.Embedding(config.vocab_size, config.n_embd),\n            wpe = nn.Embedding(config.block_size, config.n_embd),\n 279 Building a GPT to generate text\n            drop = nn.Dropout(config.embd_pdrop),\n            h = nn.ModuleList([Block(config) \n                               for _ in range(config.n_layer)]),   \n            ln_f = nn.LayerNorm(config.n_embd),))\n        self.lm_head = nn.Linear(config.n_embd,\n                                 config.vocab_size, bias=False)      \n        for pn, p in self.named_parameters():\n            if pn.endswith('c_proj.weight'):    \n                torch.nn.init.normal_(p, mean=0.0, \n                  std=0.02/math.sqrt(2 * config.n_layer))\n    def forward(self, idx, targets=None):\n        b, t = idx.size()\n        pos=torch.arange(0,t,dtype=\\n            torch.long).unsqueeze(0).to(device)    \n        tok_emb = self.transformer.wte(idx) \n        pos_emb = self.transformer.wpe(pos) \n        x = self.transformer.drop(tok_emb + pos_emb)\n        for block in self.transformer.h:\n            x = block(x)\n        x = self.transformer.ln_f(x)\n        logits = self.lm_head(x)\n        return logits\nThe positional encoding is created within the Model()  class. Therefore, we need to \nmove it to a compute unified device architecture (CUDA)-enabled GPU (if available) \nto ensure that all inputs to the model are on the same device. Failing to do this will \nresult in an error message.\nThe input to the model consists of sequences of indexes corresponding to tokens \nin the vocabulary. We pass the input through word embedding and positional encod -\ning and add the two to form the input embedding. The input embedding then goes \nthrough the three decoder blocks. After that, we apply layer normalization to the out -\nput and attach a linear head to it so that the number of outputs is 10,600, the size of \nthe vocabulary. The outputs are the logits corresponding to the 10,600 tokens in the \nvocabulary. Later, we’ll apply the softmax activation function to the logits to obtain the \nprobability distribution over the unique tokens in the vocabulary when generating text.\nNext, we’ll create our GPT model by instantiating the Model()  class we defined \nearlier:\nmodel=Model(config)\nmodel.to(device)\nnum=sum(p.numel() for p in model.transformer.parameters())\nprint(""number of parameters: %.2fM"" % (num/1e6,))\nprint(model)\nThe output is \nnumber of parameters: 5.12M\nModel(\n  (transformer): ModuleDict(\n    (wte): Embedding(10600, 256)\n    (wpe): Embedding(128, 256)\n    (drop): Dropout(p=0.1, inplace=False)Moves the positional \nencoding to CUDA-enabled \nGPU, if available",10192
115-12.4 Training the GPT model to generate text.pdf,115-12.4 Training the GPT model to generate text,,0
116-12.4.2 A function to generate text.pdf,116-12.4.2 A function to generate text,"280 chapter  12 Training a Transformer to generate text\n    (h): ModuleList(\n      (0-2): 3 x Block(\n        (ln_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n        (attn): CausalSelfAttention(\n          (c_attn): Linear(in_features=256, out_features=768, bias=True)\n          (c_proj): Linear(in_features=256, out_features=256, bias=True)\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n        (mlp): ModuleDict(\n          (c_fc): Linear(in_features=256, out_features=1024, bias=True)\n          (c_proj): Linear(in_features=1024, out_features=256, bias=True)\n          (act): GELU()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=256, out_features=10600, bias=False)\n)\nOur GPT model has 5.12 million parameters. The structure of our model is similar to \nthat of GPT-2XL. If you compare the output above with that from chapter 11, you’ll see \nthat the only differences are in the hyperparameters, such as the embedding dimen -\nsion, number of decoder layers, vocabulary size, and so on. \n12.4 Training the GPT model to generate text\nIn this section, you’ll train the GPT model you just built using the batches of training \ndata we prepared earlier in this chapter. A related question is how many epochs we \nshould train the model. While training too few epochs may lead to incoherent text, \ntraining too many epochs may lead to an overfitted model, which may generate text \nidentical to passages in the training text. \nTherefore, we will train the model for 40 epochs. We’ll save the model after every \n10 epochs and assess which version of the trained model can generate coherent text \nwithout simply copying passages from the training text. Another potential approach is \nto create a validation set and stop training when the model’s performance converges in \nthe validation set, as we did in chapter 2. \n12.4.1  Training the GPT model\nAs always, we’ll use the Adam optimizer. Since our GPT model is essentially perform -\ning a multicategory classification, we’ll use cross-entropy loss as our loss function:\nlr=0.0001\noptimizer = torch.optim.Adam(model.parameters(), lr=lr)\nloss_func = nn.CrossEntropyLoss()\nWe will train the model for 40 epochs, as shown in the following listing.\n 281 Training the GPT model to generate text\nListing 12.6    Training the GPT model to generate text\nmodel.train()  \nfor i in range(1,41):\n    tloss = 0.\n    for idx, (x,y) in enumerate(loader):    \n        x,y=x.to(device),y.to(device)\n        output = model(x)\n        loss=loss_func(output.view(-1,output.size(-1)),\n                           y.view(-1))    \n        optimizer.zero_grad()\n        loss.backward()\n        nn.utils.clip_grad_norm_(model.parameters(),1)    \n        optimizer.step()    \n        tloss += loss.item()\n    print(f'epoch {i} loss {tloss/(idx+1)}') \n    if i%10==0:\n        torch.save(model.state_dict(),f'files/GPTe{i}.pth')    \nDuring training, we pass all the input sequences x in a batch through the model to \nobtain predictions. We compare these predictions with the output sequences y in \nthe batch and calculate the cross-entropy loss. We then adjust the model parameters \nto minimize this loss. Note that we have clipped the gradient norm to 1 to avoid the \npotential problem of exploding gradients.\nGradient norm clipping\nGradient norm clipping is a technique used in training neural networks to prevent the \nexploding gradient problem. This problem occurs when the gradients of the loss function \nwith respect to the model’s parameters become excessively large, leading to unstable \ntraining and poor model performance. In gradient norm clipping, the gradients are scaled \ndown if their norm (magnitude) exceeds a certain threshold. This ensures that the gradi -\nents do not become too large, maintaining stable training and improving convergence.\nThis training process takes a couple of hours if you have a CUDA-enabled GPU. After \ntraining, four files, GPTe10.pth, GPTe20.pth, ..., GPTe40.pth, will be saved on your \ncomputer. Alternatively, you can download the trained models from my website: \nhttps: //gattonweb.uky.edu/faculty/lium/gai/GPT.zip . \n12.4.2  A function to generate text\nNow that we have multiple versions of the trained model, we can proceed to text gener -\nation and compare the performance of different versions. We can assess which version \nperforms the best and use that version to generate text. \nSimilar to the process in GPT-2XL, text generation begins with feeding a sequence of \nindexes (representing tokens) to the model as a prompt. The model predicts the index \nof the next token, which is then appended to the prompt to form a new sequence. This Iterates through all \nbatches of training data\nCompares model \npredictions with \nactual outputs\nClips gradient \nnorm to 1\nTweaks model parameters \nto minimize loss\nSaves model \nafter every \nten epochs\n282 chapter  12 Training a Transformer to generate text\nnew sequence is fed back into the model for further predictions, and this process is \nrepeated until a desired number of new tokens is generated.\nTo facilitate this process, we define a sample()  function. This function takes a \nsequence of indexes as input, representing the current state of the text. It then iteratively \npredicts and appends new indexes to the sequence until the specified number of new \ntokens, max_new_tokens , is reached. The following listing shows the implementation.\nListing 12.7    A sample()  function to predict subsequent indexes \ndef sample(idx, weights, max_new_tokens, temperature=1.0, top_k=None):\n    model.eval()\n    model.load_state_dict(torch.load(weights,\n        map_location=device))    \n    original_length=len(idx[0])\n    for _ in range(max_new_tokens):    \n        if idx.size(1) <= config.block_size:\n            idx_cond = idx  \n        else:\n            idx_cond = idx[:, -config.block_size:]\n        logits = model(idx_cond.to(device))    \n        logits = logits[:, -1, :] / temperature\n        if top_k is not None:\n            v, _ = torch.topk(logits, top_k)\n            logits[logits < v[:, [-1]]] = -float('Inf')\n        probs = F.softmax(logits, dim=-1)\n        idx_next=torch.multinomial(probs,num_samples=1)\n        idx = torch.cat((idx, idx_next.cpu()), dim=1)    \n    return idx[:, original_length:]    \nOne of the arguments of the sample()  function is weights , which represents the \ntrained weights of one of the models saved on your computer. Unlike the sample()  \nfunction we defined in chapter 11, our function here returns only the newly generated \nindexes, not including the original indexes that were fed to the sample()  function. \nWe made this change to accommodate cases where the prompt contains unknown \ntokens. In such cases, our sample()  function ensures that the final output retains the \noriginal prompt. Otherwise, all unknown tokens would be replaced with ""UNK""  in the \nfinal output.\nNext, we define a generate()  function to generate text based on a prompt. The \nfunction first converts the prompt to a sequence of indexes. It then uses the sample()  \nfunction to generate a new sequence of indexes. After that, the generate()  function \nconcatenates all indexes together and converts them back to text. The implementation \nis shown in the following listing.\nListing 12.8    A function to generate text with the trained GPT model\nUNK=word_to_int[""UNK""]\ndef generate(prompt, weights, max_new_tokens, temperature=1.0,\n             top_k=None):Loads up a version of \nthe trained model\nGenerates a fixed \nnumber of new indexes \nUses the model to \nmake predictions\nAttaches the new \nindex to the end of \nthe sequence\nOutputs only the \nnew indexes",8022
117-12.4.3 Text generation with different versions of the trained model.pdf,117-12.4.3 Text generation with different versions of the trained model,"283 Training the GPT model to generate text\n    assert len(prompt)>0, ""prompt must contain at least one token""    \n    text=prompt.lower().replace(""\n"", "" "")\n    for x in punctuations:\n        text=text.replace(f""{x}"", f"" {x} "")\n    text_tokenized=text.split() \n    idx=[word_to_int.get(w,UNK) for w in text_tokenized]    \n    idx=torch.LongTensor(idx).unsqueeze(0)\n    idx=sample(idx, weights, max_new_tokens, \n               temperature=1.0, top_k=None)    \n    tokens=[int_to_word[i] for i in idx.squeeze().numpy()]    \n    text="" "".join(tokens)\n    for x in '''"").:;!?,-''''':\n        text=text.replace(f"" {x}"", f""{x}"") \n    for x in '''""(-''''':\n        text=text.replace(f""{x} "", f""{x}"")     \n    return prompt+"" ""+text\nWe ensure that the prompt is not empty. If it is, you’ll receive an error message say -\ning “prompt must contain at least one token.” The generate()  function allows you \nto select which version of the model to use by specifying the weights saved on your \ncomputer. For example, you can choose ‘files/GPTe10.pth’ as the value of the weights \nargument for the function. The function converts the prompt into a series of indexes, \nwhich are then fed into the model to predict the next index. After generating a fixed \nnumber of new indexes, the function converts the entire index sequence back into \ntextual form.\n12.4.3  Text generation with different versions of the trained model\nNext, we’ll experiment with different versions of the trained model to generate text. \nWe can use the unknown token ""UNK""  as the prompt for unconditional text gener -\nation. This is especially beneficial in our context because we want to check if the gen -\nerated text is directly copied from the training text. While a unique prompt that’s very \ndifferent from the training text unlikely leads to passages directly from the training \ntext, unconditionally generated text is more likely to be from the training text. \nWe first use the model after 20 epochs of training to generate text unconditionally:\nprompt=""UNK""\nfor i in range(10):\n    torch.manual_seed(i)\n    print(generate(prompt,'files/GPTe20.pth',max_new_tokens=20)[4:]))\nThe output is \nway."" ""kümmel,"" i said. ""it's the way to talk about it\n--------------------------------------------------\n,"" robert jordan said. ""but do not realize how far he is ruined."" ""pero\n--------------------------------------------------\nin the fog, robert jordan thought. and then, without looking at last, so \ngood, he \n--------------------------------------------------Makes sure the \nprompt is not empty\nConverts prompt \ninto a sequence of \nindexes\nUses the sample() \nfunction to generate \nnew indexes\nConverts the \nnew sequence \nof indexes back \nto text\n284 chapter  12 Training a Transformer to generate text\npot of yellow rice and fish and the boy loved him. ""no,"" the boy said.\n--------------------------------------------------\nthe line now. it's wonderful."" ""he's crazy about the brave.""\n--------------------------------------------------\ncandle to us. ""and if the maria kisses thee again i will commence kissing \nthee myself. it \n--------------------------------------------------\n?"" ""do you have to for the moment."" robert jordan got up and walked away in\n--------------------------------------------------\n. a uniform for my father, he thought. i'll say them later. just then he\n--------------------------------------------------\nand more practical to read and relax in the evening; of all the things he \nhad enjoyed the next \n--------------------------------------------------\nin bed and rolled himself a cigarette. when he gave them a log to a second \ngrenade. "" \n--------------------------------------------------\nWe set the prompt to ""UNK""  and ask the generate()  function to unconditionally gen -\nerate 20 new tokens 10 times. We use the manual_seed()  method to fix the random \nseeds so results are reproducible. As you can see, the 10 short passages generated here \nare all grammatically correct, and they sound like passages from Hemingway’s novels. \nFor example, the word “kummel” in the first passage was a type of liqueur that was \nmentioned in A Farewell to Arms  quite often. At the same time, none of the above 10 \npassages are directly copied from the training text.\nNext, we use the model after 40 epochs of training instead to generate text uncondi -\ntionally and see what happens:\nprompt=""UNK""\nfor i in range(10):\n    torch.manual_seed(i)\n    print(generate(prompt,'files/GPTe40.pth',max_new_tokens=20)[4:]))\nThe output is  \nway."" ""kümmel, and i will enjoy the killing. they must have brought me a spit\n--------------------------------------------------\n,"" robert jordan said. ""but do not tell me that he saw anything."" ""not\n--------------------------------------------------\nin the first time he had bit the ear like that and held onto it, his neck \nand jaws\n--------------------------------------------------\npot of yellow rice with fish. it was cold now in the head and he could not \nsee the\n--------------------------------------------------\nthe line of his mouth. he thought."" ""the laughing hurt him."" ""i can\n--------------------------------------------------\ncandle made? that was the worst day of my life until one other day."" ""don'\n--------------------------------------------------\n?"" ""do you have to for the moment."" robert jordan took the glasses and \nopened the\n--------------------------------------------------\n. that's what they don't marry."" i reached for her hand. ""don\n--------------------------------------------------\n 285 Training the GPT model to generate text\nand more grenades. that was the last for next year. it crossed the river \naway from the front\n--------------------------------------------------\nin a revolutionary army,"" robert jordan said. ""that's really nonsense. it's\n--------------------------------------------------\nThe 10 short passages generated here are again all grammatically correct, and they \nsound like passages from Hemingway’s novels. However, if you examine them closely, a \nlarge part of the eighth passage is directly copied from the novel A Farewell to Arms . The \npart they don't marry."" i reached for her hand. ""don  appeared in the novel \nas well. You can verify by searching in the file ThreeNovels.txt that was saved on your \ncomputer earlier.   \nExercise 12.1\nGenerate a passage of text with 50 new tokens unconditionally using the model trained \nfor 10 epochs. Set the random seed to 42 and keep the temperature  and top-K  sam -\npling at the default setting. Examine whether the generated passage is grammatically \ncorrect and if any parts are directly copied from the training text. \nAlternatively, you can use a unique prompt that’s not in the training text to generate \nnew text. For example, you might use “the old man saw the shark near the” as the \nprompt and ask the generate()  function to add 20 new tokens to the prompt, repeat -\ning this process 10 times:\nprompt=""the old man saw the shark near the""\nfor i in range(10):\n    torch.manual_seed(i)\n    print(generate(prompt,'files/GPTe40.pth',max_new_tokens=20))\n    print(""-""*50)   \nThe output is \nthe old man saw the shark near the old man's head with his tail out and the \nold man hit him squarely in the center of\n--------------------------------------------------\nthe old man saw the shark near the boat with one hand. he had no feeling of\nthe morning but he started to pull on it gently\n--------------------------------------------------\nthe old man saw the shark near the old man's head. then he went back to \nanother man in and leaned over and dipped the\n--------------------------------------------------\nthe old man saw the shark near the fish now, and the old man was asleep in \nthe water as he rowed he was out of the\n--------------------------------------------------\nthe old man saw the shark near the boat. it was a nice-boat. he saw the old\n man's head and he started\n--------------------------------------------------\nthe old man saw the shark near the boat to see him clearly and he was \nafraid that he was higher out of the water and the old\n--------------------------------------------------\n286 chapter  12 Training a Transformer to generate text\nthe old man saw the shark near the old man's head and then, with his tail \nlashing and his jaws clicking, the shark plowed\n--------------------------------------------------\nthe old man saw the shark near the line with his tail which was not sweet \nsmelling it. the old man knew that the fish was coming\n--------------------------------------------------\nthe old man saw the shark near the fish with his jaws hooked and the old \nman stabbed him in his left eye. the shark still hung\n--------------------------------------------------\nthe old man saw the shark near the fish and he started to shake his head \nagain. the old man was asleep in the stern and he\n--------------------------------------------------\nThe generated text is grammatically correct and coherent, closely resembling passages \nfrom Hemingway’s novel The Old Man and the Sea . Since we used the model trained for \n40 epochs, there’s a higher likelihood of generating text that directly mirrors the train -\ning data. However, using a unique prompt can reduce this probability.\nBy setting the temperature and using top-K  sampling, we can further control the \ndiversity of the generated text. In this case, with a prompt like “the old man saw the \nshark near the,” and a temperature of 0.9 with top-50 sampling, the output remains \nmostly grammatically correct: \nprompt=""the old man saw the shark near the""\nfor i in range(10):\n    torch.manual_seed(i)\n    print(generate(prompt,'files/GPTe20.pth',max_new_tokens=20,\n                  temperature=0.9,top_k=50))\n    print(""-""*50) \nThe output is\n The old man saw the shark near the boat. then he swung the great fish that \nwas more comfortable in the sun. the old man could\n--------------------------------------------------\nthe old man saw the shark near the boat with one hand. he wore his overcoat\n and carried the submachine gun muzzle down, carrying it in\n--------------------------------------------------\nthe old man saw the shark near the boat with its long dip sharply and the \nold man stabbed him in the morning. he could not see\n--------------------------------------------------\nthe old man saw the shark near the fish that was now heavy and long and \ngrave he had taken no part in. he was still under\n--------------------------------------------------\nthe old man saw the shark near the boat. it was a nice little light. then \nhe rowed out and the old man was asleep over\n--------------------------------------------------\nthe old man saw the shark near the boat to come. ""old man's shack and i'll \nfill the water with him in\n--------------------------------------------------\nthe old man saw the shark near the boat and then rose with his lines close \nhim over the stern. ""no,"" the old man\n--------------------------------------------------\nthe old man saw the shark near the line with his tail go under. he was \ncutting away onto the bow and his face was just a\n--------------------------------------------------\n 287 Summary\nthe old man saw the shark near the fish with his tail that he swung him in.\n the shark's head was out of water and\n--------------------------------------------------\nthe old man saw the shark near the boat and he started to cry. he could \nalmost have them come down and whipped him in again.\n--------------------------------------------------\nSince we used the model trained for 20 epochs instead of 40 epochs, the output is less \ncoherent, with occasional grammar errors. For example, “with its long dip sharply” in \nthe third passage is not grammatically correct. However, the risk of generating text \ndirectly copied from the training data is also lower.\nExercise 12.2\nGenerate a passage of text with 50 new tokens using the model trained for 40 epochs. \nUse “the old man saw the shark near the” as the prompt; set the random seed  to 42, \nthe temperature  to 0.95, and the top_k  to 100. Check if the generated passage is \ngrammatically correct and if any part of the text is directly copied from the training text. \nIn this chapter, you’ve learned how to construct and train a GPT-style Transformer \nmodel from the ground up. Specifically, you’ve created a simplified version of the \nGPT-2 model with only 5.12 million parameters. Using three novels by Ernest Heming -\nway as training data, you have successfully trained the model. You have also generated \ntext that is coherent and stylistically consistent with Hemingway’s writing.\nSummary\n¡ The style of the generated text from a GPT model will be heavily influenced by \nthe training data. For effective text generation, it’s important to have a balance \nof text length and variation in the training material. The training dataset should \nbe sufficiently large for the model to learn and emulate a specific writing style \naccurately. However, if the dataset lacks diversity, the model might end up repro -\nducing passages directly from the training text. Conversely, overly long training \ndatasets can require excessive computational resources for training. \n¡ Choosing the right hyperparameters in the GPT model is crucial for successful \nmodel training and text generation. Setting the hyperparameters too large may \nlead to too many parameters. This results in longer training time and an overfit -\nted model. Setting the hyperparameters too small may hinder the model’s ability \nto learn effectively and capture the writing style in the training data. This may \nlead to incoherent generated text.\n¡ The appropriate number of training epochs is important for text generation. \nWhile training too few epochs may lead to incoherent text, training for too many \nepochs may lead to an overfitted model that generates text identical to passages \nin the training text.",14084
118-Part 4.pdf,118-Part 4,"Part 4\nApplications and new \ndevelopments\nT his part covers some applications of the generative models from earlier \nchapters as well as some new developments in the field of generative AI. \nIn chapters 13 and 14, you’ll learn two ways of generating music: MuseGAN, \nwhich treats a piece of music as a multidimensional object akin to an image, and \nMusic Transformer, which treats a piece of music as a sequence of musical events. \nChapter 15 introduces you to diffusion models, which form the foundation of all \nleading text-to-image Transformers (such as DALL-E 2 or Imagen). Chapter 16 \nuses the LangChain library to combine pretrained large language models with \nWolfram Alpha and Wikipedia APIs to create a zero-shot know-it-all personal \nassistant.",766
119-13 Music generation with MuseGAN.pdf,119-13 Music generation with MuseGAN,"29113Music generation  \nwith MuseGAN\nThis chapter covers\n¡ Music representation using musical instrument  \n digital interface  \n¡ Treating music generation as an object creation  \n problem similar to image generation\n¡ Building and training a generative adversarial  \n network to generate music\n¡ Generating music using the trained MuseGAN  \n model\nUp to now, we have successfully generated shapes, numbers, images, and text. In \nthis chapter and the next, we will explore two different ways of generating lifelike \nmusic. This chapter will apply the techniques from image GANs, treating a piece of \nmusic as a multidimensional object akin to an image. The generator will produce \na complete piece of music and submit it to the critic (serving as the discrimina -\ntor because we use the Wasserstein distance with gradient penalty, as discussed in \nchapter 5) for evaluation. The generator will then modify the music based on the \n292 chapter  13 Music generation with MuseGAN \ncritic’s feedback until it closely resembles real music from the training dataset. In the \nnext chapter, we will treat music as a sequence of musical events, employing natural \nlanguage processing (NLP) techniques. We will use a GPT-style Transformer to predict \nthe most probable musical event in a sequence based on previous events. This Trans -\nformer will generate a long sequence of musical events that can be converted into \nrealistic-sounding music.\nThe field of music generation using AI has gained significant attention; MuseGAN \nis a prominent model, which was introduced by Dong, Hsiao, Yang, and Yang in 2017.1 \nMuseGAN is a deep neural network that utilizes generative adversarial networks (GANs) \nto create multitrack music, with the word Muse signifying the creative inspiration \nbehind music. The model is adept at understanding the complex interactions between \ndifferent tracks that represent different musical instruments or different voices (which \nis the case in our training data). As a result, MuseGAN can generate compositions that \nare harmonious and cohesive.\nMuseGAN, similar to other GAN models, consists of two primary components: the \ngenerator and the critic (who provides a continuous measure of how real the sample is \nrather than classifying a sample into real or fake). The generator’s task is to generate \nmusic, whereas the critic assesses the music’s quality and offers feedback to the genera -\ntor. This adversarial interaction enables the generator to gradually improve, leading to \nthe creation of more realistic and appealing music. \nSuppose you’re an avid fan of Johann Sebastian Bach and have listened to all his \ncompositions. You might wonder if it’s possible to use MuseGAN to create synthetic \nmusic that mimics his style. The answer is yes, and you’ll learn how to do that in this \nchapter.\nSpecifically, you’ll first explore how to represent a piece of multitrack music as a mul -\ntidimensional object. A track is essentially an individual line of music or sound, which \ncan be a different instrument such as piano, bass, or drums or a different voice such as \nsoprano, alto, tenor, or bass. When composing a track in electronic music, you typically \norganize it into bars (segments of time), subdivide each bar into steps for finer control \nover rhythm, and then assign a specific note to each step to craft your melodies and \nrhythms. As a result, each piece of music in our training set is structured with a (4, 2, 16, \n84) shape: this means there are four music tracks, with each track consisting of 2 bars, \neach bar containing 16 steps, and each step capable of playing one of the 84 different \nnotes. \nThe style of the music generated by our MuseGAN will be influenced by the training \ndata. Since you are interested in Bach’s work, you’ll be training MuseGAN with The JSB \nChorales dataset, which is a collection of chorales composed by Bach, arranged for four \ntracks. These chorales have been converted into a piano roll representation, a method \n1 Hao-Wen Dong, Wen-Yi Hsiao, Li-Chia Yang, Yi-Hsuan Yang, 2017, “MuseGAN: Multi-track Sequential Generative \nAdversarial Networks for Symbolic Music Generation and Accompaniment.” https: //arxiv.org/abs/1709.06298 .",4258
120-13.1 Digital music representation.pdf,120-13.1 Digital music representation,,0
121-13.1.2 An introduction to multitrack music.pdf,121-13.1.2 An introduction to multitrack music,"293 Digital music representation \nused for visualizing and encoding music, especially for digital processing purposes. \nYou’ll learn how to transform a piece of music represented in the shape of (4, 2, 16, 84) \ninto a musical instrument digital interface (MIDI) file, which can then be played on \nyour computer.\nWhile the generator uses just one single noise vector from the latent space to gener -\nate different formats of content such as shapes, numbers, and images in earlier chap -\nters, the generator in MuseGAN will use four noise vectors when producing a piece \nof music. The use of four separate noise vectors (chords, style, melody, and groove, \nwhich I’ll explain in detail later in this chapter) is a design choice that allows for greater \ncontrol and diversity in the music generation process. Each of these noise vectors rep -\nresents a different aspect of music, and by manipulating them individually, the model \ncan generate more complex and nuanced compositions.\nOnce the model is trained, we’ll discard the critic network, a common practice in \nGAN models. We’ll then utilize the trained generator to produce music pieces by input -\nting four noise vectors from the latent space. The music generated in this way closely \nmirrors the style of Bach.\n13.1 Digital music representation \nOur goal is to master the art of building and training a GAN model from scratch for \nmusic generation. To achieve this, we need to start with the fundamentals of music \ntheory, including understanding musical notes, octaves, and pitch numbers. Following \nthat, we’ll dive into the inner workings of digital music, specifically focusing on MIDI \nfiles.\nDepending on the type of machine learning model we use for music generation, the \nrepresentation of a piece of music in digital form will vary. For instance, in this chapter, \nwe’ll represent music as a multidimensional object, while in the next chapter, we’ll use a \ndifferent format: a sequence of indexes.\nIn this section, we’ll cover basic music theory and then move on to represent music \ndigitally using piano rolls. You’ll learn to load and play an example MIDI file on your \ncomputer. We’ll also introduce the music21 Python library, which you’ll install and use \nto visualize the staff notes associated with the music piece. Finally, you’ll learn to repre -\nsent a piece of music as a multidimensional object with the shape of (4, 2, 16, 84).\n13.1.1  Musical notes, octave, and pitch\nIn this chapter, we’ll be working with a training dataset that represents music pieces as \n4D objects. To grasp the meaning of the music pieces in the training data, it’s essential \nto first familiarize ourselves with some fundamental concepts in music theory, such \nas musical notes, octaves, and pitch. These concepts are interrelated and crucial for \nunderstanding the dataset. \nFigure 13.1 illustrates the relationships among these concepts. \n294 chapter  13 Music generation with MuseGAN \nFigure 13.1    The relationship between musical notes, octaves, and pitches (also known as note \nnumbers). The first column displays the 11 octaves (ranging from –1 to 9), representing different levels \nof musical sound. Each octave is subdivided into 12 semitones, which are listed in the top row: C, C#, D, \nD#, ..., B. Within each octave, each note is assigned a specific pitch number, ranging from 0 to 127, as \nindicated in the figure.\nA musical note is a symbol representing a specific sound in music. These notes are the \nfoundational elements of music, used to craft melodies, chords, and rhythms. Each \nnote is assigned a name (such as A, B, C, D, E, F, G) and corresponds to a specific fre -\nquency, which determines its pitch: whether the note sounds high or low. For instance, \na middle C (C4) typically has a frequency of about 262 hertz, meaning its sound waves \nvibrate 262 times per second.\nYou might be wondering about the meaning of the term “middle C (C4).” The num -\nber 4 in C4 refers to the octave, which is the distance between one level of musical pitch \nand the next. In figure 13.1, the far-left column displays 11 octave levels, ranging from \n–1 to 9. The frequency of a sound doubles as you move from one octave level to the \nnext. For example, note A4 is usually tuned to 440 hertz, while A5, one octave above A4, \nis tuned to 880 hertz..\nIn Western music, an octave is divided into 12 semitones, each corresponding to a \nspecific note. The top row of figure 13.1 lists these 12 semitones: C, C#, D, D#, ..., B. \nMoving up or down by 12 semitones takes you to the same note name but in a higher or \nlower octave. As mentioned earlier, A5 is one octave above A4.\nEach note within a specific octave is assigned a pitch number, ranging from 0 to 127, \nas depicted in figure 13.1. For example, the note C4 has a pitch number of 60, while F3 \nhas a pitch number of 53. The pitch number is a more efficient way to represent musical \nnotes since it specifies both the octave level and the semitone. The training data you’ll \nbe using in this chapter is encoded using pitch numbers for this very reason.\n13.1.2  An introduction to multitrack music\nLet’s first talk about how multitrack music works and how it is represented digitally. In \nelectronic music production, a “track” typically refers to an individual layer or com -\nponent of the music, such as a drum track, a bass track, or a melody track. In classical \n 295 Digital music representation \nmusic, tracks might represent different vocal parts, like soprano, alto, tenor, and bass. \nFor instance, the training dataset we’re using in this chapter, the JSB Chorales data -\nset, consists of four tracks corresponding to four vocal parts. In music production, \neach track can be individually edited and processed within a digital audio workstation \n(DAW). These tracks are composed of various musical elements, including bars, steps, \nand notes.\nA bar (or measure) is a segment of time defined by a specified number of beats, with \neach beat having a certain note duration. In many popular music genres, a bar typically \ncontains four beats, although this can vary based on the time signature of the piece. \nThe total number of bars in a track is determined by the track’s length and structure. \nFor example, in our training dataset, each track comprises two bars.\nIn the context of step sequencing, a technique commonly used for programming \nrhythms and melodies in electronic music, a “step” represents a subdivision of a bar. In \na standard 4/4 time signature (four beats in a bar and four steps in a beat), you might \nfind 16 steps per bar, with each step corresponding to a sixteenth of a bar. \nLastly, each step contains a musical note. In our dataset, we limit the range to the 84 \nmost frequently used notes (with pitch numbers from 0 to 83). Therefore, the musical \nnote in a step is encoded as a one-hot vector with 84 values.\nTo illustrate these concepts with a practical example, download the file example.\nmidi from the book’s GitHub repository at https: //github.com/markhliu/DGAI  and \nsave it in the /files/ directory on your computer. A file with the .midi extension is a \nMIDI file. MIDI is a technical standard that outlines a protocol, digital interface, and \nconnectors for enabling electronic musical instruments, computers, and other related \ndevices to connect and communicate with each other.\nMIDI files can be played on most music players on your computer. To get a sense of \nthe type of music in our training data, open the file example.midi you just downloaded \nwith a music player on your computer. It should sound like this music file I placed on \nmy website: https: //mng.bz/lrJB . The file example.midi is converted from one of the \nmusic pieces in the training dataset in this chapter. Later you’ll learn how to convert a \npiece of music in the training dataset with a shape of (4, 2, 16, 84) into a MIDI file that \ncan be played on your computer. \nWe’ll use the music21 Python library, a powerful and comprehensive toolkit designed \nfor music analysis, composition, and manipulation, to visualize how various music con -\ncepts work. Therefore, run the following line of code in a new cell in the Jupyter Note -\nbook app on your computer:\n!pip install music21\nThe music21 library enables you to visualize music as staff notation to have a better \nunderstanding of tracks, bars, steps, and notes. To achieve this, you must first install the \nMuseScore application on your computer. Visit https: //musescore.org/en/download  \nand download the most recent version of the MuseScore app for your operating system. \nAs of this writing, the latest version is MuseScore 4, which we’ll use as our example. \nEnsure you know the file path of the MuseScore app on your computer. For instance, \n296 chapter  13 Music generation with MuseGAN \nin Windows, the path is C:\Program Files\MuseScore 4\bin\MuseScore4.exe. Run the \ncode cell in the following listing to visualize the staff notation for the file example.midi.\nListing 13.1    Visualizing the staff notation using the music21 library\n%matplotlib inline    \nfrom music21 import midi, environment\nmf = midi.MidiFile()    \nmf.open(""files/example.midi"")    \nmf.read()\nmf.close()\nstream = midi.translate.midiFileToStream(mf)\nus = environment.Environment() \npath = r'C:\Program Files\MuseScore 4\bin\MuseScore4.exe'    \nus['musescoreDirectPNGPath'] = path    \nstream.show()    \nFor users of the macOS operating system, change the path in the preceding code cell \nto /Applications/MuseScore 4.app/Contents/MacOS/mscore. For Linux users, mod -\nify the path to /home/[user name]/.local/bin/mscore4portable, substituting [user \nname] with your actual username. For instance, my username is mark , so the path is /\nhome/mark/.local/bin/mscore4portable.\nExecuting the previous code cell will display a staff notation similar to what is illus -\ntrated in figure 13.2. Please note that the annotations in the figure are added by me, so \nyou will only see the staff notation without any annotations.\nFigure 13.2    Staff notation for a piece of music in JSB Chorales dataset. The music has four tracks, \nrepresenting the four voices in a chorale: soprano, alto, tenor, and bass. The notation is structured into \ntwo bars for each track, with the left and right halves representing the first and second bars, respectively. \nEach bar consists of 16 steps, aligning with the 4/4 time signature where a bar contains four beats, \neach subdivided into four sixteenth notes. A total of 84 different pitches are possible, and each note is \nrepresented as a one-hot vector with 84 values.  Shows the image in \nJupyter notebook instead \nof in the original app \nOpens the MIDI file\nDefines the \npath of the \nMuseScore app\nShows the staff notation",10861
122-13.1.3 Digitally represent music Piano rolls.pdf,122-13.1.3 Digitally represent music Piano rolls,"297 Digital music representation \nThe JSB Chorales dataset, which consists of chorale music pieces by Johann Sebastian \nBach, is often used for training machine learning models in music generation tasks. \nThe shape (4, 2, 16, 84) of each music piece in the dataset can be explained as follows. \nFour represents the four voices in a chorale: soprano, alto, tenor, and bass. Each voice \nis treated as a separate track in the dataset. Each piece is divided into two bars (also \ncalled measures). The dataset is formatted this way to standardize the length of the \nmusic pieces for training purposes. The number 16 represents the number of steps (or \nsubdivisions) in each bar. Finally, the note is one-hot encoded with 84 values, denoting \nthe number of possible pitches (or notes) that can be played in each step. \n13.1.3  Digitally represent music: Piano rolls\nA piano roll is a visual representation of music often used in MIDI sequencing software \nand DAWs. It is named after the traditional piano rolls used in player pianos, which \ncontained a physical roll of paper with holes punched in it to represent musical notes. \nIn a digital context, the piano roll serves a similar function but in a virtual format.\nThe piano roll is displayed as a grid, with time represented horizontally (from left \nto right) and pitch represented vertically (from bottom to top). Each row corresponds \nto a specific musical note, with higher notes at the top and lower notes at the bottom, \nsimilar to the layout of a piano keyboard.\nNotes are represented as bars or blocks on the grid. The position of a note block \nalong the vertical axis indicates its pitch, while its position along the horizontal axis \nindicates its timing in the music. The length of the note block represents the duration \nof the note.\nLet’s use the music21 library to illustrate what a piano roll looks like. Run this line of \ncode in a new cell in your Jupyter Notebook app:\nstream.plot()\nThe output is shown in figure 13.3.\nThe music21 library also allows you to see the quantized notes corresponding to the \npreceding piano roll:\nfor n in stream.recurse().notes:\n    print(n.offset, n.pitches)\nThe output is \n0.0 (<music21.pitch.Pitch E4>,)\n0.25 (<music21.pitch.Pitch A4>,)\n0.5 (<music21.pitch.Pitch G4>,)\n0.75 (<music21.pitch.Pitch F4>,)\n1.0 (<music21.pitch.Pitch E4>,)\n1.25 (<music21.pitch.Pitch D4>,)\n1.75 (<music21.pitch.Pitch E4>,)\n2.0 (<music21.pitch.Pitch E4>,)\n2.5 (<music21.pitch.Pitch D4>,)\n3.0 (<music21.pitch.Pitch C4>,)\n3.25 (<music21.pitch.Pitch A3>,)\n3.75 (<music21.pitch.Pitch B3>,)\n298 chapter  13 Music generation with MuseGAN \n0.0 (<music21.pitch.Pitch G3>,)\n0.25 (<music21.pitch.Pitch A3>,)\n0.5 (<music21.pitch.Pitch B3>,)\n…\n3.25 (<music21.pitch.Pitch F2>,)\n3.75 (<music21.pitch.Pitch E2>,)\nFigure 13.3    The piano roll for a piece of music. The piano roll is a graphical representation of a musical \npiece, depicted as a grid with time progressing horizontally from left to right and pitch represented \nvertically from bottom to top. Each row on the grid corresponds to a distinct musical note, arranged in \na manner akin to the keyboard of a piano, with higher notes positioned at the top and lower notes at the \nbottom. This specific piece of music comprises two bars, resulting in two distinct sections visible in the \ngraph. The vertical placement of a note block signifies its pitch, while its horizontal location indicates \nwhen the note is played in the piece. Additionally, the length of the note block reflects the duration for \nwhich the note is sustained. \nI omitted most of the output. The first value in each line in the previous output rep -\nresents time. It increases by 0.25 seconds after each line in most cases. If the time \nincrease in the next line is more than 0.25 seconds, it means a note lasts more than 0.25 \nseconds. As you can see, the starting note is E4. After 0.25 seconds, the note changes to \nA4, and then G4, and so on. This explains the first three blocks (far left) in figure 13.3, \nwhich have values E, A, and G, respectively. \nYou might be curious about how to convert the sequence of musical notes into an \nobject with the shape (4, 2, 16, 84). To understand this, let’s examine the pitch number \nat each time step in the musical notes:\n 299 Digital music representation \nfor n in stream.recurse().notes:\n    print(n.offset,n.pitches[0].midi)\nThe output is \n0.0 64\n0.25 69\n0.5 67\n0.75 65\n1.0 64\n1.25 62\n1.75 64\n2.0 64\n2.5 62\n3.0 60\n3.25 57\n3.75 59\n0.0 55\n0.25 57\n0.5 59\n…\n3.25 41\n3.75 40\nThe preceding code block has converted the musical note in each time step into a \npitch number, in the range of 0 to 83 based on the mapping used in figure 13.1. Each \nof the pitch numbers is then converted to a one-hot variable with 84 values, with value \n–1 everywhere, except 1 in one position. We use –1 and 1 in one-hot encoding instead \nof 0 and 1 because placing values between –1 and 1 centers the data around 0, which \ncan make training more stable and faster. Many activation functions and weight initial -\nization methods assume input data is centered around 0. Figure 13.4 illustrates how a \npiece of MIDI music is encoded into an object in the shape of (4, 2, 16, 84).\nTwo bars (i.e., two measures) \nin each music trackEach note is represented by a one-hot variable,\nwith -1 everywhere and 1 in one place4 beats in a bar; each beat has 4 notes;\nhence each bar has 16 notesFour tracks, representing the four \nvoices soprano, alto, tenor, and bass\n(4, 2, 16, 84)\nFigure 13.4    How to represent a piece of music using a 4D object. In our training data, each piece of \nmusic is represented by a 4D object in the shape of (4, 2, 16, 84). The first dimension represents the four \nmusic tracks, which are the four voices in the music (soprano, alto, tenor, and bass). Each music track \nis divided into two bars. There are four beats in each bar, and each beat has four notes; we therefore \nhave 16 notes in a bar. Finally, each note is represented by a one-hot variable with 84 values, with –1 \neverywhere and 1 in one place.",6150
123-13.2 A blueprint for music generation.pdf,123-13.2 A blueprint for music generation,,0
124-13.2.1 Constructing music with chords style melody and groove.pdf,124-13.2.1 Constructing music with chords style melody and groove,"300 chapter  13 Music generation with MuseGAN \nFigure 13.4 explains the dimensions of the music object shaped (4, 2, 16, 84). In \nessence, each musical piece comprises four tracks, with each track containing two bars. \nEach bar is subdivided into 16 notes. Given that the pitch numbers range from 0 to 83 \nin our training set, each note is represented by a one-hot vector with 84 values. \nIn subsequent discussions on preparing training data, we will explore how to trans -\nform an object with the shape (4, 2, 16, 84) back into a music piece in MIDI format, \nenabling playback on a computer. \n13.2 A blueprint for music generation \nWhen creating music, we need to incorporate more detailed inputs for enhanced con -\ntrol and variety. Unlike the approach of utilizing a single noise vector from the latent \nspace for generating shapes, numbers, and images, we will employ four distinct noise \nvectors in the music generation process. Since each music piece comprises four tracks \nand two bars, we’ll utilize four vectors to manage this structure. We’ll use one vector \nto govern all tracks and bars collectively, another vector to control each bar across \nall tracks, a third vector to oversee all tracks across bars, and a fourth one to manage \neach individual bar in each track. This section will introduce you to the concepts of \nchords, style, melody, and groove and explain how they influence various aspects of the \nmusic generation. After that, we’ll discuss the steps involved in building and training \nthe MuseGAN model. \n13.2.1  Constructing music with chords, style, melody, and groove\nLater, in the music generation stage, we obtain four noise vectors (chords, style, mel -\nody, and groove) from the latent space and feed them to the generator to create a \npiece of music. You may be wondering the meaning of these four pieces of informa -\ntion. In music, chords, style, melody, and groove are key elements that contribute to a \npiece’s overall sound and feel. Next I provide a brief explanation of each element. \nStyle refers to the characteristic way in which music is composed, performed, and \nexperienced. It includes the genre (such as jazz, classical, rock, and so on), the era in \nwhich the music was created, and the unique approach of the composer or performer. \nStyle is influenced by cultural, historical, and personal factors, and it helps to define the \nmusic’s identity.\nGroove is the rhythmic feel or swing in music, especially in styles like funk, jazz, and \nsoul. It’s what makes you want to tap your foot or dance. A groove is created by the pat -\ntern of accents, the interplay between the rhythm section (drums, bass, etc.), and the \ntempo. It’s the element that gives music its sense of motion and flow.\nChords are combinations of two or more notes played simultaneously. They provide \nthe harmonic foundation for music. Chords are built on scales and are used to create \nprogressions that give music its structure and emotional depth. Different chord types \n(major, minor, diminished, augmented, etc.) and their arrangements can evoke various \nmoods and feelings in the listener.\n 301 A blueprint for music generation \nFinally, melody is the sequence of notes that is most easily recognizable in a piece of \nmusic. It’s the part that you might hum or sing along to. Melodies are often built from \nscales and are characterized by their pitch, rhythm, and contour (the pattern of rises \nand falls in pitch). A good melody is memorable and expressive, conveying the main \nmusical and emotional themes of the piece.\nTogether, these elements work in harmony to create the overall sound and experi -\nence of a musical piece. Each element has its role, but they all interact and influence \neach other to produce the final music piece. Specifically, a music piece consists of four \ntracks, each with two bars, resulting in eight bar/track combinations. We’ll use one \nnoise vector for style, applied to all eight bars. We’ll use eight different noise vectors \nfor melody, each used in a unique bar. There are four noise vectors for groove, each \napplied to a different track, remaining the same across both bars. Two noise vectors will \nbe used for chords, one for each bar. Figure 13.5 provides a diagram of how these four \nelements contribute to the creation of a complete piece of music. \nTrack 1 Track 2 Track 3 Track 4\nBar 1\nBar 2Chords\nsize (1, 32)\nTemporal network\nNew chords\nsize (2, 32)\n1 chord for each bar, \nsame across tracksMelody\nsize (4, 32)\nTemporal network\nNew melody\nsize (2, 4, 32)\n8 different melodies, 1 \nfor each track each bar\nStyle\nsize (1, 32)Groove\nsize (4, 32)1 style, same across \nbars and across tracks1 groove for each track, \nsame across bars\nFigure 13.5    Music generation using chords, style, melody, and groove. Each music composition \nconsists of four tracks and spans two bars. We will extract four noise vectors from the latent space \nfor this purpose. The first vector, representing chords, has a dimension of (1, 32). This vector will be \nprocessed through a temporal network to expand the chords into two (1, 32) vectors, corresponding \nto the two bars, with identical values across all tracks. The second vector, denoting style, also has a \ndimension of (1, 32) and remains constant across all tracks and bars. The third vector, melody, is shaped \nas (4, 32). It will be stretched through a temporal network into two (4, 32) vectors, resulting in eight  \n(1, 32) vectors, each representing a unique track and bar combination. Lastly, the fourth vector, groove, \nwith a dimension of (4, 32), will be applied to the four tracks, maintaining the same values for both bars. \nThe generator creates a piece of music by generating one bar in one track at a time. \nFor this, it requires four noise vectors, each with a shape of (1, 32), as input. These vec -\ntors represent chords, style, melody, and groove, and each controls a distinct aspect of",5993
125-13.2.2 A blueprint to train a MuseGAN.pdf,125-13.2.2 A blueprint to train a MuseGAN,"302 chapter  13 Music generation with MuseGAN \nthe music, as previously explained. Since the music piece consists of four tracks, each \nwith two bars, there are a total of eight bar/track combinations. Consequently, we need \neight sets of chords, style, melody, and groove to generate all parts of the music piece.\nWe’ll obtain four noise vectors from the latent space corresponding to chords, style, \nmelody, and groove. We’ll also introduce a temporal network later, whose role is to \nexpand the input along the bar dimension. With two bars, this means doubling the size \nof the input. Music is inherently temporal, with patterns and structures that unfold over \ntime. The temporal network in MuseGAN is designed to capture these temporal depen -\ndencies, ensuring that the generated music has a coherent and logical progression.\nThe noise vector for chords has a shape of (1, 32). After processing it through the \ntemporal network, we obtain two (1, 32) sized vectors. The first vector is used across all \nfour tracks in the first bar, while the second vector is used across all four tracks in the \nsecond bar.\nThe noise vector for style, also with a shape of (1, 32), is applied uniformly across all \neight track/bar combinations. Note that we’ll not pass the style vector through the tem -\nporal network since the style vector is designed to be the same across bars. \nThe noise vector for melody has a shape of (4, 32). When passed through the tem -\nporal network, it yields two (4, 32) sized vectors, which further break down into eight  \n(1, 32) sized vectors. Each of these is used in a unique track/bar combination.\nLastly, the noise vector for groove, shaped as (4, 32), is used such that each (1, 32) \nsized vector is applied to a different track, remaining the same across both bars. We \nwon’t pass the groove vector through the temporal network since the groove vector is \ndesigned to be the same across bars.\nAfter generating a bar of music for each of the eight bar/track combinations, we’ll \nmerge them to create a full piece of music, consisting of four distinct tracks, each com -\nprising two unique bars.\n13.2.2  A blueprint to train a MuseGAN\nChapter 1 provided an overview of the foundational concepts behind GANs. In chap -\nters 3 to 5, you explored the creation and training of GANs for generating shapes, \nnumbers, and images. This subsection will summarize the steps for building and train -\ning MuseGAN, highlighting the differences from the previous chapters.\nThe style of music generated by MuseGAN is influenced by the training data’s style. \nTherefore, you should first collect a dataset of Bach’s compositions in a format suitable \nfor training. Next, you’ll create a MuseGAN model, which consists of a generator and a \ncritic. The generator network takes four random noise vectors as input (chords, style, \nmelody, and groove) and outputs a piece of music. The critic network evaluates a piece \nof music and assigns a rating, with higher scores for real music (from the training set) \nand lower scores for fake music (produced by the generator). Both the generator and \ncritic networks utilize deep convolutional layers to capture the spatial features of the \ninputs.\n 303 A blueprint for music generation \nFour noise \nvectors \n(chords, \nstyle, \nmelody, \ngroove)\nCritic ScoreTotal\nlossStep 1\nStep 2Fake Music\nStep 3 Step 4Step 3 Step 3\nWasserstein \nloss\n+ Gradient\npenaltyStep 5: Feedback\nGenerator\nStep 5: FeedbackInterpolated Music Real Music\nFigure 13.6    A diagram of the steps involved in training MuseGAN to generate music. The generator \nproduces a fake music piece by drawing four random noise vectors from the latent space (top left) \nand presents it to the critic (middle). The critic evaluates the piece and assigns a rating. A high rating \nsuggests that the piece is likely from the training dataset, while a lower rating indicates that the piece \nis likely fake (generated by the generator). Additionally, an interpolated music piece created from a mix \nof real and fake samples (top left) is presented to the critic. The training process incorporates a gradient \npenalty based on the critic’s rating of this interpolated piece, which is added to the total loss. The ratings \nare then compared to the ground truth, allowing both the critic and the generator to learn from these \nevaluations. After numerous training iterations, the generator becomes proficient at producing music \npieces that are virtually indistinguishable from real samples. \nFigure 13.6 illustrates the training process of MuseGAN. The generator (the bottom \nleft of the figure) receives four random noise vectors (chords, style, melody, and \ngroove) as input and produces fake music pieces (step 1 in figure 13.6). These noise \nvectors are drawn from the latent space, which represents the range of potential out -\nputs the GAN can generate, enabling the creation of diverse data samples. These fake \nmusic pieces, along with real ones from the training set (top right), are then evaluated \nby the critic (step 3). The critic (bottom center) assigns scores to all music pieces, aim -\ning to give high scores to real music and low scores to fake music (step 4).\nTo guide the adjustment of model parameters, appropriate loss functions must be \nchosen for both the generator and the critic. The generator’s loss function is designed \nto encourage the production of data points that closely resemble those from the train -\ning dataset. Specifically, the loss function for the generator is the negative of the critic’s \nrating. By minimizing this loss function, the generator strives to create music pieces \nthat receive high ratings from the critic. On the other hand, the critic’s loss function is \nformulated to encourage accurate assessment of real and generated data points. Thus, \nthe loss function for the critic is the rating itself if the music piece is from the training \nset and the negative of the rating if it is generated by the generator. In essence, the critic \naims to assign high ratings to real music pieces and low ratings to fake ones.",6153
126-13.3 Preparing the training data for MuseGAN.pdf,126-13.3 Preparing the training data for MuseGAN,,0
127-13.3.2 Converting multidimensional objects to music pieces.pdf,127-13.3.2 Converting multidimensional objects to music pieces,"304 chapter  13 Music generation with MuseGAN \nAdditionally, we incorporate the Wasserstein distance with gradient penalty into the \nloss function, as we did in chapter 5, to enhance the training stability and performance \nof GAN models. To achieve this, an interpolated music piece, blending real and fake \nmusic (top left in figure 13.6), is evaluated by the critic. The gradient penalty, based on \nthe critic’s rating of this interpolated piece, is then added to the total loss during the \ntraining process.\nThroughout the training loop, we alternate between training the critic and the gen -\nerator. In each training iteration, we sample a batch of real music pieces from the train -\ning set and a batch of fake music pieces generated by the generator. We calculate the \ntotal loss by comparing the critic’s ratings (i.e., scores) with the ground truth (whether \na music piece is real or fake). We then slightly adjust the weights in both the generator \nand critic networks so that, in subsequent iterations, the generator produces more real -\nistic music pieces, and the critic assigns higher scores to real music and lower scores to \nfake music.\nOnce MuseGAN is fully trained, music can be created by inputting four random \nnoise vectors into the trained generator.\n13.3 Preparing the training data for MuseGAN\nWe’ll use chorale compositions by Johann Sebastian Bach as our training dataset, \nexpecting the generated music to resemble Bach’s style. If you prefer the style of a \ndifferent musician, you can use their work as the training data instead. In this section, \nwe’ll start by downloading the training data and organizing it into batches for later \ntraining.\nAdditionally, we’ve learned that the music pieces in the training set will be repre -\nsented as 4D objects. In this section, you’ll also learn how to convert these multidimen -\nsional objects into playable music pieces on a computer. This conversion is essential \nbecause MuseGAN generates multidimensional objects similar to those in the train -\ning set. Later in the chapter, we’ll transform the multidimensional objects produced \nby MuseGAN into MIDI files, enabling you to listen to the generated music on your \ncomputer.\n13.3.1  Downloading the training data\nWe’ll use the JSB Chorales piano rolls dataset as our training set. Go to Cheng-Zhi Anna \nHuang’s GitHub repository ( https: //github.com/czhuang/JSB-Chorales-dataset ) and \ndownload the music file Jsb16thSeparated.npz. Save the file in the /files/ directory on \nyour computer.\nThen, download the two utility modules midi_util.py and MuseGAN_util.py from \nthe book’s GitHub repository ( https: //github.com/markhliu/DGAI ) and save them in \nthe /utils/ directory on your computer. The code in this chapter is adapted from the \nexcellent GitHub repository by Azamat Kanametov ( https: //github.com/akanametov/\nmusegan ). With these files in place, we can now load the music files and organize them \ninto batches for processing:\n 305 Preparing the training data for MuseGAN\nfrom torch.utils.data import DataLoader\nfrom utils.midi_util import MidiDataset\ndataset = MidiDataset('files/Jsb16thSeparated.npz')\nfirst_song=dataset[0]\nprint(first_song.shape)\nloader = DataLoader(dataset, batch_size=64, \n                        shuffle=True, drop_last=True)\nWe load the dataset you just downloaded into Python, then extract the first song and \nname it first_song . Since songs are represented as multidimensional objects, we \nprint out the shape of the first song. Finally, we place the training data in batches of 64, \nto be used later in the chapter. \nThe output from the preceding code block is\ntorch.Size([4, 2, 16, 84])\nEach song in the dataset has a shape of (4, 2, 16, 84), as shown in the previous output. \nThis indicates that each song consists of four tracks, each with two bars. Each bar con -\ntains 16 time steps, and at each time step, the musical note is represented by a one-hot \nvector with 84 values. In each one-hot vector, all values are set to –1, except for one \nposition where the value is set to 1, indicating the presence of a note. You can verify the \nrange of values in the dataset as follows:\nflat=first_song.reshape(-1,)\nprint(set(flat.tolist()))\nThe output is\n{1.0, -1.0}\nThe previous output shows that the values in each music piece are either –1 or 1. \n13.3.2  Converting multidimensional objects to music pieces\nCurrently, the songs are formatted as PyTorch tensors and are ready to be inputted \ninto the MuseGAN model. However, before we proceed, it’s important to gain a better \nunderstanding of how to convert these multidimensional objects into playable music \npieces on your computer. This will help us later to convert generated music pieces into \nplayable files.\nTo begin, we’ll convert all the 84-value one-hot variables into pitch numbers ranging \nfrom 0 to 83:\nimport numpy as np\nfrom music21 import note, stream, duration, tempo\nparts = stream.Score()\nparts.append(tempo.MetronomeMark(number=66))\nmax_pitches = np.argmax(first_song, axis=-1)    \nmidi_note_score = max_pitches.reshape([2 * 16, 4])    \nprint(midi_note_score)Converts 84-value one-hot \nvectors to numbers \nbetween 0 and 83\nReshapes the \nresult to (32, 4)\n306 chapter  13 Music generation with MuseGAN \nThe output is \ntensor([[74, 74, 74, 74],\n…\n        [70, 70, 69, 69],\n        [67, 67, 69, 69],\n        [70, 70, 70, 70],\n        [69, 69, 69, 69],\n        [69, 69, 69, 69],\n        [65, 65, 65, 65],\n        [58, 58, 60, 60],\n…\n        [53, 53, 53, 53]])\nIn the output displayed here, each column represents a music track, with numbers \nranging from 0 to 83. These numbers correspond to pitch numbers, as you have seen \nearlier in figure 13.1. \nNow, we’ll proceed to convert the tensor midi_note_score  in the previous code \nblock into an actual MIDI file, allowing you to play it on your computer.\nListing 13.2    Converting pitch numbers to a MIDI file\nfor i in range(4):    \n    last_x = int(midi_note_score[:, i][0])\n    s = stream.Part()\n    dur = 0\n    for idx, x in enumerate(midi_note_score[:, i]):    \n        x = int(x)\n        if (x != last_x or idx % 4 == 0) and idx > 0:\n            n = note.Note(last_x)\n            n.duration = duration.Duration(dur)\n            s.append(n)\n            dur = 0\n        last_x = x\n        dur = dur + 0.25    \n    n = note.Note(last_x)\n    n.duration = duration.Duration(dur)\n    s.append(n)    \n    parts.append(s)  \nparts.write(""midi"",""files/first_song.midi"")\nAfter running the preceding code cell, you’ll see a MIDI file, first_song.midi , on \nyour computer. Play it with a music player on your computer to get a sense of what type \nof music we are using to train the MuseGAN.\nExercise 13.1\nConvert the second song in the training dataset into a MIDI file. Save it as second_\nsong.midi  and play it using a music player on your computer.  Iterates through \nfour music tracks\nIterates through all \nnotes in each track\nAdds 0.25 seconds \nto each time step\nAdds the note to \nthe music stream",7109
128-13.4 Building a MuseGAN.pdf,128-13.4 Building a MuseGAN,,0
129-13.4.2 A generator in MuseGAN.pdf,129-13.4.2 A generator in MuseGAN,"307 Building a MuseGAN\n13.4 Building a MuseGAN\nIn essence, we will treat a music piece as an object with multiple dimensions. Using \ntechniques from chapters 4 to 6, we will tackle this task using deep convolutional neu -\nral networks for their ability to effectively extract spatial features from multidimen -\nsional objects. In MuseGAN, we’ll construct a generator and a critic, similar to how a \ngenerator in image creation refines an image based on a critic’s feedback. The genera -\ntor will produce a music piece as a 4D object.\nBoth real music from our training set and fake music from the generator will be \npresented to the critic. The critic will score each piece from negative infinity to positive \ninfinity, with higher scores indicating a higher likelihood of the music being real. The \ncritic aims to give high scores to real music and low scores to fake music. Conversely, \nthe generator aims to produce music that is indistinguishable from real music, thereby \nreceiving high scores from the critic.\nIn this section, we will build a MuseGAN model, comprising a generator network \nand a critic network. The critic network employs deep convolutional layers to extract \ndistinct features from multidimensional objects, enhancing its ability to evaluate music \npieces. On the other hand, the generator network utilizes deep transposed convolu -\ntional layers to produce feature maps aimed at generating realistic music pieces. Later, \nwe will train the MuseGAN model using music pieces from the training set.\n13.4.1  A critic in MuseGAN\nAs explained in chapter 5, incorporating the Wasserstein distance into the loss func -\ntion can help stabilize training. Therefore, in MuseGAN, we adopt a similar approach \nand use a critic instead of a discriminator. The critic is not a binary classifier; rather, it \nevaluates the output of the generator (in this case, a music piece) and assigns a score \nranging from –∞ to ∞. A higher score indicates a greater likelihood that the music is \nreal (i.e., from the training set).\nWe construct a music critic neural network as shown in the following listing, and its \ndefinition can be found in the file MuseGAN_util.py that you downloaded earlier.\nListing 13.3    The critic network in MuseGAN\nclass MuseCritic(nn.Module):\n    def __init__(self,hid_channels=128,hid_features=1024,\n        out_features=1,n_tracks=4,n_bars=2,n_steps_per_bar=16,\n        n_pitches=84):\n        super().__init__()\n        self.n_tracks = n_tracks\n        self.n_bars = n_bars\n        self.n_steps_per_bar = n_steps_per_bar\n        self.n_pitches = n_pitches\n        in_features = 4 * hid_channels if n_bars == 2\\n            else 12 * hid_channels\n        self.seq = nn.Sequential(\n            nn.Conv3d(self.n_tracks, hid_channels, \n                      (2, 1, 1), (1, 1, 1), padding=0),    Passes the input \nthrough several \nConv3d layers\n308 chapter  13 Music generation with MuseGAN \n            nn.LeakyReLU(0.3, inplace=True),\n            nn.Conv3d(hid_channels, hid_channels, \n              (self.n_bars - 1, 1, 1), (1, 1, 1), padding=0),\n            nn.LeakyReLU(0.3, inplace=True),\n            nn.Conv3d(hid_channels, hid_channels, \n                      (1, 1, 12), (1, 1, 12), padding=0),\n            nn.LeakyReLU(0.3, inplace=True),\n            nn.Conv3d(hid_channels, hid_channels, \n                      (1, 1, 7), (1, 1, 7), padding=0),\n            nn.LeakyReLU(0.3, inplace=True),\n            nn.Conv3d(hid_channels, hid_channels, \n                      (1, 2, 1), (1, 2, 1), padding=0),\n            nn.LeakyReLU(0.3, inplace=True),\n            nn.Conv3d(hid_channels, hid_channels, \n                      (1, 2, 1), (1, 2, 1), padding=0),\n            nn.LeakyReLU(0.3, inplace=True),\n            nn.Conv3d(hid_channels, 2 * hid_channels, \n                      (1, 4, 1), (1, 2, 1), padding=(0, 1, 0)),\n            nn.LeakyReLU(0.3, inplace=True),\n            nn.Conv3d(2 * hid_channels, 4 * hid_channels,     \n                      (1, 3, 1), (1, 2, 1), padding=(0, 1, 0)),\n            nn.LeakyReLU(0.3, inplace=True),\n            nn.Flatten(),    \n            nn.Linear(in_features, hid_features),\n            nn.LeakyReLU(0.3, inplace=True),\n            nn.Linear(hid_features, out_features))    \n    def forward(self, x):  \n        return self.seq(x)\nThe input to the critic network is a music piece with dimensions (4, 2, 16, 84). The \nnetwork primarily consists of several Conv3d layers. These layers treat each track of the \nmusic piece as a 3D object and apply filters to extract spatial features. The operation \nof the Conv3d layers is similar to the Conv2d layers used in image generation, as dis -\ncussed in earlier chapters.\nIt’s important to note that the final layer of the critic model is linear, and we do not \napply any activation function to its output. As a result, the output from the critic model \nis a value ranging from –∞ to ∞, which can be interpreted as the critic’s rating of a music \npiece.\n13.4.2  A generator in MuseGAN\nAs discussed earlier in this chapter, the generator will produce one bar of music at a \ntime, and we will then combine these eight bars to form a complete piece of music.\nInstead of using just a single noise vector, the generator in MuseGAN takes four inde -\npendent noise vectors as input to control various aspects of the music being generated. \nTwo of these vectors will be processed through a temporal network to extend them \nalong the bar dimension. While the style and groove vectors are designed to remain \nconstant across both bars, the chords and melody vectors are designed to vary between \nbars. Therefore, we will first establish a temporal network to stretch the chords and mel -\nody vectors across the two bars, ensuring that the generated music has a coherent and \nlogical progression over time.Flattens the output\nPasses the output \nthrough two linear layers\n 309 Building a MuseGAN\nIn the local module MuseGAN_util  you downloaded earlier, we define the \nTemporal  Network() class as follows:\nclass TemporalNetwork(nn.Module):\n    def __init__(self,z_dimension=32,hid_channels=1024,n_bars=2):\n        super().__init__()\n        self.n_bars = n_bars\n        self.net = nn.Sequential(\n            Reshape(shape=[z_dimension, 1, 1]),    \n            nn.ConvTranspose2d(z_dimension,hid_channels,\n                kernel_size=(2, 1),stride=(1, 1),padding=0,),\n            nn.BatchNorm2d(hid_channels),\n            nn.ReLU(inplace=True),\n            nn.ConvTranspose2d(hid_channels,z_dimension,\n                kernel_size=(self.n_bars - 1, 1),stride=(1, 1),\n                padding=0,),\n            nn.BatchNorm2d(z_dimension),\n            nn.ReLU(inplace=True),\n            Reshape(shape=[z_dimension, self.n_bars]),)    \n    def forward(self, x):\n        return self.net(x)\nThe TemporalNetwork()  class described here employs two ConvTranspose2d layers to \nexpand a single noise vector into two distinct noise vectors, each corresponding to one \nof the two bars. As we covered in chapter 4, transposed convolutional layers serve the \npurpose of upsampling and generating feature maps. In this context, they are utilized \nto extend noise vectors across different bars.\nInstead of generating all bars in all tracks at once, we’ll generate the music one bar at \na time. Doing so allows MuseGAN to balance computational efficiency, flexibility, and \nmusical coherence, resulting in more structured and appealing musical compositions. \nTherefore, we proceed to construct a bar generator that is responsible for generating a \nsegment of the music piece: one bar within a track. We introduce the BarGenerator()  \nclass within the local MuseGAN_util  module:\nclass BarGenerator(nn.Module):\n    def __init__(self,z_dimension=32,hid_features=1024,hid_channels=512,\n        out_channels=1,n_steps_per_bar=16,n_pitches=84):\n        super().__init__()\n        self.n_steps_per_bar = n_steps_per_bar\n        self.n_pitches = n_pitches\n        self.net = nn.Sequential(\n            nn.Linear(4 * z_dimension, hid_features),    \n            nn.BatchNorm1d(hid_features),\n            nn.ReLU(inplace=True),\n            Reshape(shape=[hid_channels,hid_features//hid_channels,1]),    \n            nn.ConvTranspose2d(hid_channels,hid_channels,\n               kernel_size=(2, 1),stride=(2, 1),padding=0),    \n            nn.BatchNorm2d(hid_channels),\n            nn.ReLU(inplace=True),\n            nn.ConvTranspose2d(hid_channels,hid_channels // 2,\n                kernel_size=(2, 1),stride=(2, 1),padding=0),\n            nn.BatchNorm2d(hid_channels // 2),\n            nn.ReLU(inplace=True),The input dimension to \nthe TemporalNetwork() \nclass is ( 1, 32).\nThe output \ndimension is (2, 32).\nWe concatenate \nchords, style, \nmelody, and groove \ninto one vector, with \na size of 4 * 32.\nThe input is then \nreshaped into 2D, \nand we use several \nConvTranspose2d \nlayers for \nupsampling and \nmusic feature \ngeneration.\n310 chapter  13 Music generation with MuseGAN \n            nn.ConvTranspose2d(hid_channels // 2,hid_channels // 2,\n                kernel_size=(2, 1),stride=(2, 1),padding=0),\n            nn.BatchNorm2d(hid_channels // 2),\n            nn.ReLU(inplace=True),\n            nn.ConvTranspose2d(hid_channels // 2,hid_channels // 2,\n                kernel_size=(1, 7),stride=(1, 7),padding=0),\n            nn.BatchNorm2d(hid_channels // 2),\n            nn.ReLU(inplace=True),\n            nn.ConvTranspose2d(hid_channels // 2,out_channels,\n                kernel_size=(1, 12),stride=(1, 12),padding=0),\n            Reshape([1, 1, self.n_steps_per_bar, self.n_pitches]))    \n    def forward(self, x):\n        return self.net(x)\nThe BarGenerator()  class accepts four noise vectors as input, each representing \nchords, style, melody, and groove for a specific bar in a different track, all with a shape \nof (1, 32). These vectors are concatenated into a single 128-value vector before being \nfed into the BarGenerator()  class. The output from the BarGenerator()  class is a \nbar of music, with dimensions (1, 1, 16, 84), indicating 1 track, 1 bar, and 16 notes, with \neach note represented by an 84-value vector.\nFinally, we will employ the MuseGenerator()  class to generate a complete piece of \nmusic, consisting of four tracks with two bars per track. Each bar is constructed using the \nBarGenerator()  class defined earlier. To achieve this, we define the MuseGenerator()  \nclass in the local MuseGAN_util module.\nListing 13.4    The music generator in MuseGAN\nclass MuseGenerator(nn.Module):\n    def __init__(self,z_dimension=32,hid_channels=1024,\n        hid_features=1024,out_channels=1,n_tracks=4,\n        n_bars=2,n_steps_per_bar=16,n_pitches=84):\n        super().__init__()\n        self.n_tracks = n_tracks\n        self.n_bars = n_bars\n        self.n_steps_per_bar = n_steps_per_bar\n        self.n_pitches = n_pitches\n        self.chords_network=TemporalNetwork(z_dimension, \n                            hid_channels, n_bars=n_bars)\n        self.melody_networks = nn.ModuleDict({})\n        for n in range(self.n_tracks):\n            self.melody_networks.add_module(\n                ""melodygen_"" + str(n),\n                TemporalNetwork(z_dimension, \n                 hid_channels, n_bars=n_bars))\n        self.bar_generators = nn.ModuleDict({})\n        for n in range(self.n_tracks):\n            self.bar_generators.add_module(\n                „bargen_"" + str(n),BarGenerator(z_dimension,\n            hid_features,hid_channels // 2,out_channels,\n            n_steps_per_bar=n_steps_per_bar,n_pitches=n_pitches))The output has a shape of ( 1, 1, \n16, 84): 1 track, 1 bar, and 16 \nnotes, and each note is \nrepresented by a 84-value vector.",11857
130-13.5.1 Training the MuseGAN.pdf,130-13.5.1 Training the MuseGAN,"311 Building a MuseGAN\n    def forward(self,chords,style,melody,groove):\n        chord_outs = self.chords_network(chords)\n        bar_outs = []\n        for bar in range(self.n_bars):    \n            track_outs = []\n            chord_out = chord_outs[:, :, bar]\n            style_out = style\n            for track in range(self.n_tracks):    \n                melody_in = melody[:, track, :]\n                melody_out = self.melody_networks[""melodygen_""\\n                          + str(track)](melody_in)[:, :, bar]\n                groove_out = groove[:, track, :]\n                z = torch.cat([chord_out, style_out, melody_out,\\n                               groove_out], dim=1)    \n                track_outs.append(self.bar_generators[""bargen_""\\n                                          + str(track)](z))    \n            track_out = torch.cat(track_outs, dim=1)\n            bar_outs.append(track_out)\n        out = torch.cat(bar_outs, dim=2)    \n        return out\nThe generator takes four noise vectors as inputs. It iterates through four tracks and \ntwo bars. In each iteration, it utilizes the bar generator to create a single bar of music. \nUpon completing all iterations, the MuseGenerator() class merges the eight bars into \none cohesive music piece, which has dimensions of (4, 2, 16, 84). \n13.4.3  Optimizers and the loss function\nWe create a generator and a critic based on the MuseGenerator()  and MuseCritic()  \nclasses in the local module:\nimport torch\nfrom utils.MuseGAN_util import (init_weights, MuseGenerator, MuseCritic)\ndevice = ""cuda"" if torch.cuda.is_available() else ""cpu""\ngenerator = MuseGenerator(z_dimension=32, hid_channels=1024, \n              hid_features=1024, out_channels=1).to(device)\ncritic = MuseCritic(hid_channels=128,\n                    hid_features=1024,\n                    out_features=1).to(device)\ngenerator = generator.apply(init_weights)\ncritic = critic.apply(init_weights) \nAs we discussed in chapter 5, the critic generates a rating instead of a classification, so \nthe loss function is defined as the negative average of the product between the pre -\ndiction and the target. As a result, we define the following loss_fn()  function in the \nlocal module MuseGAN_util :\ndef loss_fn(pred, target):\n    return -torch.mean(pred*target)\nDuring training, for the generator, we’ll assign a value of 1 to the target argument in \nthe loss_fn()  function. This setting aims to guide the generator in producing music Iterates through \ntwo bars\nIterates through \nfour tracks\nConcatenates \nchords, style, \nmelody, and \ngroove into \none input\nGenerates one \nbar using the \nbar generator\nConcatenates eight bars into \none complete piece of music\n312 chapter  13 Music generation with MuseGAN \nthat can achieve the highest possible rating (i.e., the variable pred in the loss_fn()  \nfunction). For the critic, we’ll set the target to 1 for real music and –1 for fake music in \nthe loss function. This setting guides the critic to assign a high rating to real music and \na low rating to fake music.\nSimilar to the approach in chapter 5, we incorporate the Wasserstein distance with a \ngradient penalty into the critic’s loss function to ensure training stability. The gradient \npenalty is defined in the MuseGAN_util.py  file as follows:\nclass GradientPenalty(nn.Module):\n    def __init__(self):\n        super().__init__()\n    def forward(self, inputs, outputs):\n        grad = torch.autograd.grad(\n            inputs=inputs,\n            outputs=outputs,\n            grad_outputs=torch.ones_like(outputs),\n            create_graph=True,\n            retain_graph=True,\n        )[0]\n        grad_=torch.norm(grad.view(grad.size(0),-1),p=2,dim=1)\n        penalty = torch.mean((1. - grad_) ** 2)\n        return penalty\nThe GradientPenalty()  class requires two inputs: interpolated music, which is a \nblend of real and fake music, and the ratings assigned by the critic network to this \ninterpolated music. The class computes the gradient of the critic’s ratings concerning \nthe interpolated music. The gradient penalty is then calculated as the squared differ -\nence between the norms of these gradients and the target value of 1, following a similar \napproach to what we did in chapter 5.\nAs usual, we’ll use the Adam optimizer for both the critic and the generator:\nlr = 0.001\ng_optimizer = torch.optim.Adam(generator.parameters(),\n                               lr=lr, betas=(0.5, 0.9))\nc_optimizer = torch.optim.Adam(critic.parameters(),\n                               lr=lr, betas=(0.5, 0.9))\nWith that, we have successfully constructed a MuseGAN, which is now ready to be \ntrained using the data we prepared earlier in the chapter. \n13.5 Training the MuseGAN to generate music\nNow that we have both the MuseGAN model and the training data, we’ll proceed to \ntrain the model in this section.\nSimilar to our approach in chapters 3 and 4, when training GANs, we’ll alternate \nbetween training the critic and the generator. In each training iteration, we’ll sample a \nbatch of real music from the training dataset and a batch of generated music from the \ngenerator and present them to the critic for evaluation. During critic training, we com -\npare the critic’s ratings with the ground truth and adjust the critic network’s weights \nslightly so that, in the next iteration, the ratings will be as high as possible for real music \n 313 Training the MuseGAN to generate music\nand as low as possible for generated music. During generator training, we feed gener -\nated music to the critic model to obtain a rating and then slightly adjust the generator \nnetwork’s weights so that, in the next iteration, the rating will be higher (as the genera -\ntor aims to create music pieces that fool the critic into thinking they are real). We repeat \nthis process for many iterations, gradually enabling the generator network to create \nmore realistic music pieces.\nOnce the model is trained, we’ll discard the critic network and use the trained gen -\nerator to create music pieces by feeding it four noise vectors (chords, style, melody, and \ngroove). \n13.5.1  Training the MuseGAN\nBefore we embark on the training loops for the MuseGAN model, we first define a \nfew hyperparameters and helper functions. The hyperparameter repeat  controls how \nmany times we train the critic in each iteration, display_step  specifies how often we \ndisplay output, and epochs  is the number of epochs we train the model.\nListing 13.5    Hyperparameters and helper functions\nfrom utils.MuseGAN_util import loss_fn, GradientPenalty\nbatch_size=64\nrepeat=5\ndisplay_step=10\nepochs=500    \nalpha=torch.rand((batch_size,1,1,1,1)).requires_grad_().to(device)    \ngp=GradientPenalty()    \ndef noise():    \n    chords = torch.randn(batch_size, 32).to(device)\n    style = torch.randn(batch_size, 32).to(device)\n    melody = torch.randn(batch_size, 4, 32).to(device)\n    groove = torch.randn(batch_size, 4, 32).to(device)\n    return chords,style,melody,groove\nThe batch size is set at 64, and this helps us determine how many sets of random noise \nvectors to retrieve to create a batch of fake music. We’ll train the critic for five itera -\ntions and the generator just once in each training loop because an effective critic is \nessential for training the generator. We’ll display training losses after every 10 epochs. \nWe’ll train the model for 500 epochs.  \nWe instantiate the GradientPenalty()  class in the local module to create a gp() \nfunction to calculate the gradient penalty. We also define a noise()  function to gener -\nate four random noise vectors to feed to the generator. \nNext, we define the following function, train_epoch() , to train the model for one \nepoch.Defines a few \nhyperparameters\nDefines alpha to create \ninterpolated music\nDefines a gp() \nfunction to calculate \ngradient penalty\nDefines a noise() \nfunction to retrieve \nfour random noise \nvectors\n314 chapter  13 Music generation with MuseGAN \nListing 13.6    Training the MuseGAN model for one epoch\ndef train_epoch():\n    e_gloss = 0\n    e_closs = 0\n    for real in loader:    \n        real = real.to(device)\n        for _ in range(repeat):    \n            chords,style,melody,groove=noise()\n            c_optimizer.zero_grad()\n            with torch.no_grad():\n                fake = generator(chords, style, melody,groove).detach()\n            realfake = alpha * real + (1 - alpha) * fake\n            fake_pred = critic(fake)\n            real_pred = critic(real)\n            realfake_pred = critic(realfake)\n            fake_loss =  loss_fn(fake_pred,-torch.ones_like(fake_pred))\n            real_loss = loss_fn(real_pred,torch.ones_like(real_pred))\n            penalty = gp(realfake, realfake_pred)\n            closs = fake_loss + real_loss + 10 * penalty    \n            closs.backward(retain_graph=True)\n            c_optimizer.step()\n            e_closs += closs.item() / (repeat*len(loader))\n        g_optimizer.zero_grad()\n        chords,style,melody,groove=noise()\n        fake = generator(chords, style, melody, groove)\n        fake_pred = critic(fake)\n        gloss = loss_fn(fake_pred, torch.ones_like(fake_pred))    \n        gloss.backward()\n        g_optimizer.step()\n        e_gloss += gloss.item() / len(loader)\n    return e_gloss, e_closs \nThe training process is very much like that we used in chapter 5 when we train the con -\nditional GAN with gradient penalty.\nWe now train the model for 500 epochs:\nfor epoch in range(1,epochs+1):\n    e_gloss, e_closs = train_epoch()\n    if epoch % display_step == 0:\n        print(f""Epoch {epoch}, G loss {e_gloss} C loss {e_closs}"")\nIf you use GPU training, it takes about an hour. Otherwise, it may take several hours. \nOnce done, you can save the trained generator to the local folder as follows:\ntorch.save(generator.state_dict(),'files/MuseGAN_G.pth')\nAlternatively, you can download the trained generator from my website: https: //mng  \n.bz/Bglr .\nNext, we’ll discard the critic network and use the trained generator to create music \nthat mimics the style of Bach. Iterates through \nall batches\nTrains the critic five \ntimes in each iteration\nThe total loss \nfor the critic \nhas three \ncomponents: \nloss from \nevaluating real \nmusic, loss \nfrom \nevaluating fake \nmusic, and the \ngradient \npenalty loss.\nTrains the \ngenerator",10482
131-13.5.2 Generating music with the trained MuseGAN.pdf,131-13.5.2 Generating music with the trained MuseGAN,"315 Training the MuseGAN to generate music\n13.5.2  Generating music with the trained MuseGAN\nTo generate music with the trained generator, we’ll feed four noise vectors from the \nlatent space to the generator. Note that we can generate multiple music objects at the \nsame time and decode them together to form one continuous piece of music. You’ll \nlearn how to do that in this subsection.  \nWe first load the trained weights in the generator:\ngenerator.load_state_dict(torch.load('files/MuseGAN_G.pth',\n    map_location=device))\nRather than producing a single 4D music object, we can simultaneously generate mul -\ntiple 4D music objects and convert them into one continuous piece of music later. For \ninstance, if we aim to create five music objects, we begin by sampling five sets of noise \nvectors from the latent spaces. Each set consists of four vectors: chords, style, melody, \nand groove, like so:\nnum_pieces = 5\nchords = torch.rand(num_pieces, 32).to(device)\nstyle = torch.rand(num_pieces, 32).to(device)\nmelody = torch.rand(num_pieces, 4, 32).to(device)\ngroove = torch.rand(num_pieces, 4, 32).to(device)\nEach generated music object can be transformed into a music piece that lasts approx -\nimately 8 seconds. In this case, we choose to generate five music objects and decode \nthem into a single music piece later, resulting in a duration of about 40 seconds. You \ncan adjust the value of the variable num_pieces  according to your preference, depend -\ning on the desired length of the music piece.\nNext, we supply the generator with the five sets of latent variables to produce a set of \nmusic objects:\npreds = generator(chords, style, melody, groove).detach()\nThe output, preds , consists of five music objects. Next, we decode these objects into a \nsingle piece of music, represented as a MIDI file:\nfrom utils.midi_util import convert_to_midi\nmusic_data = convert_to_midi(preds.cpu().numpy())\nmusic_data.write('midi', 'files/MuseGAN_song.midi')\nWe import the convert_to_midi()  function from the local module midi_util . \nOpen the file midi_util.py  that you downloaded earlier and review the definition of \nthe convert_to_midi()  function. This process is similar to what we have done earlier \nin this chapter when we converted the first music object in the training set into the file \nfirst_song.midi . Since MIDI files represent sequences of notes over time, we simply \nconcatenate the five music pieces corresponding to the five music objects into one \nextended sequence of notes. This combined sequence is then saved as MuseGAN_song  \n.midi  on your computer.\n316 chapter  13 Music generation with MuseGAN \nFind the generated music piece, MuseGAN_song.midi , on your computer. Open it \nwith a music player of your choice and listen to see if it resembles the music pieces \nfrom the training set. For comparison, you can listen to a piece of music generated by \nthe trained model on my website at https: //mng.bz/dZJv . Note that since the input to \nthe generator, the noise vectors, are randomly drawn from the latent space, the music \npieces you generate will sound different. \nExercise 13.2\nObtain three sets of random noise vectors (each set should contain chords, style, melody, \nand groove) from the latent space. Feed them to the trained generator to obtain three \nmusic objects. Decode them into one single piece of music in the form of a MIDI file. Save \nit as generated_song.midi  on your computer, and play it using a music player.  \nIn this chapter, you’ve learned how to build and train a MuseGAN to generate music \nin the style of Bach. Specifically, you’ve approached a piece of music as a 4D object and \napplied the techniques from chapter 4 on deep convolutional layers to develop a GAN \nmodel. In the next chapter, you’ll explore a different way of generating music: treating \na piece of music as a sequence of indexes and utilizing techniques from NLP to gener -\nate music pieces by predicting one index at a time.\nSummary\n¡ MuseGAN treats a piece of music as a multidimensional object akin to an image. \nThe generator produces a piece of music and submits it, along with real music \npieces from the training set, to the critic for evaluation. The generator then mod -\nifies the music based on the critic’s feedback until it closely resembles real music \nfrom the training dataset. \n¡ Musical notes, octaves, and pitch are fundamental concepts in music theory. \nOctaves represent different levels of musical sound. Each octave is subdivided \ninto 12 semitones: C, C#, D, D#, E, F, F#, G, G#, A, A#, B. Within an octave, a note \nis assigned a specific pitch number. \n¡ In electronic music production, a track typically refers to an individual layer or \ncomponent of the music. Each track contains multiple bars (or measures). A bar \nis further divided into multiple steps.\n¡ To represent a piece of music as a multidimensional object, we structure it with a \n(4, 2, 16, 84) shape: 4 music tracks, with each track consisting of 2 bars, each bar \ncontaining 16 steps, and each step capable of playing 1 of the 84 different notes.\n¡ In music creation, incorporating more detailed inputs is essential for achieving \ngreater control and variety. Instead of using a single noise vector from the latent \nspace for generating shapes, numbers, and images as in previous chapters, we \n 317 Summary\nemploy four distinct noise vectors in the music generation process. Given that \neach music piece consists of four tracks and two bars, we use these four vectors to \neffectively manage this structure. One vector controls all tracks and bars collec -\ntively, another controls each bar across all tracks, a third oversees all tracks across \nbars, and the fourth manages each individual bar in each track.",5805
132-14 Building and training a music Transformer.pdf,132-14 Building and training a music Transformer,"31814Building and training a \nmusic Transformer\nThis chapter covers\n¡ Representing music with control messages and  \n velocity values\n¡ Tokenizing music into a sequence of indexes\n¡ Building and training a music Transformer \n¡ Generating musical events using the trained   \n Transformer\n¡ Converting musical events back to a playable MIDI  \n file\nSad that your favorite musician is no longer with us? Sad no more: generative AI can \nbring them back to the stage!\nTake, for example, Layered Reality, a London-based company that’s working on \na project called Elvis Evolution.1 The goal? To resurrect the legendary Elvis Presley \nusing AI. By feeding a vast array of Elvis’ official archival material, including video \nclips, photographs, and music, into a sophisticated computer model, this AI Elvis \n1 Chloe Veltman, March 15, 2024. “Just because your favorite singer is dead doesn’t mean you can’t see them ‘live.’” \nhttps: //mng.bz/r1de . \n 319  \nlearns to mimic his singing, speaking, dancing, and walking with remarkable resem -\nblance. The result? A digital performance that captures the essence of the late King \nhimself.\nThe Elvis Evolution project is a shining example of the transformative effect of gen -\nerative AI across various industries. In the previous chapter, you explored the use of \nMuseGAN to create music that could pass as authentic multitrack compositions. Muse -\nGAN views a piece of music as a multidimensional object, similar to an image, and gen -\nerates complete music pieces that resemble those in the training dataset. Both real and \nAI-generated music are then evaluated by a critic, which helps refine the AI-generated \nmusic until it’s indistinguishable from the real thing.\nIn this chapter, you’ll take a different approach to AI music creation, treating it as a \nsequence of musical events. We’ll apply techniques from text generation, as discussed \nin chapters 11 and 12, to predict the next element in a sequence. Specifically, you’ll \ndevelop a GPT-style model to predict the next musical event based on all previous \nevents in the sequence. GPT-style Transformers are ideal for this task because of their \nscalability and the self-attention mechanism, which helps them capture long-range \ndependencies and understand context. This makes them highly effective for sequence \nprediction and generation across a wide range of content, including music. The music \nTransformer you will create has 20.16 million parameters, large enough to capture the \nlong-term relations of different notes in music pieces but small enough to be trained in \na reasonable amount of time. \nWe’ll use the Maestro piano music from Google’s Magenta group as our training \ndata. You’ll learn how to first convert a musical instrument digital interface (MIDI) file \ninto a sequence of music notes, analogous to raw text data in natural language process -\ning (NLP). You’ll then break the musical notes down into small pieces called musical \nevents, analogous to tokens in NLP. Since neural networks can only accept numerical \ninputs, you’ll map each unique event token to an index. With this, the music pieces in \nthe training data are converted into sequences of indexes, ready to be fed into neural \nnetworks. \nTo train the music Transformer to predict the next token based on the current token \nand all previous tokens in the sequence, we’ll create sequences of 2,048 indexes as \ninputs (features x). We then shift the sequences one index to the right and use them \nas the outputs (targets y). We feed pairs of (x, y) to the music Transformer to train the \nmodel. Once trained, we’ll use a short sequence of indexes as the prompt and feed it \nto the music Transformer to predict the next token, which is then appended to the \nprompt to form a new sequence. This new sequence is fed back into the model for \nfurther predictions, and this process is repeated until the sequence reaches a desired \nlength.\nYou’ll see that the trained music Transformer can generate lifelike music that mim -\nics the style in the training dataset. Further, unlike the music generated in chapter 13, \nyou’ll learn to control the creativity of the music piece. You’ll achieve this by scaling \nthe predicted logits with the temperature parameter, just as you did in earlier chapters \nwhen controlling the creativity of the generated text. chapter  14 Building and training a music Transformer",4444
133-14.1 Introduction to the music Transformer.pdf,133-14.1 Introduction to the music Transformer,,0
134-14.1.1 Performance-based music representation.pdf,134-14.1.1 Performance-based music representation,"320 chapter  14 Building and training a music Transformer\n14.1 Introduction to the music Transformer\nThe concept of the music Transformer was introduced in 2018.2 This innovative \napproach extends the Transformer architecture, initially devised for NLP tasks, to the \nfield of music generation. As discussed in previous chapters, Transformers employ \nself-attention mechanisms to effectively grasp the context and capture long-range \ndependencies among elements in a sequence.\nIn a similar vein, the music Transformer is engineered to generate a sequence of \nmusical notes by learning from a vast dataset of existing music. The model is trained to \npredict the next musical event in a sequence based on preceding events by understand -\ning the patterns, structures, and relationships between different musical elements in \nthe training data.\nA crucial step in training a music Transformer lies in figuring out how to rep -\nresent music as a sequence of unique musical events, akin to tokens in NLP. In the \nprevious chapter, you learned to represent a piece of music as a 4D object. In this \nchapter, you will explore an alternative approach to music representation, specifically \nperformance-based music representation through control messages and velocity val -\nues.3 Based on this, you will convert a piece of music into four types of musical events: \nnote-on, note-off, time-shift, and velocity.  \nNote-on signals the start of a musical note being played, specifying the note’s pitch. \nNote-off indicates the end of a note, telling the instrument to stop playing that note. \nTime-shift represents the amount of time that passes between two musical events. Veloc -\nity measures the force or speed with which a note is played, with higher values corre -\nsponding to a stronger, louder sound. Each type of musical event has many different \nvalues. Each unique event will be mapped to a different index, effectively transforming \na piece of music into a sequence of indexes. You will then apply the GPT models, as dis -\ncussed in chapters 11 and 12, to create a decoder-only music Transformer that predicts \nthe next musical event in the sequence.\nIn this section, you will begin by learning about performance-based music repre -\nsentation through control messages and velocity values. You will then explore how to \nrepresent music pieces as sequences of musical events. Finally, you will learn the steps \ninvolved in building and training a Transformer to generate music.\n14.1.1  Performance-based music representation\nPerformance-based music representation is often achieved using the MIDI format, \nwhich captures the nuances of a musical performance through control messages and \nvelocity values. In MIDI, musical notes are represented by note-on and note-off mes -\nsages, which include information about the pitch and velocity of each note.\n2 Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit, Noam Shazeer, Ian Simon, Curtis Hawthorne, Andrew M. \nDai, Matthew D. Hoffman, Monica Dinculescu, and Douglas Eck, 2018, “Music Transformer.” https: //arxiv.org/\nabs/1809.04281 .\n3 See, for example, Hawthorne et al., 2018, “Enabling Factorized Piano Music Modeling and Generation with the \nMAESTRO Dataset.” https: //arxiv.org/abs/1810.12247 .\n 321 Introduction to the music Transformer\nAs we discussed in chapter 13, the pitch value ranges from 0 to 127, with each value \ncorresponding to a semitone in an octave level. For instance, the pitch value 60 corre -\nsponds to a C4 note, while the pitch value 74 corresponds to a D5 note. The velocity \nvalue, also ranging from 0 to 127, represents the dynamics of the note, with higher \nvalues indicating louder or more forceful playing. By combining these control mes -\nsages and velocity values, a MIDI sequence can capture the expressive details of a live \nperformance, allowing for expressive playback through MIDI-compatible instruments \nand software.\nTo give you a concrete example of how a piece of music can be represented by con -\ntrol messages and velocity values, consider the five notes shown in the following listing.\nListing 14.1    Example notes in a performance-based music representation \n<[SNote] time: 1.0325520833333333 type: note_on, value: 74, velocity: 86>\n<[SNote] time: 1.0442708333333333 type: note_on, value: 38, velocity: 77>\n<[SNote] time: 1.2265625 type: note_off, value: 74, velocity: None>\n<[SNote] time: 1.2395833333333333 type: note_on, value: 73, velocity: 69>\n<[SNote] time: 1.2408854166666665 type: note_on, value: 37, velocity: 64>\nThese are the first five notes from a piece of music in the training dataset you’ll use in \nthis chapter. The first note has a timestamp of approximately 1.03 seconds, with a note \nof pitch value 74 (D5) starting to play at a velocity of 86. Looking at the second note, \nyou can infer that after about 0.01 seconds (since the timestamp is now 1.04 seconds), \na note with a pitch value of 38 starts to play at a velocity of 77, and so on.\nThese musical notes are similar to raw text in NLP; we cannot directly feed them to \na music Transformer to train the model. We first need to “tokenize” the notes and then \nconvert the tokens to indexes before feeding them to the model.\nTo tokenize the musical notes, we’ll represent the music using increments of 0.01 \nseconds to reduce the number of time steps in the music piece. Additionally, we’ll sep -\narate control messages from velocity values and treat them as different elements of the \nmusic piece. Specifically, we’ll represent music using a combination of note-on, note-\noff, time-shift, and velocity events. Once we do that, the preceding five musical notes \ncan be represented by the following events (some events are omitted for brevity).\nListing 14.2    Tokenized representation of a piece of music\n<Event type: time_shift, value: 99>, \n <Event type: time_shift, value: 2>, \n <Event type: velocity, value: 21>, \n <Event type: note_on, value: 74>, \n <Event type: time_shift, value: 0>, \n <Event type: velocity, value: 19>, \n <Event type: note_on, value: 38>, \n <Event type: time_shift, value: 17>, \n <Event type: note_off, value: 74>, \n <Event type: time_shift, value: 0>, \n <Event type: velocity, value: 17>, \n <Event type: note_on, value: 73>, \n322 chapter  14 Building and training a music Transformer\n <Event type: velocity, value: 16>, \n <Event type: note_on, value: 37>, \n <Event type: time_shift, value: 0>\n…\nWe’ll count time shifts in increments of 0.01 seconds and tokenize time shifts from \n0.01 seconds to 1 second with 100 different values. Thus, time-shift events are toke -\nnized into 1 of 100 unique event tokens: a value of 0 indicates a time lapse of 0.01 \nseconds, 1 indicates a time lapse of 0.02 seconds, and so on, up to 99, which indicates \na time lapse of 1 second. If a time-shift lasts more than 1 second, you can use multiple \ntime-shift tokens to indicate it. For example, the first two tokens in listing 14.2 are both \ntime-shift tokens, with values 99 and 2, respectively, indicating time lapses of 1 second \nand 0.03 seconds. This matches the timestamp of the first musical note in listing 14.1: \n1.0326 seconds.\nListing 14.2 also shows that velocity is a separate type of musical event. We place the \nvalue of velocity into 32 equally spaced bins, converting the original velocity values, \nwhich range from 0 to 127, into 1 of 32 values, ranging from 0 to 31. This is why the orig -\ninal velocity value of 86 in the first note in listing 14.1 is now represented as a velocity \nevent with a value of 21 in listing 14.2 (the number 86 falls into the 22nd bin, and Python \nuses zero-based indexing).\nTable 14.1 shows the meaning of four types of different tokenized events, their value \nranges, and the meaning of each event token.\nTable 14.1    Meanings of different event tokens \nEvent token typeEvent token \nvalue rangeMeaning of the event tokens\nnote_on 0–127 Starting to play at a certain pitch value. For example, note_on \nwith value 74 means starting to play note D5.\nnote_off 0–127 Releasing a certain note. For example, note_off  with value 60 \nmeans to stop playing note C4.\ntime_shift 0–99 The time_shift  values are increments of 0.01 seconds. For \nexample, 0 indicates 0.01 seconds, 2 indicates 0.03 seconds, and \n99 indicates 1 second.\nvelocity 0–31 The original velocity values are placed into 32 bins. The bin value \nis used. For example, an original velocity value of 86 now has a \ntokenized value of 21.\nSimilar to the approach taken in NLP, we’ll convert each unique token into an index \nso that we can input the data into neural networks. According to table 14.1, there are \n128 unique note-on event tokens, 128 note-off event tokens, 32 velocity event tokens, \nand 100 time-shift event tokens. This results in a total of 128 + 128 + 32 + 100 = 388 \nunique tokens. Consequently, we convert these 388 unique tokens into indexes rang -\ning from 0 to 387, based on the mappings provided in table 14.2.\n 323 Introduction to the music Transformer\nTable 14.2    Mapping event tokens to indexes and indexes to event tokens\nToken type Index range Event token to index Index to event token\nnote_on 0–127 The value of the note_on  \ntoken. For example, the note_\non token with a value of 74 is \nassigned an index value of 74.If the index range is 0 to 127, \nset token type to note_on  and \nvalue to the index value. For \nexample, the index value 63 is \nmapped to a note_on  token \nwith a value of 63.\nnote_off 128–255 128 plus the value of the note_\noff  token. For example, the \nnote_off  token with a value \nof 60 is assigned an index value \nof 188 (since 128 + 60 = 188).If the index range is 128 to 255, \nset token type to note_off  \nand value to index minus 128. \nFor example, index 180 is \nmapped to the note_off  \ntoken with value 52.\ntime_shift 256–355 256 plus the value of the time_\nshift  token. For example, the \ntime_shift  token with a \nvalue of 16 is assigned an index \nvalue of 272 (since 256 + 16 = \n272).If the index range is 256 to 355, \nset token type to time_shift  \nand value to index minus 256. \nFor example, index 288 is \nmapped to the time_shift \ntoken with value 32.\nvelocity 356–387 356 plus the value of the velocity \ntoken. For example, the veloc -\nity token with a value of 21 is \nassigned an index value of 377 \n(since 356+21=377).If the index range is 356 to 387, \nset token type to velocity  \nand value to index minus 356. \nFor example, index 380 is \nmapped to the velocity  \ntoken with value 24.\nThe third column in table 14.2 outlines the conversion of event tokens to indexes. \nNote-on tokens are assigned index values ranging from 0 to 127, where the index value \ncorresponds to the pitch number in the token. Note-off tokens are assigned index val -\nues from 128 to 255, with the index value being 128 plus the pitch number. Time-shift \ntokens are assigned index values from 256 to 355, with the index value being 256 plus \nthe time-shift value. Lastly, velocity tokens are assigned index values from 356 to 387, \nwith the index value being 356 plus the velocity bin number.\nUsing this token-to-index mapping, we’ll convert each piece of music into a sequence \nof indexes. We’ll apply this conversion to all music pieces in the training dataset and use \nthe resulting sequences to train our music Transformer (the details of which will be \nexplained later). Once trained, we’ll use the Transformer to generate music in the form \nof a sequence of indexes. The final step is to convert this sequence back into MIDI for -\nmat so that we can play and enjoy the music on a computer.\nThe last column in table 14.2 provides guidance on converting indexes back to event \ntokens. We first determine the token type based on the range in which the index falls. \nThe four ranges in the second column of table 14.2 correspond to the four token types \nin the first column of the table. To obtain the value for each token type, we subtract \nthe index value by 0, 128, 256, and 356 for the four types of tokens, respectively. These",12150
135-14.1.2 The music Transformer architecture.pdf,135-14.1.2 The music Transformer architecture,"324 chapter  14 Building and training a music Transformer\ntokenized events are then converted into musical notes in MIDI format, ready to be \nplayed on a computer.  \n14.1.2  The music Transformer architecture\nIn chapter 9, we built an encoder-decoder Transformer, and in chapters 11 and 12, we \nfocused on decoder-only Transformers. Unlike language translation tasks where the \nencoder captures the meaning of the source language and passes it to the decoder for \ngenerating the translation, music generation does not require an encoder to under -\nstand a different language. Instead, the model generates subsequent event tokens \nbased on previous event tokens in the music sequence. Therefore, we’ll construct a \ndecoder-only Transformer for our music generation task.\nOur music Transformer, like other Transformer models, utilizes self-attention mech -\nanisms to capture the long-range dependencies among different musical events in a \npiece of music, thereby generating coherent and lifelike music. Although our music \nTransformer differs in size from the GPT models we built in chapters 11 and 12, it shares \nthe same core architecture. It follows the same structural design as GPT-2 models but is \nsignificantly smaller, making it feasible to train without the need for supercomputing \nfacilities.\nSpecifically, our music Transformer consists of 6 decoder layers with an embedding \ndimension of 512, meaning each token is represented by a 512-value vector after word \nembedding. Instead of using sine and cosine functions for positional encoding as in the \noriginal 2017 paper “Attention Is All You Need,” we use embedding layers to learn the \npositional encodings for different positions in a sequence. As a result, each position in a \nsequence is also represented by a 512-value vector. For calculating causal self-attention, \nwe use 8 parallel attention heads to capture different aspects of the meanings of a token \nin the sequence, giving each attention head a dimension of 64 (512/8).\nCompared to the vocabulary size of 50,257 in GPT-2 models, our model has a much \nsmaller vocabulary size of 390 (388 different event tokens, plus a token to signify the \nend of a sequence and a token to pad shorter sequences; I’ll explain later why padding \nis needed). This allows us to set the maximum sequence length in our music Trans -\nformer to 2,048, which is much longer than the maximum sequence length of 1,024 \nin GPT-2 models. This choice is necessary to capture the long-term relations of music \nnotes in a sequence. With these hyperparameter values, our music Transformer has a \nsize of 20.16 million parameters.\nFigure 14.1 illustrates the architecture of the music Transformer we will create in this \nchapter. It is similar to the architecture of the GPT models you built in chapters 11 and \n12. Figure 14.1 also shows the size of the training data as it passes through the model \nduring training.\nThe input to the music Transformer we constructed comprises input embeddings, \nas depicted at the bottom of figure 14.1. The input embedding is the sum of the word \nembedding and positional encoding of the input sequence. This input embedding is \nthen passed sequentially through six decoder blocks.\n 325 Introduction to the music Transformer\nNormalize & add \nFeed forward \nsize (512, 512) Six identical\ndecoder layersSecond sublayer of each\ndecoder layer:\nFeed forward with layer \nnormalization and residual\nconnection \nNormalize & add \nCausal self-attention \nsize (512, 512) First sublayer of each\ndecoder layer: \nCausal self-attention with layer \nnormalization and residual\nconnection \nWord embedding layer\nsize (512, 512)\nToken representation\nsize (2, 2048, 512)NormalizeLinear head \nsize (512, 390) Output\nSize (2, 2048, 390)\nPositional encoding layer \nsize (2048, 512)\nPositional representation  \nsize (2, 2048, 2048) Word embedding\nsize (2, 2048, 512)Positional encoding\nsize (2, 2048, 512)Size (2, 2048, 512)\nFigure 14.1    The architecture of a music Transformer. Music files in MIDI formats are first converted into \nsequences of musical events. These events are then tokenized and converted into indexes. We organize \nthese indexes into sequences of 2,048 elements, and each batch contains 2 such sequences. The input \nsequence first undergoes word embedding and positional encoding; the input embedding is the sum of \nthese two components. This input embedding is then processed through six decoder layers, each utilizing \nself-attention mechanisms to capture the relationships among different musical events in the sequence. \nAfter passing through the decoder layers, the output undergoes layer normalization to ensure stability \nin the training process. It then passes through a linear layer, resulting in an output size of 390, which \ncorresponds to the number of unique tokens in the vocabulary. This final output represents the predicted \nlogits for the next musical event in the sequence. \nAs discussed in chapters 11 and 12, each decoder layer consists of two sublayers: a \ncausal self-attention layer and a feed-forward network. In addition, we apply layer nor -\nmalization and residual connections to each sublayer to enhance the model’s stability \nand learning capability.\nAfter passing through the decoder layers, the output undergoes layer normalization \nand is then fed into a linear layer. The number of outputs in our model corresponds to \nthe number of unique musical event tokens in the vocabulary, which is 390. The output \nof the model is the logits for the next musical event token.",5611
136-14.2.2 Tokenizing MIDI files.pdf,136-14.2.2 Tokenizing MIDI files,"326 chapter  14 Building and training a music Transformer\nLater, we will apply the softmax function to these logits to obtain the probability \ndistribution over all possible event tokens. The model is designed to predict the next \nevent token based on the current token and all previous tokens in the music sequence, \nenabling it to generate coherent and musically sensible sequences.\n14.1.3  Training the music Transformer \nNow that we understand how to construct a music Transformer for music generation, \nlet’s outline the training process for the music Transformer.\nThe style of the music generated by the model is influenced by the music pieces used \nfor training. We’ll use piano performances from Google’s Magenta group to train our \nmodel. Figure 14.2 illustrates the steps involved in training the Transformer for music \ngeneration. \nShift 1 index\nto the right[372, 27, 256, ...,\n369, 55][ 27, 256, ..., 369, \n55, 259]\nMusic\ntransformerIndex for the next\nmusic event tokenCross-entropy lossStep 3\nStep 4\nStep 5Step 6 Step 7\nPredictFeedbackMusic pieces\n[..., 372,  27, 256,  ..., 256, 369,  55, 259, ...]Tokenize & indexStep 1\nCreate input (a sequence of 2048 indexes) Step 2\nGround truth\nFigure 14.2    The training process for a music Transformer to generate music\nSimilar to the approach we’ve taken in NLP tasks, the first step in the training process \nfor our music Transformer is to convert the raw training data into a numerical form so \nthat it can be fed into the model. Specifically, we start by converting MIDI files in the \ntraining set into sequences of musical notes. We then further tokenize these notes by \nconverting them into 1 of 388 unique events/tokens. After tokenization, we assign a \nunique index (i.e., an integer) to each token, converting the music pieces in the train -\ning set into sequences of integers (see step 1 in figure 14.2).\nNext, we transform the sequence of integers into training data by dividing this \nsequence into sequences of equal length (step 2 in figure 14.2). We allow a maximum \nlength of 2,048 indexes in each sequence. The choice of 2,048 enables us to capture \nlong-range dependencies among musical events in a music sequence to create lifelike \nmusic. These sequences form the features (the x variable) of our model. As we did \nin previous chapters when training GPT models to generate text, we slide the input \nsequence window one index to the right and use it as the output in the training data \n 327 Tokenizing music pieces\n(the y variable; step 3 in figure 14.2). Doing so forces our model to predict the next \nmusic token in a sequence based on the current token and all previous tokens in the \nmusic sequence.\nThe input and output pairs serve as the training data (x, y) for the music Trans -\nformer. During training, you will iterate through the training data. In the forward \npasses, you feed the input sequence x through the music Transformer (step 4). The \nmusic Transformer then makes a prediction based on the current parameters in the \nmodel (step 5). You compute the cross-entropy loss by comparing the predicted next \ntokens with the output obtained from step 3. In other words, you compare the model’s \nprediction with the ground truth (step 6). Finally, you will adjust the parameters in the \nmusic Transformer so that in the next iteration, the model’s predictions move closer to \nthe actual output, minimizing the cross-entropy loss (step 7). The model is essentially \nperforming a multicategory classification problem: it’s predicting the next token from \nall unique music tokens in the vocabulary.\nYou will repeat steps 3 to 7 through many iterations. After each iteration, the model \nparameters are adjusted to improve the prediction of the next token. This process will \nbe repeated for 50 epochs. \nTo generate a new piece of music with the trained model, we obtain a music piece \nfrom the test set, tokenize it, and convert it to a long sequence of indexes. We’ll use the \nfirst, say, 250 indexes as the prompt (200 or 300 will lead to similar results). We then ask \nthe trained music Transformer to generate new indexes until the sequence reaches a \ncertain length (say, 1,000 indexes). We then convert the sequence of indexes back into \na MIDI file to be played on your computer.\n14.2 Tokenizing music pieces\nHaving grasped the structure of the music Transformer and its training methodology, \nwe’ll start with the first step: tokenization and indexing of the musical compositions in \nour training dataset.\nWe’ll begin with employing a performance-based representation (as explained in \nthe first section) to portray music pieces as musical notes, akin to raw text in NLP. After \nthat, we’ll divide these musical notes into a series of events, similar to tokens in NLP. \nEach unique event will be assigned a different index. Utilizing this mapping, we’ll trans -\nform all music pieces in the training dataset into sequences of indexes.\nNext, we’ll standardize these sequences of indexes into a fixed length, specifically \nsequences with 2,048 indexes, and use them as the feature inputs (x). By shifting the \nwindow one index to the right, we’ll generate the corresponding output sequences (y). \nWe’ll then group pairs of input and output (x, y) into batches, preparing them for train -\ning the music Transformer later in the chapter.\nAs we’ll require the pretty_midi  and music21  libraries for processing MIDI files, \nexecute the following line of code in a new cell in the Jupyter Notebook application:\n!pip install pretty_midi music21\n328 chapter  14 Building and training a music Transformer\n14.2.1  Downloading training data\nWe’ll obtain the piano performances from the MAESTRO dataset, which is made \navailable by Google’s Magenta group ( https: //storage.googleapis.com/magentadata/\ndatasets/maestro/v2.0.0/maestro-v2.0.0-midi.zip ) and download the ZIP file. After \ndownloading, unzip it and move the resulting folder, /maestro-v2.0.0/, into the /files/ \ndirectory on your computer.\nEnsure that the /maestro-v2.0.0/ folder contains 4 files (one of which should be \nnamed “maestro-v2.0.0.json”) and 10 subfolders. Each subfolder should contain more \nthan 100 MIDI files. To familiarize yourself with the sound of the music pieces in the \ntraining data, try opening some of the MIDI files with your preferred music player.\nNext, we’ll split the MIDI files into train, validation, and test subsets. To start, create \nthree subfolders within /files/maestro-v2.0.0/:\nimport os\nos.makedirs(""files/maestro-v2.0.0/train"", exist_ok=True)\nos.makedirs(""files/maestro-v2.0.0/val"", exist_ok=True)\nos.makedirs(""files/maestro-v2.0.0/test"", exist_ok=True)\nTo facilitate the processing of MIDI files, visit Kevin Yang’s GitHub repository at \nhttps: //github.com/jason9693/midi-neural-processor , download the processor.py file, \nand place it in the /utils/ folder on your computer. Alternatively, you can obtain the \nfile from the book’s GitHub repository: https: //github.com/markhliu/DGAI . We’ll \nuse this file as a local module to transform a MIDI file into a sequence of indexes and \nvice versa. This approach allows us to concentrate on developing, training, and utiliz -\ning a music Transformer without getting bogged down in the details of music format \nconversion. At the same time, I’ll provide a simple example of how this process works \nso that you can convert between a MIDI file and a sequence of indexes yourself using \nthe module.\nAdditionally, you need to download the ch14util.py file from the book’s GitHub \nrepository and place it in the /utils/ directory on your computer. We’ll use the ch14util  \n.py file as another local module to define the music Transformer model.\nThe file maestro-v2.0.0.json within the /maestro-v2.0.0/ folder contains the names \nof all MIDI files and their designated subsets (train, validation, or test). Based on this \ninformation, we’ll categorize the MIDI files into three corresponding subfolders.\nListing 14.3    Splitting training data into train, validation, and test subsets\nimport json\nimport pickle\nfrom utils.processor import encode_midi\nfile=""files/maestro-v2.0.0/maestro-v2.0.0.json""\nwith open(file,""r"") as fb:\n    maestro_json=json.load(fb)    Loads JSON file\n 329 Tokenizing music pieces\nfor x in maestro_json:    \n    mid=rf'files/maestro-v2.0.0/{x[""midi_filename""]}'\n    split_type = x[""split""]    \n    f_name = mid.split(""/"")[-1] + "".pickle""\n    if(split_type == ""train""):\n        o_file = rf'files/maestro-v2.0.0/train/{f_name}'\n    elif(split_type == ""validation""):\n        o_file = rf'files/maestro-v2.0.0/val/{f_name}'\n    elif(split_type == ""test""):\n        o_file = rf'files/maestro-v2.0.0/test/{f_name}'\n    prepped = encode_midi(mid)\n    with open(o_file,""wb"") as f:\n        pickle.dump(prepped, f)\nThe JavaScript object notation (JSON) file you downloaded categorizes each file in the \ntraining dataset into one of three subsets: train, validation, and test. After executing \nthe previous code listing, if you explore the /train/, /val/, and /test/ folders on your \ncomputer, you should find numerous files in each. To verify the number of files in each \nof these three folders, you can perform the following checks:\ntrain_size=len(os.listdir('files/maestro-v2.0.0/train'))\nprint(f""there are {train_size} files in the train set"")\nval_size=len(os.listdir('files/maestro-v2.0.0/val'))\nprint(f""there are {val_size} files in the validation set"")\ntest_size=len(os.listdir('files/maestro-v2.0.0/test'))\nprint(f""there are {test_size} files in the test set"")\nThe output from the preceding code block is\nthere are 967 files in the train set\nthere are 137 files in the validation set\nthere are 178 files in the test set\nResults show that there are 967, 137, and 178 pieces of music in the train, validation, \nand test subsets, respectively.\n14.2.2  Tokenizing MIDI files\nNext, we’ll represent each MIDI file as a sequence of musical notes. \nListing 14.4    Converting a MIDI file to a sequence of music notes\nimport pickle\nfrom utils.processor import encode_midi\nimport pretty_midi\nfrom utils.processor import (_control_preprocess,\n    _note_preprocess,_divide_note,\n    _make_time_sift_events,_snote2events)\nfile='MIDI-Unprocessed_Chamber1_MID--AUDIO_07_R3_2018_wav--2'\nname=rf'files/maestro-v2.0.0/2018/{file}.midi'    \nevents=[]\nnotes=[]Iterates through all files \nin the training data\nPlaces a file in \ntrain, validation, \nor test subfolder \nbased on \ninstructions in \nthe JSON file\nSelects a MIDI file from \nthe training dataset\n330 chapter  14 Building and training a music Transformer\nsong=pretty_midi.PrettyMIDI(name)\nfor inst in song.instruments:\n    inst_notes=inst.notes\n    ctrls=_control_preprocess([ctrl for ctrl in \n       inst.control_changes if ctrl.number == 64])\n    notes += _note_preprocess(ctrls, inst_notes)    \ndnotes = _divide_note(notes)    \ndnotes.sort(key=lambda x: x.time)    \nfor i in range(5):\n    print(dnotes[i])   \nWe have selected one MIDI file from the training dataset and used the processor.py \nlocal module to convert it into a sequence of musical notes. The output from the pre -\nceding code listing is\n<[SNote] time: 1.0325520833333333 type: note_on, value: 74, velocity: 86>\n<[SNote] time: 1.0442708333333333 type: note_on, value: 38, velocity: 77>\n<[SNote] time: 1.2265625 type: note_off, value: 74, velocity: None>\n<[SNote] time: 1.2395833333333333 type: note_on, value: 73, velocity: 69>\n<[SNote] time: 1.2408854166666665 type: note_on, value: 37, velocity: 64>\nThe output displayed here shows the first five musical notes from the MIDI file. You \nmight have observed that the time representation in the output is continuous. Certain \nmusical notes contain both a note_on  and a velocity attribute, complicating the \ntokenization process due to the vast number of unique musical events resulting from \nthe continuous nature of time representation. Additionally, the combination of differ -\nent note_on  and velocity values is large (each can assume 128 distinct values, rang -\ning from 0 to 127), leading to an excessively large vocabulary size. This, in turn, would \nrender training impractical.\nTo mitigate this problem and decrease the vocabulary size, we further convert these \nmusical notes into tokenized events:\ncur_time = 0\ncur_vel = 0\nfor snote in dnotes:\n    events += _make_time_sift_events(prev_time=cur_time,    \n                                     post_time=snote.time)\n    events += _snote2events(snote=snote, prev_vel=cur_vel)    \n    cur_time = snote.time\n    cur_vel = snote.velocity    \nindexes=[e.to_int() for e in events]   \nfor i in range(15):    \n    print(events[i]) \nThe output is as follows:\n<Event type: time_shift, value: 99>\n<Event type: time_shift, value: 2>\n<Event type: velocity, value: 21>\n<Event type: note_on, value: 74>\n<Event type: time_shift, value: 0>\n<Event type: velocity, value: 19>\n<Event type: note_on, value: 38>\n<Event type: time_shift, value: 17>Extracts musical events \nfrom the music\nPlaces all musical events \nin the list dnotes\nDiscretizes time \nto reduce the \nnumber of \nunique events\nConverts \nmusical notes \nto events\nPrints out the first 15 events",13324
137-14.2.3 Preparing the training data.pdf,137-14.2.3 Preparing the training data,"331 Tokenizing music pieces\n<Event type: note_off, value: 74>\n<Event type: time_shift, value: 0>\n<Event type: velocity, value: 17>\n<Event type: note_on, value: 73>\n<Event type: velocity, value: 16>\n<Event type: note_on, value: 37>\n<Event type: time_shift, value: 0>\nThe music piece is now represented by four types of events: note-on, note-off, time-\nshift, and velocity. Each event type includes different values, resulting in a total of 388 \nunique events, as detailed in table 14.2 earlier. The specifics of converting a MIDI file \ninto a sequence of such unique events are not essential for constructing and training a \nmusic Transformer. Therefore, we will not dive deeply into this topic; interested read -\ners can refer to Huang et al (2018) cited earlier. All you need to know is how to use \nthe processor.py module to transform a MIDI file into a sequence of indexes and vice \nversa. In the following subsection, you’ll learn how to accomplish that.\n14.2.3  Preparing the training data\nWe’ve learned how to convert music pieces into tokens and then into indexes. The \nnext step involves preparing the training data so that we can utilize it to train the music \nTransformer later in this chapter. To achieve this, we define the create _xys()  func -\ntion shown in the following listing.\nListing 14.5    Creating training data\nimport torch,os,pickle\nmax_seq=2048\ndef create_xys(folder):  \n    files=[os.path.join(folder,f) for f in os.listdir(folder)]\n    xys=[]\n    for f in files:\n        with open(f,""rb"") as fb:\n            music=pickle.load(fb)\n        music=torch.LongTensor(music)      \n        x=torch.full((max_seq,),389, dtype=torch.long)\n        y=torch.full((max_seq,),389, dtype=torch.long)    \n        length=len(music)\n        if length<=max_seq:\n            print(length)\n            x[:length]=music    \n            y[:length-1]=music[1:]    \n            y[length-1]=388    \n        else:\n            x=music[:max_seq]\n            y=music[1:max_seq+1]   \n        xys.append((x,y))\n    return xys\nAs we’ve seen repeatedly throughout this book, in sequence prediction tasks, we use \na sequence x as input. We then shift the sequence one position to the right to create Creates (x, y) \nsequences, with \nequal lengths of \n2,048 indexes and \nsets index 399 as \nthe padding index\nUses a sequence of up to \n2,048 indexes as input\nSlides the window one \nindex to the right and \nuses it as the output\nSets the end index as 388\n332 chapter  14 Building and training a music Transformer\nthe output sequence. This approach compels the model to predict the next element \nbased on the current element and all preceding elements in the sequence. To prepare \ntraining data for our music Transformer, we’ll construct pairs (x, y), where x is the \ninput and y is the output. Both x and y contain 2,048 indexes—long enough to capture \nthe long-term relations of music notes in a sequence but not too long to hinder the \ntraining process.\nWe’ll iterate through all the music pieces in the training dataset we downloaded. If a \nmusic piece exceeds 2,048 indexes in length, we’ll use the first 2,048 indexes as input x. \nFor the output y, we’ll use indexes from the second position to the 2,049th position. In \nthe rare case where the music piece is less than or equal to 2,048 indexes long, we’ll pad \nthe sequence with index 389 to ensure that both x and y are 2,048 indexes long. Addi -\ntionally, we use index 388 to signal the end of the sequence y. \nAs mentioned in the first section, there are a total of 388 unique event tokens, \nindexed from 0 to 387. Since we use 388 to signal the end of the y sequence and 389 to \npad sequences, we have a total of 390 unique indexes, ranging from 0 to 389.\nWe can now apply the create_xys()  function to the train subset:\ntrainfolder='files/maestro-v2.0.0/train'\ntrain=create_xys(trainfolder)\nThe output is\n15\n5\n1643\n1771\n586\nThis shows that out of the 967 music pieces in the train subset, only 5 are shorter than \n2,048 indexes. Their lengths are shown in the previous output. \nWe also apply the create_xys()  function to the validation and test subsets:\nvalfolder='files/maestro-v2.0.0/val'\ntestfolder='files/maestro-v2.0.0/test'\nprint(""processing the validation set"")\nval=create_xys(valfolder)\nprint(""processing the test set"")\ntest=create_xys(testfolder)\nThe output is\nprocessing the validation set\nprocessing the test set\n1837\nThis shows that all music pieces in the validation subset are longer than 2,048 indexes. \nOnly one music piece in the test subset is shorter than 2,048 indexes. \nLet’s print out a file from the validation subset and see what it looks like:\nval1, _ = val[0]\nprint(val1.shape)\nprint(val1)",4765
138-14.4 Training and using the music Transformer.pdf,138-14.4 Training and using the music Transformer,"333 Building a GPT to generate music\nThe output is as follows:\ntorch.Size([2048])\ntensor([324, 366,  67,  ...,  60, 264, 369])\nThe x sequence from the first pair in the validation set has a length of 2,048 indexes, \nwith values such as 324, 367, and so on. Let’s use the module processor.py  to decode \nthe sequence to a MIDI file so that you can hear what it sounds like:\nfrom utils.processor import decode_midi\nfile_path=""files/val1.midi""\ndecode_midi(val1.cpu().numpy(), file_path=file_path)\nThe decode_midi()  function converts a sequence of indexes into a MIDI file, playable \non your computer. After running the preceding code block, open the file val1.midi \nwith a music player on your computer to hear what it sounds like. \nExercise 14.1\nUse the decode_midi()  function from the processor.py local module to convert the \nfirst music piece in the train subset into a MIDI file. Save it as train1.midi on your com -\nputer. Open it with a music player on your computer and get a sense of what type of \nmusic we use for training data.\nFinally, we create a data loader so that the data are in batches for training:\nfrom torch.utils.data import DataLoader\nbatch_size=2\ntrainloader=DataLoader(train,batch_size=batch_size,\n                       shuffle=True)\nTo prevent your GPU from running out of memory, we’ll use a batch size of 2, as we’ve \ncreated very long sequences, each comprising 2,048 indexes. If needed, reduce the \nbatch size to one or switch to CPU training.\nWith that, our training data is prepared. In the next two sections, we’ll construct \na music Transformer from scratch and then train it using the training data we’ve just \nprepared.\n14.3 Building a GPT to generate music\nNow that our training data is prepared, we’ll construct a GPT model from scratch for \nmusic generation. The architecture of this model will be similar to the GPT-2XL model \nwe developed in chapter 11 and the text generator from chapter 12. However, the size \nof our music Transformer will differ due to the specific hyperparameters we select.\nTo conserve space, we’ll place the model construction within the local module \nch14util.py. Our focus here will be on the hyperparameters chosen for the music \nTransformer. Specifically, we’ll decide the values of n_layer,  the number of decoder \n334 chapter  14 Building and training a music Transformer\nlayers in the model; n_head , the number of parallel heads to use to calculate causal \nself-attention; n_embd,  the embedding dimension; and block_size,  the number of \ntokens in the input sequence. \n14.3.1  Hyperparameters in the music Transformer\nOpen the file ch14util.py that you downloaded earlier from the book’s GitHub reposi -\ntory. Inside, you’ll find several functions and classes that are identical to those defined \nin chapter 12.\nAs in all GPT models we have seen in this book, the feed-forward network in the \ndecoder block utilizes the Gaussian error linear unit (GELU) activation function. Con -\nsequently, we define a GELU class in ch14util.py, exactly as we did in chapter 12.\nWe employ a Config()  class to store all the hyperparameters used in the music \nTransformer:\nfrom torch import nn\nclass Config():\n    def __init__(self):\n        self.n_layer = 6\n        self.n_head = 8\n        self.n_embd = 512\n        self.vocab_size = 390\n        self.block_size = 2048 \n        self.embd_pdrop = 0.1\n        self.resid_pdrop = 0.1\n        self.attn_pdrop = 0.1\nconfig=Config()\ndevice=""cuda"" if torch.cuda.is_available() else ""cpu""\nThe attributes within the Config()  class serve as hyperparameters for our music \nTransformer. We assign a value of 6 to the n_layer  attribute, indicating that our music \nTransformer consists of 6 decoder layers. This is more than the number of decoder \nlayers in the GPT model we built in chapter 12. Each decoder layer processes the input \nsequence and introduces a level of abstraction or representation. As the information \ntraverses through more layers, the model is capable of capturing more complex pat -\nterns and relationships in the data. This depth is crucial for our music Transformer to \ncomprehend and generate intricate music pieces.\nThe n_head  attribute is set to 8, signifying that we will divide the query Q, key K, \nand value V vectors into eight parallel heads during the computation of causal self-\nattention. The n_embd  attribute is set to 512, indicating an embedding dimension of \n512: each event token will be represented by a vector of 512 values. The vocab_size  \nattribute is determined by the number of unique tokens in the vocabulary, which is \n390. As explained earlier, there are 388 unique event tokens, and we added 1 token to \nsignify the end of the sequence and another token to pad shorter sequences so that all \nsequences have a length of 2,048. The block_size  attribute is set to 2,048, indicating \nthat the input sequence contains a maximum of 2,048 tokens. We set the dropout rates \nto 0.1, as in chapters 11 and 12.\n 335 Building a GPT to generate music\nLike all Transformers, our music Transformer employs self-attention mechanisms \nto capture relationships among different elements in a sequence. Consequently, we \ndefine a CausalSelfAttention()  class in the local module ch14util, which is identical \nto the CausalSelfAttention()  class defined in chapter 12. \n14.3.2  Building a music Transformer\nWe combine a feed-forward network with the causal self-attention sublayer to form \na decoder block (i.e., a decoder layer). We apply layer normalization and a residual \nconnection to each sublayer for improved stability and performance. To this end, we \ndefine a Block()  class in the local module to create a decoder block, which is identical \nto the Block()  class we defined in chapter 12.\nWe then stack six decoder blocks on top of each other to form the main body of our \nmusic Transformer. To achieve this, we define a Model()  class in the local module. As \nin all GPT models we have seen in this book, we use learned positional encoding by \nemploying the Embedding()  class in PyTorch, instead of the fixed positional encoding \nin the original 2017 paper “Attention Is All You Need.” Refer to chapter 11 on the dif -\nferences between the two positional encoding methods.\nThe input to the model consists of sequences of indexes corresponding to musical \nevent tokens in the vocabulary. We pass the input through word embedding and posi -\ntional encoding and add the two to form the input embedding. The input embedding \nthen goes through the six decoder layers. After that, we apply layer normalization to \nthe output and attach a linear head to it so that the number of outputs is 390, the size \nof the vocabulary. The outputs are the logits corresponding to the 390 tokens in the \nvocabulary. Later, we’ll apply the softmax activation function to the logits to obtain the \nprobability distribution over the unique music tokens in the vocabulary when generat -\ning music.\nNext, we’ll create our music Transformer by instantiating the Model()  class we \ndefined in the local module:\nfrom utils.ch14util import Model\nmodel=Model(config)\nmodel.to(device)\nnum=sum(p.numel() for p in model.transformer.parameters())\nprint(""number of parameters: %.2fM"" % (num/1e6,))\nprint(model)\nThe output is \nnumber of parameters: 20.16M\nModel(\n  (transformer): ModuleDict(\n    (wte): Embedding(390, 512)\n    (wpe): Embedding(2048, 512)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-5): 6 x Block(\n        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)",7642
139-14.4.2 Music generation with the trained Transformer.pdf,139-14.4.2 Music generation with the trained Transformer,"336 chapter  14 Building and training a music Transformer\n        (attn): CausalSelfAttention(\n          (c_attn): Linear(in_features=512, out_features=1536, bias=True)\n          (c_proj): Linear(in_features=512, out_features=512, bias=True)\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        (mlp): ModuleDict(\n          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n          (act): GELU()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=512, out_features=390, bias=False)\n)\nOur music Transformer consists of 20.16 million parameters, a figure substantially \nsmaller than the GPT-2XL, which boasts over 1.5 billion parameters. Nonetheless, our \nmusic Transformer surpasses the size of the text generator we constructed in chapter \n12, which contains only 5.12 million parameters. Despite these differences, all three \nmodels are based on the decoder-only Transformer architecture. The variations lie \nsolely in the hyperparameters, such as the embedding dimension, number of decoder \nlayers, vocabulary size, and so on. \n14.4 Training and using the music Transformer \nIn this section, you’ll train the music Transformer you’ve just constructed using the \nbatches of training data we prepared earlier in this chapter. To expedite the process, \nwe’ll train the model for 100 epochs and then stop the training process. For those \ninterested, you can utilize the validation set to determine when to stop training, based \non the performance of the model on the validation set, as we did in chapter 2.\nOnce the model is trained, we’ll provide it with a prompt in the form of a sequence of \nindexes. We’ll then request the trained music Transformer to generate the next index. \nThis new index is appended to the prompt, and the updated prompt is fed back into the \nmodel for another prediction. This process is repeated iteratively until the sequence \nreaches a certain length.\nUnlike the music generated in chapter 13, we can control the creativity of the music \npiece by applying different temperatures. \n14.4.1  Training the music Transformer\nAs always, we’ll use the Adam optimizer for training. Given that our music Transformer \nis essentially executing a multicategory classification task, we’ll utilize cross-entropy \nloss as our loss function:\n 337 Training and using the music Transformer \nlr=0.0001\noptimizer = torch.optim.Adam(model.parameters(), lr=lr)\nloss_func=torch.nn.CrossEntropyLoss(ignore_index=389)\nThe ignore_index=389  argument in the previous loss function instructs the program \nto disregard index 389 whenever it occurs in the target sequence (i.e., sequence y), \nas this index is used solely for padding purposes and does not represent any specific \nevent token in the music piece. \nWe will then train the model for 100 epochs.\nListing 14.6    Training the music Transformer to generate music\nmodel.train()  \nfor i in range(1,101):\n    tloss = 0.\n    for idx, (x,y) in enumerate(trainloader):    \n        x,y=x.to(device),y.to(device)\n        output = model(x)\n        loss=loss_func(output.view(-1,output.size(-1)),\n                           y.view(-1))    \n        optimizer.zero_grad()\n        loss.backward()\n        nn.utils.clip_grad_norm_(model.parameters(),1)    \n        optimizer.step()    \n        tloss += loss.item()\n    print(f'epoch {i} loss {tloss/(idx+1)}') \ntorch.save(model.state_dict(),f'files/musicTrans.pth')    \nDuring training, we feed all the input sequences x in a batch through the model to \nobtain predictions. We then compare these predictions with the corresponding output \nsequences y in the batch and calculate the cross-entropy loss. After that, we adjust the \nmodel parameters to minimize this loss. It’s important to note that we’ve clipped the \ngradient norm to 1 to prevent the potential problem of exploding gradients.\nThe training process described above takes approximately 3 hours if you have a \nCUDA-enabled GPU. After training, the trained model weights, musicTrans.pth, are \nsaved on your computer. Alternatively, you can download the trained weights from my \nwebsite at https: //mng.bz/V2pW . \n14.4.2  Music generation with the trained Transformer\nNow that we have a trained music Transformer, we can proceed with music generation.\nSimilar to the process in text generation, music generation begins with feeding a \nsequence of indexes (representing event tokens) to the model as a prompt. We’ll select \na music piece from the test set and use the first 250 musical events as the prompt:\nfrom utils.processor import decode_midi\nprompt, _  = test[42]\nprompt = prompt.to(device)Iterates through all \nbatches of training data\nCompares model \npredictions with \nactual outputs\nClips gradient \nnorm to 1\nTweaks model \nparameters to \nminimize loss\nSaves model \nafter training\n338 chapter  14 Building and training a music Transformer\nlen_prompt=250\nfile_path = ""files/prompt.midi""\ndecode_midi(prompt[:len_prompt].cpu().numpy(),\n            file_path=file_path)\nWe have randomly selected an index (42, in our case) and used it to retrieve a song \nfrom the test subset. We keep only the first 250 musical events, which we’ll later feed to \nthe trained model to predict the next musical events. For comparison purposes, we’ll \nsave the prompt as a MIDI file, prompt.midi, in the local folder.\nExercise 14.2\nUse the decode_midi()  function to convert the first 250 musical events in the second \nmusic piece in the test set into a MIDI file. Save it as prompt2.midi on your computer. \nTo streamline the music generation process, we’ll define a sample()  function. This \nfunction accepts a sequence of indexes as input, representing a short piece of music. \nIt then iteratively predicts and appends new indexes to the sequence until a specified \nlength, seq_length , is achieved. The implementation is shown in the following listing.\nListing 14.7    A sample()  function in music generation \nsoftmax=torch.nn.Softmax(dim=-1)\ndef sample(prompt,seq_length=1000,temperature=1):\n    gen_seq=torch.full((1,seq_length),389,dtype=torch.long).to(device)\n    idx=len(prompt)\n    gen_seq[..., :idx]=prompt.type(torch.long).to(device)    \n    while(idx < seq_length):    \n        y=softmax(model(gen_seq[..., :idx])/temperature)[...,:388]    \n        probs=y[:, idx-1, :]\n        distrib=torch.distributions.categorical.Categorical(probs=probs)\n        next_token=distrib.sample()    \n        gen_seq[:, idx]=next_token\n        idx+=1\n    return gen_seq[:, :idx]    \nOne of the parameters of the sample()  function is temperature, which regulates the \ncreativity of the generated music. Refer to chapter 8 on how this works if needed. Since \nwe can adjust the originality and diversity of the generated music with the temperature \nparameter alone, we have omitted top-K  sampling for simplicity in this instance. As we \nhave discussed top-K  sampling three times earlier in this book (in chapters 8, 11, and \n12), interested readers can experiment with incorporating top-K  sampling into the \nsample() function here.\nNext, we’ll load the trained weights into the model:Generates the new indexes \nuntil the sequence reaches \na certain length\nDivides the prediction by the \ntemperature and then applies \nthe softmax function on logits\nSamples from the predicted probability \ndistribution to generate a new index \nOutputs the whole sequence\n 339 Training and using the music Transformer \nmodel.load_state_dict(torch.load(""files/musicTrans.pth"",\n    map_location=device))\nmodel.eval()\nWe then call the sample()  function to generate a piece of music:\nfrom utils.processor import encode_midi\nfile_path = ""files/prompt.midi""\nprompt = torch.tensor(encode_midi(file_path))\ngenerated_music=sample(prompt, seq_length=1000)\nFirst, we utilize the encode_midi()  function from the processor.py module to convert \nthe MIDI file, prompt.midi, into a sequence of indexes. We then use this sequence \nas the prompt in the sample()  function to generate a music piece comprising 1,000 \nindexes.\nFinally, we convert the generated sequence of indexes into the MIDI format:\nmusic_data = generated_music[0].cpu().numpy()\nfile_path = 'files/musicTrans.midi'\ndecode_midi(music_data, file_path=file_path)\nWe employ the decode_midi()  function in the processor.py module to transform the \ngenerated sequence of indexes into a MIDI file, musicTrans.midi, on your computer. \nOpen both files, prompt.midi and musicTrans.midi, on your computer and listen to \nthem. The music from prompt.midi lasts about 10 seconds. The music from music -\nTrans.midi lasts about 40 seconds, with the final 30 seconds being new music generated \nby the music Transformer. The generated music should sound like the music piece on \nmy website: https: //mng.bz/x6dg .\nThe preceding code block may produce output similar to the following:\ninfo removed pitch: 52\ninfo removed pitch: 83\ninfo removed pitch: 55\ninfo removed pitch: 68\nIn the generated music, there may be instances where certain notes need to be \nremoved. For example, if the generated music piece attempts to turn off note 52, but \nnote 52 was never turned on initially, then we cannot turn it off. Therefore, we need to \nremove such notes. \nExercise 14.3\nGenerate a piece of music consisting of 1,200 notes using the trained Music Transformer \nmodel, keeping the temperature parameter at 1. Use the sequence of indexes from the \nfile prompt2.midi you just generated in exercise 14.2 as the prompt. Save the generated \nmusic in a file named musicTrans2.midi on your computer.  \nYou can increase the creativity of the music by setting the temperature argument to a \nvalue greater than 1, as follows:\n340 chapter  14 Building and training a music Transformer\nfile_path = ""files/prompt.midi""\nprompt = torch.tensor(encode_midi(file_path))\ngenerated_music=sample(prompt, seq_length=1000,temperature=1.5)\nmusic_data = generated_music[0].cpu().numpy()\nfile_path = 'files/musicHiTemp.midi'\ndecode_midi(music_data, file_path=file_path)\nWe set the temperature to 1.5. The generated music is saved as musicHiTemp.midi on \nyour computer. Open the file and listen to the generated music to see if you can dis -\ncern any differences compared to the music in the file musicTrans.midi.\nExercise 14.4\nGenerate a piece of music consisting of 1,000 indexes using the trained Music \nTransformer model, setting the temperature parameter to 0.7. Use the sequence of \nindexes in the file prompt.midi as the prompt. Save the generated music in a file named \nmusicLowTemp.midi on your computer. Open this file to listen to the generated music \nand see if there are any discernible differences between the new piece of music and the \nmusic in the file musicTrans.midi.  \nIn this chapter, you’ve learned how to construct and train a music Transformer from \nscratch, based on the decoder-only Transformer architecture you used in earlier chap -\nters. In the next chapter, you’ll explore diffusion-based models, which are at the heart \nof text-to-image Transformers such as OpenAI’s DALL-E 2 and Google’s Imagen. \nSummary\n¡ The performance-based representation of music enables us to represent a music \npiece as a sequence of notes, which include control messages and velocity values. \nThese notes can be further reduced to four kinds of musical events: note-on, \nnote-off, time-shift, and velocity. Each event type can assume various values. Con -\nsequently, we can transform a music piece into a sequence of tokens and then \ninto indexes.  \n¡ A music Transformer adapts the Transformer architecture, originally designed \nfor NLP tasks, for music generation. This model is designed to generate \nsequences of musical notes by learning from a large dataset of existing music. It \nis trained to predict the next note in a sequence based on previous notes, by rec -\nognizing patterns, structures, and relationships among various musical elements \nin the training data.\n¡ Just as in text generation, we can use temperature to regulate the creativity of the \ngenerated music.",12480
140-15.1.1 The forward diffusion process.pdf,140-15.1.1 The forward diffusion process,"34115Diffusion models and  \ntext-to-image Transformers\nThis chapter covers\n¡ How forward diffusion and reverse diffusion work\n¡ How to build and train a denoising U-Net model\n¡ Using the trained U-Net to generate flower images\n¡ Concepts behind text-to-image Transformers\n¡ Writing a Python program to generate an image  \n through text with DALL-E 2\nIn recent years, multimodal large language models (LLMs) have gained significant \nattention for their ability to handle various content formats, such as text, images, \nvideo, audio, and code. A notable example of this is text-to-image Transformers, \nsuch as OpenAI’s DALL-E 2, Google’s Imagen, and Stability AI’s Stable Diffusion. \nThese models are capable of generating high-quality images based on textual \ndescriptions.\nThese text-to-image models comprise three essential components: a text encoder \nthat compresses text into a latent representation, a method to incorporate text infor -\nmation into the image generation process, and a diffusion mechanism to gradually \nrefine an image to produce realistic output. Understanding the diffusion mechanism \n342 chapter  15 Diffusion models and text-to-image Transformers \nis particularly crucial for comprehending text-to-image Transformers, as diffusion mod -\nels form the foundation of all leading text-to-image Transformers. For this reason, you \nwill start by building and training a diffusion model to generate flower images in this \nchapter. This will provide you with a deep understanding of the forward diffusion pro -\ncess, where noise is incrementally added to images until they become random noise. \nSubsequently, you will train a model to reverse the diffusion process by gradually remov -\ning noise from images until the model can generate a new, clean image from random \nnoise, resembling those in the training dataset.\nDiffusion models have become the go-to choice for generating high-resolution \nimages. The success of diffusion models lies in their ability to simulate and reverse a \ncomplex noise addition process, which mimics a deep understanding of how images \nare structured and how to construct them from abstract patterns. This method not only \nensures high quality but also maintains a balance between diversity and accuracy in the \ngenerated images.\nAfter that, we’ll explain how a text-to-image Transformer works conceptually. We’ll \nfocus on the contrastive language–image pretraining (CLIP) model developed by \nOpenAI, which is designed to comprehend and link visual and textual information. \nCLIP processes two types of inputs: images and text (typically in the form of captions or \ndescriptions). These inputs are handled separately through two encoders in the model.\nThe image branch of CLIP employs a Vision Transformer (ViT) to encode images \ninto a high-dimensional vector space, extracting visual features in the process. Mean -\nwhile, the text branch uses a Transformer-based language model to encode textual \ndescriptions into the same vector space, capturing semantic features from the text. \nCLIP has been trained on many pairs of matching images and text descriptions to \nclosely align the representations of matching pairs in the vector space.\nOpenAI’s text-to-image Transformers, such as DALL-E 2, incorporate CLIP as a \ncore component. In this chapter, you’ll learn to obtain an OpenAI API key and write a \nPython program to generate images using DALL-E 2 based on text descriptions. \n15.1 Introduction to denoising diffusion models\nThe concept of diffusion-based models can be illustrated using the following example. \nConsider the goal of generating high-resolution flower images using a diffusion-based \nmodel. To do that, you first acquire a set of high-quality flower images for training. The \nmodel is then instructed to incrementally introduce small amounts of random noise \ninto these images, a process known as forward diffusion. After many steps of adding \nnoise, the training images eventually become random noise. The next phase involves \ntraining the model to reverse this process, starting with pure noise images and pro -\ngressively reducing the noise until the images are indistinguishable from those in the \noriginal training set.\nOnce trained, the model is given random noise images to work with. It systematically \neliminates noise from the image over many iterations until it generates a high-resolution \n 343 Introduction to denoising diffusion models\nflower image that resembles those in the training set. This is the underlying principle of \ndiffusion-based models.1\nIn this section, you will first explore the mathematical foundations of diffusion-based \nmodels. Then you will dive into the architecture of U-Nets, the type of model used for \ndenoising images and producing high-resolution flower images. Specifically, the U-Net \nemploys a scaled dot product attention (SDPA) mechanism, similar to what you have \nseen in Transformer models in chapters 9 to 12. Finally, you will learn the training pro -\ncess of diffusion-based models and the image-generation process of the trained model.\n15.1.1  The forward diffusion process\nSeveral papers have proposed diffusion-based models with similar underlying mecha -\nnisms.2 Let’s use the flower images as a concrete example to explain the idea behind \ndenoising diffusion models. Figure 15.1 is a diagram of how the forward diffusion pro -\ncess works. \nx0\n x1Noise\n...\nxt\n xt+1Noise\n...\nxT\nRandom noise Clean image Noisy images\nFigure 15.1    A diagram of the forward diffusion process. We start with a clean image from the training \nset, x0, and add noise є0 to it to form a noisy image x1 = √(1 – β1)x0 + √(β1)є0. We repeat this process for \n1,000 time steps until the image x1000 becomes random noise. \nAssume that flower images,  x0 (illustrated in the left image in figure 15.1), follow a dis -\ntribution of q(x). In the forward diffusion process, we’ll add small amounts of noise to \nthe images in each of the T = 1,000 steps. The noise tensor is normally distributed and \nhas the same shape as the flower images: (3, 64, 64), meaning three color channels, \nwith a height and width of 64 pixels.\n1 Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli, 2015, “Deep Unsupervised Learn -\ning Using Nonequilibrium Thermodynamics.” International Conference on Machine Learning, http: //arxiv.org/\nabs/1503.03585 .\n2 Sohl-Dickstein et al., 2015, “Deep Unsupervised Learning Using Nonequilibrium Thermodynamics,”  https://arxiv  \n.org/abs/1503.03585 . Yang Song and Stefano Ermon, 2019, “Generative Modeling by Estimating Gradients of the \nData Distribution.”  https: //arxiv.org/abs/1907.05600 . Jonathan Ho, Ajay Jain, and Pieter Abbeel, 2020, “Denoising \nDiffusion Probabilistic Models,” https: //arxiv.org/abs/2006.11239 .\n344 chapter  15 Diffusion models and text-to-image Transformers \nTime steps in diffusion models\nIn diffusion models, time steps refer to the discrete stages during the process of gradu -\nally adding noise to data and subsequently reversing this process to generate samples. \nThe forward phase of a diffusion model progressively adds noise over a series of time \nsteps, transforming data from its original, clean state into a noisy distribution. During the \nreverse phase, the model operates over a similar series of time steps but in a reverse \norder. It systematically removes noise from the data to reconstruct the original or gener -\nate new, high-fidelity samples. Each time step in this reverse process involves predicting \nthe noise that was added in the corresponding forward step and subtracting it, thereby \ngradually denoising the data until reaching a clean state.  \nIn time step 1, we add noise є0 to the image x0, so that we obtain a noisy image x1:\n(15.1) \nThat is, x1 is a weighted sum of  x0 and є0, where  β1 measures the weight placed on the \nnoise. The value of β changes in different time steps—hence the subscript in β1. If we \nassume x0 and є0 are independent of each other and follow a standard normal distri -\nbution (i.e., with mean 0 and variance 1), the noisy image x1 will also follow a standard \nnormal distribution. This is easy to prove since \nand \nWe can keep adding noise to the image for the next T–1 time steps so that\n (15.2)\nWe can use a reparameterization trick and define αt = 1 − βt and\nto allow us to sample xt at any arbitrary time step t, where t can take any value in [1, 2, \n. . ., T−1, T]. Then we have \n(15.3)",8542
141-15.1.2 Using the U-Net model to denoise images.pdf,141-15.1.2 Using the U-Net model to denoise images,"345 Introduction to denoising diffusion models\nWhere є is a combination of є0, є1, . . ., and єt–1, using the fact that we can add two nor -\nmal distributions to obtain a new normal distribution. See, for example, the blog of \nLilian Weng at https: //mng.bz/Aalg  for proof. \nThe farther left of figure 15.1 shows a clean flower, x0, from the training set. In the \nfirst time step, we inject noise є0 to it to form a noisy image x1 (second image in figure \n15.1). We repeat this process for 1,000 time steps, until the image becomes random \nnoise (the rightmost image).\n15.1.2  Using the U-Net model to denoise images\nNow that you understand the forward diffusion process, let’s discuss the reverse dif -\nfusion process (i.e., the denoising process). If we can train a model to reverse the for -\nward diffusion process, we can feed the model with random noise and ask it to produce \na noisy flower image. We can then feed the noisy image to the trained model again and \nproduce a clearer, though still noisy, image. We can iteratively repeat the process for \nmany time steps until we obtain a clean image, indistinguishable from images from the \ntraining set. The use of multiple inference steps in the reverse diffusion process, rather \nthan just a single step, is crucial for gradually reconstructing high-quality data from a \nnoisy distribution. It allows for a more controlled, stable, and high-quality generation \nof data.\nTo that end, we’ll create a denoising U-Net model. The U-Net architecture, which \nwas originally designed for biomedical image segmentation, is characterized by its sym -\nmetric shape, with a contracting path (encoder) and an expansive path (decoder), con -\nnected by a bottleneck layer. In the context of denoising, U-Net models are adapted to \nremove noise from images while preserving important details. U-Nets outperform sim -\nple convolutional networks in denoising tasks due to their efficient capturing of local \nand global features in images. \nFigure 15.2 is a diagram of the structure of the denoising U-Net we use in this chapter.\nThe model takes a noisy image and the time step the noisy image is in ( xt and t in \nequation 15.3) as input and predicts the noise in the image (i.e., є). Since the noisy \nimage is a weighted sum of the original clean image and noise (see equation 15.3), \nknowing the noise allows us to deduce and reconstruct the original image. \nThe contracting path (i.e., the encoder; left side of figure 15.2) consists of multi -\nple convolutional layers and pooling layers. It progressively downsamples the image, \nextracting and encoding features at different levels of abstraction. This part of the net -\nwork learns to recognize patterns and features that are relevant for denoising. \nThe bottleneck layer (bottom of figure 15.2) connects the encoder and decoder \npaths. It consists of convolutional layers and is responsible for capturing the most \nabstract representations of the image.\nThe expansive path (i.e., the decoder; right side of figure 15.2) consists of upsam -\npling layers and convolutional layers. It progressively upsamples the feature maps, \nreconstructing the image while incorporating features from the encoder through skip \nconnections. Skip connections (denoted by dashed lines in figure 15.2) are crucial in \n346 chapter  15 Diffusion models and text-to-image Transformers \nU-Net models, as they allow the model to retain fine-grained details from the input \nimage by combining low-level and high-level features. Next, I briefly explain how skip \nconnections work. \nConv2D Input: noisy image\nshape (3, 64, 64)\nDown block 1 \nDown block 2 \nDownsampleshape (128, 64, 64)\nDown block 1 \nDown block 2 \nDownsample shape (256, 32, 32)\nDown block 1 \nDown block 2 \nAttn/Norm/Add \nConv2Dshape (512, 16, 16)\nMid block 1 \nAttn/Norm/Add \nMid block 2Conv2D Output: predicted noise\nshape (3, 64, 64)\nUpsample \nUp block 2 \nUp block 1\nshape (1024, 16, 16) shape (1024, 16, 16)Upsample \nUp block 2 \nUp block 1Conv2D \nAttn/Norm/Add \nUp block 2\nUp block 1shape (128, 64, 64)\nshape (256, 64, 64)\nshape (512, 32,32)Skip connections\n(128, 64, 64) \n(128, 64, 64)(256, 64, 64) \n(256, 64, 64) \n(256, 64, 64)\n(1024, 16, 16)(512, 32, 32)(256, 32, 32)\n(256, 32, 32)(512, 32, 32)\n(1024, 16, 16)(512, 16, 16) \n(512, 16, 16)\n(512, 16, 16)\nFigure 15.2    The architecture of the denoising U-Net model. The U-Net architecture is characterized by \nits symmetric shape, with a contracting path (encoder) and an expansive path (decoder), connected by a \nbottleneck layer. The model is designed to remove noise from images while preserving important details. \nThe input to the model is a noisy image, along with which time step the image is in, and the output is the \npredicted noise in the image. \nIn a U-Net model, skip connections are implemented by concatenating feature maps \nfrom the encoder path with corresponding feature maps in the decoder path. These \nfeature maps are typically of the same spatial dimensions but may have been processed \ndifferently due to the separate paths they have traversed. During the encoding process, \nthe input image is progressively downsampled, and some spatial information (such as \nedges and textures) may be lost. Skip connections help preserve this information by \ndirectly passing feature maps from the encoder to the decoder, bypassing the informa -\ntion bottleneck.\nFor example, the dashed line at the top of figure 15.2 indicates that the model con -\ncatenates the output  from the Conv2D layer in the encoder, which has a shape of (128, \n64, 64), with the input  to the Conv2D layer in the decoder, which also has a shape of \n(128, 64, 64). As a result, the final input to the Conv2D layer in the decoder has a shape \nof (256, 64, 64).",5829
142-15.2 Preparing the training data.pdf,142-15.2 Preparing the training data,"347 Introduction to denoising diffusion models\nBy combining high-level, abstract features from the decoder with low-level, detailed \nfeatures from the encoder, skip connections enable the model to better reconstruct \nfine details in the denoised image. This is particularly important in denoising tasks, \nwhere retaining subtle image details is crucial.\nThe scaled dot product attention (SDPA) mechanism is implemented in both the \nfinal block of the contracting path and the final block of the expansive path in our \ndenoising U-Net model, accompanied by layer normalization and residual connections \n(as shown in figure 15.2 with the label Attn/Norm/Add). This SDPA mechanism is \nessentially the same as the one we developed in chapter 9; the key difference is its appli -\ncation to image pixels rather than text tokens.\nThe use of skip connections and the model’s size lead to redundant feature \nextractions in our denoising U-Net, ensuring that no important feature is lost during \nthe denoising process. However, the large size of the model also complicates the identi -\nfication of relevant features, akin to searching for a needle in a haystack. The attention \nmechanism empowers the model to emphasize significant features while disregarding \nirrelevant ones, thereby enhancing the effectiveness of the learning process.\n15.1.3  A blueprint to train the denoising U-Net model\nThe output of the denoising U-Net is the noise injected into the noisy image. The \nmodel is trained to minimize the difference between the output (predicted noise) and \nthe ground truth (actual noise). \nThe denoising U-Net model uses the U-Net architecture’s ability to capture both \nlocal and global context, making it effective for removing noise while preserving \nimportant details such as edges and textures. These models are widely used in various \napplications, including medical image denoising, photographic image restoration, and \nmore. Figure 15.3 is a diagram of the training process of our denoising U-Net model. \nClean Image\nU-Net\nTakes in noisy image; \npredicts the noise in \nthe image\nNoisy imageStep 4Step 1\nStep 2\nNoise\nPredicted noiseStep 6: FeedbackStep 5: Calculate L1 loss \nmean absolute error\nStep 3\nFigure 15.3    The training process of the denoising U-Net model. We first obtain clean flower images as \nour training set. We add noise to clean flower images and present them to the U-Net model. The model \npredicts the noise in the noisy images. We compare the predicted noise with the actual noise injected \ninto the flower images and tweak the model weights to minimize the mean absolute error.",2644
143-15.2.1 Flower images as the training data.pdf,143-15.2.1 Flower images as the training data,"348 chapter  15 Diffusion models and text-to-image Transformers \nThe first step is to gather a dataset of flower images. We’ll use the Oxford 102 Flower \ndataset as our training set. We’ll resize all images to a fixed resolution of 64 × 64 pixels \nand normalize pixel values to the range [–1, 1]. For denoising, we need pairs of clean \nand noisy images. We’ll synthetically add noise to the clean flower images to create \nnoisy counterparts (step 2 in figure 15.3) based on the formula specified in equation \n15.3. \nWe’ll then build a denoising U-Net model with a structure as outlined in figure 15.2. \nDuring each epoch of training, we iterate over the dataset in batches. We add noise to \nthe flower images and present the noisy images to the U-Net model (step 3), along with \nthe time steps the noisy images are in, t. The U-Net model predicts the noise in the noisy \nimages (step 4) based on current parameters in the model. \nWe compare the predicted noise with the actual noise and calculate the L1 loss (i.e., \nmean absolute error) at the pixel level (step 5). L1 loss is usually preferred in such \nsituations because it’s less sensitive to outliers compared to the L2 loss (mean squared \nerror). We then tweak the model parameters to minimize the L1 loss (step 6) so that in \nthe next iteration, the model makes better predictions. We repeat this process for many \niterations until the model parameters converge. \n15.2 Preparing the training data \nWe’ll use the Oxford 102 Flower dataset, which is freely available on Hugging Face, as \nour training data. The dataset contains about 8,000 flower images and can be down -\nloaded directly by using the datasets  library you installed earlier. \nTo save space, we’ll place most helper functions and classes in two local modules, \nch15util.py and unet_util.py. Download these two files from the book’s GitHub repos -\nitory ( https: //github.com/markhliu/DGAI ) and place them in the /utils/ folder on \nyour computer. The Python programs in this chapter are adapted from Hugging Face’s \nGitHub repository ( https: //github.com/huggingface/diffusers ) and Filip Basara’s \nGitHub repository ( https: //github.com/filipbasara0/simple-diffusion ). \nYou’ll use Python to download the dataset to your computer. After that, we’ll demon -\nstrate the forward diffusion process by gradually adding noise to clean images in the \ntraining dataset until they become random noise. Finally, you’ll place the training data \nin batches so that we can use them to train the denoising U-Net model later in the \nchapter. \nYou’ll use the following Python libraries in this chapter: datasets, einops, diffusers, \nand openai. To install these libraries, execute the following line of code in a new cell in \nyour Jupyter Notebook application on your computer:\n!pip install datasets einops diffusers openai\nFollow the on-screen instructions to finish the installation. \n15.2.1  Flower images as the training data\nThe load_dataset() method from the datasets  library you installed earlier allows you \nto directly download the Oxford 102 Flower dataset from Hugging Face. We’ll then use \n 349 Preparing the training data \nthe matplotlib  library to show some flower images in the dataset so that we have an idea \nof what the images in the training dataset look like.\nRun the lines of code shown in the following listing in a cell in Jupyter Notebook.\nListing 15.1    Downloading and visualizing flower images \nfrom datasets import load_dataset\nfrom utils.ch15util import transforms\ndataset = load_dataset(""huggan/flowers-102-categories"",\n    split=""train"",)    \ndataset.set_transform(transforms)\nimport matplotlib.pyplot as plt\nfrom torchvision.utils import make_grid\n# Plot all the images of the 1st batch in grid\ngrid = make_grid(dataset[:16][""input""], 8, 2)    \nplt.figure(figsize=(8,2),dpi=300)\nplt.imshow(grid.numpy().transpose((1,2,0)))\nplt.axis(""off"")\nplt.show()\nAfter running the preceding code listing, you’ll see the first 16 flower images in the \ndataset, as displayed in figure 15.4. These are high-resolution color images of various \ntypes of flowers. We have standardized the size of each image to (3, 64, 64). \nFigure 15.4    The first 16 Images from the Oxford 102 Flower dataset.\nWe place the dataset in batches of 4 so that we can use them to train the denoising \nU-Net model later. We choose a batch size of 4 to keep the memory size small enough \nto fit on a GPU during training. Adjust the batch size to 2 or even 1 if your GPU mem -\nory is small:\nimport torch\nresolution=64\nbatch_size=4\ntrain_dataloader=torch.utils.data.DataLoader(\n    dataset, batch_size=batch_size, shuffle=True)\nNext, we’ll code in and visualize the forward diffusion process.  Downloads the images \nfrom Hugging Face\nPlots the first \n16 images",4835
144-15.4.1 Training the denoising U-Net model.pdf,144-15.4.1 Training the denoising U-Net model,"350 chapter  15 Diffusion models and text-to-image Transformers \n15.2.2  Visualizing the forward diffusion process\nWe have defined a class DDIMScheduler()  in the local module ch15util.py you just \ndownloaded. Take a look at the class in the file; we’ll use it to add noise to images. We’ll \nalso use the class to produce clean images later, along with the trained denoising U-Net \nmodel. The DDIMScheduler()  class manages the step sizes and sequence of denoising \nsteps, enabling deterministic inference that can produce high-quality samples through \nthe denoising process. \nWe first select four clean images from the training set and generate noise tensors that \nhave the same shape as these images:\nclean_images=next(iter(train_dataloader))[""input""]*2-1    \nprint(clean_images.shape)\nnums=clean_images.shape[0]\nnoise=torch.randn(clean_images.shape)    \nprint(noise.shape)\nThe output from the preceding code block is\ntorch.Size([4, 3, 64, 64])\ntorch.Size([4, 3, 64, 64])\nBoth the images and the noise tensors have a shape of (4, 3, 64, 64), meaning 4 images \nin the batch and 3 color channels per image, and the height and width of the images \nare 64 pixels. \nDuring the forward diffusion process, there are 999 transitional noisy images \nbetween the clean images ( x0 as we explained in the first section) and random noise \n(xT). The transitional noisy images are a weighted sum of the clean image and the noise. \nAs t goes from 0 to 1,000, the weight on the clean image gradually decreases, and the \nweight on the noise gradually increases, as specified in equation 15.3. \nNext, we generate and visualize some transitional noisy images. \nListing 15.2    Visualizing the forward diffusion process\nfrom utils.ch15util import DDIMScheduler\nnoise_scheduler=DDIMScheduler(num_train_timesteps=1000)    \nallimgs=clean_images\nfor step in range(200,1001,200):    \n    timesteps=torch.tensor([step-1]*4).long()\n    noisy_images=noise_scheduler.add_noise(clean_images,\n                 noise, timesteps)    \n    allimgs=torch.cat((allimgs,noisy_images))    \nimport torchvision\nimgs=torchvision.utils.make_grid(allimgs,4,6)\nfig = plt.figure(dpi=300)\nplt.imshow((imgs.permute(2,1,0)+1)/2)    \nplt.axis(""off"")\nplt.show()Obtains four \nclean images\nGenerates a tensor, noise, which has the \nsame shape as the clean images; each \nvalue in the noise follows an independent \nstandard normal distribution.\nInstantiates the \nDDIMScheduler() class \nwith 1,000 time steps\nLooks at time steps 200, \n400, 600, 800, and 1,000\nCreates noisy images \nat these time steps\nConcatenates noisy \nimages with clean images\nDisplays all images\n 351 Building a denoising U-Net model\nThe add_noise()  method in the DDIMScheduler()  class takes three arguments: \nclean_images , noise , and timesteps . It produces a weighted sum of the clean image \nand the noise, which is a noisy image. Further, the weight is a function of the time step, \nt. As the time step, t, moves from 0 to 1,000, the weight on the clean image decreases \nand that on the noise increases. If you run the previous code listing, you’ll see an image \nsimilar to figure 15.5.\nFigure 15.5    The forward diffusion process. The four images in the first column are clean images from \nthe training dataset. We then gradually add noise to these images from time step 1 to time step 1,000. \nAs the time step increases, more and more noise is injected into the images. The four images in the \nsecond column are images after 200 time steps. The third column contains images after 400 time steps, \nand they have more noise than those in the second column. The last column contains images after 1,000 \ntime steps, and they are 100% random noise. \nThe first column contains the four clean images without noise. As we move to the right, \nwe gradually add more and more noise to the images. The very last column contains \npure random noise. \n15.3 Building a denoising U-Net model\nEarlier in this chapter, we discussed the architecture of the denoising U-Net model. In \nthis section, I will guide you through implementing it using Python and PyTorch.\nThe U-Net model we are going to construct is quite large, containing over 133 million \nparameters, reflecting the complexity of its intended task. It is engineered to capture \nboth local and global features within an image through a process of downsampling and \nupsampling the input. The model uses multiple convolutional layers interconnected \nby skip connections, which combine features from various levels of the network. This \narchitecture helps maintain spatial information, facilitating more effective learning.\nGiven the substantial size of the denoising U-Net model and its redundant fea -\nture extraction, the SDPA attention mechanism is employed to enable the model to \n352 chapter  15 Diffusion models and text-to-image Transformers \nconcentrate on the most relevant aspects of the input for the task at hand. To compute \nSDPA attention, we will flatten the image and treat its pixels as a sequence. We will then \nuse SDPA to learn the dependencies among different pixels in the image in a manner \nakin to how we learned dependencies among different tokens in text in chapter 9. \n15.3.1  The attention mechanism in the denoising U-Net model \nTo implement the attention mechanism, we have defined an Attention()  class in the \nlocal module ch15util.py, as shown in the following code listing.\nListing 15.3    The attention mechanism in the denoising U-Net model\nimport torch\nfrom torch import nn, einsum\nfrom einops import rearrange\nclass Attention(nn.Module):\n    def __init__(self, dim, heads=4, dim_head=32):\n        super().__init__()\n        self.scale = dim_head**-0.5\n        self.heads = heads\n        hidden_dim = dim_head * heads\n        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias=False)\n        self.to_out = nn.Conv2d(hidden_dim, dim, 1)\n    def forward(self, x):\n        b, c, h, w = x.shape\n        qkv = self.to_qkv(x).chunk(3, dim=1)    \n        q, k, v = map(\n        lambda t: rearrange(t, 'b (h c) x y -> b h c (x y)', h=self.heads),\n        qkv)    \n        q = q * self.scale    \n        sim = einsum('b h d i, b h d j -> b h i j', q, k)\n        attn = sim.softmax(dim=-1)    \n        out = einsum('b h i j, b h d j -> b h i d', attn, v)    \n        out = rearrange(out, 'b h (x y) d -> b (h d) x y', x=h, y=w)\n        return self.to_out(out)    \nattn=Attention(128)\nx=torch.rand(1,128,64,64)\nout=attn(x)\nprint(out.shape)\nThe output after running the preceding code listing is\ntorch.Size([1, 128, 64, 64])\nThe attention mechanism used here, SDPA, is the same as the one we utilized in chap -\nter 9, where we applied SDPA to a sequence of indices representing tokens in text. \nHere, we apply it to pixels in an image. We treat the flattened pixels of an image as a \nsequence and use SDPA to extract dependencies among different areas of the input \nimage, enhancing the efficiency of the denoising process.\nListing 15.3 demonstrates how SDPA operates in our context. To give you a concrete \nexample, we have created a hypothetical image, x, with dimensions (1, 128, 64, 64), Passes the input through \nthree linear layers to obtain \nquery, key, and value\nSplits query, key, and value into four heads\nCalculates attention \nweights\nCalculates \nthe \nattention \nvector in \neach head\nConcatenates the four \nattention vectors into one\n 353 Building a denoising U-Net model\nindicating one image in the batch, 128 feature channels, and a size of 64 × 64 pixels in \neach channel. The input x is then processed through the attention layer. Specifically, \neach feature channel in the image is flattened into a sequence of 64 × 64 = 4,096 pixels. \nThis sequence is then passed through three distinct neural network layers to produce \nthe query Q, key K, and value V, which are subsequently split into four heads. The atten -\ntion vector in each head is calculated as follows:\nwhere dk represents the dimension of the key vector K. The attention vectors from the \nfour heads are concatenated back into a single attention vector. \n15.3.2  The denoising U-Net model\nIn the local module unet_util.py you just downloaded, we have defined a UNet()  class \nto represent the denoising U-Net model. Take a look at the definition in the file, and \nI’ll provide a brief explanation of how it works later. The following code listing pres -\nents a portion of the UNet()  class.\nListing 15.4    Defining the UNet()  class\nclass UNet(nn.Module):\n… \n    def forward(self, sample, timesteps):    \n        if not torch.is_tensor(timesteps):\n            timesteps = torch.tensor([timesteps],\n                                     dtype=torch.long,\n                                     device=sample.device)\n        timesteps = torch.flatten(timesteps)\n        timesteps = timesteps.broadcast_to(sample.shape[0])\n        t_emb = sinusoidal_embedding(timesteps, self.hidden_dims[0])\n        t_emb = self.time_embedding(t_emb)    \n        x = self.init_conv(sample)\n        r = x.clone()\n        skips = []\n        for block1, block2, attn, downsample in self.down_blocks:    \n            x = block1(x, t_emb)\n            skips.append(x)\n            x = block2(x, t_emb)\n            x = attn(x)\n            skips.append(x)\n            x = downsample(x)\n        x = self.mid_block1(x, t_emb)\n        x = self.mid_attn(x)\n        x = self.mid_block2(x, t_emb)    \n        for block1, block2, attn, upsample in self.up_blocks:    \n            x = torch.cat((x, skips.pop()), dim=1)    \n            x = block1(x, t_emb)\n            x = torch.cat((x, skips.pop()), dim=1)The model takes a batch \nof noisy images and the \ntime steps as input.\nThe embedded time steps are \nadded to the images as \ninputs in various stages.\nPasses the input \nthrough the \ncontracting path\nPasses the input through \nthe bottleneck path\nPasses the input through \nthe expansive path, with \nskip connections\n354 chapter  15 Diffusion models and text-to-image Transformers \n            x = block2(x, t_emb)\n            x = attn(x)\n            x = upsample(x)\n        x = self.out_block(torch.cat((x, r), dim=1), t_emb)\n        out = self.conv_out(x)\n        return {""sample"": out}    \nThe job of the denoising U-Net is to predict the noise in the input images based on \nthe time steps these images are in. As described in equation 15.3, a noisy image at any \ntime step t, xt, can be represented as a weighted sum of the clean image, xo, and stan -\ndard normally distributed random noise, є. The weight assigned to the clean image \ndecreases, and the weight assigned to the random noise increases as the time step t \nprogresses from 0 to T. Therefore, to deduce the noise in noisy images, the denoising \nU-Net needs to know which time step a noisy image is in. \nTime steps are embedded using sine and cosine functions in a manner akin to posi -\ntional encoding in Transformers (discussed in chapters 9 and 10), resulting in a 128-\nvalue vector. These embeddings are then expanded to match the dimensions of the \nimage features at various layers within the model. For instance, in the first down block, \nthe time embeddings are broadcasted to a shape of (128, 64, 64) before being added to \nthe image features, which also have dimensions of (128, 64, 64). \nNext, we create a denoising U-Net model by instantiating the UNet()  class in the \nlocal module: \nfrom utils.unet_util import UNet\ndevice=""cuda"" if torch.cuda.is_available() else ""cpu""\nresolution=64\nmodel=UNet(3,hidden_dims=[128,256,512,1024],\n           image_size=resolution).to(device)\nnum=sum(p.numel() for p in model.parameters())\nprint(""number of parameters: %.2fM"" % (num/1e6,))\nprint(model) \nThe output is \nnumber of parameters: 133.42M\nThe model has more than 133 million parameters, as you can see from the previous \noutput. Given the large number of parameters, the training process in this chapter will \nbe time-consuming, requiring approximately 3 to 4 hours of GPU training. However, \nfor those who do not have access to GPU training, the trained weights are also available \non my website. The link to these weights will be provided in the following section. \n15.4 Training and using the denoising U-Net model\nNow that we have both the training data and the denoising U-Net model, we’re ready \nto train the model using the training data.\nDuring each training epoch, we’ll cycle through all the batches in the training data. \nFor each image, we’ll randomly select a time step and add noise to the clean images The output is the predicted \nnoise in the input images.\n 355 Training and using the denoising U-Net model\nin the training data based on this time step value, resulting in a noisy image. These \nnoisy images and their corresponding time step values are then fed into the denoising \nU-Net model to predict the noise in each image. We compare the predicted noise to the \nground truth (the actual noise added to the image) and adjust the model parameters to \nminimize the mean absolute error between the predicted and actual noise.\nAfter training, we’ll use the trained model to generate flower images. We’ll perform \nthis generation in 50 inference steps (i.e., we’ll set time step values to 980, 960, . . ., 20, \nand 0). Starting with random noise, we’ll input it into the trained model to obtain a \nnoisy image. This noisy image is then fed back into the trained model to denoise it. We \nrepeat this process for 50 inference steps, resulting in an image that is indistinguishable \nfrom the flowers in the training set. \n15.4.1  Training the denoising U-Net model\nNext, we’ll first define the optimizer and the learning rate scheduler for the training \nprocess.\nWe’ll use the AdamW optimizer, a variant of the Adam optimizer that we have been \nusing throughout this book. The AdamW optimizer, first proposed by Ilya Loshchilov \nand Frank Hutter, decouples weight decay (a form of regularization) from the optimi -\nzation steps.3 Instead of applying weight decay directly to the gradients, AdamW applies \nweight decay directly to the parameters (weights) after the optimization step. This mod -\nification helps achieve better generalization performance by preventing the decay rate \nfrom being adapted along with the learning rates. Interested readers can learn more \nabout the AdamW optimizer in the original paper by Loshchilov and Hutter.\nWe will also use a learning rate scheduler from the diffusers library to adjust the learn -\ning rate during the training process. Initially using a higher learning rate can help the \nmodel escape local minima, while gradually lowering the learning rate in later stages of \ntraining can help the model converge more steadily and accurately towards a global min -\nimum. The learning rate scheduler is defined as shown in the following listing.\nListing 15.5    Choosing the optimizer and learning rate in training\nfrom diffusers.optimization import get_scheduler\nnum_epochs=100    \noptimizer=torch.optim.AdamW(model.parameters(),lr=0.0001,\n    betas=(0.95,0.999),weight_decay=0.00001,eps=1e-8)    \nlr_scheduler=get_scheduler(    \n    ""cosine"",\n    optimizer=optimizer,\n    num_warmup_steps=300,\n    num_training_steps=(len(train_dataloader) * num_epochs))       \n3 Ilya Loshchilov and Frank Hutter, 2017, “Decoupled Weight Decay Regularization.” https: //arxiv.org/\nabs/1711.05101 . Will train the model \nfor 100 epochs\nUses the AdamW \noptimizer\nUses the learning \nrate scheduler in \nthe diffusers library \nto control the \nlearning rate\n356 chapter  15 Diffusion models and text-to-image Transformers \nThe exact definition of the get_scheduler()  function is defined on GitHub by \nHugging Face: https: //mng.bz/ZVo5 . In the first 300 training steps (warmup steps), \nthe learning rate increases linearly from 0 to 0.0001 (the learning rate we set in the \nAdamW optimizer). After 300 steps, the learning rate decreases following the values of \nthe cosine function between 0.0001 and 0. We train the model for 100 epochs in the \nfollowing listing.\nListing 15.6    Training the denoising U-Net model\nfor epoch in range(num_epochs):\n    model.train()\n    tloss = 0\n    print(f""start epoch {epoch}"")\n    for step, batch in enumerate(train_dataloader):\n        clean_images = batch[""input""].to(device)*2-1\n        nums = clean_images.shape[0]\n        noise = torch.randn(clean_images.shape).to(device)\n        timesteps = torch.randint(0,\n                noise_scheduler.num_train_timesteps,\n                (nums, ),\n                device=device).long()\n        noisy_images = noise_scheduler.add_noise(clean_images,\n                     noise, timesteps)    \n        \nnoise_pred = model(noisy_images, \n                       timesteps)[""sample""]    \n        loss=torch.nn.functional.l1_loss(noise_pred, noise)\n        loss.backward()\n        optimizer.step()    \n        lr_scheduler.step()\n        optimizer.zero_grad()\n        tloss += loss.detach().item()\n        if step%100==0:\n            print(f""step {step}, average loss {tloss/(step+1)}"")\ntorch.save(model.state_dict(),'files/diffusion.pth')\nDuring each epoch, we cycle through all batches of clean flower images in the training \nset. We introduce noise to these clean images and feed them to the denoising U-Net to \npredict the noise in these images. We then compare the predicted noise to the actual \nnoise and adjust the model parameters to minimize the mean absolute error (pixel-\nwise) between the two.\nThe training process described here takes several hours with GPU training. After \ntraining, the trained model weights are saved on your computer. Alternatively, you can \ndownload the trained weights from my website at https: //mng.bz/RNlD . Unzip the file \nafter downloading. Adds noise \nto clean \nimages in \nthe training \nset\nUses the denoising \nU-Net to predict noise \nin noisy images\nCompares the \npredicted \nnoise with the \nactual noise \nto calculate \nthe loss\nTweaks model \nparameters to \nminimize the mean \nabsolute error",18195
145-15.4.2 Using the trained model to generate flower images.pdf,145-15.4.2 Using the trained model to generate flower images,"357 Training and using the denoising U-Net model\n15.4.2  Using the trained model to generate flower images\nTo generate flower images, we’ll use 50 inference steps. This means we’ll look at 50 \nequally spaced time steps between t = 0 and t = T, with T = 1,000 in our case. Therefore, \nthe 50 inference time steps are t = 980, 960, 940, . . . , 20, and 0. We’ll start with pure \nrandom noise, which corresponds to the image at t = 1000. We use the trained denois -\ning U-Net model to denoise it and create a noisy image at t = 980. We then present the \nnoisy image at t = 980 to the trained model to denoise it and obtain the noisy image \nat t = 960. We repeat the process for many iterations until we obtain an image at t = 0, \nwhich is a clean image. This process is implemented through the generate()  method \nin the DDIMScheduler()  class within the local module ch15util.py.\nListing 15.7    Defining a generate()  method in the DDIMScheduler()  class\n    @torch.no_grad()\n    def generate(self,model,device,batch_size=1,generator=None,\n         eta=1.0,use_clipped_model_output=True,num_inference_steps=50):\n        imgs=[]\n        image=torch.randn((batch_size,model.in_channels,model.sample_size,\n            \nmodel.sample_size),\n            generator=generator).to(device)    \n        self.set_timesteps(num_inference_steps)\n        for t in tqdm(self.timesteps):    \n            model_output = model(image, t)[""sample""]    \n            image = self.step(model_output,t,image,eta,\n                  use_clipped_model_output=\\n                  use_clipped_model_output)     \n            img = unnormalize_to_zero_to_one(image)\n            img = img.cpu().permute(0, 2, 3, 1).numpy()\n            imgs.append(img)    \n        image = unnormalize_to_zero_to_one(image)\n        image = image.cpu().permute(0, 2, 3, 1).numpy()\n        return {""sample"": image}, imgs \nIn this generate() method, we have also created a list, imgs, to store all intermediate \nimages at time steps t = 980, 960,. . . , 20, and 0. We’ll use them to visualize the denois -\ning process later. The generate() method returns a dictionary with the generated \nimages and the list, imgs.\nNext, we’ll use the previous generate() method to create 10 clean images.\nListing 15.8     Image generation with the trained denoising U-Net model\nsd=torch.load('files/diffusion.pth',map_location=device)\nmodel.load_state_dict(sd)\nwith torch.no_grad():\n    generator = torch.manual_seed(1)    \n    generated_images,imgs = noise_scheduler.generate(\n        model,device,\n        num_inference_steps=50,Uses random noise as the starting \npoint (i.e., image at t = 1,000)\nUses 50 inference time steps \n(t = 980, 960, 940,  . . , 20, 0)\nUses the trained \ndenoising U-Net model \nto predict noise\nCreates an image based \non predicted noise\nSaves intermediate \nimages in a list, imgs\nSets the random seed \nto 1 so results are \nreproducible\n358 chapter  15 Diffusion models and text-to-image Transformers \n        generator=generator,\n        eta=1.0,\n        use_clipped_model_output=True,\n        batch_size=10)    \nimgnp=generated_images[""sample""]    \nimport matplotlib.pyplot as plt\nplt.figure(figsize=(10,4),dpi=300)\nfor i in range(10):    \n    ax = plt.subplot(2,5, i + 1)\n    plt.imshow(imgnp[i])\n    plt.xticks([])\n    plt.yticks([])\n    plt.tight_layout()\nplt.show()  \nWe set the random seed to 1. As a result, if you use the trained model from my website, \nyou’ll get identical results as shown in figure 15.6. We use the generate()  method \ndefined earlier to create 10 clean images, using 50 inference steps. We then plot the 10 \nimages in a 2 × 5 grid, as shown in figure 15.6.\nFigure 15.6    Flower images created by the trained denoising U-Net model.\nAs you can see from figure 15.6, the generated flower images look real and resemble \nthose in the training dataset. \nExercise 15.1\nModify code listing 15.8 and change the random seed to 2. Keep the rest of the code the \nsame. Rerun the code listing and see what the generated images look like. \nThe generate()  method also returns a list, imgs, which contains all the images in the \n50 intermediate steps. We’ll use them to visualize the denoising process.\nListing 15.9    Visualizing the denoising process\nsteps=imgs[9::10]    \nimgs20=[]\nfor j in [1,3,6,9]:\n    for i in range(5):Uses the defined generate() \nmethod to create 10 clean images\nPlots the generated images\nKeeps time steps 800, \n600, 400, 200, and 0\n 359 Training and using the denoising U-Net model\n        imgs20.append(steps[i][j])    \nplt.figure(figsize=(10,8),dpi=300)\nfor i in range(20):    \n    k=i%5\n    ax = plt.subplot(4,5, i + 1)\n    plt.imshow(imgs20[i])\n    plt.xticks([])\n    plt.yticks([])\n    plt.tight_layout()\n    plt.title(f't={800-200*k}',fontsize=15,c=""r"")\nplt.show()\nThe list, imgs, contains 10 sets of images in all 50 inference steps, t = 980, 960, . . . , 20, \n0. So there are a total of 500 images in the list. We select five time steps (t = 800, 600, \n400, 200, and 0) for four different flowers (the 2nd, 4th, 7th, and 10th images in figure \n15.6). We then plot the 20 images in a 4 × 5 grid, as shown in figure 15.7.\nFigure 15.7    How the trained denoising U-Net model gradually converts random noise into clean flower \nimages. We feed random noise to the trained model to obtain the image at time step 980. We then \nfeed the noisy image at t = 980 to the model to obtain the image at t = 960. We repeat this process 50 \ninference steps until we obtain the image at t = 0. The first column in this figure shows the four flowers at \nt = 800; the second column shows the same four flowers at t = 600 . . . ; the last column shows the four \nflowers at t = 0 (i.e., clean flower images). Selects 4 sets of \nflowers out of 10\nPlots the 20 images \nin a 4 × 5 grid",5886
146-15.5 Text-to-image Transformers.pdf,146-15.5 Text-to-image Transformers,,0
147-15.5.1 CLIP A multimodal Transformer.pdf,147-15.5.1 CLIP A multimodal Transformer,"360 chapter  15 Diffusion models and text-to-image Transformers \nThe first column in figure 15.7 shows the four flower images at t = 800. They are close \nto random noise. The second column shows the flowers at t = 600, and they start to \nlook like flowers. As we move to the right, the images become clearer and clearer. The \nrightmost column shows the four clean flower images at t = 0. \nNow that you understand how diffusion models work, we’ll discuss text-to-image \ngeneration. The image generation process of text-to-image Transformers such as \nDALL-E 2, Imagen, and Stable Diffusion is very much like the reverse diffusion process \nwe discussed earlier in the chapter, except that the model takes the text embedding as a \nconditioning signal when generating an image. \n15.5 Text-to-image Transformers\nText-to-image Transformers such as OpenAI’s DALL-E 2, Google’s Imagen, and Sta -\nbility AI’s Stable Diffusion use diffusion models to generate images from textual \ndescriptions. An important component of these text-to-image Transformers is a diffu -\nsion model. The process of text-to-image generation involves encoding the text input \ninto a latent representation, which is then used as a conditioning signal for the diffu -\nsion model. These Transformers learn to generate lifelike images that correspond to \nthe textual description by iteratively denoising a random noise vector, guided by the \nencoded text.\nThe key to all these text-to-image Transformers is a model to understand content in \ndifferent modalities. In this case, the model must understand the text descriptions and \nlink them to images and vice versa. \nIn this section, we’ll use OpenAI’s CLIP model as an example. CLIP is a key compo -\nnent in DALL-E 2. We’ll discuss how CLIP was trained to understand the connection \nbetween text descriptions and images. We then use a short Python program to generate \nan image from a text prompt by using OpenAI’s DALL-E 2.\n15.5.1  CLIP: A multimodal Transformer\nIn recent years, the intersection of computer vision and natural language processing \n(NLP) has witnessed significant advancements, one of which is the creation of the \nCLIP model by OpenAI. This innovative model is designed to understand and inter -\npret images in the context of natural language, a capability that holds immense poten -\ntial for various applications such as image generation and image classification. \nThe CLIP model is a multimodal Transformer that bridges the gap between visual \nand textual data. It is trained to understand images by associating them with corre -\nsponding textual descriptions. Unlike traditional models that require explicit labeling \nof images, CLIP uses a vast dataset of images and their natural language descriptions to \nlearn a more generalizable representation of visual concepts.\n 361 Text-to-image Transformers\nHorses walk \non the beach\nI1T1I1T2I1T3... I1TN\nI2T1 I2T2I2T3... I2TN\nI3T1 I3T2I3T3... I3TN\n... ... ... ... ...\nINT1INT2INT3... INTNT1 T2 T3... TN\nI1\nI2\nI3\n...\nIN......\nImage \nencoderText \nEncoder\nText embedding\nImage embeddingText 1\nA dog plays on \nthe lawnText N...\nImage 1\nImage N\n...\nFigure 15.8    How OpenAI’s CLIP model is trained. A large-scale training dataset of text–image \npairs is collected. The text encoder of the model compresses the text description into a D-value text \nembedding. The image encoder converts the corresponding image into an image embedding also with \nD values. During training, a batch of N text–image pairs are converted to N text embeddings and N \nimage embeddings. CLIP uses a contrastive learning approach to maximize the similarity between \npaired embeddings (the sum of diagonal values in the figure) while minimizing the similarity between \nembeddings from nonmatching text–image pairs (the sum of off-diagonal values in the figure).\nThe training of the CLIP model, which is illustrated in figure 15.8, begins with the col -\nlection of a large-scale dataset comprising images and their associated textual descrip -\ntions. OpenAI utilizes a diverse set of sources, including publicly available datasets and \nweb-crawled data, to ensure a wide variety of visual and textual content. The dataset is \nthen preprocessed to standardize the images so they all have the same shape and to \ntokenize the text, preparing them for input into the model.\nCLIP employs a dual-encoder architecture, consisting of an image encoder and a \ntext encoder. The image encoder processes the input images while the text encoder \nprocesses the corresponding textual descriptions. These encoders project the images \nand text into a shared embedding space where they can be compared and aligned.\nThe core of CLIP’s training lies in its contrastive learning approach. For each batch \nof N image–text pairs in the dataset, the model aims to maximize the similarity between \npaired embeddings (measured by the sum of diagonal values in figure 15.8) while min -\nimizing the similarity between embeddings from nonmatching text-image pairs (the \nsum of off-diagonal values). Figure 15.9 is a diagram of how text-to-image Transformers \nsuch as DALL-E 2 generate realistic images based on text prompts.",5225
148-15.5.2 Text-to-image generation with DALL-E 2.pdf,148-15.5.2 Text-to-image generation with DALL-E 2,"362 chapter  15 Diffusion models and text-to-image Transformers \n...\nFinal output Noisy image Random noise\nText description:\ne.g., A yellow aster\nﬂowerText\nencoderText embedding\nU-Net\ndenoiserU-Net\ndenoiserTrained\nCLIP\nmodelPrior\nConditioning vector\nFigure 15.9    How text-to-image Transformers such as DALL-E 2 create images based on text prompts. \nThe text encoder in the trained text-to-image Transformer first converts the text description in the \nprompt into text embedding. The text embedding is fed to the CLIP model to obtain a prior vector that \nrepresents the image in the latent space. The text embedding and the prior are concatenated into a \nconditioning vector. To generate an image, the U-Net denoiser first takes a random noise vector as \ninput to generate a noisy image using the conditioning vector. It then takes the noisy image and the \nconditioning vector as input and generates another image, which is less noisy. The process is repeated \nfor many iterations until the final output, a clean image, is generated.  \nThe image generation process of text-to-image Transformers is similar to the reverse \ndiffusion process we discussed earlier in the chapter. Let’s take DALL-E 2, for exam -\nple, which was proposed by OpenAI researchers in 2022.4 The text encoder in the \nmodel first converts the text description in the prompt into a text embedding. The \ntext embedding is fed to the CLIP model to obtain a prior vector that represents the \nimage in the latent space. The text embedding and the prior are concatenated into a \nconditioning vector. In the first iteration, we feed a random noise vector to the U-Net \ndenoiser in the model and ask it to generate a noisy image based on the conditioning \nvector. In the second iteration, we feed the noisy image from the previous iteration to \nthe U-Net denoiser and ask it to generate another noisy image based on the condition -\ning vector. We repeat this process for many iterations, and the final output is a clean \nimage. \n15.5.2  Text-to-image generation with DALL-E 2\nNow that you understand how text-to-image Transformers work, let’s write a Python \nprogram to interact with DALL-E 2 to create an image based on a text prompt. \n4 Aditya Rames, Prafulla  Dhariwal, Alex Nichol, Casey Chu, and Mark Chen, 2022, “Hierarchical Text-Conditional \nImage Generation with CLIP Latents.” https: //arxiv.org/abs/2204.06125 .\n 363 Text-to-image Transformers\nFirst, you need to apply for an OpenAI API key. OpenAI offers various pricing tiers \nthat vary based on the number of tokens processed and the type of models used. Go \nto https: //chat.openai.com/auth/login  and click on the Sign up button to create an \naccount. After that, log in to your account, and go to https: //platform.openai.com/\napi-keys  to view your API key. Save it in a secure place for later use. We can generate an \nimage by using OpenAI’s DALL-E 2.\nListing 15.10    Image generation with DALL-E 2\nfrom openai import OpenAI\nopenai_api_key=your actual OpenAI API key here, in quotes    \nclient=OpenAI(api_key=openai_api_key)    \nresponse = client.images.generate(\n  model=""dall-e-2"",\n  prompt=""an astronaut in a space suit riding a unicorn"",\n  size=""512x512"",\n  quality=""standard"",\n  n=1,\n)    \nimage_url = response.data[0].url\nprint(image_url)    \nYou should place the OpenAI API key you obtained earlier in listing 15.10. We create \nan agent by instantiating the OpenAI()  class. To generate an image, we need to specify \nthe model, a text prompt, and the size of the image. We have used “an astronaut in a \nspace suit riding a unicorn” as the prompt, and the code listing provides a URL for us \nto visualize and download the image. The URL expires in an hour, and the resulting \nimage is shown in figure 15.10. \nFigure 15.10    \nAn image \ngenerated by \nDALL-E 2 with \nthe text prompt \n“an astronaut \nin a space suit \nriding a unicorn”Makes sure you \nprovide your actual \nOpenAI API key \nhere, in quotes\nInstantiates the OpenAI() \nclass to create an agent\nUses the images.generate() method to \ngenerate image based on the text prompt\nPrints out the \nimage URL\n364 chapter  15 Diffusion models and text-to-image Transformers \nRun listing 15.10 yourself and see what image DALLE-2 generates for you. Note that \nyour result will be different since the output from DALLE-2 (and all LLMs) is stochas -\ntic rather than deterministic. \nExercise 15.2\nApply for an OpenAI API key. Then modify code listing 15.10 to generate an image using \nthe text prompt “a cat in a suit working on a computer.”\nIn this chapter, you learned the inner workings of diffusion-based models and their \nsignificance in text-to-image Transformers, such as OpenAI’s CLIP model. You also dis -\ncovered how to obtain your OpenAI API key and used a brief Python script to generate \nimages from text descriptions with DALL-E 2, which incorporates CLIP.\nIn the next chapter, you will continue to use the OpenAI API key obtained earlier to \nuse pretrained LLMs for generating diverse content, including text, audio, and images. \nAdditionally, you will integrate the LangChain Python library with other APIs, enabling \nyou to create a know-it-all personal assistant. \nSummary\n¡ In forward diffusion, we gradually add small amounts of random noise to clean \nimages until they transform into pure noise. Conversely, in reverse diffusion, we \nbegin with random noise and employ a denoising model to progressively elimi -\nnate noise from the images, transforming the noise back into a clean image.  \n¡ The U-Net architecture, originally designed for biomedical image segmenta -\ntion, has a symmetric shape with a contracting encoder path and an expansive \ndecoder path, connected by a bottleneck layer. In denoising, U-Nets are adapted \nto remove noise while preserving details. Skip connections link encoder and \ndecoder feature maps of the same spatial dimensions, helping to preserve spatial \ninformation like edges and textures that may be lost during downsampling in the \nencoding process.\n¡ Incorporating an attention mechanism into a denoising U-Net model enables it \nto concentrate on important features and disregard irrelevant ones. By treating \nimage pixels as a sequence, the attention mechanism learns pixel dependencies, \nsimilar to how it learns token dependencies in NLP. This enhances the model’s \nability to identify relevant features effectively.\n¡ Text-to-image Transformers like OpenAI’s DALL-E 2, Google’s Imagen, and Sta -\nbility AI’s Stable Diffusion use diffusion models to create images from textual \ndescriptions. They encode the text into a latent representation that conditions \nthe diffusion model, which then iteratively denoises a random noise vector, \nguided by the encoded text, to generate lifelike images matching the textual \ndescription.",6887
149-16 Pretrained large language models and the LangChain library.pdf,149-16 Pretrained large language models and the LangChain library,"36516Pretrained large  \nlanguage models and the \nLangChain library\nThis chapter covers\n¡ Using pretrained large language models for text,  \n image, speech, and code generation\n¡ Few-shot, one-shot, and zero-shot prompting   \n techniques\n¡ Creating a zero-shot personal assistant with   \n LangChain\n¡ Limitations and ethical concerns of generative AI\nThe rise of pretrained large language models (LLMs) has transformed the field of \nnatural language processing (NLP) and generative tasks. OpenAI’s GPT series, a \nnotable example, showcases the extensive capabilities of these models in produc -\ning life-like text, images, speech, and even code. The effective utilization of these \npretrained LLMs is essential for several reasons. It enables us to deploy advanced AI \nfunctionalities without the need for vast resources to develop and train these mod -\nels. Moreover, understanding these LLMs paves the way for innovative applications \nthat leverage NLP and generative AI, fostering progress across various industries.\n366 chapter  16 Pretrained large language models and the LangChain library \nIn a world increasingly influenced by AI, mastering the integration and customiza -\ntion of pretrained LLMs offers a crucial competitive advantage. As AI evolves, leverag -\ning these sophisticated models becomes vital for innovation and success in the digital \nlandscape.\nTypically, these models are operated through browser-based interfaces, which vary \nacross different LLMs that function independently of each other. Each model has \nunique strengths and specialties. Interfacing through a browser limits our ability to \nfully take advantage of the potential of each specific LLM. Utilizing programming lan -\nguages like Python, particularly through tools such as the LangChain library, provides \nsubstantial benefits for the following reasons.\nPython’s role in interacting with LLMs enhances the automation of workflows and \nprocesses. Python scripts, capable of running autonomously, facilitate uninterrupted \noperations without the need for manual input. This is especially beneficial for busi -\nnesses that regularly handle large amounts of data. For instance, a Python script could \nautonomously generate monthly reports by querying an LLM, synthesizing the data \ninsights, and disseminating these findings via email or into a database. Python offers \na greater level of customization and control in managing interactions with LLMs than \nbrowser-based interfaces do, enabling us to craft custom code to meet specific opera -\ntional needs such as implementing conditional logic, processing multiple requests in \nloops, or managing exceptions. This adaptability is essential for customizing outputs to \nmeet particular business objectives or research inquiries.\nPython’s extensive collection of libraries makes it ideally suited for integrating LLMs \nwith existing software and systems. A prime example of this is the LangChain library, \nwhich extends Python’s functionality with LLMs. LangChain enables the combination \nof multiple LLMs or the integration of LLM capabilities with other services, such as the \nWikipedia API or the Wolfram Alpha API, which will be covered later in this chapter. \nThis capability of “chaining” different services allows for the construction of sophisti -\ncated, multistep AI systems where tasks are segmented and handled by the best-suited \nmodels or services, enhancing both performance and accuracy.\nTo that end, in this chapter, you’ll first learn how to use the OpenAI API to create var -\nious content using Python programming: text, images, speech, and Python code. You’ll \nalso learn the difference between few-shot, one-shot, and zero-shot content generation. \nFew-shot prompting means you give the model multiple examples to help it understand \nthe task, while one-shot or zero-shot prompting means one example or no example is \nprovided. \nModern LLMs such as ChatGPT are trained on preexisting knowledge a few months \nago so they cannot provide recent or real-time information such as weather conditions, \nflight status, or stock prices. You’ll learn to combine LLMs with Wolfram Alpha and \nWikipedia APIs using the LangChain library to create a zero-shot know-it-all personal \nassistant. \nDespite LLMs’ impressive capabilities, they do not possess an intrinsic understand -\ning of the content. This can lead to errors in logic, factual inaccuracies, and a failure",4467
150-16.1 Content generation with the OpenAI API.pdf,150-16.1 Content generation with the OpenAI API,,0
151-16.1.1 Text generation tasks with OpenAI API.pdf,151-16.1.1 Text generation tasks with OpenAI API,"367 Content generation with the OpenAI API\nto grasp complex concepts or nuances. The rapid advancement and widespread appli -\ncation of these models also lead to various ethical concerns such as bias, misinforma -\ntion, privacy, and copyright. These issues demand careful consideration and proactive \nmeasures to ensure that the development and deployment of LLMs align with ethical \nstandards and societal values. \n16.1 Content generation with the OpenAI API\nWhile there are other LLMs such as Meta’s LLAMA and Google’s Gemini, OpenAI’s \nGPT series is the most prominent one. We therefore use OpenAI GPTs as our examples \nin this chapter. \nOpenAI allows you to use LLMs to generate various content such as text, images, \naudio, and code. You can access their service either through a web browser or an API. \nWe’ll focus on content generation with Python programs via an API in this chapter due \nto the advantages of interacting with LLMs using Python mentioned earlier. \nYou do need your OpenAI API key for the programs in this chapter to work. I assume \nyou have already obtained your API key in chapter 15. If not, go back to chapter 15 for \ndetailed instructions on how to get one. \nI’ll focus mainly on text generation in this section but will provide an example for \neach of the cases of code, image, and speech generation. \nThis chapter involves the use of several new Python libraries. To install them, run the \nfollowing lines of code in a new cell in your Jupypter Notebook app on your computer:\n!pip install --upgrade openai langchain_openai langchain\n!pip install wolframalpha langchainhub\n!pip install --upgrade --quiet wikipedia\nFollow the on-screen instructions to finish the installation.\n16.1.1  Text generation tasks with OpenAI API\nYou can generate text for many different purposes, such as question-answering, text \nsummarization, and creative writing. \nWhen you ask OpenAI GPT a question, keep in mind that all LLMs, including Ope -\nnAI GPTs, are trained on historical data gathered through automated web crawling. As \nof this writing, GPT-4 was trained using data up to December 2023, with a three-month \nlag. GPT-3.5 was trained on data up to September 2021. \nLet’s first ask GPT a question about historical facts. Enter the lines of code in the \nfollowing listing in a new cell.\nListing 16.1    Checking historical facts with OpenAI API\nfrom openai import OpenAI\nopenai_api_key=put your actual OpenAI API key here, in quotes    \nclient=OpenAI(api_key=openai_api_key)    \ncompletion = client.chat.completions.create(Provides your \nOpenAI API key\nCreates an OpenAI() \nclass instance and \nnames it client \n368 chapter  16 Pretrained large language models and the LangChain library \n  model=""gpt-3.5-turbo"",\n  messages=[\n    {""role"": ""system"", ""content"":    \n     '''You are a helpful assistant, knowledgeable about recent facts.'''},\n    {""role"": ""user"", ""content"": \n     '''Who won the Nobel Prize in Economics in 2000?'''}    \n  ]\n)\nprint(completion.choices[0].message.content)\nMake sure you provide your OpenAI API key in the listing 16.1. We first instantiate \nthe OpenAI() class and call it client . In the chat.completions.create()  method, \nwe specify the model as gpt-3.5-turbo . The site https: //platform.openai.com/docs/\nmodels  provides various models available. You can use either gpt-4 or gpt-3.5-turbo for \ntext generation. The former provides better results but also incurs higher expenses. \nWe’ll use the latter for most cases since our examples are simple enough, so it provides \nequally good results. \nThe messages  parameter in the preceding code block consists of several message \nobjects, with each object containing a role (which can be “system,” “user,” or “assistant”) \nand content. A system message determines the assistant’s behavior; absent a system \nmessage, the default setting characterizes the assistant as “a helpful assistant.” User \nmessages include inquiries or remarks for the assistant to address. For instance, in \nthe previous example, the user message is “Who won the Nobel Prize in Economics in \n2000?” The output is\nThe Nobel Prize in Economics in 2000 was awarded to James J. Heckman and \nDaniel L. McFadden for their work on microeconometrics and microeconomic \ntheory. \nOpenAI has provided the correct answer.\nYou can also ask the LLM to write an essay on a certain topic. Next, we ask it to write a \nshort essay on the importance of self-motivation:\ncompletion = client.chat.completions.create(\n  model=""gpt-3.5-turbo"",\n  n=1,\n  messages=[\n    {""role"": ""system"", ""content"": \n     '''You are a helpful assistant, capable of writing essays.'''},\n    {""role"": ""user"", ""content"": \n     '''Write a short essay on the importance of self-motivation.'''}\n  ]\n)\nprint(completion.choices[0].message.content)\nThe n=1 argument here tells the assistant to generate one response. If you want mul -\ntiple responses, you can set n to a different number. The default value for n is 1. The \noutput is Defines the role \nof the system\nAsks the question\n 369 Content generation with the OpenAI API\nSelf-motivation is a key factor in achieving success and personal growth in\n various aspects of life. It serves as the driving force behind our \n actions, decisions, and goals, pushing us to overcome obstacles and \n challenges along the way.\nOne of the primary benefits of self-motivation is that it helps individuals\n take initiative and control of their lives…\nThe output is six paragraphs long, and I have included only the first few sentences. \nYou can go to the book’s GitHub repository ( https: //github.com/markhliu/DGAI ) to \nsee the whole essay. As you can see, the writing is coherent, to the point, and without \ngrammatical errors. \nYou can even ask OpenAI’s GPT to write a joke for you:\ncompletion = client.chat.completions.create(\n  model=""gpt-3.5-turbo"",\n  messages=[\n    {""role"": ""system"", ""content"": \n     '''You are a helpful assistant, capable of telling jokes.'''},\n    {""role"": ""user"", ""content"": \n     '''Tell me a math joke.'''}\n  ]\n)\nprint(completion.choices[0].message.content)\nWe asked it to tell a math joke, and the result is \nWhy was the equal sign so humble? Because he knew he wasn't less than or \ngreater than anyone else!\nYou can carry out back-and-forth conversations with the assistant. The messages param -\neter automatically includes conversation history. For example, after running the previ -\nous code block, if you run the following:\ncompletion = client.chat.completions.create(\n  model=""gpt-3.5-turbo"",\n  messages=[\n    {""role"": ""user"", ""content"": \n     '''Haha, that's funny! Tell me another one.'''} \n  ]\n)\nprint(completion.choices[0].message.content)\nyou’ll get a response similar to\nSure! Why was the math book sad? Because it had too many problems.\nThe user’s query “Haha, that’s funny! Tell me another one.” only makes sense in the \ncontext of the prior messages where you ask the assistant to tell a math joke. Other text \ngeneration capabilities include text summarization and classification, and you’ll see \nsuch examples later in the chapter.",7167
152-16.1.4 Speech generation with OpenAI API.pdf,152-16.1.4 Speech generation with OpenAI API,"370 chapter  16 Pretrained large language models and the LangChain library \n16.1.2  Code generation with OpenAI API\nCodex is specifically designed to understand and generate code. It can work with mul -\ntiple programming languages and can translate natural language descriptions into \ncode. Codex is now an integrated part of OpenAI GPTs. \nNext, we ask OpenAI GPT to produce a Python program to plot a sine curve:\ncompletion = client.chat.completions.create(\n  model=""gpt-3.5-turbo"",\n  messages=[\n    {""role"": ""system"", ""content"": \n     '''You are a helpful assistant, \n     capable of generating Python programs.'''},\n    {""role"": ""user"", ""content"": \n     '''Write a Python program to plot a sine graph.'''}\n  ]\n)\nprint(completion.choices[0].message.content)\nNote that ChatGPT uses GPT-3.5-Turbo to manage both the conversational aspect and \nthe code generation tasks effectively. It does not provide a separate Codex model for \ncode generation only. The output is \nSure! Here's a simple Python program using the matplotlib library to plot \na sine graph:\n```python\nimport matplotlib.pyplot as plt\nimport numpy as np\n# Generate x values from 0 to 2*pi\nx = np.linspace(0, 2*np.pi, 100)\n# Calculate y values using the sine function\ny = np.sin(x)\n# Plot the sine graph\nplt.figure()\nplt.plot(x, y)\nplt.title('Sine Graph')\nplt.xlabel('x')\nplt.ylabel('sin(x)')\nplt.grid(True)\nplt.show()\n```\nYou can run this program in a Python environment with the matplotlib \nlibrary installed to visualize the sine graph.\nIf you copy and paste the generated Python program into a cell in Jupyter Notebook \nand run it, you’ll see an image similar to figure 16.1.\n 371 Content generation with the OpenAI API\nFigure 16.1    Use OpenAI GPT-3.5 to generate Python code to plot a sine curve. We use the text \ndescription “Write a Python program to plot a sine graph” to ask it to generate a Python program. We \nthen run the program to create the graph. \nThe LLM not only provides the Python code, but it also lets you know that you need to \nrun the code in a Python environment with the matplotlib library installed. \n16.1.3  Image generation with OpenAI DALL-E 2\nDALL-E 2 is an AI model developed by OpenAI, designed to generate images from \ntextual descriptions. It is a successor to the original DALL-E model and represents \nadvancements in the field of generative AI for visual content.\nDALL-E 2 uses a diffusion model similar to what we discussed in chapter 15, which \nstarts with a random pattern of pixels and gradually refines it into a coherent image \nthat matches the input text. It has improved upon the original DALL-E by producing \nhigher-quality images with more accurate and detailed representations of the textual \ndescriptions. \nIncorporating DALL-E 2 into OpenAI’s GPT series allows us to not only generate \ntext but also create images based on text prompts. Next, we ask DALL-E 2 to create an \nimage of someone fishing at the riverbank:\nresponse = client.images.generate(\n  model=""dall-e-2"",\n  prompt=""someone fishing at the river bank"",\n  size=""512x512"",\n  quality=""standard"",\n  n=1,\n)\nimage_url = response.data[0].url\nprint(image_url)\nThe code block generates a URL. If you click on the URL, you’ll see an image similar \nto figure 16.2.",3301
153-16.2.2 Using the OpenAI API in LangChain.pdf,153-16.2.2 Using the OpenAI API in LangChain,"372 chapter  16 Pretrained large language models and the LangChain library \nFigure 16.2    An image generated by DALL-E 2 with the text prompt “someone fishing at the riverbank”\nThe URL expires in an hour, so make sure you access it promptly. Furthermore, the \nimage generated by DALL-E 2 is slightly different even if you use the same text prompt \nbecause the output is randomly generated. \n16.1.4  Speech generation with OpenAI API\nText-to-speech (TTS) is a technology that converts written text into spoken words. TTS \nis trained through multimodal Transformers in which the input is text and the output \nis in audio format. In the context of ChatGPT, integrating TTS capabilities means that \nthe LLM can not only generate textual responses but can also speak them out loud. \nNext, we ask OpenAI API to convert a short text into speech:\nresponse = client.audio.speech.create(\n  model=""tts-1-hd"",\n  voice=""shimmer"",\n  input='''This is an audio file generated by \n    OpenAI's text to speech AI model.'''\n)\nresponse.stream_to_file(""files/speech.mp3"")\nAfter running the previous code cell, a file, speech.mp3, is saved on your computer, \nand you can listen to it. The documentation site ( https: //platform.openai.com/docs/\nguides/text-to-speech ) provides voice options. Here we have chosen the shimmer  \noption. Other options include alloy , echo , and so on. \n16.2 Introduction to LangChain\nLangChain is a Python library designed to facilitate the use of LLMs in various appli -\ncations. It provides a suite of tools and abstractions that make it easier to build, deploy, \nand manage applications powered by LLMs like GPT-3, GPT-4, and other similar \nmodels.\n 373 Introduction to LangChain\nLangChain abstracts away the complexities of interacting with different LLMs and \napplications, allowing developers to focus on building their application logic without \nworrying about the underlying model specifics. It is particularly well suited for building \na “know-it-all” agent by chaining together an LLM with applications like Wolfram Alpha \nand Wikipedia that can provide real-time information or recent facts. LangChain’s \nmodular architecture allows for easy integration of different components, enabling the \nagent to leverage the strengths of various LLMs and applications.\n16.2.1  The need for the LangChain library\nImagine that your goal is to build a zero-shot know-it-all agent so that it can produce \nvarious content, retrieve real-time information, and answer factual questions for us. \nYou want the agent to automatically go to the right source to retrieve the relevant infor -\nmation based on the task at hand without explicitly telling it what to do. The Lang -\nChain library is the right tool for this.\nIn this project, you’ll learn to use the LangChain library to combine LLMs with the \nWolfram Alpha and Wikipedia APIs to create a zero-shot know-it-all agent. We use Wol -\nfram Alpha API to retrieve real-time information and the Wikipedia API to answer ques -\ntions about recent facts. LangChain allows us to create an agent to utilize multiple tools \nto answer a question. The agent first understands the query and then decides which \ntool in the toolbox to use to answer the question.\nTo show you that even the most advanced LLMs lack these abilities, let’s ask who won \nthe Best Actor Award in the 2024 Academy Awards:\ncompletion = client.chat.completions.create(\n  model=""gpt-4"",\n  messages=[\n    {""role"": ""system"", ""content"": \n     '''You are a helpful assistant, knowledgeable about recent facts.'''},\n    {""role"": ""user"", ""content"": \n     '''Who won the Best Actor Award in 2024 Academy Awards?'''}\n  ]\n)\nprint(completion.choices[0].message.content)\nThe output is\nI'm sorry, but I cannot provide real-time information or make predictions \nabout future events such as the 2024 Academy Awards. For the most accurate \nand up-to-date information, I recommend checking reliable sources or news \noutlets closer to the date of the awards show.\nI made this query on March 17, 2024, and GPT-4 was not able to answer the question. \nIt’s possible that when you make the same query, you’ll get the correct answer because \nthe model has been updated using more recent data. If that’s the case, change the \nquestion to an event a few days ago, and you should get a similar response. \nTherefore, we’ll use LangChain to chain together an LLM with the Wolfram Alpha \nand Wikipedia APIs. Wolfram Alpha is good at scientific computations and retrieving",4528
154-16.2.3 Zero-shot one-shot and few-shot prompting.pdf,154-16.2.3 Zero-shot one-shot and few-shot prompting,"374 chapter  16 Pretrained large language models and the LangChain library \nreal-time information, while Wikipedia is famous for providing information on both \nhistorical and recent events and facts. \n16.2.2  Using the OpenAI API in LangChain\nThe langchain-openai library you installed earlier in this chapter allows you to use \nOpenAI GPTs with minimal prompt engineering. You only need to explain what you \nwant the LLM to do in plain English. \nHere is an example of how we ask it to correct grammar errors in text:\nfrom langchain_openai import OpenAI\nllm = OpenAI(openai_api_key=openai_api_key)\nprompt = """"""\nCorrect the grammar errors in the text:\ni had went to stor buy phone. No good. returned get new phone.\n""""""\nres=llm.invoke(prompt)\nprint(res)\nThe output is\nI went to the store to buy a phone, but it was no good. I returned it and \ngot a new phone.\nNote that we didn’t use any prompt engineering. We didn’t specify which model to use \neither. LangChain found the best model for the job based on the task requirements \nand other factors such as cost, latency, and performance. It also automatically formats \nand structures the queries to be suitable for the model it uses. The preceding prompt \nsimply asks the agent, in plain English, to correct the grammar errors in the text. It \nreturns text with the correct grammar, as shown in the previous output. \nHere is another example. We asked the agent to name the capital city of Kentucky:\nprompt = """"""\nWhat is the capital city of the state of Kentucky?\n""""""\nres=llm.invoke(prompt)\nprint(res)\nThe output is\nThe capital city of Kentucky is Frankfort.\nIt tells us the correct answer, which is Frankfort, Kentucky. \n16.2.3  Zero-shot, one-shot, and few-shot prompting\nFew-shot, one-shot, and zero-shot prompting refer to different ways of providing exam -\nples or instructions to LLMs to guide their responses. These techniques are used to \n 375 Introduction to LangChain\nhelp the model understand the task at hand and generate more accurate or relevant \noutputs.\nIn zero-shot prompting, the model is given a task or a question without any examples. \nThe prompt typically includes a clear description of what is expected, but the model \nmust generate a response based solely on its preexisting knowledge and understanding. \nIn one-shot prompting, the model is provided with a single example to illustrate the \ntask. In few-shot prompting, the model is given multiple examples to help it understand \nthe task. Few-shot prompting is based on the idea that providing more examples can \nhelp the model better grasp the pattern or the rules of the task, leading to more accu -\nrate responses.\nAll your interactions so far with OpenAI GPTs are zero-shot prompting since you \nhaven’t provided them with any examples. \nLet’s try an example of few-shot prompting. Suppose you want the LLM to conduct \nsentiment analysis: you want it to classify a sentence as positive or negative. You can pro -\nvide several examples in the prompt:\nprompt = """"""\nThe movie is awesome! // Positive\nIt is so bad! // Negative\nWow, the movie was incredible! // Positive\nHow horrible the movie is! //\n""""""\nres=llm.invoke(prompt)\nprint(res)\nThe output is\nNegative\nIn the prompt, we provided three examples. Two reviews are classified as positive, while \none is classified as negative. We then provided the sentence, “How horrible the movie \nis!” The LLM classified it correctly as negative. \nWe used // to separate the sentence and the corresponding sentiment in the previ -\nous example. You can use other separators such as ->, so long as you are consistent.\nHere is an example of one-shot prompting:\nprompt = """"""\nCar -> Driver\nPlane ->\n""""""\nres=llm.invoke(prompt)\nprint(res)\nThe output is\nPilot\nBy providing one single example, we are effectively asking the LLM, “What is to a plane \nas a driver is to a car?” The LLM correctly answered Pilot .",3941
155-16.3.2 Creating an agent in LangChain.pdf,155-16.3.2 Creating an agent in LangChain,"376 chapter  16 Pretrained large language models and the LangChain library \nExercise 16.1\nSuppose you want to ask the LLM, “What is to a garden as a chef is to a kitchen?” Use \none-shot prompting to get the answer. \nFinally, here is an example of zero-shot prompting:\nprompt = """"""\nIs the tone in the sentence ""Today is a great day for me"" positive, \nnegative, or neutral?\n""""""\nres=llm.invoke(prompt)\nprint(res)\nThe output is\nPositive\nWe didn’t provide any examples in the prompt. However, we provided instruction in \nplain English to ask the LLM to classify the tone in the sentence as positive, negative, \nor neutral.\n16.3 A zero-shot know-it-all agent in LangChain\nYou’ll learn to create a zero-shot know-it-all agent in LangChain in this section. You’ll \nuse OpenAI GPTs to generate various content such as text, images, and code. To com -\npensate for LLM’s inability to provide real-time information, you’ll learn to add Wol -\nfram Alpha and Wikipedia APIs to the toolbox.\nWolfram Alpha is a computational knowledge engine designed to handle factual \nqueries online, specializing in numerical and computational tasks, particularly in the \nscience and technology fields. By integrating the Wolfram Alpha API, the agent gains \nthe ability to answer virtually any question across various subjects. Should Wolfram \nAlpha be unable to provide a response, we will use Wikipedia as a secondary source for \nfact-based questions on specific topics.\nFigure 16.3 is a diagram of the steps we’ll take to create the zero-shot know-it-all \nagent in this section.\nAn agent with one tool: \nWolfram Alpha API to \nretrieve real-time info \nand recent facts\nAdd OpenAI GPT tools: \ntext summarizer, joke teller, \nsentiment classiﬁer, etc.Add Wikipedia API \nas a backup\nAn agent that can \nhandle all real-time info \nand recent-fact \nquestionsAn agent that can \nhandle almost all \nqueriesAdd image and code \ngeneration functionalitiesA zero-shot know-it-all\nagent \nFigure 16.3    \nSteps to \ncreate a zero-\nshot know-it-all \nagent with \nthe LangChain \nlibrary\n 377 A zero-shot know-it-all agent in LangChain\nSpecifically, we’ll first create an agent in LangChain with just one tool—the Wolfram \nAlpha API—to answer questions related to real-time information and recent facts. \nWe’ll then add the Wikipedia API to the toolbox as a backup on questions related \nto recent facts. We’ll add various tools utilizing the OpenAI API such as text summa -\nrizer, joke teller, and sentiment classifier. Finally, we’ll add image and code generation \nfunctionalities. \n16.3.1  Applying for a Wolfram Alpha API Key\nWolfram Alpha gives you up to 2,000 noncommercial API calls per month for free. To \nobtain an API key, first go to https: //account.wolfram.com/login/create/  and com -\nplete the steps to create an account. \nThe Wolfram account itself gives you only browser access; you need to apply for an \nAPI key at https: //products.wolframalpha.com/api/ . Once there, click Get API Access \nin the bottom left corner. A small dialog should pop up, fill in the fields Name and \nDescription, select Simple API from the dropdown menu, and then click Submit, as \nshown in figure 16.4.\nFigure 16.4    Applying for a Wolfram Alpha AppID\nAfter that, your AppID should appear in a new window. Copy the API key and save it in \na file for later use.\nHere is how you can use the Wolfram Alpha API to conduct math operations:\nimport os\nos.environ['WOLFRAM_ALPHA_APPID'] = ""your Wolfram Alpha AppID"" \nfrom langchain_community.utilities.wolfram_alpha import \\n378 chapter  16 Pretrained large language models and the LangChain library \nWolframAlphaAPIWrapper\nwolfram = WolframAlphaAPIWrapper()\nres=wolfram.run(""how much is 23*55+123?"")\nprint(res)\nThe output is\nAssumption: 23×55 + 123 \nAnswer: 1388\nThe Wolfram Alpha API provides the correct answer. \nWe’ll also include the Wikipedia API to provide answers to various topics. You don’t \nneed to apply for an API key if you have installed the Wikipedia library on your com -\nputer. Here is an example of using the Wikipedia API in the LangChain library:\nfrom langchain.tools import WikipediaQueryRun\nfrom langchain_community.utilities import WikipediaAPIWrapper\nwikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\nres=wikipedia.run(""University of Kentucky"")\nprint(res)\nThe output is\nPage: University of Kentucky\nSummary: The University of Kentucky (UK, UKY, or U of K) is a public \nland-grant research university in Lexington, Kentucky. Founded in 1865 by \nJohn Bryan Bowman as the Agricultural and Mechanical College of Kentucky, \nthe university is one of the state's two land-grant universities (the \nother being Kentucky State University)… \nWe have omitted most of the output for brevity. \n16.3.2  Creating an agent in LangChain\nNext, we’ll create an agent in LangChain, with only the Wolfram Alpha API in the \ntoolbox. An agent in this context refers to an individual entity designed to handle spe -\ncific tasks or processes through natural language interactions. We’ll then gradually add \nmore tools to the chain so that the agent becomes capable of handling more tasks. \nListing 16.2    Creating an agent in LangChain\nos.environ['OPENAI_API_KEY'] = openai_api_key \nfrom langchain.agents import load_tools\nfrom langchain_openai import ChatOpenAI\nfrom langchain import hub\nfrom langchain.agents import AgentExecutor, create_react_agent\nfrom langchain_openai import OpenAI\nprompt = hub.pull(""hwchase17/react"")\nllm = ChatOpenAI(model_name='gpt-3.5-turbo')    \ntool_names = [""wolfram-alpha""]\ntools = load_tools(tool_names,llm=llm)    \nagent = create_react_agent(llm, tools, prompt)\nagent_executor = AgentExecutor(agent=agent, tools=tools,Defines which \nLLM to use\nAdds Wolfram Alpha \nto the toolbox\n 379 A zero-shot know-it-all agent in LangChain\n       handle_parsing_errors=True,verbose=True)    \nres=agent_executor.invoke({""input"": """"""\nWhat is the temperature in Lexington, Kentucky now?\n""""""})    \nprint(res[""output""])\nThe hwchase17/react  in LangChain refers to a specific type of ReAct agent \nconfiguration. ReAct stands for Reactive Action, which is a framework within \nLangChain designed to optimize the use of language model capabilities in combination \nwith other tools to solve complex tasks effectively. See https: //python.langchain.com/\ndocs/modules/agents/agent_types/react/  for more details. When you create an agent \nin Lang  Chain, you need to specify the tools to be used by the agent. In the previous \nexample, we use only one tool, the Wolfram Alpha API. \nAs an example, we ask the current temperature in Lexington, Kentucky, and here is \nthe output:\n> Entering new AgentExecutor chain...\nI should use Wolfram Alpha to find the current temperature in Lexington, \nKentucky.\nAction: wolfram_alpha\nAction Input: temperature in Lexington, KentuckyAssumption: temperature | \nLexington, Kentucky \nAnswer: 44 °F (wind chill: 41 °F)\n(27 minutes ago)I now know the current temperature in Lexington, Kentucky.\nFinal Answer: The temperature in Lexington, Kentucky is 44 °F with a wind \nchill of 41 °F.\n> Finished chain.\nThe temperature in Lexington, Kentucky is 44 °F with a wind chill of 41 °F.\nThe output not only shows the final answer, which says the current temperature in Lex -\nington, Kentucky, is 44 degrees Fahrenheit, but it also shows the chain of thoughts. It \nuses Wolfram Alpha as the source to obtain the answer. \nWe can also add Wikipedia to the toolbox:\ntool_names += [""wikipedia""]\ntools = load_tools(tool_names,llm=llm)\nagent = create_react_agent(llm, tools, prompt)\nagent_executor = AgentExecutor(agent=agent, tools=tools,\n       handle_parsing_errors=True,verbose=True)\nres=agent_executor.invoke({""input"": """"""\nWho won the Best Actor Award in 2024 Academy Awards?\n""""""})\nprint(res[""output""])\nI ask who won the Best Actor Award in the 2024 Academy Awards, and the agent uses \nWikipedia to get the correct answer:\nI need to find information about the winner of the Best Actor Award at the \n2024 Academy Awards.\nAction: wikipediaDefines an agent\nAsks the agent \na question",8181
156-16.3.3 Adding tools by using OpenAI GPTs.pdf,156-16.3.3 Adding tools by using OpenAI GPTs,"380 chapter  16 Pretrained large language models and the LangChain library \nAction Input: 2024 Academy Awards Best Actor\n…\nCillian Murphy won the Best Actor Award at the 2024 Academy Awards for his \nperformance in Oppenheimer.\nIn the preceding output, the agent first decides to use Wikipedia as the tool to solve \nthe problem. After searching through various Wikipedia sources, the agent provides \nthe correct answer. \nNext, you’ll learn to add various OpenAI GPT tools to the agent’s toolbox.\n16.3.3  Adding tools by using OpenAI GPTs\nWe first add a text summarizer so that the agent can summarize text.\nListing 16.3    Adding a text summarizer to the agent’s tool box\nfrom langchain.agents import Tool\nfrom langchain.prompts import PromptTemplate\nfrom langchain.chains import LLMChain\ntemp = PromptTemplate(input_variables=[""text""],    \ntemplate=""Write a one sentence summary of the following text: {text}"")\nsummarizer = LLMChain(llm=llm, prompt=temp)    \nsum_tool = Tool.from_function(\n    func=summarizer.run,\n    name=""Text Summarizer"",\n    description=""A tool for summarizing texts"")    \ntools+=[sum_tool]\nagent = create_react_agent(llm, tools, prompt)    \nagent_executor = AgentExecutor(agent=agent, tools=tools,\n       handle_parsing_errors=True,verbose=True)\nres=agent_executor.invoke({""input"": \n'''Write a one sentence summary of the following text:\nThe University of Kentucky's Master of Science\n in Finance (MSF) degree prepares students for\n a professional career in the finance and banking\n industries. The program is designed to provide\n rigorous and focused training in finance, \n broaden opportunities in your career, and \n sharpened skills for the fast-changing \n and competitive world of modern finance.'''})\nprint(res[""output""])\nWe first provide a template to summarize text. We then define a summarizer function \nand add it to the toolbox. Finally, we redefine the agent by using the updated toolbox \nand ask it to summarize the example text with one sentence. Make sure your prompt \nhas the same format as those described in the template so that the agent knows which \ntool to use. Defines a template\nDefines a summarizer \nfunction\nAdds summarizer as a tool\nRedefines the agent \nwith the updated \ntoolbox\n 381 A zero-shot know-it-all agent in LangChain\nThe output from listing 16.3 is\n> Entering new AgentExecutor chain...\nI need to summarize the text provided.\nAction: Summarizer\n…\n> Finished chain.\nThe University of Kentucky's MSF program offers specialized training in \nfinance to prepare students for successful careers in the finance and \nbanking industries.\nThe agent chooses the summarizer as the tool for the task since the input matches the \ntemplate described in the summarizer function. We use two long sentences as the text \ninput and the preceding output is a one-sentence summary.\nYou can add as many tools as you like. For example, you can add a tool to tell a joke \non a certain subject:\ntemp = PromptTemplate(input_variables=[""text""],\ntemplate=""Tell a joke on the following subject: {subject}"")\njoke_teller = LLMChain(llm=llm, prompt=temp)\ntools+=[Tool.from_function(name='Joke Teller',\n       func=joke_teller.run,\n       description='A tool for telling jokes')]\nagent = create_react_agent(llm, tools, prompt)\nagent_executor = AgentExecutor(agent=agent, tools=tools,\n       handle_parsing_errors=True,verbose=True)\nres=agent_executor.invoke({""input"": \n'''Tell a joke on the following subject: coding'''})\nprint(res[""output""])\nThe output is\n> Entering new AgentExecutor chain...\nI should use the Joke Teller tool to find a coding-related joke.\nAction: Joke Teller\nAction Input: coding\nObservation: Why was the JavaScript developer sad?\nBecause he didn't know how to ""null"" his feelings.\nThought:That joke was funny!\nFinal Answer: Why was the JavaScript developer sad? Because he didn't know \nhow to ""null"" his feelings.\n> Finished chain.\nWhy was the JavaScript developer sad? Because he didn't know how to ""null"" \nhis feelings.\nWe ask the agent to tell a joke on the subject of coding . The agent identifies Joke Teller  as \nthe tool. The  joke is indeed related to coding.",4203
157-16.3.4 Adding tools to generate code and images.pdf,157-16.3.4 Adding tools to generate code and images,"382 chapter  16 Pretrained large language models and the LangChain library \nExercise 16.2\nAdd a tool to the agent’s toolbox to conduct sentiment analysis. Name the tool Sentiment \nClassifier. Then ask the agent to classify the text “this movie is so-so” as positive, nega -\ntive, or neutral. \n16.3.4  Adding tools to generate code and images\nYou can add various tools to the toolbox in LangChain. Interested readers can find \nmore details at https: //python.langchain.com/docs/modules/tools/ . Next, we add \ntools to generate other content forms such as code and images.\nTo add a tool to generate code, you can do the following:\ntemp = PromptTemplate(input_variables=[""text""],\ntemplate='''Write a Python program based on the \n    description in the following text: {text}''')\ncode_generator = LLMChain(llm=llm, prompt=temp)\ntools+=[Tool.from_function(name='Code Generator',\n       func=code_generator.run,\n       description='A tool to generate code')]\nagent = create_react_agent(llm, tools, prompt)\nagent_executor = AgentExecutor(agent=agent, tools=tools,\n       handle_parsing_errors=True,verbose=True)\nres=agent_executor.invoke({""input"": \n'''Write a Python program based on the \n    description in the following text: \nwrite a python program to plot a sine curve and a cosine curve\nin the same graph. The sine curve is in solid line and the cosine\ncurve is in dashed line. Add a legend to the graph. Set the x-axis \nrange to -5 to 5. The title should be ""Comparing sine and cosine curves.""\n'''})\nprint(res[""output""])\nThe output is \n> Entering new AgentExecutor chain...\nI should use the Code Generator tool to generate the Python program based on \nthe given description.\nAction: Code Generator\nAction Input: Write a Python program to plot a sine curve and a cosine \ncurve in the same graph. The sine curve is in solid line and the cosine \ncurve is in dashed line. Add a legend to the graph. Set the x-axis range \nto -5 to 5. The title should be ""Comparing sine and cosine curves.""\nObservation: import matplotlib.pyplot as plt\nimport numpy as np\nx = np.linspace(-5, 5, 100)\ny1 = np.sin(x)\ny2 = np.cos(x)\n 383 A zero-shot know-it-all agent in LangChain\nplt.plot(x, y1, label='Sine Curve', linestyle='solid')\nplt.plot(x, y2, label='Cosine Curve', linestyle='dashed')\nplt.legend()\nplt.title('Comparing Sine and Cosine Curves')\nplt.xlim(-5, 5)\nplt.show()\nThought:The Python program has been successfully generated to plot the sine\n and cosine curves. I now know the final answer.\nFinal Answer: The Python program to plot a sine curve and a cosine curve in\n the same graph with the specified requirements has been generated.\n> Finished chain.\nThe Python program to plot a sine curve and a cosine curve in the same \ngraph with the specified requirements has been generated.\nIf you run the generated code in a cell, you’ll see an image as in figure 16.5.\nFigure 16.5    Adding a tool in LangChain to generate Python code. The tool then generates code to plot \nsine and cosine curves in the same graph, with a legend and line styles. \nTo add an image generator, you can do the following:\nfrom langchain_community.utilities.dalle_image_generator import DallEAPIWrapper\ntemp = PromptTemplate(input_variables=[""text""],\ntemplate=""Create an image base on the following text: {text}"")\ngrapher = LLMChain(llm=llm, prompt=temp)\ntools+=[Tool.from_function(name='Text to image',\n       func=grapher.run,\n       description='A tool for text to image')]\nagent = create_react_agent(llm, tools, prompt)\nagent_executor = AgentExecutor(agent=agent, tools=tools,\n       handle_parsing_errors=True,verbose=True)\nimage_url = DallEAPIWrapper().run(agent_executor.invoke({""input"": \n'''Create an image base on the following text: \n    a horse grazes on the grassland.'''})[""output""])\nprint(image_url)",3846
158-16.4 Limitations and ethical concerns of LLMs.pdf,158-16.4 Limitations and ethical concerns of LLMs,,0
159-16.4.2 Ethical concerns for LLMs.pdf,159-16.4.2 Ethical concerns for LLMs,"384 chapter  16 Pretrained large language models and the LangChain library \nThe output is a URL for you to visualize and download an image. We asked the agent to \ncreate an image of a horse grazing on the grassland. The image is shown in figure 16.6. \nFigure 16.6    An image generated by a know-it-all agent in LangChain\nWith that, you have learned how to create a zero-shot know-it-all agent in LangChain. \nYou can add more tools to the toolbox depending on what you want the agent to \naccomplish. \n16.4 Limitations and ethical concerns of LLMs\nLLMs such as OpenAI’s GPT series have made significant strides in the field of NLP \nand generative AI. Despite their impressive capabilities, these models are not with -\nout limitations. Understanding these constraints is crucial for both leveraging their \nstrengths and mitigating their weaknesses. \nAt the same time, the rapid advancement and widespread application of these mod -\nels have also given rise to a host of ethical concerns such as bias, inaccuracies, breach of \nprivacy, and copyright infringements. These issues demand careful consideration and \nproactive measures to ensure that the development and deployment of LLMs align with \nethical standards and societal values. \nIn this section, we’ll explore the limitations of LLMs, provide insights into why these \nissues persist, and present examples of notable failures to underscore the importance \nof addressing these challenges. We’ll also examine the key ethical concerns associated \nwith LLMs and propose pathways for mitigating these concerns. \n16.4.1  Limitations of LLMs\nOne of the fundamental limitations of LLMs is their lack of true understanding and \nreasoning. While they can generate coherent and contextually relevant responses, they \n 385 Limitations and ethical concerns of LLMs\ndo not possess an intrinsic understanding of the content. This can lead to errors in \nlogic, factual inaccuracies, and a failure to grasp complex concepts or nuances.\nThis manifests in many epic mistakes made by LLMs. The book Smart Until It’s Dumb  \nprovides many entertaining instances of such mistakes made by GPT-3 and ChatGPT.1 \nFor example, consider this question: Mrs. March gave the mother tea and gruel, while \nshe dressed the little baby as tenderly as if it had been her own. Who’s the baby’s mother? \nThe answer provided by GPT-3 is Mrs. March.\nTo be fair, with the rapid advancement of LLMs, many of these mistakes are corrected \nover time. However, LLMs still make low-level mistakes. A LinkedIn article in June 2023 \nby David Johnston ( https: //www.linkedin.com/pulse/intelligence-tests-llms-fail-why  \n-david-johnston/ ) tests the intelligence of LLMs on a dozen problems that humans can \neasily solve. LLMs, including GPT-4, struggle with these problems. One of the problems \nis as follows: name an animal such that the length of the word is equal to the number of \nlegs they have minus the number of tails they have. \nThis mistake has not been corrected as of this writing. Figure 16.7 is a screenshot of \nthe answer by GPT-4 when I used a browser interface.\nFigure 16.7    How GPT-4 still makes low-level mistakes\nThe output in figure 16.7 shows that, according to GPT-4, five is equal to the number \nof letters in the word “bee.”\n16.4.2  Ethical concerns for LLMs\nOne of the most pressing ethical concerns is the potential for LLMs to perpetuate and \namplify biases in their training data. Since these models learn from vast datasets often \nderived from human-generated content, they can inherit biases related to gender, \nrace, ethnicity, and other social factors. This can result in biased outputs that reinforce \nstereotypes and discrimination.\nTo mitigate bias, it is essential to adopt diverse and inclusive training datasets, imple -\nment bias detection and correction algorithms, and ensure transparency in model \n1 Maggiori, Emmanuel, 2023, Smart Until It’s Dumb: Why Artificial Intelligence Keeps Making Epic Mistakes (and Why the AI \nBubble Will Burst) , Applied Maths Ltd. Kindle Edition.\n386 chapter  16 Pretrained large language models and the LangChain library \ndevelopment and evaluation. It’s particularly important to establish industry-wide col -\nlaboration to set standards for bias mitigation practices and promote responsible AI \ndevelopment. \nHowever, we must keep in mind not to overcorrect. A counterexample is that Goo -\ngle’s Gemini overcorrected the stereotypes in image generation by including people of \ncolor in groups like Nazi-era German soldiers.2 \nAnother concern for LLMs is their potential for misinformation and manipulation. \nLLMs have the ability to generate realistic and persuasive text, which can be exploited \nfor creating and spreading misinformation, propaganda, or manipulative content. This \nposes significant risks to public discourse, democracy, and trust in information.\nThe solution to this concern lies in developing robust content moderation systems. \nEstablishing guidelines for responsible use and fostering collaborations between AI \ndevelopers, policymakers, and media organizations are crucial steps in combating \nmisinformation.\nThe third concern is related to privacy. The vast amount of data used to train LLMs \nraises privacy concerns, as sensitive information can be inadvertently revealed in the \nmodel’s outputs. Additionally, the potential for LLMs to be used in cyberattacks or to \nbypass security measures poses significant security risks. \nFurthermore, the data used to train LLMs is mostly gathered without authorization. \nSupporters argue that the way data is used to train LLMs is transformative: the model \ndoesn’t merely regurgitate the data but uses it to generate new, original content. This \ntransformation could qualify under the “fair use” doctrine, which allows limited use of \ncopyrighted material without permission if the use adds new expression or meaning. \nCritics argue that LLMs are trained on vast amounts of copyrighted texts without per -\nmission, which goes beyond what might be considered fair use. The scale of data used \nand the direct ingestion of copyrighted material without transformation during train -\ning could be seen as infringing. The debate is ongoing. The current copyright laws were \nnot designed with generative AI in mind, leading to ambiguities about how they apply \nto technologies like LLMs. It’s a debate that likely needs to be resolved by legislative and \njudicial bodies to provide clear guidelines and ensure that the interests of all parties are \nfairly represented.\nThe ethical concerns surrounding LLMs are multifaceted and require a holistic \napproach. Collaborative efforts among AI researchers, developers, and policymakers \nare crucial in developing ethical guidelines and frameworks that guide the responsible \ndevelopment and deployment of these powerful models. As we continue to harness the \npotential of LLMs, ethical considerations must remain at the forefront of our endeav -\nors to ensure that AI advances in harmony with societal values and human well-being.\n2 Adi Robertson, February 21, 2024, “Google Apologizes for 'Missing the Mark’ after Gemini Generated Racially Di -\nverse Nazis.” The Verge, https: //mng.bz/2ga9 .\n 387 Summary\nSummary\n¡ Few-shot prompting means you give LLMs multiple examples to help them \nunderstand the task, while one-shot or zero-shot prompting means one example \nor no example is provided. \n¡ LangChain is a Python library designed to facilitate the use of LLMs in various \napplications. It abstracts away the complexities of interacting with different LLMs \nand applications. It allows the agent to automatically go to the right tool in the \ntoolbox based on the task at hand without explicitly telling it what to do.\n¡ Modern pretrained LLMs such as OpenAI’s GPT series can create various for -\nmats of content such as text, images, audio, and code.\n¡ Despite their impressive achievements, LLMs lack a true understanding of the \ncontent or the ability to reason. These limitations can lead to errors in logic, \nfactual inaccuracies, and a failure to grasp complex concepts or nuances. Fur -\nthermore, the rapid advancement and widespread application of these models \nhave given rise to a host of ethical concerns such as bias, misinformation, breach \nof privacy, and copyright infringements. These issues demand careful consider -\nation and proactive measures to ensure that the development and deployment of \nLLMs align with ethical standards and societal values.",8570
160-Appendix A Installing Python Jupyter Notebook and PyTorch.pdf,160-Appendix A Installing Python Jupyter Notebook and PyTorch,"388appendix A\nInstalling Python, Jupyter \nNotebook, and PyTorch\nVarious ways of installing Python and managing libraries and packages on your \ncomputer exist. This book uses Anaconda, an open-source Python distribution, \npackage manager, and environment management tool. Anaconda stands out for its \nuser-friendly nature and capacity to facilitate the effortless installation of numerous \nlibraries and packages, which could be painful or downright impossible to install \notherwise.\nSpecifically, Anaconda allows users to install packages through both ‘conda \ninstall’ and ‘pip install,’ broadening the spectrum of available resources. This appen -\ndix will guide you to create a dedicated Python virtual environment for all projects \nin this book. This segmentation ensures that the libraries and packages used in this \nbook remain isolated from any libraries utilized in other, unrelated projects, thus \neliminating any potential interference.\nWe will use Jupyter Notebook as our integrated development environment (IDE). \nI will guide you through the installation of Jupyter Notebook in the Python virtual \nenvironment you just created. Finally, I will guide you through the process of install -\ning PyTorch, Torchvision, and Torchaudio, based on whether your computer is \nequipped with a compute unified device architecture (CUDA)-enabled GPU.\n 389 Installing Python and setting up a virtual environment\nA.1 Installing Python and setting up a virtual environment\nIn this section, I’ll guide you through the process of installing Anaconda on your com -\nputer based on your operating system. After that, you’ll create a Python virtual environ -\nment for all projects in this book. Finally, you’ll install Jupyter Notebook as your IDE to \nrun Python programs in this book. \nA.1.1 Installing Anaconda\nTo install Python through the Anaconda distribution, follow these steps. \nFirst, go to the https: //www.anaconda.com/download/success  and scroll to the bot -\ntom of the webpage. Locate and download the most recent Python 3 version tailored to \nyour specific operating system (be it Windows, macOS, or Linux).\nIf you are using Windows, download the latest Python 3 graphical installer from this \nlink. Click on the installer and follow the provided instructions to install. To confirm \nthe successful installation of Anaconda on your computer, you can search for the “Ana -\nconda Navigator” application on your computer. If you can launch the application, \nAnaconda has been successfully installed.\nFor macOS users, the latest Python 3 graphical installer for Mac is recommended, \nalthough a command line installer option is also available. Execute the installer and fol -\nlow the provided instructions. Verify the successful installation of Anaconda by search -\ning for the “Anaconda Navigator” application on your computer. If you can launch the \napplication, Anaconda has been successfully installed.\nThe installation process for Linux is more complex than for other operating systems, \nas there is no graphical installer. Begin by identifying the latest Linux version. Select the \nappropriate x86 or Power8 and Power9 package. Click to download the latest installer \nbash script. The installer bash script is typically saved to your computer’s Downloads \nfolder by default. Install Anaconda by executing the bash script within a terminal. Upon \ncompleting the installation, activate it by running the following command:\nsource ~/.bashrc\nTo access Anaconda Navigator, enter the following command in a terminal:\nanaconda-navigator\nIf you can successfully launch the Anaconda Navigator on your Linux system, your \ninstallation of Anaconda is complete.\nExercise A.1\nInstall Anaconda on your computer based on your operating system. After that, open the \nAnaconda Navigator app on your computer to confirm the installation.\n390 appendix  a Installing Python, Jupyter Notebook, and PyTorch\nA.1.2 Setting up a Python virtual environment\nIt’s highly recommended that you create a separate virtual environment for this book. \nLet’s name the virtual environment dgai. Execute the following command in the Ana -\nconda prompt (Windows) or a terminal (Mac and Linux):\nconda create -n dgai\nAfter pressing the Enter key on your keyboard, follow the instructions on the screen \nand press y when the prompt asks you y/n. To activate the virtual environment, run the \nfollowing command in the same Anaconda prompt (Windows) or terminal (Mac and \nLinux):\nconda activate dgai\nThe virtual environment isolates the Python packages and libraries that you use for this \nbook from other packages and libraries that you use for other purposes. This prevents \nany undesired interference. \nExercise A.2\nCreate a Python virtual environment dgai on your computer. After that, activate the vir -\ntual environment.\nA.1.3 Installing Jupyter Notebook\nNow, let’s install Jupyter Notebook in the newly created virtual environment on your \ncomputer.\nFirst, activate the virtual environment by running the following line of code in the \nAnaconda prompt (in Windows) or a terminal (in Mac or Linux):\nconda activate dgai\nTo install Jupyter Notebook in the virtual environment, run the command\nconda install notebook\nFollow the on-screen instructions all the way through to install the app. \nTo launch Jupyter Notebook, execute the following command:\njupyter notebook\nThe Jupyter Notebook app will open in your default browser. \nExercise A.3\nInstall Jupyter Notebook in the Python virtual environment dgai. After that, open the \nJupter Notebook app on your computer to confirm the installation.\n 391 Installing PyTorch\nA.2 Installing PyTorch\nIn this section, I’ll guide you through the installation of PyTorch, based on whether you \nhave a CUDA-enabled GPU on your computer. The official PyTorch website, https: //\npytorch.org/get-started/locally/ , provides updates on PyTorch installation with or \nwithout CUDA. I encourage you to check the website for any updates. \nCUDA is only available on Windows or Linux, not on Mac. To find out if your com -\nputer is equipped with a CUDA-enabled GPU, open the Windows PowerShell (in Win -\ndows) or a terminal (in Linux) and issue the following command: \nnvidia-smi\nIf your computer has a CUDA-enabled GPU, you should see an output similar to figure \nA.1. Further, make a note of the CUDA version as shown at the top right corner of the \nfigure because you’ll need this piece of information later when you install PyTorch. \nFigure A.1 shows that the CUDA version is 11.8 on my computer. The version may be \ndifferent on your computer. \nFigure A.1    Checking if your computer has a CUDA-enabled GPU\nIf you see an error message after running the command nvidia-smi , your computer \ndoesn’t have a CUDA-enabled GPU. \nIn the first subsection, I’ll discuss how to install PyTorch if you don’t have a CUDA-\nenabled GPU on your computer. You can use the CPU to train all generative AI models \nin this book. It just takes much longer. However, I’ll provide you with the pretrained \nmodels so that you can witness generative AI in action. \nOn the other hand, if you are using a Windows or Linux operating system and you do \nhave a CUDA-enabled GPU on your computer, I’ll guide you through the installation of \nPyTorch with CUDA in the second subsection.\nA.2.1 Installing PyTorch without CUDA\nTo install PyTorch with CPU training, first activate the virtual environment dgai by run -\nning the following line of code in the Anaconda prompt (in Windows) or a terminal \n(in Mac or Linux):\nconda activate dgai\n392 appendix  a Installing Python, Jupyter Notebook, and PyTorch\nYou should be able to see (dgai)  in front of your prompt, which indicates that you are \nnow in the dgai virtual environment. To install PyTorch, issue the following line of \ncommand:\nconda install pytorch torchvision torchaudio cpuonly -c pytorch\nFollow the on-screen instructions to finish the installation. Here, we install three librar -\nies together: PyTorch, Torchaudio, and Torchvision. Torchaudio is a library to process \naudio and signals, and we need it to generate music in this book. We’ll also use the \nTorchvision library extensively in the book to process images.  \nIf your Mac computer has an Apple silicon or AMD GPU with macOS 12.3 or later, \nyou can potentially use the new Metal Performance Shaders backend for GPU train -\ning acceleration. More information is available at https: //developer.apple.com/metal/\npytorch/  and https: //pytorch.org/get-started/locally/ .\nTo check if the three libraries are successfully installed on your computer, run the \nfollowing lines of code:\nimport torch, torchvision, torchaudio\nprint(torch.__version__)\nprint(torchvision.__version__)\nprint(torchaudio.__version__)\nThe output on my computer says\n2.0.1\n0.15.2\n2.0.2\nIf you don’t see an error message, you have successfully installed PyTorch on your \ncomputer.\nA.2.2 Installing PyTorch with CUDA\nTo install PyTorch with CUDA, first find out the CUDA version of your GPU, as shown \nat the top right corner of figure A.1. My CUDA version is 11.8, so I’ll use it as an exam -\nple in the installation here. \nIf you go to the PyTorch website at https: //pytorch.org/get-started/locally/ , you’ll \nsee an interactive interface as shown in figure A.2.\nOnce there, choose your operating system, select Conda as the Package, Python as \nthe Language, and either CUDA 11.8 or CUDA 12.1 as your computer platform (based \non what you have found out in the previous step). If the CUDA version on your com -\nputer is neither 11.8 nor 12.1, choose the one closest to your version and it will work. \nFor example, if a computer has a CUDA version of 12.4 and someone used CUDA 12.1, \nthe installation would be successful. \nThe command you need to run will be shown at the bottom panel. For example, I am \nusing the Windows operating system, and I have CUDA 11.8 on my GPU. Therefore, the \ncommand for me is shown at the bottom panel of figure A.2. \n 393 Installing PyTorch\nFigure A.2    The interactive interface on how to install PyTorch\nOnce you know what command to run to install PyTorch with CUDA, activate the vir -\ntual environment by running the following line of code in the Anaconda prompt (Win -\ndows) or a terminal (Linux):\nconda activate dgai\nThen issue the line of command you have found out in the last step. For me, the com -\nmand line is\nconda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c \nnvidia\nFollow the on-screen instructions to finish the installation. Here, we install three librar -\nies together: PyTorch, Torchaudio, and Torchvision. Torchaudio is a library to process \naudio and signals and we need it to generate music in this book. We also use the Torch -\nvision library extensively in the book to process images.\nTo make sure you have PyTorch correctly installed, run the following lines of code in \na new cell in Jupyter Notebook:\nimport torch, torchvision, torchaudio\nprint(torch.__version__)\nprint(torchvision.__version__)\nprint(torchaudio.__version__)\ndevice=”cuda” if torch.cuda.is_available() else ""cpu""\nprint(device)\nThe output is as follows on my computer:\n2.0.1\n0.15.2\n2.0.2\ncuda\nThe last line of the output says cuda , indicating that I have installed PyTorch with \nCUDA. If you have installed PyTorch without CUDA on your computer, the output is \ncpu.\n394 appendix  a Installing Python, Jupyter Notebook, and PyTorch\nExercise A.4\nInstall PyTorch, Torchvision, and Torchaudio on your computer based on your operating \nsystem and on whether your computer has GPU training acceleration. After that, print out \nthe versions of the three libraries you just installed.",11776
161-Appendix B Minimally qualified readers and deep learning basics.pdf,161-Appendix B Minimally qualified readers and deep learning basics,"395appendix B\nMinimally qualified readers \nand deep learning basics\nThis book is intended for machine learning enthusiasts and data scientists across \nvarious business fields who possess intermediate Python programming skills and are \ninterested in learning about generative AI. Through this book, readers will learn to \ncreate novel and innovative content—such as images, text, numbers, shapes, and \naudio—that can benefit their employers’ businesses and advance their own careers.\nThis book is designed for those who have a solid grasp of Python. You should be \nfamiliar with variable types like integers, floats, strings, and Booleans. You should also \nbe comfortable creating for and while  loops and understand conditional execution \nand branching (e.g., using if, elif, and else statements). The book involves frequent \nuse of Python functions and classes, and you should know how to install and import \nthird-party Python libraries and packages. If you need to brush up on these skills, the \nfree online Python tutorial provided by W3Schools is a great resource ( https: //www \n.w3schools.com/python/ ). \nAdditionally, you should have a basic understanding of machine learning, partic -\nularly neural networks and deep learning. In this appendix, we will review key con -\ncepts such as loss functions, activation functions, and optimizers, which are essential \nfor developing and training deep neural networks. However, this appendix is not \n396 appendix  b Minimally qualified readers and deep learning basics\nmeant to be a comprehensive tutorial on these topics. If you find gaps in your under -\nstanding, it is strongly recommended that you address them before proceeding with \nthe projects in this book. A good book for this purpose is Deep Learning with PyTorch  by \nStevens, Antiga, and Viehmann (2020).1  \nNo prior experience with PyTorch or generative AI is required. In chapter 2, you will \nlearn the basics of PyTorch, starting with its basic data types. You will also implement an \nend-to-end deep learning project in PyTorch to get hands-on experience. The goal of \nchapter 2 is to prepare you to use PyTorch for building and training various generative \nmodels in the book.\nB.1 Deep learning and deep neural networks\nMachine learning (ML) represents a new paradigm in AI. Unlike traditional rule-based \nAI, which involves programming explicit rules into a computer, ML involves feeding \nthe computer various examples and allowing it to learn the rules on its own. Deep \nlearning is a subset of ML that employs deep neural networks for this learning process.\nIn this section, you’ll learn about neural networks and why some are considered \ndeep neural networks. \nB.1.1  Anatomy of a neural network\nA neural network aims to mimic the functioning of the human brain. It consists of an \ninput layer, an output layer, and zero, one, or more hidden layers in between. The term \n“deep neural networks” refers to networks with many hidden layers, which tend to be \nmore powerful.\nWe’ll start with a simpler example featuring two hidden layers, as shown in figure B.1. \nInputInput layer\nWeightsA Neuron\nOutput layer\nHidden layersOutput\nOutputInput\nInput\nFigure B.1    The structure of a neural network. A neural network is composed of an input layer; zero, \none, or more hidden layers; and an output layer. Each layer contains one or more neurons. Neurons in \neach layer are connected to those in the preceding and subsequent layers, with the strength of these \nconnections represented by weights. In this figure, the neural network features an input layer with three \nneurons, two hidden layers with six and four neurons, respectively, and an output layer with two neurons. \n1 Eli Stevens, Luca Antiga, and Thomas Viehmann, 2020, Deep Learning with PyTorch , Manning Publications. \n 397 Deep learning and deep neural networks\nA neural network consists of an input layer, a variable number of hidden layers, and an \noutput layer. Each layer is made up of one or more neurons. Neurons in one layer con -\nnect to neurons in the previous and next layers, with the connection strengths mea -\nsured by weights. In the example illustrated in figure B.1, the neural network features \nan input layer with three neurons, two hidden layers containing six and four neurons, \nrespectively, and an output layer with two neurons. \nB.1.2  Different types of layers in neural networks\nWithin a neural network, various types of layers serve distinct purposes. The most com -\nmon is the dense layer, where each neuron is connected to every neuron in the next \nlayer. Because of this full connectivity, a dense layer is also referred to as a fully con -\nnected layer. \nAnother frequently used type of neural layer, especially in this book, is the convolu -\ntional layer. Convolutional layers treat input as multidimensional data and are adept \nat extracting patterns from it. In our book, convolutional layers are often employed to \nextract spatial features from images. \nConvolutional layers differ from fully connected (dense) layers in several key ways. \nFirst, each neuron in a convolutional layer connects only to a small region of the input. \nThis design is based on the understanding that in image data, local groups of pixels \nare more likely to be related. This local connectivity significantly reduces the number \nof parameters, making convolutional neural networks (CNNs) more efficient. Second, \nCNNs utilize shared weights—the same weights are applied across different regions of \nthe input. This mechanism is similar to sliding a filter across the entire input space. This \nfilter detects specific features (e.g., edges or textures) regardless of their position in \nthe input, which leads to the property of translation invariance. Due to their structure, \nCNNs are more efficient for image processing, requiring fewer parameters than fully \nconnected networks of similar size. This results in faster training times and lower com -\nputational costs. Additionally, CNNs are generally more effective at capturing spatial \nhierarchies in image data. We discuss CNNs in detail in chapter 4.\nThe third type of neural network is the recurrent neural network (RNN). Fully con -\nnected networks treat each input independently, processing each input separately with -\nout considering any relationship or order between different inputs. In contrast, RNNs \nare specifically designed to handle sequential data. In an RNN, the output at a given \ntime step depends not only on the current input but also on previous inputs. This allows \nRNNs to maintain a form of memory, capturing information from previous time steps \nto influence the processing of the current input. See chapter 8 for details on RNNs.\nB.1.3  Activation Functions\nActivation functions are a crucial component of neural networks, functioning as the \nmechanisms that transform inputs into outputs and determine when a neuron should \nactivate. Some functions are akin to on-off switches, playing a pivotal role in enhancing \nthe power of neural networks. Without activation functions, neural networks would \n398 appendix  b Minimally qualified readers and deep learning basics\nbe limited to learning only linear relationships in data. By introducing nonlinearity, \nactivation functions enable the creation of complex, nonlinear relationships between \ninputs and outputs.\nThe most commonly-used activation function is the rectified linear unit (ReLU). A \nReLU activates the neuron when the input is positive, effectively allowing information \nto pass through. When the input is negative, the neuron is deactivated. This straightfor -\nward on-off behavior facilitates the modeling of nonlinear relationships.\nAnother commonly used activation function is the sigmoid function, which is par -\nticularly suited for binary classification problems. The sigmoid function compresses \ninputs into a range between 0 and 1, effectively representing the probabilities of a \nbinary outcome.\nFor multicategory classification tasks, the softmax function is employed. The soft -\nmax function transforms a vector of values into a probability distribution, where the \nvalues sum to 1. This is ideal for modeling the probabilities of multiple outcomes.\nLastly, the tanh activation function is noteworthy. Similar to the sigmoid function, \ntanh produces values between –1 and 1. This characteristic is especially useful when \nworking with images, as image data often contains values within this range.\nB.2 Training a deep neural network\nThis section provides an overview of the steps involved in training a neural network. A \nkey aspect of this process is dividing your training dataset into a train set, a validation \nset, and a test set, which is crucial for developing a robust deep neural network. We will \nalso discuss various loss functions and optimizers used in training neural networks. \nB.2.1  The training process\nOnce a neural network is built, the next step is to gather a training dataset to train the \nmodel. Figure B.2 illustrates the steps in the training process.\nOn the left side of figure B.2, we see the initial division of the training dataset into \nthree subsets: the train set, the validation set, and the test set. This division is critical for \nbuilding a robust deep neural network. The training set is the subset of data used to \ntrain the model, where the model learns patterns, weights, and biases. The validation \nset is used to evaluate the model’s performance during training and to decide when \nto stop training. The test set is used to assess the final performance of the model after \ntraining is complete, providing an unbiased evaluation of the model’s ability to general -\nize to new, unseen data.\nDuring the training phase, the model is trained on data in the train set. It iteratively \nadjusts its parameters to minimize the loss function (see the next subsection on differ -\nent loss functions). After each epoch, the model’s performance is evaluated using the \nvalidation set. If the performance on the validation set continues to improve, train -\ning proceeds. If the performance ceases to improve, training is stopped to prevent \noverfitting.\n 399 Training a deep neural network\nNeural network\nTrain the model and\nadjust parameters\nYesModel \nstops \nimproving?Stop training\nstart testingTraining\ndatasetTrain\nset\nValidation\nset\nTest\nsetNoEvaluate the model\nusing validation setTesting results\nFigure B.2    Training a neural network. The training dataset is divided into three subsets: the train set, \nthe validation set, and the test set. The process for training a neural network involves the following \nsteps. In the training phase, the train set is used to train the neural network and adjust its parameters to \nminimize the loss function. During each iteration of training, the model updates its parameters based on \ndata in the train set. In the validation phase of each iteration, the model is evaluated using the validation \nset. The performance on the validation set helps determine if the model is still improving. If the model’s \nperformance on the validation set continues to improve, the next iteration of training proceeds using \nthe train set. If the model’s performance on the validation set stops improving, the training process is \nstopped to prevent overfitting. Once training is complete, the trained model is evaluated on the test \nset. This evaluation provides the final testing results, giving an estimate of the model’s performance on \nunseen data.\nOnce training is complete, the testing phase begins. The model is applied to the test \nset (unseen data) to assess its final performance and report results.\nDividing the dataset into three different sets is essential for several reasons. The train \nsubset allows the model to learn patterns and features from the data and to adjust its \nparameters. The validation subset serves as a check against overfitting by enabling per -\nformance monitoring during training. The test subset provides an unbiased evaluation \nof the model’s generalization ability, estimating its real-world performance.\nBy appropriately splitting the data and utilizing each set for its intended purpose, we \nensure that the model is well trained and unbiasedly evaluated.\nB.2.2  Loss functions\nLoss functions are essential for measuring the accuracy of our predictions and guiding \nthe optimization process when training deep neural networks.\nA commonly used loss function is the mean squared error (MSE or L2 loss). MSE cal -\nculates the average squared difference between the model’s predictions and the actual \nvalues. A closely related loss function is the mean absolute error (MAE or L1 loss). MAE \ncalculates the average absolute difference between predictions and actual values. MAE \n400 appendix  b Minimally qualified readers and deep learning basics\nis often used if the data are noisy and have many outliers since it punishes extreme val -\nues less than the L2 loss. \nFor binary classification tasks, where predictions are binary (0 or 1), the preferred \nloss function is binary cross-entropy. This function measures the average difference \nbetween predicted probabilities and actual binary labels.\nIn multicategory classification tasks, where predictions can take multiple discrete val -\nues, the categorical cross-entropy loss function is employed. This function measures the \naverage difference between predicted probability distributions and actual distributions.\nDuring the training of ML models such as deep neural networks, we adjust the model \nparameters to minimize the loss function. The adjustment magnitude is proportional \nto the first derivative of the loss function with respect to the model parameters. The \nlearning rate controls the speed of these adjustments. If the learning rate is too high, \nthe model parameters may oscillate around the optimal values and never converge. \nConversely, if the learning rate is too low, the learning process becomes slow, and it \ntakes a long time for the parameters to converge. \nB.2.3  Optimizers\nOptimizers are algorithms used in training deep neural networks to adjust the model’s \nweights to minimize the loss function. They guide the learning process by determining \nhow the model’s parameters should be updated at each step, thus enhancing perfor -\nmance over time.\nOne example of an optimizer is a stochastic gradient descent (SGD). A SGD adjusts \nweights by moving them in the direction of the negative gradient of the loss function. \nIt updates weights using a subset of the data (mini-batch) at each iteration, which helps \nspeed up the training process and improve generalization.\nIn this book, the most commonly used optimizer is Adam (Adaptive Moment Esti -\nmation). Adam combines the benefits of two other extensions of SGD: AdaGrad and \nRMSProp. It computes adaptive learning rates for each parameter based on estimates \nof the first and second moments of the gradients. This adaptability makes Adam partic -\nularly suitable for problems involving large datasets and/or numerous parameters.",15203
162-index.pdf,162-index,"401index\nA\nactivation functions  398\nAdaGrad (Adaptive Gradient Algorithm)  30\nAdam optimizer  35, 135\nAdamW optimizer  355\nadding tools\nby using OpenAI GPTs  380\nto generate code and images  382\nadd_noise() method  351\nadjusting parameters and weights  54\nAE() class  148, 149\nAEs (autoencoders)  142, 143\nagents, creating in LangChain  378\nAI (artificial intelligence)\nattention  205–209\ndiffusion models and text-to-image Transformers  341\ngenerative, line-by-line implementation of attention and \nTransformer  194\nimage generation with OpenAI DALL-E 2  371\nalbumentations library  127, 130\nalloy option  372\nAnaconda, installing  389–390\nanna token  181\nAntiga, Luca  396\nAPI keys, applying for Wolfram Alpha API key  377\narchitecture, of GPT-2  240–247\nattention\nbuilding encoders  205–209line-by-line implementation of  194, 213–215\nmechanism  15, 196–200\noverview of  195–205\nTransformer architecture  200–204\ntypes of Transformers  204\nAttention() class  352\nattention() function  205, 206\nautoencoders\noverview of  145–146\nVAEs (variational autoencoders), building  155\nB\nbackpropagation  54\nbackward() method  53\nBarGenerator() class  309, 310\nBatch() class  224, 225, 226\nbatch creation  223\nBatchLoader() class  227\nBatchNorm2d layer  105, 106\nbatch normalization  83\nbias argument  80\nbinary classification  33–37\nbuilding and training model  34–36\ncreating batches  33\ntesting model  36\nbinary cross-entropy  102\nblack hair, using CycleGAN to translate between black and \nblond hair  135–140\n402\nindex\nBlack_Hair attribute  128\nBlock() class  335\nblock() method  105, 106\nblock_size attribute  251, 277, 334\nblond hair, using CycleGAN to translate between black and \nblond hair  135–140\nBlond_Hair attribute  128\nBOS (beginning of sentence)  218\nBOS token  204\nBPE (byte pair encoder)  242, 247\ntokenization  248–249\nbuilding generative models from scratch  19\nC\ncat() method  27\ncausal self-attention  251–252\nGPT-2  240\nCausalSelfAttention() class  251, 252, 257, 278, 335\ncausal self-attention mechanism  277\ncelebrity faces dataset  127–130\ndownloading  127\nprocessing black and blond hair image data  129–130\ncGAN (conditional GAN)  98\ncreating  104–108\ntraining  108–112\nWasserstein distance  101–104\nch06util module  129, 132, 135, 136\nch06util.py file  129, 131, 132\nchat.completions.create() method  368\nchords  300–302\nClarke, Arthur C.  98\nclassification, multicategory  37–42\nCLIP (contrastive language–image pretraining)  341, 342, \n360\nCNNs (convolutional neural networks)  7, 70, 147, 194, \n397\ncolor images, of anime faces  87–89\nCompose() class  31, 130\ncomputational graph  8\nConfig() class  251, 276, 277, 334\ncontent generation, with OpenAI API  367–372\nConv2d layers  91, 104, 105, 106, 133, 134\nConv3d layers  308\nconvert_to_midi() function  315\nconvolutional layers  78–82\nconvolutional operations  78–81\nstride and padding  82\nConvTranspose2d layers  91, 106, 133\ncreate_model() function  214, 215, 230\ncreate_xys() function  331, 332criterion element  233\nCritic() class  107\ncritic in MuseGAN  307\nCUDA (compute unified device architecture)  9, 279, 388\ncycle consistency loss  124–127, 170\nCycleGAN (Cycle-Consistent Generative Adversarial \nNetwork)  123, 169\nbuilding model  130–135\ncelebrity faces dataset  127\ncycle consistency loss  124–127\noverview of  125\nusing to translate between black and blond hair   \n135–140\nD\nDALL-E 2  362\ndata, preparing training data for MuseGAN  304–306\nDataLoader class  34, 37\nDataset class  130\ndatasets package  88, 147\ndata_to_num() function  61\nDAW (digital audio workstation)  295\nDDIMScheduler() class  350, 351, 357\ndecode_midi() function  333, 338, 339\ndecoder  215\nDecoder() class  156, 157, 212, 213, 215\nDecoderLayer() class  210\ndeep learning  21\nbinary classification  33–37\ndeep neural networks  396–398\nmulticategory classification  37–42\noverview  395\nwith PyTorch  29–32\nDeep Learning with PyTorch  (Stevens, Antiga, and \nViehmann)  396\ndeep neural networks  396–398\nactivation functions  398\nanatomy of neural networks  396\nlayers in neural networks  397\noverview of training  398–400\ndense layers  154\ndetach() method  136\ndgai virtual environment  391\ndict.p file  223\ndiffusion models  341\ndenoising  342–348\ndenoising U-Net model  351–354\npreparing training data  348–351\ntext-to-image Transformers  360–364\ntraining and using denoising U-Net model  354–360\n 403\nindex\ntraining data  348–351\ndigits, building and training VAEs to generate\nbuilding and training AEs  148–150\ngathering handwritten digits  147\nsaving and using trained AEs  151\ndim argument  27\nDiscriminator() class  131\ndiscriminators, creating  131\ndisplay_step hyperparameter  313\ndistance() function  63\ndownloading training data  328–329\ndtype argument  24\nE\nEarlyStop() class  37, 38, 40\nearly stopping  37, 51–52\necho option  372\nembedding  227–230\nEmbeddings() class  227, 228\nencode_midi() function  339\nEncoder() class  155, 157, 209, 213, 215\nencoder-decoder Transformer  210–213\ncreating  212\ncreating decoder layer  210–211\nEncoderLayer() class  208, 209\nencoders  205–209, 215\nattention mechanism  205–208\ncreating  208–209\nencoding arithmetic, with trained VAEs  160\nEnglish-to-French translation\ntokenizing English and French phrases  218\ntraining Transformer for  227–234\nen_idx_dict dictionary  221, 222\nen_word_dict dictionary  221, 222, 227\nEOS (end of sentence)  218\nEOS token  204\nepochs hyperparameter  313\nexploding gradients  83\nexponential growth curve, training dataset that forms  47\neyeglasses dataset  98–100\ndownloading  99\nvisualizing images in  100\nF\nfake_A tensor  136\nfake_B tensor  136\nfake_labels tensor  53, 55\nFashionMNIST() class  148\nfloat16 data type  135\nfloat32 data type  24, 25, 135float64 data type  24\nflower images  349\nforward diffusion process  343\nvisualizing  350–351\nfr_idx_dict dictionary  222\nfr_word_dict dictionary  222, 227\nfully connected (dense) layers  154\nG\nGANs (generative adversarial networks)  6, 9–14, 22, 43, \n65, 69, 97, 144, 169, 292\nbatch normalization  83–87\ncGANs  104–108\ncolor images of anime faces  87–89\nconvolutional layers  78–82\ncreating  49–52\nDCGAN (deep convolutional GAN)  90–95\ngenerating anime faces  11\nimage generation with  6, 70–77\nmusic generation with  291–293\nnumber generation  59–65\noverview of  9\npreparing training data  47–49\nsteps involved in training  44–47\nreasons for caring about  12\ntraining  44–47\ntraining and using for shape generation  52–58\ntransposed convolution  83–87\nGELU (Gaussian error linear unit)  276\nactivation function  249, 334\ngen_batch() function  61\ngen_black.pth file  138\ngen_blond.pth file  138\ngenerate() function  188–190, 259, 260, 282–285\ndefining to produce text  258–260\ngenerating text  256–262\ngenerative AI  3\nbuilding models from scratch  19\ndefined  5–7\noverview of  5–9\nPython programming language  7\nTransformers  14–18\nVAEs (variational autoencoders), building  155\nGenerator() class  107, 113, 132, 134, 213, 214, 215\ngenerator element  233\ngenerator in MuseGAN  308–311\ngenerators\ncreating  132–135\nin Transformer  213\nto create grayscale images  72–74\n404\nindex\nget_scheduler() function  356\nglasses label  99, 100\ngp() function  313\nGP() function  108\nGPT-2 (Generative Pretrained Transformer-2)  238–240\narchitecture of  240–247\nbuilding from scratch  247–255\ncausal self-attention in  243–247\nloading pretrained parameters in  \nGPT-2XL  256–258\nloading pretrained weights and generating text   \n256–262\nword embedding and positional encoding in  242\nGPT2XL() class  254\nGPT-2XL model\ngenerating text with  260–262\nloading pretrained parameters in  256–258\nGPT (Generative Pre-trained Transformer)\nbuilding from scratch  247–255\nbuilding to generate music  333–336\nbuilding to generate text  276–280\ntraining from scratch  266–270\nGPT (Generative Pre-trained Transformer) model  7\ntraining to generate text  280–287\ngradient norm clipping  281\ngradient penalty  101–104\nGradientPenalty() class  312, 313\ngroove  300–302\nH\nhair color conversion, using CycleGAN to translate \nbetween black and blond hair  135–140\nheights_reshaped 2D tensor  28\nheights_tensor 1D tensor  28\nHorizontalFlip() argument  130\nhuman face images, generating with VAEs  154\nhyperparameters, in music Transformer  334\nI\nIDE (integrated development environment)  388\nid variable  99\nImageFolder() class  87, 88\nimage generation  142–144, 169\nbatch normalization  83–87\nconvolutional layers  78–82\nencoding arithmetic with trained VAs  160\ntransposed convolution  83–87\nVAEs (variational autoencoders)  151–154\nwith GANs (generative adversarial networks)  69, \n70–77, 90–95with OpenAI DALL-E 2  371\nwith VAEs  142–144, 147–151,154, 157, 158, 170\nwith variational autoencoders, building  155\nimages\ncharacteristics in  104–108\nselecting characteristics in generated  112–121\nin_channels argument  80\ninit_hidden() method  183\n__init__.py file  129\ninputs  73\nInstanceNorm2d layer  105, 133, 134\nint_to_onehot() function  60\nint_to_word dictionary  181, 186\nisalpha() method  273\nisdigit() method  273\niter() method  49\nJ\nJSON (JavaScript object notation)  329\nJupyter Notebook, installing  390\nK\nkernel_size argument  80\nkeys 257\nKL (Kullback–Liebler) divergence  143, 152\nL\nlabels, adding to inputs  108–110\nlabels_g label  116, 117, 118, 121\nLabelSmoothing() class  231, 233\nlabels_ng label  114, 116, 117, 118, 121\nLangChain library  365–367, 372–376\nadding tools to generate code and images  382\napplying for Wolfram Alpha API key  377\ncreating agents in  378\nneed for  373\nusing OpenAI API in  374\nzero-shot know-it-all agents  376\nzero-shot, one-shot, and few-shot prompting  375\nLayerNorm() class  209\nlayers\nin neural networks  397\ntransposed convolution  83\nLeakyReLU activation  105\nLinear() class  35\nLinear layer  62\nlist_attr_celeba.csv file  127, 128\nLLMs (large language models)  7, 194, 238, 341\nadding tools by using OpenAI GPTs  380\n 405\nindex\ncontent generation with OpenAI API  367–372\nLangChain library  372–376\nlimitations and ethical concerns of  384–386\nzero-shot know-it-all agents  376\nLoadData() class  129, 130\nload_dataset() method  349\nloss_fn() function  311, 312\nloss functions  51–52, 230–233, 311, 399\nLSTM (long short-term memory)  14, 179–181, 195\nLSTM (long short-term memory) models, building and \ntraining  174–175, 182–185\nLSTM (long short-term memory) networks, generating \ntext with trained model  185–192\nM\nmake_std_mask() function  226\nmanual_seed() method  31, 48, 284\nmap_location=device argument  58\nmatplotlib library  349\nmax() method  28\nmean() method  28\nmedian() method  28\nmelody  300–302\nmessages parameter  368\nMIDI (musical instrument digital interface)  291, 293, 319\ntokenizing files  329–331\nmidi_util module  315\nML (machine learning)  21, 44\nlibrary  8\nMNIST() class  148\nMNIST (Modified National Institute of Standards and \nTechnology)  31\nmode collapse  101\nModel() class  279, 335\nmodel hyperparameters  276\nmodeling, causal self-attention mechanism  277\nmodel weights  54\nmodified trg attribute  225\nmulticategory classification  37–42\nbuilding and training model  38–42\nearly stopping  37\nvalidation sets  37\nmultihead attention  200\nMultiHeadAttention() class  206, 209\nmultimodal Transformers  17\nmultitrack music  295–297\nmu mean  156\nMuseCritic() class  311\nMuseGAN (Music Generative Adversarial Network)   \n291–293\nbuilding  307–312digital music representation  293–300\npreparing training data for  304–306\ntraining to generate music  312–316\nMuseGAN_util module  309, 310, 311\nMuseGAN_util.py file  312\nMuseGenerator() class  310, 311\nMuseScore app  296\nmusic21 library  327\nmusical notes  293\nmusic generation\nblueprint for  300–304\nbuilding GPT to generate music  333–336\npreparing training data for MuseGAN  304–306\nwith MuseGAN  291–293\nwith trained Transformer  337–340\nmusic Transformer\narchitecture  324–326\nbuilding and training  318–319\noverview  320–327\nperformance-based music representation  320–324\ntokenizing music pieces  327–333\ntraining  326\ntraining and using  336–340\nN\nn=1 argument  368\nn_embd attribute  251, 277, 334\nneural networks\nactivation functions  398\nanatomy of  396\nlayers in  397\nnewdata list  110\nnext() method  49\nn_head attribute  251, 277, 334\nn_layer attribute  251, 277, 334\nNLP (natural language processing)  14, 175–179, 195, \n238, 292, 319, 365\ntokenization methods  176\nword embedding  177–179\nnn.BCELoss() function  35\nnn.CrossEntropyLoss() class  39\nnn.Embedding() layer  276\nnn.LogSoftmax() class  39\nnn.Module class  34, 58\nnn.NLLLoss() class  39\nnn.Sequential class  34\nNoamOpt() class  232\nnoise() function  313\nnoise_g label  115\nnoise_ng label  115\n406\nindex\nnoise_ng random noise vector  114\nNormalize() class  31, 88\nnote_on attribute  330\nnotes  293\nnumber generation, with patterns  59–65\nGANs to generate numbers with patterns  61–63\none-hot variables  59–61\nsaving and using trained model  64\ntraining GANs to generate numbers with patterns  63\nnum_pieces variable  315\nO\noctaves  293\nonehot_encoder() function  59\nonehot_to_int() function  60\none-hot variables  59\nones() method  25\nOpenAI API  374\ncontent generation with  367–372\nOpenAI() class  363, 368\nOpenAI GPTs (generative pretraining transformers), \nadding tools by using  380\nopt element  233\noptimizers  51–52, 230–233, 311, 400\noriginals list  150\nos library  99\nout_channels argument  80\noutputs  73\nP\npadding  82\nPAD token  221\npandas library  127, 128\nparameters  54\npatience argument  38\nperformance-based music representation  320–324\npermute() method  88\npiano rolls  297–300\nPIL (Python Imaging Library)  31\npitch  293\nplot_digits() function  149, 151\nplot_epoch() function  112, 158\nplot_images() function  89\npositional encoding  227–230\nin GPT-2  242\nPositionalEncoding() class  228, 229\nPositionwiseFeedForward() class  207, 208, 209\npreprocessing data  31–32\npretrained large language models (LLMs)  17, 365–367\napplying for Wolfram Alpha API key  377pretrained parameters, loading in GPT-2XL  256–258\npretrained weights, loading and generating text  256–262\ndefining generate() function to produce text  258–260\nloading pretrained parameters in GPT-2XL  256–258\nwith GPT-2XL  260–262\npretty_midi library  327\nprocessor.py module  333\nprompting, zero-shot, one-shot, and few-shot  375\nPython programming language  7\ncreating virtual environment  389–390\ninstalling  388–390\nPyTorch\nas AI framework  8\ndeep learning with  29–32\ndefined  5–7\ninstalling  391–393\noverview of  5–9\nR\nrandint() method  61\nrand() method  48\nReAct (Reactive Action)  379\nreal_labels tensor  53, 55\nregister_buffer method  252\nReLU (rectified linear unit)  35, 49, 249, 398\nactivation  106, 133, 134\nactivation function  62\nrepeat hyperparameter  313\nrequires_grad_(False) argument  228\nresidual connections  133\nRMSProp (Root Mean Squared Propagation)  30\nRNNs (recurrent neural networks)  7, 194, 397\nLSTM models  182–185\ntext generation with  171–182, 185–192\nroot_A list  130\nroot_B list  130\nround-trip conversions  138–140\nS\nsample() function  186–189, 258, 259, 282, 338, 339\nsc component  185\nSDPA (scaled dot product attention)  195, 217, 347\nsee_output() function  74\nselecting characteristics\ncGAN and Wasserstein distance  101–104\ntraining cGAN  108–112\nselecting characteristics in generated images  97, 112–121\nselecting images with or without eyeglasses  113–116\nselecting two characteristics simultaneously  118–121\nvector arithmetic in latent space  116\n 407\nindex\nseq_padding() function  224\nsequence padding  223\nset() method  273\nSGD (Stochastic Gradient Descent)  30\nshape generation, training and using GANs for  52–58\nsh component  185\nshimmer option  372\nshuffle=True argument  49\nshutil library  99\nsigmoid activation function  72\nSimpleLossCompute() class  232, 233\nslow convergence  101\nsmoothing parameter  231\nsoftmax function  398\nsplit() method  273\nsrc_embed component  213\nsrc input  225\nsrc_mask tensor  225\nstate_dict() method  85\nstd standard deviation  156\nstep() method  53\nStevens, Eli  396\nstop() method  38\nstride  82\nstyle 300–302\nSublayerConnection() class  208\nsubsequent_mask() function  226\nsubword tokenization  218\nsum() method  28\nT\ntanh activation function  398\ntemperature  188–192, 285\nsampling  239\nTemporalNetwork() class  309\ntensors  23\ntest.csv file  98\ntest_epoch() function  55, 93\ntest() function  135\ntext, tokenizing  270–276\ntext generation\nbuilding GPT to generate text  276–280\nloading pretrained weights and generating text   \n256–262\ntraining GPT from scratch  266–270\ntraining GPT model to generate text  280–287\nwith RNNs  171–182, 185–192\ntext-to-image Transformers  360–364\nCLIP  360\nDALL-E 2  362tgt_embed component  213\ntgt_mask  211\ntokenization\nEnglish and French phrases  218\nmethods  176\ntokenizing music pieces  327–333\ndownloading training data  328–329\npreparing training data  331–333\ntokenizing MIDI files  329–331\ntokenizing text  270–276\ncreating batches for training  274\ntools, adding to generate code and images  382\ntop-K sampling  188–192, 239, 285, 338\ntorch.argmax() method  40, 41\ntorch.cuda.amp package  135\ntorch.jit.load() function  58\ntorch.jit.script() method  58\ntorch.manual_seed() method  41\ntorch.max() method  29\ntorch.min() method  29\ntorch.nn.Embedding() class  183\ntorch.no_grad() function  81\ntorch.rand() method  48\ntorch.sin() function  48\ntorch.tensor() method  24\ntorch.where() method  36\nToTensor() class  31, 88, 148\ntrain argument  148\ntrain_batch() function  111\ntrain.csv file  99, 100\ntrain_data tensor  48\ntrain_D_G() function  63\ntrain_D_on_fake() function  54, 63, 75, 93\ntrain_D_on_real() function  53, 63, 75, 93\ntrain_epoch() function  39, 136, 313\ntrain_G() function  54, 63, 75, 93\ntraining\ncreating batches for training  274\nCycleGAN to translate between black and blond \nhair 135–138\nTransformer to generate text  270–271, 274–276\ncreating batches for training  274\ntokenizing text\nTransformer to generate text\ntokenizing text  270–271, 274–276\nTransformer to translate English to French  217, \n227–230\ntraining data  348–351\nflower images as training data  349\npreparing  47–49\nvisualizing forward diffusion process  350\n408\nindex\ntraining music Transformer  336\ntransconv layer  85\nTransformer\narchitecture of  200–204\nattention mechanism  196–200\nencoder-decoder Transformer  210–213\nline-by-line implementation of  213–215\nline-by-line implementation of attention and  194\noverview of  195–205\ntraining sequence padding and batch creation  223\ntraining to generate text  264–266, 270–276\ntraining to translate English to French  217, 218, \n230–234\ntypes of  204\nTransformer() class  213, 215\nTransformers  14–18\narchitecture  15\nattention mechanism  15\nmultimodal Transformers and pretrained LLMs  17\ntraining to translate English to French  227–230\nword embedding  227–230\ntransformers library  219, 256\ntransforms  130\ntransposed convolution, layers  83\ntrg input  225\ntrg_mask mask  226\ntrg output  226\nTTS (text-to-speech)  372\nU\nUNet() class  353, 354\nU-Net model\ndenoising  351–354\ndenoising images  345–347\ntraining and using  347, 354–360\nUNK token  221\nV\nVAE() class  157\nVAEs (variational autoencoders)  18, 70, 140, 142, 169\nbuilding  155\nbuilding and training to generate digits  147–151\ncycle consistency loss  170\ndifferences between AEs and  152encoding arithmetic with trained VAEs  160\ngenerating human face images with  154\nimage generation with  142–144, 158\noverview of  145–147, 151–154\ntraining  157\ntraining to generate human face images  153–154\nval_epoch() function  39\nvalidation sets  37\nvanishing gradients  83, 101\nvector arithmetic, in latent space  116\nvelocity attribute  330\nViehmann, Thomas  396\nvirtual environment, creating and setting up  389–390\nViT (Vision Transformer)  342\nvocab_size attribute  251, 277, 334\nW\nWasserstein distance, with gradient penalty  101–104\nWebText dataset  241\nweights  54\nweights_init() function  106, 107, 132\nWGAN (Wasserstein GAN)  101\nWikipedia API  366\nWolfram Alpha  376\nword embedding  177–179, 227–230\nin GPT-2  242\nwordidx list  181\nWordLSTM() class  182, 183, 184\nword_to_int dictionary  181, 185, 186, 274\nX\nXLM model  220\nZ\nzero_grad() method  53, 54\nzero-shot know-it-all agents  376\nadding tools to generate code and images  382\nzeros() method  25\nz_female_g random noise vector  116, 118, 119\nz_female_ng random noise vector  116, 119, 121\nz_male_g random noise vector  116, 118, 119\nz_male_ng random noise vector  116, 119, 121\nz vector  156\nWQWKWV\nInput X Input X Input XMatrix \n multiplicationScaleSoftmax \nactivationMatrix multiplication\nAttention score\nK Q VScaled\nattention scoreAttention\nAttention weight\nInput X is passed through three linear  \nneural networks, with weights W Q, \nWK, and W V, respectively, to obtain\nquery Q, key K, and value V. \nThe weights WQ, WK, and W V are \nﬁrst randomly initialized and then\nlearned from the training data. \nA diagram of the self-attention mechanism. To calculate attention, the input embedding X is first \npassed through three neural layers with weights, WQ, WK, and WV, respectively. The outputs are \nquery Q, key K, and value V. The scaled attention score is the product of Q and K divided by the \nsquare root of the dimension of K, dk. We apply the softmax function on the scaled attention score \nto obtain the attention weight. The attention is the product of the attention weight and value V. \nMadhusudhan Konda ● Foreword by  Shay Banon\nISBN-13: 978-1-63343-646-6\nTransformers, Generative Adversarial Networks (GANs), \ndiff usion models, LLMs , and other powerful deep learn-\ning patterns have radically changed the way we manipu-\nlate text, images, and sound. Generative AI may seem like \nmagic at fi  rst, but with a little Python, the PyTorch frame-\nwork, and some practice, you can build interesting and useful models that will train and run on your laptop. Th  is book \nshows you how.\nLearn Generative AI with PyTorch introduces the underlying \nmechanics of generative AI by helping you build your own working \nAI models. You’ll begin by creating simple images \nusing a GAN , and then progress to writing a language transla-\ntion transformer line-by-line. As you work through the fun and fascinating projects, you’ll train models to create anime images, write like Hemingway, make music like Mozart, and more. You just need Python and a few machine learning basics to get started. You’ll learn the rest as you go! \nWhat’s Inside\n● Build an English-to-French translator\n● Create a text-generation LLM\n● T rain a diff  usion model to produce high-resolution images\n● Music generators using GAN s and T ransformers\nExamples use simple Python. No deep learning experience \nrequired.\nMark Liu  is the founding director of the Master of Science in \nFinance program at the University of Kentucky.\nTh e technical editor on this book was Emmanuel Maggiori.\nSOFTWARE DEVELOPMENT\nMANNING“Mark has an engaging \nwriting style and makes \ncomplex concepts accessible, \nand enjoyable. I highly \n  recommend this book.”—Simon Sheather, Dean of the \nGatton College of Business & \nEconomics, University of Kentucky \n“An exceptional guide with \nreal-world examples. \nInteractive, approachable, \n  and highly practical!”—Noah Flynn, Research Scientist, \nAmazon  \n“Comprehensive, hands-on, \nand state-of-the-art. After \nreading it, you’ll understand \ngenerative AI models and be \nequipped with the tools to \n  build your own.”—Arpit Singh, Senior Software \nEngineer, Nvidia\n“A great book! \n  Clear and accessible.”—Tong Yu, Professor of Finance, \nUniversity of CincinnatiSee first page\nFor print book owners, all ebook formats are free:\nhttps:/ /www.manning.com/freebookMark Liu ● Foreword by  Sarah SandersLearnGenerative AI with PyTorch",24343
