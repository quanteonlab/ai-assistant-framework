filename,title,text,len
01-1 Introduction.pdf,01-1 Introduction,"What Every Programmer Should Know About Memory\nUlrich Drepper\nRed Hat, Inc.\ndrepper@redhat.com\nNovember 21, 2007\nAbstract\nAs CPU cores become both faster and more numerous, the limiting factor for most programs is\nnow, and will be for some time, memory access. Hardware designers have come up with ever\nmore sophisticated memory handling and acceleration techniques–such as CPU caches–but\nthese cannot work optimally without some help from the programmer. Unfortunately, neither\nthe structure nor the cost of using the memory subsystem of a computer or the caches on CPUs\nis well understood by most programmers. This paper explains the structure of memory subsys-\ntems in use on modern commodity hardware, illustrating why CPU caches were developed, how\nthey work, and what programs should do to achieve optimal performance by utilizing them.\n1 Introduction\nIn the early days computers were much simpler. The var-\nious components of a system, such as the CPU, memory,\nmass storage, and network interfaces, were developed to-\ngether and, as a result, were quite balanced in their per-\nformance. For example, the memory and network inter-\nfaces were not (much) faster than the CPU at providing\ndata.\nThis situation changed once the basic structure of com-\nputers stabilized and hardware developers concentrated\non optimizing individual subsystems. Suddenly the per-\nformance of some components of the computer fell sig-\nniﬁcantly behind and bottlenecks developed. This was\nespecially true for mass storage and memory subsystems\nwhich, for cost reasons, improved more slowly relative\nto other components.\nThe slowness of mass storage has mostly been dealt with\nusing software techniques: operating systems keep most\noften used (and most likely to be used) data in main mem-\nory, which can be accessed at a rate orders of magnitude\nfaster than the hard disk. Cache storage was added to the\nstorage devices themselves, which requires no changes in\nthe operating system to increase performance.1For the\npurposes of this paper, we will not go into more details\nof software optimizations for the mass storage access.\nUnlike storage subsystems, removing the main memory\nas a bottleneck has proven much more difﬁcult and al-\nmost all solutions require changes to the hardware. To-\n1Changes are needed, however, to guarantee data integrity when\nusing storage device caches.\nCopyright © 2007 Ulrich Drepper\nAll rights reserved. No redistribution allowed.day these changes mainly come in the following forms:\n• RAM hardware design (speed and parallelism).\n• Memory controller designs.\n• CPU caches.\n• Direct memory access (DMA) for devices.\nFor the most part, this document will deal with CPU\ncaches and some effects of memory controller design.\nIn the process of exploring these topics, we will explore\nDMA and bring it into the larger picture. However, we\nwill start with an overview of the design for today’s com-\nmodity hardware. This is a prerequisite to understand-\ning the problems and the limitations of efﬁciently us-\ning memory subsystems. We will also learn about, in\nsome detail, the different types of RAM and illustrate\nwhy these differences still exist.\nThis document is in no way all inclusive and ﬁnal. It is\nlimited to commodity hardware and further limited to a\nsubset of that hardware. Also, many topics will be dis-\ncussed in just enough detail for the goals of this paper.\nFor such topics, readers are recommended to ﬁnd more\ndetailed documentation.\nWhen it comes to operating-system-speciﬁc details and\nsolutions, the text exclusively describes Linux. At no\ntime will it contain any information about other OSes.\nThe author has no interest in discussing the implications\nfor other OSes. If the reader thinks s/he has to use a\ndifferent OS they have to go to their vendors and demand\nthey write documents similar to this one.\nOne last comment before the start. The text contains a\nnumber of occurrences of the term “usually” and other,\nsimilar qualiﬁers. The technology discussed here exists\nin many, many variations in the real world and this paper\nonly addresses the most common, mainstream versions.\nIt is rare that absolute statements can be made about this\ntechnology, thus the qualiﬁers.\nDocument Structure\nThis document is mostly for software developers. It does\nnot go into enough technical details of the hardware to be\nuseful for hardware-oriented readers. But before we can\ngo into the practical information for developers a lot of\ngroundwork must be laid.\nTo that end, the second section describes random-access\nmemory (RAM) in technical detail. This section’s con-\ntent is nice to know but not absolutely critical to be able\nto understand the later sections. Appropriate back refer-\nences to the section are added in places where the content\nis required so that the anxious reader could skip most of\nthis section at ﬁrst.\nThe third section goes into a lot of details of CPU cache\nbehavior. Graphs have been used to keep the text from\nbeing as dry as it would otherwise be. This content is es-\nsential for an understanding of the rest of the document.\nSection 4 describes brieﬂy how virtual memory is imple-\nmented. This is also required groundwork for the rest.\nSection 5 goes into a lot of detail about Non Uniform\nMemory Access (NUMA) systems.\nSection 6 is the central section of this paper. It brings to-\ngether all the previous sections’ information and gives\nprogrammers advice on how to write code which per-\nforms well in the various situations. The very impatient\nreader could start with this section and, if necessary, go\nback to the earlier sections to freshen up the knowledge\nof the underlying technology.\nSection 7 introduces tools which can help the program-\nmer do a better job. Even with a complete understanding\nof the technology it is far from obvious where in a non-\ntrivial software project the problems are. Some tools are\nnecessary.\nIn section 8 we ﬁnally give an outlook of technology\nwhich can be expected in the near future or which might\njust simply be good to have.\nReporting Problems\nThe author intends to update this document for some\ntime. This includes updates made necessary by advances\nin technology but also to correct mistakes. Readers will-\ning to report problems are encouraged to send email to\nthe author. They are asked to include exact version in-\nformation in the report. The version information can be\nfound on the last page of the document.Thanks\nI would like to thank Johnray Fuller and the crew at LWN\n(especially Jonathan Corbet for taking on the daunting\ntask of transforming the author’s form of English into\nsomething more traditional. Markus Armbruster provided\na lot of valuable input on problems and omissions in the\ntext.\nAbout this Document\nThe title of this paper is an homage to David Goldberg’s\nclassic paper “What Every Computer Scientist Should\nKnow About Floating-Point Arithmetic” [12]. This pa-\nper is still not widely known, although it should be a\nprerequisite for anybody daring to touch a keyboard for\nserious programming.\nOne word on the PDF: xpdf draws some of the diagrams\nrather poorly. It is recommended it be viewed with evince\nor, if really necessary, Adobe’s programs. If you use\nevince be advised that hyperlinks are used extensively\nthroughout the document even though the viewer does\nnot indicate them like others do.\n2 Version 1.0 What Every Programmer Should Know About Memory",7520
02-2 Commodity Hardware Today.pdf,02-2 Commodity Hardware Today,"2 Commodity Hardware Today\nIt is important to understand commodity hardware be-\ncause specialized hardware is in retreat. Scaling these\ndays is most often achieved horizontally instead of verti-\ncally, meaning today it is more cost-effective to use many\nsmaller, connected commodity computers instead of a\nfew really large and exceptionally fast (and expensive)\nsystems. This is the case because fast and inexpensive\nnetwork hardware is widely available. There are still sit-\nuations where the large specialized systems have their\nplace and these systems still provide a business opportu-\nnity, but the overall market is dwarfed by the commodity\nhardware market. Red Hat, as of 2007, expects that for\nfuture products, the “standard building blocks” for most\ndata centers will be a computer with up to four sockets,\neach ﬁlled with a quad core CPU that, in the case of Intel\nCPUs, will be hyper-threaded.2This means the standard\nsystem in the data center will have up to 64 virtual pro-\ncessors. Bigger machines will be supported, but the quad\nsocket, quad CPU core case is currently thought to be the\nsweet spot and most optimizations are targeted for such\nmachines.\nLarge differences exist in the structure of computers built\nof commodity parts. That said, we will cover more than\n90% of such hardware by concentrating on the most im-\nportant differences. Note that these technical details tend\nto change rapidly, so the reader is advised to take the date\nof this writing into account.\nOver the years personal computers and smaller servers\nstandardized on a chipset with two parts: the Northbridge\nand Southbridge. Figure 2.1 shows this structure.\nSouthbridge PCI-ESATA\nUSBNorthbridge RAMCPU 1 CPU 2\nFSB\nFigure 2.1: Structure with Northbridge and Southbridge\nAll CPUs (two in the previous example, but there can be\nmore) are connected via a common bus (the Front Side\nBus, FSB) to the Northbridge. The Northbridge contains,\namong other things, the memory controller, and its im-\nplementation determines the type of RAM chips used for\nthe computer. Different types of RAM, such as DRAM,\nRambus, and SDRAM, require different memory con-\ntrollers.\nTo reach all other system devices, the Northbridge must\ncommunicate with the Southbridge. The Southbridge,\noften referred to as the I/O bridge, handles communica-\n2Hyper-threading enables a single processor core to be used for two\nor more concurrent executions with just a little extra hardware.tion with devices through a variety of different buses. To-\nday the PCI, PCI Express, SATA, and USB buses are of\nmost importance, but PATA, IEEE 1394, serial, and par-\nallel ports are also supported by the Southbridge. Older\nsystems had AGP slots which were attached to the North-\nbridge. This was done for performance reasons related to\ninsufﬁciently fast connections between the Northbridge\nand Southbridge. However, today the PCI-E slots are all\nconnected to the Southbridge.\nSuch a system structure has a number of noteworthy con-\nsequences:\n• All data communication from one CPU to another\nmust travel over the same bus used to communicate\nwith the Northbridge.\n• All communication with RAM must pass through\nthe Northbridge.\n• The RAM has only a single port.3\n• Communication between a CPU and a device at-\ntached to the Southbridge is routed through the\nNorthbridge.\nA couple of bottlenecks are immediately apparent in this\ndesign. One such bottleneck involves access to RAM for\ndevices. In the earliest days of the PC, all communica-\ntion with devices on either bridge had to pass through the\nCPU, negatively impacting overall system performance.\nTo work around this problem some devices became ca-\npable of direct memory access (DMA). DMA allows de-\nvices, with the help of the Northbridge, to store and re-\nceive data in RAM directly without the intervention of\nthe CPU (and its inherent performance cost). Today all\nhigh-performance devices attached to any of the buses\ncan utilize DMA. While this greatly reduces the work-\nload on the CPU, it also creates contention for the band-\nwidth of the Northbridge as DMA requests compete with\nRAM access from the CPUs. This problem, therefore,\nmust be taken into account.\nA second bottleneck involves the bus from the North-\nbridge to the RAM. The exact details of the bus depend\non the memory types deployed. On older systems there\nis only one bus to all the RAM chips, so parallel ac-\ncess is not possible. Recent RAM types require two sep-\narate buses (or channels as they are called for DDR2,\nsee page 8) which doubles the available bandwidth. The\nNorthbridge interleaves memory access across the chan-\nnels. More recent memory technologies (FB-DRAM, for\ninstance) add more channels.\nWith limited bandwidth available, it is important for per-\nformance to schedule memory access in ways that mini-\nmize delays. As we will see, processors are much faster\n3We will not discuss multi-port RAM in this document as this type\nof RAM is not found in commodity hardware, at least not in places\nwhere the programmer has access to it. It can be found in specialized\nhardware such as network routers which depend on utmost speed.\nUlrich Drepper Version 1.0 3\nand must wait to access memory, despite the use of CPU\ncaches. If multiple hyper-threads, cores, or processors\naccess memory at the same time, the wait times for mem-\nory access are even longer. This is also true for DMA\noperations.\nThere is more to accessing memory than concurrency,\nhowever. Access patterns themselves also greatly inﬂu-\nence the performance of the memory subsystem, espe-\ncially with multiple memory channels. In section 2.2 we\nwil cover more details of RAM access patterns.\nOn some more expensive systems, the Northbridge does\nnot actually contain the memory controller. Instead the\nNorthbridge can be connected to a number of external\nmemory controllers (in the following example, four of\nthem).\nSouthbridge PCI-ESATA\nUSBNorthbridgeMC 2 RAMMC 1 RAM\nMC 4 RAMMC 3 RAMCPU 1 CPU 2\nFigure 2.2: Northbridge with External Controllers\nThe advantage of this architecture is that more than one\nmemory bus exists and therefore total available band-\nwidth increases. This design also supports more memory.\nConcurrent memory access patterns reduce delays by si-\nmultaneously accessing different memory banks. This\nis especially true when multiple processors are directly\nconnected to the Northbridge, as in Figure 2.2. For such\na design, the primary limitation is the internal bandwidth\nof the Northbridge, which is phenomenal for this archi-\ntecture (from Intel).4\nUsing multiple external memory controllers is not the\nonly way to increase memory bandwidth. One other in-\ncreasingly popular way is to integrate memory controllers\ninto the CPUs and attach memory to each CPU. This\narchitecture is made popular by SMP systems based on\nAMD’s Opteron processor. Figure 2.3 shows such a sys-\ntem. Intel will have support for the Common System In-\nterface (CSI) starting with the Nehalem processors; this\nis basically the same approach: an integrated memory\ncontroller with the possibility of local memory for each\nprocessor.\nWith an architecture like this there are as many memory\nbanks available as there are processors. On a quad-CPU\nmachine the memory bandwidth is quadrupled without\nthe need for a complicated Northbridge with enormous\nbandwidth. Having a memory controller integrated into\nthe CPU has some additional advantages; we will not dig\n4For completeness it should be mentioned that such a memory con-\ntroller arrangement can be used for other purposes such as “memory\nRAID” which is useful in combination with hotplug memory.CPU 3 CPU 4CPU 1 CPU 2\nRAMRAM\nRAMRAM\nSouthbridge PCI-ESATA\nUSB\nFigure 2.3: Integrated Memory Controller\ndeeper into this technology here.\nThere are disadvantages to this architecture, too. First of\nall, because the machine still has to make all the mem-\nory of the system accessible to all processors, the mem-\nory is not uniform anymore (hence the name NUMA -\nNon-Uniform Memory Architecture - for such an archi-\ntecture). Local memory (memory attached to a proces-\nsor) can be accessed with the usual speed. The situation\nis different when memory attached to another processor\nis accessed. In this case the interconnects between the\nprocessors have to be used. To access memory attached\nto CPU 2from CPU 1requires communication across one\ninterconnect. When the same CPU accesses memory at-\ntached to CPU 4two interconnects have to be crossed.\nEach such communication has an associated cost. We\ntalk about “NUMA factors” when we describe the ex-\ntra time needed to access remote memory. The example\narchitecture in Figure 2.3 has two levels for each CPU:\nimmediately adjacent CPUs and one CPU which is two\ninterconnects away. With more complicated machines\nthe number of levels can grow signiﬁcantly. There are\nalso machine architectures (for instance IBM’s x445 and\nSGI’s Altix series) where there is more than one type\nof connection. CPUs are organized into nodes; within a\nnode the time to access the memory might be uniform or\nhave only small NUMA factors. The connection between\nnodes can be very expensive, though, and the NUMA\nfactor can be quite high.\nCommodity NUMA machines exist today and will likely\nplay an even greater role in the future. It is expected that,\nfrom late 2008 on, every SMP machine will use NUMA.\nThe costs associated with NUMA make it important to\nrecognize when a program is running on a NUMA ma-\nchine. In section 5 we will discuss more machine archi-\ntectures and some technologies the Linux kernel provides\nfor these programs.\nBeyond the technical details described in the remainder\nof this section, there are several additional factors which\ninﬂuence the performance of RAM. They are not con-\ntrollable by software, which is why they are not covered\nin this section. The interested reader can learn about\nsome of these factors in section 2.1. They are really only\nneeded to get a more complete picture of RAM technol-\nogy and possibly to make better decisions when purchas-\ning computers.\n4 Version 1.0 What Every Programmer Should Know About Memory",10276
03-2.1 RAM Types.pdf,03-2.1 RAM Types,,0
04-2.1.1 Static RAM.pdf,04-2.1.1 Static RAM,,0
05-2.2 DRAM Access Technical Details.pdf,05-2.2 DRAM Access Technical Details,"The following two sections discuss hardware details at\nthe gate level and the access protocol between the mem-\nory controller and the DRAM chips. Programmers will\nlikely ﬁnd this information enlightening since these de-\ntails explain why RAM access works the way it does. It\nis optional knowledge, though, and the reader anxious to\nget to topics with more immediate relevance for everyday\nlife can jump ahead to section 2.2.5.\n2.1 RAM Types\nThere have been many types of RAM over the years and\neach type varies, sometimes signiﬁcantly, from the other.\nThe older types are today really only interesting to the\nhistorians. We will not explore the details of those. In-\nstead we will concentrate on modern RAM types; we will\nonly scrape the surface, exploring some details which\nare visible to the kernel or application developer through\ntheir performance characteristics.\nThe ﬁrst interesting details are centered around the ques-\ntion why there are different types of RAM in the same\nmachine. More speciﬁcally, why are there both static\nRAM (SRAM5) and dynamic RAM (DRAM). The for-\nmer is much faster and provides the same functionality.\nWhy is not all RAM in a machine SRAM? The answer\nis, as one might expect, cost. SRAM is much more ex-\npensive to produce and to use than DRAM. Both these\ncost factors are important, the second one increasing in\nimportance more and more. To understand these differ-\nences we look at the implementation of a bit of storage\nfor both SRAM and DRAM.\nIn the remainder of this section we will discuss some\nlow-level details of the implementation of RAM. We will\nkeep the level of detail as low as possible. To that end,\nwe will discuss the signals at a “logic level” and not at a\nlevel a hardware designer would have to use. That level\nof detail is unnecessary for our purpose here.\n2.1.1 Static RAM\nM1M3M2 M4\nM5M6Vdd\nBL BLWL\nFigure 2.4: 6-T Static RAM\nFigure 2.4 shows the structure of a 6 transistor SRAM\ncell. The core of this cell is formed by the four transistors\nM1toM4which form two cross-coupled inverters. They\nhave two stable states, representing 0 and 1 respectively.\nThe state is stable as long as power on Vddis available.\n5In other contexts SRAM might mean “synchronous RAM”.If access to the state of the cell is needed the word access\nlineWL is raised. This makes the state of the cell imme-\ndiately available for reading on BLandBL. If the cell\nstate must be overwritten the BLandBLlines are ﬁrst\nset to the desired values and then WL is raised. Since the\noutside drivers are stronger than the four transistors ( M1\nthrough M4) this allows the old state to be overwritten.\nSee [20] for a more detailed description of the way the\ncell works. For the following discussion it is important\nto note that\n• one cell requires six transistors. There are variants\nwith four transistors but they have disadvantages.\n• maintaining the state of the cell requires constant\npower.\n• the cell state is available for reading almost im-\nmediately once the word access line WL is raised.\nThe signal is as rectangular (changing quickly be-\ntween the two binary states) as other transistor-\ncontrolled signals.\n• the cell state is stable, no refresh cycles are needed.\nThere are other, slower and less power-hungry, SRAM\nforms available, but those are not of interest here since\nwe are looking at fast RAM. These slow variants are\nmainly interesting because they can be more easily used\nin a system than dynamic RAM because of their simpler\ninterface.\n2.1.2 Dynamic RAM\nDynamic RAM is, in its structure, much simpler than\nstatic RAM. Figure 2.5 shows the structure of a usual\nDRAM cell design. All it consists of is one transistor\nand one capacitor. This huge difference in complexity of\ncourse means that it functions very differently than static\nRAM.\nDLAL\nMC\nFigure 2.5: 1-T Dynamic RAM\nA dynamic RAM cell keeps its state in the capacitor C.\nThe transistor Mis used to guard the access to the state.\nTo read the state of the cell the access line ALis raised;\nthis either causes a current to ﬂow on the data line DLor\nnot, depending on the charge in the capacitor. To write\nto the cell the data line DLis appropriately set and then\nALis raised for a time long enough to charge or drain\nthe capacitor.\nThere are a number of complications with the design of\ndynamic RAM. The use of a capacitor means that reading\nUlrich Drepper Version 1.0 5\nthe cell discharges the capacitor. The procedure cannot\nbe repeated indeﬁnitely, the capacitor must be recharged\nat some point. Even worse, to accommodate the huge\nnumber of cells (chips with 109or more cells are now\ncommon) the capacity to the capacitor must be low (in\nthe femto-farad range or lower). A fully charged capac-\nitor holds a few 10’s of thousands of electrons. Even\nthough the resistance of the capacitor is high (a couple of\ntera-ohms) it only takes a short time for the capacity to\ndissipate. This problem is called “leakage”.\nThis leakage is why a DRAM cell must be constantly\nrefreshed. For most DRAM chips these days this refresh\nmust happen every 64ms. During the refresh cycle no\naccess to the memory is possible since a refresh is simply\na memory read operation where the result is discarded.\nFor some workloads this overhead might stall up to 50%\nof the memory accesses (see [3]).\nA second problem resulting from the tiny charge is that\nthe information read from the cell is not directly usable.\nThe data line must be connected to a sense ampliﬁer\nwhich can distinguish between a stored 0 or 1 over the\nwhole range of charges which still have to count as 1.\nA third problem is that reading a cell causes the charge\nof the capacitor to be depleted. This means every read\noperation must be followed by an operation to recharge\nthe capacitor. This is done automatically by feeding the\noutput of the sense ampliﬁer back into the capacitor. It\ndoes mean, though, the reading memory content requires\nadditional energy and, more importantly, time.\nA fourth problem is that charging and draining a capac-\nitor is not instantaneous. The signals received by the\nsense ampliﬁer are not rectangular, so a conservative es-\ntimate as to when the output of the cell is usable has to\nbe used. The formulas for charging and discharging a\ncapacitor are\nQCharge(t) =Q0(1 e t\nRC)\nQDischarge (t) =Q0e t\nRC\nThis means it takes some time (determined by the capac-\nity C and resistance R) for the capacitor to be charged and\ndischarged. It also means that the current which can be\ndetected by the sense ampliﬁers is not immediately avail-\nable. Figure 2.6 shows the charge and discharge curves.\nThe X–axis is measured in units of RC (resistance multi-\nplied by capacitance) which is a unit of time.\nUnlike the static RAM case where the output is immedi-\nately available when the word access line is raised, it will\nalways take a bit of time until the capacitor discharges\nsufﬁciently. This delay severely limits how fast DRAM\ncan be.\nThe simple approach has its advantages, too. The main\nadvantage is size. The chip real estate needed for one\nDRAM cell is many times smaller than that of an SRAM1RC 2RC 3RC 4RC 5RC 6RC 7RC 8RC 9RC0102030405060708090100 Percentage Char geCharge Discharge\nFigure 2.6: Capacitor Charge and Discharge Timing\ncell. The SRAM cells also need individual power for\nthe transistors maintaining the state. The structure of\nthe DRAM cell is also simpler and more regular which\nmeans packing many of them close together on a die is\nsimpler.\nOverall, the (quite dramatic) difference in cost wins. Ex-\ncept in specialized hardware – network routers, for exam-\nple – we have to live with main memory which is based\non DRAM. This has huge implications on the program-\nmer which we will discuss in the remainder of this paper.\nBut ﬁrst we need to look into a few more details of the\nactual use of DRAM cells.\n2.1.3 DRAM Access\nA program selects a memory location using a virtual ad-\ndress. The processor translates this into a physical ad-\ndress and ﬁnally the memory controller selects the RAM\nchip corresponding to that address. To select the individ-\nual memory cell on the RAM chip, parts of the physical\naddress are passed on in the form of a number of address\nlines.\nIt would be completely impractical to address memory\nlocations individually from the memory controller: 4GB\nof RAM would require 232address lines. Instead the\naddress is passed encoded as a binary number using a\nsmaller set of address lines. The address passed to the\nDRAM chip this way must be demultiplexed ﬁrst. A\ndemultiplexer with Naddress lines will have 2Noutput\nlines. These output lines can be used to select the mem-\nory cell. Using this direct approach is no big problem for\nchips with small capacities.\nBut if the number of cells grows this approach is not suit-\nable anymore. A chip with 1Gbit6capacity would need\n30 address lines and 230select lines. The size of a de-\nmultiplexer increases exponentially with the number of\ninput lines when speed is not to be sacriﬁced. A demulti-\nplexer for 30 address lines needs a whole lot of chip real\nestate in addition to the complexity (size and time) of\nthe demultiplexer. Even more importantly, transmitting\n6I hate those SI preﬁxes. For me a giga-bit will always be 230and\nnot109bits.\n6 Version 1.0 What Every Programmer Should Know About Memory\n30 impulses on the address lines synchronously is much\nharder than transmitting “only” 15 impulses. Fewer lines\nhave to be laid out at exactly the same length or timed\nappropriately.7RowAddr essSelectiona0\na1\nColumn Address Selectiona2\na3\nData\nFigure 2.7: Dynamic RAM Schematic\nFigure 2.7 shows a DRAM chip at a very high level. The\nDRAM cells are organized in rows and columns. They\ncould all be aligned in one row but then the DRAM chip\nwould need a huge demultiplexer. With the array ap-\nproach the design can get by with one demultiplexer and\none multiplexer of half the size.8This is a huge saving\non all fronts. In the example the address lines a0and a1\nthrough the row address selection (RAS)9demultiplexer\nselect the address lines of a whole row of cells. When\nreading, the content of all cells is thusly made available to\nthecolumn address selection (CAS)9multiplexer. Based\non the address lines a2and a3the content of one col-\numn is then made available to the data pin of the DRAM\nchip. This happens many times in parallel on a number\nof DRAM chips to produce a total number of bits corre-\nsponding to the width of the data bus.\nFor writing, the new cell value is put on the data bus and,\nwhen the cell is selected using the RAS and CAS, it is\nstored in the cell. A pretty straightforward design. There\nare in reality – obviously – many more complications.\nThere need to be speciﬁcations for how much delay there\nis after the signal before the data will be available on the\ndata bus for reading. The capacitors do not unload instan-\ntaneously, as described in the previous section. The sig-\nnal from the cells is so weak that it needs to be ampliﬁed.\nFor writing it must be speciﬁed how long the data must\nbe available on the bus after the RAS and CAS is done to\nsuccessfully store the new value in the cell (again, capac-\n7Modern DRAM types like DDR3 can automatically adjust the tim-\ning but there is a limit as to what can be tolerated.\n8Multiplexers and demultiplexers are equivalent and the multiplexer\nhere needs to work as a demultiplexer when writing. So we will drop\nthe differentiation from now on.\n9The line over the name indicates that the signal is negated.itors do not ﬁll or drain instantaneously). These timing\nconstants are crucial for the performance of the DRAM\nchip. We will talk about this in the next section.\nA secondary scalability problem is that having 30 address\nlines connected to every RAM chip is not feasible either.\nPins of a chip are precious resources. It is “bad” enough\nthat the data must be transferred as much as possible in\nparallel (e.g., in 64 bit batches). The memory controller\nmust be able to address each RAM module (collection of\nRAM chips). If parallel access to multiple RAM mod-\nules is required for performance reasons and each RAM\nmodule requires its own set of 30 or more address lines,\nthen the memory controller needs to have, for 8 RAM\nmodules, a whopping 240+ pins only for the address han-\ndling.\nTo counter these secondary scalability problems DRAM\nchips have, for a long time, multiplexed the address it-\nself. That means the address is transferred in two parts.\nThe ﬁrst part consisting of address bits ( a0and a1in the\nexample in Figure 2.7) select the row. This selection re-\nmains active until revoked. Then the second part, address\nbits a2and a3, select the column. The crucial difference\nis that only two external address lines are needed. A few\nmore lines are needed to indicate when the RAS and CAS\nsignals are available but this is a small price to pay for\ncutting the number of address lines in half. This address\nmultiplexing brings its own set of problems, though. We\nwill discuss them in section 2.2.\n2.1.4 Conclusions\nDo not worry if the details in this section are a bit over-\nwhelming. The important things to take away from this\nsection are:\n• there are reasons why not all memory is SRAM\n• memory cells need to be individually selected to\nbe used\n• the number of address lines is directly responsi-\nble for the cost of the memory controller, mother-\nboards, DRAM module, and DRAM chip\n• it takes a while before the results of the read or\nwrite operation are available\nThe following section will go into more details about the\nactual process of accessing DRAM memory. We are not\ngoing into more details of accessing SRAM, which is\nusually directly addressed. This happens for speed and\nbecause the SRAM memory is limited in size. SRAM\nis currently used in CPU caches and on-die where the\nconnections are small and fully under control of the CPU\ndesigner. CPU caches are a topic which we discuss later\nbut all we need to know is that SRAM cells have a certain\nmaximum speed which depends on the effort spent on the\nSRAM. The speed can vary from only slightly slower\nUlrich Drepper Version 1.0 7",14322
06-2.2.2 Precharge and Activation.pdf,06-2.2.2 Precharge and Activation,"than the CPU core to one or two orders of magnitude\nslower.\n2.2 DRAM Access Technical Details\nIn the section introducing DRAM we saw that DRAM\nchips multiplex the addresses in order to save resources\nint the form of address pins. We also saw that access-\ning DRAM cells takes time since the capacitors in those\ncells do not discharge instantaneously to produce a stable\nsignal; we also saw that DRAM cells must be refreshed.\nNow it is time to put this all together and see how all\nthese factors determine how the DRAM access has to\nhappen.\nWe will concentrate on current technology; we will not\ndiscuss asynchronous DRAM and its variants as they are\nsimply not relevant anymore. Readers interested in this\ntopic are referred to [3] and [19]. We will also not talk\nabout Rambus DRAM (RDRAM) even though the tech-\nnology is not obsolete. It is just not widely used for sys-\ntem memory. We will concentrate exclusively on Syn-\nchronous DRAM (SDRAM) and its successors Double\nData Rate DRAM (DDR).\nSynchronous DRAM, as the name suggests, works rel-\native to a time source. The memory controller provides\na clock, the frequency of which determines the speed of\nthe Front Side Bus (FSB) – the memory controller in-\nterface used by the DRAM chips. As of this writing,\nfrequencies of 800MHz, 1,066MHz, or even 1,333MHz\nare available with higher frequencies (1,600MHz) being\nannounced for the next generation. This does not mean\nthe frequency used on the bus is actually this high. In-\nstead, today’s buses are double- or quad-pumped, mean-\ning that data is transported two or four times per cy-\ncle. Higher numbers sell so the manufacturers like to\nadvertise a quad-pumped 200MHz bus as an “effective”\n800MHz bus.\nFor SDRAM today each data transfer consists of 64 bits\n– 8 bytes. The transfer rate of the FSB is therefore 8\nbytes multiplied by the effective bus frequency (6.4GB/s\nfor the quad-pumped 200MHz bus). That sounds a lot\nbut it is the burst speed, the maximum speed which will\nnever be surpassed. As we will see now the protocol for\ntalking to the RAM modules has a lot of downtime when\nno data can be transmitted. It is exactly this downtime\nwhich we must understand and minimize to achieve the\nbest performance.\n2.2.1 Read Access Protocol\nFigure 2.8 shows the activity on some of the connectors\nof a DRAM module which happens in three differently\ncolored phases. As usual, time ﬂows from left to right.\nA lot of details are left out. Here we only talk about the\nbus clock, RAS and CAS signals, and the address and\ndata buses. A read cycle begins with the memory con-\ntroller making the row address available on the addressCLK\nRAS\nCAS\nRow\nAddrCol\nAddrAddress\nData\nOutData\nOutData\nOutData\nOutDQtRCDCL\nFigure 2.8: SDRAM Read Access Timing\nbus and lowering the RAS signal. All signals are read on\nthe rising edge of the clock (CLK) so it does not matter if\nthe signal is not completely square as long as it is stable\nat the time it is read. Setting the row address causes the\nRAM chip to start latching the addressed row.\nThe CAS signal can be sent after t RCD (RAS-to- CAS\nDelay) clock cycles. The column address is then trans-\nmitted by making it available on the address bus and low-\nering the CAS line. Here we can see how the two parts\nof the address (more or less halves, nothing else makes\nsense) can be transmitted over the same address bus.\nNow the addressing is complete and the data can be trans-\nmitted. The RAM chip needs some time to prepare for\nthis. The delay is usually called CAS Latency (CL). In\nFigure 2.8 the CAS latency is 2. It can be higher or lower,\ndepending on the quality of the memory controller, moth-\nerboard, and DRAM module. The latency can also have\nhalf values. With CL=2.5 the ﬁrst data would be avail-\nable at the ﬁrst falling ﬂank in the blue area.\nWith all this preparation to get to the data it would be\nwasteful to only transfer one data word. This is why\nDRAM modules allow the memory controller to spec-\nify how much data is to be transmitted. Often the choice\nis between 2, 4, or 8 words. This allows ﬁlling entire\nlines in the caches without a new RAS/ CAS sequence. It\nis also possible for the memory controller to send a new\nCAS signal without resetting the row selection. In this\nway, consecutive memory addresses can be read from\nor written to signiﬁcantly faster because the RAS sig-\nnal does not have to be sent and the row does not have\nto be deactivated (see below). Keeping the row “open”\nis something the memory controller has to decide. Spec-\nulatively leaving it open all the time has disadvantages\nwith real-world applications (see [3]). Sending new CAS\nsignals is only subject to the Command Rate of the RAM\nmodule (usually speciﬁed as T x, wherexis a value like\n1 or 2; it will be 1 for high-performance DRAM modules\nwhich accept new commands every cycle).\nIn this example the SDRAM spits out one word per cy-\ncle. This is what the ﬁrst generation does. DDR is able\nto transmit two words per cycle. This cuts down on the\ntransfer time but does not change the latency. In princi-\n8 Version 1.0 What Every Programmer Should Know About Memory",5206
07-2.2.4 Memory Types.pdf,07-2.2.4 Memory Types,"ple, DDR2 works the same although in practice it looks\ndifferent. There is no need to go into the details here. It is\nsufﬁcient to note that DDR2 can be made faster, cheaper,\nmore reliable, and is more energy efﬁcient (see [6] for\nmore information).\n2.2.2 Precharge and Activation\nFigure 2.8 does not cover the whole cycle. It only shows\nparts of the full cycle of accessing DRAM. Before a new\nRAS signal can be sent the currently latched row must be\ndeactivated and the new row must be precharged. We can\nconcentrate here on the case where this is done with an\nexplicit command. There are improvements to the pro-\ntocol which, in some situations, allows this extra step to\nbe avoided. The delays introduced by precharging still\naffect the operation, though.\nCLK\nRAStRP\nCASWE\nCol\nAddrRow\nAddrCol\nAddrAddress\nData\nOutData\nOutDQtRCDCL\nFigure 2.9: SDRAM Precharge and Activation\nFigure 2.9 shows the activity starting from one CAS sig-\nnal to the CAS signal for another row. The data requested\nwith the ﬁrst CAS signal is available as before, after CL\ncycles. In the example two words are requested which,\non a simple SDRAM, takes two cycles to transmit. Al-\nternatively, imagine four words on a DDR chip.\nEven on DRAM modules with a command rate of one\nthe precharge command cannot be issued right away. It\nis necessary to wait as long as it takes to transmit the\ndata. In this case it takes two cycles. This happens to be\nthe same as CL but that is just a coincidence. The pre-\ncharge signal has no dedicated line; instead, some imple-\nmentations issue it by lowering the Write Enable ( WE)\nandRAS line simultaneously. This combination has no\nuseful meaning by itself (see [18] for encoding details).\nOnce the precharge command is issued it takes t RP(Row\nPrecharge time) cycles until the row can be selected. In\nFigure 2.9 much of the time (indicated by the purplish\ncolor) overlaps with the memory transfer (light blue).\nThis is good! But t RPis larger than the transfer time\nand so the next RAS signal is stalled for one cycle.\nIf we were to continue the timeline in the diagram we\nwould ﬁnd that the next data transfer happens 5 cycles\nafter the previous one stops. This means the data bus isonly in use two cycles out of seven. Multiply this with\nthe FSB speed and the theoretical 6.4GB/s for a 800MHz\nbus become 1.8GB/s. That is bad and must be avoided.\nThe techniques described in section 6 help to raise this\nnumber. But the programmer usually has to do her share.\nThere is one more timing value for a SDRAM module\nwhich we have not discussed. In Figure 2.9 the precharge\ncommand was only limited by the data transfer time. An-\nother constraint is that an SDRAM module needs time\nafter a RAS signal before it can precharge another row\n(denoted as t RAS). This number is usually pretty high,\nin the order of two or three times the t RPvalue. This is\na problem if, after a RAS signal, only one CAS signal\nfollows and the data transfer is ﬁnished in a few cycles.\nAssume that in Figure 2.9 the initial CAS signal was pre-\nceded directly by a RAS signal and that t RAS is 8 cycles.\nThen the precharge command would have to be delayed\nby one additional cycle since the sum of t RCD , CL, and\ntRP(since it is larger than the data transfer time) is only\n7 cycles.\nDDR modules are often described using a special nota-\ntion: w-x-y-z-T. For instance: 2-3-2-8-T1. This means:\nw 2CAS Latency (CL)\nx 3RAS-to- CAS delay (t RCD )\ny 2RAS Precharge (t RP)\nz 8Active to Precharge delay (t RAS)\nT T1 Command Rate\nThere are numerous other timing constants which affect\nthe way commands can be issued and are handled. Those\nﬁve constants are in practice sufﬁcient to determine the\nperformance of the module, though.\nIt is sometimes useful to know this information for the\ncomputers in use to be able to interpret certain measure-\nments. It is deﬁnitely useful to know these details when\nbuying computers since they, along with the FSB and\nSDRAM module speed, are among the most important\nfactors determining a computer’s speed.\nThe very adventurous reader could also try to tweak a\nsystem. Sometimes the BIOS allows changing some or\nall these values. SDRAM modules have programmable\nregisters where these values can be set. Usually the BIOS\npicks the best default value. If the quality of the RAM\nmodule is high it might be possible to reduce the one\nor the other latency without affecting the stability of the\ncomputer. Numerous overclocking websites all around\nthe Internet provide ample of documentation for doing\nthis. Do it at your own risk, though and do not say you\nhave not been warned.\n2.2.3 Recharging\nA mostly-overlooked topic when it comes to DRAM ac-\ncess is recharging. As explained in section 2.1.2, DRAM\ncells must constantly be refreshed. This does not happen\nUlrich Drepper Version 1.0 9\ncompletely transparently for the rest of the system. At\ntimes when a row10is recharged no access is possible.\nThe study in [3] found that “ [s]urprisingly, DRAM re-\nfresh organization can affect performance dramatically ”.\nEach DRAM cell must be refreshed every 64ms accord-\ning to the JEDEC (Joint Electron Device Engineering\nCouncil) speciﬁcation. If a DRAM array has 8,192 rows\nthis means the memory controller has to issue a refresh\ncommand on average every 7.8125 s (refresh commands\ncan be queued so in practice the maximum interval be-\ntween two requests can be higher). It is the memory\ncontroller’s responsibility to schedule the refresh com-\nmands. The DRAM module keeps track of the address\nof the last refreshed row and automatically increases the\naddress counter for each new request.\nThere is really not much the programmer can do about\nthe refresh and the points in time when the commands are\nissued. But it is important to keep this part of the DRAM\nlife cycle in mind when interpreting measurements. If a\ncritical word has to be retrieved from a row which cur-\nrently is being refreshed the processor could be stalled\nfor quite a long time. How long each refresh takes de-\npends on the DRAM module.\n2.2.4 Memory Types\nIt is worth spending some time on the current and soon-\nto-be current memory types in use. We will start with\nSDR (Single Data Rate) SDRAMs since they are the ba-\nsis of the DDR (Double Data Rate) SDRAMs. SDRs\nwere pretty simple. The memory cells and the data trans-\nfer rate were identical.\nDRAM\nCell\nArrayf f\nFigure 2.10: SDR SDRAM Operation\nIn Figure 2.10 the DRAM cell array can output the mem-\nory content at the same rate it can be transported over\nthe memory bus. If the DRAM cell array can operate at\n100MHz, the data transfer rate of the bus of a single cell\nis thus 100Mb/s. The frequency ffor all components is\nthe same. Increasing the throughput of the DRAM chip\nis expensive since the energy consumption rises with the\nfrequency. With a huge number of array cells this is\nprohibitively expensive.11In reality it is even more of\na problem since increasing the frequency usually also\nrequires increasing the voltage to maintain stability of\nthe system. DDR SDRAM (called DDR1 retroactively)\nmanages to improve the throughput without increasing\nany of the involved frequencies.\n10Rows are the granularity this happens with despite what [3] and\nother literature says (see [18]).\n11Power = Dynamic Capacity V oltage2Frequency.DRAM\nCell\nArrayI/O\nBufferf f f\nFigure 2.11: DDR1 SDRAM Operation\nThe difference between SDR and DDR1 is, as can be\nseen in Figure 2.11 and guessed from the name, that twice\nthe amount of data is transported per cycle. I.e., the\nDDR1 chip transports data on the rising andfalling edge.\nThis is sometimes called a “double-pumped” bus. To\nmake this possible without increasing the frequency of\nthe cell array a buffer has to be introduced. This buffer\nholds two bits per data line. This in turn requires that,\nin the cell array in Figure 2.7, the data bus consists of\ntwo lines. Implementing this is trivial: one only has to\nuse the same column address for two DRAM cells and\naccess them in parallel. The changes to the cell array to\nimplement this are also minimal.\nThe SDR DRAMs were known simply by their frequency\n(e.g., PC100 for 100MHz SDR). To make DDR1 DRAM\nsound better the marketers had to come up with a new\nscheme since the frequency did not change. They came\nwith a name which contains the transfer rate in bytes a\nDDR module (they have 64-bit busses) can sustain:\n100MHz64bit2 = 1;600MB/s\nHence a DDR module with 100MHz frequency is called\nPC1600. With 1600>100all marketing requirements\nare fulﬁlled; it sounds much better although the improve-\nment is really only a factor of two.12\nDRAM\nCell\nArrayI/O\nBufferf 2f 2f\nFigure 2.12: DDR2 SDRAM Operation\nTo get even more out of the memory technology DDR2\nincludes a bit more innovation. The most obvious change\nthat can be seen in Figure 2.12 is the doubling of the\nfrequency of the bus. Doubling the frequency means\ndoubling the bandwidth. Since this doubling of the fre-\nquency is not economical for the cell array it is now re-\nquired that the I/O buffer gets four bits in each clock cy-\ncle which it then can send on the bus. This means the\nchanges to the DDR2 modules consist of making only the\nI/O buffer component of the DIMM capable of running\nat higher speeds. This is certainly possible and will not\nrequire measurably more energy, it is just one tiny com-\nponent and not the whole module. The names the mar-\n12I will take the factor of two but I do not have to like the inﬂated\nnumbers.\n10 Version 1.0 What Every Programmer Should Know About Memory\nketers came up with for DDR2 are similar to the DDR1\nnames only in the computation of the value the factor of\ntwo is replaced by four (we now have a quad-pumped\nbus). Table 2.1 shows the names of the modules in use\ntoday.\nArray Bus Data Name Name\nFreq. Freq. Rate (Rate) (FSB)\n133MHz 266MHz 4,256MB/s PC2-4200 DDR2-533\n166MHz 333MHz 5,312MB/s PC2-5300 DDR2-667\n200MHz 400MHz 6,400MB/s PC2-6400 DDR2-800\n250MHz 500MHz 8,000MB/s PC2-8000 DDR2-1000\n266MHz 533MHz 8,512MB/s PC2-8500 DDR2-1066\nTable 2.1: DDR2 Module Names\nThere is one more twist to the naming. The FSB speed\nused by CPU, motherboard, and DRAM module is spec-\niﬁed by using the effective frequency. I.e., it factors in\nthe transmission on both ﬂanks of the clock cycle and\nthereby inﬂates the number. So, a 133MHz module with\na 266MHz bus has an FSB “frequency” of 533MHz.\nThe speciﬁcation for DDR3 (the real one, not the fake\nGDDR3 used in graphics cards) calls for more changes\nalong the lines of the transition to DDR2. The voltage\nwill be reduced from 1.8V for DDR2 to 1.5V for DDR3.\nSince the power consumption equation is calculated us-\ning the square of the voltage this alone brings a 30% im-\nprovement. Add to this a reduction in die size plus other\nelectrical advances and DDR3 can manage, at the same\nfrequency, to get by with half the power consumption.\nAlternatively, with higher frequencies, the same power\nenvelope can be hit. Or with double the capacity the same\nheat emission can be achieved.\nThe cell array of DDR3 modules will run at a quarter of\nthe speed of the external bus which requires an 8 bit I/O\nbuffer, up from 4 bits for DDR2. See Figure 2.13 for the\nschematics.\nDRAM\nCell\nArrayI/O\nBufferf 4f 4f\nFigure 2.13: DDR3 SDRAM Operation\nInitially DDR3 modules will likely have slightly higher\nCAS latencies just because the DDR2 technology is more\nmature. This would cause DDR3 to be useful only at\nfrequencies which are higher than those which can be\nachieved with DDR2, and, even then, mostly when band-\nwidth is more important than latency. There is already\ntalk about 1.3V modules which can achieve the same\nCAS latency as DDR2. In any case, the possibility of\nachieving higher speeds because of faster buses will out-\nweigh the increased latency.\nOne possible problem with DDR3 is that, for 1,600Mb/s\ntransfer rate or higher, the number of modules per chan-nel may be reduced to just one. In earlier versions this\nrequirement held for all frequencies, so one can hope\nthat the requirement will at some point be lifted for all\nfrequencies. Otherwise the capacity of systems will be\nseverely limited.\nTable 2.2 shows the names of the DDR3 modules we are\nlikely to see. JEDEC agreed so far on the ﬁrst four types.\nGiven that Intel’s 45nm processors have an FSB speed of\n1,600Mb/s, the 1,866Mb/s is needed for the overclocking\nmarket. We will likely see more of this towards the end\nof the DDR3 lifecycle.\nArray Bus Data Name Name\nFreq. Freq. Rate (Rate) (FSB)\n100MHz 400MHz 6,400MB/s PC3-6400 DDR3-800\n133MHz 533MHz 8,512MB/s PC3-8500 DDR3-1066\n166MHz 667MHz 10,667MB/s PC3-10667 DDR3-1333\n200MHz 800MHz 12,800MB/s PC3-12800 DDR3-1600\n233MHz 933MHz 14,933MB/s PC3-14900 DDR3-1866\nTable 2.2: DDR3 Module Names\nAll DDR memory has one problem: the increased bus\nfrequency makes it hard to create parallel data busses. A\nDDR2 module has 240 pins. All connections to data and\naddress pins must be routed so that they have approxi-\nmately the same length. Even more of a problem is that,\nif more than one DDR module is to be daisy-chained on\nthe same bus, the signals get more and more distorted for\neach additional module. The DDR2 speciﬁcation allow\nonly two modules per bus (aka channel), the DDR3 spec-\niﬁcation only one module for high frequencies. With 240\npins per channel a single Northbridge cannot reasonably\ndrive more than two channels. The alternative is to have\nexternal memory controllers (as in Figure 2.2) but this is\nexpensive.\nWhat this means is that commodity motherboards are re-\nstricted to hold at most four DDR2 or DDR3 modules.\nThis restriction severely limits the amount of memory\na system can have. Even old 32-bit IA-32 processors\ncan handle 64GB of RAM and memory demand even for\nhome use is growing, so something has to be done.\nOne answer is to add memory controllers into each pro-\ncessor as explained in section 2. AMD does it with the\nOpteron line and Intel will do it with their CSI technol-\nogy. This will help as long as the reasonable amount of\nmemory a processor is able to use can be connected to a\nsingle processor. In some situations this is not the case\nand this setup will introduce a NUMA architecture and\nits negative effects. For some situations another solution\nis needed.\nIntel’s answer to this problem for big server machines, at\nleast at the moment, is called Fully Buffered DRAM (FB-\nDRAM). The FB-DRAM modules use the same memory\nchips as today’s DDR2 modules which makes them rela-\ntively cheap to produce. The difference is in the connec-\ntion with the memory controller. Instead of a parallel data\nbus FB-DRAM utilizes a serial bus (Rambus DRAM had\nUlrich Drepper Version 1.0 11",14909
08-2.2.5 Conclusions.pdf,08-2.2.5 Conclusions,,0
09-3.2 Cache Operation at High Level.pdf,09-3.2 Cache Operation at High Level,"this back when, too, and SATA is the successor of PATA,\nas is PCI Express for PCI/AGP). The serial bus can be\ndriven at a much higher frequency, reverting the negative\nimpact of the serialization and even increasing the band-\nwidth. The main effects of using a serial bus are\n1. more modules per channel can be used.\n2. more channels per Northbridge/memory controller\ncan be used.\n3. the serial bus is designed to be fully-duplex (two\nlines).\n4. it is cheap enough to implement a differential bus\n(two lines in each direction) and so increase the\nspeed.\nAn FB-DRAM module has only 69 pins, compared with\nthe 240 for DDR2. Daisy chaining FB-DRAM modules\nis much easier since the electrical effects of the bus can\nbe handled much better. The FB-DRAM speciﬁcation\nallows up to 8 DRAM modules per channel.\nCompared with the connectivity requirements of a dual-\nchannel Northbridge it is now possible to drive 6 chan-\nnels of FB-DRAM with fewer pins: 2240pins ver-\nsus669pins. The routing for each channel is much\nsimpler which could also help reducing the cost of the\nmotherboards.\nFully duplex parallel busses are prohibitively expensive\nfor the traditional DRAM modules, duplicating all those\nlines is too costly. With serial lines (even if they are dif-\nferential, as FB-DRAM requires) this is not the case and\nso the serial bus is designed to be fully duplexed, which\nmeans, in some situations, that the bandwidth is theoret-\nically doubled alone by this. But it is not the only place\nwhere parallelism is used for bandwidth increase. Since\nan FB-DRAM controller can run up to six channels at the\nsame time the bandwidth can be increased even for sys-\ntems with smaller amounts of RAM by using FB-DRAM.\nWhere a DDR2 system with four modules has two chan-\nnels, the same capacity can be handled via four chan-\nnels using an ordinary FB-DRAM controller. The actual\nbandwidth of the serial bus depends on the type of DDR2\n(or DDR3) chips used on the FB-DRAM module.\nWe can summarize the advantages like this:\nDDR2 FB-DRAM\nPins 240 69\nChannels 2 6\nDIMMs/Channel 2 8\nMax Memory1316GB14192GB\nThroughput1510GB/s40GB/s\n13Assuming 4GB modules.\n14An Intel presentation, for some reason I do not see, says 8GB. . .\n15Assuming DDR2-800 modules.There are a few drawbacks to FB-DRAMs if multiple\nDIMMs on one channel are used. The signal is delayed–\nalbeit minimally–at each DIMM in the chain, thereby in-\ncreasing the latency. A second problem is that the chip\ndriving the serial bus requires signiﬁcant amounts of en-\nergy because of the very high frequency and the need to\ndrive a bus. But for the same amount of memory with\nthe same frequency FB-DRAM can always be faster than\nDDR2 and DDR3 since the up-to four DIMMS can each\nget their own channel; for large memory systems DDR\nsimply has no answer using commodity components.\n2.2.5 Conclusions\nThis section should have shown that accessing DRAM is\nnot an arbitrarily fast process. At least not fast compared\nwith the speed the processor is running and with which it\ncan access registers and cache. It is important to keep in\nmind the differences between CPU and memory frequen-\ncies. An Intel Core 2 processor running at 2.933GHz and\na 1.066GHz FSB have a clock ratio of 11:1 (note: the\n1.066GHz bus is quad-pumped). Each stall of one cycle\non the memory bus means a stall of 11 cycles for the pro-\ncessor. For most machines the actual DRAMs used are\nslower, thusly increasing the delay. Keep these numbers\nin mind when we are talking about stalls in the upcoming\nsections.\nThe timing charts for the read command have shown that\nDRAM modules are capable of high sustained data rates.\nEntire DRAM rows could be transported without a single\nstall. The data bus could be kept occupied 100%. For\nDDR modules this means two 64-bit words transferred\neach cycle. With DDR2-800 modules and two channels\nthis means a rate of 12.8GB/s.\nBut, unless designed this way, DRAM access is not al-\nways sequential. Non-continuous memory regions are\nused which means precharging and new RAS signals are\nneeded. This is when things slow down and when the\nDRAM modules need help. The sooner the precharg-\ning can happen and the RAS signal sent the smaller the\npenalty when the row is actually used.\nHardware and software prefetching (see section 6.3) can\nbe used to create more overlap in the timing and reduce\nthe stall. Prefetching also helps shift memory operations\nin time so that there is less contention at later times, right\nbefore the data is actually needed. This is a frequent\nproblem when the data produced in one round has to be\nstored and the data required for the next round has to be\nread. By shifting the read in time, the write and read op-\nerations do not have to be issued at basically the same\ntime.\n2.3 Other Main Memory Users\nBeside CPUs there are other system components which\ncan access the main memory. High-performance cards\nsuch as network and mass-storage controllers cannot af-\n12 Version 1.0 What Every Programmer Should Know About Memory\nford to pipe all the data they need or provide through the\nCPU. Instead, they read or write the data directly from/to\nthe main memory (Direct Memory Access, DMA). In\nFigure 2.1 we can see that the cards can talk through\nthe South- and Northbridge directly with the memory.\nOther buses, like USB, also require FSB bandwidth–even\nif they do not use DMA–since the Southbridge is con-\nnected via the Northbridge to the processor through the\nFSB, too.\nWhile DMA is certainly beneﬁcial, it means that there is\nmore competition for the FSB bandwidth. In times with\nhigh DMA trafﬁc the CPU might stall more than usual\nwhile waiting for data from the main memory. There\nare ways around this given the right hardware. With an\narchitecture as in Figure 2.3 one can make sure the com-\nputation uses memory on nodes which are not affected\nby DMA. It is also possible to attach a Southbridge to\neach node, equally distributing the load on the FSB of\nall the nodes. There are a myriad of possibilities. In\nsection 6 we will introduce techniques and programming\ninterfaces which help achieving the improvements which\nare possible in software.\nFinally it should be mentioned that some cheap systems\nhave graphics systems without separate, dedicated video\nRAM. Those systems use parts of the main memory as\nvideo RAM. Since access to the video RAM is frequent\n(for a 1024x768 display with 16 bpp at 60Hz we are talk-\ning 94MB/s) and system memory, unlike RAM on graph-\nics cards, does not have two ports this can substantially\ninﬂuence the systems performance and especially the la-\ntency. It is best to ignore such systems when performance\nis a priority. They are more trouble than they are worth.\nPeople buying those machines know they will not get the\nbest performance.3 CPU Caches\nCPUs are today much more sophisticated than they were\nonly 25 years ago. In those days, the frequency of the\nCPU core was at a level equivalent to that of the mem-\nory bus. Memory access was only a bit slower than reg-\nister access. But this changed dramatically in the early\n90s, when CPU designers increased the frequency of the\nCPU core but the frequency of the memory bus and the\nperformance of RAM chips did not increase proportion-\nally. This is not due to the fact that faster RAM could\nnot be built, as explained in the previous section. It is\npossible but it is not economical. RAM as fast as current\nCPU cores is orders of magnitude more expensive than\nany dynamic RAM.\nIf the choice is between a machine with very little, very\nfast RAM and a machine with a lot of relatively fast\nRAM, the second will always win given a working set\nsize which exceeds the small RAM size and the cost of\naccessing secondary storage media such as hard drives.\nThe problem here is the speed of secondary storage, usu-\nally hard disks, which must be used to hold the swapped\nout part of the working set. Accessing those disks is or-\nders of magnitude slower than even DRAM access.\nFortunately it does not have to be an all-or-nothing deci-\nsion. A computer can have a small amount of high-speed\nSRAM in addition to the large amount of DRAM. One\npossible implementation would be to dedicate a certain\narea of the address space of the processor as containing\nthe SRAM and the rest the DRAM. The task of the op-\nerating system would then be to optimally distribute data\nto make use of the SRAM. Basically, the SRAM serves\nin this situation as an extension of the register set of the\nprocessor.\nWhile this is a possible implementation it is not viable.\nIgnoring the problem of mapping the physical resources\nof such SRAM-backed memory to the virtual address\nspaces of the processes (which by itself is terribly hard)\nthis approach would require each process to administer\nin software the allocation of this memory region. The\nsize of the memory region can vary from processor to\nprocessor (i.e., processors have different amounts of the\nexpensive SRAM-backed memory). Each module which\nmakes up part of a program will claim its share of the\nfast memory, which introduces additional costs through\nsynchronization requirements. In short, the gains of hav-\ning fast memory would be eaten up completely by the\noverhead of administering the resources.\nSo, instead of putting the SRAM under the control of\nthe OS or user, it becomes a resource which is transpar-\nently used and administered by the processors. In this\nmode, SRAM is used to make temporary copies of (to\ncache, in other words) data in main memory which is\nlikely to be used soon by the processor. This is possible\nbecause program code and data has temporal and spa-\ntial locality. This means that, over short periods of time,\nthere is a good chance that the same code or data gets\nUlrich Drepper Version 1.0 13\nreused. For code this means that there are most likely\nloops in the code so that the same code gets executed\nover and over again (the perfect case for spatial locality ).\nData accesses are also ideally limited to small regions.\nEven if the memory used over short time periods is not\nclose together there is a high chance that the same data\nwill be reused before long ( temporal locality ). For code\nthis means, for instance, that in a loop a function call is\nmade and that function is located elsewhere in the ad-\ndress space. The function may be distant in memory, but\ncalls to that function will be close in time. For data it\nmeans that the total amount of memory used at one time\n(the working set size) is ideally limited but the memory\nused, as a result of the random access nature of RAM, is\nnot close together. Realizing that locality exists is key to\nthe concept of CPU caches as we use them today.\nA simple computation can show how effective caches\ncan theoretically be. Assume access to main memory\ntakes 200 cycles and access to the cache memory take\n15 cycles. Then code using 100 data elements 100 times\neach will spend 2,000,000 cycles on memory operations\nif there is no cache and only 168,500 cycles if all data\ncan be cached. That is an improvement of 91.5%.\nThe size of the SRAM used for caches is many times\nsmaller than the main memory. In the author’s experi-\nence with workstations with CPU caches the cache size\nhas always been around 1/1000th of the size of the main\nmemory (today: 4MB cache and 4GB main memory).\nThis alone does not constitute a problem. If the size of\nthe working set (the set of data currently worked on) is\nsmaller than the cache size it does not matter. But com-\nputers do not have large main memories for no reason.\nThe working set is bound to be larger than the cache.\nThis is especially true for systems running multiple pro-\ncesses where the size of the working set is the sum of the\nsizes of all the individual processes and the kernel.\nWhat is needed to deal with the limited size of the cache\nis a set of good strategies to determine what should be\ncached at any given time. Since not all data of the work-\ning set is used at exactly the same time we can use tech-\nniques to temporarily replace some data in the cache with\nothers. And maybe this can be done before the data is\nactually needed. This prefetching would remove some\nof the costs of accessing main memory since it happens\nasynchronously with respect to the execution of the pro-\ngram. All these techniques and more can be used to make\nthe cache appear bigger than it actually is. We will dis-\ncuss them in section 3.3. Once all these techniques are\nexploited it is up to the programmer to help the processor.\nHow this can be done will be discussed in section 6.\n3.1 CPU Caches in the Big Picture\nBefore diving into technical details of the implementa-\ntion of CPU caches some readers might ﬁnd it useful to\nﬁrst see in some more details how caches ﬁt into the “big\npicture” of a modern computer system.Cache CPU CoreBusMain Memory\nFigure 3.1: Minimum Cache Conﬁguration\nFigure 3.1 shows the minimum cache conﬁguration. It\ncorresponds to the architecture which could be found in\nearly systems which deployed CPU caches. The CPU\ncore is no longer directly connected to the main mem-\nory.16All loads and stores have to go through the cache.\nThe connection between the CPU core and the cache is\na special, fast connection. In a simpliﬁed representation,\nthe main memory and the cache are connected to the sys-\ntem bus which can also be used for communication with\nother components of the system. We introduced the sys-\ntem bus as “FSB” which is the name in use today; see\nsection 2.2. In this section we ignore the Northbridge; it\nis assumed to be present to facilitate the communication\nof the CPU(s) with the main memory.\nEven though most computers for the last several decades\nhave used the von Neumann architecture, experience has\nshown that it is of advantage to separate the caches used\nfor code and for data. Intel has used separate code and\ndata caches since 1993 and never looked back. The mem-\nory regions needed for code and data are pretty much\nindependent of each other, which is why independent\ncaches work better. In recent years another advantage\nemerged: the instruction decoding step for the most com-\nmon processors is slow; caching decoded instructions\ncan speed up the execution, especially when the pipeline\nis empty due to incorrectly predicted or impossible-to-\npredict branches.\nSoon after the introduction of the cache the system got\nmore complicated. The speed difference between the\ncache and the main memory increased again, to a point\nthat another level of cache was added, bigger and slower\nthan the ﬁrst-level cache. Only increasing the size of the\nﬁrst-level cache was not an option for economical rea-\nsons. Today, there are even machines with three levels\nof cache in regular use. A system with such a processor\nlooks like Figure 3.2. With the increase on the number of\ncores in a single CPU the number of cache levels might\nincrease in the future even more.\nFigure 3.2 shows three levels of cache and introduces the\nnomenclature we will use in the remainder of the docu-\nment. L1d is the level 1 data cache, L1i the level 1 in-\nstruction cache, etc. Note that this is a schematic; the\ndata ﬂow in reality need not pass through any of the\nhigher-level caches on the way from the core to the main\n16In even earlier systems the cache was attached to the system bus\njust like the CPU and the main memory. This was more a hack than a\nreal solution.\n14 Version 1.0 What Every Programmer Should Know About Memory\nL1d Cache CPU CoreL2 Cache L1i CacheL3 CacheBusMain Memory\nFigure 3.2: Processor with Level 3 Cache\nmemory. CPU designers have a lot of freedom design-\ning the interfaces of the caches. For programmers these\ndesign choices are invisible.\nIn addition we have processors which have multiple cores\nand each core can have multiple “threads”. The differ-\nence between a core and a thread is that separate cores\nhave separate copies of (almost17) all the hardware re-\nsources. The cores can run completely independently\nunless they are using the same resources–e.g., the con-\nnections to the outside–at the same time. Threads, on the\nother hand, share almost all of the processor’s resources.\nIntel’s implementation of threads has only separate reg-\nisters for the threads and even that is limited, some regis-\nters are shared. The complete picture for a modern CPU\ntherefore looks like Figure 3.3.\nMain Memory\nBus\nFigure 3.3: Multi processor, multi-core, multi-thread\nIn this ﬁgure we have two processors, each with two\ncores, each of which has two threads. The threads share\nthe Level 1 caches. The cores (shaded in the darker gray)\nhave individual Level 1 caches. All cores of the CPU\nshare the higher-level caches. The two processors (the\ntwo big boxes shaded in the lighter gray) of course do\nnot share any caches. All this will be important, espe-\ncially when we are discussing the cache effects on multi-\nprocess and multi-thread applications.\n17Early multi-core processors even had separate 2ndlevel caches and\nno3rdlevel cache.3.2 Cache Operation at High Level\nTo understand the costs and savings of using a cache we\nhave to combine the knowledge about the machine ar-\nchitecture and RAM technology from section 2 with the\nstructure of caches described in the previous section.\nBy default all data read or written by the CPU cores is\nstored in the cache. There are memory regions which\ncannot be cached but this is something only the OS im-\nplementers have to be concerned about; it is not visible\nto the application programmer. There are also instruc-\ntions which allow the programmer to deliberately bypass\ncertain caches. This will be discussed in section 6.\nIf the CPU needs a data word the caches are searched\nﬁrst. Obviously, the cache cannot contain the content\nof the entire main memory (otherwise we would need\nno cache), but since all memory addresses are cacheable,\neach cache entry is tagged using the address of the data\nword in the main memory. This way a request to read or\nwrite to an address can search the caches for a matching\ntag. The address in this context can be either the virtual\nor physical address, varying based on the cache imple-\nmentation.\nSince for the tag, in addition to the actual memory, addi-\ntional space is required, it is inefﬁcient to chose a word as\nthe granularity of the cache. For a 32-bit word on an x86\nmachine the tag itself might need 32 bits or more. Fur-\nthermore, since spatial locality is one of the principles on\nwhich caches are based, it would be bad to not take this\ninto account. Since neighboring memory is likely to be\nused together it should also be loaded into the cache to-\ngether. Remember also what we learned in section 2.2.1:\nRAM modules are much more effective if they can trans-\nport many data words in a row without a new CAS or\neven RAS signal. So the entries stored in the caches are\nnot single words but, instead, “lines” of several contigu-\nous words. In early caches these lines were 32 bytes\nlong; nowadays the norm is 64 bytes. If the memory\nbus is 64 bits wide this means 8 transfers per cache line.\nDDR supports this transport mode efﬁciently.\nWhen memory content is needed by the processor the\nentire cache line is loaded into the L1d. The memory\naddress for each cache line is computed by masking the\naddress value according to the cache line size. For a 64\nbyte cache line this means the low 6 bits are zeroed. The\ndiscarded bits are used as the offset into the cache line.\nThe remaining bits are in some cases used to locate the\nline in the cache and as the tag. In practice an address\nvalue is split into three parts. For a 32-bit address it might\nlook as follows:\nTag Cache Set Offset31 0\nT S O\nWith a cache line size of 2Othe low Obits are used\nUlrich Drepper Version 1.0 15\nas the offset into the cache line. The next Sbits select\nthe “cache set”. We will go into more detail soon on\nwhy sets, and not single slots, are used for cache lines.\nFor now it is sufﬁcient to understand there are 2Ssets of\ncache lines. This leaves the top 32 S O=Tbits which\nform the tag. These Tbits are the value associated with\neach cache line to distinguish all the aliases18which are\ncached in the same cache set. The Sbits used to address\nthe cache set do not have to be stored since they are the\nsame for all cache lines in the same set.\nWhen an instruction modiﬁes memory the processor still\nhas to load a cache line ﬁrst because no instruction mod-\niﬁes an entire cache line at once (exception to the rule:\nwrite-combining as explained in section 6.1). The con-\ntent of the cache line before the write operation therefore\nhas to be loaded. It is not possible for a cache to hold\npartial cache lines. A cache line which has been written\nto and which has not been written back to main memory\nis said to be “dirty”. Once it is written the dirty ﬂag is\ncleared.\nTo be able to load new data in a cache it is almost always\nﬁrst necessary to make room in the cache. An eviction\nfrom L1d pushes the cache line down into L2 (which\nuses the same cache line size). This of course means\nroom has to be made in L2. This in turn might push the\ncontent into L3 and ultimately into main memory. Each\neviction is progressively more expensive. What is de-\nscribed here is the model for an exclusive cache as is\npreferred by modern AMD and VIA processors. Intel\nimplements inclusive caches19where each cache line in\nL1d is also present in L2. Therefore evicting from L1d is\nmuch faster. With enough L2 cache the disadvantage of\nwasting memory for content held in two places is mini-\nmal and it pays off when evicting. A possible advantage\nof an exclusive cache is that loading a new cache line\nonly has to touch the L1d and not the L2, which could be\nfaster.\nThe CPUs are allowed to manage the caches as they like\nas long as the memory model deﬁned for the processor\narchitecture is not changed. It is, for instance, perfectly\nﬁne for a processor to take advantage of little or no mem-\nory bus activity and proactively write dirty cache lines\nback to main memory. The wide variety of cache archi-\ntectures among the processors for the x86 and x86-64,\nbetween manufacturers and even within the models of\nthe same manufacturer, are testament to the power of the\nmemory model abstraction.\nIn symmetric multi-processor (SMP) systems the caches\nof the CPUs cannot work independently from each other.\nAll processors are supposed to see the same memory con-\ntent at all times. The maintenance of this uniform view\nof memory is called “cache coherency”. If a processor\nwere to look simply at its own caches and main mem-\n18All cache lines with the same Spart of the address are known by\nthe same alias.\n19This generalization is not completely correct. A few caches are\nexclusive and some inclusive caches have exclusive cache properties.ory it would not see the content of dirty cache lines in\nother processors. Providing direct access to the caches\nof one processor from another processor would be terri-\nbly expensive and a huge bottleneck. Instead, processors\ndetect when another processor wants to read or write to a\ncertain cache line.\nIf a write access is detected and the processor has a clean\ncopy of the cache line in its cache, this cache line is\nmarked invalid. Future references will require the cache\nline to be reloaded. Note that a read access on another\nCPU does not necessitate an invalidation, multiple clean\ncopies can very well be kept around.\nMore sophisticated cache implementations allow another\npossibility to happen. Assume a cache line is dirty in\none processor’s cache and a second processor wants to\nread or write that cache line. In this case the main mem-\nory is out-of-date and the requesting processor must, in-\nstead, get the cache line content from the ﬁrst proces-\nsor. Through snooping, the ﬁrst processor notices this\nsituation and automatically sends the requesting proces-\nsor the data. This action bypasses main memory, though\nin some implementations the memory controller is sup-\nposed to notice this direct transfer and store the updated\ncache line content in main memory. If the access is for\nwriting the ﬁrst processor then invalidates its copy of the\nlocal cache line.\nOver time a number of cache coherency protocols have\nbeen developed. The most important is MESI, which we\nwill introduce in section 3.3.4. The outcome of all this\ncan be summarized in a few simple rules:\n• A dirty cache line is not present in any other pro-\ncessor’s cache.\n• Clean copies of the same cache line can reside in\narbitrarily many caches.\nIf these rules can be maintained, processors can use their\ncaches efﬁciently even in multi-processor systems. All\nthe processors need to do is to monitor each others’ write\naccesses and compare the addresses with those in their\nlocal caches. In the next section we will go into a few\nmore details about the implementation and especially the\ncosts.\nFinally, we should at least give an impression of the costs\nassociated with cache hits and misses. These are the\nnumbers Intel lists for a Pentium M:\nTo Where Cycles\nRegister1\nL1d3\nL214\nMain Memory240\nThese are the actual access times measured in CPU cy-\ncles. It is interesting to note that for the on-die L2 cache\n16 Version 1.0 What Every Programmer Should Know About Memory",25599
10-3.3 CPU Cache Implementation Details.pdf,10-3.3 CPU Cache Implementation Details,,0
11-3.3.1 Associativity.pdf,11-3.3.1 Associativity,"a large part (probably even the majority) of the access\ntime is caused by wire delays. This is a physical lim-\nitation which can only get worse with increasing cache\nsizes. Only process shrinking (for instance, going from\n60nm for Merom to 45nm for Penryn in Intel’s lineup)\ncan improve those numbers.\nThe numbers in the table look high but, fortunately, the\nentire cost does not have to be paid for each occurrence\nof the cache load and miss. Some parts of the cost can\nbe hidden. Today’s processors all use internal pipelines\nof different lengths where the instructions are decoded\nand prepared for execution. Part of the preparation is\nloading values from memory (or cache) if they are trans-\nferred to a register. If the memory load operation can\nbe started early enough in the pipeline, it may happen in\nparallel with other operations and the entire cost of the\nload might be hidden. This is often possible for L1d; for\nsome processors with long pipelines for L2 as well.\nThere are many obstacles to starting the memory read\nearly. It might be as simple as not having sufﬁcient re-\nsources for the memory access or it might be that the ﬁnal\naddress of the load becomes available late as the result of\nanother instruction. In these cases the load costs cannot\nbe hidden (completely).\nFor write operations the CPU does not necessarily have\nto wait until the value is safely stored in memory. As\nlong as the execution of the following instructions ap-\npears to have the same effect as if the value were stored\nin memory there is nothing which prevents the CPU from\ntaking shortcuts. It can start executing the next instruc-\ntion early. With the help of shadow registers which can\nhold values no longer available in a regular register it is\neven possible to change the value which is to be stored in\nthe incomplete write operation.\n1251020501002005001000\n210213216219222225228\nWorking Set Size (Bytes)Cycles/Oper ation\nFigure 3.4: Access Times for Random Writes\nFor an illustration of the effects of cache behavior see\nFigure 3.4. We will talk about the program which gener-\nated the data later; it is a simple simulation of a program\nwhich accesses a conﬁgurable amount of memory repeat-\nedly in a random fashion. Each data item has a ﬁxed size.\nThe number of elements depends on the selected work-ing set size. The Y–axis shows the average number of\nCPU cycles it takes to process one element; note that the\nscale for the Y–axis is logarithmic. The same applies in\nall the diagrams of this kind to the X–axis. The size of\nthe working set is always shown in powers of two.\nThe graph shows three distinct plateaus. This is not sur-\nprising: the speciﬁc processor has L1d and L2 caches,\nbut no L3. With some experience we can deduce that the\nL1d is 213bytes in size and that the L2 is 220bytes in\nsize. If the entire working set ﬁts into the L1d the cycles\nper operation on each element is below 10. Once the L1d\nsize is exceeded the processor has to load data from L2\nand the average time springs up to around 28. Once the\nL2 is not sufﬁcient anymore the times jump to 480 cycles\nand more. This is when many or most operations have to\nload data from main memory. And worse: since data is\nbeing modiﬁed dirty cache lines have to be written back,\ntoo.\nThis graph should give sufﬁcient motivation to look into\ncoding improvements which help improve cache usage.\nWe are not talking about a difference of a few measly per-\ncent here; we are talking about orders-of-magnitude im-\nprovements which are sometimes possible. In section 6\nwe will discuss techniques which allow writing more ef-\nﬁcient code. The next section goes into more details of\nCPU cache designs. The knowledge is good to have but\nnot necessary for the rest of the paper. So this section\ncould be skipped.\n3.3 CPU Cache Implementation Details\nCache implementers have the problem that each cell in\nthe huge main memory potentially has to be cached. If\nthe working set of a program is large enough this means\nthere are many main memory locations which ﬁght for\neach place in the cache. Previously it was noted that a\nratio of 1-to-1000 for cache versus main memory size is\nnot uncommon.\n3.3.1 Associativity\nIt would be possible to implement a cache where each\ncache line can hold a copy of any memory location (see\nFigure 3.5). This is called a fully associative cache . To\naccess a cache line the processor core would have to\ncompare the tags of each and every cache line with the\ntag for the requested address. The tag would be com-\nprised of the entire part of the address which is not the\noffset into the cache line (that means, Sin the ﬁgure on\npage 15 is zero).\nThere are caches which are implemented like this but,\nby looking at the numbers for an L2 in use today, will\nshow that this is impractical. Given a 4MB cache with\n64B cache lines the cache would have 65,536 entries.\nTo achieve adequate performance the cache logic would\nhave to be able to pick from all these entries the one\nmatching a given tag in just a few cycles. The effort to\nUlrich Drepper Version 1.0 17\nTag DataCompTT\nO\nOCompTT\nO\nOCompTT\nO\nOCompTT\nO\nOTag Offset\nFigure 3.5: Fully Associative Cache Schematics\nimplement this would be enormous.\nFor each cache line a comparator is needed to compare\nthe large tag (note, Sis zero). The letter next to each\nconnection indicates the width in bits. If none is given\nit is a single bit line. Each comparator has to compare\ntwoT-bit-wide values. Then, based on the result, the ap-\npropriate cache line content is selected and made avail-\nable. This requires merging as many sets of Odata lines\nas there are cache buckets. The number of transistors\nneeded to implement a single comparator is large espe-\ncially since it must work very fast. No iterative com-\nparator is usable. The only way to save on the number\nof comparators is to reduce the number of them by iter-\natively comparing the tags. This is not suitable for the\nsame reason that iterative comparators are not: it takes\ntoo long.\nFully associative caches are practical for small caches\n(for instance, the TLB caches on some Intel processors\nare fully associative) but those caches are small, really\nsmall. We are talking about a few dozen entries at most.\nFor L1i, L1d, and higher level caches a different ap-\nproach is needed. What can be done is to restrict the\nsearch. In the most extreme restriction each tag maps\nto exactly one cache entry. The computation is simple:\ngiven the 4MB/64B cache with 65,536 entries we can\ndirectly address each entry by using bits 6 to 21 of the\naddress (16 bits). The low 6 bits are the index into the\ncache line.\nTag\nMUXTCompO\nMUX\nO\nDataS STag Set Offset\nT\nT OT OT OT O\nFigure 3.6: Direct-Mapped Cache Schematics\nSuch a direct-mapped cache is fast and relatively easy\nto implement as can be seen in Figure 3.6. It requiresexactly one comparator, one multiplexer (two in this di-\nagram where tag and data are separated, but this is not\na hard requirement on the design), and some logic to\nselect only valid cache line content. The comparator is\ncomplex due to the speed requirements but there is only\none of them now; as a result more effort can be spent\non making it fast. The real complexity in this approach\nlies in the multiplexers. The number of transistors in a\nsimple multiplexer grows with O(log N), where N is the\nnumber of cache lines. This is tolerable but might get\nslow, in which case speed can be increased by spending\nmore real estate on transistors in the multiplexers to par-\nallelize some of the work and to increase the speed. The\ntotal number of transistors can grow slowly with a grow-\ning cache size which makes this solution very attractive.\nBut it has a drawback: it only works well if the addresses\nused by the program are evenly distributed with respect\nto the bits used for the direct mapping. If they are not,\nand this is usually the case, some cache entries are heav-\nily used and therefore repeated evicted while others are\nhardly used at all or remain empty.\n\n\n\n\nTagMUX\n\nData\n\n\nMUX\nS S\nCompTT\nO\nOCompTT\nO\nOCompTT\nO\nOCompTT\nO\nOTag Set Offset\nFigure 3.7: Set-Associative Cache Schematics\nThis problem can be solved by making the cache set as-\nsociative . A set-associative cache combines the good\nfeatures of the full associative and direct-mapped caches\nto largely avoid the weaknesses of those designs. Fig-\nure 3.7 shows the design of a set-associative cache. The\ntag and data storage are divided into sets, one of which is\nselected by the address of a cache line. This is similar to\nthe direct-mapped cache. But instead of only having one\nelement for each set value in the cache a small number\nof values is cached for the same set value. The tags for\nall the set members are compared in parallel, which is\nsimilar to the functioning of the fully associative cache.\nThe result is a cache which is not easily defeated by\nunfortunate–or deliberate–selection of addresses with the\nsame set numbers and at the same time the size of the\ncache is not limited by the number of comparators which\ncan be implemented economically. If the cache grows it\nis (in this ﬁgure) only the number of columns which in-\ncreases, not the number of rows. The number of rows\n(and therefore comparators) only increases if the asso-\nciativity of the cache is increased. Today processors are\nusing associativity levels of up to 24 for L2 caches or\nhigher. L1 caches usually get by with 8 sets.\n18 Version 1.0 What Every Programmer Should Know About Memory\nL2 Associativity\nCache Direct 2 4 8\nSizeCL=32 CL=64 CL=32 CL=64 CL=32 CL=64 CL=32 CL=64\n512k 27,794,595 20,422,527 25,222,611 18,303,581 24,096,510 17,356,121 23,666,929 17,029,334\n1M 19,007,315 13,903,854 16,566,738 12,127,174 15,537,500 11,436,705 15,162,895 11,233,896\n2M 12,230,962 8,801,403 9,081,881 6,491,011 7,878,601 5,675,181 7,391,389 5,382,064\n4M 7,749,986 5,427,836 4,736,187 3,159,507 3,788,122 2,418,898 3,430,713 2,125,103\n8M 4,731,904 3,209,693 2,690,498 1,602,957 2,207,655 1,228,190 2,111,075 1,155,847\n16M 2,620,587 1,528,592 1,958,293 1,089,580 1,704,878 883,530 1,671,541 862,324\nTable 3.1: Effects of Cache Size, Associativity, and Line Size\nGiven our 4MB/64B cache and 8-way set associativity\nthe cache we are left with has 8,192 sets and only 13\nbits of the tag are used in addressing the cache set. To\ndetermine which (if any) of the entries in the cache set\ncontains the addressed cache line 8 tags have to be com-\npared. That is feasible to do in very short time. With an\nexperiment we can see that this makes sense.\nTable 3.1 shows the number of L2 cache misses for a\nprogram (gcc in this case, the most important benchmark\nof them all, according to the Linux kernel people) for\nchanging cache size, cache line size, and associativity set\nsize. In section 7.2 we will introduce the tool to simulate\nthe caches as required for this test.\nJust in case this is not yet obvious, the relationship of all\nthese values is that the cache size is\ncache line sizeassociativitynumber of sets\nThe addresses are mapped into the cache by using\nO= log2cache line size\nS= log2number of sets\nin the way the ﬁgure on page 15 shows.\nFigure 3.8 makes the data of the table more comprehen-\nsible. It shows the data for a ﬁxed cache line size of\n32 bytes. Looking at the numbers for a given cache size\nwe can see that associativity can indeed help to reduce\nthe number of cache misses signiﬁcantly. For an 8MB\ncache going from direct mapping to 2-way set associative\ncache saves almost 44% of the cache misses. The proces-\nsor can keep more of the working set in the cache with\na set associative cache compared with a direct mapped\ncache.\nIn the literature one can occasionally read that introduc-\ning associativity has the same effect as doubling cache\nsize. This is true in some extreme cases as can be seen\nin the jump from the 4MB to the 8MB cache. But it\ncertainly is not true for further doubling of the associa-\ntivity. As we can see in the data, the successive gains are0481216202428\n512k 1M 2M 4M 8M 16M\nCache SizeCache Misses (inMillions)\nDirect 2-way 4-way 8-way\nFigure 3.8: Cache Size vs Associativity (CL=32)\nmuch smaller. We should not completely discount the ef-\nfects, though. In the example program the peak memory\nuse is 5.6M. So with a 8MB cache there are unlikely to\nbe many (more than two) uses for the same cache set.\nWith a larger working set the savings can be higher as\nwe can see from the larger beneﬁts of associativity for\nthe smaller cache sizes.\nIn general, increasing the associativity of a cache above\n8 seems to have little effects for a single-threaded work-\nload. With the introduction of hyper-threaded proces-\nsors where the ﬁrst level cache is shared and multi-core\nprocessors which use a shared L2 cache the situation\nchanges. Now you basically have two programs hitting\non the same cache which causes the associativity in prac-\ntice to be halved (or quartered for quad-core processors).\nSo it can be expected that, with increasing numbers of\ncores, the associativity of the shared caches should grow.\nOnce this is not possible anymore (16-way set associa-\ntivity is already hard) processor designers have to start\nusing shared L3 caches and beyond, while L2 caches are\npotentially shared by a subset of the cores.\nAnother effect we can study in Figure 3.8 is how the in-\ncrease in cache size helps with performance. This data\ncannot be interpreted without knowing about the working\nUlrich Drepper Version 1.0 19",13713
12-3.3.2 Measurements of Cache Effects.pdf,12-3.3.2 Measurements of Cache Effects,"set size. Obviously, a cache as large as the main mem-\nory would lead to better results than a smaller cache, so\nthere is in general no limit to the largest cache size with\nmeasurable beneﬁts.\nAs already mentioned above, the size of the working set\nat its peak is 5.6M. This does not give us any absolute\nnumber of the maximum beneﬁcial cache size but it al-\nlows us to estimate the number. The problem is that\nnot all the memory used is contiguous and, therefore,\nwe have, even with a 16M cache and a 5.6M working\nset, conﬂicts (see the beneﬁt of the 2-way set associa-\ntive 16MB cache over the direct mapped version). But\nit is a safe bet that with the same workload the beneﬁts\nof a 32MB cache would be negligible. But who says the\nworking set has to stay the same? Workloads are grow-\ning over time and so should the cache size. When buying\nmachines, and one has to choose the cache size one is\nwilling to pay for, it is worthwhile to measure the work-\ning set size. Why this is important can be seen in the\nﬁgures on page 21.\nSequentialRandom\nFigure 3.9: Test Memory Layouts\nTwo types of tests are run. In the ﬁrst test the elements\nare processed sequentially. The test program follows the\npointer nbut the array elements are chained so that they\nare traversed in the order in which they are found in mem-\nory. This can be seen in the lower part of Figure 3.9.\nThere is one back reference from the last element. In the\nsecond test (upper part of the ﬁgure) the array elements\nare traversed in a random order. In both cases the array\nelements form a circular single-linked list.\n3.3.2 Measurements of Cache Effects\nAll the ﬁgures are created by measuring a program which\ncan simulate working sets of arbitrary size, read and write\naccess, and sequential or random access. We have al-\nready seen some results in Figure 3.4. The program cre-\nates an array corresponding to the working set size of\nelements of this type:\nstruct l {\nstruct l *n;\nlong int pad[NPAD];\n};All entries are chained in a circular list using the nel-\nement, either in sequential or random order. Advancing\nfrom one entry to the next always uses the pointer, even if\nthe elements are laid out sequentially. The pad element\nis the payload and it can grow arbitrarily large. In some\ntests the data is modiﬁed, in others the program only per-\nforms read operations.\nIn the performance measurements we are talking about\nworking set sizes. The working set is made up of an ar-\nray of struct l elements. A working set of 2Nbytes\ncontains\n2N=sizeof(struct l)\nelements. Obviously sizeof(struct l) depends on\nthe value of NPAD . For 32-bit systems, NPAD =7 means the\nsize of each array element is 32 bytes, for 64-bit systems\nthe size is 64 bytes.\nSingle Threaded Sequential Access The simplest\ncase is a simple walk over all the entries in the list. The\nlist elements are laid out sequentially, densely packed.\nWhether the order of processing is forward or backward\ndoes not matter, the processor can deal with both direc-\ntions equally well. What we measure here–and in all the\nfollowing tests–is how long it takes to handle a single list\nelement. The time unit is a processor cycle. Figure 3.10\nshows the result. Unless otherwise speciﬁed, all mea-\nsurements are made on a Pentium 4 machine in 64-bit\nmode which means the structure lwith NPAD=0 is eight\nbytes in size.\nThe ﬁrst two measurements are polluted by noise. The\nmeasured workload is simply too small to ﬁlter the ef-\nfects of the rest of the system out. We can safely assume\nthat the values are all at the 4 cycles level. With this in\nmind we can see three distinct levels:\n• Up to a working set size of 214bytes.\n• From 215bytes to 220bytes.\n• From 221bytes and up.\nThese steps can be easily explained: the processor has a\n16kB L1d and 1MB L2. We do not see sharp edges in the\ntransition from one level to the other because the caches\nare used by other parts of the system as well and so the\ncache is not exclusively available for the program data.\nSpeciﬁcally the L2 cache is a uniﬁed cache and also used\nfor the instructions (NB: Intel uses inclusive caches).\nWhat is perhaps not quite expected are the actual times\nfor the different working set sizes. The times for the L1d\nhits are expected: load times after an L1d hit are around\n4 cycles on the P4. But what about the L2 accesses?\nOnce the L1d is not sufﬁcient to hold the data one might\nexpect it would take 14 cycles or more per element since\nthis is the access time for the L2. But the results show\nthat only about 9 cycles are required. This discrepancy\n20 Version 1.0 What Every Programmer Should Know About Memory\n012345678910\n210213216219222225228\nWorking Set Size (Bytes)Cycles/List Element\nFigure 3.10: Sequential Read Access, NPAD=0\n050100150200250300350\n210213216219222225228\nWorking Set Size (Bytes)Cycles/List Element\nNPAD=0 NPAD=7 NPAD=15 NPAD=31\nFigure 3.11: Sequential Read for Several Sizes\n050100150200250300350400\n210212214216218220222224\nWorking Set Size (Bytes)Cycles/List Element\nOn Cache Line On Page\nFigure 3.12: TLB Inﬂuence for Sequential Readcan be explained by the advanced logic in the processors.\nIn anticipation of using consecutive memory regions, the\nprocessor prefetches the next cache line. This means that\nwhen the next line is actually used it is already halfway\nloaded. The delay required to wait for the next cache line\nto be loaded is therefore much less than the L2 access\ntime.\nThe effect of prefetching is even more visible once the\nworking set size grows beyond the L2 size. Before we\nsaid that a main memory access takes 200+ cycles. Only\nwith effective prefetching is it possible for the processor\nto keep the access times as low as 9 cycles. As we can\nsee from the difference between 200 and 9, this works\nout nicely.\nWe can observe the processor while prefetching, at least\nindirectly. In Figure 3.11 we see the times for the same\nworking set sizes but this time we see the graphs for dif-\nferent sizes of the structure l. This means we have fewer\nbut larger elements in the list. The different sizes have\nthe effect that the distance between the nelements in the\n(still consecutive) list grows. In the four cases of the\ngraph the distance is 0, 56, 120, and 248 bytes respec-\ntively.\nAt the bottom we can see the line from Figure 3.10, but\nthis time it appears more or less as a ﬂat line. The times\nfor the other cases are simply so much worse. We can see\nin this graph, too, the three different levels and we see the\nlarge errors in the tests with the small working set sizes\n(ignore them again). The lines more or less all match\neach other as long as only the L1d is involved. There is\nno prefetching necessary so all element sizes just hit the\nL1d for each access.\nFor the L2 cache hits we see that the three new lines\nall pretty much match each other but that they are at a\nhigher level (about 28). This is the level of the access\ntime for the L2. This means prefetching from L2 into\nL1d is basically disabled. Even with NPAD =7 we need a\nnew cache line for each iteration of the loop; for NPAD =0,\ninstead, the loop has to iterate eight times before the next\ncache line is needed. The prefetch logic cannot load a\nnew cache line every cycle. Therefore we see a stall to\nload from L2 in every iteration.\nIt gets even more interesting once the working set size\nexceeds the L2 capacity. Now all four lines vary widely.\nThe different element sizes play obviously a big role in\nthe difference in performance. The processor should rec-\nognize the size of the strides and not fetch unnecessary\ncache lines for NPAD =15 and 31 since the element size\nis smaller than the prefetch window (see section 6.3.1).\nWhere the element size is hampering the prefetching ef-\nforts is a result of a limitation of hardware prefetching:\nit cannot cross page boundaries. We are reducing the ef-\nfectiveness of the hardware scheduler by 50% for each\nsize increase. If the hardware prefetcher were allowed to\ncross page boundaries and the next page is not resident\nor valid the OS would have to get involved in locating the\nUlrich Drepper Version 1.0 21\npage. That means the program would experience a page\nfault it did not initiate itself. This is completely unaccept-\nable since the processor does not know whether a page is\nnot present or does not exist. In the latter case the OS\nwould have to abort the process. In any case, given that,\nforNPAD =7 and higher, we need one cache line per list\nelement the hardware prefetcher cannot do much. There\nsimply is no time to load the data from memory since\nall the processor does is read one word and then load the\nnext element.\nAnother big reason for the slowdown are the misses of\nthe TLB cache. This is a cache where the results of the\ntranslation of a virtual address to a physical address are\nstored, as is explained in more detail in section 4. The\nTLB cache is quite small since it has to be extremely\nfast. If more pages are accessed repeatedly than the TLB\ncache has entries for the translation from virtual to phys-\nical address has to be constantly repeated. This is a very\ncostly operation. With larger element sizes the cost of\na TLB lookup is amortized over fewer elements. That\nmeans the total number of TLB entries which have to be\ncomputed per list element is higher.\nTo observe the TLB effects we can run a different test.\nFor one measurement we lay out the elements sequen-\ntially as usual. We use NPAD =7 for elements which oc-\ncupy one entire cache line. For the second measurement\nwe place each list element on a separate page. The rest\nof each page is left untouched and we do not count it in\nthe total for the working set size.20The consequence is\nthat, for the ﬁrst measurement, each list iteration requires\na new cache line and, for every 64 elements, a new page.\nFor the second measurement each iteration requires load-\ning a new cache line which is on a new page.\nThe result can be seen in Figure 3.12. The measurements\nwere performed on the same machine as Figure 3.11.\nDue to limitations of the available RAM the working set\nsize had to be restricted to 224bytes which requires 1GB\nto place the objects on separate pages. The lower, red\ncurve corresponds exactly to the NPAD =7 curve in Fig-\nure 3.11. We see the distinct steps showing the sizes of\nthe L1d and L2 caches. The second curve looks radically\ndifferent. The important feature is the huge spike start-\ning when the working set size reaches 213bytes. This\nis when the TLB cache overﬂows. With an element size\nof 64 bytes we can compute that the TLB cache has 64\nentries. There are no page faults affecting the cost since\nthe program locks the memory to prevent it from being\nswapped out.\nAs can be seen the number of cycles it takes to compute\nthe physical address and store it in the TLB is very high.\nThe graph in Figure 3.12 shows the extreme case, but it\nshould now be clear that a signiﬁcant factor in the slow-\n20Yes, this is a bit inconsistent because in the other tests we count\nthe unused part of the struct in the element size and we could deﬁne\nNPAD so that each element ﬁlls a page. In that case the working set\nsizes would be very different. This is not the point of this test, though,\nand since prefetching is ineffective anyway this makes little difference.down for larger NPAD values is the reduced efﬁciency of\nthe TLB cache. Since the physical address has to be com-\nputed before a cache line can be read for either L2 or\nmain memory the address translation penalties are addi-\ntive to the memory access times. This in part explains\nwhy the total cost per list element for NPAD =31 is higher\nthan the theoretical access time for the RAM.\nWe can glimpse a few more details of the prefetch im-\nplementation by looking at the data of test runs where\nthe list elements are modiﬁed. Figure 3.13 shows three\nlines. The element width is in all cases 16 bytes. The ﬁrst\nline is the now familiar list walk which serves as a base-\nline. The second line, labeled “Inc”, simply increments\nthepad[0] member of the current element before going\non to the next. The third line, labeled “Addnext0”, takes\nthepad[0] list element of the next element and adds it\nto the pad[0] member of the current list element.\nThe na ¨ıve assumption would be that the “Addnext0” test\nruns slower because it has more work to do. Before ad-\nvancing to the next list element a value from that element\nhas to be loaded. This is why it is surprising to see that\nthis test actually runs, for some working set sizes, faster\nthan the “Inc” test. The explanation for this is that the\nload from the next list element is basically a forced pre-\nfetch. Whenever the program advances to the next list\nelement we know for sure that element is already in the\nL1d cache. As a result we see that the “Addnext0” per-\nforms as well as the simple “Follow” test as long as the\nworking set size ﬁts into the L2 cache.\nThe “Addnext0” test runs out of L2 faster than the “Inc”\ntest, though. It needs more data loaded from main mem-\nory. This is why the “Addnext0” test reaches the 28 cy-\ncles level for a working set size of 221bytes. The 28 cy-\ncles level is twice as high as the 14 cycles level the “Fol-\nlow” test reaches. This is easy to explain, too. Since the\nother two tests modify memory an L2 cache eviction to\nmake room for new cache lines cannot simply discard the\ndata. Instead it has to be written to memory. This means\nthe available bandwidth on the FSB is cut in half, hence\ndoubling the time it takes to transfer the data from main\nmemory to L2.\nOne last aspect of the sequential, efﬁcient cache han-\ndling is the size of the cache. This should be obvious\nbut it still should be pointed out. Figure 3.14 shows the\ntiming for the Increment benchmark with 128-byte ele-\nments ( NPAD =15 on 64-bit machines). This time we see\nthe measurement from three different machines. The ﬁrst\ntwo machines are P4s, the last one a Core2 processor.\nThe ﬁrst two differentiate themselves by having different\ncache sizes. The ﬁrst processor has a 32k L1d and an 1M\nL2. The second one has 16k L1d, 512k L2, and 2M L3.\nThe Core2 processor has 32k L1d and 4M L2.\nThe interesting part of the graph is not necessarily how\nwell the Core2 processor performs relative to the other\ntwo (although it is impressive). The main point of in-\nterest here is the region where the working set size is\n22 Version 1.0 What Every Programmer Should Know About Memory\n051015202530\n210213216219222225228\nWorking Set Size (Bytes)Cycles/List Element\nFollow Inc Addnext0\nFigure 3.13: Sequential Read and Write, NPAD=1\n0100200300400500600700\n210213216219222225228\nWorking Set Size (Bytes)Cycles/List Element\nP4/32k/1M P4/16k/512k/2M Core2/32k/4M\nFigure 3.14: Advantage of Larger L2/L3 Caches\n050100150200250300350400450500\n210213216219222225228\nWorking Set Size (Bytes)Cycles/List Element\nSequential Random\nFigure 3.15: Sequential vs Random Read, NPAD=0too large for the respective last level cache and the main\nmemory gets heavily involved.\nAs expected, the larger the last level cache is the longer\nthe curve stays at the low level corresponding to the L2\naccess costs. The important part to notice is the perfor-\nmance advantage this provides. The second processor\n(which is slightly older) can perform the work on the\nworking set of 220bytes twice as fast as the ﬁrst proces-\nsor. All thanks to the increased last level cache size. The\nCore2 processor with its 4M L2 performs even better.\nFor a random workload this might not mean that much.\nBut if the workload can be tailored to the size of the last\nlevel cache the program performance can be increased\nquite dramatically. This is why it sometimes is worth-\nwhile to spend the extra money for a processor with a\nlarger cache.\nSingle Threaded Random Access We have seen that\nthe processor is able to hide most of the main memory\nand even L2 access latency by prefetching cache lines\ninto L2 and L1d. This can work well only when the mem-\nory access is predictable, though.\nIf the access pattern is unpredictable or random the situa-\ntion is quite different. Figure 3.15 compares the per-list-\nelement times for the sequential access (same as in Fig-\nure 3.10) with the times when the list elements are ran-\ndomly distributed in the working set. The order is deter-\nmined by the linked list which is randomized. There is no\nway for the processor to reliably prefetch data. This can\nonly work by chance if elements which are used shortly\nafter one another are also close to each other in memory.\nThere are two important points to note in Figure 3.15.\nThe ﬁrst is the large number of cycles needed for grow-\ning working set sizes. The machine makes it possible\nto access the main memory in 200-300 cycles but here\nwe reach 450 cycles and more. We have seen this phe-\nnomenon before (compare Figure 3.11). The automatic\nprefetching is actually working to a disadvantage here.\nThe second interesting point is that the curve is not ﬂat-\ntening at various plateaus as it has been for the sequen-\ntial access cases. The curve keeps on rising. To explain\nthis we can measure the L2 access of the program for\nthe various working set sizes. The result can be seen in\nFigure 3.16 and Table 3.2.\nThe ﬁgure shows that, when the working set size is larger\nthan the L2 size, the cache miss ratio (L2 accesses / L2\nmisses) starts to grow. The curve has a similar form to\nthe one in Figure 3.15: it rises quickly, declines slightly,\nand starts to rise again. There is a strong correlation with\nthe cycles per list element graph. The L2 miss rate will\ngrow until it eventually reaches close to 100%. Given a\nlarge enough working set (and RAM) the probability that\nany of the randomly picked cache lines is in L2 or is in\nthe process of being loaded can be reduced arbitrarily.\nUlrich Drepper Version 1.0 23\nSequential Random\nSet Ratio Ł2 Accesses Ratio L2 Accesses\nSize L2 Hit L2 Miss #Iter Miss/Hit per Iteration L2 Hit L2 Miss #Iter Miss/Hit per Iteration\n22088,636 843 16,384 0.94% 5.5 30,462 4721 1,024 13.42% 34.4\n22188,105 1,584 8,192 1.77% 10.9 21,817 15,151 512 40.98% 72.2\n22288,106 1,600 4,096 1.78% 21.9 22,258 22,285 256 50.03% 174.0\n22388,104 1,614 2,048 1.80% 43.8 27,521 26,274 128 48.84% 420.3\n22488,114 1,655 1,024 1.84% 87.7 33,166 29,115 64 46.75% 973.1\n22588,112 1,730 512 1.93% 175.5 39,858 32,360 32 44.81% 2,256.8\n22688,112 1,906 256 2.12% 351.6 48,539 38,151 16 44.01% 5,418.1\n22788,114 2,244 128 2.48% 705.9 62,423 52,049 8 45.47% 14,309.0\n22888,120 2,939 64 3.23% 1,422.8 81,906 87,167 4 51.56% 42,268.3\n22988,137 4,318 32 4.67% 2,889.2 119,079 163,398 2 57.84% 141,238.5\nTable 3.2: L2 Hits and Misses for Sequential and Random Walks, NPAD=0\nThe increasing cache miss rate alone explains some of\nthe costs. But there is another factor. Looking at Ta-\nble 3.2 we can see in the L2/#Iter columns that the total\nnumber of L2 uses per iteration of the program is grow-\ning. Each working set is twice as large as the one be-\nfore. So, without caching we would expect double the\nmain memory accesses. With caches and (almost) per-\nfect predictability we see the modest increase in the L2\nuse shown in the data for sequential access. The increase\nis due to the increase of the working set size and nothing\nelse.\nFor random access the per-element access time more than\ndoubles for each doubling of the working set size. This\nmeans the average access time per list element increases\nsince the working set size only doubles. The reason be-\nhind this is a rising rate of TLB misses. In Figure 3.17 we\nsee the cost for random accesses for NPAD =7. Only this\ntime the randomization is modiﬁed. While in the normal\ncase the entire list of randomized as one block (indicated\nby the label1) the other 11 curves show randomizations\nwhich are performed in smaller blocks. For the curve\nlabeled ‘60’ each set of 60 pages (245.760 bytes) is ran-\ndomized individually. That means all list elements in the\nblock are traversed before going over to an element in\nthe next block. This has the effect that number of TLB\nentries which are used at any one time is limited.\nThe element size for NPAD =7 is 64 bytes, which corre-\nsponds to the cache line size. Due to the randomized or-\nder of the list elements it is unlikely that the hardware\nprefetcher has any effect, most certainly not for more\nthan a handful of elements. This means the L2 cache\nmiss rate does not differ signiﬁcantly from the random-\nization of the entire list in one block. The performance\nof the test with increasing block size approaches asymp-\ntotically the curve for the one-block randomization. This\nmeans the performance of this latter test case is signiﬁ-\ncantly inﬂuenced by the TLB misses. If the TLB misses\ncan be lowered the performance increases signiﬁcantly0%10%20%30%40%50%60%\n210213216219222225228\nWorking Set Size (Bytes)L2Misses\nSequential Random\nFigure 3.16: L2d Miss Ratio\n050100150200250300350400450500550\n210213216219222225228\nWorking Set Size (Bytes)Cycles/List Element\n60 120 240 480 960 1920 3840 7680\n15360 30720 614401\nFigure 3.17: Page-Wise Randomization, NPAD=7\n24 Version 1.0 What Every Programmer Should Know About Memory",21445
13-3.3.3 Write Behavior.pdf,13-3.3.3 Write Behavior,,0
14-3.3.4 Multi-Processor Support.pdf,14-3.3.4 Multi-Processor Support,"(in one test we will see later up to 38%).\n3.3.3 Write Behavior\nBefore we start looking at the cache behavior when mul-\ntiple execution contexts (threads or processes) use the\nsame memory we have to explore a detail of cache im-\nplementations. Caches are supposed to be coherent and\nthis coherency is supposed to be completely transparent\nfor the userlevel code. Kernel code is a different story; it\noccasionally requires cache ﬂushes.\nThis speciﬁcally means that, if a cache line is modiﬁed,\nthe result for the system after this point in time is the\nsame as if there were no cache at all and the main mem-\nory location itself had been modiﬁed. This can be imple-\nmented in two ways or policies:\n• write-through cache implementation;\n• write-back cache implementation.\nThe write-through cache is the simplest way to imple-\nment cache coherency. If the cache line is written to,\nthe processor immediately also writes the cache line into\nmain memory. This ensures that, at all times, the main\nmemory and cache are in sync. The cache content could\nsimply be discarded whenever a cache line is replaced.\nThis cache policy is simple but not very fast. A pro-\ngram which, for instance, modiﬁes a local variable over\nand over again would create a lot of trafﬁc on the FSB\neven though the data is likely not used anywhere else and\nmight be short-lived.\nThe write-back policy is more sophisticated. Here the\nprocessor does not immediately write the modiﬁed cache\nline back to main memory. Instead, the cache line is only\nmarked as dirty. When the cache line is dropped from the\ncache at some point in the future the dirty bit will instruct\nthe processor to write the data back at that time instead\nof just discarding the content.\nWrite-back caches have the chance to be signiﬁcantly\nbetter performing, which is why most memory in a sys-\ntem with a decent processor is cached this way. The pro-\ncessor can even take advantage of free capacity on the\nFSB to store the content of a cache line before the line\nhas to be evacuated. This allows the dirty bit to be cleared\nand the processor can just drop the cache line when the\nroom in the cache is needed.\nBut there is a signiﬁcant problem with the write-back im-\nplementation. When more than one processor (or core or\nhyper-thread) is available and accessing the same mem-\nory it must still be assured that both processors see the\nsame memory content at all times. If a cache line is dirty\non one processor (i.e., it has not been written back yet)\nand a second processor tries to read the same memory lo-\ncation, the read operation cannot just go out to the main\nmemory. Instead the content of the ﬁrst processor’s cacheline is needed. In the next section we will see how this is\ncurrently implemented.\nBefore we get to this there are two more cache policies\nto mention:\n• write-combining; and\n• uncacheable.\nBoth these policies are used for special regions of the\naddress space which are not backed by real RAM. The\nkernel sets up these policies for the address ranges (on\nx86 processors using the Memory Type Range Regis-\nters, MTRRs) and the rest happens automatically. The\nMTRRs are also usable to select between write-through\nand write-back policies.\nWrite-combining is a limited caching optimization more\noften used for RAM on devices such as graphics cards.\nSince the transfer costs to the devices are much higher\nthan the local RAM access it is even more important\nto avoid doing too many transfers. Transferring an en-\ntire cache line just because a word in the line has been\nwritten is wasteful if the next operation modiﬁes the next\nword. One can easily imagine that this is a common oc-\ncurrence, the memory for horizontal neighboring pixels\non a screen are in most cases neighbors, too. As the name\nsuggests, write-combining combines multiple write ac-\ncesses before the cache line is written out. In ideal cases\nthe entire cache line is modiﬁed word by word and, only\nafter the last word is written, the cache line is written to\nthe device. This can speed up access to RAM on devices\nsigniﬁcantly.\nFinally there is uncacheable memory. This usually means\nthe memory location is not backed by RAM at all. It\nmight be a special address which is hardcoded to have\nsome functionality implemented outside the CPU. For\ncommodity hardware this most often is the case for mem-\nory mapped address ranges which translate to accesses\nto cards and devices attached to a bus (PCIe etc). On\nembedded boards one sometimes ﬁnds such a memory\naddress which can be used to turn an LED on and off.\nCaching such an address would obviously be a bad idea.\nLEDs in this context are used for debugging or status re-\nports and one wants to see this as soon as possible. The\nmemory on PCIe cards can change without the CPU’s\ninteraction, so this memory should not be cached.\n3.3.4 Multi-Processor Support\nIn the previous section we have already pointed out the\nproblem we have when multiple processors come into\nplay. Even multi-core processors have the problem for\nthose cache levels which are not shared (at least the L1d).\nIt is completely impractical to provide direct access from\none processor to the cache of another processor. The con-\nnection is simply not fast enough, for a start. The prac-\ntical alternative is to transfer the cache content over to\nUlrich Drepper Version 1.0 25\nthe other processor in case it is needed. Note that this\nalso applies to caches which are not shared on the same\nprocessor.\nThe question now is when does this cache line transfer\nhave to happen? This question is pretty easy to answer:\nwhen one processor needs a cache line which is dirty in\nanother processor’s cache for reading or writing. But\nhow can a processor determine whether a cache line is\ndirty in another processor’s cache? Assuming it just be-\ncause a cache line is loaded by another processor would\nbe suboptimal (at best). Usually the majority of mem-\nory accesses are read accesses and the resulting cache\nlines are not dirty. Processor operations on cache lines\nare frequent (of course, why else would we have this\npaper?) which means broadcasting information about\nchanged cache lines after each write access would be im-\npractical.\nWhat developed over the years is the MESI cache co-\nherency protocol (Modiﬁed, Exclusive, Shared, Invalid).\nThe protocol is named after the four states a cache line\ncan be in when using the MESI protocol:\nModiﬁed: The local processor has modiﬁed the cache\nline. This also implies it is the only copy in any\ncache.\nExclusive: The cache line is not modiﬁed but known to\nnot be loaded into any other processor’s cache.\nShared: The cache line is not modiﬁed and might exist\nin another processor’s cache.\nInvalid: The cache line is invalid, i.e., unused.\nThis protocol developed over the years from simpler ver-\nsions which were less complicated but also less efﬁcient.\nWith these four states it is possible to efﬁciently imple-\nment write-back caches while also supporting concurrent\nuse of read-only data on different processors.\nSM\nIE\nlocal read\nlocal write\nremote read\nremote write\nFigure 3.18: MESI Protocol Transitions\nThe state changes are accomplished without too much\neffort by the processors listening, or snooping, on the\nother processors’ work. Certain operations a processor\nperforms are announced on external pins and thus makethe processor’s cache handling visible to the outside. The\naddress of the cache line in question is visible on the ad-\ndress bus. In the following description of the states and\ntheir transitions (shown in Figure 3.18) we will point out\nwhen the bus is involved.\nInitially all cache lines are empty and hence also Invalid.\nIf data is loaded into the cache for writing the cache\nchanges to Modiﬁed. If the data is loaded for reading\nthe new state depends on whether another processor has\nthe cache line loaded as well. If this is the case then the\nnew state is Shared, otherwise Exclusive.\nIf a Modiﬁed cache line is read from or written to on\nthe local processor, the instruction can use the current\ncache content and the state does not change. If a sec-\nond processor wants to read from the cache line the ﬁrst\nprocessor has to send the content of its cache to the sec-\nond processor and then it can change the state to Shared.\nThe data sent to the second processor is also received\nand processed by the memory controller which stores the\ncontent in memory. If this did not happen the cache line\ncould not be marked as Shared. If the second processor\nwants to write to the cache line the ﬁrst processor sends\nthe cache line content and marks the cache line locally\nas Invalid. This is the infamous “Request For Owner-\nship” (RFO) operation. Performing this operation in the\nlast level cache, just like the I !M transition is compara-\ntively expensive. For write-through caches we also have\nto add the time it takes to write the new cache line con-\ntent to the next higher-level cache or the main memory,\nfurther increasing the cost.\nIf a cache line is in the Shared state and the local pro-\ncessor reads from it no state change is necessary and the\nread request can be fulﬁlled from the cache. If the cache\nline is locally written to the cache line can be used as well\nbut the state changes to Modiﬁed. It also requires that all\nother possible copies of the cache line in other proces-\nsors are marked as Invalid. Therefore the write operation\nhas to be announced to the other processors via an RFO\nmessage. If the cache line is requested for reading by a\nsecond processor nothing has to happen. The main mem-\nory contains the current data and the local state is already\nShared. In case a second processor wants to write to the\ncache line (RFO) the cache line is simply marked Invalid.\nNo bus operation is needed.\nThe Exclusive state is mostly identical to the Shared state\nwith one crucial difference: a local write operation does\nnothave to be announced on the bus. The local cache\nis known to be the only one holding this speciﬁc cache\nline. This can be a huge advantage so the processor will\ntry to keep as many cache lines as possible in the Exclu-\nsive state instead of the Shared state. The latter is the\nfallback in case the information is not available at that\nmoment. The Exclusive state can also be left out com-\npletely without causing functional problems. It is only\nthe performance that will suffer since the E !M transi-\ntion is much faster than the S !M transition.\n26 Version 1.0 What Every Programmer Should Know About Memory\nFrom this description of the state transitions it should be\nclear where the costs speciﬁc to multi-processor opera-\ntions are. Yes, ﬁlling caches is still expensive but now\nwe also have to look out for RFO messages. Whenever\nsuch a message has to be sent things are going to be slow.\nThere are two situations when RFO messages are neces-\nsary:\n• A thread is migrated from one processor to another\nand all the cache lines have to be moved over to the\nnew processor once.\n• A cache line is truly needed in two different pro-\ncessors.21\nIn multi-thread or multi-process programs there is always\nsome need for synchronization; this synchronization is\nimplemented using memory. So there are some valid\nRFO messages. They still have to be kept as infrequent\nas possible. There are other sources of RFO messages,\nthough. In section 6 we will explain these scenarios. The\nCache coherency protocol messages must be distributed\namong the processors of the system. A MESI transition\ncannot happen until it is clear that all the processors in\nthe system have had a chance to reply to the message.\nThat means that the longest possible time a reply can\ntake determines the speed of the coherency protocol.22\nCollisions on the bus are possible, latency can be high in\nNUMA systems, and of course sheer trafﬁc volume can\nslow things down. All good reasons to focus on avoiding\nunnecessary trafﬁc.\nThere is one more problem related to having more than\none processor in play. The effects are highly machine\nspeciﬁc but in principle the problem always exists: the\nFSB is a shared resource. In most machines all proces-\nsors are connected via one single bus to the memory con-\ntroller (see Figure 2.1). If a single processor can saturate\nthe bus (as is usually the case) then two or four processors\nsharing the same bus will restrict the bandwidth available\nto each processor even more.\nEven if each processor has its own bus to the memory\ncontroller as in Figure 2.2 there is still the bus to the\nmemory modules. Usually this is one bus but, even in\nthe extended model in Figure 2.2, concurrent accesses to\nthe same memory module will limit the bandwidth.\nThe same is true with the AMD model where each pro-\ncessor can have local memory. All processors can indeed\nconcurrently access their local memory quickly, espe-\ncially with the integrated memory controller. But multi-\nthread and multi-process programs–at least from time to\n21At a smaller level the same is true for two cores on the same pro-\ncessor. The costs are just a bit smaller. The RFO message is likely to\nbe sent many times.\n22Which is why we see nowadays, for instance, AMD Opteron sys-\ntems with three sockets. Each processor is exactly one hop away given\nthat the processors only have three hyperlinks and one is needed for the\nNorthbridge connection.time–have to access the same memory regions to syn-\nchronize.\nConcurrency is severely limited by the ﬁnite bandwidth\navailable for the implementation of the necessary syn-\nchronization. Programs need to be carefully designed to\nminimize accesses from different processors and cores\nto the same memory locations. The following measure-\nments will show this and the other cache effects related\nto multi-threaded code.\nMulti Threaded Access To ensure that the gravity of\nthe problems introduced by concurrently using the same\ncache lines on different processors is understood, we will\nlook here at some more performance graphs for the same\nprogram we used before. This time, though, more than\none thread is running at the same time. What is measured\nis the fastest runtime of any of the threads. This means\nthe time for a complete run when all threads are done is\neven higher. The machine used has four processors; the\ntests use up to four threads. All processors share one bus\nto the memory controller and there is only one bus to the\nmemory modules.\nFigure 3.19 shows the performance for sequential read-\nonly access for 128 bytes entries ( NPAD =15 on 64-bit ma-\nchines). For the curve for one thread we can expect a\ncurve similar to Figure 3.11. The measurements are for a\ndifferent machine so the actual numbers vary.\nThe important part in this ﬁgure is of course the behavior\nwhen running multiple threads. Note that no memory is\nmodiﬁed and no attempts are made to keep the threads\nin sync when walking the linked list. Even though no\nRFO messages are necessary and all the cache lines can\nbe shared, we see up to an 18% performance decrease\nfor the fastest thread when two threads are used and up\nto 34% when four threads are used. Since no cache lines\nhave to be transported between the processors this slow-\ndown is solely caused by the one or both of the two bot-\ntlenecks: the shared bus from the processor to the mem-\nory controller and bus from the memory controller to the\nmemory modules. Once the working set size is larger\nthan the L3 cache in this machine all three threads will\nbe prefetching new list elements. Even with two threads\nthe available bandwidth is not sufﬁcient to scale linearly\n(i.e., have no penalty from running multiple threads).\nWhen we modify memory things get even uglier. Fig-\nure 3.20 shows the results for the sequential Increment\ntest. This graph is using a logarithmic scale for the Y\naxis. So, do not be fooled by the apparently small dif-\nferences. We still have about a 18% penalty for run-\nning two threads and now an amazing 93% penalty for\nrunning four threads. This means the prefetch trafﬁc to-\ngether with the write-back trafﬁc is pretty much saturat-\ning the bus when four threads are used.\nWe use the logarithmic scale to show the results for the\nL1d range. What can be seen is that, as soon as more\nUlrich Drepper Version 1.0 27\n050100150200250300350400450500\n210213216219222225228\nWorking Set Size (Bytes)Cycles/List Element\n#Threads=1 #Threads=2 #Threads=4\nFigure 3.19: Sequential Read Access, Multiple Threads\n1251020501002005001000\n210213216219222225228\nWorking Set Size (Bytes)Cycles/List Element\n#Threads=1 #Threads=2 #Threads=4\nFigure 3.20: Sequential Increment, Multiple Threads\n02004006008001000120014001600\n210213216219222225228\nWorking Set Size (Bytes)Cycles/List Element\n#Threads=1 #Threads=2 #Threads=4\nFigure 3.21: Random Addnextlast, Multiple Threadsthan one thread is running, the L1d is basically ineffec-\ntive. The single-thread access times exceed 20 cycles\nonly when the L1d is not sufﬁcient to hold the work-\ning set. When multiple threads are running, those access\ntimes are hit immediately, even with the smallest work-\ning set sizes.\nOne aspect of the problem is not shown here. It is hard to\nmeasure with this speciﬁc test program. Even though the\ntest modiﬁes memory and we therefore must expect RFO\nmessages we do not see higher costs for the L2 range\nwhen more than one thread is used. The program would\nhave to use a large amount of memory and all threads\nmust access the same memory in parallel. This is hard\nto achieve without a lot of synchronization which would\nthen dominate the execution time.\nFinally in Figure 3.21 we have the numbers for the Add-\nnextlast test with random access of memory. This ﬁgure\nis provided mainly to show the appallingly high numbers.\nIt now takes around 1,500 cycles to process a single list\nelement in the extreme case. The use of more threads\nis even more questionable. We can summarize the efﬁ-\nciency of multiple thread use in a table.\n#Threads Seq Read Seq Inc Rand Add\n2 1.69 1.69 1.54\n4 2.98 2.07 1.65\nTable 3.3: Efﬁciency for Multiple Threads\nThe table shows the efﬁciency for the multi-thread run\nwith the largest working set size in the three ﬁgures on\npage 28. The number shows the best possible speed-up\nthe test program incurs for the largest working set size by\nusing two or four threads. For two threads the theoretical\nlimits for the speed-up are 2 and, for four threads, 4. The\nnumbers for two threads are not that bad. But for four\nthreads the numbers for the last test show that it is almost\nnot worth it to scale beyond two threads. The additional\nbeneﬁt is minuscule. We can see this more easily if we\nrepresent the data in Figure 3.21 a bit differently.\nThe curves in Figure 3.22 show the speed-up factors, i.e.,\nrelative performance compared to the code executed by\na single thread. We have to ignore the smallest sizes, the\nmeasurements are not accurate enough. For the range of\nthe L2 and L3 cache we can see that we indeed achieve\nalmost linear acceleration. We almost reach factors of\n2 and 4 respectively. But as soon as the L3 cache is\nnot sufﬁcient to hold the working set the numbers crash.\nThey crash to the point that the speed-up of two and four\nthreads is identical (see the fourth column in Table 3.3).\nThis is one of the reasons why one can hardly ﬁnd moth-\nerboard with sockets for more than four CPUs all using\nthe same memory controller. Machines with more pro-\ncessors have to be built differently (see section 5).\nThese numbers are not universal. In some cases even\nworking sets which ﬁt into the last level cache will not\n28 Version 1.0 What Every Programmer Should Know About Memory\n00.511.522.533.544.5\n210213216219222225228\nWorking Set Size (Bytes)Speed-Up\n#Threads=1 #Threads=2 #Threads=4\nFigure 3.22: Speed-Up Through Parallelism\nallow linear speed-ups. In fact, this is the norm since\nthreads are usually not as decoupled as is the case in this\ntest program. On the other hand it is possible to work\nwith large working sets and still take advantage of more\nthan two threads. Doing this requires thought, though.\nWe will talk about some approaches in section 6.\nSpecial Case: Hyper-Threads Hyper-Threads (some-\ntimes called Symmetric Multi-Threading, SMT) are im-\nplemented by the CPU and are a special case since the\nindividual threads cannot really run concurrently. They\nall share almost all the processing resources except for\nthe register set. Individual cores and CPUs still work\nin parallel but the threads implemented on each core are\nlimited by this restriction. In theory there can be many\nthreads per core but, so far, Intel’s CPUs at most have\ntwo threads per core. The CPU is responsible for time-\nmultiplexing the threads. This alone would not make\nmuch sense, though. The real advantage is that the CPU\ncan schedule another hyper-thread and take advantage of\navailable resources such as arithmetic logic units (ALUs)\nwhen the currently running hyper-thread is delayed. In\nmost cases this is a delay caused by memory accesses.\nIf two threads are running on one hyper-threaded core the\nprogram is only more efﬁcient than the single-threaded\ncode if the combined runtime of both threads is lower\nthan the runtime of the single-threaded code. This is pos-\nsible by overlapping the wait times for different memory\naccesses which usually would happen sequentially. A\nsimple calculation shows the minimum requirement on\nthe cache hit rate to achieve a certain speed-up.\nThe execution time for a program can be approximated\nwith a simple model with only one level of cache as fol-\nlows (see [16]):\nTexe =N\n(1 Fmem )Tproc\n+Fmem (GhitTcache+ (1 Ghit)Tmiss)The meaning of the variables is as follows:\nN=Number of instructions.\nFmem =Fraction ofNthat access memory.\nGhit=Fraction of loads that hit the cache.\nTproc =Number of cycles per instruction.\nTcache=Number of cycles for cache hit.\nTmiss=Number of cycles for cache miss.\nTexe =Execution time for program.\nFor it to make any sense to use two threads the execution\ntime of each of the two threads must be at most half of\nthat of the single-threaded code. The only variable on\neither side is the number of cache hits. If we solve the\nequation for the minimum cache hit rate required to not\nslow down the thread execution by 50% or more we get\nthe graph in Figure 3.23.\n60% 70% 80% 90% 100%0%10%20%30%40%50%60%70%80%90%100%\nFigure 3.23: Minimum Cache Hit Rate For Speed-Up\nThe input, measured on the X–axis, is the cache hit rate\nGhitof the single-thread code. The Y–axis shows the\ncache hit rate for the multi-threaded code. This value can\nnever be higher than the single-threaded hit rate since,\notherwise, the single-threaded code would use that im-\nproved code, too. For single-threaded hit rates–in this\nspeciﬁc case–below 55% the program can in all cases\nbeneﬁt from using threads. The CPU is more or less idle\nenough due to cache misses to enable running a second\nhyper-thread.\nThe green area is the target. If the slowdown for the\nthread is less than 50% and the workload of each thread\nis halved the combined runtime might be less than the\nsingle-thread runtime. For the modeled processor (num-\nbers for a P4 with hyper-threads were used) a program\nUlrich Drepper Version 1.0 29",23508
15-3.4 Instruction Cache.pdf,15-3.4 Instruction Cache,"with a hit rate of 60% for the single-threaded code re-\nquires a hit rate of at least 10% for the dual-threaded pro-\ngram. That is usually doable. But if the single-threaded\ncode has a hit rate of 95% then the multi-threaded code\nneeds a hit rate of at least 80%. That is harder. Espe-\ncially, and this is the problem with hyper-threads, be-\ncause now the effective cache size (L1d here, in practice\nalso L2 and so on) available to each hyper-thread is cut\nin half. Both hyper-threads use the same cache to load\ntheir data. If the working set of the two threads is non-\noverlapping the original 95% hit rate could also be cut in\nhalf and is therefore much lower than the required 80%.\nHyper-threads are therefore only useful in a limited range\nof situations. The cache hit rate of the single-threaded\ncode must be low enough that given the equations above\nand reduced cache size the new hit rate still meets the\ngoal. Then and only then can it make any sense at all to\nuse hyper-threads. Whether the result is faster in prac-\ntice depends on whether the processor is sufﬁciently able\nto overlap the wait times in one thread with execution\ntimes in the other threads. The overhead of parallelizing\nthe code must be added to the new total runtime and this\nadditional cost often cannot be neglected.\nIn section 6.3.4 we will see a technique where threads\ncollaborate closely and the tight coupling through the\ncommon cache is actually an advantage. This technique\ncan be applicable to many situations if only the program-\nmers are willing to put in the time and energy to extend\ntheir code.\nWhat should be clear is that if the two hyper-threads ex-\necute completely different code (i.e., the two threads are\ntreated like separate processors by the OS to execute sep-\narate processes) the cache size is indeed cut in half which\nmeans a signiﬁcant increase in cache misses. Such OS\nscheduling practices are questionable unless the caches\nare sufﬁciently large. Unless the workload for the ma-\nchine consists of processes which, through their design,\ncan indeed beneﬁt from hyper-threads it might be best to\nturn off hyper-threads in the computer’s BIOS.23\n3.3.5 Other Details\nSo far we talked about the address as consisting of three\nparts, tag, set index, and cache line offset. But what ad-\ndress is actually used? All relevant processors today pro-\nvide virtual address spaces to processes, which means\nthat there are two different kinds of addresses: virtual\nand physical.\nThe problem with virtual addresses is that they are not\nunique. A virtual address can, over time, refer to dif-\nferent physical memory addresses. The same address in\ndifferent processes also likely refers to different physi-\ncal addresses. So it is always better to use the physical\nmemory address, right?\n23Another reason to keep hyper-threads enabled is debugging. SMT\nis astonishingly good at ﬁnding some sets of problems in parallel code.The problem here are the virtual addresses used during\nexecution which must to be translated with the help of\nthe Memory Management Unit (MMU) into physical ad-\ndresses. This is a non-trivial operation. In the pipeline to\nexecute an instruction the physical address might only be\navailable at a later stage. This means that the cache logic\nhas to be very quick in determining whether the memory\nlocation is cached. If virtual addresses could be used the\ncache lookup can happen much earlier in the pipeline and\nin case of a cache hit the memory content can be made\navailable. The result is that more of the memory access\ncosts could be hidden by the pipeline.\nProcessor designers are currently using virtual address\ntagging for the ﬁrst level caches. These caches are rather\nsmall and can be cleared without too much pain. At\nleast partial clearing the cache is necessary if the page\ntable tree of a process changes. It might be possible to\navoid a complete ﬂush if the processor has an instruc-\ntion which allows to specify the virtual address range\nwhich has changed. Given the low latency of L1i and\nL1d caches (3cycles) using virtual addresses is almost\nmandatory.\nFor larger caches including L2, L3, . . . caches physical\naddress tagging is needed. These caches have a higher\nlatency and the virtual !physical address translation can\nﬁnish in time. Because these caches are larger (i.e., a lot\nof information is lost when they are ﬂushed) and reﬁlling\nthem takes a long time due to the main memory access\nlatency, ﬂushing them often would be costly.\nIt should, in general, not be necessary to know about the\ndetails of the address handling in those caches. They can-\nnot be changed and all the factors which would inﬂuence\nthe performance are normally something which should\nbe avoided or is associated with high cost. Overﬂowing\nthe cache capacity is bad and all caches run into prob-\nlems early if the majority of the used cache lines fall into\nthe same set. The latter can be avoided with virtually ad-\ndressed caches but is impossible for user-level processes\nto avoid for caches addressed using physical addresses.\nThe only detail one might want to keep in mind is to not\nmap the same physical memory location to two or more\nvirtual addresses in the same process, if at all possible.\nAnother detail of the caches which is rather uninterest-\ning to programmers is the cache replacement strategy.\nMost caches evict the Least Recently Used (LRU) ele-\nment ﬁrst. This is always a good default strategy. With\nlarger associativity (and associativity might indeed grow\nfurther in the coming years due to the addition of more\ncores) maintaining the LRU list becomes more and more\nexpensive and we might see different strategies adopted.\nAs for the cache replacement there is not much a pro-\ngrammer can do. If the cache is using physical address\ntags there is no way to ﬁnd out how the virtual addresses\ncorrelate with the cache sets. It might be that cache lines\nin all logical pages are mapped to the same cache sets,\n30 Version 1.0 What Every Programmer Should Know About Memory",6106
16-3.5 Cache Miss Factors.pdf,16-3.5 Cache Miss Factors,"leaving much of the cache unused. If anything, it is the\njob of the OS to arrange that this does not happen too\noften.\nWith the advent of virtualization things get even more\ncomplicated. Now not even the OS has control over the\nassignment of physical memory. The Virtual Machine\nMonitor (VMM, aka Hypervisor) is responsible for the\nphysical memory assignment.\nThe best a programmer can do is to a) use logical mem-\nory pages completely and b) use page sizes as large as\nmeaningful to diversify the physical addresses as much\nas possible. Larger page sizes have other beneﬁts, too,\nbut this is another topic (see section 4).\n3.4 Instruction Cache\nNot just the data used by the processor is cached; the\ninstructions executed by the processor are also cached.\nHowever, this cache is much less problematic than the\ndata cache. There are several reasons:\n• The quantity of code which is executed depends on\nthe size of the code that is needed. The size of the\ncode in general depends on the complexity of the\nproblem. The complexity of the problem is ﬁxed.\n• While the program’s data handling is designed by\nthe programmer the program’s instructions are usu-\nally generated by a compiler. The compiler writers\nknow about the rules for good code generation.\n• Program ﬂow is much more predictable than data\naccess patterns. Today’s CPUs are very good at\ndetecting patterns. This helps with prefetching.\n• Code always has quite good spatial and temporal\nlocality.\nThere are a few rules programmers should follow but\nthese mainly consist of rules on how to use the tools. We\nwill discuss them in section 6. Here we talk only about\nthe technical details of the instruction cache.\nEver since the core clock of CPUs increased dramati-\ncally and the difference in speed between cache (even\nﬁrst level cache) and core grew, CPUs have been de-\nsigned with pipelines. That means the execution of an\ninstruction happens in stages. First an instruction is de-\ncoded, then the parameters are prepared, and ﬁnally it\nis executed. Such a pipeline can be quite long ( >20\nstages for Intel’s Netburst architecture). A long pipeline\nmeans that if the pipeline stalls (i.e., the instruction ﬂow\nthrough it is interrupted) it takes a while to get up to\nspeed again. Pipeline stalls happen, for instance, if the\nlocation of the next instruction cannot be correctly pre-\ndicted or if it takes too long to load the next instruction\n(e.g., when it has to be read from memory).As a result CPU designers spend a lot of time and chip\nreal estate on branch prediction so that pipeline stalls\nhappen as infrequently as possible.\nOn CISC processors the decoding stage can also take\nsome time. The x86 and x86-64 processors are espe-\ncially affected. In recent years these processors therefore\ndo not cache the raw byte sequence of the instructions in\nL1i but instead they cache the decoded instructions. L1i\nin this case is called the “trace cache”. Trace caching\nallows the processor to skip over the ﬁrst steps of the\npipeline in case of a cache hit which is especially good if\nthe pipeline stalled.\nAs said before, the caches from L2 on are uniﬁed caches\nwhich contain both code and data. Obviously here the\ncode is cached in the byte sequence form and not de-\ncoded.\nTo achieve the best performance there are only a few\nrules related to the instruction cache:\n1. Generate code which is as small as possible. There\nare exceptions when software pipelining for the\nsake of using pipelines requires creating more code\nor where the overhead of using small code is too\nhigh.\n2. Help the processor making good prefetching de-\ncisions. This can be done through code layout or\nwith explicit prefetching.\nThese rules are usually enforced by the code generation\nof a compiler. There are a few things the programmer\ncan do and we will talk about them in section 6.\n3.4.1 Self Modifying Code\nIn early computer days memory was a premium. People\nwent to great lengths to reduce the size of the program\nto make more room for program data. One trick fre-\nquently deployed was to change the program itself over\ntime. Such Self Modifying Code (SMC) is occasionally\nstill found, these days mostly for performance reasons or\nin security exploits.\nSMC should in general be avoided. Though it is gener-\nally correctly executed there are boundary cases which\nare not and it creates performance problems if not done\ncorrectly. Obviously, code which is changed cannot be\nkept in the trace cache which contains the decoded in-\nstructions. But even if the trace cache is not used because\nthe code has not been executed at all (or for some time)\nthe processor might have problems. If an upcoming in-\nstruction is changed while it already entered the pipeline\nthe processor has to throw away a lot of work and start\nall over again. There are even situations where most of\nthe state of the processor has to be tossed away.\nFinally, since the processor assumes–for simplicity rea-\nsons and because it is true in 99.9999999% of all cases–\nUlrich Drepper Version 1.0 31",5103
17-3.5.1 Cache and Memory Bandwidth.pdf,17-3.5.1 Cache and Memory Bandwidth,"that the code pages are immutable, the L1i implementa-\ntion does not use the MESI protocol but instead a simpli-\nﬁed SI protocol. This means if modiﬁcations are detected\na lot of pessimistic assumptions have to be made.\nIt is highly advised to avoid SMC whenever possible.\nMemory is not such a scarce resource anymore. It is\nbetter to write separate functions instead of modifying\none function according to speciﬁc needs. Maybe one day\nSMC support can be made optional and we can detect\nexploit code trying to modify code this way. If SMC ab-\nsolutely has to be used, the write operations should by-\npass the cache as to not create problems with data in L1d\nneeded in L1i. See section 6.1 for more information on\nthese instructions.\nOn Linux it is normally quite easy to recognize programs\nwhich contain SMC. All program code is write-protected\nwhen built with the regular toolchain. The programmer\nhas to perform signiﬁcant magic at link time to create\nan executable where the code pages are writable. When\nthis happens, modern Intel x86 and x86-64 processors\nhave dedicated performance counters which count uses\nof self-modifying code. With the help of these counters it\nis quite easily possible to recognize programs with SMC\neven if the program will succeed due to relaxed permis-\nsions.\n3.5 Cache Miss Factors\nWe have already seen that when memory accesses miss\nthe caches the costs skyrocket. Sometimes this is not\navoidable and it is important to understand the actual\ncosts and what can be done to mitigate the problem.\n3.5.1 Cache and Memory Bandwidth\nTo get a better understanding of the capabilities of the\nprocessors we measure the bandwidth available in opti-\nmal circumstances. This measurement is especially in-\nteresting since different processor versions vary widely.\nThis is why this section is ﬁlled with the data of sev-\neral different machines. The program to measure perfor-\nmance uses the SSE instructions of the x86 and x86-64\nprocessors to load or store 16 bytes at once. The working\nset is increased from 1kB to 512MB just as in our other\ntests and it is measured how many bytes per cycle can be\nloaded or stored.\nFigure 3.24 shows the performance on a 64-bit Intel Net-\nburst processor. For working set sizes which ﬁt into L1d\nthe processor is able to read the full 16 bytes per cy-\ncle, i.e., one load instruction is performed per cycle (the\nmovaps instruction moves 16 bytes at once). The test\ndoes not do anything with the read data, we test only the\nread instructions themselves. As soon as the L1d is not\nsufﬁcient anymore the performance goes down dramati-\ncally to less than 6 bytes per cycle. The step at 218bytes\nis due to the exhaustion of the DTLB cache which means\nadditional work for each new page. Since the reading0246810121416\n210213216219222225228\nWorking Set Size (Bytes)Bytes/Cycles\nRead Write Copy\nFigure 3.24: Pentium 4 Bandwidth\n0246810121416\n210213216219222225228\nWorking Set Size (Bytes)Bytes/Cycles\nRead Write Copy\nFigure 3.25: P4 Bandwidth with 2 Hyper-Threads\n0246810121416\n210213216219222225228\nWorking Set Size (Bytes)Bytes/Cycles\nRead Write Copy\nFigure 3.26: Core 2 Bandwidth\n32 Version 1.0 What Every Programmer Should Know About Memory\nis sequential prefetching can predict the accesses per-\nfectly and the FSB can stream the memory content at\nabout 5.3 bytes per cycle for all sizes of the working set.\nThe prefetched data is not propagated into L1d, though.\nThese are of course numbers which will never be achiev-\nable in a real program. Think of them as practical limits.\nWhat is more astonishing than the read performance is\nthe write and copy performance. The write performance,\neven for small working set sizes, does not ever rise above\n4 bytes per cycle. This indicates that, in these Netburst\nprocessors, Intel elected to use a Write-Through mode\nfor L1d where the performance is obviously limited by\nthe L2 speed. This also means that the performance of\nthe copy test, which copies from one memory region into\na second, non-overlapping memory region, is not signiﬁ-\ncantly worse. The necessary read operations are so much\nfaster and can partially overlap with the write operations.\nThe most noteworthy detail of the write and copy mea-\nsurements is the low performance once the L2 cache is\nnot sufﬁcient anymore. The performance drops to 0.5\nbytes per cycle! That means write operations are by a\nfactor of ten slower than the read operations. This means\noptimizing those operations is even more important for\nthe performance of the program.\nIn Figure 3.25 we see the results on the same processor\nbut with two threads running, one pinned to each of the\ntwo hyper-threads of the processor. The graph is shown\nat the same scale as the previous one to illustrate the dif-\nferences and the curves are a bit jittery simply because of\nthe problem of measuring two concurrent threads. The\nresults are as expected. Since the hyper-threads share all\nthe resources except the registers each thread has only\nhalf the cache and bandwidth available. That means even\nthough each thread has to wait a lot and could award\nthe other thread with execution time this does not make\nany difference since the other thread also has to wait for\nthe memory. This truly shows the worst possible use of\nhyper-threads.\nCompared to Figure 3.24 and 3.25 the results in Fig-\nure 3.26 and 3.27 look quite different for an Intel Core 2\nprocessor. This is a dual-core processor with shared L2\nwhich is four times as big as the L2 on the P4 machine.\nThis only explains the delayed drop-off of the write and\ncopy performance, though.\nThere are other, bigger differences. The read perfor-\nmance throughout the working set range hovers around\nthe optimal 16 bytes per cycle. The drop-off in the read\nperformance after 220bytes is again due to the working\nset being too big for the DTLB. Achieving these high\nnumbers means the processor is not only able to prefetch\nthe data and transport the data in time. It also means the\ndata is prefetched into L1d.\nThe write and copy performance is dramatically differ-\nent, too. The processor does not have a Write-Through\npolicy; written data is stored in L1d and only evicted\nwhen necessary. This allows for write speeds close to theoptimal 16 bytes per cycle. Once L1d is not sufﬁcient\nanymore the performance drops signiﬁcantly. As with\nthe Netburst processor, the write performance is signiﬁ-\ncantly lower. Due to the high read performance the dif-\nference is even higher here. In fact, when even the L2 is\nnot sufﬁcient anymore the speed difference increases to\na factor of 20! This does not mean the Core 2 proces-\nsors perform poorly. To the contrary, their performance\nis always better than the Netburst core’s.\nIn Figure 3.27 the test runs two threads, one on each of\nthe two cores of the Core 2 processor. Both threads ac-\ncess the same memory, not necessarily perfectly in sync,\nthough. The results for the read performance are not dif-\nferent from the single-threaded case. A few more jitters\nare visible which is to be expected in any multi-threaded\ntest case.\nThe interesting point is the write and copy performance\nfor working set sizes which would ﬁt into L1d. As can be\nseen in the ﬁgure, the performance is the same as if the\ndata had to be read from the main memory. Both threads\ncompete for the same memory location and RFO mes-\nsages for the cache lines have to be sent. The problematic\npoint is that these requests are not handled at the speed\nof the L2 cache, even though both cores share the cache.\nOnce the L1d cache is not sufﬁcient anymore modiﬁed\nentries are ﬂushed from each core’s L1d into the shared\nL2. At that point the performance increases signiﬁcantly\nsince now the L1d misses are satisﬁed by the L2 cache\nand RFO messages are only needed when the data has\nnot yet been ﬂushed. This is why we see a 50% reduction\nin speed for these sizes of the working set. The asymp-\ntotic behavior is as expected: since both cores share the\nsame FSB each core gets half the FSB bandwidth which\nmeans for large working sets each thread’s performance\nis about half that of the single threaded case.\nBecause there are signiﬁcant differences even between\nthe processor versions of one vendor it is certainly worth-\nwhile looking at the performance of other vendors’ pro-\ncessors, too. Figure 3.28 shows the performance of an\nAMD family 10h Opteron processor. This processor has\n64kB L1d, 512kB L2, and 2MB of L3. The L3 cache is\nshared between all cores of the processor. The results of\nthe performance test can be seen in Figure 3.28.\nThe ﬁrst detail one notices about the numbers is that the\nprocessor is capable of handling two instructions per cy-\ncle if the L1d cache is sufﬁcient. The read performance\nexceeds 32 bytes per cycle and even the write perfor-\nmance is, with 18.7 bytes per cycle, high. The read curve\nﬂattens quickly, though, and is, with 2.3 bytes per cycle,\npretty low. The processor for this test does not prefetch\nany data, at least not efﬁciently.\nThe write curve on the other hand performs according to\nthe sizes of the various caches. The peak performance\nis achieved for the full size of the L1d, going down to 6\nbytes per cycle for L2, to 2.8 bytes per cycle for L3, and\nﬁnally .5 bytes per cycle if not even L3 can hold all the\nUlrich Drepper Version 1.0 33",9453
18-4 Virtual Memory.pdf,18-4 Virtual Memory,"0246810121416\n210213216219222225228\nWorking Set Size (Bytes)Bytes/Cycles\nRead Write Copy\nFigure 3.27: Core 2 Bandwidth with 2 Threads\n04812162024283236\n210213216219222225228\nWorking Set Size (Bytes)Bytes/Cycles\nRead Write Copy\nFigure 3.28: AMD Family 10h Opteron Bandwidth\n0481216202428\n210213216219222225228\nWorking Set Size (Bytes)Bytes/Cycles\nRead Write Copy\nFigure 3.29: AMD Fam 10h Bandwidth with 2 Threadsdata. The performance for the L1d cache exceeds that of\nthe (older) Core 2 processor, the L2 access is equally fast\n(with the Core 2 having a larger cache), and the L3 and\nmain memory access is slower.\nThe copy performance cannot be better than either the\nread or write performance. This is why we see the curve\ninitially dominated by the read performance and later by\nthe write performance.\nThe multi-thread performance of the Opteron processor\nis shown in Figure 3.29. The read performance is largely\nunaffected. Each thread’s L1d and L2 works as before\nand the L3 cache is in this case not prefetched very well\neither. The two threads do not unduly stress the L3 for\ntheir purpose. The big problem in this test is the write\nperformance. All data the threads share has to go through\nthe L3 cache. This sharing seems to be quite inefﬁcient\nsince even if the L3 cache size is sufﬁcient to hold the\nentire working set the cost is signiﬁcantly higher than an\nL3 access. Comparing this graph with Figure 3.27 we see\nthat the two threads of the Core 2 processor operate at the\nspeed of the shared L2 cache for the appropriate range of\nworking set sizes. This level of performance is achieved\nfor the Opteron processor only for a very small range of\nthe working set sizes and even here it approaches only\nthe speed of the L3 which is slower than the Core 2’s L2.\n3.5.2 Critical Word Load\nMemory is transferred from the main memory into the\ncaches in blocks which are smaller than the cache line\nsize. Today 64 bitsare transferred at once and the cache\nline size is 64 or 128 bytes . This means 8 or 16 transfers\nper cache line are needed.\nThe DRAM chips can transfer those 64-byte blocks in\nburst mode. This can ﬁll the cache line without any fur-\nther commands from the memory controller and the pos-\nsibly associated delays. If the processor prefetches cache\nlines this is probably the best way to operate.\nIf a program’s cache access of the data or instruction\ncaches misses (that means, it is a compulsory cache miss,\nbecause the data is used for the ﬁrst time, or a capacity\ncache miss, because the limited cache size requires evic-\ntion of the cache line) the situation is different. The word\ninside the cache line which is required for the program\nto continue might not be the ﬁrst word in the cache line.\nEven in burst mode and with double data rate transfer\nthe individual 64-bit blocks arrive at noticeably different\ntimes. Each block arrives 4 CPU cycles or more later\nthan the previous one. If the word the program needs to\ncontinue is the eighth of the cache line the program has to\nwait an additional 30 cycles or more after the ﬁrst word\narrives.\nThings do not necessarily have to be like this. The mem-\nory controller is free to request the words of the cache\nline in a different order. The processor can communicate\nwhich word the program is waiting on, the critical word ,\n34 Version 1.0 What Every Programmer Should Know About Memory\nand the memory controller can request this word ﬁrst.\nOnce the word arrives the program can continue while\nthe rest of the cache line arrives and the cache is not yet in\na consistent state. This technique is called Critical Word\nFirst & Early Restart.\nProcessors nowadays implement this technique but there\nare situations when that is not possible. If the processor\nprefetches data the critical word is not known. Should\nthe processor request the cache line during the time the\nprefetch operation is in ﬂight it will have to wait until the\ncritical word arrives without being able to inﬂuence the\norder.\n 1.2% 1% 0.8% 0.6% 0.4% 0.2%0%\n210213216219222225228\nWorking Set Size (Bytes)SlowdownVsCritical WordatStart\nSequential Random\nFigure 3.30: Critical Word at End of Cache Line\nEven with these optimizations in place the position of the\ncritical word on a cache line matters. Figure 3.30 shows\nthe Follow test for sequential and random access. Shown\nis the slowdown of running the test with the pointer used\nin the chase in the ﬁrst word versus the case when the\npointer is in the last word. The element size is 64 bytes,\ncorresponding the cache line size. The numbers are quite\nnoisy but it can be seen that, as soon as the L2 is not sufﬁ-\ncient to hold the working set size, the performance of the\ncase where the critical word is at the end is about 0.7%\nslower. The sequential access appears to be affected a bit\nmore. This would be consistent with the aforementioned\nproblem when prefetching the next cache line.\n3.5.3 Cache Placement\nWhere the caches are placed in relationship to the hyper-\nthreads, cores, and processors is not under control of the\nprogrammer. But programmers can determine where the\nthreads are executed and then it becomes important how\nthe caches relate to the used CPUs.\nHere we will not go into details of when to select what\ncores to run the threads. We will only describe architec-\nture details which the programmer has to take into ac-\ncount when setting the afﬁnity of the threads.Hyper-threads, by deﬁnition share everything but the reg-\nister set. This includes the L1 caches. There is not much\nmore to say here. The fun starts with the individual cores\nof a processor. Each core has at least its own L1 caches.\nAside from this there are today not many details in com-\nmon:\n• Early multi-core processors had no shared caches\nat all.\n• Later Intel models have shared L2 caches for dual-\ncore processors. For quad-core processors we have\nto deal with separate L2 caches for each pair of two\ncores. There are no higher level caches.\n• AMD’s family 10h processors have separate L2\ncaches and a uniﬁed L3 cache.\nA lot has been written in the propaganda material of the\nprocessor vendors about the advantage of their respec-\ntive models. Having no shared cache has an advantage\nif the working sets handled by the cores do not over-\nlap. This works well for single-threaded programs. Since\nthis is still often the reality today this approach does not\nperform too badly. But there is always some overlap.\nThe caches all contain the most actively used parts of\nthe common runtime libraries which means some cache\nspace is wasted.\nCompletely sharing all caches beside L1 as Intel’s dual-\ncore processors do can have a big advantage. If the work-\ning set of the threads working on the two cores over-\nlaps signiﬁcantly the total available cache memory is in-\ncreased and working sets can be bigger without perfor-\nmance degradation. If the working sets do not overlap In-\ntel’s Advanced Smart Cache management is supposed to\nprevent any one core from monopolizing the entire cache.\nIf both cores use about half the cache for their respective\nworking sets there is some friction, though. The cache\nconstantly has to weigh the two cores’ cache use and the\nevictions performed as part of this rebalancing might be\nchosen poorly. To see the problems we look at the results\nof yet another test program.\nThe test program has one process constantly reading or\nwriting, using SSE instructions, a 2MB block of mem-\nory. 2MB was chosen because this is half the size of the\nL2 cache of this Core 2 processor. The process is pinned\nto one core while a second process is pinned to the other\ncore. This second process reads and writes a memory\nregion of variable size. The graph shows the number of\nbytes per cycle which are read or written. Four different\ngraphs are shown, one for each combination of the pro-\ncesses reading and writing. The read/write graph is for\nthe background process, which always uses a 2MB work-\ning set to write, and the measured process with variable\nworking set to read.\nThe interesting part of the graph is the part between 220\nand223bytes. If the L2 cache of the two cores were com-\npletely separate we could expect that the performance of\nUlrich Drepper Version 1.0 35\n0246810121416\n210213216219222225228\nWorking Set Size (Bytes)Bytes/Cycle\nRead/Read Write/Read Read/Write Write/Write\nFigure 3.31: Bandwidth with two Processes\nall four tests would drop between 221and222bytes, that\nmeans, once the L2 cache is exhausted. As we can see\nin Figure 3.31 this is not the case. For the cases where\nthe background process is writing this is most visible.\nThe performance starts to deteriorate before the working\nset size reaches 1MB. The two processes do not share\nmemory and therefore the processes do not cause RFO\nmessages to be generated. These are pure cache eviction\nproblems. The smart cache handling has its problems\nwith the effect that the experienced cache size per core is\ncloser to 1MB than the 2MB per core which are available.\nOne can only hope that, if caches shared between cores\nremain a feature of upcoming processors, the algorithm\nused for the smart cache handling will be ﬁxed.\nHaving a quad-core processor with two L2 caches was\njust a stop-gap solution before higher-level caches could\nbe introduced. This design provides no signiﬁcant per-\nformance advantage over separate sockets and dual-core\nprocessors. The two cores communicate via the same bus\nwhich is, at the outside, visible as the FSB. There is no\nspecial fast-track data exchange.\nThe future of cache design for multi-core processors will\nlie in more layers. AMD’s 10h processor family makes\nthe start. Whether we will continue to see lower level\ncaches be shared by a subset of the cores of a proces-\nsor remains to be seen (in the 2008 generation of proces-\nsors L2 caches are not shared). The extra levels of cache\nare necessary since the high-speed and frequently used\ncaches cannot be shared among many cores. The per-\nformance would be impacted. It would also require very\nlarge caches with high associativity. Both numbers, the\ncache size and the associativity, must scale with the num-\nber of cores sharing the cache. Using a large L3 cache\nand reasonably-sized L2 caches is a reasonable trade-off.\nThe L3 cache is slower but it is ideally not as frequently\nused as the L2 cache.\nFor programmers all these different designs mean com-\nplexity when making scheduling decisions. One has toknow the workloads and the details of the machine archi-\ntecture to achieve the best performance. Fortunately we\nhave support to determine the machine architecture. The\ninterfaces will be introduced in later sections.\n3.5.4 FSB Inﬂuence\nThe FSB plays a central role in the performance of the\nmachine. Cache content can only be stored and loaded\nas quickly as the connection to the memory allows. We\ncan show how much so by running a program on two\nmachines which only differ in the speed of their memory\nmodules. Figure 3.32 shows the results of the Addnext0\ntest (adding the content of the next elements pad[0] el-\nement to the own pad[0] element) for NPAD =7 on a 64-\nbit machine. Both machines have Intel Core 2 proces-\nsors, the ﬁrst uses 667MHz DDR2 modules, the second\n800MHz modules (a 20% increase).\n0255075100125150175\n210213216219222225228\nWorking Set Size (Bytes)Cycles/List Element\nCore2/667 Core2/800\nFigure 3.32: Inﬂuence of FSB Speed\nThe numbers show that, when the FSB is really stressed\nfor large working set sizes, we indeed see a large bene-\nﬁt. The maximum performance increase measured in this\ntest is 18.2%, close to the theoretical maximum. What\nthis shows is that a faster FSB indeed can pay off big\ntime. It is not critical when the working set ﬁts into\nthe caches (and these processors have a 4MB L2). It\nmust be kept in mind that we are measuring one program\nhere. The working set of a system comprises the memory\nneeded by all concurrently running processes. This way\nit is easily possible to exceed 4MB memory or more with\nmuch smaller programs.\nToday some of Intel’s processors support FSB speeds\nup to 1,333MHz which would mean another 60% in-\ncrease. The future is going to see even higher speeds.\nIf speed is important and the working set sizes are larger,\nfast RAM and high FSB speeds are certainly worth the\nmoney. One has to be careful, though, since even though\nthe processor might support higher FSB speeds the moth-\nerboard/Northbridge might not. It is critical to check the\nspeciﬁcations.\n36 Version 1.0 What Every Programmer Should Know About Memory",12711
19-4.1 Simplest Address Translation.pdf,19-4.1 Simplest Address Translation,,0
20-4.4 Impact Of Virtualization.pdf,20-4.4 Impact Of Virtualization,"4 Virtual Memory\nThe virtual memory (VM) subsystem of a processor im-\nplements the virtual address spaces provided to each pro-\ncess. This makes each process think it is alone in the\nsystem. The list of advantages of virtual memory are de-\nscribed in detail elsewhere so they will not be repeated\nhere. Instead this section concentrates on the actual im-\nplementation details of the virtual memory subsystem\nand the associated costs.\nA virtual address space is implemented by the Memory\nManagement Unit (MMU) of the CPU. The OS has to\nﬁll out the page table data structures, but most CPUs do\nthe rest of the work themselves. This is actually a pretty\ncomplicated mechanism; the best way to understand it\nis to introduce the data structures used to describe the\nvirtual address space.\nThe input to the address translation performed by the\nMMU is a virtual address. There are usually few–if any–\nrestrictions on its value. Virtual addresses are 32-bit val-\nues on 32-bit systems, and 64-bit values on 64-bit sys-\ntems. On some systems, for instance x86 and x86-64,\nthe addresses used actually involve another level of indi-\nrection: these architectures use segments which simply\ncause an offset to be added to every logical address. We\ncan ignore this part of address generation, it is trivial and\nnot something that programmers have to care about with\nrespect to performance of memory handling.24\n4.1 Simplest Address Translation\nThe interesting part is the translation of the virtual ad-\ndress to a physical address. The MMU can remap ad-\ndresses on a page-by-page basis. Just as when addressing\ncache lines, the virtual address is split into distinct parts.\nThese parts are used to index into various tables which\nare used in the construction of the ﬁnal physical address.\nFor the simplest model we have only one level of tables.\nDirectory EntryPage DirectoryPhysical AddressPhysical PageVirtual Address\nDirectory Offset\nFigure 4.1: 1-Level Address Translation\nFigure 4.1 shows how the different parts of the virtual\naddress are used. A top part is used to select an entry\nin a Page Directory; each entry in that directory can be\n24Segment limits on x86 are performance-relevant but that is another\nstory.individually set by the OS. The page directory entry de-\ntermines the address of a physical memory page; more\nthan one entry in the page directory can point to the same\nphysical address. The complete physical address of the\nmemory cell is determined by combining the page ad-\ndress from the page directory with the low bits from the\nvirtual address. The page directory entry also contains\nsome additional information about the page such as ac-\ncess permissions.\nThe data structure for the page directory is stored in main\nmemory. The OS has to allocate contiguous physical\nmemory and store the base address of this memory re-\ngion in a special register. The appropriate bits of the\nvirtual address are then used as an index into the page\ndirectory, which is actually an array of directory entries.\nFor a concrete example, this is the layout used for 4MB\npages on x86 machines. The Offset part of the virtual\naddress is 22 bits in size, enough to address every byte in\na 4MB page. The remaining 10 bits of the virtual address\nselect one of the 1024 entries in the page directory. Each\nentry contains a 10 bit base address of a 4MB page which\nis combined with the offset to form a complete 32 bit\naddress.\n4.2 Multi-Level Page Tables\n4MB pages are not the norm, they would waste a lot of\nmemory since many operations an OS has to perform re-\nquire alignment to memory pages. With 4kB pages (the\nnorm on 32-bit machines and, still, often on 64-bit ma-\nchines), the Offset part of the virtual address is only 12\nbits in size. This leaves 20 bits as the selector of the\npage directory. A table with 220entries is not practical.\nEven if each entry would be only 4 bytes the table would\nbe 4MB in size. With each process potentially having its\nown distinct page directory much of the physical memory\nof the system would be tied up for these page directories.\nThe solution is to use multiple levels of page tables. The\nlevel then form a huge, sparse page directory; address\nspace regions which are not actually used do not require\nallocated memory. The representation is therefore much\nmore compact, making it possible to have the page tables\nfor many processes in memory without impacting perfor-\nmance too much.\nToday the most complicated page table structures com-\nprise four levels. Figure 4.2 shows the schematics of such\nan implementation. The virtual address is, in this exam-\nple, split into at least ﬁve parts. Four of these parts are\nindexes into the various directories. The level 4 directory\nis referenced using a special-purpose register in the CPU.\nThe content of the level 4 to level 2 directories is a ref-\nerence to next lower level directory. If a directory entry\nis marked empty it obviously need not point to any lower\ndirectory. This way the page table tree can be sparse and\ncompact. The entries of the level 1 directory are, just like\nin Figure 4.1, partial physical addresses, plus auxiliary\ndata like access permissions.\nUlrich Drepper Version 1.0 37\nLevel 4 Index Level 3 Index Level 2 Index Level 1 Index Offset\nLevel 4 EntryLevel 4 Directory\nLevel 3 EntryLevel 3 Directory\nLevel 2 EntryLevel 2 Directory\nLevel 1 EntryLevel 1 DirectoryPhysical AddressPhysical PageVirtual Address\nFigure 4.2: 4-Level Address Translation\nTo determine the physical address corresponding to a vir-\ntual address the processor ﬁrst determines the address of\nthe highest level directory. This address is usually stored\nin a register. Then the CPU takes the index part of the\nvirtual address corresponding to this directory and uses\nthat index to pick the appropriate entry. This entry is the\naddress of the next directory, which is indexed using the\nnext part of the virtual address. This process continues\nuntil it reaches the level 1 directory, at which point the\nvalue of the directory entry is the high part of the physi-\ncal address. The physical address is completed by adding\nthe page offset bits from the virtual address. This process\nis called page tree walking. Some processors (like x86\nand x86-64) perform this operation in hardware, others\nneed assistance from the OS.\nEach process running on the system might need its own\npage table tree. It is possible to partially share trees but\nthis is rather the exception. It is therefore good for per-\nformance and scalability if the memory needed by the\npage table trees is as small as possible. The ideal case\nfor this is to place the used memory close together in the\nvirtual address space; the actual physical addresses used\ndo not matter. A small program might get by with using\njust one directory at each of levels 2, 3, and 4 and a few\nlevel 1 directories. On x86-64 with 4kB pages and 512\nentries per directory this allows the addressing of 2MB\nwith a total of 4 directories (one for each level). 1GB of\ncontiguous memory can be addressed with one directory\nfor levels 2 to 4 and 512 directories for level 1.\nAssuming all memory can be allocated contiguously is\ntoo simplistic, though. For ﬂexibility reasons the stack\nand the heap area of a process are, in most cases, allo-\ncated at pretty much opposite ends of the address space.\nThis allows either area to grow as much as possible if\nneeded. This means that there are most likely two level 2\ndirectories needed and correspondingly more lower level\ndirectories.\nBut even this does not always match current practice. For\nsecurity reasons the various parts of an executable (code,\ndata, heap, stack, Dynamic Shared Objects (DSOs), akashared libraries) are mapped at randomized addresses [9].\nThe randomization extends to the relative position of the\nvarious parts; that implies that the various memory re-\ngions in use in a process are widespread throughout the\nvirtual address space. By applying some limits to the\nnumber of bits of the address which are randomized the\nrange can be restricted, but it certainly, in most cases, will\nnot allow a process to run with just one or two directories\nfor levels 2 and 3.\nIf performance is really much more important than se-\ncurity, randomization can be turned off. The OS will\nthen usually at least load all DSOs contiguously in vir-\ntual memory.\n4.3 Optimizing Page Table Access\nAll the data structures for the page tables are kept in the\nmain memory; this is where the OS constructs and up-\ndates the tables. Upon creation of a process or a change\nof a page table the CPU is notiﬁed. The page tables are\nused to resolve every virtual address into a physical ad-\ndress using the page table walk described above. More to\nthe point: at least one directory for each level is used in\nthe process of resolving a virtual address. This requires\nup to four memory accesses (for a single access by the\nrunning process) which is slow. It is possible to treat\nthese directory table entries as normal data and cache\nthem in L1d, L2, etc., but this would still be far too slow.\nFrom the earliest days of virtual memory, CPU designers\nhave used a different optimization. A simple computa-\ntion can show that only keeping the directory table en-\ntries in the L1d and higher cache would lead to horrible\nperformance. Each absolute address computation would\nrequire a number of L1d accesses corresponding to the\npage table depth. These accesses cannot be parallelized\nsince they depend on the previous lookup’s result. This\nalone would, on a machine with four page table levels,\nrequire at the very least 12 cycles. Add to that the proba-\nbility of an L1d miss and the result is nothing the instruc-\ntion pipeline can hide. The additional L1d accesses also\nsteal precious bandwidth to the cache.\n38 Version 1.0 What Every Programmer Should Know About Memory\nSo, instead of just caching the directory table entries,\nthe complete computation of the address of the physi-\ncal page is cached. For the same reason that code and\ndata caches work, such a cached address computation is\neffective. Since the page offset part of the virtual address\ndoes not play any part in the computation of the physi-\ncal page address, only the rest of the virtual address is\nused as the tag for the cache. Depending on the page size\nthis means hundreds or thousands of instructions or data\nobjects share the same tag and therefore same physical\naddress preﬁx.\nThe cache into which the computed values are stored is\ncalled the Translation Look-Aside Buffer (TLB). It is\nusually a small cache since it has to be extremely fast.\nModern CPUs provide multi-level TLB caches, just as\nfor the other caches; the higher-level caches are larger\nand slower. The small size of the L1TLB is often made\nup for by making the cache fully associative, with an\nLRU eviction policy. Recently, this cache has been grow-\ning in size and, in the process, was changed to be set as-\nsociative. As a result, it might not be the oldest entry\nwhich gets evicted and replaced whenever a new entry\nhas to be added.\nAs noted above, the tag used to access the TLB is a part\nof the virtual address. If the tag has a match in the cache,\nthe ﬁnal physical address is computed by adding the page\noffset from the virtual address to the cached value. This\nis a very fast process; it has to be since the physical ad-\ndress must be available for every instruction using abso-\nlute addresses and, in some cases, for L2 look-ups which\nuse the physical address as the key. If the TLB lookup\nmisses the processor has to perform a page table walk;\nthis can be quite costly.\nPrefetching code or data through software or hardware\ncould implicitly prefetch entries for the TLB if the ad-\ndress is on another page. This cannot be allowed for\nhardware prefetching because the hardware could initiate\npage table walks that are invalid. Programmers therefore\ncannot rely on hardware prefetching to prefetch TLB en-\ntries. It has to be done explicitly using prefetch instruc-\ntions. TLBs, just like data and instruction caches, can\nappear in multiple levels. Just as for the data cache, the\nTLB usually appears in two ﬂavors: an instruction TLB\n(ITLB) and a data TLB (DTLB). Higher-level TLBs such\nas the L2TLB are usually uniﬁed, as is the case with the\nother caches.\n4.3.1 Caveats Of Using A TLB\nThe TLB is a processor-core global resource. All threads\nand processes executed on the processor core use the\nsame TLB. Since the translation of virtual to physical ad-\ndresses depends on which page table tree is installed, the\nCPU cannot blindly reuse the cached entries if the page\ntable is changed. Each process has a different page ta-\nble tree (but not the threads in the same process) as does\nthe kernel and the VMM (hypervisor) if present. It isalso possible that the address space layout of a process\nchanges. There are two ways to deal with this problem:\n• The TLB is ﬂushed whenever the page table tree is\nchanged.\n• The tags for the TLB entries are extended to ad-\nditionally and uniquely identify the page table tree\nthey refer to.\nIn the ﬁrst case the TLB is ﬂushed whenever a context\nswitch is performed. Since, in most OSes, a switch from\none thread/process to another requires executing some\nkernel code, TLB ﬂushes are restricted to leaving (and\nsometimes entering) the kernel address space. On vir-\ntualized systems it also happens when the kernel has to\ncall the VMM and on the way back. If the kernel and/or\nVMM does not have to use virtual addresses, or can reuse\nthe same virtual addresses as the process or kernel which\nmade the system/VMM call (i.e., the address spaces are\noverlaid), the TLB only has to be ﬂushed if, upon leaving\nthe kernel or VMM, the processor resumes execution of\na different process or kernel.\nFlushing the TLB is effective but expensive. When exe-\ncuting a system call, for instance, the kernel code might\nbe restricted to a few thousand instructions which touch,\nperhaps, a handful of new pages (or one huge page, as\nis the case for Linux on some architectures). This work\nwould replace only as many TLB entries as pages are\ntouched. For Intel’s Core2 architecture with its 128 ITLB\nand 256 DTLB entries, a full ﬂush would mean that more\nthan 100 and 200 entries (respectively) would be ﬂushed\nunnecessarily. When the system call returns to the same\nprocess, all those ﬂushed TLB entries can be used again,\nbut they will be gone. The same is true for often-used\ncode in the kernel or VMM. On each entry into the ker-\nnel the TLB has to be ﬁlled from scratch even though\nthe page tables for the kernel and VMM usually do not\nchange and, therefore, TLB entries could, in theory, be\npreserved for a very long time. This also explains why\nthe TLB caches in today’s processors are not bigger: pro-\ngrams most likely will not run long enough to ﬁll all these\nentries.\nThis fact, of course, did not escape the CPU architects.\nOne possibility to optimize the cache ﬂushes is to indi-\nvidually invalidate TLB entries. For instance, if the ker-\nnel code and data falls into a speciﬁc address range, only\nthe pages falling into this address range have to evicted\nfrom the TLB. This only requires comparing tags and,\ntherefore, is not very expensive. This method is also use-\nful in case a part of the address space is changed, for\ninstance, through a call to munmap .\nA much better solution is to extend the tag used for the\nTLB access. If, in addition to the part of the virtual ad-\ndress, a unique identiﬁer for each page table tree (i.e., a\nprocess’s address space) is added, the TLB does not have\nto be completely ﬂushed at all. The kernel, VMM, and\nUlrich Drepper Version 1.0 39\nthe individual processes all can have unique identiﬁers.\nThe only issue with this scheme is that the number of\nbits available for the TLB tag is severely limited, while\nthe number of address spaces is not. This means some\nidentiﬁer reuse is necessary. When this happens the TLB\nhas to be partially ﬂushed (if this is possible). All en-\ntries with the reused identiﬁer must be ﬂushed but this is,\nhopefully, a much smaller set.\nThis extended TLB tagging is of advantage outside the\nrealm of virtualization when multiple processes are run-\nning on the system. If the memory use (and hence TLB\nentry use) of each of the runnable processes is limited,\nthere is a good chance the most recently used TLB entries\nfor a process are still in the TLB when it gets scheduled\nagain. But there are two additional advantages:\n1. Special address spaces, such as those used by the\nkernel and VMM, are often only entered for a short\ntime; afterward control is often returned to the ad-\ndress space which initiated the entry. Without tags,\none or two TLB ﬂushes are performed. With tags\nthe calling address space’s cached translations are\npreserved and, since the kernel and VMM address\nspace do not often change TLB entries at all, the\ntranslations from previous system calls, etc. can\nstill be used.\n2. When switching between two threads of the same\nprocess no TLB ﬂush is necessary at all. With-\nout extended TLB tags the entry into the kernel\ndestroys the ﬁrst thread’s TLB entries, though.\nSome processors have, for some time, implemented these\nextended tags. AMD introduced a 1-bit tag extension\nwith the Paciﬁca virtualization extensions. This 1-bit Ad-\ndress Space ID (ASID) is, in the context of virtualization,\nused to distinguish the VMM’s address space from that of\nthe guest domains. This allows the OS to avoid ﬂushing\nthe guest’s TLB entries every time the VMM is entered\n(for instance, to handle a page fault) or the VMM’s TLB\nentries when control returns to the guest. The architec-\nture will allow the use of more bits in the future. Other\nmainstream processors will likely follow suit and support\nthis feature.\n4.3.2 Inﬂuencing TLB Performance\nThere are a couple of factors which inﬂuence TLB per-\nformance. The ﬁrst is the size of the pages. Obviously,\nthe larger a page is, the more instructions or data objects\nwill ﬁt into it. So a larger page size reduces the overall\nnumber of address translations which are needed, mean-\ning that fewer entries in the TLB cache are needed. Most\narchitectures nowadays allow the use of multiple differ-\nent page sizes; some sizes can be used concurrently. For\ninstance, the x86/x86-64 processors have a normal page\nsize of 4kB but they can also use 4MB and 2MB pages\nrespectively. IA-64 and PowerPC allow sizes like 64kB\nas the base page size.The use of large page sizes brings some problems with\nit, though. The memory regions used for the large pages\nmust be contiguous in physical memory. If the unit size\nfor the administration of physical memory is raised to the\nsize of the virtual memory pages, the amount of wasted\nmemory will grow. All kinds of memory operations (like\nloading executables) require alignment to page bound-\naries. This means, on average, that each mapping wastes\nhalf the page size in physical memory for each mapping.\nThis waste can easily add up; it thus puts an upper limit\non the reasonable unit size for physical memory alloca-\ntion.\nIt is certainly not practical to increase the unit size to\n2MB to accommodate large pages on x86-64. This is\njust too large a size. But this in turn means that each\nlarge page has to be comprised of many smaller pages.\nAnd these small pages have to be contiguous in physical\nmemory. Allocating 2MB of contiguous physical mem-\nory with a unit page size of 4kB can be challenging. It\nrequires ﬁnding a free area with 512 contiguous pages.\nThis can be extremely difﬁcult (or impossible) after the\nsystem runs for a while and physical memory becomes\nfragmented.\nOn Linux it is therefore necessary to allocate these big\npages at system start time using the special hugetlbfs\nﬁlesystem. A ﬁxed number of physical pages are re-\nserved for exclusive use as big virtual pages. This ties\ndown resources which might not always be used. It also\nis a limited pool; increasing it normally means restart-\ning the system. Still, huge pages are the way to go in\nsituations where performance is a premium, resources\nare plenty, and cumbersome setup is not a big deterrent.\nDatabase servers are an example.\nIncreasing the minimum virtual page size (as opposed to\noptional big pages) has its problems, too. Memory map-\nping operations (loading applications, for example) must\nconform to these page sizes. No smaller mappings are\npossible. The location of the various parts of an exe-\ncutable have, for most architectures, a ﬁxed relationship.\nIf the page size is increased beyond what has been taken\ninto account when the executable or DSO was built, the\nload operation cannot be performed. It is important to\nkeep this limitation in mind. Figure 4.3 shows how the\nalignment requirements of an ELF binary can be deter-\nmined. It is encoded in the ELF program header. In\nthis example, an x86-64 binary, the value is 200000 16=\n2;097;152 = 2MB which corresponds to the maximum\npage size supported by the processor.\nThere is a second effect of using larger page sizes: the\nnumber of levels of the page table tree is reduced. Since\nthe part of the virtual address corresponding to the page\noffset increases, there are not that many bits left which\nneed to be handled through page directories. This means\nthat, in case of a TLB miss, the amount of work which\nhas to be done is reduced.\nBeyond using large page sizes, it is possible to reduce the\n40 Version 1.0 What Every Programmer Should Know About Memory\n$ eu-readelf -l /bin/ls\nProgram Headers:\nType Offset VirtAddr PhysAddr FileSiz MemSiz Flg Align\n...\nLOAD 0x000000 0x0000000000400000 0x0000000000400000 0x0132ac 0x0132ac R E 0x200000\nLOAD 0x0132b0 0x00000000006132b0 0x00000000006132b0 0x001a71 0x001a71 RW 0x200000\n...\nFigure 4.3: ELF Program Header Indicating Alignment Requirements\nnumber of TLB entries needed by moving data which is\nused at the same time to fewer pages. This is similar to\nsome optimizations for cache use we talked about above.\nOnly now the alignment required is large. Given that\nthe number of TLB entries is quite small this can be an\nimportant optimization.\n4.4 Impact Of Virtualization\nVirtualization of OS images will become more and more\nprevalent; this means another layer of memory handling\nis added to the picture. Virtualization of processes (basi-\ncally jails) or OS containers do not fall into this category\nsince only one OS is involved. Technologies like Xen or\nKVM enable–with or without help from the processor–\nthe execution of independent OS images. In these situa-\ntions there is one piece of software alone which directly\ncontrols access to the physical memory.\nXen VMMDomU Kernel DomU KernelXen I/O\nSupportXen I/O\nSupport\nDom0 Kernel\nFigure 4.4: Xen Virtualization Model\nIn the case of Xen (see Figure 4.4) the Xen VMM is\nthat piece of software. The VMM does not implement\nmany of the other hardware controls itself, though. Un-\nlike VMMs on other, earlier systems (and the ﬁrst re-\nlease of the Xen VMM) the hardware outside of memory\nand processors is controlled by the privileged Dom0 do-\nmain. Currently, this is basically the same kernel as the\nunprivileged DomU kernels and, as far as memory han-\ndling is concerned, they do not differ. Important here is\nthat the VMM hands out physical memory to the Dom0\nand DomU kernels which, themselves, then implement\nthe usual memory handling as if they were running di-\nrectly on a processor.\nTo implement the separation of the domains which is re-\nquired for the virtualization to be complete, the mem-ory handling in the Dom0 and DomU kernels does not\nhave unrestricted access to physical memory. The VMM\ndoes not hand out memory by giving out individual phys-\nical pages and letting the guest OSes handle the address-\ning; this would not provide any protection against faulty\nor rogue guest domains. Instead, the VMM creates its\nown page table tree for each guest domain and hands out\nmemory using these data structures. The good thing is\nthat access to the administrative information of the page\ntable tree can be controlled. If the code does not have\nappropriate privileges it cannot do anything.\nThis access control is exploited in the virtualization Xen\nprovides, regardless of whether para- or hardware (aka\nfull) virtualization is used. The guest domains construct\ntheir page table trees for each process in a way which is\nintentionally quite similar for para- and hardware virtu-\nalization. Whenever the guest OS modiﬁes its page ta-\nbles the VMM is invoked. The VMM then uses the up-\ndated information in the guest domain to update its own\nshadow page tables. These are the page tables which are\nactually used by the hardware. Obviously, this process\nis quite expensive: each modiﬁcation of the page table\ntree requires an invocation of the VMM. While changes\nto the memory mapping are not cheap without virtualiza-\ntion they become even more expensive now.\nThe additional costs can be really large, considering that\nthe changes from the guest OS to the VMM and back\nthemselves are already quite expensive. This is why the\nprocessors are starting to have additional functionality to\navoid the creation of shadow page tables. This is good\nnot only because of speed concerns but it also reduces\nmemory consumption by the VMM. Intel has Extended\nPage Tables (EPTs) and AMD calls it Nested Page Ta-\nbles (NPTs). Basically both technologies have the page\ntables of the guest OSes produce “host virtual addresses”\nfrom the “guest virtual address”. The host virtual ad-\ndresses must then be further translated, using the per-\ndomain EPT/NPT trees, into actual physical addresses.\nThis will allow memory handling at almost the speed of\nthe no-virtualization case since most VMM entries for\nmemory handling are removed. It also reduces the mem-\nory use of the VMM since now only one page table tree\nfor each domain (as opposed to process) has to be main-\ntained.\nThe results of the additional address translation steps are\nalso stored in the TLB. That means the TLB does not\nstore the virtual physical address but, instead, the com-\nplete result of the lookup. It was already explained that\nUlrich Drepper Version 1.0 41\nAMD’s Paciﬁca extension introduced the ASID to avoid\nTLB ﬂushes on each entry. The number of bits for the\nASID is one in the initial release of the processor exten-\nsions; this is just enough to differentiate VMM and guest\nOS. Intel has virtual processor IDs (VPIDs) which serve\nthe same purpose, only there are more of them. But the\nVPID is ﬁxed for each guest domain and therefore it can-\nnot be used to mark separate processes and avoid TLB\nﬂushes at that level, too.\nThe amount of work needed for each address space mod-\niﬁcation is one problem with virtualized OSes. There\nis another problem inherent in VMM-based virtualiza-\ntion, though: there is no way around having two layers\nof memory handling. But memory handling is hard (es-\npecially when taking complications like NUMA into ac-\ncount, see section 5). The Xen approach of using a sep-\narate VMM makes optimal (or even good) handling hard\nsince all the complications of a memory management im-\nplementation, including “trivial” things like discovery of\nmemory regions, must be duplicated in the VMM. The\nOSes have fully-ﬂedged and optimized implementations;\none really wants to avoid duplicating them.\nLinux KernelUserlevel\nProcess KVM VMMGuest Kernel\nKVM VMMGuest Kernel\nFigure 4.5: KVM Virtualization Model\nThis is why carrying the VMM/Dom0 model to its con-\nclusion is such an attractive alternative. Figure 4.5 shows\nhow the KVM Linux kernel extensions try to solve the\nproblem. There is no separate VMM running directly\non the hardware and controlling all the guests; instead, a\nnormal Linux kernel takes over this functionality. This\nmeans the complete and sophisticated memory handling\nfunctionality in the Linux kernel is used to manage the\nmemory of the system. Guest domains run alongside\nthe normal user-level processes in what the creators call\n“guest mode”. The virtualization functionality, para- or\nfull virtualization, is controlled by the KVM VMM. This\nis just another userlevel process which happens to control\na guest domain using the special KVM device the kernel\nimplements.\nThe beneﬁt of this model over the separate VMM of the\nXen model is that, even though there are still two mem-\nory handlers at work when guest OSes are used, there\nonly needs to be one implementation, that in the Linux\nkernel. It is not necessary to duplicate the same function-\nality in another piece of code like the Xen VMM. Thisleads to less work, fewer bugs, and, perhaps, less friction\nwhere the two memory handlers touch since the memory\nhandler in a Linux guest makes the same assumptions as\nthe memory handler in the outer Linux kernel which runs\non the bare hardware.\nOverall, programmers must be aware that, with virtual-\nization used, the cost of cache misses (instruction, data,\nor TLB) is even higher than without virtualization. Any\noptimization which reduces this work will pay off even\nmore in virtualized environments. Processor designers\nwill, over time, reduce the difference more and more\nthrough technologies like EPT and NPT but it will never\ncompletely go away.\n42 Version 1.0 What Every Programmer Should Know About Memory",29698
21-5 NUMA Support.pdf,21-5 NUMA Support,,0
22-5.1 NUMA Hardware.pdf,22-5.1 NUMA Hardware,,0
23-5.4 Remote Access Costs.pdf,23-5.4 Remote Access Costs,"5 NUMA Support\nIn section 2 we saw that, on some machines, the cost\nof access to speciﬁc regions of physical memory differs\ndepending on where the access originated. This type of\nhardware requires special care from the OS and the ap-\nplications. We will start with a few details of NUMA\nhardware, then we will cover some of the support the\nLinux kernel provides for NUMA.\n5.1 NUMA Hardware\nNon-uniform memory architectures are becoming more\nand more common. In the simplest form of NUMA, a\nprocessor can have local memory (see Figure 2.3) which\nis cheaper to access than memory local to other proces-\nsors. The difference in cost for this type of NUMA sys-\ntem is not high, i.e., the NUMA factor is low.\nNUMA is also–and especially–used in big machines. We\nhave described the problems of having many processors\naccess the same memory. For commodity hardware all\nprocessors would share the same Northbridge (ignoring\nthe AMD Opteron NUMA nodes for now, they have their\nown problems). This makes the Northbridge a severe\nbottleneck since allmemory trafﬁc is routed through it.\nBig machines can, of course, use custom hardware in\nplace of the Northbridge but, unless the memory chips\nused have multiple ports–i.e. they can be used from mul-\ntiple busses–there still is a bottleneck. Multiport RAM\nis complicated and expensive to build and support and,\ntherefore, it is hardly ever used.\nThe next step up in complexity is the model AMD uses\nwhere an interconnect mechanism (Hyper Transport in\nAMD’s case, technology they licensed from Digital) pro-\nvides access for processors which are not directly con-\nnected to the RAM. The size of the structures which can\nbe formed this way is limited unless one wants to in-\ncrease the diameter (i.e., the maximum distance between\nany two nodes) arbitrarily.\n12\nC= 112\n34\nC= 21 32 4576 8\nC= 3\nFigure 5.1: Hypercubes\nAn efﬁcient topology for connecting the nodes is the hy-\npercube, which limits the number of nodes to 2Cwhere\nCis the number of interconnect interfaces each node has.\nHypercubes have the smallest diameter for all systems\nwith 2nCPUs andninterconnects. Figure 5.1 shows\nthe ﬁrst three hypercubes. Each hypercube has a diam-eter ofCwhich is the absolute minimum. AMD’s ﬁrst-\ngeneration Opteron processors have three hypertransport\nlinks per processor. At least one of the processors has to\nhave a Southbridge attached to one link, meaning, cur-\nrently, that a hypercube with C= 2can be implemented\ndirectly and efﬁciently. The next generation will at some\npoint have four links, at which point C= 3 hypercubes\nwill be possible.\nThis does not mean, though, that larger accumulations\nof processors cannot be supported. There are companies\nwhich have developed crossbars allowing larger sets of\nprocessors to be used (e.g., Newisys’s Horus). But these\ncrossbars increase the NUMA factor and they stop being\neffective at a certain number of processors.\nThe next step up means connecting groups of CPUs and\nimplementing a shared memory for all of them. All such\nsystems need specialized hardware and are by no means\ncommodity systems. Such designs exist at several levels\nof complexity. A system which is still quite close to a\ncommodity machine is IBM x445 and similar machines.\nThey can be bought as ordinary 4U, 8-way machines with\nx86 and x86-64 processors. Two (at some point up to\nfour) of these machines can then be connected to work\nas a single machine with shared memory. The intercon-\nnect used introduces a signiﬁcant NUMA factor which\nthe OS, as well as applications, must take into account.\nAt the other end of the spectrum, machines like SGI’s Al-\ntix are designed speciﬁcally to be interconnected. SGI’s\nNUMAlink interconnect fabric is very fast and has low\nlatency at the same time; both properties are require-\nments for high-performance computing (HPC), speciﬁ-\ncally when Message Passing Interfaces (MPI) are used.\nThe drawback is, of course, that such sophistication and\nspecialization is very expensive. They make a reason-\nably low NUMA factor possible but with the number of\nCPUs these machines can have (several thousands) and\nthe limited capacity of the interconnects, the NUMA fac-\ntor is actually dynamic and can reach unacceptable levels\ndepending on the workload.\nMore commonly used are solutions where many com-\nmodity machines are connected using high-speed net-\nworking to form a cluster. These are no NUMA ma-\nchines, though; they do not implement a shared address\nspace and therefore do not fall into any category which is\ndiscussed here.\n5.2 OS Support for NUMA\nTo support NUMA machines, the OS has to take the dis-\ntributed nature of the memory into account. For instance,\nif a process is run on a given processor, the physical\nRAM assigned to the process’s address space should ide-\nally come from local memory. Otherwise each instruc-\ntion has to access remote memory for code and data.\nThere are special cases to be taken into account which\nare only present in NUMA machines. The text segment\nof DSOs is normally present exactly once in a machine’s\nUlrich Drepper Version 1.0 43\nphysical RAM. But if the DSO is used by processes and\nthreads on all CPUs (for instance, the basic runtime li-\nbraries like libc ) this means that all but a few proces-\nsors have to have remote accesses. The OS ideally would\n“mirror” such DSOs into each processor’s physical RAM\nand use local copies. This is an optimization, not a re-\nquirement, and generally hard to implement. It might\nnot be supported or only in a limited fashion.\nTo avoid making the situation worse, the OS should not\nmigrate a process or thread from one node to another.\nThe OS should already try to avoid migrating processes\non normal multi-processor machines because migrating\nfrom one processor to another means the cache content is\nlost. If load distribution requires migrating a process or\nthread off of a processor, the OS can usually pick an ar-\nbitrary new processor which has sufﬁcient capacity left.\nIn NUMA environments the selection of the new proces-\nsor is a bit more limited. The newly selected processor\nshould not have higher access costs to the memory the\nprocess is using than the old processor; this restricts the\nlist of targets. If there is no free processor matching that\ncriteria available, the OS has no choice but to migrate to\na processor where memory access is more expensive.\nIn this situation there are two possible ways forward.\nFirst, one can hope the situation is temporary and the\nprocess can be migrated back to a better-suited proces-\nsor. Alternatively, the OS can also migrate the process’s\nmemory to physical pages which are closer to the newly-\nused processor. This is quite an expensive operation.\nPossibly huge amounts of memory have to be copied, al-\nbeit not necessarily in one step. While this is happening\nthe process, at least brieﬂy, has to be stopped so that mod-\niﬁcations to the old pages are correctly migrated. There\nare a whole list of other requirements for page migration\nto be efﬁcient and fast. In short, the OS should avoid it\nunless it is really necessary.\nGenerally, it cannot be assumed that all processes on a\nNUMA machine use the same amount of memory such\nthat, with the distribution of processes across the pro-\ncessors, memory usage is also equally distributed. In\nfact, unless the applications running on the machines are\nvery speciﬁc (common in the HPC world, but not out-\nside) the memory use will be very unequal. Some appli-\ncations will use vast amounts of memory, others hardly\nany. This will, sooner or later, lead to problems if mem-\nory is always allocated local to the processor where the\nrequest is originated. The system will eventually run out\nof memory local to nodes running large processes.\nIn response to these severe problems, memory is, by de-\nfault, not allocated exclusively on the local node. To uti-\nlize all the system’s memory the default strategy is to\nstripe the memory. This guarantees equal use of all the\nmemory of the system. As a side effect, it becomes possi-\nble to freely migrate processes between processors since,\non average, the access cost to all the memory used does\nnot change. For small NUMA factors, striping is accept-\nable but still not optimal (see data in section 5.4).This is a pessimization which helps the system avoid se-\nvere problems and makes it more predictable under nor-\nmal operation. But it does decrease overall system per-\nformance, in some situations signiﬁcantly. This is why\nLinux allows the memory allocation rules to be selected\nby each process. A process can select a different strategy\nfor itself and its children. We will introduce the inter-\nfaces which can be used for this in section 6.\n5.3 Published Information\nThe kernel publishes, through the syspseudo ﬁle system\n(sysfs), information about the processor caches below\n/sys/devices/system/cpu/cpu */cache\nIn section 6.2.1 we will see interfaces which can be used\nto query the size of the various caches. What is important\nhere is the topology of the caches. The directories above\ncontain subdirectories (named index *) which list infor-\nmation about the various caches the CPU possesses. The\nﬁles type ,level , and shared_cpu_map are the im-\nportant ﬁles in these directories as far as the topology is\nconcerned. For an Intel Core 2 QX6700 the information\nlooks as in Table 5.1.\ntype level shared_cpu_map\nindex0 Data 1 00000001\ncpu0 index1 Instruction 1 00000001\nindex2 Uniﬁed 2 00000003\nindex0 Data 1 00000002\ncpu1 index1 Instruction 1 00000002\nindex2 Uniﬁed 2 00000003\nindex0 Data 1 00000004\ncpu2 index1 Instruction 1 00000004\nindex2 Uniﬁed 2 0000000c\nindex0 Data 1 00000008\ncpu3 index1 Instruction 1 00000008\nindex2 Uniﬁed 2 0000000c\nTable 5.1: sysfs Information for Core 2 CPU Caches\nWhat this data means is as follows:\n• Each core25has three caches: L1i, L1d, L2.\n• The L1d and L1i caches are not shared with any\nother core–each core has its own set of caches.\nThis is indicated by the bitmap in shared_cpu_-\nmaphaving only one set bit.\n• The L2 cache on cpu0 andcpu1 is shared, as is\nthe L2 on cpu2 andcpu3 .\nIf the CPU had more cache levels, there would be more\nindex *directories.\nFor a four-socket, dual-core Opteron machine the cache\ninformation looks like Table 5.2. As can be seen these\n25The knowledge that cpu0 tocpu3 are cores comes from another\nplace that will be explained shortly.\n44 Version 1.0 What Every Programmer Should Know About Memory\ntype level shared_cpu_map\nindex0 Data 1 00000001\ncpu0 index1 Instruction 1 00000001\nindex2 Uniﬁed 2 00000001\nindex0 Data 1 00000002\ncpu1 index1 Instruction 1 00000002\nindex2 Uniﬁed 2 00000002\nindex0 Data 1 00000004\ncpu2 index1 Instruction 1 00000004\nindex2 Uniﬁed 2 00000004\nindex0 Data 1 00000008\ncpu3 index1 Instruction 1 00000008\nindex2 Uniﬁed 2 00000008\nindex0 Data 1 00000010\ncpu4 index1 Instruction 1 00000010\nindex2 Uniﬁed 2 00000010\nindex0 Data 1 00000020\ncpu5 index1 Instruction 1 00000020\nindex2 Uniﬁed 2 00000020\nindex0 Data 1 00000040\ncpu6 index1 Instruction 1 00000040\nindex2 Uniﬁed 2 00000040\nindex0 Data 1 00000080\ncpu7 index1 Instruction 1 00000080\nindex2 Uniﬁed 2 00000080\nTable 5.2: sysfs Information for Opteron CPU Caches\nprocessors also have three caches: L1i, L1d, L2. None\nof the cores shares any level of cache. The interesting\npart for this system is the processor topology. Without\nthis additional information one cannot make sense of the\ncache data. The sysﬁle system exposes this information\nin the ﬁles below\n/sys/devices/system/cpu/cpu */topology\nTable 5.3 shows the interesting ﬁles in this hierarchy for\nthe SMP Opteron machine.\nphysical_ core_ thread_\npackage_id core_id siblings siblings\ncpu0 0 00000003 00000001\ncpu101 00000003 00000002\ncpu2 0 0000000c 00000004\ncpu311 0000000c 00000008\ncpu4 0 00000030 00000010\ncpu521 00000030 00000020\ncpu6 0 000000c0 00000040\ncpu731 000000c0 00000080\nTable 5.3: sysfs Information for Opteron CPU Topol-\nogy\nTaking Table 5.2 and Table 5.3 together we can see that\na) none of the CPU has hyper-threads (the thread_-\nsiblings bitmaps have one bit set),\nb) the system in fact has a total of four processors(physical_package_id 0 to 3),\nc) each processor has two cores, and\nd) none of the cores share any cache.\nThis is exactly what corresponds to earlier Opterons.\nWhat is completely missing in the data provided so far is\ninformation about the nature of NUMA on this machine.\nAny SMP Opteron machine is a NUMA machine. For\nthis data we have to look at yet another part of the sys\nﬁle system which exists on NUMA machines, namely in\nthe hierarchy below\n/sys/devices/system/node\nThis directory contains a subdirectory for every NUMA\nnode on the system. In the node-speciﬁc directories there\nare a number of ﬁles. The important ﬁles and their con-\ntent for the Opteron machine described in the previous\ntwo tables are shown in Table 5.4.\ncpumap distance\nnode0 00000003 10 20 20 20\nnode1 0000000c 20 10 20 20\nnode2 00000030 20 20 10 20\nnode3 000000c0 20 20 20 10\nTable 5.4: sysfs Information for Opteron Nodes\nThis information ties all the rest together; now we have\na complete picture of the architecture of the machine.\nWe already know that the machine has four processors.\nEach processor constitutes its own node as can be seen\nby the bits set in the value in cpumap ﬁle in the node *\ndirectories. The distance ﬁles in those directories con-\ntains a set of values, one for each node, which represent\na cost of memory accesses at the respective nodes. In\nthis example all local memory accesses have the cost 10,\nall remote access to any other node has the cost 20.26\nThis means that, even though the processors are orga-\nnized as a two-dimensional hypercube (see Figure 5.1),\naccesses between processors which are not directly con-\nnected is not more expensive. The relative values of the\ncosts should be usable as an estimate of the actual differ-\nence of the access times. The accuracy of all this infor-\nmation is another question.\n5.4 Remote Access Costs\nThe distance is relevant, though. In [1] AMD documents\nthe NUMA cost of a four socket machine. For write op-\nerations the numbers are shown in Figure 5.3. Writes\n26This is, by the way, incorrect. The ACPI information is appar-\nently wrong since, although the processors used have three coherent\nHyperTransport links, at least one processor must be connected to a\nSouthbridge. At least one pair of nodes must therefore have a larger\ndistance.\nUlrich Drepper Version 1.0 45\n00400000 default file=/bin/cat mapped=3 N3=3\n00504000 default file=/bin/cat anon=1 dirty=1 mapped=2 N3=2\n00506000 default heap anon=3 dirty=3 active=0 N3=3\n38a9000000 default file=/lib64/ld-2.4.so mapped=22 mapmax=47 N1=22\n38a9119000 default file=/lib64/ld-2.4.so anon=1 dirty=1 N3=1\n38a911a000 default file=/lib64/ld-2.4.so anon=1 dirty=1 N3=1\n38a9200000 default file=/lib64/libc-2.4.so mapped=53 mapmax=52 N1=51 N2=2\n38a933f000 default file=/lib64/libc-2.4.so\n38a943f000 default file=/lib64/libc-2.4.so anon=1 dirty=1 mapped=3 mapmax=32 N1=2 N3=1\n38a9443000 default file=/lib64/libc-2.4.so anon=1 dirty=1 N3=1\n38a9444000 default anon=4 dirty=4 active=0 N3=4\n2b2bbcdce000 default anon=1 dirty=1 N3=1\n2b2bbcde4000 default anon=2 dirty=2 N3=2\n2b2bbcde6000 default file=/usr/lib/locale/locale-archive mapped=11 mapmax=8 N0=11\n7fffedcc7000 default stack anon=2 dirty=2 N3=2\nFigure 5.2: Content of /proc/ PID/numa_maps\nare slower than reads, this is no surprise. The interest-\ning parts are the costs of the 1- and 2-hop cases. The\ntwo 1-hop cases actually have slightly different costs.\nSee [1] for the details. The fact we need to remem-\nber from this chart is that 2-hop reads and writes are\n30% and 49% (respectively) slower than 0-hop reads. 2-\nhop writes are 32% slower than 0-hop writes, and 17%\nslower than 1-hop writes. The relative position of pro-\ncessor and memory nodes can make a big difference.\nThe next generation of processors from AMD will fea-\nture four coherent HyperTransport links per processor. In\nthat case a four socket machine would have diameter of\none. With eight sockets the same problem returns, with a\nvengeance, since the diameter of a hypercube with eight\nnodes is three.\n0%20%40%60%80%100%120%140%160%\n0 Hop 1 Hop 1 Hop 2 Hop\nNumber of HopsSlowdownVs0-Hop Read\nReads Writes\nFigure 5.3: Read/Write Performance with Multiple\nNodes\nAll this information is available but it is cumbersome to\nuse. In section 6.5 we will see an interface which helps\naccessing and using this information easier.\nThe last piece of information the system provides is in thestatus of a process itself. It is possible to determine how\nthe memory-mapped ﬁles, the Copy-On-Write (COW)27\npages and anonymous memory are distributed over the\nnodes in the system. For each process the kernel pro-\nvides a pseudo-ﬁle /proc/ PID/numa_maps , where PID\nis the ID of the process, as shown in Figure 5.2. The\nimportant information in the ﬁle is the values for N0to\nN3, which indicate the number of pages allocated for the\nmemory area on nodes 0 to 3. It is a good guess that the\nprogram was executed on a core on node 3. The pro-\ngram itself and the dirtied pages are allocated on that\nnode. Read-only mappings, such as the ﬁrst mapping for\nld-2.4.so andlibc-2.4.so as well as the shared ﬁle\nlocale-archive are allocated on other nodes.\nAs we have seen in Figure 5.3, when performed across\nnodes the read performance falls by 9% and 30% respec-\ntively for 1- and 2-hop reads. For execution, such reads\nare needed and, if the L2 cache is missed, each cache line\nincurs these additional costs. All the costs measured for\nlarge workloads beyond the size of the cache would have\nto be increased by 9%/30% if the memory is remote to\nthe processor.\nTo see the effects in the real world we can measure the\nbandwidth as in section 3.5.1 but this time with the mem-\nory being on a remote node, one hop away. The result\nof this test when compared with the data for using local\nmemory can be seen in Figure 5.4. The numbers have a\nfew big spikes in both directions which are the result of\na problem of measuring multi-threaded code and can be\nignored. The important information in this graph is that\nread operations are always 20% slower. This is signiﬁ-\ncantly slower than the 9% in Figure 5.3, which is, most\nlikely, not a number for uninterrupted read/write opera-\ntions and might refer to older processor revisions. Only\n27Copy-On-Write is a method often used in OS implementations\nwhen a memory page has one user at ﬁrst and then has to be copied\nto allow independent users. In many situations the copying is unneces-\nsary, at all or at ﬁrst, in which case it makes sense to only copy when\neither user modiﬁes the memory. The operating system intercepts the\nwrite operation, duplicates the memory page, and then allows the write\ninstruction to proceed.\n46 Version 1.0 What Every Programmer Should Know About Memory",19176
24-6 What Programmers Can Do.pdf,24-6 What Programmers Can Do,,0
25-6.1 Bypassing the Cache.pdf,25-6.1 Bypassing the Cache," 40% 35% 30% 25% 20% 15% 10% 5%0%5%10%15%\n210213216219222225228\nWorking Set Size (Bytes)SlowdownVsLocal Memor y\nRead Write Copy\nFigure 5.4: Operating on Remote Memory\nAMD knows.\nFor working set sizes which ﬁt into the caches, the perfor-\nmance of write and copy operations is also 20% slower.\nFor working sets exceeding the size of the caches, the\nwrite performance is not measurably slower than the op-\neration on the local node. The speed of the interconnect\nis fast enough to keep up with the memory. The dominat-\ning factor is the time spent waiting on the main memory.6 What Programmers Can Do\nAfter the descriptions in the previous sections it is clear\nthat there are many, many opportunities for programmers\nto inﬂuence a program’s performance, positively or neg-\natively. And this is for memory-related operations only.\nWe will proceed in covering the opportunities from the\nground up, starting with the lowest levels of physical\nRAM access and L1 caches, up to and including OS func-\ntionality which inﬂuences memory handling.\n6.1 Bypassing the Cache\nWhen data is produced and not (immediately) consumed\nagain, the fact that memory store operations read a full\ncache line ﬁrst and then modify the cached data is detri-\nmental to performance. This operation pushes data out\nof the caches which might be needed again in favor of\ndata which will not be used soon. This is especially true\nfor large data structures, like matrices, which are ﬁlled\nand then used later. Before the last element of the matrix\nis ﬁlled the sheer size evicts the ﬁrst elements, making\ncaching of the writes ineffective.\nFor this and similar situations, processors provide sup-\nport for non-temporal write operations. Non-temporal in\nthis context means the data will not be reused soon, so\nthere is no reason to cache it. These non-temporal write\noperations do not read a cache line and then modify it;\ninstead, the new content is directly written to memory.\nThis might sound expensive but it does not have to be.\nThe processor will try to use write-combining (see sec-\ntion 3.3.3) to ﬁll entire cache lines. If this succeeds no\nmemory read operation is needed at all. For the x86 and\nx86-64 architectures a number of intrinsics are provided\nby gcc:\n#include <emmintrin.h>\nvoid _mm_stream_si32(int *p, int a);\nvoid _mm_stream_si128(int *p, __m128i a);\nvoid _mm_stream_pd(double *p, __m128d a);\n#include <xmmintrin.h>\nvoid _mm_stream_pi(__m64 *p, __m64 a);\nvoid _mm_stream_ps(float *p, __m128 a);\n#include <ammintrin.h>\nvoid _mm_stream_sd(double *p, __m128d a);\nvoid _mm_stream_ss(float *p, __m128 a);\nThese instructions are used most efﬁciently if they pro-\ncess large amounts of data in one go. Data is loaded from\nmemory, processed in one or more steps, and then written\nback to memory. The data “streams” through the proces-\nsor, hence the names of the intrinsics.\nThe memory address must be aligned to 8 or 16 bytes re-\nspectively. In code using the multimedia extensions it is\npossible to replace the normal _mm_store_ *intrinsics\nUlrich Drepper Version 1.0 47\nwith these non-temporal versions. In the matrix multi-\nplication code in section A.1 we do not do this since the\nwritten values are reused in a short order of time. This\nis an example where using the stream instructions is not\nuseful. More on this code in section 6.2.1.\nThe processor’s write-combining buffer can hold requests\nfor partial writing to a cache line for only so long. It\nis generally necessary to issue all the instructions which\nmodify a single cache line one after another so that the\nwrite-combining can actually take place. An example for\nhow to do this is as follows:\n#include <emmintrin.h>\nvoid setbytes(char *p, int c)\n{\n__m128i i = _mm_set_epi8(c, c, c, c,\nc, c, c, c,\nc, c, c, c,\nc, c, c, c);\n_mm_stream_si128((__m128i *)&p[0], i);\n_mm_stream_si128((__m128i *)&p[16], i);\n_mm_stream_si128((__m128i *)&p[32], i);\n_mm_stream_si128((__m128i *)&p[48], i);\n}\nAssuming the pointer pis appropriately aligned, a call to\nthis function will set all bytes of the addressed cache line\ntoc. The write-combining logic will see the four gener-\natedmovntdq instructions and only issue the write com-\nmand for the memory once the last instruction has been\nexecuted. To summarize, this code sequence not only\navoids reading the cache line before it is written, it also\navoids polluting the cache with data which might not be\nneeded soon. This can have huge beneﬁts in certain situa-\ntions. An example of everyday code using this technique\nis the memset function in the C runtime, which should\nuse a code sequence like the above for large blocks.\nSome architectures provide specialized solutions. The\nPowerPC architecture deﬁnes the dcbz instruction which\ncan be used to clear an entire cache line. The instruction\ndoes not really bypass the cache since a cache line is al-\nlocated for the result, but no data is read from memory. It\nis more limited than the non-temporal store instructions\nsince a cache line can only be set to all-zeros and it pol-\nlutes the cache (in case the data is non-temporal), but no\nwrite-combining logic is needed to achieve the results.\nTo see the non-temporal instructions in action we will\nlook at a new test which is used to measure writing to a\nmatrix, organized as a two-dimensional array. The com-\npiler lays out the matrix in memory so that the leftmost\n(ﬁrst) index addresses the row which has all elements laid\nout sequentially in memory. The right (second) index ad-\ndresses the elements in a row. The test program iterates\nover the matrix in two ways: ﬁrst by increasing the col-\numn number in the inner loop and then by increasing the\nrow index in the inner loop. This means we get the be-j ii j\nFigure 6.1: Matrix Access Pattern\nhavior shown in Figure 6.1.\nWe measure the time it takes to initialize a 30003000\nmatrix. To see how memory behaves, we use store in-\nstructions which do not use the cache. On IA-32 proces-\nsors the “non-temporal hint” is used for this. For com-\nparison we also measure ordinary store operations. The\nresults can be seen in Table 6.1.\nInner Loop Increment\nRow Column\nNormal 0.048s 0.127s\nNon-Temporal 0.048s 0.160s\nTable 6.1: Timing Matrix Initialization\nFor the normal writes which do use the cache we see\nthe expected result: if memory is used sequentially we\nget a much better result, 0.048s for the whole operation\ntranslating to about 750MB/s, compared to the more-or-\nless random access which takes 0.127s (about 280MB/s).\nThe matrix is large enough that the caches are essentially\nineffective.\nThe part we are mainly interested in here are the writes\nbypassing the cache. It might be surprising that the se-\nquential access is just as fast here as in the case where the\ncache is used. The reason for this result is that the proces-\nsor is performing write-combining as explained above.\nIn addition, the memory ordering rules for non-temporal\nwrites are relaxed: the program needs to explicitly in-\nsert memory barriers ( sfence instructions for the x86\nand x86-64 processors). This means the processor has\nmore freedom to write back the data and thereby using\nthe available bandwidth as well as possible.\nIn the case of column-wise access in the inner loop the\nsituation is different. The results for uncached accesses\nare signiﬁcantly slower than in the case of cached ac-\ncesses (0.16s, about 225MB/s). Here we can see that no\nwrite combining is possible and each memory cell must\nbe addressed individually. This requires constantly se-\nlecting new rows in the RAM chips with all the associ-\nated delays. The result is a 25% worse result than the\ncached run.\n48 Version 1.0 What Every Programmer Should Know About Memory",7810
26-6.2 Cache Access.pdf,26-6.2 Cache Access,,0
27-6.2.1 Optimizing Level 1 Data Cache Access.pdf,27-6.2.1 Optimizing Level 1 Data Cache Access,"On the read side, processors, until recently, lacked sup-\nport aside from weak hints using non-temporal access\n(NTA) prefetch instructions. There is no equivalent to\nwrite-combining for reads, which is especially bad for\nuncacheable memory such as memory-mapped I/O. In-\ntel, with the SSE4.1 extensions, introduced NTA loads.\nThey are implemented using a small number of streaming\nload buffers; each buffer contains a cache line. The ﬁrst\nmovntdqa instruction for a given cache line will load a\ncache line into a buffer, possibly replacing another cache\nline. Subsequent 16-byte aligned accesses to the same\ncache line will be serviced from the load buffer at little\ncost. Unless there are other reasons to do so, the cache\nline will not be loaded into a cache, thus enabling the\nloading of large amounts of memory without polluting\nthe caches. The compiler provides an intrinsic for this\ninstruction:\n#include <smmintrin.h>\n__m128i _mm_stream_load_si128 (__m128i *p);\nThis intrinsic should be used multiple times, with ad-\ndresses of 16-byte blocks passed as the parameter, un-\ntil each cache line is read. Only then should the next\ncache line be started. Since there are a few streaming\nread buffers it might be possible to read from two mem-\nory locations at once.\nWhat we should take away from this experiment is that\nmodern CPUs very nicely optimize uncached write and\nmore recently even read accesses as long as they are se-\nquential. This knowledge can come in very handy when\nhandling large data structures which are used only once.\nSecond, caches can help to cover up some–but not all–of\nthe costs of random memory access. Random access in\nthis example is 70% slower due to the implementation of\nRAM access. Until the implementation changes, random\naccesses should be avoided whenever possible.\nIn the section about prefetching we will again take a look\nat the non-temporal ﬂag.\n6.2 Cache Access\nProgrammers wishing to improve their programs’ perfor-\nmance will ﬁnd it best to focus on changes affected the\nlevel 1 cache since those will likely yield the best results.\nWe will discuss it ﬁrst before extending the discussion to\nthe other levels. Obviously, all the optimizations for the\nlevel 1 cache also affect the other caches. The theme for\nall memory access is the same: improve locality (spatial\nand temporal) and align the code and data.\n6.2.1 Optimizing Level 1 Data Cache Access\nIn section section 3.3 we have already seen how much\nthe effective use of the L1d cache can improve perfor-mance. In this section we will show what kinds of code\nchanges can help to improve that performance. Contin-\nuing from the previous section, we ﬁrst concentrate on\noptimizations to access memory sequentially. As seen in\nthe numbers of section 3.3, the processor automatically\nprefetches data when memory is accessed sequentially.\nThe example code used is a matrix multiplication. We\nuse two square matrices of 10001000 double ele-\nments. For those who have forgotten the math, given\ntwo matrices AandBwith elements aijandbijwith\n0i;j <N the product is\n(AB)ij=N 1X\nk=0aikbkj=ai1b1j+ai2b2j++ai(N 1)b(N 1)j\nA straight-forward C implementation of this can look like\nthis:\nfor (i = 0; i < N; ++i)\nfor (j = 0; j < N; ++j)\nfor (k = 0; k < N; ++k)\nres[i][j] += mul1[i][k] *mul2[k][j];\nThe two input matrices are mul1 andmul2 . The result\nmatrix res is assumed to be initialized to all zeroes. It\nis a nice and simple implementation. But it should be\nobvious that we have exactly the problem explained in\nFigure 6.1. While mul1 is accessed sequentially, the in-\nner loop advances the row number of mul2 . That means\nthatmul1 is handled like the left matrix in Figure 6.1\nwhile mul2 is handled like the right matrix. This cannot\nbe good.\nThere is one possible remedy one can easily try. Since\neach element in the matrices is accessed multiple times it\nmight be worthwhile to rearrange (“transpose,” in math-\nematical terms) the second matrix mul2 before using it.\n(AB)ij=N 1X\nk=0aikbT\njk=ai1bT\nj1+ai2bT\nj2++ai(N 1)bT\nj(N 1)\nAfter the transposition (traditionally indicated by a su-\nperscript ‘T’) we now iterate over both matrices sequen-\ntially. As far as the C code is concerned, it now looks\nlike this:\ndouble tmp[N][N];\nfor (i = 0; i < N; ++i)\nfor (j = 0; j < N; ++j)\ntmp[i][j] = mul2[j][i];\nfor (i = 0; i < N; ++i)\nfor (j = 0; j < N; ++j)\nfor (k = 0; k < N; ++k)\nres[i][j] += mul1[i][k] *tmp[j][k];\nUlrich Drepper Version 1.0 49\nWe create a temporary variable to contain the transposed\nmatrix. This requires touching additional memory, but\nthis cost is, hopefully, recovered since the 1000 non-\nsequential accesses per column are more expensive (at\nleast on modern hardware). Time for some performance\ntests. The results on a Intel Core 2 with 2666MHz clock\nspeed are (in clock cycles):\nOriginal Transposed\nCycles 16,765,297,870 3,922,373,010\nRelative 100% 23.4%\nThrough the simple transformation of the matrix we can\nachieve a 76.6% speed-up! The copy operation is more\nthan made up. The 1000 non-sequential accesses really\nhurt.\nThe next question is whether this is the best we can do.\nWe certainly need an alternative method anyway which\ndoes not require the additional copy. We will not always\nhave the luxury to be able to perform the copy: the matrix\ncan be too large or the available memory too small.\nThe search for an alternative implementation should start\nwith a close examination of the math involved and the op-\nerations performed by the original implementation. Triv-\nial math knowledge allows us to see that the order in\nwhich the additions for each element of the result ma-\ntrix are performed is irrelevant as long as each addend\nappears exactly once.28This understanding allows us to\nlook for solutions which reorder the additions performed\nin the inner loop of the original code.\nNow let us examine the actual problem in the execution\nof the original code. The order in which the elements\nofmul2 are accessed is: (0,0), (1,0), . . . , ( N-1,0), (0,1),\n(1,1), . . . . The elements (0,0) and (0,1) are in the same\ncache line but, by the time the inner loop completes one\nround, this cache line has long been evicted. For this\nexample, each round of the inner loop requires, for each\nof the three matrices, 1000 cache lines (with 64 bytes for\nthe Core 2 processor). This adds up to much more than\nthe 32k of L1d available.\nBut what if we handle two iterations of the middle loop\ntogether while executing the inner loop? In this case\nwe use two double values from the cache line which\nis guaranteed to be in L1d. We cut the L1d miss rate in\nhalf. That is certainly an improvement, but, depending\non the cache line size, it still might not be as good as we\ncan get it. The Core 2 processor has a L1d cache line size\nof 64 bytes. The actual value can be queried using\nsysconf (_SC_LEVEL1_DCACHE_LINESIZE)\nat runtime or using the getconf utility from the com-\nmand line so that the program can be compiled for a spe-\nciﬁc cache line size. With sizeof(double) being 8\n28We ignore arithmetic effects here which might change the occur-\nrence of overﬂows, underﬂows, or rounding.this means that, to fully utilize the cache line, we should\nunroll the middle loop 8 times. Continuing this thought,\nto effectively use the res matrix as well, i.e., to write 8\nresults at the same time, we should unroll the outer loop 8\ntimes as well. We assume here cache lines of size 64 but\nthe code works also well on systems with 32 byte cache\nlines since both cache lines are also 100% utilized. In\ngeneral it is best to hardcode cache line sizes at compile\ntime by using the getconf utility as in:\ngcc -DCLS=$(getconf LEVEL1_DCACHE_LINESIZE) ...\nIf the binaries are supposed to be generic, the largest\ncache line size should be used. With very small L1ds\nthis might mean that not all the data ﬁts into the cache\nbut such processors are not suitable for high-performance\nprograms anyway. The code we arrive at looks some-\nthing like this:\n#define SM (CLS / sizeof (double))\nfor (i = 0; i < N; i += SM)\nfor (j = 0; j < N; j += SM)\nfor (k = 0; k < N; k += SM)\nfor (i2 = 0, rres = &res[i][j],\nrmul1 = &mul1[i][k]; i2 < SM;\n++i2, rres += N, rmul1 += N)\nfor (k2 = 0, rmul2 = &mul2[k][j];\nk2 < SM; ++k2, rmul2 += N)\nfor (j2 = 0; j2 < SM; ++j2)\nrres[j2] += rmul1[k2] *rmul2[j2];\nThis looks quite scary. To some extent it is but only\nbecause it incorporates some tricks. The most visible\nchange is that we now have six nested loops. The outer\nloops iterate with intervals of SM(the cache line size di-\nvided by sizeof(double) ). This divides the multipli-\ncation in several smaller problems which can be handled\nwith more cache locality. The inner loops iterate over\nthe missing indexes of the outer loops. There are, once\nagain, three loops. The only tricky part here is that the k2\nandj2loops are in a different order. This is done since,\nin the actual computation, only one expression depends\nonk2but two depend on j2.\nThe rest of the complication here results from the fact\nthat gcc is not very smart when it comes to optimizing ar-\nray indexing. The introduction of the additional variables\nrres ,rmul1 , and rmul2 optimizes the code by pulling\ncommon expressions out of the inner loops, as far down\nas possible. The default aliasing rules of the C and C++\nlanguages do not help the compiler making these deci-\nsions (unless restrict is used, all pointer accesses are\npotential sources of aliasing). This is why Fortran is still\na preferred language for numeric programming: it makes\nwriting fast code easier.29\n29In theory the restrict keyword introduced into the C lan-\nguage in the 1999 revision should solve the problem. Compilers have\nnot caught up yet, though. The reason is mainly that too much incorrect\ncode exists which would mislead the compiler and cause it to generate\nincorrect object code.\n50 Version 1.0 What Every Programmer Should Know About Memory\nOriginal Transposed Sub-Matrix Vectorized\nCycles 16,765,297,870 3,922,373,010 2,895,041,480 1,588,711,750\nRelative 100% 23.4% 17.3% 9.47%\nTable 6.2: Matrix Multiplication Timing\nHow all this work pays off can be seen in Table 6.2. By\navoiding the copying we gain another 6.1% of perfor-\nmance. Plus, we do not need any additional memory.\nThe input matrices can be arbitrarily large as long as the\nresult matrix ﬁts into memory as well. This is a require-\nment for a general solution which we have now achieved.\nThere is one more column in Table 6.2 which has not\nbeen explained. Most modern processors nowadays in-\nclude special support for vectorization. Often branded\nas multi-media extensions, these special instructions al-\nlow processing of 2, 4, 8, or more values at the same\ntime. These are often SIMD (Single Instruction, Mul-\ntiple Data) operations, augmented by others to get the\ndata in the right form. The SSE2 instructions provided\nby Intel processors can handle two double values in one\noperation. The instruction reference manual lists the in-\ntrinsic functions which provide access to these SSE2 in-\nstructions. If these intrinsics are used the program runs\nanother 7.3% (relative to the original) faster. The result is\na program which runs in 10% of the time of the original\ncode. Translated into numbers which people recognize,\nwe went from 318 MFLOPS to 3.35 GFLOPS. Since we\nare only interested in memory effects here, the program\ncode is pushed out into section A.1.\nIt should be noted that, in the last version of the code,\nwe still have some cache problems with mul2 ; prefetch-\ning still will not work. But this cannot be solved with-\nout transposing the matrix. Maybe the cache prefetching\nunits will get smarter to recognize the patterns, then no\nadditional change would be needed. 3.19 GFLOPS on a\n2.66 GHz processor with single-threaded code is not bad,\nthough.\nWhat we optimized in the example of the matrix multi-\nplication is the use of the loaded cache lines. All bytes of\na cache line are always used. We just made sure they are\nused before the cache line is evacuated. This is certainly\na special case.\nIt is much more common to have data structures which\nﬁll one or more cache lines where the program uses only\na few members at any one time. In Figure 3.11 we have\nalready seen the effects of large structure sizes if only\nfew members are used.\nFigure 6.2 shows the results of yet another set of bench-\nmarks performed using the by now well-known program.\nThis time two values of the same list element are added.\nIn one case, both elements are in the same cache line; in\nanother case, one element is in the ﬁrst cache line of the\nlist element and the second is in the last cache line. The\ngraph shows the slowdown we are experiencing. 5%0%5%10%15%20%25%30%35%\n210213216219222225228\nWorking Set Size (Bytes)SlowdownVs1Cache Line\nSequential 4 CLs Random 2 CLs Random 4 CLs\nFigure 6.2: Spreading Over Multiple Cache Lines\nUnsurprisingly, in all cases there are no negative effects\nif the working set ﬁts into L1d. Once L1d is no longer\nsufﬁcient, penalties are paid by using two cache lines in\nthe process instead of one. The red line shows the data\nwhen the list is laid out sequentially in memory. We see\nthe usual two step patterns: about 17% penalty when the\nL2 cache is sufﬁcient and about 27% penalty when the\nmain memory has to be used.\nIn the case of random memory accesses the relative data\nlooks a bit different. The slowdown for working sets\nwhich ﬁt into L2 is between 25% and 35%. Beyond that\nit goes down to about 10%. This is not because the penal-\nties get smaller but, instead, because the actual memory\naccesses get disproportionally more costly. The data also\nshows that, in some cases, the distance between the el-\nements does matter. The Random 4 CLs curve shows\nhigher penalties because the ﬁrst and fourth cache lines\nare used.\nAn easy way to see the layout of a data structure com-\npared to cache lines is to use the pahole program (see\n[4]). This program examines the data structures deﬁned\nin a binary. Take a program containing this deﬁnition:\nstruct foo {\nint a;\nlong fill[7];\nint b;\n};\nUlrich Drepper Version 1.0 51\nstruct foo {\nint a; / * 0 4 */\n/*XXX 4 bytes hole, try to pack */\nlong int fill[7]; / * 8 56 */\n/*--- cacheline 1 boundary (64 bytes) --- */\nint b; / * 64 4 */\n}; / *size: 72, cachelines: 2 */\n/*sum members: 64, holes: 1, sum holes: 4 */\n/*padding: 4 */\n/*last cacheline: 8 bytes */\nFigure 6.3: Output of pahole Run\nWhen compiled on a 64-bit machine, the output of pa-\nhole contains (among other things) the output shown in\nFigure 6.3. This output tells us a lot. First, it shows that\nthe data structure uses up more than one cache line. The\ntool assumes the currently used processor’s cache line\nsize, but this value can be overridden using a command\nline parameter. Especially in cases where the size of the\nstructure is barely over the limit of a cache line, and many\nobjects of this type are allocated, it makes sense to seek\na way to compress that structure. Maybe a few elements\ncan have a smaller type, or maybe some ﬁelds are actu-\nally ﬂags which can be represented using individual bits.\nIn the case of the example the compression is easy and it\nis hinted at by the program. The output shows that there\nis a hole of four bytes after the ﬁrst element. This hole is\ncaused by the alignment requirement of the structure and\nthefill element. It is easy to see that the element b,\nwhich has a size of four bytes (indicated by the 4 at the\nend of the line), ﬁts perfectly into the gap. The result in\nthis case is that the gap no longer exists and that the data\nstructure ﬁts onto one cache line. The pahole tool can\nperform this optimization itself. If the --reorganize\nparameter is used and the structure name is added at the\nend of the command line the output of the tool is the op-\ntimized structure and the cache line use. Besides moving\nelements to ﬁll gaps, the tool can also optimize bit ﬁelds\nand combine padding and holes. For more details see [4].\nHaving a hole which is just large enough for the trailing\nelement is, of course, the ideal situation. For this opti-\nmization to be useful it is required that the object itself is\naligned to a cache line. We get to that in a bit.\nThe pahole output also allows to see easily whether ele-\nments have to be reordered so that those elements which\nare used together are also stored together. Using the pa-\nhole tool, it is easily possible to determine which ele-\nments are on the same cache line and when, instead, the\nelements have to be reshufﬂed to achieve that. This is not\nan automatic process but the tool can help quite a bit.\nThe position of the individual structure elements and the\nway they are used is important, too. As we have seen\nin section 3.5.2 the performance of code with the critical\nword late in the cache line is worse. This means a pro-grammer should always follow the following two rules:\n1. Always move the structure element which is most\nlikely to be the critical word to the beginning of\nthe structure.\n2. When accessing the data structures, and the order\nof access is not dictated by the situation, access the\nelements in the order in which they are deﬁned in\nthe structure.\nFor small structures, this means that the elements should\nbe arranged in the order in which they are likely accessed.\nThis must be handled in a ﬂexible way to allow the other\noptimizations, such as ﬁlling holes, to be applied as well.\nFor bigger data structures each cache line-sized block\nshould be arranged to follow the rules.\nIf the object itself is not aligned as expected, reorder-\ning elements is not worth the time it takes, though. The\nalignment of an object is determined by the alignment\nrequirement of the data type. Each fundamental type has\nits own alignment requirement. For structured types the\nlargest alignment requirement of any of its elements de-\ntermines the alignment of the structure. This is almost\nalways smaller than the cache line size. This means even\nif the members of a structure are lined up to ﬁt into the\nsame cache line an allocated object might not have an\nalignment matching the cache line size. There are two\nways to ensure that the object has the alignment which\nwas used when designing the layout of the structure:\n• the object can be allocated with an explicit align-\nment requirement. For dynamic allocation a call\ntomalloc would only allocate the object with an\nalignment matching that of the most demanding\nstandard type (usually long double ). It is pos-\nsible to use posix_memalign , though, to request\nhigher alignments.\n#include <stdlib.h>\nint posix_memalign(void **memptr,\nsize_t align,\nsize_t size);\n52 Version 1.0 What Every Programmer Should Know About Memory\nThe function stores a pointer pointing to the newly-\nallocated memory in the pointer variable pointed to\nbymemptr . The memory block is size bytes in\nsize and is aligned on a align -byte boundary.\nFor objects allocated by the compiler (in .data ,\n.bss , etc, and on the stack) a variable attribute can\nbe used:\nstruct strtype variable\n__attribute((aligned(64)));\nIn this case the variable is aligned at a 64 byte\nboundary regardless of the alignment requirement\nof the strtype structure. This works for global\nvariables as well as automatic variables.\nFor arrays this method does not work as one might\nexpect. Only the ﬁrst element of the array would\nbe aligned unless the size of each array element is\na multiple of the alignment value. It also means\nthat every single variable must be annotated ap-\npropriately. The use of posix_memalign is also\nnot entirely free since the alignment requirements\nusually lead to fragmentation and/or higher mem-\nory consumption.\n• the alignment requirement of a user-deﬁned type\ncan be changed by using a type attribute:\nstruct strtype {\n...members...\n} __attribute((aligned(64)));\nThis will cause the compiler to allocate all objects\nwith the appropriate alignment, including arrays.\nThe programmer has to take care of requesting the\nappropriate alignment for dynamically allocated ob-\njects, though. Here once again posix_memalign\nmust be used. It is easy enough to use the alignof\noperator gcc provides and pass the value as the sec-\nond parameter to posix_memalign .\nThe multimedia extensions previously mentioned in this\nsection almost always require that the memory accesses\nare aligned. I.e., for 16 byte memory accesses the address\nis supposed to be 16 byte aligned. The x86 and x86-64\nprocessors have special variants of the memory opera-\ntions which can handle unaligned accesses but these are\nslower. This hard alignment requirement is nothing new\nfor most RISC architectures which require full alignment\nfor all memory accesses. Even if an architecture sup-\nports unaligned accesses this is sometimes slower than\nusing appropriate alignment, especially if the misalign-\nment causes a load or store to use two cache lines instead\nof one.0%50%100%150%200%250%300%350%400%\n210213216219222225228\nWorking Set Size (Bytes)SlowdownVsAligned Access\nSequential Inc Random Inc\nFigure 6.4: Overhead of Unaligned Accesses\nFigure 6.4 shows the effects of unaligned memory ac-\ncesses. The now well-known tests which increment a\ndata element while visiting memory (sequentially or ran-\ndomly) are measured, once with aligned list elements and\nonce with deliberately misaligned elements. The graph\nshows the slowdown the program incurs because of the\nunaligned accesses. The effects are more dramatic for the\nsequential access case than for the random case because,\nin the latter case, the costs of unaligned accesses are par-\ntially hidden by the generally higher costs of the mem-\nory access. In the sequential case, for working set sizes\nwhich do ﬁt into the L2 cache, the slowdown is about\n300%. This can be explained by the reduced effective-\nness of the L1 cache. Some increment operations now\ntouch two cache lines, and beginning work on a list ele-\nment now often requires reading of two cache lines. The\nconnection between L1 and L2 is simply too congested.\nFor very large working set sizes, the effects of the un-\naligned access are still 20% to 30%–which is a lot given\nthat the aligned access time for those sizes is long. This\ngraph should show that alignment must be taken seri-\nously. Even if the architecture supports unaligned ac-\ncesses, this must not be taken as “they are as good as\naligned accesses”.\nThere is some fallout from these alignment requirements,\nthough. If an automatic variable has an alignment re-\nquirement, the compiler has to ensure that it is met in all\nsituations. This is not trivial since the compiler has no\ncontrol over the call sites and the way they handle the\nstack. This problem can be handled in two ways:\n1. The generated code actively aligns the stack, in-\nserting gaps if necessary. This requires code to\ncheck for alignment, create alignment, and later\nundo the alignment.\n2. Require that all callers have the stack aligned.\nUlrich Drepper Version 1.0 53\nAll of the commonly used application binary interfaces\n(ABIs) follow the second route. Programs will likely fail\nif a caller violates the rule and alignment is needed in the\ncallee. Keeping alignment intact does not come for free,\nthough.\nThe size of a stack frame used in a function is not neces-\nsarily a multiple of the alignment. This means padding is\nneeded if other functions are called from this stack frame.\nThe big difference is that the stack frame size is, in most\ncases, known to the compiler and, therefore, it knows\nhow to adjust the stack pointer to ensure alignment for\nany function which is called from that stack frame. In\nfact, most compilers will simply round the stack frame\nsize up and be done with it.\nThis simple way to handle alignment is not possible if\nvariable length arrays (VLAs) or alloca are used. In\nthat case, the total size of the stack frame is only known\nat runtime. Active alignment control might be needed in\nthis case, making the generated code (slightly) slower.\nOn some architectures, only the multimedia extensions\nrequire strict alignment; stacks on those architectures are\nalways minimally aligned for the normal data types, usu-\nally 4 or 8 byte alignment for 32- and 64-bit architectures\nrespectively. On these systems, enforcing the alignment\nincurs unnecessary costs. That means that, in this case,\nwe might want to get rid of the strict alignment require-\nment if we know that it is never depended upon. Tail\nfunctions (those which call no other functions) which do\nno multimedia operations do not need alignment. Nei-\nther do functions which only call functions which need\nno alignment. If a large enough set of functions can be\nidentiﬁed, a program might want to relax the alignment\nrequirement. For x86 binaries gcc has support for relaxed\nstack alignment requirements:\n-mpreferred-stack-boundary=2\nIf this option is given a value of N, the stack alignment\nrequirement will be set to 2Nbytes. So, if a value of 2\nis used, the stack alignment requirement is reduced from\nthe default (which is 16 bytes) to just 4 bytes. In most\ncases this means no additional alignment operation is\nneeded since normal stack push and pop operations work\non four-byte boundaries anyway. This machine-speciﬁc\noption can help to reduce code size and also improve ex-\necution speed. But it cannot be applied for many other\narchitectures. Even for x86-64 it is generally not appli-\ncable since the x86-64 ABI requires that ﬂoating-point\nparameters are passed in an SSE register and the SSE in-\nstructions require full 16 byte alignment. Nevertheless,\nwhenever the option is usable it can make a noticeable\ndifference.\nEfﬁcient placement of structure elements and alignment\nare not the only aspects of data structures which inﬂuence\ncache efﬁciency. If an array of structures is used, the en-\ntire structure deﬁnition affects performance. Rememberthe results in Figure 3.11: in this case we had increas-\ning amounts of unused data in the elements of the array.\nThe result was that prefetching was increasingly less ef-\nfective and the program, for large data sets, became less\nefﬁcient.\nFor large working sets it is important to use the available\ncache as well as possible. To achieve this, it might be\nnecessary to rearrange data structures. While it is easier\nfor the programmer to put all the data which conceptually\nbelongs together in the same data structure, this might\nnot be the best approach for maximum performance. As-\nsume we have a data structure as follows:\nstruct order {\ndouble price;\nbool paid;\nconst char *buyer[5];\nlong buyer_id;\n};\nFurther assume that these records are stored in a big array\nand that a frequently-run job adds up the expected pay-\nments of all the outstanding bills. In this scenario, the\nmemory used for the buyer andbuyer_id ﬁelds is un-\nnecessarily loaded into the caches. Judging from the data\nin Figure 3.11 the program will perform up to 5 times\nworse than it could.\nIt is much better to split the order data structure in two\npieces, storing the ﬁrst two ﬁelds in one structure and the\nother ﬁelds elsewhere. This change certainly increases\nthe complexity of the program, but the performance gains\nmight justify this cost.\nFinally, let us consider another cache use optimization\nwhich, while also applying to the other caches, is pri-\nmarily felt in the L1d access. As seen in Figure 3.8 an\nincreased associativity of the cache beneﬁts normal op-\neration. The larger the cache, the higher the associativity\nusually is. The L1d cache is too large to be fully associa-\ntive but not large enough to have the same associativity as\nL2 caches. This can be a problem if many of the objects\nin the working set fall into the same cache set. If this\nleads to evictions due to overuse of a set, the program\ncan experience delays even though much of the cache is\nunused. These cache misses are sometimes called con-\nﬂict misses . Since the L1d addressing uses virtual ad-\ndresses, this is actually something the programmer can\nhave control over. If variables which are used together\nare also stored together the likelihood of them falling into\nthe same set is minimized. Figure 6.5 shows how quickly\nthe problem can hit.\nIn the ﬁgure, the now familiar Follow30with NPAD =15\ntest is measured with a special setup. The X–axis is the\ndistance between two list elements, measured in empty\n30The test was performed on a 32-bit machine, hence NPAD =15\nmeans one 64-byte cache line per list element.\n54 Version 1.0 What Every Programmer Should Know About Memory",28943
28-6.2.2 Optimizing Level 1 Instruction Cache Access.pdf,28-6.2.2 Optimizing Level 1 Instruction Cache Access,"16\n14\n12\n10\n8\n6\n4\n2\n16 32 48 64 80 96 112 128012345678910\nxz\ny\nFigure 6.5: Cache Associativity Effects\nlist elements. In other words, a distance of 2 means that\nthe next element’s address is 128 bytes after the previ-\nous one. All elements are laid out in the virtual ad-\ndress space with the same distance. The Y–axis shows\nthe total length of the list. Only one to 16 elements are\nused, meaning that the total working set size is 64 to\n1024 bytes. The z–axis shows the average number of\ncycles needed to traverse each list element.\nThe result shown in the ﬁgure should not be surprising.\nIf few elements are used, all the data ﬁts into L1d and the\naccess time is only 3 cycles per list element. The same\nis true for almost all arrangements of the list elements:\nthe virtual addresses are nicely mapped to L1d slots with\nalmost no conﬂicts. There are two (in this graph) spe-\ncial distance values for which the situation is different.\nIf the distance is a multiple of 4096 bytes (i.e., distance\nof 64 elements) and the length of the list is greater than\neight, the average number of cycles per list element in-\ncreases dramatically. In these situations all entries are in\nthe same set and, once the list length is greater than the\nassociativity, entries are ﬂushed from L1d and have to be\nre-read from L2 the next round. This results in the cost\nof about 10 cycles per list element.\nWith this graph we can determine that the processor used\nhas an L1d cache with associativity 8 and a total size of\n32kB. That means that the test could, if necessary, be\nused to determine these values. The same effects can be\nmeasured for the L2 cache but, here, it is more complex\nsince the L2 cache is indexed using physical addresses\nand it is much larger.\nProgrammers will hopefully see this data as an indica-\ntion that set associativity is something worth paying at-\ntention to. Laying out data at boundaries that are powers\nof two happens often enough in the real world, but this is\nexactly the situation which can easily lead to the aboveeffects and degraded performance. Unaligned accesses\ncan increase the probability of conﬂict misses since each\naccess might require an additional cache line.\nIndex Bank Byte14 7 6 43 0\nFigure 6.6: Bank Address of L1d on AMD\nIf this optimization is performed, another related opti-\nmization is possible, too. AMD’s processors, at least,\nimplement the L1d as several individual banks. The L1d\ncan receive two data words per cycle but only if both\nwords are stored in different banks or in a bank with the\nsame index. The bank address is encoded in the low bits\nof the virtual address as shown in Figure 6.6. If variables\nwhich are used together are also stored together the like-\nlihood that they are in different banks or the same bank\nwith the same index is high.\n6.2.2 Optimizing Level 1 Instruction Cache Access\nPreparing code for good L1i use needs similar techniques\nas good L1d use. The problem is, though, that the pro-\ngrammer usually does not directly inﬂuence the way L1i\nis used unless s/he writes code in assembler. If compil-\ners are used, programmers can indirectly determine the\nL1i use by guiding the compiler to create a better code\nlayout.\nCode has the advantage that it is linear between jumps.\nIn these periods the processor can prefetch memory efﬁ-\nciently. Jumps disturb this nice picture because\n• the jump target might not be statically determined;\nUlrich Drepper Version 1.0 55\n• and even if it is static the memory fetch might take\na long time if it misses all caches.\nThese problems create stalls in execution with a possibly\nsevere impact on performance. This is why today’s pro-\ncessors invest heavily in branch prediction (BP). Highly\nspecialized BP units try to determine the target of a jump\nas far ahead of the jump as possible so that the processor\ncan initiate loading the instructions at the new location\ninto the cache. They use static and dynamic rules and are\nincreasingly good at determining patterns in execution.\nGetting data into the cache as soon as possible is even\nmore important for the instruction cache. As mentioned\nin section 3.1, instructions have to be decoded before\nthey can be executed and, to speed this up (important\non x86 and x86-64), instructions are actually cached in\nthe decoded form, not in the byte/word form read from\nmemory.\nTo achieve the best L1i use programmers should look out\nfor at least the following aspects of code generation:\n1. reduce the code footprint as much as possible. This\nhas to be balanced with optimizations like loop un-\nrolling and inlining.\n2. code execution should be linear without bubbles.31\n3. aligning code when it makes sense.\nWe will now look at some compiler techniques available\nto help with optimizing programs according to these as-\npects.\nCompilers have options to enable levels of optimization;\nspeciﬁc optimizations can also be individually enabled.\nMany of the optimizations enabled at high optimization\nlevels (-O2 and -O3 for gcc) deal with loop optimizations\nand function inlining. In general, these are good opti-\nmizations. If the code which is optimized in these ways\naccounts for a signiﬁcant part of the total execution time\nof the program, overall performance can be improved.\nInlining of functions, in particular, allows the compiler to\noptimize larger chunks of code at a time which, in turn,\nenables the generation of machine code which better ex-\nploits the processor’s pipeline architecture. The handling\nof both code and data (through dead code elimination or\nvalue range propagation, and others) works better when\nlarger parts of the program can be considered as a single\nunit.\nA larger code size means higher pressure on the L1i (and\nalso L2 and higher level) caches. This canlead to less\nperformance. Smaller code can be faster. Fortunately gcc\nhas an optimization option to specify this. If -Os is used\nthe compiler will optimize for code size. Optimizations\n31Bubbles describe graphically the holes in the execution in the\npipeline of a processor which appear when the execution has to wait\nfor resources. For more details the reader is referred to literature on\nprocessor design.which are known to increase the code size are disabled.\nUsing this option often produces surprising results. Es-\npecially if the compiler cannot really take advantage of\nloop unrolling and inlining, this option is a big win.\nInlining can be controlled individually as well. The com-\npiler has heuristics and limits which guide inlining; these\nlimits can be controlled by the programmer. The -ﬁnline-\nlimit option speciﬁes how large a function must be to be\nconsidered too large for inlining. If a function is called\nin multiple places, inlining it in all of them would cause\nan explosion in the code size. But there is more. As-\nsume a function inlcand is called in two functions f1\nandf2. The functions f1andf2are themselves called\nin sequence.\nstart f1 start inlcand\ncode f1 code inlcand\ninlined inlcand end inlcand\nmore code f1\nend f1 start f1\ncode f1\nstart f2 end f1\ncode f2\ninlined inlcand start f2\nmore code f2 code f2\nend f2 end f2\nTable 6.3: Inlining Vs Not\nTable 6.3 shows how the generated code could look like\nin the cases of no inline and inlining in both functions.\nIf the function inlcand is inlined in both f1andf2the\ntotal size of the generated code is size f1+ size f2+\n2sizeinlcand . If no inlining happens, the total size\nis smaller by size inlcand . This is how much more L1i\nand L2 cache is needed if f1andf2are called shortly af-\nter one another. Plus: if inlcand is not inlined, the code\nmight still be in L1i and it will not have to be decoded\nagain. Plus: the branch prediction unit might do a bet-\nter job of predicting jumps since it has already seen the\ncode. If the compiler default for the upper limit on the\nsize of inlined functions is not the best for the program,\nit should be lowered.\nThere are cases, though, when inlining always makes\nsense. If a function is only called once it might as well be\ninlined. This gives the compiler the opportunity to per-\nform more optimizations (like value range propagation,\nwhich might signiﬁcantly improve the code). That inlin-\ning might be thwarted by the selection limits. gcc has, for\ncases like this, an option to specify that a function is al-\nways inlined. Adding the always_inline function at-\ntribute instructs the compiler to do exactly what the name\nsuggests.\nIn the same context, if a function should never be inlined\ndespite being small enough, the noinline function at-\ntribute can be used. Using this attribute makes sense even\nfor small functions if they are called often from different\n56 Version 1.0 What Every Programmer Should Know About Memory\nplaces. If the L1i content can be reused and the overall\nfootprint is reduced this often makes up for the additional\ncost of the extra function call. Branch prediction units\nare pretty good these days. If inlining can lead to more\naggressive optimizations things look different. This is\nsomething which must be decided on a case-by-case ba-\nsis.\nThealways_inline attribute works well if the inline\ncode is always used. But what if this is not the case?\nWhat if the inlined function is called only occasionally:\nvoid fct(void) {\n... code block A ...\nif (condition)\ninlfct()\n... code block C ...\n}\nThe code generated for such a code sequence in general\nmatches the structure of the sources. That means ﬁrst\ncomes the code block A, then a conditional jump which,\nif the condition evaluates to false, jumps forward. The\ncode generated for the inlined inlfct comes next, and\nﬁnally the code block C. This looks all reasonable but it\nhas a problem.\nIf the condition is frequently false, the execution is\nnot linear. There is a big chunk of unused code in the\nmiddle which not only pollutes the L1i due to prefetch-\ning, it also can cause problems with branch prediction. If\nthe branch prediction is wrong the conditional expression\ncan be very inefﬁcient.\nThis is a general problem and not speciﬁc to inlining\nfunctions. Whenever conditional execution is used and\nit is lopsided (i.e., the expression far more often leads to\none result than the other) there is the potential for false\nstatic branch prediction and thus bubbles in the pipeline.\nThis can be prevented by telling the compiler to move\nthe less often executed code out of the main code path.\nIn that case the conditional branch generated for an if\nstatement would jump to a place out of the order as can\nbe seen in the following ﬁgure.\nA I B C\nA I C B\nThe upper parts represents the simple code layout. If the\narea B, e.g. generated from the inlined function inlfct\nabove, is often not executed because the conditional I\njumps over it, the prefetching of the processor will pull\nin cache lines containing block B which are rarely used.\nUsing block reordering this can be changed, with a re-\nsult that can be seen in the lower part of the ﬁgure. Theoften-executed code is linear in memory while the rarely-\nexecuted code is moved somewhere where it does not\nhurt prefetching and L1i efﬁciency.\ngcc provides two methods to achieve this. First, the com-\npiler can take proﬁling output into account while recom-\npiling code and lay out the code blocks according to the\nproﬁle. We will see how this works in section 7. The\nsecond method is through explicit branch prediction. gcc\nrecognizes __builtin_expect :\nlong __builtin_expect(long EXP, long C);\nThis construct tells the compiler that the expression EXP\nmost likely will have the value C. The return value is EXP.\n__builtin_expect is meant to be used in an condi-\ntional expression. In almost all cases will it be used in the\ncontext of boolean expressions in which case it is much\nmore convenient to deﬁne two helper macros:\n#define unlikely(expr) __builtin_expect(!!(expr), 0)\n#define likely(expr) __builtin_expect(!!(expr), 1)\nThese macros can then be used as in\nif (likely(a > 1))\nIf the programmer makes use of these macros and then\nuses the -freorder-blocks optimization option gcc\nwill reorder blocks as in the ﬁgure above. This option is\nenabled with -O2 but disabled for -Os. There is another\ngcc option to reorder block ( -freorder-blocks-and-\npartition ) but it has limited usefulness because it does\nnot work with exception handling.\nThere is another big advantage of small loops, at least\non certain processors. The Intel Core 2 front end has a\nspecial feature called Loop Stream Detector (LSD). If a\nloop has no more than 18 instructions (none of which\nis a call to a subroutine), requires only up to 4 decoder\nfetches of 16 bytes, has at most 4 branch instructions, and\nis executed more than 64 times, than the loop is some-\ntimes locked in the instruction queue and therefore more\nquickly available when the loop is used again. This ap-\nplies, for instance, to small inner loops which are entered\nmany times through an outer loop. Even without such\nspecialized hardware compact loops have advantages.\nInlining is not the only aspect of optimization with re-\nspect to L1i. Another aspect is alignment, just as for\ndata. There are obvious differences: code is a mostly lin-\near blob which cannot be placed arbitrarily in the address\nspace and it cannot be inﬂuenced directly by the pro-\ngrammer as the compiler generates the code. There are\nsome aspects which the programmer can control, though.\nUlrich Drepper Version 1.0 57",13592
29-6.3 Prefetching.pdf,29-6.3 Prefetching,"Aligning each single instruction does not make any sense.\nThe goal is to have the instruction stream be sequential.\nSo alignment only makes sense in strategic places. To\ndecide where to add alignments it is necessary to under-\nstand what the advantages can be. Having an instruction\nat the beginning of a cache line32means that the prefetch\nof the cache line is maximized. For instructions this also\nmeans the decoder is more effective. It is easy to see that,\nif an instruction at the end of a cache line is executed, the\nprocessor has to get ready to read a new cache line and\ndecode the instructions. There are things which can go\nwrong (such as cache line misses), meaning that an in-\nstruction at the end of the cache line is, on average, not\nas effectively executed as one at the beginning.\nCombine this with the follow-up deduction that the prob-\nlem is most severe if control was just transferred to the\ninstruction in question (and hence prefetching is not ef-\nfective) and we arrive at our ﬁnal conclusion where align-\nment of code is most useful:\n• at the beginning of functions;\n• at the beginning of basic blocks which are reached\nonly through jumps;\n• to some extent, at the beginning of loops.\nIn the ﬁrst two cases the alignment comes at little cost.\nExecution proceeds at a new location and, if we choose\nit to be at the beginning of a cache line, we optimize\nprefetching and decoding.33The compiler accomplishes\nthis alignment through the insertion of a series of no-op\ninstructions to ﬁll the gap created by aligning the code.\nThis “dead code” takes a little space but does not nor-\nmally hurt performance.\nThe third case is slightly different: aligning the begin-\nning of each loop might create performance problems.\nThe problem is that beginning of a loop often follows\nother code sequentially. If the circumstances are not very\nlucky there will be a gap between the previous instruc-\ntion and the aligned beginning of the loop. Unlike in the\nprevious two cases, this gap cannot be completely dead.\nAfter execution of the previous instruction the ﬁrst in-\nstruction in the loop must be executed. This means that,\nfollowing the previous instruction, there either must be a\nnumber of no-op instructions to ﬁll the gap or there must\nbe an unconditional jump to the beginning of the loop.\nNeither possibility is free. Especially if the loop itself\nis not executed often, the no-ops or the jump might cost\nmore than one saves by aligning the loop.\nThere are three ways the programmer can inﬂuence the\nalignment of code. Obviously, if the code is written in\n32For some processors cache lines are not the atomic blocks for in-\nstructions. The Intel Core 2 front end issues 16 byte blocks to the de-\ncoder. They are appropriately aligned and so no issued block can span\na cache line boundary. Aligning at the beginning of a cache line still\nhas advantages since it optimizes the positive effects of prefetching.\n33For instruction decoding processors often use a smaller unit than\ncache lines, 16 bytes in case of x86 and x86-64.assembler the function and all instructions in it can be\nexplicitly aligned. The assembler provides for all archi-\ntectures the .align pseudo-op to do that. For high-level\nlanguages the compiler must be told about alignment re-\nquirements. Unlike for data types and variables this is not\npossible in the source code. Instead a compiler option is\nused:\n-falign-functions=N\nThis option instructs the compiler to align all functions\nto the next power-of-two boundary greater than N. That\nmeans a gap of up to Nbytes is created. For small func-\ntions using a large value for Nis a waste. Equally for\ncode which is executed only rarely. The latter can hap-\npen a lot in libraries which can contain both popular and\nnot-so-popular interfaces. A wise choice of the option\nvalue can speed things up or save memory by avoiding\nalignment. All alignment is turned off by using one as\nthe value of Nor by using the -fno-align-functions\noption.\nThe alignment for the second case above–beginning of\nbasic blocks which are not reached sequentially–can be\ncontrolled with a different option:\n-falign-jumps=N\nAll the other details are equivalent, the same warning\nabout waste of memory applies.\nThe third case also has its own option:\n-falign-loops=N\nYet again, the same details and warnings apply. Except\nthat here, as explained before, alignment comes at a run-\ntime cost since either no-ops or a jump instruction has to\nbe executed if the aligned address is reached sequentially.\ngcc knows about one more option for controlling align-\nment which is mentioned here only for completeness.\n-falign-labels aligns every single label in the code\n(basically the beginning of each basic block). This, in\nall but a few exceptional cases, slows down the code and\ntherefore should not be used.\n6.2.3 Optimizing Level 2 and Higher Cache Access\nEverything said about optimizations for level 1 caches\nalso applies to level 2 and higher cache accesses. There\nare two additional aspects of last level caches:\n• cache misses are always very expensive. While\nL1 misses (hopefully) frequently hit L2 and higher\ncache, thus limiting the penalties, there is obvi-\nously no fallback for the last level cache.\n58 Version 1.0 What Every Programmer Should Know About Memory\n• L2 caches and higher are often shared by multiple\ncores and/or hyper-threads. The effective cache\nsize available to each execution unit is therefore\nusually less than the total cache size.\nTo avoid the high costs of cache misses, the working\nset size should be matched to the cache size. If data is\nonly needed once this obviously is not necessary since\nthe cache would be ineffective anyway. We are talking\nabout workloads where the data set is needed more than\nonce. In such a case the use of a working set which is\ntoo large to ﬁt into the cache will create large amounts\nof cache misses which, even with prefetching being per-\nformed successfully, will slow down the program.\nA program has to perform its job even if the data set\nis too large. It is the programmer’s job to do the work\nin a way which minimizes cache misses. For last-level\ncaches this is possible–just as for L1 caches–by work-\ning on the job in smaller pieces. This is very similar to\nthe optimized matrix multiplication on page 50. One dif-\nference, though, is that, for last level caches, the data\nblocks which are be worked on can be bigger. The code\nbecomes yet more complicated if L1 optimizations are\nneeded, too. Imagine a matrix multiplication where the\ndata sets–the two input matrices and the output matrix–\ndo not ﬁt into the last level cache together. In this case\nit might be appropriate to optimize the L1 and last level\ncache accesses at the same time.\nThe L1 cache line size is usually constant over many pro-\ncessor generations; even if it is not, the differences will\nbe small. It is no big problem to just assume the larger\nsize. On processors with smaller cache sizes two or more\ncache lines will then be used instead of one. In any case,\nit is reasonable to hardcode the cache line size and opti-\nmize the code for it.\nFor higher level caches this is not the case if the program\nis supposed to be generic. The sizes of those caches can\nvary widely. Factors of eight or more are not uncommon.\nIt is not possible to assume the larger cache size as a de-\nfault since this would mean the code performs poorly on\nall machines except those with the biggest cache. The\nopposite choice is bad too: assuming the smallest cache\nmeans throwing away 87% of the cache or more. This is\nbad; as we can see from Figure 3.14 using large caches\ncan have a huge impact on the program’s speed.\nWhat this means is that the code must dynamically ad-\njust itself to the cache line size. This is an optimiza-\ntion speciﬁc to the program. All we can say here is\nthat the programmer should compute the program’s re-\nquirements correctly. Not only are the data sets them-\nselves needed, the higher level caches are also used for\nother purposes; for example, all the executed instructions\nare loaded from cache. If library functions are used this\ncache usage might add up to a signiﬁcant amount. Those\nlibrary functions might also need data of their own which\nfurther reduces the available memory.Once we have a formula for the memory requirement we\ncan compare it with the cache size. As mentioned before,\nthe cache might be shared with multiple other cores. Cur-\nrently34the only way to get correct information without\nhardcoding knowledge is through the /sys ﬁlesystem.\nIn Table 5.2 we have seen the what the kernel publishes\nabout the hardware. A program has to ﬁnd the directory:\n/sys/devices/system/cpu/cpu */cache\nfor the last level cache. This can be recognized by the\nhighest numeric value in the level ﬁle in that directory.\nWhen the directory is identiﬁed the program should read\nthe content of the size ﬁle in that directory and divide\nthe numeric value by the number of bits set in the bitmask\nin the ﬁle shared_cpu_map .\nThe value which is computed this way is a safe lower\nlimit. Sometimes a program knows a bit more about the\nbehavior of other threads or processes. If those threads\nare scheduled on a core or hyper-thread sharing the cache,\nand the cache usage is known to not exhaust its fraction\nof the total cache size, then the computed limit might be\ntoo low to be optimal. Whether more than the fair share\nshould be used really depends on the situation. The pro-\ngrammer has to make a choice or has to allow the user to\nmake a decision.\n6.2.4 Optimizing TLB Usage\nThere are two kinds of optimization of TLB usage. The\nﬁrst optimization is to reduce the number of pages a pro-\ngram has to use. This automatically results in fewer TLB\nmisses. The second optimization is to make the TLB\nlookup cheaper by reducing the number higher level di-\nrectory tables which must be allocated. Fewer tables\nmeans less memory is used which can result in higher\ncache hit rates for the directory lookup.\nThe ﬁrst optimization is closely related to the minimiza-\ntion of page faults. We will cover that topic in detail\nin section 7.5. While page faults usually are a one-time\ncost, TLB misses are a perpetual penalty given that the\nTLB cache is usually small and it is ﬂushed frequently.\nPage faults are orders of magnitude more expensive than\nTLB misses but, if a program is running long enough\nand certain parts of the program are executed frequently\nenough, TLB misses can outweigh even page fault costs.\nIt is therefore important to regard page optimization not\nonly from the perspective of page faults but also from the\nTLB miss perspective. The difference is that, while page\nfault optimizations only require page-wide grouping of\nthe code and data, TLB optimization requires that, at any\npoint in time, as few TLB entries are in use as possible.\nThe second TLB optimization is even harder to control.\nThe number of page directories which have to be used\ndepends on the distribution of the address ranges used in\nthe virtual address space of the process. Widely vary-\ning locations in the address space mean more directories.\n34There deﬁnitely will sometime soon be a better way!\nUlrich Drepper Version 1.0 59",11371
30-6.4 Multi-Thread Optimizations.pdf,30-6.4 Multi-Thread Optimizations,"A complication is that Address Space Layout Random-\nization (ASLR) leads to exactly these situations. The\nload addresses of stack, DSOs, heap, and possibly exe-\ncutable are randomized at runtime to prevent attackers of\nthe machine from guessing the addresses of functions or\nvariables.\nOnly if maximum performance is critical ASLR should\nbe turned off. The costs of the extra directories is low\nenough to make this step unnecessary in all but a few ex-\ntreme cases. One possible optimization the kernel could\nat any time perform is to ensure that a single mapping\ndoes not cross the address space boundary between two\ndirectories. This would limit ASLR in a minimal fashion\nbut not enough to substantially weaken it.\nThe only way a programmer is directly affected by this\nis when an address space region is explicitly requested.\nThis happens when using mmap with MAP_FIXED . Allo-\ncating new a address space region this way is very dan-\ngerous and hardly ever done. It is possible, though, and,\nif it is used and the addresses can be freely chosen, the\nprogrammer should know about the boundaries of the last\nlevel page directory and select the requested address ap-\npropriately.\n6.3 Prefetching\nThe purpose of prefetching is to hide the latency of a\nmemory access. The command pipeline and out-of-order\n(OOO) execution capabilities of today’s processors can\nhide some latency but, at best, only for accesses which\nhit the caches. To cover the latency of main memory ac-\ncesses, the command queue would have to be incredibly\nlong. Some processors without OOO try to compensate\nby increasing the number of cores, but this is a bad trade\nunless all the code in use is parallelized.\nPrefetching can further help to hide latency. The proces-\nsor performs prefetching on its own, triggered by certain\nevents (hardware prefetching) or explicitly requested by\nthe program (software prefetching).\n6.3.1 Hardware Prefetching\nThe trigger for the CPU to start hardware prefetching is\nusually a sequence of two or more cache misses in a\ncertain pattern. These cache misses can be to succeed-\ning or preceding cache lines. In old implementations\nonly cache misses to adjacent cache lines are recognized.\nWith contemporary hardware, strides are recognized as\nwell, meaning that skipping a ﬁxed number of cache lines\nis recognized as a pattern and handled appropriately.\nIt would be bad for performance if every single cache\nmiss triggered a hardware prefetch. Random memory\naccess patterns, for instance to global variables, are quite\ncommon and the resulting prefetches would mostly waste\nFSB bandwidth. This is why, to kickstart prefetching,\nat least two cache misses are needed. Processors todayall expect there to be more than one stream of mem-\nory accesses. The processor tries to automatically assign\neach cache miss to such a stream and, if the threshold\nis reached, start hardware prefetching. CPUs today can\nkeep track of eight to sixteen separate streams for the\nhigher level caches.\nThe units responsible for the pattern recognition are asso-\nciated with the respective cache. There can be a prefetch\nunit for the L1d and L1i caches. There is most probably\na prefetch unit for the L2 cache and higher. The L2 and\nhigher prefetch unit is shared with all the other cores and\nhyper-threads using the same cache. The number of eight\nto sixteen separate streams therefore is quickly reduced.\nPrefetching has one big weakness: it cannot cross page\nboundaries. The reason should be obvious when one\nrealizes that the CPUs support demand paging. If the\nprefetcher were allowed to cross page boundaries, the\naccess might trigger an OS event to make the page avail-\nable. This by itself can be bad, especially for perfor-\nmance. What is worse is that the prefetcher does not\nknow about the semantics of the program or the OS itself.\nIt might therefore prefetch pages which, in real life, never\nwould be requested. That means the prefetcher would\nrun past the end of the memory region the processor ac-\ncessed in a recognizable pattern before. This is not only\npossible, it is very likely. If the processor, as a side effect\nof a prefetch, triggered a request for such a page the OS\nmight even be completely thrown off its tracks if such a\nrequest could never otherwise happen.\nIt is therefore important to realize that, regardless of how\ngood the prefetcher is at predicting the pattern, the pro-\ngram will experience cache misses at page boundaries\nunless it explicitly prefetches or reads from the new page.\nThis is another reason to optimize the layout of data as\ndescribed in section 6.2 to minimize cache pollution by\nkeeping unrelated data out.\nBecause of this page limitation the processors today do\nnot have terribly sophisticated logic to recognize prefetch\npatterns. With the still predominant 4k page size there is\nonly so much which makes sense. The address range in\nwhich strides are recognized has been increased over the\nyears, but it probably does not make much sense to go\nbeyond the 512 byte window which is often used today.\nCurrently prefetch units do not recognize non-linear ac-\ncess patterns. It is more likely than not that such patterns\nare truly random or, at least, sufﬁciently non-repeating\nthat it makes no sense to try recognizing them.\nIf hardware prefetching is accidentally triggered there is\nonly so much one can do. One possibility is to try to\ndetect this problem and change the data and/or code lay-\nout a bit. This is likely to prove hard. There might be\nspecial localized solutions like using the ud2 instruc-\ntion35on x86 and x86-64 processors. This instruction,\nwhich cannot be executed itself, is used after an indirect\njump instruction; it is used as a signal to the instruction\n35Or non-instruction. It is the recommended undeﬁned opcode.\n60 Version 1.0 What Every Programmer Should Know About Memory\nfetcher that the processor should not waste efforts decod-\ning the following memory since the execution will con-\ntinue at a different location. This is a very special sit-\nuation, though. In most cases one has to live with this\nproblem.\nIt is possible to completely or partially disable hardware\nprefetching for the entire processor. On Intel proces-\nsors an Model Speciﬁc Register (MSR) is used for this\n(IA32 MISC ENABLE, bit 9 on many processors; bit 19\ndisables only the adjacent cache line prefetch). This, in\nmost cases, has to happen in the kernel since it is a privi-\nleged operation. If proﬁling shows that an important ap-\nplication running on a system suffers from bandwidth ex-\nhaustion and premature cache evictions due to hardware\nprefetches, using this MSR is a possibility.\n6.3.2 Software Prefetching\nThe advantage of hardware prefetching is that programs\ndo not have to be adjusted. The drawbacks, as just de-\nscribed, are that the access patterns must be trivial and\nthat prefetching cannot happen across page boundaries.\nFor these reasons we now have more possibilities, soft-\nware prefetching the most important of them. Software\nprefetching does require modiﬁcation of the source code\nby inserting special instructions. Some compilers sup-\nport pragmas to more or less automatically insert pre-\nfetch instructions. On x86 and x86-64 Intel’s convention\nfor compiler intrinsics to insert these special instructions\nis generally used:\n#include <xmmintrin.h>\nenum _mm_hint\n{\n_MM_HINT_T0 = 3,\n_MM_HINT_T1 = 2,\n_MM_HINT_T2 = 1,\n_MM_HINT_NTA = 0\n};\nvoid _mm_prefetch(void *p,\nenum _mm_hint h);\nPrograms can use the _mm_prefetch intrinsic on any\npointer in the program. Most processors (certainly all\nx86 and x86-64 processors) ignore errors resulting from\ninvalid pointers which makes the life of the programmer\nsigniﬁcantly easier. If the passed pointer references valid\nmemory, the prefetch unit will be instructed to load the\ndata into cache and, if necessary, evict other data. Unnec-\nessary prefetches should deﬁnitely be avoided since this\nmight reduce the effectiveness of the caches and it con-\nsumes memory bandwidth (possibly for two cache lines\nin case the evicted cache line is dirty).\nThe different hints to be used with the _mm_prefetch\nintrinsic are implementation deﬁned. That means each\nprocessor version can implement them (slightly) differ-ently. What can generally be said is that _MM_HINT_T0\nfetches data to all levels of the cache for inclusive caches\nand to the lowest level cache for exclusive caches. If\nthe data item is in a higher level cache it is loaded into\nL1d. The _MM_HINT_T1 hint pulls the data into L2 and\nnot into L1d. If there is an L3 cache the _MM_HINT_T2\nhints can do something similar for it. These are details,\nthough, which are weakly speciﬁed and need to be veri-\nﬁed for the actual processor in use. In general, if the data\nis to be used right away using _MM_HINT_T0 is the right\nthing to do. Of course this requires that the L1d cache\nsize is large enough to hold all the prefetched data. If the\nsize of the immediately used working set is too large, pre-\nfetching everything into L1d is a bad idea and the other\ntwo hints should be used.\nThe fourth hint, _MM_HINT_NTA , allows telling the pro-\ncessor to treat the prefetched cache line specially. NTA\nstands for non-temporal aligned which we already ex-\nplained in section 6.1. The program tells the processor\nthat polluting caches with this data should be avoided as\nmuch as possible since the data is only used for a short\ntime. The processor can therefore, upon loading, avoid\nreading the data into the lower level caches for inclusive\ncache implementations. When the data is evicted from\nL1d the data need not be pushed into L2 or higher but,\ninstead, can be written directly to memory. There might\nbe other tricks the processor designers can deploy if this\nhint is given. The programmer must be careful using this\nhint: if the immediate working set size is too large and\nforces eviction of a cache line loaded with the NTA hint,\nreloading from memory will occur.\n0100200300400500600700800900100011001200\n210213216219222225228\nWorking Set Size (Bytes)Cycles/List Element\nOne Thread With Prefetch\nFigure 6.7: Average with Prefetch, NPAD=31\nFigure 6.7 shows the results of a test using the now fa-\nmiliar pointer chasing framework. The list is randomly\nlaid out in memory. The difference to previous test is\nthat the program actually spends some time at each list\nnode (about 160 cycles). As we learned from the data in\nFigure 3.15, the program’s performance suffers badly as\nsoon as the working set size is larger than the last-level\ncache.\nUlrich Drepper Version 1.0 61\nWe can now try to improve the situation by issuing pre-\nfetch requests ahead of the computation. I.e., in each\nround of the loop we prefetch a new element. The dis-\ntance between the prefetched node in the list and the node\nwhich is currently worked on must be carefully chosen.\nGiven that each node is processed in 160 cycles and that\nwe have to prefetch two cache lines ( NPAD =31), a dis-\ntance of ﬁve list elements is enough.\nThe results in Figure 6.7 show that the prefetch does in-\ndeed help. As long as the working set size does not ex-\nceed the size of the last level cache (the machine has\n512kB = 219B of L2) the numbers are identical. The\nprefetch instructions do not add a measurable extra bur-\nden. As soon as the L2 size is exceeded the prefetching\nsaves between 50 to 60 cycles, up to 8%. The use of\nprefetch cannot hide all the penalties but it does help a\nbit.\nAMD implements, in their family 10h of the Opteron\nline, another instruction: prefetchw . This instruction\nhas so far no equivalent on the Intel side and is not avail-\nable through intrinsics. The prefetchw instruction tells\nthe CPU to prefetch the cache line into L1 just like the\nother prefetch instructions. The difference is that the\ncache line is immediately put into ’M’ state. This will be\na disadvantage if no write to the cache line follows later.\nIf there are one or more writes, they will be accelerated\nsince the writes do not have to change the cache state–\nthat happened when the cache line was prefetched. This\nis especially important for contended cache lines where a\nsimple read of a cache line in another processor’s cache\nwould ﬁrst change the state to ’S’ in both caches.\nPrefetching can have bigger advantages than the mea-\nger 8% we achieved here. But it is notoriously hard\nto do right, especially if the same binary is supposed\nto perform well on a variety of machines. The perfor-\nmance counters provided by the CPU can help the pro-\ngrammer to analyze prefetches. Events which can be\ncounted and sampled include hardware prefetches, soft-\nware prefetches, useful/used software prefetches, cache\nmisses at the various levels, and more. In section 7.1 we\nwill introduce a number of these events. All these coun-\nters are machine speciﬁc.\nWhen analyzing programs one should ﬁrst look at the\ncache misses. When a large source of cache misses is\nlocated one should try to add a prefetch instruction for\nthe problematic memory accesses. This should be done\nin one place at a time. The result of each modiﬁca-\ntion should be checked by observing the performance\ncounters measuring useful prefetch instructions. If those\ncounters do not increase the prefetch might be wrong, it\nis not given enough time to load from memory, or the pre-\nfetch evicts memory from the cache which is still needed.\ngcc today is able to emit prefetch instructions automati-\ncally in one situation. If a loop is iterating over an array\nthe following option can be used:\n-fprefetch-loop-arraysThe compiler will ﬁgure out whether prefetching makes\nsense and, if so, how far ahead it should look. For small\narrays this can be a disadvantage and, if the size of the\narray is not known at compile time, the results might be\nworse. The gcc manual warns that the beneﬁts highly\ndepend on the form of the code and that in some situation\nthe code might actually run slower. Programmers have to\nuse this option carefully.\n6.3.3 Special Kind of Prefetch: Speculation\nThe OOO execution capability of a modern processor al-\nlows moving instructions around if they do not conﬂict\nwith each other. For instance (using this time IA-64 for\nthe example):\nst8 [r4] = 12\nadd r5 = r6, r7;;\nst8 [r18] = r5\nThis code sequence stores 12 at the address speciﬁed by\nregister r4, adds the content of registers r6andr7and\nstores it in register r5. Finally it stores the sum at the\naddress speciﬁed by register r18. The point here is that\nthe add instruction can be executed before–or at the same\ntime as–the ﬁrst st8 instruction since there is no data\ndependency. But what happens if one of the addends has\nto be loaded?\nst8 [r4] = 12\nld8 r6 = [r8];;\nadd r5 = r6, r7;;\nst8 [r18] = r5\nThe extra ld8 instruction loads the value from the ad-\ndress speciﬁed by the register r8. There is an obvious\ndata dependency between this load instruction and the\nfollowing add instruction (this is the reason for the ;;\nafter the instruction, thanks for asking). What is criti-\ncal here is that the new ld8 instruction–unlike the add\ninstruction–cannot be moved in front of the ﬁrst st8.\nThe processor cannot determine quickly enough during\nthe instruction decoding whether the store and load con-\nﬂict, i.e., whether r4andr8might have same value. If\nthey do have the same value, the st8 instruction would\ndetermine the value loaded into r6. What is worse, the\nld8 might also bring with it a large latency in case the\nload misses the caches. The IA-64 architecture supports\nspeculative loads for this case:\nld8.a r6 = [r8];;\n[... other instructions ...]\nst8 [r4] = 12\n62 Version 1.0 What Every Programmer Should Know About Memory\nld8.c.clr r6 = [r8];;\nadd r5 = r6, r7;;\nst8 [r18] = r5\nThe new ld8.a andld8.c.clr instructions belong to-\ngether and replace the ld8 instruction in the previous\ncode sequence. The ld8.a instruction is the speculative\nload. The value cannot be used directly but the processor\ncan start the work. At the time when the ld8.c.clr in-\nstruction is reached the content might have been loaded\nalready (given there is a sufﬁcient number of instruc-\ntions in the gap). The arguments for this instruction must\nmatch that for the ld8.a instruction. If the preceding\nst8instruction does not overwrite the value (i.e., r4and\nr8are the same), nothing has to be done. The speculative\nload does its job and the latency of the load is hidden. If\nthe store and load do conﬂict the ld8.c.clr reloads the\nvalue from memory and we end up with the semantics of\na normal ld8instruction.\nSpeculative loads are not (yet?) widely used. But as the\nexample shows it is a very simple yet effective way to\nhide latencies. Prefetching is basically equivalent and,\nfor processors with few registers, speculative loads prob-\nably do not make much sense. Speculative loads have\nthe (sometimes big) advantage of loading the value di-\nrectly into the register and not into the cache line where\nit might be evicted again (for instance, when the thread\nis descheduled). If speculation is available it should be\nused.\n6.3.4 Helper Threads\nWhen one tries to use software prefetching one often runs\ninto problems with the complexity of the code. If the\ncode has to iterate over a data structure (a list in our case)\none has to implement two independent iterations in the\nsame loop: the normal iteration doing the work and the\nsecond iteration, which looks ahead, to use prefetching.\nThis easily gets complex enough that mistakes are likely.\nFurthermore, it is necessary to determine how far to look\nahead. Too little and the memory will not be loaded in\ntime. Too far and the just loaded data might have been\nevicted again. Another problem is that prefetch instruc-\ntions, although they do not block and wait for the mem-\nory to be loaded, take time. The instruction has to be\ndecoded, which might be noticeable if the decoder is too\nbusy, for instance, due to well written/generated code.\nFinally, the code size of the loop is increased. This de-\ncreases the L1i efﬁciency. If one tries to avoid parts of\nthis cost by issuing multiple prefetch requests in a row\n(in case the second load does not depend on the result\nof the ﬁrst) one runs into problems with the number of\noutstanding prefetch requests.\nAn alternative approach is to perform the normal oper-\nation and the prefetch completely separately. This can\nhappen using two normal threads. The threads must ob-viously be scheduled so that the prefetch thread is pop-\nulating a cache accessed by both threads. There are two\nspecial solutions worth mentioning:\n• Use hyper-threads (see page 29) on the same core.\nIn this case the prefetch can go into L2 (or even\nL1d).\n• Use “dumber” threads than SMT threads which\ncan do nothing but prefetch and other simple oper-\nations. This is an option processor manufacturers\nmight explore.\nThe use of hyper-threads is particularly intriguing. As\nwe have seen on page 29, the sharing of caches is a prob-\nlem if the hyper-threads execute independent code. If,\ninstead, one thread is used as a prefetch helper thread\nthis is not a problem. To the contrary, it is the desired\neffect since the lowest level cache is preloaded. Further-\nmore, since the prefetch thread is mostly idle or wait-\ning for memory, the normal operation of the other hyper-\nthread is not disturbed much if it does not have to access\nmain memory itself. The latter is exactly what the pre-\nfetch helper thread prevents.\nThe only tricky part is to ensure that the helper thread is\nnot running too far ahead. It must not completely pollute\nthe cache so that the oldest prefetched values are evicted\nagain. On Linux, synchronization is easily done using\nthefutex system call [7] or, at a little bit higher cost,\nusing the POSIX thread synchronization primitives.\n0100200300400500600700800900100011001200\n210213216219222225228\nWorking Set Size (Bytes)Cycles/List Element\nOne Thread With Prefetch With Helper Thread\nFigure 6.8: Average with Helper Thread, NPAD=31\nThe beneﬁts of the approach can be seen in Figure 6.8.\nThis is the same test as in Figure 6.7 only with the ad-\nditional result added. The new test creates an additional\nhelper thread which runs about 100 list entries ahead and\nreads (not only prefetches) all the cache lines of each list\nelement. In this case we have two cache lines per list el-\nement ( NPAD =31 on a 32-bit machine with 64 byte cache\nline size).\nUlrich Drepper Version 1.0 63\nSouthbridge EthernetNorthbridge RAMCPU\n(a) DMA InitiatedSouthbridge EthernetNorthbridge RAMCPU\n(b) DMA and DCA Executed\nFigure 6.9: Direct Cache Access\nThe two threads are scheduled on two hyper-threads of\nthe same core. The test machine has only one core but\nthe results should be about the same if there is more than\none core. The afﬁnity functions, which we will introduce\nin section 6.4.3, are used to tie the threads down to the\nappropriate hyper-thread.\nTo determine which two (or more) processors the OS\nknows are hyper-threads, the NUMA_cpu_level_mask\ninterface from libNUMA can be used (see Appendix D).\n#include <libNUMA.h>\nssize_t NUMA_cpu_level_mask(size_t destsize,\ncpu_set_t *dest,\nsize_t srcsize,\nconst cpu_set_t *src,\nunsigned int level);\nThis interface can be used to determine the hierarchy of\nCPUs as they are connected through caches and mem-\nory. Of interest here is level 1 which corresponds to\nhyper-threads. To schedule two threads on two hyper-\nthreads the libNUMA functions can be used (error han-\ndling dropped for brevity):\ncpu_set_t self;\nNUMA_cpu_self_current_mask(sizeof(self),\n&self);\ncpu_set_t hts;\nNUMA_cpu_level_mask(sizeof(hts), &hts,\nsizeof(self), &self, 1);\nCPU_XOR(&hts, &hts, &self);\nAfter this code is executed we have two CPU bit sets.\nself can be used to set the afﬁnity of the current thread\nand the mask in hts can be used to set the afﬁnity of\nthe helper thread. This should ideally happen before the\nthread is created. In section 6.4.3 we will introduce the\ninterface to set the afﬁnity. If there is no hyper-thread\navailable the NUMA_cpu_level_mask function will re-\nturn 1. This can be used as a sign to avoid this optimiza-\ntion.\nThe result of this benchmark might be surprising (or per-\nhaps not). If the working set ﬁts into L2, the overheadof the helper thread reduces the performance by between\n10% and 60% (mostly at the lower end, ignore the small-\nest working set sizes again, the noise is too high). This\nshould be expected since, if all the data is already in the\nL2 cache, the prefetch helper thread only uses system re-\nsources without contributing to the execution.\nOnce the L2 size is not sufﬁcient is exhausted the pic-\nture changes, though. The prefetch helper thread helps\nto reduce the runtime by about 25%. We still see a ris-\ning curve simply because the prefetches cannot be pro-\ncessed fast enough. The arithmetic operations performed\nby the main thread and the memory load operations of\nthe helper thread do complement each other, though. The\nresource collisions are minimal which causes this syner-\ngistic effect.\nThe results of this test should be transferable to many\nother situations. Hyper-threads, often not useful due to\ncache pollution, shine in these situations and should be\ntaken advantage of. The NUMA library introduced in\nAppendix D makes ﬁnding thread siblings very easy (see\nthe example in that appendix). If the library is not avail-\nable the sys ﬁle system allows a program to ﬁnd the\nthread siblings (see the thread_siblings column in\nTable 5.3). Once this information is available the pro-\ngram just has to deﬁne the afﬁnity of the threads and\nthen run the loop in two modes: normal operation and\nprefetching. The amount of memory prefetched should\ndepend on the size of the shared cache. In this example\nthe L2 size is relevant and the program can query the size\nusing\nsysconf(_SC_LEVEL2_CACHE_SIZE)\nWhether or not the progress of the helper thread must be\nrestricted depends on the program. In general it is best to\nmake sure there is some synchronization since schedul-\ning details could otherwise cause signiﬁcant performance\ndegradations.\n6.3.5 Direct Cache Access\nOne sources of cache misses in a modern OS is the han-\ndling of incoming data trafﬁc. Modern hardware, like\nNetwork Interface Cards (NICs) and disk controllers, has\nthe ability to write the received or read data directly into\n64 Version 1.0 What Every Programmer Should Know About Memory",24740
31-6.4.1 Concurrency Optimizations.pdf,31-6.4.1 Concurrency Optimizations,"memory without involving the CPU. This is crucial for\nthe performance of the devices we have today, but it also\ncauses problems. Assume an incoming packet from a\nnetwork: the OS has to decide how to handle it by look-\ning at the header of the packet. The NIC places the packet\ninto memory and then notiﬁes the processor about the ar-\nrival. The processor has no chance to prefetch the data\nsince it does not know when the data will arrive, and\nmaybe not even where exactly it will be stored. The re-\nsult is a cache miss when reading the header.\nIntel has added technology in their chipsets and CPUs\nto alleviate this problem [14]. The idea is to populate\nthe cache of the CPU which will be notiﬁed about the\nincoming packet with the packet’s data. The payload of\nthe packet is not critical here, this data will, in general, be\nhandled by higher-level functions, either in the kernel or\nat user level. The packet header is used to make decisions\nabout the way the packet has to be handled and so this\ndata is needed immediately.\nThe network I/O hardware already has DMA to write the\npacket. That means it communicates directly with the\nmemory controller which potentially is integrated in the\nNorthbridge. Another side of the memory controller is\nthe interface to the processors through the FSB (assum-\ning the memory controller is not integrated into the CPU\nitself).\nThe idea behind Direct Cache Access (DCA) is to ex-\ntend the protocol between the NIC and the memory con-\ntroller. In Figure 6.9 the ﬁrst ﬁgure shows the beginning\nof the DMA transfer in a regular machine with North-\nand Southbridge. The NIC is connected to (or is part of)\nthe Southbridge. It initiates the DMA access but pro-\nvides the new information about the packet header which\nshould be pushed into the processor’s cache.\nThe traditional behavior would be, in step two, to simply\ncomplete the DMA transfer with the connection to the\nmemory. For the DMA transfers with the DCA ﬂag set\nthe Northbridge additionally sends the data on the FSB\nwith a special, new DCA ﬂag. The processor always\nsnoops the FSB and, if it recognizes the DCA ﬂag, it tries\nto load the data directed to the processor into the lowest\ncache. The DCA ﬂag is, in fact, a hint; the processor can\nfreely ignore it. After the DMA transfer is ﬁnished the\nprocessor is signaled.\nThe OS, when processing the packet, ﬁrst has to deter-\nmine what kind of packet it is. If the DCA hint is not\nignored, the loads the OS has to perform to identify the\npacket most likely hit the cache. Multiply this saving of\nhundreds of cycles per packet with tens of thousands of\npackets which can be processed per second, and the sav-\nings add up to very signiﬁcant numbers, especially when\nit comes to latency.\nWithout the integration of I/O hardware (a NIC in this\ncase), chipset, and CPUs such an optimization is not pos-\nsible. It is therefore necessary to make sure to select the\nplatform wisely if this technology is needed.6.4 Multi-Thread Optimizations\nWhen it comes to multi-threading, there are three differ-\nent aspects of cache use which are important:\n• Concurrency\n• Atomicity\n• Bandwidth\nThese aspects also apply to multi-process situations but,\nbecause multiple processes are (mostly) independent, it\nis not so easy to optimize for them. The possible multi-\nprocess optimizations are a subset of those available for\nthe multi-thread scenario. So we will deal exclusively\nwith the latter here.\nIn this context concurrency refers to the memory effects a\nprocess experiences when running more than one thread\nat a time. A property of threads is that they all share\nthe same address space and, therefore, can all access the\nsame memory. In the ideal case, the memory regions\nused by the threads most of the time are distinct, in which\ncase those threads are coupled only lightly (common in-\nput and/or output, for instance). If more than one thread\nuses the same data, coordination is needed; this is when\natomicity comes into play. Finally, depending on the\nmachine architecture, the available memory and inter-\nprocessor bus bandwidth available to the processors is\nlimited. We will handle these three aspects separately\nin the following sections–although they are, of course,\nclosely linked.\n6.4.1 Concurrency Optimizations\nInitially, in this section, we will discuss two separate is-\nsues which actually require contradictory optimizations.\nA multi-threaded application uses common data in some\nof its threads. Normal cache optimization calls for keep-\ning data together so that the footprint of the application\nis small, thus maximizing the amount of memory which\nﬁts into the caches at any one time.\nThere is a problem with this approach, though: if mul-\ntiple threads write to a memory location, the cache line\nmust be in ‘E’ (exclusive) state in the L1d of each respec-\ntive core. This means that a lot of RFO messages are sent,\nin the worst case one for each write access. So a normal\nwrite will be suddenly very expensive. If the same mem-\nory location is used, synchronization is needed (maybe\nthrough the use of atomic operations, which is handled\nin the next section). The problem is also visible, though,\nwhen all the threads are using different memory locations\nand are supposedly independent.\nFigure 6.10 shows the results of this “false sharing”. The\ntest program (shown in section A.3) creates a number of\nthreads which do nothing but increment a memory loca-\ntion (500 million times). The measured time is from the\nUlrich Drepper Version 1.0 65\n024681012141618\n1 2 3 4\nNumber of ThreadsTime (sec)\nFigure 6.10: Concurrent Cache Line Access Overhead\nprogram start until the program ﬁnishes after waiting for\nthe last thread. The threads are pinned to individual pro-\ncessors. The machine has four P4 processors. The blue\nvalues represent runs where the memory allocations as-\nsigned to each thread are on separate cache lines. The\nred part is the penalty occurred when the locations for\nthe threads are moved to just one cache line.\nThe blue measurements (time needed when using indi-\nvidual cache lines) match what one would expect. The\nprogram scales without penalty to many threads. Each\nprocessor keeps its cache line in its own L1d and there\nare no bandwidth issues since not much code or data has\nto be read (in fact, it is all cached). The measured slight\nincrease is really system noise and probably some pre-\nfetching effects (the threads use sequential cache lines).\nThe measured overhead, computed by dividing the time\nneeded when using one single cache line versus a sep-\narate cache line for each thread, is 390%, 734%, and\n1,147% respectively. These large numbers might be sur-\nprising at ﬁrst sight but, when thinking about the cache\ninteraction needed, it should be obvious. The cache line\nis pulled from one processor’s cache just after it has ﬁn-\nished writing to the cache line. All processors, except the\none which has the cache line at any given moment, are\ndelayed and cannot do anything. Each additional proces-\nsor will just cause more delays.\nIt is clear from these measurements that this scenario\nmust be avoided in programs. Given the huge penalty,\nthis problem is, in many situations, obvious (proﬁling\nwill show the code location, at least) but there is a pitfall\nwith modern hardware. Figure 6.11 shows the equivalent\nmeasurements when running the code on a single pro-\ncessor, quad core machine (Intel Core 2 QX 6700). Even\nwith this processor’s two separate L2s the test case does\nnot show any scalability issues. There is a slight over-\nhead when using the same cache line more than once\nbut it does not increase with the number of cores.36If\n36I cannot explain the lower number when all four cores are used but\nit is reproducible.012\n1 2 3 4\nNumber of ThreadsTime (sec)\nFigure 6.11: Overhead, Quad Core\nmore than one of these processors were used we would,\nof course, see results similar to those in Figure 6.10. De-\nspite the increasing use of multi-core processors, many\nmachines will continue to use multiple processors and,\ntherefore, it is important to handle this scenario correctly,\nwhich might mean testing the code on real SMP ma-\nchines.\nThere is a very simple “ﬁx” for the problem: put every\nvariable on its own cache line. This is where the conﬂict\nwith the previously mentioned optimization comes into\nplay, speciﬁcally, the footprint of the application would\nincrease a lot. This is not acceptable; it is therefore nec-\nessary to come up with a more intelligent solution.\nWhat is needed is to identify which variables are used by\nonly one thread at a time, those used by only one thread\never, and maybe those which are contested at times. Dif-\nferent solutions for each of these scenarios are possible\nand useful. The most basic criterion for the differentia-\ntion of variables is: are they ever written to and how often\ndoes this happen.\nVariables which are never written to and those which are\nonly initialized once are basically constants. Since RFO\nmessages are only needed for write operations, constants\ncan be shared in the cache (‘S’ state). So, these vari-\nables do not have to be treated specially; grouping them\ntogether is ﬁne. If the programmer marks the variables\ncorrectly with const , the tool chain will move the vari-\nables away from the normal variables into the .rodata\n(read-only data) or .data.rel.ro (read-only after relo-\ncation) section37No other special action is required. If,\nfor some reason, variables cannot be marked correctly\nwith const , the programmer can inﬂuence their place-\nment by assigning them to a special section.\nWhen the linker constructs the ﬁnal binary, it ﬁrst ap-\npends the sections with the same name from all input\nﬁles; those sections are then arranged in an order deter-\nmined by the linker script. This means that, by mov-\ning all variables which are basically constant but are not\nmarked as such into a special section, the programmer\ncan group all of those variables together. There will not\nbe a variable which is often written to between them. By\naligning the ﬁrst variable in that section appropriately,\n37Sections, identiﬁed by their names are the atomic units containing\ncode and data in an ELF ﬁle.\n66 Version 1.0 What Every Programmer Should Know About Memory\nit is possible to guarantee that no false sharing happens.\nAssume this little example:\nint foo = 1;\nint bar __attribute__((section("".data.ro""))) = 2;\nint baz = 3;\nint xyzzy __attribute__((section("".data.ro""))) = 4;\nIf compiled, this input ﬁle deﬁnes four variables. The in-\nteresting part is that the variables foo andbaz, and bar\nandxyzzy are grouped together respectively. Without\nthe attribute deﬁnitions the compiler would allocate all\nfour variables in the sequence in which they are deﬁned\nin the source code the a section named .data .38With the\ncode as-is the variables bar andxyzzy are placed in a\nsection named .data.ro . The section name .data.ro\nis more or less arbitrary. A preﬁx of .data. guarantees\nthat the GNU linker will place the section together with\nthe other data sections.\nThe same technique can be applied to separate out vari-\nables which are mostly read but occasionally also written\nto. Simply choose a different section name. This sepa-\nration seems to make sense in some cases like the Linux\nkernel.\nIf a variable is only ever used by one thread, there is an-\nother way to specify the variable. In this case it is possi-\nble and useful to use thread-local variables (see [8]). The\nC and C++ language in gcc allow variables to be deﬁned\nas per-thread using the __thread keyword.\nint foo = 1;\n__thread int bar = 2;\nint baz = 3;\n__thread int xyzzy = 4;\nThe variables bar andxyzzy are not allocated in the\nnormal data segment; instead each thread has its own\nseparate area where such variables are stored. The vari-\nables can have static initializers. All thread-local vari-\nables are addressable by all other threads but, unless a\nthread passes a pointer to a thread-local variable to those\nother threads, there is no way the other threads can ﬁnd\nthat variable. Due to the variable being thread-local, false\nsharing is not a problem–unless the program artiﬁcially\ncreates a problem. This solution is easy to set up (the\ncompiler and linker do all the work), but it has its cost.\nWhen a thread is created, it has to spend some time on\nsetting up the thread-local variables, which requires time\nand memory. In addition, addressing thread-local vari-\nables is usually more expensive than using global or auto-\nmatic variables (see [8] for explanations of how the costs\nare minimized automatically, if possible).\n38This is not guaranteed by the ISO C standard but it is how gcc\nworks.Another drawback of using thread-local storage (TLS)\nis that, if the use of the variable shifts over to another\nthread, the current value of the variable in the old thread\nis not available to new thread. Each thread’s copy of the\nvariable is distinct. Often this is not a problem at all and,\nif it is, the shift over to the new thread needs coordina-\ntion, at which time the current value can be copied.\nA bigger problem is possible waste of resources. If only\none thread ever uses the variable at any one time, all\nthreads have to pay a price in terms of memory. If a\nthread does not use any TLS variables, the lazy alloca-\ntion of the TLS memory area prevents this from being a\nproblem (except for TLS in the application itself). If a\nthread uses just one TLS variable in a DSO, the memory\nfor all the other TLS variables in this object will be allo-\ncated, too. This could potentially add up if TLS variables\nare used on a large scale.\nIn general the best advice which can be given is\n1. Separate at least read-only (after initialization) and\nread-write variables. Maybe extend this separation\nto read-mostly variables as a third category.\n2. Group read-write variables which are used together\ninto a structure. Using a structure is the only way\nto ensure the memory locations for all of those\nvariables are close together in a way which is trans-\nlated consistently by all gcc versions..\n3. Move read-write variables which are often written\nto by different threads onto their own cache line.\nThis might mean adding padding at the end to ﬁll\na remainder of the cache line. If combined with\nstep 2, this is often not really wasteful. Extending\nthe example above, we might end up with code as\nfollows (assuming barandxyzzy are meant to be\nused together):\nint foo = 1;\nint baz = 3;\nstruct {\nstruct al1 {\nint bar;\nint xyzzy;\n};\nchar pad[CLSIZE - sizeof(struct al1)];\n} rwstruct __attribute__((aligned(CLSIZE))) =\n{ { .bar = 2, .xyzzy = 4 } };\nSome code changes are needed (references to bar\nhave to be replaced with rwstruct.bar , likewise\nforxyzzy ) but that is all. The compiler and linker\ndo all the rest.39\n4. If a variable is used by multiple threads, but every\nuse is independent, move the variable into TLS.\n39So far this code has to be compiled with -fms-extensions on\nthe command line.\nUlrich Drepper Version 1.0 67",15250
32-6.4.2 Atomicity Optimizations.pdf,32-6.4.2 Atomicity Optimizations,"for (i = 0; i < N; ++i)\n__sync_add_and_fetch(&var,1);\n1. Add and Read Resultfor (i = 0; i < N; ++i)\n__sync_fetch_and_add(&var,1);\n2. Add and Return Old Valuefor (i = 0; i < N; ++i) {\nlong v, n;\ndo {\nv = var;\nn = v + 1;\n} while (!__sync_bool_compare_and_swap(&var,\nv,n));\n}\n3. Atomic Replace with New Value\nFigure 6.12: Atomic Increment in a Loop\n6.4.2 Atomicity Optimizations\nIf multiple threads modify the same memory location\nconcurrently, processors do not guarantee any speciﬁc\nresult. This is a deliberate decision made to avoid costs\nwhich are unnecessary in 99.999% of all cases. For in-\nstance, if a memory location is in the ‘S’ state and two\nthreads concurrently have to increment its value, the exe-\ncution pipeline does not have to wait for the cache line to\nbe available in the ‘E’ state before reading the old value\nfrom the cache to perform the addition. Instead it reads\nthe value currently in the cache and, once the cache line\nis available in state ‘E’, the new value is written back.\nThe result is not as expected if the two cache reads in the\ntwo threads happen simultaneously; one addition will be\nlost.\nFor situations where concurrent operations can happen,\nprocessors provide atomic operations. These atomic op-\nerations would, for instance, not read the old value un-\ntil it is clear that the addition could be performed in a\nway that the addition to the memory location appears as\natomic. In addition to waiting for other cores and proces-\nsors, some processors even signal atomic operations for\nspeciﬁc addresses to other devices on the motherboard.\nAll this makes atomic operations slower.\nProcessor vendors decided to provide different sets of\natomic operations. Early RISC processors, in line with\nthe ‘R’ for r educed, provided very few atomic operations,\nsometimes only an atomic bit set and test.40At the other\nend of the spectrum, we have x86 and x86-64 which pro-\nvide a large number of atomic operations. The generally\navailable atomic operations can be categorized in four\nclasses:\nBit Test These operations set or clear a bit atomically\nand return a status indicating whether the bit was\nset before or not.\nLoad Lock/Store Conditional (LL/SC)41The LL/SC\noperations work as a pair where the special load\ninstruction is used to start an transaction and the\nﬁnal store will only succeed if the location has not\nbeen modiﬁed in the meantime. The store oper-\nation indicates success or failure, so the program\ncan repeat its efforts if necessary.\n40HP Parisc still does not provide more. . .\n41Some people use “linked” instead of “lock”, it is all the same.Compare-and-Swap (CAS) This is a ternary operation\nwhich writes a value provided as a parameter into\nan address (the second parameter) only if the cur-\nrent value is the same as the third parameter value;\nAtomic Arithmetic These operations are only available\non x86 and x86-64, which can perform arithmetic\nand logic operations on memory locations. These\nprocessors have support for non-atomic versions of\nthese operations but RISC architectures do not. So\nit is no wonder that their availability is limited.\nAn architecture supports either the LL/SC or the CAS in-\nstruction, not both. Both approaches are basically equiv-\nalent; they allow the implementation of atomic arithmetic\noperations equally well, but CAS seems to be the pre-\nferred method these days. All other operations can be\nindirectly implemented using it. For instance, an atomic\naddition:\nint curval;\nint newval;\ndo {\ncurval = var;\nnewval = curval + addend;\n} while (CAS(&var, curval, newval));\nThe result of the CAScall indicates whether the operation\nsucceeded or not. If it returns failure (non-zero value),\nthe loop is run again, the addition is performed, and the\nCAS call is tried again. This repeats until it is success-\nful. Noteworthy about the code is that the address of the\nmemory location has to be computed in two separate in-\nstructions.42For LL/SC the code looks about the same:\nint curval;\nint newval;\ndo {\ncurval = LL(var);\nnewval = curval + addend;\n} while (SC(var, newval));\n42TheCAS opcode on x86 and x86-64 can avoid the load of the value\nin the second and later iterations but, on this platform, we can write the\natomic addition in a simpler way, with a single addition opcode.\n68 Version 1.0 What Every Programmer Should Know About Memory\nHere we have to use a special load instruction ( LL) and\nwe do not have to pass the current value of the memory\nlocation to SCsince the processor knows if the memory\nlocation has been modiﬁed in the meantime.\nThe big differentiators are x86 and x86-64, where we\nhave the atomic operations and, here, it is important to\nselect the proper atomic operation to achieve the best re-\nsult. Figure 6.12 shows three different ways to imple-\nment an atomic increment operation. All three produce\ndifferent code on x86 and x86-64 while the code might\nbe identical on other architectures. There are huge per-\nformance differences. The following table shows the ex-\necution time for 1 million increments by four concurrent\nthreads. The code uses the built-in primitives of gcc ( _-\n_sync_ *).\n1. Exchange Add 2. Add Fetch 3. CAS\n0.23s 0.21s 0.73s\nThe ﬁrst two numbers are similar; we see that returning\nthe old value is a little bit faster. The important piece of\ninformation is the highlighted ﬁeld, the cost when using\nCAS. It is, unsurprisingly, a lot more expensive. There\nare several reasons for this: 1. there are two memory op-\nerations, 2. the CAS operation by itself is more compli-\ncated and requires even conditional operation, and 3. the\nwhole operation has to be done in a loop in case two con-\ncurrent accesses cause a CAS call to fail.\nNow a reader might ask a question: why would some-\nbody use the complicated and longer code which uti-\nlizes CAS? The answer to this is: the complexity is usu-\nally hidden. As mentioned before, CAS is currently the\nunifying atomic operation across all interesting architec-\ntures. So some people think it is sufﬁcient to deﬁne all\natomic operations in terms of CAS. This makes programs\nsimpler. But as the numbers show, the results can be ev-\nerything but optimal. The memory handling overhead of\nthe CAS solution is huge. The following illustrates the\nexecution of just two threads, each on its own core.\nThread #1 Thread #2 varCache State\nv = var ‘E’ on Proc 1\nn = v + 1 v = var ‘S’ on Proc 1+2\nCAS( var) n = v + 1 ‘E’ on Proc 1\nCAS( var) ‘E’ on Proc 2\nWe see that, within this short period of execution, the\ncache line status changes at least three times; two of the\nchanges are RFOs. Additionally, the second CAS will\nfail, so that thread has to repeat the whole operation. Dur-\ning that operation the same can happen again.\nIn contrast, when the atomic arithmetic operations are\nused, the processor can keep the load and store opera-\ntions needed to perform the addition (or whatever) to-\ngether. It can ensure that concurrently-issued cache line\nrequests are blocked until the atomic operation is done.Each loop iteration in the example therefore results in, at\nmost, one RFO cache request and nothing else.\nWhat all this means is that it is crucial to deﬁne the ma-\nchine abstraction at a level at which atomic arithmetic\nand logic operations can be utilized. CAS should not be\nuniversally used as the uniﬁcation mechanism.\nFor most processors, the atomic operations are, by them-\nselves, always atomic. One can avoid them only by pro-\nviding completely separate code paths for the case when\natomicity is not needed. This means more code, a con-\nditional, and further jumps to direct execution appropri-\nately.\nFor x86 and x86-64 the situation is different: the same\ninstructions can be used in both atomic and non-atomic\nways. To make them atomic, a special preﬁx for the in-\nstruction is used: the lock preﬁx. This opens the door\nfor atomic operations to avoid the high costs if the atom-\nicity requirement in a given situation is not needed. Code\nin libraries, for example, which always has to be thread-\nsafe if needed, can beneﬁt from this. No information is\nneeded when writing the code, the decision can be made\nat runtime. The trick is to jump over the lock preﬁx.\nThis trick applies to all the instructions which the x86\nand x86-64 processor allow to preﬁx with lock .\ncmpl $0, multiple_threads\nje 1f\nlock\n1: add $1, some_var\nIf this assembler code appears cryptic, do not worry, it\nis simple. The ﬁrst instruction checks whether a vari-\nable is zero or not. Nonzero in this case indicates that\nmore than one thread is running. If the value is zero,\nthe second instruction jumps to label 1. Otherwise, the\nnext instruction is executed. This is the tricky part. If\nthejeinstruction does not jump, the add instruction is\nexecuted with the lock preﬁx. Otherwise it is executed\nwithout the lock preﬁx.\nAdding a potentially expensive operation like a condi-\ntional jump (expensive in case the branch prediction is\nwrong) seems to be counter productive. Indeed it can be:\nif multiple threads are running most of the time, the per-\nformance is further decreased, especially if the branch\nprediction is not correct. But if there are many situa-\ntions where only one thread is in use, the code is sig-\nniﬁcantly faster. The alternative of using an if-then-else\nconstruct introduces an additional unconditional jump in\nboth cases which can be slower. Given that an atomic\noperation costs on the order of 200 cycles, the cross-\nover point for using the trick (or the if-then-else block)\nis pretty low. This is deﬁnitely a technique to be kept in\nmind. Unfortunately this means gcc’s __sync_ * primi-\ntives cannot be used.\nUlrich Drepper Version 1.0 69",9815
33-6.4.3 Bandwidth Considerations.pdf,33-6.4.3 Bandwidth Considerations,"6.4.3 Bandwidth Considerations\nWhen many threads are used, and they do not cause cache\ncontention by using the same cache lines on different\ncores, there still are potential problems. Each proces-\nsor has a maximum bandwidth to the memory which is\nshared by all cores and hyper-threads on that processor.\nDepending on the machine architecture (e.g., the one in\nFigure 2.1), multiple processors might share the same\nbus to memory or the Northbridge.\nThe processor cores themselves run at frequencies where,\nat full speed, even in perfect conditions, the connection\nto the memory cannot fulﬁll all load and store requests\nwithout waiting. Now, further divide the available band-\nwidth by the number of cores, hyper-threads, and pro-\ncessors sharing a connection to the Northbridge and sud-\ndenly parallelism becomes a big problem. Efﬁcient pro-\ngrams may be limited in their performance by the avail-\nable memory bandwidth.\nFigure 3.32 shows that increasing the FSB speed of a pro-\ncessor can help a lot. This is why, with growing numbers\nof cores on a processor, we will also see an increase in\nthe FSB speed. Still, this will never be enough if the\nprogram uses large working sets and it is sufﬁciently op-\ntimized. Programmers have to be prepared to recognize\nproblems due to limited bandwidth.\nThe performance measurement counters of modern pro-\ncessors allow the observation of FSB contention. On\nCore 2 processors the NUS_BNR_DRV event counts the\nnumber of cycles a core has to wait because the bus is\nnot ready. This indicates that the bus is highly used and\nloads from or stores to main memory take even longer\nthan usual. The Core 2 processors support more events\nwhich can count speciﬁc bus actions like RFOs or the\ngeneral FSB utilization. The latter might come in handy\nwhen investigating the possibility of scalability of an ap-\nplication during development. If the bus utilization rate\nis already close to 1.0 then the scalability opportunities\nare minimal.\nIf a bandwidth problem is recognized, there are several\nthings which can be done. They are sometimes con-\ntradictory so some experimentation might be necessary.\nOne solution is to buy faster computers, if there are some\navailable. Getting more FSB speed, faster RAM mod-\nules, and possibly memory local to the processor, can–\nand probably will–help. It can cost a lot, though. If the\nprogram in question is only needed on one (or a few ma-\nchines) the one-time expense for the hardware might cost\nless than reworking the program. In general, though, it is\nbetter to work on the program.\nAfter optimizing the program code itself to avoid cache\nmisses, the only option left to achieve better bandwidth\nutilization is to place the threads better on the available\ncores. By default, the scheduler in the kernel will assign\na thread to a processor according to its own policy. Mov-\ning a thread from one core to another is avoided whenpossible. The scheduler does not really know anything\nabout the workload, though. It can gather information\nfrom cache misses etc but this is not much help in many\nsituations.\nCore 1 Core 2\nCacheCore 3 Core 4\nCache\nMemory\nFigure 6.13: Inefﬁcient Scheduling\nOne situation which can cause big memory bus usage is\nwhen two threads are scheduled on different processors\n(or cores in different cache domains) and they use the\nsame data set. Figure 6.13 shows such a situation. Core 1\nand 3 access the same data (indicated by the same color\nfor the access indicator and the memory area). Similarly\ncore 2 and 4 access the same data. But the threads are\nscheduled on different processors. This means each data\nset has to be read twice from memory. This situation can\nbe handled better.\nCore 1 Core 2\nCacheCore 3 Core 4\nCache\nMemory\nFigure 6.14: Efﬁcient Scheduling\nIn Figure 6.14 we see how it should ideally look like.\nNow the total cache size in use is reduced since now\ncore 1 and 2 and core 3 and 4 work on the same data.\nThe data sets have to be read from memory only once.\nThis is a simple example but, by extension, it applies to\nmany situations. As mentioned before, the scheduler in\nthe kernel has no insight into the use of data, so the pro-\ngrammer has to ensure that scheduling is done efﬁciently.\nThere are not many kernel interfaces available to commu-\nnicate this requirement. In fact, there is only one: deﬁn-\ning thread afﬁnity.\nThread afﬁnity means assigning a thread to one or more\ncores. The scheduler will then choose among those cores\n(only) when deciding where to run the thread. Even if\nother cores are idle they will not be considered. This\nmight sound like a disadvantage, but it is the price one\nhas to pay. If too many threads exclusively run on a set\nof cores the remaining cores might mostly be idle and\nthere is nothing one can do except change the afﬁnity.\nBy default threads can run on any core.\nThere are a number of interfaces to query and change the\nafﬁnity of a thread:\n70 Version 1.0 What Every Programmer Should Know About Memory\n#define _GNU_SOURCE\n#include <sched.h>\nint sched_setaffinity(pid_t pid, size_t size,\nconst cpu_set_t *cpuset);\nint sched_getaffinity(pid_t pid, size_t size,\ncpu_set_t *cpuset);\nThese two interfaces are meant to be used for single-\nthreaded code. The pid argument speciﬁes which pro-\ncess’s afﬁnity should be changed or determined. The\ncaller obviously needs appropriate privileges to do this.\nThe second and third parameter specify the bitmask for\nthe cores. The ﬁrst function requires the bitmask to be\nﬁlled in so that it can set the afﬁnity. The second ﬁlls\nin the bitmask with the scheduling information of the se-\nlected thread. The interfaces are declared in <sched.h> .\nThecpu_set_t type is also deﬁned in that header, along\nwith a number of macros to manipulate and use objects\nof this type.\n#define _GNU_SOURCE\n#include <sched.h>\n#define CPU_SETSIZE\n#define CPU_SET(cpu, cpusetp)\n#define CPU_CLR(cpu, cpusetp)\n#define CPU_ZERO(cpusetp)\n#define CPU_ISSET(cpu, cpusetp)\n#define CPU_COUNT(cpusetp)\nCPU_SETSIZE speciﬁes how many CPUs can be rep-\nresented in the data structure. The other three macros\nmanipulate cpu_set_t objects. To initialize an object\nCPU_ZERO should be used; the other two macros should\nbe used to select or deselect individual cores. CPU_-\nISSET tests whether a speciﬁc processor is part of the\nset.CPU_COUNT returns the number of cores selected in\nthe set. The cpu_set_t type provide a reasonable de-\nfault value for the upper limit on the number of CPUs.\nOver time it certainly will prove too small; at that point\nthe type will be adjusted. This means programs always\nhave to keep the size in mind. The above convenience\nmacros implicitly handle the size according to the deﬁ-\nnition of cpu_set_t . If more dynamic size handling is\nneeded an extended set of macros should be used:\n#define _GNU_SOURCE\n#include <sched.h>\n#define CPU_SET_S(cpu, setsize, cpusetp)\n#define CPU_CLR_S(cpu, setsize, cpusetp)\n#define CPU_ZERO_S(setsize, cpusetp)\n#define CPU_ISSET_S(cpu, setsize, cpusetp)\n#define CPU_COUNT_S(setsize, cpusetp)\nThese interfaces take an additional parameter with the\nsize. To be able to allocate dynamically sized CPU sets\nthree macros are provided:#define _GNU_SOURCE\n#include <sched.h>\n#define CPU_ALLOC_SIZE(count)\n#define CPU_ALLOC(count)\n#define CPU_FREE(cpuset)\nThe return value of the CPU_ALLOC_SIZE macro is the\nnumber of bytes which have to be allocated for a cpu_-\nset_t structure which can handle count CPUs. To al-\nlocate such a block the CPU_ALLOC macro can be used.\nThe memory allocated this way must be freed with a call\ntoCPU_FREE . These macros will likely use malloc and\nfree behind the scenes but this does not necessarily have\nto remain this way.\nFinally, a number of operations on CPU set objects are\ndeﬁned:\n#define _GNU_SOURCE\n#include <sched.h>\n#define CPU_EQUAL(cpuset1, cpuset2)\n#define CPU_AND(destset, cpuset1, cpuset2)\n#define CPU_OR(destset, cpuset1, cpuset2)\n#define CPU_XOR(destset, cpuset1, cpuset2)\n#define CPU_EQUAL_S(setsize, cpuset1, cpuset2)\n#define CPU_AND_S(setsize, destset, cpuset1,\ncpuset2)\n#define CPU_OR_S(setsize, destset, cpuset1,\ncpuset2)\n#define CPU_XOR_S(setsize, destset, cpuset1,\ncpuset2)\nThese two sets of four macros can check two sets for\nequality and perform logical AND, OR, and XOR op-\nerations on sets. These operations come in handy when\nusing some of the libNUMA functions (see Appendix D).\nA process can determine on which processor it is cur-\nrently running using the sched_getcpu interface:\n#define _GNU_SOURCE\n#include <sched.h>\nint sched_getcpu(void);\nThe result is the index of the CPU in the CPU set. Due\nto the nature of scheduling this number cannot always be\n100% correct. The thread might have been moved to a\ndifferent CPU between the time the result was returned\nand when the thread returns to userlevel. Programs al-\nways have to take this possibility of inaccuracy into ac-\ncount. More important is, in any case, the set of CPUs the\nthread is allowed to run on. This set can be retrieved us-\ningsched_getaffinity . The set is inherited by child\nthreads and processes. Threads cannot rely on the set to\nbe stable over the lifetime. The afﬁnity mask can be set\nfrom the outside (see the pidparameter in the prototypes\nUlrich Drepper Version 1.0 71",9420
34-6.5.1 Memory Policy.pdf,34-6.5.1 Memory Policy,"above); Linux also supports CPU hot-plugging which\nmeans CPUs can vanish from the system–and, therefore,\nalso from the afﬁnity CPU set.\nIn multi-threaded programs, the individual threads of-\nﬁcially have no process ID as deﬁned by POSIX and,\ntherefore, the two functions above cannot be used. In-\nstead <pthread.h> declares four different interfaces:\n#define _GNU_SOURCE\n#include <pthread.h>\nint pthread_setaffinity_np(pthread_t th,\nsize_t size,\nconst cpu_set_t *cpuset);\nint pthread_getaffinity_np(pthread_t th,\nsize_t size,\ncpu_set_t *cpuset);\nint pthread_attr_setaffinity_np(\npthread_attr_t *at,\nsize_t size,\nconst cpu_set_t *cpuset);\nint pthread_attr_getaffinity_np(\npthread_attr_t *at,\nsize_t size,\ncpu_set_t *cpuset);\nThe ﬁrst two interfaces are basically equivalent to the two\nwe have already seen, except that they take a thread han-\ndle in the ﬁrst parameter instead of a process ID. This\nallows addressing individual threads in a process. It also\nmeans that these interfaces cannot be used from another\nprocess, they are strictly for intra-process use. The third\nand fourth interfaces use a thread attribute. These at-\ntributes are used when creating a new thread. By setting\nthe attribute, a thread can be scheduled from the start on a\nspeciﬁc set of CPUs. Selecting the target processors this\nearly–instead of after the thread already started–can be\nof advantage on many different levels, including (and es-\npecially) memory allocation (see NUMA in section 6.5).\nSpeaking of NUMA, the afﬁnity interfaces play a big role\nin NUMA programming, too. We will come back to that\ncase shortly.\nSo far, we have talked about the case where the working\nset of two threads overlaps such that having both threads\non the same core makes sense. The opposite can be true,\ntoo. If two threads work on separate data sets, having\nthem scheduled on the same core can be a problem. Both\nthreads ﬁght for the same cache, thereby reducing each\nothers effective use of the cache. Second, both data sets\nhave to be loaded into the same cache; in effect this in-\ncreases the amount of data that has to be loaded and,\ntherefore, the available bandwidth is cut in half.\nThe solution in this case is to set the afﬁnity of the threads\nso that they cannot be scheduled on the same core. This is\nthe opposite from the previous situation, so it is important\nto understand the situation one tries to optimize before\nmaking any changes.\nOptimizing for cache sharing to optimize bandwidth is inreality an aspect of NUMA programming which is cov-\nered in the next section. One only has to extend the no-\ntion of “memory” to the caches. This will become ever\nmore important once the number of levels of cache in-\ncreases. For this reason, a solution to multi-core schedul-\ning is available in the NUMA support library. See the\ncode samples in Appendix D for ways to determine the\nafﬁnity masks without hardcoding system details or div-\ning into the depth of the /sys ﬁlesystem.\n6.5 NUMA Programming\nFor NUMA programming everything said so far about\ncache optimizations applies as well. The differences only\nstart below that level. NUMA introduces different costs\nwhen accessing different parts of the address space. With\nuniform memory access we can optimize to minimize\npage faults (see section 7.5) but that is about it. All pages\nare created equal.\nNUMA changes this. Access costs can depend on the\npage which is accessed. Differing access costs also in-\ncrease the importance of optimizing for memory page\nlocality. NUMA is inevitable for most SMP machines\nsince both Intel with CSI (for x86,x86-64, and IA-64)\nand AMD (for Opteron) use it. With an increasing num-\nber of cores per processor we are likely to see a sharp\nreduction of SMP systems being used (at least outside\ndata centers and ofﬁces of people with terribly high CPU\nusage requirements). Most home machines will be ﬁne\nwith just one processor and hence no NUMA issues. But\nthis a) does not mean programmers can ignore NUMA\nand b) it does not mean there are not related issues.\nIf one thinks about generalizations to NUMA one quickly\nrealizes the concept extends to processor caches as well.\nTwo threads on cores using the same cache will collabo-\nrate faster than threads on cores not sharing a cache. This\nis not a fabricated case:\n• early dual-core processors had no L2 sharing.\n• Intel’s Core 2 QX 6700 and QX 6800 quad core\nchips, for instance, have two separate L2 caches.\n• as speculated early, with more cores on a chip and\nthe desire to unify caches, we will have more levels\nof caches.\nCaches form their own hierarchy; placement of threads\non cores becomes important for sharing (or not) of the\nvarious caches. This is not very different from the prob-\nlems NUMA is facing and, therefore, the two concepts\ncan be uniﬁed. Even people only interested in non-SMP\nmachines should therefore read this section.\nIn section 5.3 we have seen that the Linux kernel pro-\nvides a lot of information which is useful–and needed–in\nNUMA programming. Collecting this information is not\nthat easy, though. The currently available NUMA library\n72 Version 1.0 What Every Programmer Should Know About Memory",5244
35-6.5.3 Swapping and Policies.pdf,35-6.5.3 Swapping and Policies,"on Linux is wholly inadequate for this purpose. A much\nmore suitable version is currently under construction by\nthe author.\nThe existing NUMA library, libnuma , part of the nu-\nmactl package, provides no access to system architecture\ninformation. It is only a wrapper around the available\nsystem calls together with some convenience interfaces\nfor commonly used operations. The system calls avail-\nable on Linux today are:\nmbind Select binding of speciﬁed memory pages.\nset_mempolicy Set the default memory binding pol-\nicy.\nget_mempolicy Get the default memory binding pol-\nicy.\nmigrate_pages Migrate all pages of a process on a\ngiven set of nodes to a different set of nodes.\nmove_pages Move selected pages to given node or re-\nquest node information about pages.\nThese interfaces are declared in the <numaif.h> header\nwhich comes along with the libnuma library. Before we\ngo into more details we have to understand the concept\nof memory policies.\n6.5.1 Memory Policy\nThe idea behind deﬁning a memory policy is to allow\nexisting code to work reasonably well in a NUMA en-\nvironment without major modiﬁcations. The policy is\ninherited by child processes, which makes it possible to\nuse the numactl tool. This tool can be used to, among\nother things, start a program with a given policy.\nThe Linux kernel supports the following policies:\nMPOL_BIND Memory is allocated only from the given\nset of nodes. If this is not possible allocation fails.\nMPOL_PREFERRED Memory is preferably allocated from\nthe given set of nodes. If this fails memory from\nother nodes is considered.\nMPOL_INTERLEAVE Memory is allocated equally from\nthe speciﬁed nodes. The node is selected either by\nthe offset in the virtual memory region for VMA-\nbased policies, or through a free-running counter\nfor task-based policies.\nMPOL_DEFAULT Choose the allocation based on the de-\nfault for the region.\nThis list seems to recursively deﬁne policies. This is half\ntrue. In fact, memory policies form a hierarchy (see Fig-\nure 6.15). If an address is covered by a VMA policy then\nthis policy is used. A special kind of policy is used forSystem Default Policy\nTask Policy\nVMA Policy ShMem Policy\nFigure 6.15: Memory Policy Hierarchy\nshared memory segments. If no policy for the speciﬁc\naddress is present, the task’s policy is used. If this is also\nnot present the system’s default policy is used.\nThe system default is to allocate memory local to the\nthread requesting the memory. No task and VMA poli-\ncies are provided by default. For a process with multiple\nthreads the local node is the “home” node, the one which\nﬁrst ran the process. The system calls mentioned above\ncan be used to select different policies.\n6.5.2 Specifying Policies\nTheset_mempolicy call can be used to set the task pol-\nicy for the current thread (task in kernel-speak). Only the\ncurrent thread is affected, not the entire process.\n#include <numaif.h>\nlong set_mempolicy(int mode,\nunsigned long *nodemask,\nunsigned long maxnode);\nThemode parameter must be one of the MPOL_ *con-\nstants introduced in the previous section. The nodemask\nparameter speciﬁes the memory nodes to use for future\nallocations and maxnode is the number of nodes (i.e.,\nbits) in nodemask . Ifmode isMPOL_DEFAULT no mem-\nory nodes need to be speciﬁed and the nodemask param-\neter is ignored. If a null pointer is passed as nodemask\nforMPOL_PREFERRED the local node is selected. Oth-\nerwise MPOL_PREFERRED uses the lowest node number\nwith the corresponding bit set in nodemask .\nSetting a policy does not have any effect on already-\nallocated memory. Pages are not automatically migrated;\nonly future allocations are affected. Note the difference\nbetween memory allocation and address space reserva-\ntion: an address space region established using mmap\nis usually not automatically allocated. The ﬁrst read or\nwrite operation on the memory region will allocate the\nappropriate page. If the policy changes between accesses\nto different pages of the same address space region, or\nif the policy allows allocation of memory from different\nnodes, a seemingly uniform address space region might\nbe scattered across many memory nodes.\nUlrich Drepper Version 1.0 73",4257
36-6.5.6 CPU and Node Sets.pdf,36-6.5.6 CPU and Node Sets,"6.5.3 Swapping and Policies\nIf physical memory runs out, the system has to drop clean\npages and save dirty pages to swap. The Linux swap im-\nplementation discards node information when it writes\npages to swap. That means when the page is reused and\npaged in the node which is used will be chosen from\nscratch. The policies for the thread will likely cause a\nnode which is close to the executing processors to be\nchosen, but the node might be different from the one used\nbefore.\nThis changing association means that the node associa-\ntion cannot be stored by a program as a property of the\npage. The association can change over time. For pages\nwhich are shared with other processes this can also hap-\npen because a process asks for it (see the discussion of\nmbind below). The kernel by itself can migrate pages if\none node runs out of space while other nodes still have\nfree space.\nAny node association the user-level code learns about can\ntherefore be true for only a short time. It is more of a hint\nthan absolute information. Whenever accurate knowl-\nedge is required the get_mempolicy interface should\nbe used (see section 6.5.5).\n6.5.4 VMA Policy\nTo set the VMA policy for an address range a different\ninterface has to be used:\n#include <numaif.h>\nlong mbind(void *start, unsigned long len,\nint mode,\nunsigned long *nodemask,\nunsigned long maxnode,\nunsigned flags);\nThis interface registers a new VMA policy for the ad-\ndress range [ start ,start +len). Since memory han-\ndling operates on pages the start address must be page-\naligned. The len value is rounded up to the next page\nsize.\nThemode parameter speciﬁes, again, the policy; the val-\nues must be chosen from the list in section 6.5.1. As with\nset_mempolicy , thenodemask parameter is only used\nfor some policies. Its handling is identical.\nThe semantics of the mbind interface depends on the\nvalue of the flags parameter. By default, if flags is\nzero, the system call sets the VMA policy for the address\nrange. Existing mappings are not affected. If this is not\nsufﬁcient there are currently three ﬂags to modify this\nbehavior; they can be selected individually or together:\nMPOL_MF_STRICT The call to mbind will fail if not all\npages are on the nodes speciﬁed by nodemask . Incase this ﬂag is used together with MPOL_MF_MOVE\nand/or MPOL_MF_MOVEALL the call will fail if any\npage cannot be moved.\nMPOL_MF_MOVE The kernel will try to move any page\nin the address range allocated on a node not in the\nset speciﬁed by nodemask . By default, only pages\nused exclusively by the current process’s page ta-\nbles are moved.\nMPOL_MF_MOVEALL Like MPOL_MF_MOVE but the ker-\nnel will try to move all pages, not just those used\nby the current process’s page tables alone. This\noperation has system-wide implications since it in-\nﬂuences the memory access of other processes–\nwhich are possibly not owned by the same user–\nas well. Therefore MPOL_MF_MOVEALL is a privi-\nleged operation ( CAP_NICE capability is needed).\nNote that support for MPOL_MF_MOVE andMPOL_MF_-\nMOVEALL was added only in the 2.6.16 Linux kernel.\nCalling mbind without any ﬂags is most useful when the\npolicy for a newly reserved address range has to be spec-\niﬁed before any pages are actually allocated.\nvoid *p = mmap(NULL, len,\nPROT_READ|PROT_WRITE,\nMAP_ANON, -1, 0);\nif (p != MAP_FAILED)\nmbind(p, len, mode, nodemask, maxnode,\n0);\nThis code sequence reserve an address space range of\nlen bytes and speciﬁes that the policy mode referencing\nthe memory nodes in nodemask should be used. Unless\ntheMAP_POPULATE ﬂag is used with mmap , no memory\nwill have been allocated by the time of the mbind call\nand, therefore, the new policy applies to all pages in that\naddress space region.\nTheMPOL_MF_STRICT ﬂag alone can be used to deter-\nmine whether any page in the address range described by\nthestart andlen parameters to mbind is allocated on\nnodes other than those speciﬁed by nodemask . No allo-\ncated pages are changed. If all pages are allocated on the\nspeciﬁed nodes, the VMA policy for the address space\nregion will be changed according to mode .\nSometimes rebalancing of memory is needed, in which\ncase it might be necessary to move pages allocated on\none node to another node. Calling mbind with MPOL_-\nMF_MOVE set makes a best effort to achieve that. Only\npages which are solely referenced by the process’s page\ntable tree are considered for moving. There can be multi-\nple users in the form of threads or other processes which\nshare that part of the page table tree. It is not possible\nto affect other processes which happen to map the same\ndata. These pages do not share the page table entries.\n74 Version 1.0 What Every Programmer Should Know About Memory\nIf both the MPOL_MF_STRICT andMPOL_MF_MOVE bits\nare set in the flags parameter passed to mbind the ker-\nnel will try to move all pages which are not allocated on\nthe speciﬁed nodes. If this is not possible the call will\nfail. Such a call might be useful to determine whether\nthere is a node (or set of nodes) which can house all the\npages. Several combinations can be tried in succession\nuntil a suitable node is found.\nThe use of MPOL_MF_MOVEALL is harder to justify unless\nrunning the current process is the main purpose of the\ncomputer. The reason is that even pages that appear in\nmultiple page tables are moved. That can easily affect\nother processes in a negative way. This operation should\nthus be used with caution.\n6.5.5 Querying Node Information\nTheget_mempolicy interface can be used to query a\nvariety of facts about the state of NUMA for a given ad-\ndress.\n#include <numaif.h>\nlong get_mempolicy(int *policy,\nconst unsigned long *nmask,\nunsigned long maxnode,\nvoid *addr, int flags);\nWhen get_mempolicy is called with zero for the flags\nparameter, the information about the policy for address\naddr is stored in the word pointed to by policy and\nin the bitmask for the nodes pointed to by nmask . If\naddr falls into an address space region for which a VMA\npolicy has been speciﬁed, information about that policy\nis returned. Otherwise information about the task policy\nor, if necessary, system default policy will be returned.\nIf the MPOL_F_NODE ﬂag is set in flags , and the policy\ngoverning addr isMPOL_INTERLEAVE , the value stored\nin the word pointed to by policy is the index of the node\non which the next allocation is going to happen. This in-\nformation can potentially be used to set the afﬁnity of\na thread which is going to work on the newly-allocated\nmemory. This might be a less costly way to achieve prox-\nimity, especially if the thread has yet to be created.\nTheMPOL_F_ADDR ﬂag can be used to retrieve yet an-\nother completely different data item. If this ﬂag is used,\nthe value stored in the word pointed to by policy is\nthe index of the memory node on which the memory for\nthe page containing addr has been allocated. This in-\nformation can be used to make decisions about possible\npage migration, to decide which thread could work on the\nmemory location most efﬁciently, and many more things.\nThe CPU–and therefore memory node–a thread is using\nis much more volatile than its memory allocations. Mem-\nory pages are, without explicit requests, only moved inextreme circumstances. A thread can be assigned to an-\nother CPU as the result of rebalancing the CPU loads. In-\nformation about the current CPU and node might there-\nfore be short-lived. The scheduler will try to keep the\nthread on the same CPU, and possibly even on the same\ncore, to minimize performance losses due to cold caches.\nThis means it is useful to look at the current CPU and\nnode information; one only must avoid assuming the as-\nsociation will not change.\nlibNUMA provides two interfaces to query the node in-\nformation for a given virtual address space range:\n#include <libNUMA.h>\nint NUMA_mem_get_node_idx(void *addr);\nint NUMA_mem_get_node_mask(void *addr,\nsize_t size,\nsize_t __destsize,\nmemnode_set_t *dest);\nNUMA_mem_get_node_mask sets in dest the bits for all\nmemory nodes on which the pages in the range [ addr ,\naddr +size ) are (or would be) allocated, according to\nthe governing policy. NUMA_mem_get_node only looks\nat the address addr and returns the index of the mem-\nory node on which this address is (or would be) allo-\ncated. These interfaces are simpler to use than get_-\nmempolicy and probably should be preferred.\nThe CPU currently used by a thread can be queried using\nsched_getcpu (see section 6.4.3). Using this informa-\ntion, a program can determine the memory node(s) which\nare local to the CPU using the NUMA_cpu_to_memnode\ninterface from libNUMA:\n#include <libNUMA.h>\nint NUMA_cpu_to_memnode(size_t cpusetsize,\nconst cpu_set_t *cpuset,\nsize_t memnodesize,\nmemnode_set_t *\nmemnodeset);\nA call to this function will set (in the memory node set\npointed to by the fourth parameter) all the bits corre-\nsponding to memory nodes which are local to any of the\nCPUs in the set pointed to by the second parameter. Just\nlike CPU information itself, this information is only cor-\nrect until the conﬁguration of the machine changes (for\ninstance, CPUs get removed and added).\nThe bits in the memnode_set_t objects can be used in\ncalls to the low-level functions like get_mempolicy .\nIt is more convenient to use the other functions in lib-\nNUMA. The reverse mapping is available through:\n#include <libNUMA.h>\nUlrich Drepper Version 1.0 75",9539
37-7 Memory Performance Tools.pdf,37-7 Memory Performance Tools,"int NUMA_memnode_to_cpu(size_t memnodesize,\nconst memnode_set_t *\nmemnodeset,\nsize_t cpusetsize,\ncpu_set_t *cpuset);\nThe bits set in the resulting cpuset are those of the\nCPUs local to any of the memory nodes with correspond-\ning bits set in memnodeset . For both interfaces, the\nprogrammer has to be aware that the information can\nchange over time (especially with CPU hot-plugging). In\nmany situations, a single bit is set in the input bit set,\nbut it is also meaningful, for instance, to pass the entire\nset of CPUs retrieved by a call to sched_getaffinity\ntoNUMA_cpu_to_memnode to determine which are the\nmemory nodes the thread ever can have direct access to.\n6.5.6 CPU and Node Sets\nAdjusting code for SMP and NUMA environments by\nchanging the code to use the interfaces described so far\nmight be prohibitively expensive (or impossible) if the\nsources are not available. Additionally, the system ad-\nministrator might want to impose restrictions on the re-\nsources a user and/or process can use. For these situa-\ntions the Linux kernel supports so-called CPU sets. The\nname is a bit misleading since memory nodes are also\ncovered. They also have nothing to do with the cpu_-\nset_t data type.\nThe interface to CPU sets is, at the moment, a special\nﬁlesystem. It is usually not mounted (so far at least).\nThis can be changed with\nmount -t cpuset none /dev/cpuset\nThe mount point /dev/cpuset must of course exist at\nthat time. The content of this directory is a description\nof the default (root) CPU set. It comprises initially all\nCPUs and all memory nodes. The cpus ﬁle in that di-\nrectory shows the CPUs in the CPU set, the mems ﬁle the\nmemory nodes, the tasks ﬁle the processes.\nTo create a new CPU set one simply creates a new direc-\ntory somewhere in the hierarchy. The new CPU set will\ninherit all settings from the parent. Then the CPUs and\nmemory nodes for new CPU set can be changed by writ-\ning the new values into the cpus andmems pseudo ﬁles\nin the new directory.\nIf a process belongs to a CPU set, the settings for the\nCPUs and memory nodes are used as masks for the afﬁn-\nity and memory policy bitmasks. That means the pro-\ngram cannot select any CPU in the afﬁnity mask which\nis not in the cpus ﬁle for the CPU set the process is us-\ning (i.e., where it is listed in the tasks ﬁle). Similarly\nfor the node masks for the memory policy and the mems\nﬁle.The program will not experience any errors unless the\nbitmasks are empty after the masking, so CPU sets are\nan almost-invisible means to control program execution.\nThis method is especially efﬁcient on machines with lots\nof CPUs and/or memory nodes. Moving a process into a\nnew CPU set is as simple as writing the process ID into\nthetasks ﬁle of the appropriate CPU set.\nThe directories for the CPU sets contain a number of\nother ﬁles which can be used to specify details like be-\nhavior under memory pressure and exclusive access to\nCPUs and memory nodes. The interested reader is re-\nferred to the ﬁle Documentation/cpusets.txt in the\nkernel source tree.\n6.5.7 Explicit NUMA Optimizations\nAll the local memory and afﬁnity rules cannot help out\nif all threads on all the nodes need access to the same\nmemory regions. It is, of course, possible to simply re-\nstrict the number of threads to a number supportable by\nthe processors which are directly connected to the mem-\nory node. This does not take advantage of SMP NUMA\nmachines, though, and is therefore not a real option.\nIf the data in question is read-only there is a simple solu-\ntion: replication. Each node can get its own copy of the\ndata so that no inter-node accesses are necessary. Code\nto do this can look like this:\nvoid *local_data(void) {\nstatic void *data[NNODES];\nint node =\nNUMA_memnode_self_current_idx();\nif (node == -1)\n/*Cannot get node, pick one. */\nnode = 0;\nif (data[node] == NULL)\ndata[node] = allocate_data();\nreturn data[node];\n}\nvoid worker(void) {\nvoid *data = local_data();\nfor (...)\ncompute using data\n}\nIn this code the function worker prepares by getting a\npointer to the local copy of the data by a call to local_-\ndata . Then it proceeds with the loop, which uses this\npointer. The local_data function keeps a list of the al-\nready allocated copies of the data around. Each system\nhas a limited number of memory nodes, so the size of the\narray with the pointers to the per-node memory copies\nis limited in size. The NUMA_memnode_system_count\nfunction from libNUMA returns this number. If memory\nfor the given node has not yet been allocated for the cur-\nrent node (recognized by a null pointer in data at the in-\n76 Version 1.0 What Every Programmer Should Know About Memory\ndex returned by the NUMA_memnode_self_current_-\nidxcall), a new copy is allocated.\nIt is important to realize that nothing terrible happens if\nthe threads get scheduled onto another CPU connected to\na different memory node after the getcpu system call43.\nIt just means that the accesses using the data variable in\nworker access memory on another memory node. This\nslows the program down until data is computed anew,\nbut that is all. The kernel will always avoid gratuitous\nrebalancing of the per-CPU run queues. If such a trans-\nfer happens it is usually for a good reason and will not\nhappen again for the near future.\nThings are more complicated when the memory area in\nquestion is writable. Simple duplication will not work in\nthis case. Depending on the exact situation there might a\nnumber of possible solutions.\nFor instance, if the writable memory region is used to\naccumulate results, it might be possible to ﬁrst create a\nseparate region for each memory node in which the re-\nsults are accumulated. Then, when this work is done, all\nthe per-node memory regions are combined to get the to-\ntal result. This technique can work even if the work never\nreally stops, but intermediate results are needed. The re-\nquirement for this approach is that the accumulation of\na result is stateless, i.e., it does not depend on the previ-\nously collected results.\nIt will always be better, though, to have direct access to\nthe writable memory region. If the number of accesses\nto the memory region is substantial, it might be a good\nidea to force the kernel to migrate the memory pages in\nquestion to the local node. If the number of accesses\nis really high, and the writes on different nodes do not\nhappen concurrently, this could help. But be aware that\nthe kernel cannot perform miracles: the page migration\nis a copy operation and as such it is not cheap. This cost\nhas to be amortized.\n6.5.8 Utilizing All Bandwidth\nThe numbers in Figure 5.4 show that access to remote\nmemory when the caches are ineffective is not measur-\nably slower than access to local memory. This means\na program could possibly save bandwidth to the local\nmemory by writing data it does not have to read again\ninto memory attached to another processor. The band-\nwidth of the connection to the DRAM modules and the\nbandwidth of the interconnects are mostly independent,\nso parallel use could improve overall performance.\nWhether this is really possible depends on many fac-\ntors. One really has to be sure that caches are ineffec-\ntive since otherwise the slowdown related to remote ac-\ncesses is measurable. Another big problem is whether\nthe remote node has any needs for its own memory band-\n43The user-level sched_getcpu interface is implemented using\nthegetcpu system call which should not be used directly and has a\ndifferent interface.width. This possibility must be examined in detail before\nthe approach is taken. In theory, using all the bandwidth\navailable to a processor can have positive effects. A fam-\nily 10h Opteron processor can be directly connected to\nup to four other processors. Utilizing all that additional\nbandwidth, perhaps coupled with appropriate prefetches\n(especially prefetchw ) could lead to improvements if\nthe rest of the system plays along.\nUlrich Drepper Version 1.0 77",8073
38-7.1 Memory Operation Profiling.pdf,38-7.1 Memory Operation Profiling,"7 Memory Performance Tools\nA wide variety of tools is available to help programmers\nunderstand performance characteristics of a program, the\ncache and memory use among others. Modern proces-\nsors have performance monitoring hardware that can be\nused. Some events are hard to measure exactly, so there\nis also room for simulation. When it comes to higher-\nlevel functionality, there are special tools to monitor the\nexecution of a process. We will introduce a set of com-\nmonly used tools available on most Linux systems.\n7.1 Memory Operation Proﬁling\nProﬁling memory operations requires collaboration from\nthe hardware. It is possible to gather some information in\nsoftware alone, but this is either coarse-grained or merely\na simulation. Examples of simulation will be shown in\nsection 7.2 and 7.5. Here we will concentrate on measur-\nable memory effects.\nAccess to performance monitoring hardware on Linux is\nprovided by oproﬁle. Oproﬁle provides continuous pro-\nﬁling capabilities as ﬁrst described in [2]; it performs\nstatistical, system-wide proﬁling with an easy-to-use in-\nterface. Oproﬁle is by no means the only way the per-\nformance measurement functionality of processors can\nbe used; Linux developers are working on pfmon which\nmight at some point be sufﬁciently widely deployed to\nwarrant being described here, too.\nThe interface oproﬁle provides is simple and minimal but\nalso pretty low-level, even if the optional GUI is used.\nThe user has to select among the events the processor\ncan record. The architecture manuals for the processors\ndescribe the events but, oftentimes, it requires extensive\nknowledge about the processors themselves to interpret\nthe data. Another problem is the interpretation of the\ncollected data. The performance measurement counters\nare absolute values and can grow arbitrarily. How high is\ntoo high for a given counter?\nA partial answer to this problem is to avoid looking at the\nabsolute values and, instead, relate multiple counters to\neach other. Processors can monitor more than one event;\nthe ratio of the collected absolute values can then be ex-\namined. This gives nice, comparable results. Often the\ndivisor is a measure of processing time, the number of\nclock cycles or the number of instructions. As an ini-\ntial stab at program performance, relating just these two\nnumbers by themselves is useful.\nFigure 7.1 shows the Cycles Per Instruction (CPI) for the\nsimple random “Follow” test case for the various work-\ning set sizes. The names of the events to collect this infor-\nmation for most Intel processor are CPU_CLK_UNHALTED\nandINST_RETIRED . As the names suggest, the former\ncounts the clock cycles of the CPU and the latter the\nnumber of instructions. We see a picture similar to the\ncycles per list element measurements we used. For small\nworking set sizes the ratio is 1.0 or even lower. These02468101214161820\n210213216219222225228\nWorking Set Size (Bytes)Cycles/Instr uction\nFigure 7.1: Cycles per Instruction (Follow Random)\nmeasurements were made on a Intel Core 2 processor,\nwhich is multi-scalar and can work on several instruc-\ntions at once. For a program which is not limited by\nmemory bandwidth, the ratio can be signiﬁcantly below\n1.0 but, in this case, 1.0 is pretty good.\nOnce the L1d is no longer large enough to hold the work-\ning set, the CPI jumps to just below 3.0. Note that the\nCPI ratio averages the penalties for accessing L2 over\nall instructions, not just the memory instructions. Using\nthe cycles for list element data, it can be worked out how\nmany instructions per list element are needed. If even the\nL2 cache is not sufﬁcient, the CPI ratio jumps to more\nthan 20. These are expected results.\nBut the performance measurement counters are supposed\nto give more insight into what is going on in the pro-\ncessor. For this we need to think about processor im-\nplementations. In this document, we are concerned with\ncache handling details, so we have to look at events re-\nlated to the caches. These events, their names, and what\nthey count, are processor–speciﬁc. This is where opro-\nﬁle is currently hard to use, irrespective of the simple\nuser interface: the user has to ﬁgure out the performance\ncounter details by her/himself. In Appendix B we will\nsee details about some processors.\nFor the Core 2 processor the events to look for are L1D_-\nREPL ,DTLB_MISSES , and L2_LINES_IN . The latter can\nmeasure both all misses and misses caused by instruc-\ntions instead of hardware prefetching. The results for the\nrandom “Follow” test can be seen in Figure 7.2.\nAll ratios are computed using the number of retired in-\nstructions ( INST_RETIRED ). This means that to compute\nthe cache miss rate all the load and store instructions a\nsubstantial number has to be subtracted from the INST_-\nRETIRED value which means the actual cache miss rate\nof the memory operation is even higher than the numbers\nshown in the graph.\nThe L1d misses tower over all the others since an L2 miss\n78 Version 1.0 What Every Programmer Should Know About Memory\n0%2%4%6%8%10%12%14%16%18%20%22%\n210213216219222225228\nWorking Set Size (Bytes)Cache Miss Ratio\nL1D Misses L2 Misses L2 Demand Misses DTLB Misses\nFigure 7.2: Measured Cache Misses (Follow Random)\nimplies, for Intel processors, an L1d miss due to the use\nof inclusive caches. The processor has 32k of L1d and so\nwe see, as expected, the L1d rate go up from zero at about\nthat working set size (there are other uses of the cache\nbeside the list data structure, which means the increase\nhappens between the 16k and 32k mark). It is interesting\nto see that the hardware prefetching can keep the miss\nrate at 1% for a working set size up to and including 64k.\nAfter that the L1d rate skyrockets.\nThe L2 miss rate stays zero until the L2 is exhausted;\nthe few misses due to other uses of L2 do not inﬂuence\nthe numbers much. Once the size of L2 ( 221bytes) is\nexceeded, the miss rates rise. It is important to notice\nthat the L2 demand miss rate is nonzero. This indicates\nthat the hardware prefetcher does not load all the cache\nlines needed by instructions later. This is expected, the\nrandomness of the accesses prevents perfect prefetching.\nCompare this with the data for the sequential read in Fig-\nure 7.3.\nIn this graph we can see that the L2 demand miss rate\nis basically zero (note the scale of this graph is differ-\nent from Figure 7.2). For the sequential access case, the\nhardware prefetcher works perfectly: almost all L2 cache\nmisses are caused by the prefetcher. The fact that the L1d\nand L2 miss rates are the same shows that all L1d cache\nmisses are handled by the L2 cache without further de-\nlays. This is the ideal case for all programs but it is, of\ncourse, hardly ever achievable.\nThe fourth line in both graphs is the DTLB miss rate (In-\ntel has separate TLBs for code and data, DTLB is the data\nTLB). For the random access case, the DTLB miss rate\nis signiﬁcant and contributes to the delays. What is in-\nteresting is that the DTLB penalties set in before the L2\nmisses. For the sequential access case the DTLB costs\nare basically zero.\nGoing back to the matrix multiplication example in sec-0%0.5%1%1.5%2%2.5%3%3.5%\n210213216219222225228\nWorking Set Size (Bytes)Cache Miss Ratio\nL1D Misses L2 Misses L2 Demand Misses DTLB Misses\nFigure 7.3: Measured Cache Misses (Follow Sequential)\ntion 6.2.1 and the example code in section A.1, we can\nmake use of three more counters. The SSE_HIT_PRE ,\nSSE_PRE_MISS , and LOAD_PRE_EXEC counters can be\nused to see how effective the software prefetching is. If\nthe code in section A.1 is run we get the following re-\nsults:\nDescription Ratio\nUseful NTA prefetches 2.84%\nLate NTA prefetches 2.65%\nThe low useful NTA (non-temporal aligned) prefetch ra-\ntio indicates that many prefetch instructions are executed\nfor cache lines which are already loaded, so no work is\nneeded. This means the processor wastes time to decode\nthe prefetch instruction and look up the cache. One can-\nnot judge the code too harshly, though. Much depends on\nthe size of the caches of the processor used; the hardware\nprefetcher also plays a role.\nThe low late NTA prefetch ratio is misleading. The ratio\nmeans that 2.65% of all prefetch instructions are issued\ntoo late. The instruction which needs the data is executed\nbefore the data could be prefetched into the cache. It\nmust be kept in mind that only 2:84% + 2:65% = 5:5%\nof the prefetch instructions were of any use. Of the NTA\nprefetch instructions which are useful, 48% did not ﬁnish\nin time. The code therefore can be optimized further:\n• most of the prefetch instructions are not needed.\n• the use of the prefetch instruction can be adjusted\nto match the hardware better.\nIt is left as an exercise to the reader to determine the best\nsolution for the available hardware. The exact hardware\nspeciﬁcation plays a big role. On Core 2 processors the\nlatency of the SSE arithmetic operations is 1 cycle. Older\nUlrich Drepper Version 1.0 79\n$ \time ls /etc\n[...]\n0.00user 0.00system 0:00.02elapsed 17%CPU (0avgtext+0avgdata 0maxresident)k\n0inputs+0outputs (1major+335minor)pagefaults 0swaps\nFigure 7.4: Output of the time utility\nversions had a latency of 2 cycles, meaning that the hard-\nware prefetcher and the prefetch instructions had more\ntime to bring in the data.\nTo determine where prefetches might be needed–or are\nunnecessary–one can use the opannotate program. It lists\nthe source or assembler code of the program and shows\nthe instructions where the event was recognized. Note\nthat there are two sources of vagueness:\n1. Oproﬁle performs stochastic proﬁling. Only every\nNthevent (where N is a per-event threshold with\nan enforced minimum) is recorded to avoid slow-\ning down operation of the system too much. There\nmight be lines which cause 100 events and yet they\nmight not show up in the report.\n2. Not all events are recorded accurately. For exam-\nple, the instruction counter at the time a speciﬁc\nevent was recorded might be incorrect. Processors\nbeing multi-scalar makes it hard to give a 100%\ncorrect answer. A few events on some processors\nare exact, though.\nThe annotated listings are useful for more than determin-\ning the prefetching information. Every event is recorded\nwith the instruction pointer; it is therefore also possible\nto pinpoint other hot spots in the program. Locations\nwhich are the source of many INST_RETIRED events are\nexecuted frequently and deserve to be tuned. Locations\nwhere many cache misses are reported might warrant a\nprefetch instruction to avoid the cache miss.\nOne type of event which can be measured without hard-\nware support is page faults. The OS is responsible for re-\nsolving page faults and, on those occasions, it also counts\nthem. It distinguishes two kinds of page faults:\nMinor Page Faults For anonymous (i.e., not backed by\na ﬁle) pages which have not been used so far, for\ncopy-on-write pages, and for other pages whose\ncontent is already in memory somewhere.\nMajor Page Faults Resolving them requires access to\ndisk to retrieve the ﬁle-backed (or swapped-out)\ndata.\nObviously, major page faults are signiﬁcantly more ex-\npensive than minor page faults. But the latter are not\ncheap either. In either case an entry into the kernel is\nnecessary, a new page must be found, the page must be\ncleared or populated with the appropriate data, and thepage table tree must be modiﬁed accordingly. The last\nstep requires synchronization with other tasks reading or\nmodifying the page table tree, which might introduce fur-\nther delays.\nThe easiest way to retrieve information about the page\nfault counts is to use the time tool. Note: use the real\ntool, not the shell builtin. The output can be seen in Fig-\nure 7.4.44\nThe interesting part here is the last line. The time tool\nreports one major and 335 minor page faults. The exact\nnumbers vary; in particular, repeating the run immedi-\nately will likely show that there are now no major page\nfaults at all. If the program performs the same action, and\nnothing changes in the environment, the total page fault\ncount will be stable.\nAn especially sensitive phase with respect to page faults\nis program start-up. Each page which is used will pro-\nduce a page fault; the visible effect (especially for GUI\napplications) is that the more pages that are used, the\nlonger it takes for the program to start working. In sec-\ntion 7.5 we will see a tool to measure this effect speciﬁ-\ncally.\nUnder the hood, the time tool uses the rusage function-\nality. The wait4 system call ﬁlls in a struct rusage\nobject when the parent waits for a child to terminate;\nthat is exactly what is needed for the time tool. But it is\nalso possible for a process to request information about\nits own resource usage (that is where the name rusage\ncomes from) or the resource usage of its terminated chil-\ndren.\n#include <sys/resource.h>\nint getrusage(__rusage_who_t who,\nstruct rusage *usage)\nThewho parameter speciﬁes which process the informa-\ntion is requested for. Currently, only RUSAGE_SELF and\nRUSAGE_CHILDREN are deﬁned. The resource usage of\nthe child processes is accumulated when each child ter-\nminates. It is a total value, not the usage of an individ-\nual child process. Proposals to allow requesting thread-\nspeciﬁc information exist, so it is likely that we will see\nRUSAGE_THREAD in the near future. The rusage struc-\nture is deﬁned to contain all kinds of metrics, including\nexecution time, the number of IPC messages sent and\n44The leading backslash prevents the use of the built-in command.\n80 Version 1.0 What Every Programmer Should Know About Memory",13817
39-7.3 Measuring Memory Usage.pdf,39-7.3 Measuring Memory Usage,"==19645== I refs: 152,653,497\n==19645== I1 misses: 25,833\n==19645== L2i misses: 2,475\n==19645== I1 miss rate: 0.01%\n==19645== L2i miss rate: 0.00%\n==19645==\n==19645== D refs: 56,857,129 (35,838,721 rd + 21,018,408 wr)\n==19645== D1 misses: 14,187 ( 12,451 rd + 1,736 wr)\n==19645== L2d misses: 7,701 ( 6,325 rd + 1,376 wr)\n==19645== D1 miss rate: 0.0% ( 0.0% + 0.0% )\n==19645== L2d miss rate: 0.0% ( 0.0% + 0.0% )\n==19645==\n==19645== L2 refs: 40,020 ( 38,284 rd + 1,736 wr)\n==19645== L2 misses: 10,176 ( 8,800 rd + 1,376 wr)\n==19645== L2 miss rate: 0.0% ( 0.0% + 0.0% )\nFigure 7.5: Cachegrind Summary Output\nmemory used, and the number of page faults. The lat-\nter information is available in the ru_minflt andru_-\nmajflt members of the structure.\nA programmer who tries to determine where her program\nloses performance due to page faults could regularly re-\nquest the information and then compare the returned val-\nues with the previous results.\nFrom the outside, the information is also visible if the\nrequester has the necessary privileges. The pseudo ﬁle\n/proc/<PID>/stat , where <PID> is the process ID of\nthe process we are interested in, contains the page fault\nnumbers in the tenth to fourteenth ﬁelds. They are pairs\nof the process’s and its children’s cumulative minor and\nmajor page faults, respectively.\n7.2 Simulating CPU Caches\nWhile the technical description of how a cache works is\nrelatively easy to understand, it is not so easy to see how\nan actual program behaves with respect to cache. Pro-\ngrammers are not directly concerned with the values of\naddresses, be they absolute nor relative. Addresses are\ndetermined, in part, by the linker and, in part, at runtime\nby the dynamic linker and the kernel. The generated as-\nsembly code is expected to work with all possible ad-\ndresses and, in the source language, there is not even a\nhint of absolute address values left. So it can be quite\ndifﬁcult to get a sense for how a program is making use\nof memory.45\nCPU-level proﬁling tools such as oproﬁle (as described\nin section 7.1) can help to understand the cache use. The\nresulting data corresponds to the actual hardware, and it\ncan be collected relatively quickly if ﬁne-grained collec-\ntion is not needed. As soon as more ﬁne-grained data is\nneeded, oproﬁle is not usable anymore; the thread would\nhave to be interrupted too often. Furthermore, to see the\nmemory behavior of the program on different processors,\none actually has to have such machines and execute the\n45When programming close to the hardware this might be different,\nbut this is of no concern to normal programming and, in any case, is\nonly possible for special addresses such as memory-mapped devices.program on them. This is sometimes (often) not possible.\nOne example is the data from Figure 3.8. To collect such\ndata with oproﬁle one would have to have 24 different\nmachines, many of which do not exist.\nThe data in that graph was collected using a cache simu-\nlator. This program, cachegrind, uses the valgrind frame-\nwork, which was initially developed to check for memory\nhandling related problems in a program. The valgrind\nframework simulates the execution of a program and,\nwhile doing this, it allows various extensions, such as\ncachegrind, to hook into the execution framework. The\ncachegrind tool uses this to intercept all uses of memory\naddresses; it then simulates the operation of L1i, L1d,\nand L2 caches with a given size, cache line size, and as-\nsociativity.\nTo use the tool a program must be run using valgrind as\na wrapper:\nvalgrind --tool=cachegrind command arg\nIn this simplest form the program command is executed\nwith the parameter argwhile simulating the three caches\nusing sizes and associativity corresponding to that of the\nprocessor it is running on. One part of the output is\nprinted to standard error when the program is running;\nit consists of statistics of the total cache use as can be\nseen in Figure 7.5. The total number of instructions and\nmemory references is given, along with the number of\nmisses they produce for the L1i/L1d and L2 cache, the\nmiss rates, etc. The tool is even able to split the L2\naccesses into instruction and data accesses, and all data\ncache uses are split in read and write accesses.\nIt becomes even more interesting when the details of the\nsimulated caches are changed and the results compared.\nThrough the use of the --I1 ,--D1 , and --L2 parame-\nters, cachegrind can be instructed to disregard the proces-\nsor’s cache layout and use that speciﬁed on the command\nline. For example:\nvalgrind --tool=cachegrind \\n--L2=8388608,8,64 command arg\nUlrich Drepper Version 1.0 81\n--------------------------------------------------------------------------------\nIr I1mr I2mr Dr D1mr D2mr Dw D1mw D2mw file:function\n--------------------------------------------------------------------------------\n53,684,905 9 8 9,589,531 13 3 5,820,373 14 0 ???:_IO_file_xsputn@@GLIBC_2.2.5\n36,925,729 6,267 114 11,205,241 74 18 7,123,370 22 0 ???:vfprintf\n11,845,373 22 2 3,126,914 46 22 1,563,457 0 0 ???:__find_specmb\n6,004,482 40 10 697,872 1,744 484 0 0 0 ???:strlen\n5,008,448 3 2 1,450,093 370 118 0 0 0 ???:strcmp\n3,316,589 24 4 757,523 0 0 540,952 0 0 ???:_IO_padn\n2,825,541 3 3 290,222 5 1 216,403 0 0 ???:_itoa_word\n2,628,466 9 6 730,059 0 0 358,215 0 0 ???:_IO_file_overflow@@GLIBC_2.2.5\n2,504,211 4 4 762,151 2 0 598,833 3 0 ???:_IO_do_write@@GLIBC_2.2.5\n2,296,142 32 7 616,490 88 0 321,848 0 0 dwarf_child.c:__libdw_find_attr\n2,184,153 2,876 20 503,805 67 0 435,562 0 0 ???:__dcigettext\n2,014,243 3 3 435,512 1 1 272,195 4 0 ???:_IO_file_write@@GLIBC_2.2.5\n1,988,697 2,804 4 656,112 380 0 47,847 1 1 ???:getenv\n1,973,463 27 6 597,768 15 0 420,805 0 0 dwarf_getattrs.c:dwarf_getattrs\nFigure 7.6: cg annotate Output\nwould simulate an 8MB L2 cache with 8-way set asso-\nciativity and 64 byte cache line size. Note that the --L2\noption appears on the command line before the name of\nthe program which is simulated.\nThis is not all cachegrind can do. Before the process ex-\nits it writes out a ﬁle named cachegrind.out.XXXXX\nwhere XXXXX is the PID of the process. This ﬁle contains\nthe summary information and detailed information about\nthe cache use in each function and source ﬁle. The data\ncan be viewed using the cg annotate program.\nThe output this program produces contains the cache use\nsummary which was printed also when the process ter-\nminated, along with a detailed summary of the cache line\nuse in each function of the program. Generating this per-\nfunction data requires that cg annotate is able to match\naddresses to functions. This means debug information\nshould be available for best results. Failing that, the ELF\nsymbol tables can help a bit but, since internal symbols\nare not listed in the dynamic symbol table, the results are\nnot complete. Figure 7.6 shows part of the output for the\nsame program run as Figure 7.5.\nThe Ir, Dr, and Dw columns show the total cache use,\nnot cache misses, which are shown in the following two\ncolumns. This data can be used to identify the code\nwhich produces the most cache misses. First, one proba-\nbly would concentrate on L2 cache misses, then proceed\nto optimizing L1i/L1d cache misses.\ncgannotate can provide the data in more detail. If the\nname of a source ﬁle is given, it also annotates (hence\nthe program’s name) each line of the source ﬁle with the\nnumber of cache hits and misses corresponding to that\nline. This information allows the programmer to drill\ndown to the exact line where cache misses are a problem.\nThe program interface is a bit raw: as of this writing, the\ncachegrind data ﬁle and the source ﬁle must be in the\nsame directory.\nIt should, at this point, be noted again: cachegrind is\na simulator which does notuse measurements from the\nprocessor. The actual cache implementation in the pro-cessor might very well be quite different. cachegrind\nsimulates Least Recently Used (LRU) eviction, which is\nlikely to be too expensive for caches with large associa-\ntivity. Furthermore, the simulation does not take context\nswitches and system calls into account, both of which\ncan destroy large parts of L2 and must ﬂush L1i and L1d.\nThis causes the total number of cache misses to be lower\nthan experienced in reality. Nevertheless, cachegrind is a\nnice tool to learn about a program’s memory use and its\nproblems with memory.\n7.3 Measuring Memory Usage\nKnowing how much memory a program allocates and\npossibly where the allocation happens is the ﬁrst step to\noptimizing its memory use There are, fortunately, some\neasy-to-use programs available which do not require that\nthe program be recompiled or speciﬁcally modiﬁed.\nFor the ﬁrst tool, called massif, it is sufﬁcient to not strip\nthe debug information which the compiler can automat-\nically generate. It provides an overview of the accumu-\nlated memory use over time. Figure 7.7 shows an ex-\nample of the generated output. Like cachegrind (sec-\ntion 7.2), massif is a tool using the valgrind infrastruc-\nture. It is started using\nvalgrind --tool=massif command arg\nwhere command arg is the program which is to be ob-\nserved and its parameter(s), The program will be sim-\nulated and all calls to memory allocation functions are\nrecognized. The call site is recorded along with a times-\ntamp value; the new allocation size is added to both the\nwhole-program total and total for the speciﬁc call site.\nThe same applies to the functions which free memory\nwhere, obviously, the size of the freed block is subtracted\nfrom the appropriated sums. This information can then\nbe used to create a graph showing the memory use over\nthe lifetime of the program, splitting each time value ac-\ncording to the location which requested the allocation.\n82 Version 1.0 What Every Programmer Should Know About Memory\nsrc/readelf -a -w src/readelf 106,047,201 bytes x ms\nms 0.0200.0400.0600.0800.01000.01200.01400.01600.01800.02000.02200.0bytes\n0k5k10k15k20k25k30k35k40k45k50k55k\nx40B2C0:openbackendx40C896:xmallocx102D461:_nl_make_l10nflx102D594:_nl_make_l10nflheap-adminx9D75:_dl_new_objectx102CEE8:read_alias_filex102CF6C:read_alias_filex1027D7D:_nl_intern_locax4C0D64E:dwarf_begin_elfx4C0E8BE:insert_entry_2stack(s)x4C0E7D5:Dwarf_Abbrev_Hax4E29F83:file_read_elfx4C13D6C:__libdw_allocatFigure 7.7: Massif Output\nBefore the process terminates massif creates two ﬁles:\nmassif.XXXXX.txt andmassif.XXXXX.ps ;XXXXX is\nas before the PID of the process. The .txt ﬁle is a sum-\nmary of the memory use for all call sites and the .ps is\nwhat can be seen in Figure 7.7.\nMassif can also record the program’s stack usage, which\ncan be useful to determine the total memory footprint of\nan application. But this is not always possible. In some\nsituations (some thread stacks or when signaltstack\nis used) the valgrind runtime cannot know about the lim-\nits of the stack . In these situations, it also does not make\nmuch sense to add these stacks’ sizes to the total. There\nare several other situations where it makes no sense. If\na program is affected by this, massif should be started\nwith the addition option --stacks=no . Note, this is an\noption for valgrind and therefore must come before the\nname of the program which is being observed.\nSome programs provide their own memory allocation im-\nplementation or wrapper functions around the system’s\nallocation functions. In the ﬁrst case, allocations are nor-\nmally missed; in the second case, the recorded call sites\nhide information, since only the address of the call in\nthe wrapper function is recorded. For this reason, it is\npossible to add additional functions to the list of alloca-\ntion functions. The --alloc-fn=xmalloc parameter\nwould specify that the function xmalloc is also an al-\nlocation function, which is often the case in GNU pro-\ngrams. Calls to xmalloc are recorded, but not the allo-cation calls made from within xmalloc .\nThe second tool is called memusage; it is part of the GNU\nC library. It is a simpliﬁed version of massif (but existed\na long time before massif). It only records the total mem-\nory use for heap (including possible calls to mmap etc. if\nthe-moption is given) and, optionally, the stack. The\nresults can be shown as a graph of the total memory use\nover time or, alternatively, linearly over the calls made to\nallocation functions. The graphs are created separately\nby the memusage script which, just as with valgrind, has\nto be used to start the application:\nmemusage command arg\nThe-p IMGFILE option must be used to specify that the\ngraph should be generated in the ﬁle IMGFILE . This is a\nPNG ﬁle. The code to collect the data is run in the actual\nprogram itself, it is not an simulation like valgrind. This\nmeans memusage is much faster than massif and usable\nin situations where massif would be not useful. Besides\ntotal memory consumption, the code also records alloca-\ntion sizes and, on program termination, it shows a his-\ntogram of the used allocation sizes. This information is\nwritten to standard error.\nSometimes it is not possible (or feasible) to call the pro-\ngram which is supposed to be observed directly. An ex-\nample is the compiler stage of gcc, which is started by\nthe gcc driver program. In this case the name of the pro-\nUlrich Drepper Version 1.0 83\ngram which should be observed must be provided to the\nmemusage script using the -n NAME parameter. This pa-\nrameter is also useful if the program which is observed\nstarts other programs. If no program name is speciﬁed\nall started programs will be proﬁled.\nBoth programs, massif and memusage, have additional\noptions. A programmer ﬁnding herself in the position\nneeding more functionality should ﬁrst consult the man-\nual or help messages to make sure the additional func-\ntionality is not already implemented.\nNow that we know how the data about memory allocation\ncan be captured, it is necessary to discuss how this data\ncan be interpreted in the context of memory and cache\nuse. The main aspects of efﬁcient dynamic memory allo-\ncation are linear allocation and compactness of the used\nportion. This goes back to making prefetching efﬁcient\nand reducing cache misses.\nA program which has to read in an arbitrary amount of\ndata for later processing could do this by creating a list\nwhere each of the list elements contains a new data item.\nThe overhead for this allocation method might be min-\nimal (one pointer for a single-linked list) but the cache\neffects when using the data can reduce the performance\ndramatically.\nOne problem is, for instance, that there is no guarantee\nthat sequentially allocated memory is laid out sequen-\ntially in memory. There are many possible reasons for\nthis:\n• memory blocks inside a large memory chunk ad-\nministrated by the memory allocator are actually\nreturned from the back to the front;\n• a memory chunk is exhausted and a new one is\nstarted in a different part of the address space;\n• the allocation requests are for different sizes which\nare served from different memory pools;\n• the interleaving allocations in the various threads\nof multi-threaded programs.\nIf data must be allocated up front for later processing,\nthe linked-list approach is clearly a bad idea. There is\nno guarantee (or even likelihood) that the consecutive el-\nements in the list are laid out consecutively in memory.\nTo ensure contiguous allocations, that memory must not\nbe allocated in small chunks. Another layer of memory\nhandling must be used; it can easily be implemented by\nthe programmer. An alternative is to use the obstack im-\nplementation available in the GNU C library. This allo-\ncator requests large blocks of memory from the system’s\nallocator and then hands arbitrarily large or small blocks\nof memory out. These allocations are always sequential\nunless the large memory chunk is exhausted, which is,\ndepending on the requested allocation sizes, pretty rare.\nObstacks are not a complete replacement for a memoryallocator, they have limited abilities to free objects. See\nthe GNU C library manual for details.\nSo, how can a situation where the use of obstacks (or\nsimilar techniques) is advisable be recognized from the\ngraphs? Without consulting the source, possible candi-\ndates for the changes cannot be identiﬁed, but the graph\ncan provide an entry point for the search. If many al-\nlocations are made from the same location, this could\nmean that allocation in bulk might help. In Figure 7.7,\nwe can see such a possible candidate in the allocations at\naddress 0x4c0e7d5. From about 800ms into the run un-\ntil 1,800ms into the run this is the only area (except the\ntop, green one) which grows. Moreover, the slope is not\nsteep, which means we have a large number of relatively\nsmall allocations. This is, indeed, a candidate for the use\nof obstacks or similar techniques.\nAnother problem the graphs can show is when the total\nnumber of allocations is high. This is especially easy to\nsee if the graph is not drawn linearly over time but, in-\nstead, linearly over the number of calls (the default with\nmemusage). In that case, a gentle slope in the graph\nmeans a lot of small allocations. memusage will not say\nwhere the allocations took place, but the comparison with\nmassif’s output can say that, or the programmer might\nrecognize it right away. Many small allocations should\nbe consolidated to achieve linear memory use.\nBut there is another, equally important, aspect to this lat-\nter class of cases: many allocations also means higher\noverhead in administrative data. This by itself might not\nbe that problematic. The red area named “heap-admin”\nrepresents this overhead in the massif graph and it is quite\nsmall. But, depending on the malloc implementation,\nthis administrative data is allocated along with the data\nblocks, in the same memory. For the current malloc im-\nplementation in the GNU C library, this is the case: every\nallocated block has at least a 2-word header (8 bytes for\n32-bit platforms, 16 bytes for 64-bit platforms). In addi-\ntion, block sizes are often a bit larger than necessary due\nto the way memory is administrated (rounding up block\nsizes to speciﬁc multiples).\nThis all means that memory used by the program is in-\nterspersed with memory only used by the allocator for\nadministrative purposes. We might see something like\nthis:\nHeader Data Padding\nEach block represents one memory word. In this small\nregion of memory we have four allocated blocks. The\noverhead due to the block header and padding is 50%.\nDue to the placement of the header, this automatically\nmeans that the effective prefetch rate of the processor is\nlowered by up to 50% as well. If the blocks were be\nprocessed sequentially (to take maximum advantage of\nprefetching), the processor would read all the header and\npadding words into the cache, even though they are never\n84 Version 1.0 What Every Programmer Should Know About Memory",19112
40-7.5 Page Fault Optimization.pdf,40-7.5 Page Fault Optimization,"supposed to be read from or written to by the application\nitself. Only the runtime uses the header words, and the\nruntime only comes into play when the block is freed.\nOne could at this point argue that the implementation\nshould be changed to put the administrative data some-\nwhere else. This is indeed done in some implementa-\ntions, and it might prove to be a good idea. There are\nmany aspects to be kept in mind, though, security not be-\ning the least of them. Regardless of whether we might\nsee a change in the future, the padding issue will never\ngo away (amounting to 16% of the data in the example,\nwhen ignoring the headers). Only if the programmer di-\nrectly takes control of allocations can this be avoided.\nWhen alignment requirements come into play there can\nstill be holes, but this is also something under control of\nthe programmer.\n7.4 Improving Branch Prediction\nIn section 6.2.2, two methods to improve L1i use through\nbranch prediction and block reordering were mentioned:\nstatic prediction through __builtin_expect and pro-\nﬁle guided optimization (PGO). Correct branch predic-\ntion has performance impacts, but here we are interested\nin the memory usage improvements.\nThe use of __builtin_expect (or better the likely\nandunlikely macros) is simple. The deﬁnitions are\nplaced in a central header and the compiler takes care\nof the rest. There is a little problem, though: it is easy\nenough for a programmer to use likely when really\nunlikely was meant and vice versa. Even if somebody\nuses a tool like oproﬁle to measure incorrect branch pre-\ndictions and L1i misses these problems are hard to detect.\nThere is one easy method, though. The code in sec-\ntion A.2 shows an alternative deﬁnition of the likely\nandunlikely macros which measure actively, at run-\ntime, whether the static predictions are correct or not.\nThe results can then be examined by the programmer or\ntester and adjustments can be made. The measurements\ndo not actually take the performance of the program into\naccount, they simply test the static assumptions made by\nthe programmer. More details can be found, along with\nthe code, in the section referenced above.\nPGO is quite easy to use with gcc these days. It is a three-\nstep process, though, and certain requirements must be\nfulﬁlled. First, all source ﬁles must be compiled with the\nadditional -fprofile-generate option. This option\nmust be passed to all compiler runs and to the command\nwhich links the program. Mixing object ﬁles compiled\nwith and without this option is possible, but PGO will\nnot do any good for those that do not have it enabled.\nThe compiler generates a binary which behaves normally\nexcept that it is signiﬁcantly larger and slower because it\nrecords (and stores) information about whether branches\nare taken or not. The compiler also emits a ﬁle with the\nextension .gcno for each input ﬁle. This ﬁle containsinformation related to the branches in the code. It must\nbe preserved for later.\nOnce the program binary is available, it should be used\nto run a representative set of workloads. Whatever work-\nload is used, the ﬁnal binary will be optimized to do this\ntask well. Consecutive runs of the program are possible\nand, in general necessary; all the runs will contribute to\nthe same output ﬁle. Before the program terminates, the\ndata collected during the program run is written out into\nﬁles with the extension .gcda . These ﬁles are created\nin the directory which contains the source ﬁle. The pro-\ngram can be executed from any directory, and the binary\ncan be copied, but the directory with the sources must be\navailable and writable. Again, one output ﬁle is created\nfor each input source ﬁle. If the program is run multiple\ntimes, it is important that the .gcda ﬁles of the previous\nrun are found in the source directories since otherwise\nthe data of the runs cannot be accumulated in one ﬁle.\nWhen a representative set of tests has been run, it is time\nto recompile the application. The compiler has to be able\nto ﬁnd the .gcda ﬁles in the same directory which holds\nthe source ﬁles. The ﬁles cannot be moved since the com-\npiler would not ﬁnd them and the embedded checksum\nfor the ﬁles would not match anymore. For the recom-\npilation, replace the -fprofile-generate parameter\nwith -fprofile-use . It is essential that the sources\ndo not change in any way that would change the gener-\nated code. That means: it is OK to change white spaces\nand edit comments, but adding more branches or basic\nblocks invalidates the collected data and the compilation\nwill fail.\nThis is all the programmer has to do; it is a fairly sim-\nple process. The most important thing to get right is the\nselection of representative tests to perform the measure-\nments. If the test workload does not match the way the\nprogram is actually used, the performed optimizations\nmight actually do more harm than good. For this reason,\nis it often hard to use PGO for libraries. Libraries can\nbe used in many–sometimes widely different–scenarios.\nUnless the use cases are indeed similar, it is usually bet-\nter to rely exclusively on static branch prediction using\n__builtin_expect .\nA few words on the .gcno and.gcda ﬁles. These are\nbinary ﬁles which are not immediately usable for inspec-\ntion. It is possible, though, to use the gcov tool, which is\nalso part of the gcc package, to examine them. This tool\nis mainly used for coverage analysis (hence the name) but\nthe ﬁle format used is the same as for PGO. The gcov tool\ngenerates output ﬁles with the extension .gcov for each\nsource ﬁle with executed code (this might include sys-\ntem headers). The ﬁles are source listings which are an-\nnotated, according to the parameters given to gcov, with\nbranch counter, probabilities, etc.\nUlrich Drepper Version 1.0 85\n0 0x3000000000 C 0 0x3000000B50: (within /lib64/ld-2.5.so)\n1 0x 7FF000000 D 3320 0x3000000B53: (within /lib64/ld-2.5.so)\n2 0x3000001000 C 58270 0x3000001080: _dl_start (in /lib64/ld-2.5.so)\n3 0x3000219000 D 128020 0x30000010AE: _dl_start (in /lib64/ld-2.5.so)\n4 0x300021A000 D 132170 0x30000010B5: _dl_start (in /lib64/ld-2.5.so)\n5 0x3000008000 C 10489930 0x3000008B20: _dl_setup_hash (in /lib64/ld-2.5.so)\n6 0x3000012000 C 13880830 0x3000012CC0: _dl_sysdep_start (in /lib64/ld-2.5.so)\n7 0x3000013000 C 18091130 0x3000013440: brk (in /lib64/ld-2.5.so)\n8 0x3000014000 C 19123850 0x3000014020: strlen (in /lib64/ld-2.5.so)\n9 0x3000002000 C 23772480 0x3000002450: dl_main (in /lib64/ld-2.5.so)\nFigure 7.8: Output of the pagein Tool\n7.5 Page Fault Optimization\nOn operating systems like Linux with demand-paging\nsupport, an mmap call only modiﬁes the page tables. It\nmakes sure that, for ﬁle-backed pages, the underlying\ndata can be found and, for anonymous memory, that, on\naccess, pages initialized with zeros are provided. No ac-\ntual memory is allocated at the time of the mmap call.46\nThe allocation part happens when a memory page is ﬁrst\naccessed, either by reading or writing data, or by exe-\ncuting code. In response to the ensuing page fault, the\nkernel takes control and determines, using the page table\ntree, the data which has to be present on the page. This\nresolution of the page fault is not cheap, but it happens\nfor every single page which is used by a process.\nTo minimize the cost of page faults, the total number of\nused pages has to be reduced. Optimizing the code for\nsize will help with this. To reduce the cost of a spe-\nciﬁc code path (for instance, the start-up code), it is also\npossible to rearrange code so that, in that code path, the\nnumber of touched pages is minimized. It is not easy to\ndetermine the right order, though.\nThe author wrote a tool, based on the valgrind toolset,\nto measure page faults as they happen. Not the num-\nber of page faults, but the reason why they happen. The\npagein tool emits information about the order and tim-\ning of page faults. The output, written to a ﬁle named\npagein.<PID> , looks as in Figure 7.8. The second col-\numn speciﬁes the address of the page which is paged-\nin. Whether it is a code or data page is indicated in\nthe third column, which contains ‘C’ or ‘D’ respectively.\nThe fourth column speciﬁes the number of cycles which\npassed since the ﬁrst page fault. The rest of the line is\nvalgrind’s attempt to ﬁnd a name for the address which\ncaused the page fault. The address value itself is correct\nbut the name is not always accurate if no debug informa-\ntion is available.\nIn the example in Figure 7.8, execution starts at address\n3000000B50 16, which forces the system to page in the\npage at address 3000000000 16. Shortly after that, the\npage after this is also brought in; the function called on\n46If you want to say “Wrong!” wait a second, it will be qualiﬁed\nlater that there are exceptions.that page is _dl_start . The initial code accesses a vari-\nable on page 7FF000000 16. This happens just 3,320 cy-\ncles after the ﬁrst page fault and is most likely the second\ninstruction of the program (just three bytes after the ﬁrst\ninstruction). If one looks at the program, one will notice\nthat there is something peculiar about this memory ac-\ncess. The instruction in question is a call instruction,\nwhich does not explicitly load or store data. It does store\nthe return address on the stack, though, and this is ex-\nactly what happens here. This is not the ofﬁcial stack of\nthe process, though, it is valgrind’s internal stack of the\napplication. This means when interpreting the results of\npagein it is important to keep in mind that valgrind intro-\nduces some artifacts.\nThe output of pagein can be used to determine which\ncode sequences should ideally be adjacent in the pro-\ngram code. A quick look at the /lib64/ld-2.5.so\ncode shows that the ﬁrst instructions immediately call\nthe function _dl_start , and that these two places are\non different pages. Rearranging the code to move the\ncode sequences onto the same page can avoid–or at least\ndelay–a page fault. It is, so far, a cumbersome process to\ndetermine what the optimal code layout should be. Since\nthe second use of a page is, by design, not recorded, one\nneeds to use trial and error to see the effects of a change.\nUsing call graph analysis, it is possible to guess about\npossible call sequences; this might help speed up the pro-\ncess of sorting the functions and variables.\nAt a very coarse level, the call sequences can be seen\nby looking a the object ﬁles making up the executable or\nDSO. Starting with one or more entry points (i.e., func-\ntion names), the chain of dependencies can be computed.\nWithout much effort this works well at the object ﬁle\nlevel. In each round, determine which object ﬁles con-\ntain needed functions and variables. The seed set has to\nbe speciﬁed explicitly. Then determine all undeﬁned ref-\nerences in those object ﬁles and add them to the set of\nneeded symbols. Repeat until the set is stable.\nThe second step in the process is to determine an order.\nThe various object ﬁles have to be grouped together to ﬁll\nas few pages as possible. As an added bonus, no function\nshould cross over a page boundary. A complication in all\nthis is that, to best arrange the object ﬁles, it has to be\nknown what the linker will do later. The important fact\n86 Version 1.0 What Every Programmer Should Know About Memory\nhere is that the linker will put the object ﬁles into the exe-\ncutable or DSO in the same order in which they appear in\nthe input ﬁles (e.g., archives), and on the command line.\nThis gives the programmer sufﬁcient control.\nFor those who are willing to invest a bit more time, there\nhave been successful attempts at reordering made using\nautomatic call tracing via the __cyg_profile_func_-\nenter and__cyg_profile_func_exit hooks gcc in-\nserts when called with the -finstrument-functions\noption [17]. See the gcc manual for more information\non these __cyg_ *interfaces. By creating a trace of the\nprogram execution, the programmer can more accurately\ndetermine the call chains. The results in [17] are a 5%\ndecrease in start-up costs, just through reordering of the\nfunctions. The main beneﬁt is the reduced number of\npage faults, but the TLB cache also plays a role–an in-\ncreasingly important role given that, in virtualized envi-\nronments, TLB misses become signiﬁcantly more expen-\nsive.\nBy combining the analysis of the pagein tool with the\ncall sequence information, it should be possible to opti-\nmize certain phases of the program (such as start-up) to\nminimize the number of page faults.\nThe Linux kernel provides two additional mechanisms to\navoid page faults. The ﬁrst one is a ﬂag for mmap which\ninstructs the kernel to not only modify the page table but,\nin fact, to pre-fault all the pages in the mapped area. This\nis achieved by simply adding the MAP_POPULATE ﬂag to\nthe fourth parameter of the mmap call. This will cause\nthemmap call to be signiﬁcantly more expensive, but, if\nall pages which are mapped by the call are being used\nright away, the beneﬁts can be large. Instead of having a\nnumber of page faults, which each are pretty expensive\ndue to the overhead incurred by synchronization require-\nments etc., the program would have one, more expensive,\nmmap call. The use of this ﬂag has disadvantages, though,\nin cases where a large portion of the mapped pages are\nnot used soon (or ever) after the call. Mapped, unused\npages are obviously a waste of time and memory. Pages\nwhich are immediately pre-faulted and only much later\nused also can clog up the system. The memory is allo-\ncated before it is used and this might lead to shortages of\nmemory in the meantime. On the other hand, in the worst\ncase, the page is simply reused for a new purpose (since\nit has not been modiﬁed yet), which is not that expensive\nbut still, together with the allocation, adds some cost.\nThe granularity of MAP_POPULATE is simply too coarse.\nAnd there is a second possible problem: this is an op-\ntimization; it is not critical that all pages are, indeed,\nmapped in. If the system is too busy to perform the op-\neration the pre-faulting can be dropped. Once the page\nis really used the program takes the page fault, but this\nis not worse than artiﬁcially creating resource scarcity.\nAn alternative is to use the POSIX_MADV_WILLNEED ad-\nvice with the posix_madvise function. This is a hint to\nthe operating system that, in the near future, the program\nwill need the page described in the call. The kernel isfree to ignore the advice, but it also can pre-fault pages.\nThe advantage here is that the granularity is ﬁner. Indi-\nvidual pages or page ranges in any mapped address space\narea can be pre-faulted. For memory-mapped ﬁles which\ncontain a lot of data which is not used at runtime, this can\nhave huge advantages over using MAP_POPULATE .\nBeside these active approaches to minimizing the num-\nber of page faults, it is also possible to take a more pas-\nsive approach which is popular with the hardware design-\ners. A DSO occupies neighboring pages in the address\nspace, one range of pages each for the code and the data.\nThe smaller the page size, the more pages are needed to\nhold the DSO. This, in turn, means more page faults, too.\nImportant here is that the opposite is also true. For larger\npage sizes, the number of necessary pages for the map-\nping (or anonymous memory) is reduced; with it falls the\nnumber of page faults.\nMost architectures support page sizes of 4k. On IA-64\nand PPC64, page sizes of 64k are also popular. That\nmeans the smallest unit in which memory is given out\nis 64k. The value has to be speciﬁed when compiling the\nkernel and cannot be changed dynamically (at least not\nat the moment). The ABIs of the multiple-page-size ar-\nchitectures are designed to allow running an application\nwith either page size. The runtime will make the nec-\nessary adjustments, and a correctly-written program will\nnot notice a thing. Larger page sizes mean more waste\nthrough partially-used pages, but, in some situations, this\nis OK.\nMost architectures also support very large page sizes of\n1MB or more. Such pages are useful in some situations,\ntoo, but it makes no sense to have all memory given out\nin units that large. The waste of physical RAM would\nsimply be too large. But very large pages have their ad-\nvantages: if huge data sets are used, storing them in 2MB\npages on x86-64 would require 511 fewer page faults (per\nlarge page) than using the same amount of memory with\n4k pages. This can make a big difference. The solution is\nto selectively request memory allocation which, just for\nthe requested address range, uses huge memory pages\nand, for all the other mappings in the same process, uses\nthe normal page size.\nHuge page sizes come with a price, though. Since the\nphysical memory used for large pages must be contin-\nuous, it might, after a while, not be possible to allo-\ncate such pages due to memory fragmentation. People\nare working on memory defragmentation and fragmen-\ntation avoidance, but it is very complicated. For large\npages of, say, 2MB the necessary 512 consecutive pages\nare always hard to come by, except at one time: when\nthe system boots up. This is why the current solution\nfor large pages requires the use of a special ﬁlesystem,\nhugetlbfs . This pseudo ﬁlesystem is allocated on re-\nquest by the system administrator by writing the number\nof huge pages which should be reserved to\n/proc/sys/vm/nr_hugepages\nUlrich Drepper Version 1.0 87\nThis operation might fail if not enough continuous mem-\nory can be located. The situation gets especially interest-\ning if virtualization is used. A virtualized system using\nthe VMM model does not directly administrate physi-\ncal memory and, therefore, cannot by itself allocate the\nhugetlbfs . It has to rely on the VMM, and this feature\nis not guaranteed to be supported. For the KVM model,\nthe Linux kernel running the KVM module can perform\nthehugetlbfs allocation and possibly pass a subset of\nthe pages thus allocated on to one of the guest domains.\nLater, when a program needs a large page, there are mul-\ntiple possibilities:\n• the program can use the System V shared memory\ninterfaces with the SHM_HUGETLB ﬂag.\n• a ﬁlesystem of type hugetlbfs can actually be\nmounted and the program can then create a ﬁle un-\nder the mount point and use mmap to map one or\nmore pages as anonymous memory.\nIn the ﬁrst case, the hugetlbfs need not be mounted.\nCode requesting one or more large pages could look like\nthis:\nkey_t k = ftok(""/some/key/file"", 42);\nint id = shmget(k, LENGTH,\nSHM_HUGETLB|IPC_CREAT\n|SHM_R|SHM_W);\nvoid *a = shmat(id, NULL, 0);\nThe critical parts of this code sequence are the use of the\nSHM_HUGETLB ﬂag and the choice of the right value for\nLENGTH , which must be a multiple of the huge page size\nfor the system. Different architectures have different val-\nues. The use of the System V shared memory interface\nhas the nasty problem of depending on the key argument\nto differentiate (or share) mappings. The ftok interface\ncan easily produce conﬂicts which is why, if possible, it\nis better to use other mechanisms.\nIf the requirement to mount the hugetlbfs ﬁlesystem is\nnot a problem, it is better to use it instead of System V\nshared memory. The only real problems with using the\nspecial ﬁlesystem are that the kernel must support it, and\nthat there is no standardized mount point yet. Once the\nﬁlesystem is mounted, for instance at /dev/hugetlb , a\nprogram can make easy use of it:\nint fd = open(""/dev/hugetlb/file1"",\nO_RDWR|O_CREAT, 0700);\nvoid *a = mmap(NULL, LENGTH,\nPROT_READ|PROT_WRITE,\nfd, 0);By using the same ﬁle name in the open call, multiple\nprocesses can share the same huge pages and collaborate.\nIt is also possible to make the pages executable, in which\ncase the PROT_EXEC ﬂag must also be set in the mmap\ncall. As in the System V shared memory example, the\nvalue of LENGTH must be a multiple of the system’s huge\npage size.\nA defensively-written program (as all programs should\nbe) can determine the mount point at runtime using a\nfunction like this:\nchar *hugetlbfs_mntpoint(void) {\nchar *result = NULL;\nFILE *fp = setmntent(_PATH_MOUNTED, ""r"");\nif (fp != NULL) {\nstruct mntent *m;\nwhile ((m = getmntent(fp)) != NULL)\nif (strcmp(m->mnt_fsname,\n""hugetlbfs"") == 0) {\nresult = strdup(m->mnt_dir);\nbreak;\n}\nendmntent(fp);\n}\nreturn result;\n}\nMore information for both these cases can be found in\nthe hugetlbpage.txt ﬁle which comes as part of the kernel\nsource tree. The ﬁle also describes the special handling\nneeded for IA-64.\n050100150200250300350400450\n210213216219222225228\nWorking Set Size (Bytes)Cycles/List Element\n4kB Pages 2MB Pages\nFigure 7.9: Follow with Huge Pages, NPAD=0\nTo illustrate the advantages of huge pages, Figure 7.9\nshows the results of running the random Follow test for\nNPAD =0. This is the same data shown in Figure 3.15,\nbut, this time, we measure the data also with memory al-\nlocated in huge pages. As can be seen the performance\nadvantage can be huge. For 220bytes the test using huge\npages is 57% faster. This is due to the fact that this size\n88 Version 1.0 What Every Programmer Should Know About Memory",21404
41-8 Upcoming Technology.pdf,41-8 Upcoming Technology,,0
42-8.1 The Problem with Atomic Operations.pdf,42-8.1 The Problem with Atomic Operations,"still ﬁts completely into one single 2MB page and, there-\nfore, no DTLB misses occur.\nAfter this point, the winnings are initially smaller but\ngrow again with increasing working set size. The huge\npages test is 38% faster for the 512MB working set size.\nThe curve for the huge page test has a plateau at around\n250 cycles. Beyond working sets of 227bytes, the num-\nbers rise signiﬁcantly again. The reason for the plateau\nis that 64 TLB entries for 2MB pages cover 227bytes.\nAs these numbers show, a large part of the costs of using\nlarge working set sizes comes from TLB misses. Using\nthe interfaces described in this section can pay off big-\ntime. The numbers in the graph are, most likely, upper\nlimits, but even real-world programs show a signiﬁcant\nspeed-up. Databases, since they use large amounts of\ndata, are among the programs which use huge pages to-\nday.\nThere is currently no way to use large pages to map ﬁle-\nbacked data. There is interest in implementing this ca-\npability, but the proposals made so far all involve explic-\nitly using large pages, and they rely on the hugetlbfs\nﬁlesystem. This is not acceptable: large page use in this\ncase must be transparent. The kernel can easily deter-\nmine which mappings are large and automatically use\nlarge pages. A big problem is that the kernel does not al-\nways know about the use pattern. If the memory, which\ncould be mapped as a large page, later requires 4k-page\ngranularity (for instance, because the protection of parts\nof the memory range is changed using mprotect ) a lot\nof precious resources, in particular the linear physical\nmemory, will have been wasted. So it will certainly be\nsome more time before such an approach is successfully\nimplemented.8 Upcoming Technology\nIn the preceding sections about multi-processor handling\nwe have seen that signiﬁcant performance problems must\nbe expected if the number of CPUs or cores is scaled up.\nBut this scaling-up is exactly what has to be expected in\nthe future. Processors will get more and more cores, and\nprograms must be ever more parallel to take advantage\nof the increased potential of the CPU, since single-core\nperformance will not rise as quickly as it used to.\n8.1 The Problem with Atomic Operations\nSynchronizing access to shared data structures is tradi-\ntionally done in two ways:\n• through mutual exclusion, usually by using func-\ntionality of the system runtime to achieve just that;\n• by using lock-free data structures.\nThe problem with lock-free data structures is that the pro-\ncessor has to provide primitives which can perform the\nentire operation atomically. This support is limited. On\nmost architectures support is limited to atomically read\nand write a word. There are two basic ways to imple-\nment this (see section 6.4.2):\n• using atomic compare-and-exchange (CAS) oper-\nations;\n• using a load lock/store conditional (LL/SC) pair.\nIt can be easily seen how a CAS operation can be im-\nplemented using LL/SC instructions. This makes CAS\noperations the building block for most atomic operations\nand lock free data structures.\nSome processors, notably the x86 and x86-64 architec-\ntures, provide a far more elaborate set of atomic opera-\ntions. Many of them are optimizations of the CAS op-\neration for speciﬁc purposes. For instance, atomically\nadding a value to a memory location can be implemented\nusing CAS and LL/SC operations, but the native support\nfor atomic increments on x86/x86-64 processors is faster.\nIt is important for programmers to know about these op-\nerations, and the intrinsics which make them available\nwhen programming, but that is nothing new.\nThe extraordinary extension of these two architectures\nis that they have double-word CAS (DCAS) operations.\nThis is signiﬁcant for some applications but not all (see\n[5]). As an example of how DCAS can be used, let us try\nto write a lock-free array-based stack/LIFO data struc-\nture. A ﬁrst attempt using gcc’s intrinsics can be seen in\nFigure 8.1.\nThis code is clearly not thread-safe. Concurrent accesses\nin different threads will modify the global variable top\nUlrich Drepper Version 1.0 89\nstruct elem {\ndata_t d;\nstruct elem *c;\n};\nstruct elem *top;\nvoid push(struct elem *n) {\nn->c = top;\ntop = n;\n}\nstruct elem *pop(void) {\nstruct elem *res = top;\nif (res != NULL)\ntop = res->c;\nreturn res;\n}\nFigure 8.1: Not Thread-Safe LIFO\nwithout consideration of other threads’s modiﬁcations.\nElements could be lost or removed elements can magi-\ncally reappear. It is possible to use mutual exclusion but\nhere we will try to use only atomic operations.\nThe ﬁrst attempt to ﬁx the problem uses CAS operations\nwhen installing or removing list elements. The resulting\ncode looks like Figure 8.2.\n#define CAS __sync_bool_compare_and_swap\nstruct elem {\ndata_t d;\nstruct elem *c;\n};\nstruct elem *top;\nvoid push(struct elem *n) {\ndo\nn->c = top;\nwhile (!CAS(&top, n->c, n));\n}\nstruct elem *pop(void) {\nstruct elem *res;\nwhile ((res = top) != NULL)\nif (CAS(&top, res, res->c))\nbreak;\nreturn res;\n}\nFigure 8.2: LIFO using CAS\nAt ﬁrst glance this looks like a working solution. top is\nnever modiﬁed unless it matches the element which was\nat the top of the LIFO when the operation started. But\nwe have to take concurrency at all levels into account. It\nmight be that another thread working on the data struc-\nture is scheduled at the worst possible moment. One such\ncase here is the so-called ABA problem. Consider what\nhappens if a second thread is scheduled right before the\nCAS operation in pop and it performs the following op-\neration:1.l = pop()\n2.push(newelem)\n3.push(l)\nThe end effect of this operation is that the former top\nelement of the LIFO is back at the top but the second\nelement is different. Back in the ﬁrst thread, because\nthe top element is unchanged, the CAS operation will\nsucceed. But the value res->c is not the right one. It is\na pointer to the second element of the original LIFO and\nnotnewelem . The result is that this new element is lost.\nIn the literature [10] you ﬁnd suggestions to use a feature\nfound on some processors to work around this problem.\nSpeciﬁcally, this is about the ability of the x86 and x86-\n64 processors to perform DCAS operations. This is used\nin the third incarnation of the code in Figure 8.3.\n#define CAS __sync_bool_compare_and_swap\nstruct elem {\ndata_t d;\nstruct elem *c;\n};\nstruct lifo {\nstruct elem *top;\nsize_t gen;\n} l;\nvoid push(struct elem *n) {\nstruct lifo old, new;\ndo {\nold = l;\nnew.top = n->c = old.top;\nnew.gen = old.gen + 1;\n} while (!CAS(&l, old, new));\n}\nstruct elem *pop(void) {\nstruct lifo old, new;\ndo {\nold = l;\nif (old.top == NULL) return NULL;\nnew.top = old.top->c;\nnew.gen = old.gen + 1;\n} while (!CAS(&l, old, new));\nreturn old.top;\n}\nFigure 8.3: LIFO using double-word CAS\nUnlike the other two examples, this is (currently) pseudo-\ncode since gcc does not grok the use of structures in the\nCAS intrinsics. Regardless, the example should be suf-\nﬁcient understand the approach. A generation counter\nis added to the pointer to the top of the LIFO. Since it is\nchanged on every operation, push orpop, the ABA prob-\nlem described above is no longer a problem. By the time\nthe ﬁrst thread is resuming its work by actually exchang-\ning the top pointer, the generation counter has been in-\ncremented three times. The CAS operation will fail and,\nin the next round of the loop, the correct ﬁrst and second\n90 Version 1.0 What Every Programmer Should Know About Memory",7618
43-8.2 Transactional Memory.pdf,43-8.2 Transactional Memory,,0
44-8.2.1 Load LockStore Conditional Implementation.pdf,44-8.2.1 Load LockStore Conditional Implementation,,0
45-8.3 Increasing Latency.pdf,45-8.3 Increasing Latency,"element of the LIFO are determined and the LIFO is not\ncorrupted. V oil `a.\nIs this really the solution? The authors of [10] certainly\nmake it sound like it and, to their credit, it should be men-\ntioned that it is possible to construct data structures for\nthe LIFO which would permit using the code above. But,\nin general, this approach is just as doomed as the previ-\nous one. We still have concurrency problems, just now\nin a different place. Let us assume a thread executes pop\nand is interrupted after the test for old.top == NULL .\nNow a second thread uses pop and receives ownership\nof the previous ﬁrst element of the LIFO. It can do any-\nthing with it, including changing all values or, in case of\ndynamically allocated elements, freeing the memory.\nNow the ﬁrst thread resumes. The old variable is still\nﬁlled with the previous top of the LIFO. More speciﬁ-\ncally, the top member points to the element popped by\nthe second thread. In new.top = old.top->c the ﬁrst\nthread dereferences a pointer in the element. But the ele-\nment this pointer references might have been freed. That\npart of the address space might be inaccessible and the\nprocess could crash. This cannot be allowed for a generic\ndata type implementation. Any ﬁx for this problem is ter-\nribly expensive: memory must never be freed, or at least\nit must be veriﬁed that no thread is referencing the mem-\nory anymore before it is freed. Given that lock-free data\nstructures are supposed to be faster and more concurrent,\nthese additional requirements completely destroy any ad-\nvantage. In languages which support it, memory han-\ndling through garbage collection can solve the problem,\nbut this comes with its price.\nThe situation is often worse for more complex data struc-\ntures. The same paper cited above also describes a FIFO\nimplementation (with reﬁnements in a successor paper).\nBut this code has all the same problems. Because CAS\noperations on existing hardware (x86, x86-64)47are lim-\nited to modifying two words which are consecutive in\nmemory, they are no help at all in other common situa-\ntions. For instance, atomically adding or removing ele-\nments anywhere in a double-linked list is not possible.\nThe problem is that more than one memory address is\ngenerally involved, and only if none of the values of these\naddresses is changed concurrently can the entire oper-\nation succeed. This is a well-known concept in data-\nbase handling, and this is exactly where one of the most\npromising proposals to solve the dilemma comes from.\n8.2 Transactional Memory\nIn their groundbreaking 1993 paper [13] Herlihy and Moss\npropose to implement transactions for memory opera-\ntions in hardware since software alone cannot deal with\nthe problem efﬁciently. Digital Equipment Corporation,\nat that time, was already battling with scalability prob-\n47As a side note, the developers of the IA-64 did notinclude this\nfeature. They allow comparing two words, but replacing only one.lems on their high-end hardware, which featured a few\ndozen processors. The principle is the same as for data-\nbase transactions: the result of a transaction becomes vis-\nible all at once or the transaction is aborted and all the\nvalues remain unchanged.\nThis is where memory comes into play and why the pre-\nvious section bothered to develop algorithms which use\natomic operations. Transactional memory is meant as\na replacement for–and extension of–atomic operations\nin many situations, especially for lock-free data struc-\ntures. Integrating a transaction system into the processor\nsounds like a terribly complicated thing to do but, in fact,\nmost processors, to some extent, already have something\nsimilar.\nThe LL/SC operations implemented by some processors\nform a transaction. The SC instruction aborts or commits\nthe transaction based on whether the memory location\nwas touched or not. Transactional memory is an exten-\nsion of this concept. Now, instead of a simple pair of in-\nstructions, multiple instructions take part in the transac-\ntion. To understand how this can work, it is worthwhile to\nﬁrst see how LL/SC instructions can be implemented.48\n8.2.1 Load Lock/Store Conditional Implementation\nIf the LL instruction is issued, the value of the memory\nlocation is loaded into a register. As part of that oper-\nation, the value is loaded into L1d. The SC instruction\nlater can only succeed if this value has not been tampered\nwith. How can the processor detect this? Looking back\nat the description of the MESI protocol in Figure 3.18\nshould make the answer obvious. If another processor\nchanges the value of the memory location, the copy of\nthe value in L1d of the ﬁrst processor must be revoked.\nWhen the SC instruction is executed on the ﬁrst proces-\nsor, it will ﬁnd it has to load the value again into L1d.\nThis is something the processor must already detect.\nThere are a few more details to iron out with respect to\ncontext switches (possible modiﬁcation on the same pro-\ncessor) and accidental reloading of the cache line after\na write on another processor. This is nothing that poli-\ncies (cache ﬂush on context switch) and extra ﬂags, or\nseparate cache lines for LL/SC instructions, cannot ﬁx.\nIn general, the LL/SC implementation comes almost for\nfree with the implementation of a cache coherence pro-\ntocol like MESI.\n8.2.2 Transactional Memory Operations\nFor transactional memory to be generally useful, a trans-\naction must not be ﬁnished with the ﬁrst store instruction.\nInstead, an implementation should allow a certain num-\nber of load and store operations; this means we need sep-\narate commit and abort instructions. In a bit we will see\nthat we need one more instruction which allows check-\n48This does not mean it is actually implemented like this.\nUlrich Drepper Version 1.0 91\ning on the current state of the transaction and whether it\nis already aborted or not.\nThere are three different memory operations to imple-\nment:\n• Read memory\n• Read memory which is written to later\n• Write memory\nWhen looking at the MESI protocol it should be clear\nhow this special second type of read operation can be\nuseful. The normal read can be satisﬁed by a cache line\nin the ‘E’ and ‘S’ state. The second type of read opera-\ntion needs a cache line in state ‘E’. Exactly why the sec-\nond type of memory read is necessary can be glimpsed\nfrom the following discussion, but, for a more complete\ndescription, the interested reader is referred to literature\nabout transactional memory, starting with [13].\nIn addition, we need transaction handling which mainly\nconsists of the commit and abort operation we are al-\nready familiar with from database transaction handling.\nThere is one more operation, though, which is optional\nin theory but required for writing robust programs using\ntransactional memory. This instruction lets a thread test\nwhether the transaction is still on track and can (perhaps)\nbe committed later, or whether the transaction already\nfailed and will in any case be aborted.\nWe will discuss how these operations actually interact\nwith the CPU cache and how they match to bus operation.\nBut before we do that we take a look at some actual code\nwhich uses transactional memory. This will hopefully\nmake the remainder of this section easier to understand.\n8.2.3 Example Code Using Transactional Memory\nFor the example we revisit our running example and pro-\nvide a LIFO implementation which uses the transactional\nmemory primitives.\nThis code looks quite similar to the not-thread-safe code,\nwhich is an additional bonus as it makes writing code us-\ning transactional memory easier. The new parts of the\ncode are the LTX,ST,COMMIT , and VALIDATE opera-\ntions. These four operations are the way to request ac-\ncesses to transactional memory. There is actually one\nmore operation, LT, which is not used here. LTrequests\nnon-exclusive read access, LTX requests exclusive read\naccess, and STis a store into transactional memory. The\nVALIDATE operation checks whether the transaction is\nstill on track to be committed. It returns true if this trans-\naction is still OK. If the transaction is already marked as\naborting, it will be actually aborted and a value indicating\nthis is returned. The next transactional memory instruc-\ntion will start a new transaction. For this reason, the code\nuses a new ifblock in case the transaction is still going\non.struct elem {\ndata_t d;\nstruct elem *c;\n};\nstruct elem *top;\nvoid push(struct elem *n) {\nwhile (1) {\nn->c = LTX(top);\nST(&top, n);\nif (COMMIT())\nreturn;\n... delay ...\n}\n}\nstruct elem *pop(void) {\nwhile (1) {\nstruct elem *res = LTX(top);\nif (VALIDATE()) {\nif (res != NULL)\nST(&top, res->c);\nif (COMMIT())\nreturn res;\n}\n... delay ...\n}\n}\nFigure 8.4: LIFO Using Transactional Memory\nTheCOMMIT operation ﬁnishes the transaction; if it is ﬁn-\nished successfully the operation returns true. This means\nthat this part of the program is done and the thread can\nmove on. If the operation returns a false value, this usu-\nally means the whole code sequence must be repeated.\nThis is what the outer while loop is doing here. This\nis not absolutely necessary, though, in some cases giving\nup on the work is the right thing to do.\nThe interesting point about the LT,LTX, and STopera-\ntions is that they can fail without signaling this failure in\nany direct way. The way the program can request this\ninformation is through the VALIDATE orCOMMIT oper-\nation. For the load operation, this can mean that the\nvalue actually loaded into the register might be bogus;\nthat is why it is necessary in the example above to use\nVALIDATE before dereferencing the pointer. In the next\nsection, we will see why this is a wise choice for an im-\nplementation. It might be that, once transactional mem-\nory is actually widely available, the processors will im-\nplement something different. The results from [13] sug-\ngest what we describe here, though.\nThepush function can be summarized as this: the trans-\naction is started by reading the pointer to the head of the\nlist. The read requests exclusive ownership since, later in\nthe function, this variable is written to. If another thread\nhas already started a transaction, the load will fail and\nmark the still-born transaction as aborted; in this case,\nthe value actually loaded might be garbage. This value\nis, regardless of its status, stored in the next ﬁeld of the\n92 Version 1.0 What Every Programmer Should Know About Memory\nnew list member. This is ﬁne since this member is not\nyet in use, and it is accessed by exactly one thread. The\npointer to the head of the list is then assigned the pointer\nto the new element. If the transaction is still OK, this\nwrite can succeed. This is the normal case, it can only\nfail if a thread uses some code other than the provided\npush andpop functions to access this pointer. If the\ntransaction is already aborted at the time the STis ex-\necuted, nothing at all is done. Finally, the thread tries\nto commit the transaction. If this succeeds the work is\ndone; other threads can now start their transactions. If\nthe transaction fails, it must be repeated from the begin-\nning. Before doing that, however, it is best to insert an\ndelay. If this is not done the thread might run in a busy\nloop (wasting energy, overheating the CPU).\nThepop function is slightly more complex. It also starts\nwith reading the variable containing the head of the list,\nrequesting exclusive ownership. The code then immedi-\nately checks whether the LTXoperation succeeded or not.\nIf not, nothing else is done in this round except delaying\nthe next round. If the top pointer was read successfully,\nthis means its state is good; we can now dereference\nthe pointer. Remember, this was exactly the problem\nwith the code using atomic operations; with transactional\nmemory this case can be handled without any problem.\nThe following SToperation is only performed when the\nLIFO is not empty, just as in the original, thread-unsafe\ncode. Finally the transaction is committed. If this suc-\nceeds the function returns the old pointer to the head;\notherwise we delay and retry. The one tricky part of this\ncode is to remember that the VALIDATE operation aborts\nthe transaction if it has already failed. The next trans-\nactional memory operation would start a new transaction\nand, therefore, we must skip over the rest of the code in\nthe function.\nHow the delay code works will be something to see when\nimplementations of transactional memory are available\nin hardware. If this is done badly system performance\nmight suffer signiﬁcantly.\n8.2.4 Bus Protocol for Transactional Memory\nNow that we have seen the basic principles behind trans-\nactional memory, we can dive into the details of the im-\nplementation. Note that this is notbased on actual hard-\nware. It is based on the original design of transactional\nmemory and knowledge about the cache coherency pro-\ntocol. Some details are omitted, but it still should be pos-\nsible to get insight into the performance characteristics.\nDespite the name, transactional memory is not actually\nimplemented as separate memory; that would not make\nany sense given that transactions on any location in a\nthread’s address space are wanted. Instead, it is imple-\nmented as part of the ﬁrst level cache handling. The im-\nplementation could, in theory, happen in the normal L1d\nbut, as [13] points out, this is not a good idea. We will\nmore likely see the transaction cache implemented in par-allel to L1d. All accesses will use the higher level cache\nin the same way they use L1d. The transaction cache is\nlikely much smaller than L1d. If it is fully associative its\nsize is determined by the number of operations a trans-\naction can comprise. Implementations will likely have\nlimits for the architecture and/or speciﬁc processor ver-\nsion. One could easily imagine a transaction cache with\n16 elements or even less. In the above example we only\nneeded one single memory location; algorithms with a\nlarger transaction working sets get very complicated. It is\npossible that we will see processors which support more\nthan one active transaction at any one time. The num-\nber of elements in the cache then multiplies, but it is still\nsmall enough to be fully associative.\nThe transaction cache and L1d are exclusive. That means\na cache line is in, at most, one of the caches but never in\nboth. Each slot in the transaction cache is in, at any one\ntime, one of the four MESI protocol states. In addition\nto this, a slot has an transaction state. The states are as\nfollows (names according to [13]):\nEMPTY the cache slot contains no data. The MESI\nstate is always ‘I’.\nNORMAL the cache slot contains committed data. The\ndata could as well exist in L1d. The MESI state\ncan be ‘M’, ‘E’, and ‘S’. The fact that the ‘M’ state\nis allowed means that transaction commits do not\nforce the data to be written into the main memory\n(unless the memory region is declared as uncached\nor write-through). This can signiﬁcantly help to\nincrease performance.\nXABORT the cache slot contains data which is to be\ndiscarded on abort. This is obviously the opposite\nof XCOMMIT. All the data created during a trans-\naction is kept in the transaction cache, nothing is\nwritten to main memory before a commit. This\nlimits the maximum transaction size but it means\nthat, beside the transaction cache, no other mem-\nory has to be aware of the XCOMMIT/XABORT\nduality for a single memory location. The possible\nMESI states are ‘M’, ‘E’, and ‘S’.\nXCOMMIT the cache slot contains data which is dis-\ncarded on commit. This is a possible optimization\nprocessors could implement. If a memory loca-\ntion is changed using a transaction operation, the\nold content cannot be just dropped: if the transac-\ntion fails the old content needs to be restored. The\nMESI states are the same as for XABORT. One\ndifference with regard to XABORT is that, if the\ntransaction cache is full, any XCOMMIT entries\nin the ‘M’ state could be written back to memory\nand then, for all states, discarded.\nWhen an LToperation is started, the processor allocates\ntwo slots in the cache. Victims are chosen by ﬁrst looking\nfor NORMAL slots for the address of the operation, i.e.,\nUlrich Drepper Version 1.0 93\na cache hit. If such an entry is found, a second slot is\nlocated, the value copied, one entry is marked XABORT,\nand the other one is marked XCOMMIT.\nIf the address is not already cached, EMPTY cache slots\nare located. If none can be found, NORMAL slots are\nlooked for. The old content must then be ﬂushed to mem-\nory if the MESI state is ‘M’. If no NORMAL slot is\navailable either, it is possible to victimize XCOMMIT\nentries. This is likely going to be an implementation de-\ntail, though. The maximum size of a transaction is de-\ntermined by the size of the transaction cache, and, since\nthe number of slots which are needed for each operation\nin the transaction is ﬁxed, the number of transactions can\nbe capped before having to evict XCOMMIT entries.\nIf the address is not found in the transactional cache, a\nTREAD request is issued on the bus. This is just like\nthe normal READ bus request, but it indicates that this\nis for the transactional cache. Just like for the normal\nREAD request, the caches in all other processors ﬁrst get\nthe chance to respond. If none does the value is read\nfrom the main memory. The MESI protocol determines\nwhether the state of the new cache line is ‘E’ or ‘S’. The\ndifference between T READ and READ comes into play\nwhen the cache line is currently in use by an active trans-\naction on another processor or core. In this case the T -\nREAD operation plainly fails, no data is transmitted. The\ntransaction which generated the T READ bus request is\nmarked as failed and the value used in the operation (usu-\nally a simple register load) is undeﬁned. Looking back\nto the example, we can see that this behavior does not\ncause problems if the transactional memory operations\nare used correctly. Before a value loaded in a transaction\nis used, it must be veriﬁed with VALIDATE . This is, in\nalmost no cases, an extra burden. As we have seen in the\nattempts to create a FIFO implementation using atomic\noperations, the check which we added is the one missing\nfeature which would make the lock-free code work.\nTheLTXoperation is almost identical to LT. The one dif-\nference is that the bus operation is T RFO instead of T -\nREAD. T RFO, like the normal RFO bus message, re-\nquests exclusive ownership of the cache line. The state\nof the resulting cache line is ‘E’. Like the T READ bus\nrequest, T RFO can fail, in which case the used value is\nundeﬁned, too. If the cache line is already in the local\ntransaction cache with ‘M’ or ‘E’ state, nothing has to be\ndone. If the state in the local transaction cache is ‘S’ the\nbus request has to go out to invalidate all other copies.\nTheSToperation is similar to LTX. The value is ﬁrst\nmade available exclusively in the local transaction cache.\nThen the SToperation makes a copy of the value into a\nsecond slot in the cache and marks the entry as XCOM-\nMIT. Lastly, the other slot is marked as XABORT and\nthe new value is written into it. If the transaction is al-\nready aborted, or is newly aborted because the implicit\nLTXfails, nothing is written.\nNeither the VALIDATE norCOMMIT operations automat-ically and implicitly create bus operations. This is the\nhuge advantage transactional memory has over atomic\noperations. With atomic operations, concurrency is made\npossible by writing changed values back into main mem-\nory. If you have read this document thus far, you should\nknow how expensive this is. With transactional memory,\nno accesses to the main memory are forced. If the cache\nhas no EMPTY slots, current content must be evicted,\nand for slots in the ‘M’ state, the content must be writ-\nten to main memory. This is not different from regular\ncaches, and the write-back can be performed without spe-\ncial atomicity guarantees. If the cache size is sufﬁcient,\nthe content can survive for a long time. If transactions\nare performed on the same memory location over and\nover again, the speed improvements can be astronomical\nsince, in the one case, we have one or two main memory\naccesses in each round while, for transactional memory,\nall accesses hit the transactional cache, which is as fast\nas L1d.\nAll the VALIDATE andCOMMIT operations do at the time\nof an abort of a transaction is to mark the cache slots\nmarked XABORT as empty and mark the XCOMMIT\nslots as NORMAL. Similarly, when COMMIT successfully\nﬁnishes a transaction, the XCOMMIT slots are marked\nempty and the XABORT slots are marked NORMAL.\nThese are very fast operations on the transaction cache.\nNo explicit notiﬁcation to other processors which want to\nperform transactions happens; those processors just have\nto keep trying. Doing this efﬁciently is another matter. In\nthe example code above we simply have ...delay...\nin the appropriate place. We might see actual processor\nsupport for delaying in a useful way.\nTo summarize, transactional memory operations cause\nbus operation only when a new transaction is started and\nwhen a new cache line, which is not already in the trans-\naction cache, is added to a still-successful transaction.\nOperations in aborted transactions do not cause bus op-\nerations. There will be no cache line ping-pong due to\nmultiple threads trying to use the same memory.\n8.2.5 Other Considerations\nIn section 6.4.2, we already discussed how the lock pre-\nﬁx, available on x86 and x86-64, can be used to avoid the\ncoding of atomic operations in some situations. The pro-\nposed tricks falls short, though, when there are multiple\nthreads in use which do not contend for the same mem-\nory. In this case, the atomic operations are used unnec-\nessarily. With transactional memory this problem goes\naway. The expensive RFO bus message are issued only\nif memory is used on different CPUs concurrently or in\nsuccession; this is only the case when they are needed. It\nis almost impossible to do any better.\nThe attentive reader might have wondered about delays.\nWhat is the expected worst case scenario? What if the\nthread with the active transaction is descheduled, or if it\nreceives a signal and is possibly terminated, or decides to\n94 Version 1.0 What Every Programmer Should Know About Memory",22634
46-8.4 Vector Operations.pdf,46-8.4 Vector Operations,"usesiglongjmp to jump to an outer scope? The answer\nto this is: the transaction will be aborted. It is possible to\nabort a transaction whenever a thread makes a system call\nor receives a signal (i.e., a ring level change occurs). It\nmight also be that aborting the transaction is part of the\nOS’s duties when performing system calls or handling\nsignals. We will have to wait until implementations be-\ncome available to see what is actually done.\nThe ﬁnal aspect of transactional memory which should\nbe discussed here is something which people might want\nto think about even today. The transaction cache, like\nother caches, operates on cache lines. Since the transac-\ntion cache is an exclusive cache, using the same cache\nline for transactions and non-transaction operation will\nbe a problem. It is therefore important to\n• move non-transactional data off of the cache line\n• have separate cache lines for data used in separate\ntransactions\nThe ﬁrst point is not new, the same effort will pay off for\natomic operations today. The second is more problem-\natic since today objects are hardly ever aligned to cache\nlines due to the associated high cost. If the data used,\nalong with the words modiﬁed using atomic operations,\nis on the same cache line, one less cache line is needed.\nThis does not apply to mutual exclusion (where the mu-\ntex object should always have its own cache line), but one\ncan certainly make cases where atomic operations go to-\ngether with other data. With transactional memory, using\nthe cache line for two purposes will most likely be fatal.\nEvery normal access to data49would remove the cache\nline from the transactional cache, aborting the transac-\ntion in the process. Cache alignment of data objects will\nbe in future not only a matter of performance but also of\ncorrectness.\nIt is possible that transactional memory implementations\nwill use more precise accounting and will, as a result, not\nsuffer from normal accesses to data on cache lines which\nare part of a transaction. This requires a lot more effort,\nthough, since then the MESI protocol information is not\nsufﬁcient anymore.\n8.3 Increasing Latency\nOne thing about future development of memory tech-\nnology is almost certain: latency will continue to creep\nup. We already discussed, in section 2.2.4, that the up-\ncoming DDR3 memory technology will have higher la-\ntency than the current DDR2 technology. FB-DRAM,\nif it should get deployed, also has potentially higher la-\ntency, especially when FB-DRAM modules are daisy-\nchained. Passing through the requests and results does\nnot come for free.\n49From the cache line in question. Access to arbitrary other cache\nlines does not inﬂuence the transaction.The second source of latency is the increasing use of\nNUMA. AMD’s Opterons are NUMA machines if they\nhave more than one processor. There is some local mem-\nory attached to the CPU with its own memory controller\nbut, on SMP motherboards, the rest of the memory has to\nbe accessed through the Hypertransport bus. Intel’s CSI\ntechnology will use almost the same technology. Due to\nper-processor bandwidth limitations and the requirement\nto service (for instance) multiple 10Gb/s Ethernet cards,\nmulti-socket motherboards will not vanish, even if the\nnumber of cores per socket increases.\nA third source of latency are co-processors. We thought\nthat we got rid of them after math co-processors for com-\nmodity processors were no longer necessary at the be-\nginning of the 1990’s, but they are making a comeback.\nIntel’s Geneseo and AMD’s Torrenza are extensions of\nthe platform to allow third-party hardware developers to\nintegrate their products into the motherboards. I.e., the\nco-processors will not have to sit on a PCIe card but, in-\nstead, are positioned much closer to the CPU. This gives\nthem more bandwidth.\nIBM went a different route (although extensions like In-\ntel’s and AMD’s are still possible) with the Cell CPU.\nThe Cell CPU consists, beside the PowerPC core, of 8\nSynergistic Processing Units (SPUs) which are special-\nized processors mainly for ﬂoating-point computation.\nWhat co-processors and SPUs have in common is that\nthey, most likely, have even slower memory logic than\nthe real processors. This is, in part, caused by the nec-\nessary simpliﬁcation: all the cache handling, prefetching\netc is complicated, especially when cache coherency is\nneeded, too. High-performance programs will increas-\ningly rely on co-processors since the performance differ-\nences can be dramatic. Theoretical peak performance for\na Cell CPU is 210 GFLOPS, compared to 50-60 GFLOPS\nfor a high-end CPU. Graphics Processing Units (GPUs,\nprocessors on graphics cards) in use today achieve even\nhigher numbers (north of 500 GFLOPS) and those could\nprobably, with not too much effort, be integrated into the\nGeneseo/Torrenza systems.\nAs a result of all these developments, a programmer must\nconclude that prefetching will become ever more impor-\ntant. For co-processors it will be absolutely critical. For\nCPUs, especially with more and more cores, it is neces-\nsary to keep the FSB busy all the time instead of piling\non the requests in batches. This requires giving the CPU\nas much insight into future trafﬁc as possible through the\nefﬁcient use of prefetching instructions.\n8.4 Vector Operations\nThe multi-media extensions in today’s mainstream pro-\ncessors implement vector operations only in a limited\nfashion. Vector instructions are characterized by large\nnumbers of operations which are performed as the re-\nsult of one instruction (Single Instruction Multiple Data,\nSIMD). Compared with scalar operations, this can be\nUlrich Drepper Version 1.0 95\nsaid about the multi-media instructions, but it is a far\ncry from what vector computers like the Cray-1 or vector\nunits for machines like the IBM 3090 did.\nTo compensate for the limited number of operations per-\nformed for one instruction (four float or two double\noperations on most machines) the surrounding loops have\nto be executed more often. The example in section A.1\nshows this clearly, each cache line requires SMiterations.\nWith wider vector registers and operations, the number\nof loop iterations can be reduced. This results in more\nthan just improvements in the instruction decoding etc.;\nhere we are more interested in the memory effects. With\na single instruction loading or storing more data, the pro-\ncessor has a better picture about the memory use of the\napplication and does not have to try to piece together the\ninformation from the behavior of individual instructions.\nFurthermore, it becomes more useful to provide load or\nstore instructions which do not affect the caches. With\n16 byte wide loads of an SSE register in an x86 CPU, it\nis a bad idea to use uncached loads since later accesses to\nthe same cache line have to load the data from memory\nagain (in case of cache misses). If, on the other hand,\nthe vector registers are wide enough to hold one or more\ncache lines, uncached loads or stores do not have nega-\ntive impacts. It becomes more practical to perform oper-\nations on data sets which do not ﬁt into the caches.\nHaving large vector registers does not necessarily mean\nthe latency of the instructions is increased; vector in-\nstructions do not have to wait until all data is read or\nstored. The vector units could start with the data which\nhas already been read if it can recognize the code ﬂow.\nThat means, if, for instance, a vector register is to be\nloaded and then all vector elements multiplied by a scalar,\nthe CPU could start the multiplication operation as soon\nas the ﬁrst part of the vector has been loaded. It is just\na matter of sophistication of the vector unit. What this\nshows is that, in theory, the vector registers can grow\nreally wide, and that programs could potentially be de-\nsigned today with this in mind. In practice, there are lim-\nitations imposed on the vector register size by the fact\nthat the processors are used in multi-process and multi-\nthread OSes. As a result, the context switch times, which\ninclude storing and loading register values, is important.\nWith wider vector registers there is the problem that the\ninput and output data of the operations cannot be sequen-\ntially laid out in memory. This might be because a ma-\ntrix is sparse, a matrix is accessed by columns instead of\nrows, and many other factors. Vector units provide, for\nthis case, ways to access memory in non-sequential pat-\nterns. A single vector load or store can be parametrized\nand instructed to load data from many different places\nin the address space. Using today’s multi-media instruc-\ntions, this is not possible at all. The values would have\nto be explicitly loaded one by one and then painstakingly\ncombined into one vector register.\nThe vector units of the old days had different modes toallow the most useful access patterns:\n• using striding , the program can specify how big\nthe gap between two neighboring vector elements\nis. The gap between all elements must be the same\nbut this would, for instance, easily allow to read\nthe column of a matrix into a vector register in one\ninstruction instead of one instruction per row.\n• using indirection, arbitrary access patterns can be\ncreated. The load or store instruction receive a\npointer to an array which contains addresses or off-\nsets of the real memory locations which have to be\nloaded.\nIt is unclear at this point whether we will see a revival of\ntrue vector operations in future versions of mainstream\nprocessors. Maybe this work will be relegated to co-\nprocessors. In any case, should we get access to vector\noperations, it is all the more important to correctly or-\nganize the code performing such operations. The code\nshould be self-contained and replaceable, and the inter-\nface should be general enough to efﬁciently apply vector\noperations. For instance, interfaces should allow adding\nentire matrixes instead of operating on rows, columns, or\neven groups of elements. The larger the building blocks,\nthe better the chance of using vector operations.\nIn [11] the authors make a passionate plea for the revival\nof vector operations. They point out many advantages\nand try to debunk various myths. They paint an overly\nsimplistic image, though. As mentioned above, large reg-\nister sets mean high context switch times, which have to\nbe avoided in general purpose OSes. See the problems\nof the IA-64 processor when it comes to context switch-\nintensive operations. The long execution time for vector\noperations is also a problem if interrupts are involved. If\nan interrupt is raised, the processor must stop its current\nwork and start working on handling the interrupt. After\nthat, it must resume executing the interrupted code. It is\ngenerally a big problem to interrupt an instruction in the\nmiddle of the work; it is not impossible, but it is compli-\ncated. For long running instructions this has to happen,\nor the instructions must be implemented in a restartable\nfashion, since otherwise the interrupt reaction time is too\nhigh. The latter is not acceptable.\nVector units also were forgiving as far as alignment of\nthe memory access is concerned, which shaped the al-\ngorithms which were developed. Some of today’s pro-\ncessors (especially RISC processors) require strict align-\nment so the extension to full vector operations is not triv-\nial. There are big potential upsides to having vector oper-\nations, especially when striding and indirection are sup-\nported, so that we can hope to see this functionality in\nthe future.\n96 Version 1.0 What Every Programmer Should Know About Memory",11767
47-A Examples and Benchmark Programs.pdf,47-A Examples and Benchmark Programs,,0
48-A.2 Debug Branch Prediction.pdf,48-A.2 Debug Branch Prediction,"A Examples and Benchmark Programs\nA.1 Matrix Multiplication\nThis is the complete benchmark program for the matrix multiplication in section section 6.2.1. For details on the\nintrinsics used the reader is referred to Intel’s reference manual.\n#include <stdlib.h>\n#include <stdio.h>\n#include <emmintrin.h>\n#define N 1000\ndouble res[N][N] __attribute__ ((aligned (64)));\ndouble mul1[N][N] __attribute__ ((aligned (64)));\ndouble mul2[N][N] __attribute__ ((aligned (64)));\n#define SM (CLS / sizeof (double))\nint\nmain (void)\n{\n// ... Initialize mul1 and mul2\nint i, i2, j, j2, k, k2;\ndouble *restrict rres;\ndouble *restrict rmul1;\ndouble *restrict rmul2;\nfor (i = 0; i < N; i += SM)\nfor (j = 0; j < N; j += SM)\nfor (k = 0; k < N; k += SM)\nfor (i2 = 0, rres = &res[i][j], rmul1 = &mul1[i][k]; i2 < SM;\n++i2, rres += N, rmul1 += N)\n{\n_mm_prefetch (&rmul1[8], _MM_HINT_NTA);\nfor (k2 = 0, rmul2 = &mul2[k][j]; k2 < SM; ++k2, rmul2 += N)\n{\n__m128d m1d = _mm_load_sd (&rmul1[k2]);\nm1d = _mm_unpacklo_pd (m1d, m1d);\nfor (j2 = 0; j2 < SM; j2 += 2)\n{\n__m128d m2 = _mm_load_pd (&rmul2[j2]);\n__m128d r2 = _mm_load_pd (&rres[j2]);\n_mm_store_pd (&rres[j2],\n_mm_add_pd (_mm_mul_pd (m2, m1d), r2));\n}\n}\n}\n// ... use res matrix\nreturn 0;\n}\nThe structure of the loops is pretty much the same as in the ﬁnal incarnation in section 6.2.1. The one big change is\nthat loading the rmul1[k2] value has been pulled out of the inner loop since we have to create a vector where both\nelements have the same value. This is what the _mm_unpacklo_pd() intrinsic does.\nThe only other noteworthy thing is that we explicitly aligned the three arrays so that the values we expect to be in the\nsame cache line actually are found there.\nUlrich Drepper Version 1.0 97\nA.2 Debug Branch Prediction\nIf, as recommended, the deﬁnitions of likely andunlikely from section 6.2.2 are used, it is easy50to have a debug\nmode to check whether the assumptions are really true. The deﬁnitions of the macros could be replaced with this:\n#ifndef DEBUGPRED\n# define unlikely(expr) __builtin_expect (!!(expr), 0)\n# define likely(expr) __builtin_expect (!!(expr), 1)\n#else\nasm ("".section predict_data, \""aw\""; .previous\n""\n"".section predict_line, \""a\""; .previous\n""\n"".section predict_file, \""a\""; .previous"");\n# ifdef __x86_64__\n# define debugpred__(e, E) \\n({ long int _e = !!(e); \\nasm volatile ("".pushsection predict_data\n"" \\n""..predictcnt%=: .quad 0; .quad 0\n"" \\n"".section predict_line; .quad %c1\n"" \\n"".section predict_file; .quad %c2; .popsection\n"" \\n""addq $1,..predictcnt%=(,%0,8)"" \\n: : ""r"" (_e == E), ""i"" (__LINE__), ""i"" (__FILE__)); \\n__builtin_expect (_e, E); \\n})\n# elif defined __i386__\n# define debugpred__(e, E) \\n({ long int _e = !!(e); \\nasm volatile ("".pushsection predict_data\n"" \\n""..predictcnt%=: .long 0; .long 0\n"" \\n"".section predict_line; .long %c1\n"" \\n"".section predict_file; .long %c2; .popsection\n"" \\n""incl ..predictcnt%=(,%0,4)"" \\n: : ""r"" (_e == E), ""i"" (__LINE__), ""i"" (__FILE__)); \\n__builtin_expect (_e, E); \\n})\n# else\n# error ""debugpred__ definition missing""\n# endif\n# define unlikely(expt) debugpred__ ((expr), 0)\n# define likely(expr) debugpred__ ((expr), 1)\n#endif\nThese macros use a lot of functionality provided by the GNU assembler and linker when creating ELF ﬁles. The ﬁrst\nasmstatement in the DEBUGPRED case deﬁnes three additional sections; it mainly gives the assembler information about\nhow the sections should be created. All sections are available at runtime, and the predict_data section is writable.\nIt is important that all section names are valid C identiﬁers. The reason will be clear shortly.\nThe new deﬁnitions of the likely andunlikely macros refer to the machine-speciﬁc debugpred__ macro. This\nmacro has the following tasks:\n1. allocate two words in the predict_data section to contain the counts for correct and incorrect predictions.\nThese two ﬁelds get a unique name through the use of %=; the leading dots makes sure the symbols do not\npollute the symbol table.\n2. allocate one word in the predict_line section to contain the line number of the likely orunlikely macro\nuse.\n50At least with the GNU toolchain.\n98 Version 1.0 What Every Programmer Should Know About Memory\n3. allocate one word in the predict_file section to contain a pointer to the ﬁle name of the likely orunlikely\nmacro use.\n4. increment the “correct” or “incorrect” counter created for this macro according to the actual value of the expres-\nsione. We do not use atomic operations here because they are massively slower and absolute precision in the\nunlikely case of a collision is not that important. It is easy enough to change if wanted.\nThe.pushsection and.popsection pseudo-ops are described in the assembler manual. The interested reader is\nasked to explore the details of these deﬁnition with the help of the manuals and some trial and error.\nThese macros automatically and transparently take care of collecting the information about correct and incorrect branch\npredictions. What is missing is a method to get to the results. The simplest way is to deﬁne a destructor for the object\nand print out the results there. This can be achieved with a function deﬁned like this:\nextern long int __start_predict_data;\nextern long int __stop_predict_data;\nextern long int __start_predict_line;\nextern const char *__start_predict_file;\nstatic void\n__attribute__ ((destructor))\npredprint(void)\n{\nlong int *s = &__start_predict_data;\nlong int *e = &__stop_predict_data;\nlong int *sl = &__start_predict_line;\nconst char **sf = &__start_predict_file;\nwhile (s < e) {\nprintf(""%s:%ld: incorrect=%ld, correct=%ld%s\n"", *sf, *sl, s[0], s[1],\ns[0] > s[1] ? "" <==== WARNING"" : """");\n++sl;\n++sf;\ns += 2;\n}\n}\nHere the fact that the section names are valid C identiﬁers comes into play; it is used by the GNU linker to automatically\ndeﬁne, if needed, two symbols for the section. The __start_ XYZsymbols corresponds to the beginning of the section\nXYZand__stop_ XYZis the location of the ﬁrst byte following section XYZ. These symbols make it possible to iterate\nover the section content at runtime. Note that, since the content of the sections can come from all the ﬁles the linker\nuses at link time, the compiler and assembler do not have enough information to determine the size of the section. Only\nwith these magic linker-generated symbols is it possible to iterate over the section content.\nThe code does not iterate over one section only, though; there are three sections involved. Since we know that, for every\ntwo words added to the predict_data section we add one word to each of the predict_line andpredict_file\nsections, we do not have to check the boundaries of these two sections. We just carry pointers along and increment\nthem in unison.\nThe code prints out a line for every prediction which appears in the code. It highlights those uses where the prediction\nis incorrect. Of course, this can be changed, and the debug mode could be restricted to ﬂag only the entries which have\nmore incorrect predictions than correct ones. Those are candidates for change. There are details which complicate the\nissue; for example, if the branch prediction happens inside a macro which is used in multiple places, all the macro uses\nmust be considered together before making a ﬁnal judgment.\nTwo last comments: the data required for this debugging operation is not small, and, in case of DSOs, expensive\n(thepredict_file section must be relocated). Therefore the debugging mode should not be enabled in production\nbinaries. Finally, each executable and DSO creates it own output, this must be kept in mind when analyzing the data.\nUlrich Drepper Version 1.0 99",7768
49-A.3 Measure Cache Line Sharing Overhead.pdf,49-A.3 Measure Cache Line Sharing Overhead,"A.3 Measure Cache Line Sharing Overhead\nThis section contains the test program to measure the overhead of using variables on the same cache line versus\nvariables on separate cache lines.\n#include <error.h>\n#include <pthread.h>\n#include <stdlib.h>\n#define N (atomic ? 10000000 : 500000000)\nstatic int atomic;\nstatic unsigned nthreads;\nstatic unsigned disp;\nstatic long **reads;\nstatic pthread_barrier_t b;\nstatic void *\ntf(void *arg)\n{\nlong *p = arg;\nif (atomic)\nfor (int n = 0; n < N; ++n)\n__sync_add_and_fetch(p, 1);\nelse\nfor (int n = 0; n < N; ++n)\n{\n*p += 1;\nasm volatile("""" : : ""m"" ( *p));\n}\nreturn NULL;\n}\nint\nmain(int argc, char *argv[])\n{\nif (argc < 2)\ndisp = 0;\nelse\ndisp = atol(argv[1]);\nif (argc < 3)\nnthreads = 2;\nelse\nnthreads = atol(argv[2]) ?: 1;\nif (argc < 4)\natomic = 1;\nelse\natomic = atol(argv[3]);\npthread_barrier_init(&b, NULL, nthreads);\n100 Version 1.0 What Every Programmer Should Know About Memory\nvoid *p;\nposix_memalign(&p, 64, (nthreads *disp ?: 1) *sizeof(long));\nlong *mem = p;\npthread_t th[nthreads];\npthread_attr_t a;\npthread_attr_init(&a);\ncpu_set_t c;\nfor (unsigned i = 1; i < nthreads; ++i)\n{\nCPU_ZERO(&c);\nCPU_SET(i, &c);\npthread_attr_setaffinity_np(&a, sizeof(c), &c);\nmem[i *disp] = 0;\npthread_create(&th[i], &a, tf, &mem[i *disp]);\n}\nCPU_ZERO(&c);\nCPU_SET(0, &c);\npthread_setaffinity_np(pthread_self(), sizeof(c), &c);\nmem[0] = 0;\ntf(&mem[0]);\nif ((disp == 0 && mem[0] != nthreads *N)\n|| (disp != 0 && mem[0] != N))\nerror(1,0,""mem[0] wrong: %ld instead of %d"",\nmem[0], disp == 0 ? nthreads *N : N);\nfor (unsigned i = 1; i < nthreads; ++i)\n{\npthread_join(th[i], NULL);\nif (disp != 0 && mem[i *disp] != N)\nerror(1,0,""mem[%u] wrong: %ld instead of %d"", i, mem[i *disp], N);\n}\nreturn 0;\n}\nThe code is provided here mainly as an illustration of how to write a program which measures effects like cache line\noverhead. The interesting parts are the bodies of the loops in tf. The __sync_add_and_fetch intrinsic, known to\nthe compiler, generates an atomic add instruction. In the second loop we have to “consume” the result of the increment\n(through the inline asmstatement). The asmdoes not introduce any actual code; instead, it prevents the compiler from\nlifting the increment operation out of the loop.\nThe second interesting part is that the program pins the threads onto speciﬁc processors. The code assumes the pro-\ncessors are numbered 0 to 3, which is usually the case if the machine has four or more logical processors. The code\ncould have used the interfaces from libNUMA to determine the numbers of the usable processors, but this test program\nshould be widely usable without introducing this dependency. It is easy enough to ﬁx up one way or another.\nUlrich Drepper Version 1.0 101",2808
50-B Some OProfile Tips.pdf,50-B Some OProfile Tips,,0
51-B.2 How It Looks Like.pdf,51-B.2 How It Looks Like,"B Some OProﬁle Tips\nThe following is not meant as a tutorial on how to use oproﬁle. There are entire documents written on that topic.\nInstead it is meant to give a few higher-level hints on how to look at one’s programs to ﬁnd possible trouble spots. But\nbefore that we must at least have a minimal introduction.\nB.1 Oproﬁle Basics\nOproﬁle works in two phases: collection and then analysis. The collection is performed by the kernel; it cannot be\ndone at userlevel since the measurements use the performance counters of the CPU. These counters require access to\nMSRs which, in turn, requires privileges.\nEach modern processor provides its own set of performance counters. On some architectures a subset of the counters\nare provided by all processor implementations while the others differ from version to version. This makes giving\ngeneral advice about the use of oproﬁle hard. There is not (yet) a higher-level abstraction for the counters which could\nhide these details.\nThe processor version also controls how many events can be traced at any one time, and in which combination. This\nadds yet more complexity to the picture.\nIf the user knows the necessary details about the performance counters, the opcontrol program can be used to select the\nevents which should be counted. For each event it is necessary to specify the “overrun number” (the number of events\nwhich must occur before the CPU is interrupted to record an event), whether the event should be counted for userlevel\nand/or the kernel, and ﬁnally a “unit mask” (it selects sub-functions of the performance counter).\nTo count the CPU cycles on x86 and x86-64 processors, one has to issue the following command:\nopcontrol --event CPU_CLK_UNHALTED:30000:0:1:1\nThe number 30000 is the overrun number. Choosing a reasonable value is important for the behavior of the system\nand the collected data. It is a bad idea ask to receive data about every single occurrence of the event. For many events,\nthis would bring the machine to a standstill since all it would do is work on the data collection for the event overrun;\nthis is why oproﬁle enforces a minimum value. The minimum values differ for each event since different events have\na different probability of being triggered in normal code.\nChoosing a very high number reduces the resolution of the proﬁle. At each overrun oproﬁle records the address of\nthe instruction which is executed at that moment; for x86 and PowerPC it can, under some circumstances, record the\nbacktrace as well.51With a coarse resolution, the hot spots might not get a representative number of hits; it is all about\nprobabilities, which is why oproﬁle is called a probabilistic proﬁler. The lower the overrun number is the higher the\nimpact on the system in terms of slowdown but the higher the resolution.\nIf a speciﬁc program is to be proﬁled, and the system is not used for production, it is often most useful to use the lowest\npossible overrun value. The exact value for each event can be queried using\nopcontrol --list-events\nThis might be problematic if the proﬁled program interacts with another process, and the slowdown causes problems\nin the interaction. Trouble can also result if a process has some realtime requirements which cannot be met when it is\ninterrupted often. In this case a middle ground has to be found. The same is true if the entire system is to be proﬁled\nfor extended periods of time. A low overrun number would mean the massive slowdowns. In any case, oproﬁle, like\nany other proﬁling mechanism, introduces uncertainty and inaccuracy.\nThe proﬁling has to be started with opcontrol --start and can be stopped with opcontrol --stop . While\noproﬁle is active it collects data; this data is ﬁrst collected in the kernel and then send to a userlevel daemon in batches,\nwhere it is decoded and written to a ﬁlesystem. With opcontrol --dump it is possible to request all information\nbuffered in the kernel to be released to userlevel.\nThe collected data can contain events from different performance counters. The numbers are all kept in parallel unless\nthe user selects to wipe the stored data in between separate oproﬁle runs. It is possible to accumulate data from the\nsame event at different occasions. If an event is encountered during different proﬁling runs the numbers are added if\nthis is what is selected by the user.\nThe userlevel part of the data collection process demultiplexes the data. Data for each ﬁle is stored separately. It is\n51Backtrace support will hopefully be available for all architectures at some point.\n102 Version 1.0 What Every Programmer Should Know About Memory",4653
52-B.3 Starting To Profile.pdf,52-B.3 Starting To Profile,"even possible to differentiate DSOs used by individual executable and, even, data for individual threads. The data\nthus produced can be archived using oparchive . The ﬁle produced by this command can be transported to another\nmachine and the analysis can be performed there.\nWith the opreport program one can generate reports from the proﬁling results. Using opannotate it is possible to see\nwhere the various events happened: which instruction and, if the data is available, in which source line. This makes it\neasy to ﬁnd hot spots. Counting CPU cycles will point out where the most time is spent (this includes cache misses)\nwhile counting retired instructions allows ﬁnding where most of the executed instructions are–there is a big difference\nbetween the two.\nA single hit at an address usually has no meaning. A side effect of statistical proﬁling is that instructions which are\nonly executed a few times, or even only once, might be attributed with a hit. In such a case it is necessary to verify the\nresults through repetition.\nB.2 How It Looks Like\nAn oproﬁle session can look as simple as this:\n$ opcontrol -i cachebench\n$ opcontrol -e INST_RETIRED:6000:0:0:1 --start\n$ ./cachebench ...\n$ opcontrol -h\nNote that these commands, including the actual program, are run as root. Running the program as root is done here only\nfor simplicity; the program can be executed by any user and oproﬁle would pick up on it. The next step is analyzing\nthe data. With opreport we see:\nCPU: Core 2, speed 1596 MHz (estimated)\nCounted INST_RETIRED.ANY_P events (number of instructions retired) with a unit mask of\n0x00 (No unit mask) count 6000\nINST_RETIRED:6000|\nsamples| %|\n------------------\n116452 100.000 cachebench\nThis means we collected a bunch of events; opannotate can now be used to look at the data in more detail. We can see\nwhere in the program the most events were recorded. Part of the opannotate --source output looks like this:\n:static void\n:inc (struct l *l, unsigned n)\n:{\n: while (n-- > 0) / *inc total: 13980 11.7926 */\n: {\n5 0.0042 : ++l->pad[0].l;\n13974 11.7875 : l = l->n;\n1 8.4e-04 : asm volatile ("""" :: ""r"" (l));\n: }\n:}\nThat is the inner function of the test, where a large portion of the time is spent. We see the samples spread out over\nall three lines of the loop. The main reason for this is that the sampling is not always 100% accurate with respect\nto the recorded instruction pointer. The CPU executes instructions out of order; reconstructing the exact sequence of\nexecution to produce a correct instruction pointer is hard. The most recent CPU versions try to do this for a select few\nevents but it is, in general, not worth the effort–or simply not possible. In most cases it does not really matter. The\nprogrammer should be able to determine what is going on even if there is a normally-distributed set of samples.\nB.3 Starting To Proﬁle\nWhen starting to analyze a body of code, one certainly can start looking at the places in the program where the most\ntime is spent. That code should certainly be optimized as well as possible. But what happens next? Where is the\nprogram spending unnecessary time? This question is not so easy to answer.\nUlrich Drepper Version 1.0 103\nOne of the problems in this situation is that absolute values do not tell the real story. One loop in the program might\ndemand the majority of the time, and this is ﬁne. There are many possible reasons for the high CPU utilization, though.\nBut what is more common, is that CPU usage is more evenly spread throughout the program. In this case, the absolute\nvalues point to many places, which is not useful.\nIn many situations it is helpful to look at ratios of two events. For instance, the number of mispredicted branches in a\nfunction can be meaningless if there is no measure for how often a function was executed. Yes, the absolute value is\nrelevant for the program’s performance. The ratio of mispredictions per call is more meaningful for the code quality\nof the function. Intel’s optimization manual for x86 and x86-64 [15] describes ratios which should be investigated\n(Appendix B.7 in the cited document for Core 2 events). A few of the ratios relevant for memory handling are the\nfollowing.\nInstruction Fetch Stall CYCLES_L1I_MEM_STALLED\n/CPU_CLK_UNHALTED.CORERatio of cycles during which in instruction decoder is\nwaiting for new data due to cache or ITLB misses.\nITLB Miss Rate ITLB_MISS_RETIRED /\nINST_RETIRED.ANYITLB misses per instruction. If this ratio is high the\ncode is spread over too many pages.\nL1I Miss Rate L1I_MISSES /INST_-\nRETIRED.ANYL1i misses per instruction. The execution ﬂow is un-\npredictable or the code size is too large. In the former\ncase avoiding indirect jumps might help. In the latter\ncase block reordering or avoiding inlining might help.\nL2 Instruction Miss Rate L2_IFETCH.SELF.I_STATE\n/INST_RETIRED.ANYL2 misses for program code per instruction. Any\nvalue larger than zero indicates code locality problems\nwhich are even worse than L1i misses.\nLoad Rate L1D_CACHE_LD.MESI /\nCPU_CLK_UNHALTED.CORERead operations per cycle. A Core 2 core can service\none load operation. A high ratio means the execution\nis bound by memory reads.\nStore Order Block STORE_BLOCK.ORDER /\nCPU_CLK_UNHALTED.CORERatio if stores blocked by previous stores which miss\nthe cache.\nL1d Rate Blocking Loads LOAD_BLOCK.L1D /CPU_-\nCLK_UNHALTED.CORELoads from L1d blocked by lack of resources. Usually\nthis means too many concurrent L1d accesses.\nL1D Miss Rate L1D_REPL / INST_-\nRETIRED.ANYL1d misses per instruction. A high rate means that\nprefetching is not effective and L2 is used too often.\nL2 Data Miss Rate L2_LINES_IN.SELF.ANY /\nINST_RETIRED.ANYL2 misses for data per instruction. If the value is\nsigniﬁcantly greater than zero, hardware and software\nprefetching is ineffective. The processor needs more\n(or earlier) software prefetching help.\nL2 Demand Miss Rate L2_LINES_-\nIN.SELF.DEMAND /INST_-\nRETIRED.ANYL2 misses for data per instruction for which the hard-\nware prefetcher was not used at all. That means, pre-\nfetching has not even started.\nUseful NTA Prefetch Rate SSE_PRE_MISS.NTA /\nSSS_PRE_EXEC.NTARatio of useful non-temporal prefetch relative to the\ntotal number of all non-temporal prefetches. A low\nvalue means many values are already in the cache.\nThis ratio can be computed for the other prefetch types\nas well.\nLate NTA Prefetch Rate LOAD_HIT_PRE /SSS_-\nPRE_EXEC.NTARatio of load requests for data with ongoing pre-\nfetch relative to the total number of all non-temporal\nprefetches. A high value means the software prefetch\ninstruction is issued too late. This ratio can be com-\nputed for the other prefetch types as well.\nFor all these ratios, the program should be run with oproﬁle being instructed to measure both events. This guarantees\nthe two counts are comparable. Before the division, one has to make sure that the possibly different overrun values are\ntaken into account. The simplest way is to ensure this is by multiplying each events counter by the overrun value.\nThe ratios are meaningful for whole programs, at the executable/DSO level, or even at the function level. The deeper\none looks into the program, the more errors are included in the value.\nWhat is needed to make sense of the ratios are baseline values. This is not as easy as it might seem. Different types of\ncode has different characteristics and a ratio value which is bad in one program might be normal in another program.\n104 Version 1.0 What Every Programmer Should Know About Memory",7618
53-C Memory Types.pdf,53-C Memory Types,"C Memory Types\nThough it is not necessary knowledge for efﬁcient programming, it might be useful to describe some more technical\ndetails of available memory types. Speciﬁcally we are here interested in the difference of “registered” versus “unregis-\ntered” and ECC versus non-ECC DRAM types.\nThe terms “registered” and “buffered” are used synonymously when de-\nMC\nFigure C.1: Unregistered DRAM Modulescribing a DRAM type which has one additional component on the DRAM\nmodule: a buffer. All DDR memory types can come in registered and un-\nregistered form. For the unregistered modules, the memory controller is\ndirectly connected to all the chips on the module. Figure C.1 shows the\nsetup.\nElectrically this is quite demanding. The memory controller must be able\nto deal with the capacities of all the memory chips (there are more than the\nsix shown in the ﬁgure). If the memory controller (MC) has a limitation,\nor if many memory modules are to be used, this setup is not ideal.\nBuffered (or registered) memory changes the situation: instead of directly\nBuffer\nMC\nFigure C.2: Registered DRAM Moduleconnecting the RAM chips on the DRAM module to the memory, they\nare connected to a buffer which, in turn, is then connected to the memory\ncontroller. This signiﬁcantly reduces the complexity of the electrical con-\nnections. The ability of the memory controllers to drive DRAM modules\nincreases by a factor corresponding to the number of connections saved.\nWith these advantages the question is: why aren’t all DRAM modules\nbuffered? There are several reasons. Obviously, buffered modules are\na bit more complicated and, hence, more expensive. Cost is not the only\nfactor, though. The buffer delays the signals from the RAM chips a bit; the\ndelay must be high enough to ensure that all signals from the RAM chips are buffered. The result is that the latency of\nthe DRAM module increases. A last factor worth mentioning here is that the additional electrical component increases\nthe energy cost. Since the buffer has to operate at the frequency of the bus this component’s energy consumption can\nbe signiﬁcant.\nWith the other factors of the use of DDR2 and DDR3 modules it is usually not possible to have more than two DRAM\nmodules per bank. The number of pins of the memory controller limit the number of banks (to two in commodity\nhardware). Most memory controllers are able to drive four DRAM modules and, therefore, unregistered modules are\nsufﬁcient. In server environments with high memory requirements the situation might be different.\nA different aspect of some server environments is that they cannot tolerate errors. Due to the minuscule charges held\nby the capacitors in the RAM cells, errors are possible. People often joke about cosmic radiation but this is indeed a\npossibility. Together with alpha decays and other natural phenomena, they lead to errors where the content of RAM\ncell changes from 0 to 1 or vice versa. The more memory is used, the higher the probability of such an event.\nIf such errors are not acceptable, ECC (Error Correction Code) DRAM can be used. Error correction codes enable the\nhardware to recognize incorrect cell contents and, in some cases, correct the errors. In the old days, parity checks only\nrecognized errors, and the machine had to be stopped when one was detected. With ECC, instead, a small number of\nerroneous bits can be automatically corrected. If the number of errors is too high, though, the memory access cannot\nbe performed correctly and the machine still stops. This is a rather unlikely case for working DRAM modules, though,\nsince multiple errors must happen on the same module.\nWhen we speak about ECC memory we are actually not quite correct. It is not the memory which performs the error\nchecking; instead, it is the memory controller. The DRAM modules simply provide more storage and transport the\nadditional non-data bits along with the real data. Usually, ECC memory stores one additional bit for each 8 data bits.\nWhy 8 bits are used will be explained a bit later.\nUpon writing data to a memory address, the memory controller computes the ECC for the new content on the ﬂy\nbefore sending that data and ECC onto the memory bus. When reading, the data plus the ECC is received, the memory\ncontroller computes the ECC for the data, and compares it with the ECC transmitted from the DRAM module. If the\nECCs match everything is ﬁne. If they do not match, the memory controller tries to correct the error. If this correction\nis not possible, the error is logged and the machine is possibly halted.\nUlrich Drepper Version 1.0 105\nSEC SEC/DED\nData BitsW ECC BitsE Overhead ECC BitsE Overhead\n4 3 75.0% 4 100.0%\n8 4 50.0% 5 62.5%\n16 5 31.3% 6 37.5%\n32 6 18.8% 7 21.9%\n64 7 10.9% 8 12.5%\nTable C.1: ECC and Data Bits Relationship\nSeveral techniques for error correction are in use but, for DRAM ECC, usually Hamming codes are used. Hamming\ncodes originally were used to encode four data bits with the ability to recognize and correct one ﬂipped bit (SEC, Single\nError Correction). The mechanism can easily be extended to more data bits. The relationship between the number of\ndata bitsWand the number of bits for the error code Eis described by the equation\nE=dlog2(W+E+ 1)e\nSolving this equation iteratively results in the values shown in the second column of Table C.1. With an additional\nbit, we can recognize two ﬂipped bits using a simple parity bit. This is then called SEC/DED, Single Error Correc-\ntion/Double Error Detection. With this additional bit we arrive at the values in the fourth column of Table C.1. The\noverhead for W= 64 is sufﬁciently low and the numbers (64, 8) are multiples of 8, so this is a natural selection for\nECC. On most modules, each RAM chip produces 8 bits and, therefore, any other combination would lead to less\nefﬁcient solution.\nThe Hamming code computation is easy to demonstrate with a code\n7 6 5 4 3 21\nECC Word D D D PD PP\nP1Parity D –D –D –P\nP2Parity D D – –D P –\nP4Parity D D D P – ––\nTable C.2: Hamming Generation Matrix Con-\nstructionusingW= 4 andE= 3. We compute parity bits at strategic\nplaces in the encoded word. Table C.2 shows the principle. At\nthe bit positions corresponding to the powers of two the parity bits\nare added. The parity sum for the ﬁrst parity bit P 1contains every\nsecond bit. The parity sum for the second parity bit P 2contains\ndata bits 1, 3, and 4 (encoded here as 3, 6, and 7). Similar ly P 4is\ncomputed.\nThe computation of the parity bits can be more elegantly described\nusing a matrix multiplication. We construction a matrix G= [IjA]\nwhereIis the identity matrix and Ais the parity generation matrix we can determine from Table C.2.\nG=2\n6641 0 0 0\n0 1 0 0\n0 0 1 0\n0 0 0 11 1 1\n0 1 1\n1 0 1\n1 1 03\n775\nThe columns of Aare constructed from the bits used in the computation of P 1, P2, and P 4. If we now represent each\ninput data item as a 4-dimensional vector dwe can compute r=dGand get a 7-dimensional vector r. This is the\ndata which in the case of ECC DDR is stored.\nTo decode the data we construct a new matrix H= [ATjI]whereATis the transposed parity generation matrix from\nthe computation of G. That means:\nH=2\n41 0 1 1\n1 1 0 1\n1 1 1 01 0 0\n0 1 0\n0 0 13\n5\nThe result of Hrshows whether the stored data is defective. If this is not the case, the product is the 3-dimensional\nvector 0 0 0T. Otherwise the value of the product, when interpreted as the binary representation of a number,\nindicates the column number with the ﬂipped bit.\nAs an example, assume d= 1 0 0 1\n. This results in\n106 Version 1.0 What Every Programmer Should Know About Memory\nr= 1 0 0 1 0 0 1\nPerforming the test using the multiplication with Hresults in\ns=0\n@0\n0\n01\nA\nNow, assume we have a corruption of the stored data and read back from memory r0= 1 0 11 0 0 1\n.\nIn this case we get\ns0=0\n@1\n0\n11\nA\nThe vector is not the null vector and, when interpreted as a number, s0has the value 5. This is the number of the bit\nwe ﬂipped in r0(starting to count the bits from 1). The memory controller can correct the bit and the programs will not\nnotice that there has been a problem.\nHandling the extra bit for the DED part is only slightly more complex. With more effort is it possible to create\ncodes which can correct two ﬂipped bits and more. It is probability and risk which decide whether this is needed.\nSome memory manufacturers say an error can occur in 256MB of RAM every 750 hours. By doubling the amount of\nmemory the time is reduced by 75%. With enough memory the probability of experiencing an error in a short time\ncan be signiﬁcant and ECC RAM becomes a requirement. The time frame could even be so small that the SEC/DED\nimplementation is not sufﬁcient.\nInstead of implementing even more error correction capabilities, server motherboards have the ability to automatically\nread all memory over a given timeframe. That means, whether or not the memory was actually requested by the\nprocessor, the memory controller reads the data and, if the ECC check fails, writes the corrected data back to memory.\nAs long as the probablity of incurring less than two memory errors in the time frame needed to read all of memory and\nwrite it back is acceptable, SEC/DED error correction is a perfectly reasonable solution.\nAs with registered DRAM, the question has to be asked: why is ECC DRAM not the norm? The answer to this question\nis the same as for the equivalent question about registered RAM: the extra RAM chip increases the cost and the parity\ncomputation increases the delay. Unregistered, non-ECC memory can be signiﬁcantly faster. Because of the similarity\nof the problems of registered and ECC DRAM, one usually only ﬁnds registered, ECC DRAM and not registered,\nnon-ECC DRAM.\nThere is another method to overcome memory errors. Some manufacturers offer what is often incorrectly called\n“memory RAID” where the data is distributed redundantly over multiple DRAM modules, or at least RAM chips.\nMotherboards with this feature can use unregistered DRAM modules, but the increased trafﬁc on the memory busses\nis likely to negate the difference in access times for ECC and non-ECC DRAM modules.\nUlrich Drepper Version 1.0 107",10316
54-D libNUMA Introduction.pdf,54-D libNUMA Introduction,"D libNUMA Introduction\nAlthough much of the information programmers need to schedule threads optimally, allocate memory appropriately,\netc. is available, this information is cumbersome to get at. The existing NUMA support library (libnuma, in the numactl\npackage on RHEL/Fedora systems) does not, by a long shot, provide adequate functionality.\nAs a response, the author has proposed a new library which provides all the functionality needed for NUMA. Due to the\noverlap of memory and cache hierarchy handling, this library is also useful for non-NUMA systems with multi-thread\nand multi-core processors–almost every currently-available machine.\nThe functionality of this new library is urgently needed to follow the advice given in this document. This is the only\nreason why it is mentioned here. The library (as of this writing) is not ﬁnished, not reviewed, not polished, and not\n(widely) distributed. It might change signiﬁcantly in future. It is currently available at\nhttp://people.redhat.com/drepper/libNUMA.tar.bz2\nThe interfaces of this library depend heavily on the information exported by the /sys ﬁlesystem. If this ﬁlesystem\nis not mounted, many functions will simply fail or provide inaccurate information. This is particularly important to\nremember if a process is executed in a chroot jail.\nThe interface header for the library contains currently the following deﬁnitions:\ntypedef memnode_set_t;\n#define MEMNODE_ZERO_S(setsize, memnodesetp)\n#define MEMNODE_SET_S(node, setsize, memnodesetp)\n#define MEMNODE_CLR_S(node, setsize, memnodesetp)\n#define MEMNODE_ISSET_S(node, setsize, memnodesetp)\n#define MEMNODE_COUNT_S(setsize, memnodesetp)\n#define MEMNODE_EQUAL_S(setsize, memnodesetp1, memnodesetp2)\n#define MEMNODE_AND_S(setsize, destset, srcset1, srcset2)\n#define MEMNODE_OR_S(setsize, destset, srcset1, srcset2)\n#define MEMNODE_XOR_S(setsize, destset, srcset1, srcset2)\n#define MEMNODE_ALLOC_SIZE(count)\n#define MEMNODE_ALLOC(count)\n#define MEMNODE_FREE(memnodeset)\nint NUMA_cpu_system_count(void);\nint NUMA_cpu_system_mask(size_t destsize, cpu_set_t *dest);\nint NUMA_cpu_self_count(void);\nint NUMA_cpu_self_mask(size_t destsize, cpu_set_t *dest);\nint NUMA_cpu_self_current_idx(void);\nint NUMA_cpu_self_current_mask(size_t destsize, cpu_set_t *dest);\nssize_t NUMA_cpu_level_mask(size_t destsize, cpu_set_t *dest,\nsize_t srcsize, const cpu_set_t *src,\nunsigned int level);\nint NUMA_memnode_system_count(void);\nint NUMA_memnode_system_mask(size_t destsize, memnode_set_t *dest);\nint NUMA_memnode_self_mask(size_t destsize, memnode_set_t *dest);\nint NUMA_memnode_self_current_idx(void);\nint NUMA_memnode_self_current_mask(size_t destsize, memnode_set_t *dest);\nint NUMA_cpu_to_memnode(size_t cpusetsize, const cpu_set_t *cpuset,\nsize_t __memnodesize, memnode_set_t *memnodeset);\nint NUMA_memnode_to_cpu(size_t memnodesize, const memnode_set_t *memnodeset,\n108 Version 1.0 What Every Programmer Should Know About Memory\nsize_t cpusetsize, cpu_set_t *cpuset);\nint NUMA_mem_get_node_idx(void *addr);\nint NUMA_mem_get_node_mask(void *addr, size_t size,\nsize_t destsize, memnode_set_t *dest);\nTheMEMNODE_ *macros are similar in form and functionality to the CPU_ *macros introduced in section section 6.4.3.\nThere are no non- _Svariants of the macros, they all require a size parameter. The memnode_set_t type is the\nequivalent of cpu_set_t , but this time for memory nodes. Note that the number of memory nodes need not have\nanything to do with the number of CPUs and vice versa. It is possible to have many CPUs per memory node or even\nno CPU at all. The size of dynamically allocated memory node bit sets should, therefore, not be determined by the\nnumber of CPUs.\nInstead, the NUMA_memnode_system_count interface should be used. It returns the number of nodes currently reg-\nistered. This number might grow or shrink over time. More often than not, though, it will remain constant, and is\ntherefore a good value to use for sizing memory node bit sets. The allocation, again similar to the CPU_ macros,\nhappens using MEMNODE_ALLOC_SIZE ,MEMNODE_ALLOC andMEMNODE_FREE .\nAs a last parallel with the CPU_ *macros, the library also provides macros to compare memory node bit sets for equality\nand to perform logical operations.\nTheNUMA_cpu_ *functions provide functionality to handle CPU sets. In part, the interfaces only make existing func-\ntionality available under a new name. NUMA_cpu_system_count returns the number of CPUs in the system, the\nNUMA_CPU_system_mask variant returns a bit mask with the appropriate bits set–functionality which is not otherwise\navailable.\nNUMA_cpu_self_count andNUMA_cpu_self_mask return information about the CPUs the current thread is cur-\nrently allowed to run on. NUMA_cpu_self_current_idx returns the index of the currently used CPU. This informa-\ntion might already be stale when returned, due to scheduling decisions the kernel can make; it always has to be assumed\nto be inaccurate. The NUMA_cpu_self_current_mask returns the same information and sets the appropriate bit in\nthe bit set.\nNUMA_memnode_system_count has already been introduced. NUMA_memnode_system_mask is the equivalent func-\ntion which ﬁlls in a bit set. NUMA_memnode_self_mask ﬁlls in a bit set according to the memory nodes which are\ndirectly attached to any of the CPUs the thread can currently run on.\nEven more specialized information is returned by the NUMA_memnode_self_current_idx andNUMA_memnode_-\nself_current_mask functions. The information returned is the memory node which is connected to the processor\nthe thread is currently running on. Just as for the NUMA_cpu_self_current_ *functions, this information can already\nbe stale when the function returns; it can only be used as a hint.\nTheNUMA_cpu_to_memnode function can be used to map a set of CPUs to the set of directly-attached memory nodes.\nIf only a single bit is set in the CPU set, one can determine which memory node each CPU belongs to. Currently, there\nis no support in Linux for a single CPU belonging to more than one memory node; this could, theoretically, change in\nfuture. To map in the other direction the NUMA_memnode_to_cpu function can be used.\nIf memory is already allocated, it is sometimes useful to know where it is allocated. This is what the NUMA_mem_-\nget_node_idx andNUMA_mem_get_node_mask allow the programmer to determine. The former function returns\nthe index of the memory node on which the page corresponding to the address speciﬁed by the parameter is allocated–\nor will be allocated according to the currently installed policy if the page is not yet allocated. The second function can\nperform the work for a whole address range; it returns the information in the form of a bit set. The function’s return\nvalue is the number of different memory nodes which are used.\nIn the remainder of this section we will see a few example for use cases of these interfaces. In all cases we skip the\nerror handling and the case where the number of CPUs and/or memory nodes is too large for the cpu_set_t and\nmemnode_set_t types respectively. Making the code robust is left as an exercise to the reader.\nDetermine Thread Sibling of Given CPU\nTo schedule helper threads, or other threads which beneﬁt from being scheduled on a thread of a given CPU, a code\nsequence like the following can be used.\nUlrich Drepper Version 1.0 109\ncpu_set_t cur;\nCPU_ZERO(&cur);\nCPU_SET(cpunr, &cur);\ncpu_set_t hyperths;\nNUMA_cpu_level_mask(sizeof(hyperths), &hyperths, sizeof(cur), &cur, 1);\nCPU_CLR(cpunr, &hyperths);\nThe code ﬁrst generates a bit set for the CPU speciﬁed by cpunr . This bit set is then passed to NUMA_cpu_level_-\nmask along with the ﬁfth parameter specifying that we are looking for hyper-threads. The result is returned in the\nhyperths bit set. All that remains to be done is to clear the bit corresponding to the original CPU.\nDetermine Core Siblings of Given CPU\nIf two threads should not be scheduled on two hyper-threads, but can beneﬁt from cache sharing, we need to determine\nthe other cores of the processor. The following code sequence does the trick.\ncpu_set_t cur;\nCPU_ZERO(&cur);\nCPU_SET(cpunr, &cur);\ncpu_set_t hyperths;\nint nhts = NUMA_cpu_level_mask(sizeof(hyperths), &hyperths, sizeof(cur), &cur, 1);\ncpu_set_t coreths;\nint ncs = NUMA_cpu_level_mask(sizeof(coreths), &coreths, sizeof(cur), &cur, 2);\nCPU_XOR(&coreths, &coreths, &hyperths);\nncs -= nhts;\nThe ﬁrst part of the code is identical to the code to determine hyper-threads. This is no coincidence since we have to\ndistinguish the hyper-threads of the given CPU from the other cores. This is implemented in the second part which\ncalls NUMA_cpu_level_mask again, but, this time, with a level of 2. All that remains to be done is to remove all\nhyper-threads of the given CPU from the result. The variables nhts andncs are used to keep track of the number of\nbits set in the respective bit sets.\nThe resulting mask can be used to schedule another thread. If no other thread has to be explicitly scheduled, the\ndecision about the core to use can be left to the OS. Otherwise one can iteratively run the following code:\nwhile (ncs > 0) {\nsize_t idx = 0;\nwhile (! CPU_ISSET(idx, &ncs))\n++idx;\nCPU_ZERO(&cur);\nCPU_SET(idx, &cur);\nnhts = NUMA_cpu_level_mask(sizeof(hyperths), &hyperths, sizeof(cur), &cur, 1);\nCPU_XOR(&coreths, &coreths, hyperths);\nncs -= nhts;\n... schedule thread on CPU idx ...\n}\nThe loop picks, in each iteration, a CPU number from the remaining, used cores. It then computes all the hyper-threads\nfor the this CPU. The resulting bit set is then subtracted (using CPU_XOR ) from the bit set of the available cores. If the\nXOR operation does not remove anything, something is really wrong. The ncs variable is updated and we are ready\nfor the next round, but not before the scheduling decisions are made. At the end, any of idx,cur, orhyperths can\nbe used to schedule a thread, depending on the requirements of the program. Often it is best to leave the OS as much\nfreedom as possible and, therefore, to use the hyperths bit set so that the OS can select the best hyper-thread.\n110 Version 1.0 What Every Programmer Should Know About Memory",10269
55-E Index.pdf,55-E Index,"E Index\nABI, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54\nAccess Pattern, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\nActivation, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\nAddress Demultiplexing, . . . . . . . . . . . . . . . . . . . . . . . . . 6\nAddress Snooping,. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .16\nAddress Space Layout Randomization, . . . . . . . . . . . 59\nAlias,. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .15\nAliasing, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50\nAlpha Decay, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105\nALU, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\nApplication Binary Interface, . . . . . . . . . . . . . . . . . . . . 54\nArithmetic Logic Unit, . . . . . . . . . . . . . . . . . . . . . . . . . . 29\nASID, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\nASLR,. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .59\nAsynchronous DRAM, . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\nBottleneck, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\nBranch Prediction, . . . . . . . . . . . . . . . . . . . . . . . . . . 14, 56\nBurst Speed, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\nCache, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1, 13\nAccess Cost, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\nAssociativity, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\nCoherency, . . . . . . . . . . . . . . . . . . . . . . . . . . . 16, 25f.\nPollution, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60\nReplacement Strategy, . . . . . . . . . . . . . . . . . . . . . . 14\nTag, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\nCapacitor, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\nCharge Curve, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\nCapacity Cache Miss, . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\nCAS,. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .68, 89\nCAS, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7ff.\nLatency, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\nCell CPU,. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .95\nCL,. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .8\nCo-processors, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95\nColumn Address Selection, . . . . . . . . . . . . . . . . . . . . . . . 7\nCommand Rate, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\nCommodity Hardware, . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\nCommon System Interface, . . . . . . . . . . . . . . . . . . . . . . . 4\nCompare-And-Swap, . . . . . . . . . . . . . . . . . . . . . . . . 68, 89\nCompulsory Cache Miss, . . . . . . . . . . . . . . . . . . . . . . . . 34\nConﬂict Misses, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54\nCosmic Rays, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105\nCPU Pipeline,. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .14, 30\nCritical Word, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\nCSI, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\nDCAS, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89\ndcbz , . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\nDDR, DDR1, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\nDDR2, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8, 10\nDDR3, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\nDemultiplexer,. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .6\nDirect Cache Access, . . . . . . . . . . . . . . . . . . . . . . . . . . . 64\nDirect-Mapped Cache, . . . . . . . . . . . . . . . . . . . . . . . . . . 18\nDirty Flag, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16DMA, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3, 12\nDouble Error Detection, . . . . . . . . . . . . . . . . . . . . . . . . 106\nDouble-Pumped, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\nDRAM, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3, 5\nDynamic RAM, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\nECC DRAM, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105\nError Correction Code, . . . . . . . . . . . . . . . . . . . . . . . . . 105\nExclusive Cache, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\nFalse Sharing, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 65\nFB-DRAM, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\nFortran, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50\nFront Side Bus, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\nFSB,. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .3, 8f., 11, 14, 70\nFully Associative Cache, . . . . . . . . . . . . . . . . . . . . . . . . 17\nFully Duplex Bus, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\ngcov , . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85\nGeneseo,. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .95\ngetconf , . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50\nHamming Code, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106\nHardware Prefetching, . . . . . . . . . . . . . . . . . . . . . . . . . . 60\nHelper Thread, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63\nHUGE_TLB ,. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .88\nhugetlbfs , . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 88\nHyper Transport, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43\nHyper-Thread, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29, 63\nHypervisor, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31, 39\nInclusive Cache, . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16, 78\nLatency, . . . . . . . . . . . . . . . . . . . . . . . . . . . 8, 23, 30, 60, 95\nLeakage, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\nLL/SC, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68, 89\nLoad Lock/Store Conditional, . . . . . . . . . . . . . . . . 68, 89\nLoop Stream Detector, . . . . . . . . . . . . . . . . . . . . . . . . . . 57\nLSD, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 57\nMAP_FIXED , . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60\nMass Storage, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\nMemory Channel, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\nMemory Controller,. . . . . . . . . . . . . . . . . . . . . . . . . . . .1, 4\nMemory Management Unit, . . . . . . . . . . . . . . . . . . . . . 30\nMemory Model, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\nMemory Ordering, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\nMemory RAID,. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .4\nMESI, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\nMMU, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\nMPI, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43\nMulti-core,. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\nMultiplexer, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\nNorthbridge, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3, 70\nNUMA,. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .4\nNUMA factor, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4, 43\nOOO, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60\nUlrich Drepper Version 1.0 111\nopcontrol,. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .102\nopreport,. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .103\nOProﬁle,. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .102\nOut-Of-Order, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60\nPaciﬁca, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\nPage Table Walk, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\nParallel Port, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\nPATA, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\nPCI Express, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\nPCI-E, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\nPGO, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85\nPhysical Address, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\nPre-Faulting, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 87\nPrecharge, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\nPrefetch Trigger,. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .60\nPrefetching, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14, 20, 60\nProﬁle Guide Optimization,. . . . . . . . . . . . . . . . . . . . . .85\nQuad-Pumped, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\nRAM, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\nRambus, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3, 8\nRAS, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7ff.\nRAS-to- CAS Delay,. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .8\nRecharging, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\nRefresh, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\nRegistered DRAM, . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105\nRequest For Ownership, . . . . . . . . . . . . . . . . . . . . . . . . . 26\nRFO,. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .26, 65\nRow Address Selection, . . . . . . . . . . . . . . . . . . . . . . . . . . 7\nSATA, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\n_SC_LEVEL1_DCACHE_LINESIZE , . . . . . . . . . . . . . . 50\nSDRAM, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3, 8, 10\nSelf Modifying Code, . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\nSense Ampliﬁer, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\nSerial Port, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\nSet Associative, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\nSIMD, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51, 95\nSingle Error Correction, . . . . . . . . . . . . . . . . . . . . . . . . 105\nSMC,. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .31\nSMP, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\nSMT, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\nSoftware Prefetching, . . . . . . . . . . . . . . . . . . . . . . . . . . . 61\nSouthbridge, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3\nSpatial Locality, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\nSpeculation,. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .62\nSpeculative Load,. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .63\nSPU, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95\nSRAM, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\nStatic RAM,. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .5\nStream, Prefetch, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60\nStreaming Read Buffer, . . . . . . . . . . . . . . . . . . . . . . . . . 48\nStrides,. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .60, 96\nSymmetric Multi-Thread,. . . . . . . . . . . . . . . . . . . . . . . .29\nTag, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\nTemporal Locality, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\nTLB, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22, 39Torrenza, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95\nTrace Cache, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\nTransactional Memory, . . . . . . . . . . . . . . . . . . . . . . . . . . 91\nTransistor, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\nTranslation Look-Aside Buffer, . . . . . . . . . . . . . . . . . . 39\ntRAS, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\ntRCD ,. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .8\ntRP, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\nUnregistered DRAM, . . . . . . . . . . . . . . . . . . . . . . . . . . 105\nUSB, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3, 12\nVector Operation, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95\nVirtual Address, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\nVirtual Machine Monitor,. . . . . . . . . . . . . . . . . . . . . . . .31\nVMM, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31, 39\nvon Neumann Architecture, . . . . . . . . . . . . . . . . . . . . . . 14\nWE, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\nWrite Enable, . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\nWrite-Combining, . . . . . . . . . . . . . . . . . . . . . . . . . . . 25, 48\n112 Version 1.0 What Every Programmer Should Know About Memory",15132
56-G Revision History.pdf,56-G Revision History,"F Bibliography\n[1]Performance Guidelines for AMD Athlon™ 64 and AMD Opteron™ ccNUMA Multiprocessor Systems .\nAdvanced Micro Devices, June 2006. URL\nhttp://www.amd.com/us-en/assets/content type/white papers andtech docs/40555.pdf. 5.4\n[2] Jennifer M. Anderson, Lance M. Berc, Jeffrey Dean, Sanjay Ghemawat, Monika R. Henzinger, Shun-Tak A.\nLeung, Richard L. Sites, Mark T. Vandevoorde, Carl A. Waldspurger, and William E. Weihl. Continuous\nproﬁling: Where have all the cycles gone. In Proceedings of the 16th ACM Symposium of Operating Systems\nPrinciples , pages 1–14, October 1997. URL http://citeseer.ist.psu.edu/anderson97continuous.html. 7.1\n[3] Vinodh Cuppu, Bruce Jacob, Brian Davis, and Trevor Mudge. High-Performance DRAMs in Workstation\nEnvironments. IEEE Transactions on Computers , 50(11):1133–1153, November 2001. URL\nhttp://citeseer.ist.psu.edu/476689.html. 2.1.2, 2.2, 2.2.1, 2.2.3, 10\n[4] Arnaldo Carvalho de Melo. The 7 dwarves: debugging information beyond gdb. In Proceedings of the Linux\nSymposium , 2007. URL https://ols2006.108.redhat.com/2007/Reprints/melo-Reprint.pdf. 6.2.1\n[5] Simon Doherty, David L. Detlefs, Lindsay Grove, Christine H. Flood, Victor Luchangco, Paul A. Martin, Mark\nMoir, Nir Shavit, and Jr. Guy L. Steele. DCAS is not a Silver Bullet for Nonblocking Algorithm Design. In\nSPAA ’04: Proceedings of the Sixteenth Annual ACM Symposium on Parallelism in Algorithms and\nArchitectures , pages 216–224, New York, NY , USA, 2004. ACM Press. ISBN 1-58113-840-7. URL\nhttp://research.sun.com/scalable/pubs/SPAA04.pdf. 8.1\n[6] M. Dowler. Introduction to DDR-2: The DDR Memory Replacement.\nhttp://www.pcstats.com/articleview.cfm?articleID=1573, May 2004. 2.2.1\n[7] Ulrich Drepper. Futexes Are Tricky, December 2005. URL http://people.redhat.com/drepper/futex.pdf. 6.3.4\n[8] Ulrich Drepper. ELF Handling For Thread-Local Storage. Technical report, Red Hat, Inc., 2003. URL\nhttp://people.redhat.com/drepper/tls.pdf. 6.4.1\n[9] Ulrich Drepper. Security Enhancements in Red Hat Enterprise Linux, 2004. URL\nhttp://people.redhat.com/drepper/nonselsec.pdf. 4.2\n[10] Dominique Fober, Yann Orlarey, and Stephane Letz. Lock-Free Techiniques for Concurrent Access to Shared\nObjects. In GMEM, editor, Actes des Journes d’Informatique Musicale JIM2002, Marseille , pages 143–150,\n2002. URL http://www.grame.fr/pub/fober-JIM2002.pdf. 8.1, 8.1\n[11] Joe Gebis and David Patterson. Embracing and Extending 20th-Century Instruction Set Architectures.\nComputer , 40(4):68–75, April 2007. 8.4\n[12] David Goldberg. What Every Computer Scientist Should Know About Floating-Point Arithmetic. ACM\nComputing Surveys , 23(1):5–48, March 1991. URL http://citeseer.ist.psu.edu/goldberg91what.html. 1\n[13] Maurice Herlihy and J. Eliot B. Moss. Transactional memory: Architectural support for lock-free data\nstructures. In Proceedings of 20th International Symposium on Computer Architecture , 1993. URL\nhttp://citeseer.ist.psu.edu/herlihy93transactional.html. 8.2, 8.2.2, 8.2.3, 8.2.4\n[14] Ram Huggahalli, Ravi Iyer, and Scott Tetrick. Direct Cache Access for High Bandwidth Network I/O, 2005.\nURL http://www.stanford.edu/group/comparch/papers/huggahalli05.pdf. 6.3.5\n[15] IntelR64 and IA-32 Architectures Optimization Reference Manual . Intel Corporation, May 2007. URL\nhttp://www.intel.com/design/processor/manuals/248966.pdf. B.3\n[16] William Margo, Paul Petersen, and Sanjiv Shah. Hyper-Threading Technology: Impact on Compute-Intensive\nWorkloads. Intel Technology Journal , 6(1), 2002. URL\nftp://download.intel.com/technology/itj/2002/volume06issue01/art06 computeintensive/vol6iss1 art06. 3.3.4\n[17] Caol ´an McNamara. Controlling symbol ordering.\nhttp://blogs.linux.ie/caolan/2007/04/24/controlling-symbol-ordering/, April 2007. 7.5\n[18] Double Data Rate (DDR) SDRAM MT46V . Micron Technology, 2003. Rev. L 6/06 EN. 2.2.2, 10\n[19] Jon “Hannibal” Stokes. Ars Technica RAM Guide, Part II: Asynchronous and Synchronous DRAM.\nhttp://arstechnica.com/paedia/r/ram guide/ram guide.part2-1.html, 2004. 2.2\n[20] Wikipedia. Static random access memory. http://en.wikipedia.org/wiki/Static Random Access Memory, 2006.\n2.1.1\nUlrich Drepper Version 1.0 113\nG Revision History\n2007-6-24 First internal draft.\n2007-8-11 Add ﬁrst set of Johnray’s edits.\n2007-9 to 2007-11 Lots of language cleanup from Jonathan Corbet and team.\n2007-11-21 Version 1.0.\nSource File Identiﬁcation\nLast Modiﬁcation D:20071121175205-08’00’\nSource File Size 553180 Bytes\nSource File MD5 0299BAC7C5B6A501077DEB5714C944DF\nMPost Last Modiﬁcation D:20071104104838-08’00’\nMPost Source File Size 138021 Bytes\nMPost Source File MD5 7732297065D660A939A9D651A7992FA2\n114 Version 1.0 What Every Programmer Should Know About Memory",4751
