# Flashcards: 10A008---Computational-Physics---Rubin-H_-Landau_processed (Part 65)

**Starting Chapter:** 4.1.1 Random Number Generation

---

#### Pseudorandom Number Generation
Computers generate numbers that appear random but are actually deterministic. These sequences are called pseudorandom because they follow a specific algorithm and can be reproduced if the initial seed value is known.

:p What is the difference between true randomness and pseudorandomness in computing?
??x
True randomness means that each number has no correlation with any other, whereas pseudorandomness involves numbers generated by an algorithm where some form of correlation exists. However, good pseudorandom generators make it difficult to discern these correlations without knowing the seed or underlying pattern.
x??

---

#### Importance of Random Numbers in Monte Carlo Simulations
Random numbers are crucial for incorporating chance into simulations such as Monte Carlo methods. These methods use random sequences to simulate natural processes and solve equations on average.

:p Why are pseudorandom numbers used instead of true randomness in Monte Carlo calculations?
??x
Pseudorandom numbers are used because they can be generated by algorithms, making them reproducible. True randomness is hard to achieve and difficult to generate consistently within a computer program. Pseudorandom sequences provide a balance between predictability for debugging and unpredictability needed for simulations.
x??

---

#### Linear Congruential Method
A common method for generating pseudorandom numbers in the interval [0, M-1] using linear congruence is described by the formula:
\[ r_{i+1} = (a \cdot r_i + c) \mod M \]
where \( r_i \) is the current number, \( a \) and \( c \) are constants, and \( M \) is the modulus.

:p What is the linear congruential method for generating pseudorandom numbers?
??x
The linear congruential method involves multiplying the current random number by a constant \( a \), adding another constant \( c \), taking the result modulo \( M \), and keeping only the remainder. This process generates a sequence of numbers that appear random but are actually deterministic.
```python
def linear_congruential_method(a, c, M, seed):
    r = seed  # Initial value (seed)
    while True:
        r = (a * r + c) % M  # Update the next number in the sequence
        yield r  # Return the current number as a generator

# Example usage
gen = linear_congruential_method(4, 1, 9, 3)
for _ in range(10):
    print(next(gen))
```
x??

---

#### Generating Uniform Random Numbers
Uniform random numbers are those where each possible value within the interval [0, M-1] has an equal probability of occurring. To generate such sequences:
\[ r_i = \frac{r}{M} \]
where \( r \) is a pseudorandom number from the linear congruential method and \( M \) is the modulus.

:p How do you convert a sequence generated by the linear congruential method into uniform random numbers in [0, 1]?
??x
To generate uniform random numbers between 0 and 1 from a sequence produced by the linear congruential method, divide each number by \( M \). This ensures that all values are scaled to fit within the desired range.
```python
# Example of converting a pseudorandom number to a uniform distribution in [0, 1]
M = 9
r = next(gen)  # Assuming `gen` is a generator from the previous example
uniform_r = r / M
print(uniform_r)
```
x??

---

#### Using Random Numbers for Simulating Processes
Random numbers can be used to simulate various natural processes like thermal motion or radioactive decay. By scaling and shifting these numbers, you can fit them into specific ranges needed for your simulations.

:p How do you scale a sequence of pseudorandom numbers to a desired range [A, B]?
??x
To scale a sequence of pseudorandom numbers in the interval [0, 1] to a desired range [A, B], use:
\[ x_i = A + (B - A) \cdot r_i \]
where \( r_i \) is a random number from the uniform distribution.

```python
def scale_to_range(A, B, r):
    return A + (B - A) * r

# Example usage
A = 0
B = 100
scaled_value = scale_to_range(A, B, uniform_r)
print(scaled_value)
```
x??

---

#### Visualizing Random Number Sequences
Plotting successive pairs of random numbers can help visually inspect the quality of a random number generator. A lack of patterns indicates good randomness.

:p How can you plot a sequence of pseudorandom numbers to check for randomness?
??x
To check if a sequence of pseudorandom numbers is truly random, you can plot successive pairs of generated numbers and look for any discernible patterns. If the points are scattered randomly without any visible clusters or trends, the generator likely produces good random sequences.

```python
import matplotlib.pyplot as plt

def plot_random_sequence(random_generator, num_points):
    x = []
    y = []
    r1 = next(random_generator)
    for _ in range(num_points - 1):
        r2 = next(random_generator)
        x.append(r1)
        y.append(r2)
        r1 = r2
    plt.scatter(x, y)
    plt.xlabel('r_i')
    plt.ylabel('r_{i+1}')
    plt.title('Plot of Random Number Sequence')
    plt.show()

# Example usage
plot_random_sequence(gen, 100)
```
x??

---

#### Linear Congruential Method Overview
Background context: The linear congruential method is a simple way to generate pseudo-random numbers. It uses the formula:
\[ r_{i+1} = (a \cdot r_i + c) \mod M \]
where \( r_i \) is the current number, and \( r_{i+1} \) is the next number in the sequence.

If applicable, add code examples with explanations.
:p What is the linear congruential method used for?
??x
The linear congruential method is a technique to generate pseudo-random numbers. It follows the formula:
\[ r_{i+1} = (a \cdot r_i + c) \mod M \]
where \( r_i \) is the current number, and \( r_{i+1} \) is the next number in the sequence.
x??

---

#### Unwise Choice of Constants
Background context: The unwise choice of constants can lead to poor quality random sequences. For example, using small values for \( a \), \( c \), and \( M \) can result in short periods and visible correlations.

:p What are the constants (a,c,M) used in the unwise application?
??x
The constants used in the unwise application are:
- \( a = 57 \)
- \( c = 1 \)
- \( M = 256 \)

These small values can result in short periods and visible correlations.
x??

---

#### Period of the Sequence
Background context: The period is how many numbers are generated before the sequence repeats. A short period indicates poor randomness.

:p How do you determine the period of a random number generator?
??x
To determine the period of a random number generator, generate the sequence and observe when it starts repeating. For example, using the unwise constants:
\[ r_{i+1} = (57 \cdot r_i + 1) \mod 256 \]

Start generating numbers until you see a repeated value.
x??

---

#### Observing Correlations
Background context: To check for correlations, plot successive pairs of random numbers and observe clustering. Clustering indicates poor randomness.

:p How do you plot successive pairs to detect clustering?
??x
To plot successive pairs, use the formula:
\[ (r_{2i-1}, r_{2i}) \]

For example, if \( r_1, r_2, r_3, r_4, \ldots \) are generated numbers, plot points like:
\[ (r_1, r_2), (r_3, r_4), (r_5, r_6), \ldots \]

If there is clustering, it suggests poor randomness.
x??

---

#### Plotting the Built-in Random Generator
Background context: To compare with a built-in random generator, plot pairs of numbers generated by the built-in function.

:p How do you generate and plot successive pairs using Python's built-in random number generator?
??x
To generate and plot successive pairs using Python's built-in random number generator:

```python
import matplotlib.pyplot as plt
import numpy as np

# Generate 1000 numbers
random_numbers = [np.random.randint(0, 256) for _ in range(1000)]

# Plot the pairs (r_{2i-1}, r_{2i})
plt.plot(random_numbers[::2], random_numbers[1::2], 'o')
plt.show()
```

This code generates 1000 numbers and plots every other pair, showing good randomness.
x??

---

#### Testing Linear Congruential Method
Background context: To test the linear congruential method with reasonable constants, use \( a = 5DEECE66D \) (base16) and \( M = 2^{32} - 1 \).

:p How do you generate random numbers using the correct constants?
??x
To generate random numbers using the correct constants:

- Convert \( a = 5DEECE66D \) to decimal: \( 273673163155 \)
- Use \( M = 2^{32} - 1 \)

The formula is:
\[ r_{i+1} = (273673163155 \cdot r_i + 1) \mod (2^{32} - 1) \]

Generate and plot the numbers to compare with the built-in random generator.
x??

---

#### Simulating a Random Walk
Background context: A random walk models movement where each step is independent of previous steps. In 2D, a molecule diffuses in the plane.

:p What is the formula for a 2D random walk?
??x
The formula for a 2D random walk involves taking \( N \) steps with each step length not coordinates but distances:
\[ (Δx_1, Δy_1), (Δx_2, Δy_2), (Δx_3, Δy_3), \ldots, (Δx_N, Δy_N) \]

Each step is independent in direction. The total displacement along each axis just adds algebraically.
x??

---

#### Artificial Walker Simulation
Background context: An artificial walker starts at the origin and takes \( N \) steps in a 2D plane.

:p How do you simulate an artificial walker?
??x
To simulate an artificial walker:

1. Start at the origin (0,0).
2. Take \( N \) steps where each step is of length 1 but direction varies randomly.
3. Track the position after each step:
\[ x_{i+1} = x_i + Δx_i \]
\[ y_{i+1} = y_i + Δy_i \]

where \( (Δx_i, Δy_i) \) are random values from a uniform distribution.

```python
import numpy as np

def simulate_random_walk(N):
    # Initialize position at the origin
    x, y = 0.0, 0.0
    
    # Take N steps
    for _ in range(N):
        # Random direction (uniformly distributed)
        angle = np.random.uniform(0, 2 * np.pi)
        
        # Calculate new coordinates
        dx = np.cos(angle)
        dy = np.sin(angle)
        
        x += dx
        y += dy
    
    return x, y

# Example usage:
N = 1000
x_final, y_final = simulate_random_walk(N)
print(f"Final position: ({x_final}, {y_final})")
```

This code simulates a random walk and prints the final position.
x??

---

#### Radial Distance in Random Walks
Background context: The radial distance \( R \) from the starting point after \( N \) steps is given by:
\[ R^2 = (\Delta x_1 + \Delta x_2 + \cdots + \Delta x_N)^2 + (\Delta y_1 + \Delta y_2 + \cdots + \Delta y_N)^2 \]
This equation can be expanded to show the sum of individual steps and cross terms:
\[ R^2 = \sum_{i=1}^{N} (\Delta x_i)^2 + \sum_{i=1}^{N} (\Delta y_i)^2 + 2\sum_{i<j} \Delta x_i \Delta x_j + 2\sum_{i<j} \Delta y_i \Delta y_j \]
If the walk is random, each step can be equally likely in any direction. For a large number of steps, the cross terms vanish on average.

:p What does \( R^2 \) represent in this context?
??x
In this context, \( R^2 \) represents the squared radial distance from the origin to the endpoint after \( N \) random steps. The equation shows how individual step contributions and cross-terms affect the final position.
x??

---

#### Vanishing Cross Terms in Random Walks
Background context: When averaging over a large number of random steps, all cross terms in the expression for \( R^2 \) vanish because each direction is equally likely. This simplifies to:
\[ R^2_{\text{rms}} = \langle R^2 \rangle \approx \langle (\Delta x_1)^2 + (\Delta y_1)^2 + \cdots + (\Delta x_N)^2 + (\Delta y_1)^2 + \cdots + (\Delta y_N)^2 \rangle \]
\[ R^2_{\text{rms}} = N \langle r^2 \rangle = N r_{\text{rms}}^2 \]
Where \( r_{\text{rms}} = \sqrt{\langle r^2 \rangle} \) is the root-mean-square (RMS) step size.

:p Why do cross terms vanish in a large number of random steps?
??x
Cross terms vanish because each direction and component are equally likely, leading to their average over many trials tending towards zero. This means that on average, there is no preferred direction for the sum of steps.
x??

---

#### RMS Distance in Random Walks
Background context: For a large number of random steps, the RMS distance \( R_{\text{rms}} \) from the origin is given by:
\[ R_{\text{rms}} = \sqrt{\langle R^2 \rangle} \approx \sqrt{N r_{\text{rms}}^2} = \sqrt{N} r_{\text{rms}} \]
This shows that while the average displacement vector ends at zero, the RMS distance grows as \( \sqrt{N} \).

:p What is the significance of \( R_{\text{rms}} \) in a random walk?
??x
\( R_{\text{rms}} \) represents the typical or expected radial distance from the origin after a large number of steps. It indicates that while the average displacement vector is zero, the spread of possible positions grows as \( \sqrt{N} \), showing the effective randomness and diffusion in the walk.
x??

---

#### Implementation of Random Walks
Background context: The program `Walk.py` in Listing 4.1 simulates a random walk where each step's x and y components are randomly chosen from the range \([-1, 1]\):
```python
x += (random.random() - 0.5) * 2.
y += (random.random() - 0.5) * 2.
```
This ensures that steps can be in any direction with equal probability.

:p How is randomness implemented for each step in a random walk simulation?
??x
Randomness is implemented by independently choosing random values for \( \Delta x' \) and \( \Delta y' \) within the range \([-1, 1]\), then normalizing them to unit length:
```python
Δx = 1 / L * Δx', Δy = 1 / L * Δy'
L = sqrt(Δx'^2 + Δy'^2)
```
This ensures each step is of unit length but can point in any direction.
x??

---

#### Visualization of Random Walks
Background context: To visualize several independent 2D random walks, you can use a plotting program to draw maps of multiple walks with 1000 steps each. This helps understand the distribution and behavior of the walk.

:p How would you simulate a 2D random walk using a computer?
??x
You start at the origin and take 1000 steps, where each step's \( \Delta x' \) and \( \Delta y' \) are chosen independently from the range \([-1, 1]\). These values are then normalized to ensure each step is of unit length. You repeat this process multiple times (e.g., \( K \approx \sqrt{N} \)) with different seeds for each trial to get an accurate representation.
x??

---

#### Analysis of Random Walks
Background context: By conducting a large number of trials, you can observe the behavior and distribution of random walks. For \( N \) steps in a single trial, perform approximately \( K = \sqrt{N} \) trials with different seeds to average out randomness.

:p How many trials should be conducted for accurate analysis of a 2D random walk?
??x
For an accurate analysis of a 2D random walk, you should conduct \( K \approx \sqrt{N} \) trials, where each trial has \( N \) steps and starts with a different seed. This helps in averaging out the randomness and providing a more reliable result.
x??

---

#### Theory and Simulation of Random Walks

Background context: The provided text discusses theoretical predictions for random walks, focusing on 2D and 3D simulations. It introduces a method to validate these theories by calculating mean-squared distance \( \langle R^2(N) \rangle \) and checking assumptions made in deriving the theoretical result.

:p What is the formula used to calculate the average of squared distances for K trials?
??x
The formula to calculate the average of squared distances for K trials is:
\[ \langle R^2(N) \rangle = \frac{1}{K} \sum_{k=1}^{K} R^2(k)(N). \]
This equation calculates the mean-squared distance by averaging \( R^2 \) over multiple trials, where \( N \) represents the number of steps in each walk.

x??

---

#### Validating Assumptions

Background context: The text mentions checking assumptions made in deriving theoretical results by verifying if certain distances are approximately zero. This is crucial to ensure the theory holds under given conditions.

:p What assumption about the correlation between different components of random walks needs verification?
??x
The assumption that needs verification is:
\[ \langle \Delta x_i \Delta x_j \neq i \rangle R^2 \approx \langle \Delta x_i \Delta y_j \rangle R^2 \approx 0. \]
This checks if the correlation between different components of random walks is negligible, ensuring the theoretical predictions are valid.

x??

---

#### Plotting RMS Distance

Background context: The text instructs to plot the root mean square (RMS) distance \( R_{\text{rms}} = \sqrt{\langle R^2(N) \rangle} \) as a function of \( \sqrt{N} \). This helps in understanding how the distance scales with the number of steps.

:p What is the range of values for \( N \) suggested by the text?
??x
The text suggests starting with small numbers where \( R \approx \sqrt{N} \) is not expected to be accurate, and ending at a quite large value where two or three places of accuracy should be expected on average.

Example code to plot this might look like:
```python
import matplotlib.pyplot as plt

N_values = [10, 50, 100, 200, 300, 400, 500]  # Example values for N
Rms_distances = [sqrt(mean_squared_distance(N)) for N in N_values]

plt.plot(np.sqrt(N_values), Rms_distances)
plt.xlabel('sqrt(N)')
plt.ylabel('Rms distance')
plt.title('Plot of RMS Distance vs sqrt(N)')
plt.show()
```

x??

---

#### 3D Walks

Background context: The text mentions extending the analysis to 3D random walks, which is important for understanding diffusion in complex environments like the brain.

:p How does the process change when moving from 2D to 3D?
??x
When moving from 2D to 3D, the walk can now have an additional dimension. The steps and calculations are similar but now include a third component (z-axis). The theoretical predictions for \( R^2 \) would also need adjustments to account for this extra dimension.

For example:
\[ R^2 = x^2 + y^2 + z^2, \]
where \( x, y, \) and \( z \) are the steps in each respective direction.

x??

---

#### Simulations in Brain Models

Background context: The text discusses random walk simulations within a brain model to understand molecular diffusion. It mentions both unobstructed walks and those with circular impediments representing extracellular spaces.

:p What is the purpose of recording and plotting 50 walks with no impediments as shown on the left of Figure 4.5?
??x
The purpose is to record and plot 50 random walks within a brain model, each walk assigned one of six colors, starting from the origin and using equal-sized steps in two dimensions (2D). This helps visualize how molecules might diffuse without any obstacles.

Example code:
```python
import matplotlib.pyplot as plt

# Initialize data structures for positions and colors
positions = [[0, 0] for _ in range(50)]
colors = ['red', 'blue', 'green', 'yellow', 'black', 'purple']

for i in range(1500):
    for j in range(len(positions)):
        # Perform a step in one of the directions
        positions[j][0] += random.choice([-1, 1])
        positions[j][1] += random.choice([-1, 1])

plt.scatter([p[0] for p in positions], [p[1] for p in positions], c=colors)
plt.title('50 Random Walks with No Impediments')
plt.show()
```

x??

---

#### RMS Distance in Obstructive Models

Background context: The text describes adding circular obstructions to the 2D space to simulate extracellular spaces and their effects on diffusion.

:p What is the objective of inserting circular obstacles and recording 50 walks with them?
??x
The objective is to insert circular obstacles into the 2D space, similar to those shown in Figure 4.5 on the right, and record 50 random walks starting from the origin, using equal-sized steps but stopping when they hit an obstruction.

Example code:
```python
import matplotlib.pyplot as plt

obstacles = [(10, 10), (30, 20), (50, 40)]  # Example obstacles
positions_with_obstructions = [[0, 0] for _ in range(50)]
colors = ['red', 'blue', 'green', 'yellow', 'black', 'purple']

for i in range(1500):
    for j in range(len(positions_with_obstructions)):
        # Perform a step
        positions_with_obstructions[j][0] += random.choice([-1, 1])
        positions_with_obstructions[j][1] += random.choice([-1, 1])

        # Check if the position hits an obstruction and stop if it does
        for obstacle in obstacles:
            if (positions_with_obstructions[j][0] - obstacle[0])**2 + (positions_with_obstructions[j][1] - obstacle[1])**2 <= 5**2:  # Radius of 5 units
                positions_with_obstructions[j][0] -= random.choice([-1, 1])  # Reverse step
                positions_with_obstructions[j][1] -= random.choice([-1, 1])

plt.scatter([p[0] for p in positions_with_obstructions], [p[1] for p in positions_with_obstructions], c=colors)
plt.title('50 Random Walks with Circular Obstacles')
plt.show()
```

x??

---

#### Repelling Obstructive Models

Background context: The text further discusses repelling obstacles, which do not stop the walks but change their direction.

:p What is the difference between stopping and repelling in obstructive models?
??x
In repelling models, the walk continues after hitting an obstacle, but its direction changes. This contrasts with stopping models where the walk stops upon hitting an obstacle.

Example code:
```python
import matplotlib.pyplot as plt

obstacles = [(10, 10), (30, 20), (50, 40)]  # Example obstacles
positions_with_repelling_obstructions = [[0, 0] for _ in range(50)]
colors = ['red', 'blue', 'green', 'yellow', 'black', 'purple']

for i in range(1500):
    for j in range(len(positions_with_repelling_obstacles)):
        # Perform a step
        positions_with_repelling_obstacles[j][0] += random.choice([-1, 1])
        positions_with_repelling_obstacles[j][1] += random.choice([-1, 1])

        # Check if the position hits an obstacle and change direction if it does
        for obstacle in obstacles:
            if (positions_with_repelling_obstacles[j][0] - obstacle[0])**2 + (positions_with_repelling_obstacles[j][1] - obstacle[1])**2 <= 5**2:  # Radius of 5 units
                positions_with_repelling_obstacles[j][0] -= random.choice([-1, 1])  # Reverse step in one direction
                positions_with_repelling_obstacles[j][1] -= random.choice([-1, 1])

plt.scatter([p[0] for p in positions_with_repelling_obstacles], [p[1] for p in positions_with_repelling_obstacles], c=colors)
plt.title('50 Random Walks with Repelling Circular Obstacles')
plt.show()
```

x??

#### Self-Avoiding Random Walks for Protein Folding

Background context: In modeling protein folding, a self-avoiding random walk is used to simulate chains of monomers with different properties. The walk stops when it reaches a corner or cannot move to any adjacent unoccupied sites. This method helps in understanding how hydrophobic and polar residues arrange themselves.

:p What is the main objective of simulating protein folding using a self-avoiding random walk?
??x
The primary goal is to model the folding process of proteins by placing monomers (hydrophobic H and polar P) on a lattice, aiming to find configurations with the lowest energy. The simulation helps in understanding how these chains rearrange themselves into stable structures.
??x

---

#### Modifying Random Walk Program for Self-Avoiding Walks

Background context: The existing random walk program needs modification to simulate self-avoiding walks. This involves stopping the walk when it reaches a corner or cannot move to any adjacent unoccupied sites.

:p How does the modified random walk program ensure that the walk is self-avoiding?
??x
The walk stops at corners or when there are no available empty neighboring sites. The key logic ensures that once a site is occupied, it cannot be revisited.
```java
// Pseudocode for checking if a move is valid in a 2D lattice
if (newPosition.x >= 0 && newPosition.x < width && newPosition.y >= 0 && newPosition.y < height) {
    if (!lattice[newPosition.x][newPosition.y].occupied) {
        // Move to the new position and mark it as occupied
    } else {
        // Stop the walk as the move is not valid (site is already occupied)
    }
} else {
    // Stop the walk at a corner or boundary
}
```
x??

---

#### Visualizing Monomer Positions

Background context: The simulation produces different configurations of monomers, and these need to be visualized using colored dots. This helps in understanding how H and P monomers are arranged on the lattice.

:p How is the visualization produced for monomer positions?
??x
Monomers (H and P) are represented by differently colored dots. For instance, H monomers can be displayed as dark dots, while P monomers can be shown with lighter or smaller dots.
```python
# Python pseudocode for visualization
def visualize_monomers(monomer_positions):
    for position in monomer_positions:
        if is_H(position):  # Function to check if a position has an H monomer
            draw_dot(position, color='dark')
        else:  # Position must have P monomer by default
            draw_dot(position, color='light')
```
x??

---

#### Energy Calculation for Protein Chains

Background context: The energy of the protein chain is calculated based on the number of direct H–H (hydrophobic) contacts. This helps in understanding the stability and configuration of the protein.

:p How is the energy of a protein chain defined?
??x
The energy \(E\) of a protein chain is given by:
\[ E = -\epsilon f \]
where \(\epsilon\) is a positive constant, and \(f\) is the number of H–H contacts not directly connected. Each H–H contact lowers the energy, while P–P or H–P contacts do not affect the energy.
```java
// Java pseudocode for calculating energy
public int calculateEnergy(List<Monomer> chain) {
    int hHContacts = 0;
    for (int i = 0; i < chain.size() - 1; i++) {
        if (chain.get(i).isH() && chain.get(i + 1).isH()) {
            hHContacts++;
        }
    }
    return -epsilon * hHContacts; // Assuming epsilon is defined elsewhere
}
```
x??

---

#### Spontaneous Decay Simulation

Background context: Simulating spontaneous decay involves understanding the exponential nature of particle decays and converting this into a stochastic process. This helps in modeling real-world scenarios where particles decay randomly over time.

:p What does spontaneous decay entail?
??x
Spontaneous decay is a natural process where particles decay into other particles without any external stimulation. The probability of decay per unit time for any one particle remains constant, but the exact timing is random.
```java
// Pseudocode for simulating exponential decay
public class DecaySimulation {
    private double lambda; // Decay rate

    public void simulateDecay(int initialParticles) {
        int remainingParticles = initialParticles;
        while (remainingParticles > 0) {
            long currentTime = System.currentTimeMillis();
            long timeSinceLastCheck = currentTime - lastCheckedTime;
            if (timeSinceLastCheck >= 1) { // Check every second
                double probabilityOfDecay = Math.random(); // Random number between 0 and 1
                if (probabilityOfDecay < lambda) {
                    remainingParticles--;
                }
                lastCheckedTime = currentTime; // Update the time for next check
            }
        }
    }
}
```
x??

---

#### Discrete Decay Model

Background context: The discrete decay model converts the continuous exponential decay into a stochastic process, which can be simulated numerically. This helps in understanding how the number of decaying particles changes over time.

:p How is the probability of decay per unit time interval represented in the discrete decay model?
??x
The probability of decay per unit time interval for any one particle is constant and represented as:
\[ \lambda = -\frac{\Delta N(t)}{\Delta t} \]
This leads to a finite-difference equation relating the number of particles \(N(t)\) that decay over small intervals.
```java
// Pseudocode for discrete decay model
public class DecayModel {
    private double lambda; // Decay rate

    public int simulateDecay(int initialParticles, long simulationTime) {
        int remainingParticles = initialParticles;
        for (long time = 0; time < simulationTime; time += 1000) { // Check every second
            if (Math.random() < lambda) {
                remainingParticles--;
            }
        }
        return remainingParticles;
    }
}
```
x??

---

