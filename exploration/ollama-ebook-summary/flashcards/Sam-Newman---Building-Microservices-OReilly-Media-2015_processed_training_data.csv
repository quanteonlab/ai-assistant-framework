source_file,chapter_title,flashcard_title,flashcard_content,flashcard_length,input_text_excerpt,model,timestamp,usefulness_rating
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Navigating This Book,Microservices Introduction,"#### Microservices Introduction
Background context explaining microservices as an approach to distributed systems that promotes finely grained services with their own lifecycles. These services collaborate together and are primarily modeled around business domains, avoiding problems of traditional tiered architectures.

:p What is the main concept discussed in this section?
??x
The main concept is microservices, which are a method for designing software applications by dividing them into small, independent services that can be developed, deployed, and scaled independently. Each service addresses a business function and follows clean interfaces to communicate with each other.
x??",687,"Preface Microservices are an approach to distributed systems that promote the use of finely grained services with their own lifecycles, which collaborate together. Because microservices are primarily ...",qwen2.5:latest,2025-11-02 04:24:26,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Navigating This Book,Evolutionary Architect,"#### Evolutionary Architect
Explanation of the difficulties faced by architects in making trade-offs when implementing microservices, emphasizing the need for a comprehensive approach.

:p What does this chapter focus on?
??x
This chapter focuses on the challenges faced by architects when dealing with the complexities of designing and maintaining microservices. It highlights the importance of considering multiple aspects such as scalability, autonomy, and service integration.
x??",484,"Preface Microservices are an approach to distributed systems that promote the use of finely grained services with their own lifecycles, which collaborate together. Because microservices are primarily ...",qwen2.5:latest,2025-11-02 04:24:26,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Navigating This Book,Modeling Services,"#### Modeling Services
Description of using domain-driven design techniques to define the boundaries of microservices.

:p How does domain-driven design help in modeling services?
??x
Domain-driven design (DDD) helps in defining the boundaries of microservices by aligning them with business domains. It encourages a deep understanding of the problem space and focuses on creating models that reflect the essential elements of a particular domain.
```java
// Example of a service boundary defined using DDD concepts
public class CustomerService {
    private final CustomerRepository customerRepo;

    public CustomerService(CustomerRepository customerRepo) {
        this.customerRepo = customerRepo;
    }

    public void updateCustomerInformation(CustomerDetails customerDetails) {
        // Logic to update customer information
    }
}
```
x??",850,"Preface Microservices are an approach to distributed systems that promote the use of finely grained services with their own lifecycles, which collaborate together. Because microservices are primarily ...",qwen2.5:latest,2025-11-02 04:24:26,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Navigating This Book,Service Integration,"#### Service Integration
Explanation of service collaboration techniques and integration challenges, including user interfaces and legacy systems.

:p What are some specific techniques discussed for integrating microservices?
??x
The chapter discusses various service collaboration techniques such as event-driven architectures, API gateways, and direct service-to-service communication. It also covers the integration of user interfaces with microservices and handling legacy and commercial off-the-shelf (COTS) products.
```java
// Example of a simple API Gateway class
public class ApiGateway {
    private final Map<String, ServiceInvoker> services;

    public ApiGateway(Map<String, ServiceInvoker> services) {
        this.services = services;
    }

    public Object invokeService(String serviceName, String method, Object input) throws Exception {
        return services.get(serviceName).invoke(method, input);
    }
}
```
x??",937,"Preface Microservices are an approach to distributed systems that promote the use of finely grained services with their own lifecycles, which collaborate together. Because microservices are primarily ...",qwen2.5:latest,2025-11-02 04:24:26,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Navigating This Book,Splitting the Monolith,"#### Splitting the Monolith
Discussion on using microservices as an antidote to large monolithic systems.

:p What are some common issues with large monolithic applications that make them hard to change?
??x
Large monolithic applications often face issues such as complex dependencies, tight coupling between components, and difficulty in isolating changes. Microservices address these by breaking down the application into smaller, independent services that can be developed, deployed, and scaled independently.
```java
// Example of a microservice splitting a monolith
public class OrderService {
    private final ItemRepository itemRepo;
    private final CustomerRepository customerRepo;

    public OrderService(ItemRepository itemRepo, CustomerRepository customerRepo) {
        this.itemRepo = itemRepo;
        this.customerRepo = customerRepo;
    }

    public void placeOrder(Order order) throws Exception {
        // Logic to place an order
    }
}
```
x??",970,"Preface Microservices are an approach to distributed systems that promote the use of finely grained services with their own lifecycles, which collaborate together. Because microservices are primarily ...",qwen2.5:latest,2025-11-02 04:24:26,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Navigating This Book,Deployment,"#### Deployment
Explanation of the challenges and advancements in deployment strategies for microservices.

:p What are some key factors discussed in the chapter regarding deployment?
??x
The chapter discusses various deployment strategies such as containerization with Docker, orchestration tools like Kubernetes, and cloud-native approaches. It emphasizes the importance of continuous delivery and deployment practices.
```java
// Example of a simple Kubernetes Deployment YAML file
apiVersion: apps/v1
kind: Deployment
metadata:
  name: order-service-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: order-service
  template:
    metadata:
      labels:
        app: order-service
    spec:
      containers:
      - name: order-service
        image: my-order-service:latest
        ports:
        - containerPort: 8080
```
x??",853,"Preface Microservices are an approach to distributed systems that promote the use of finely grained services with their own lifecycles, which collaborate together. Because microservices are primarily ...",qwen2.5:latest,2025-11-02 04:24:26,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Navigating This Book,Testing,"#### Testing
Explanation of the importance of testing in microservices and specific techniques like consumer-driven contracts.

:p What is a consumer-driven contract, and why is it important in microservices?
??x
A consumer-driven contract (CDC) is a specification that describes how one service expects another to behave. It ensures the quality of the services by defining the expected API behavior from the perspective of consumers. This helps maintain compatibility between dependent services.
```java
// Example of a simple CDC file
{
    ""request"": {
        ""url"": ""/order/{id}"",
        ""method"": ""GET"",
        ""headers"": {},
        ""body"": null,
        ""response"": {
            ""status"": 200,
            ""headers"": {},
            ""body"": {
                ""orderId"": ""{id}"",
                ""items"": [""book"", ""pen""]
            }
        }
    }
}
```
x??",869,"Preface Microservices are an approach to distributed systems that promote the use of finely grained services with their own lifecycles, which collaborate together. Because microservices are primarily ...",qwen2.5:latest,2025-11-02 04:24:26,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Navigating This Book,Monitoring,"#### Monitoring
Explanation of monitoring and handling emergent complexity in distributed systems.

:p What are some key considerations for monitoring microservices?
??x
Key considerations include setting up comprehensive logging, tracing requests across services, handling latency and error rates, and ensuring high availability. The focus is on understanding the behavior of the system as a whole.
```java
// Example of a simple monitoring setup using Prometheus
# Define metrics in code
public class Metrics {
    private static final Counter requestCounter = Metrics.counter(""request_count"");

    public void handleRequest() {
        requestCounter.inc();
        // Request handling logic
    }
}
```
x??",711,"Preface Microservices are an approach to distributed systems that promote the use of finely grained services with their own lifecycles, which collaborate together. Because microservices are primarily ...",qwen2.5:latest,2025-11-02 04:24:26,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Navigating This Book,Security,"#### Security
Explanation of security aspects in microservices, including user-to-service and service-to-service authentication.

:p What are some key security concerns addressed in this chapter?
??x
The chapter addresses securing microservices by discussing methods for authenticating users to services (user-to-service) and services interacting with each other (service-to-service). It also covers best practices such as using secure protocols, encryption, and access controls.
```java
// Example of service-to-service authentication using OAuth2
public class ServiceClient {
    private final HttpClient httpClient;
    private final String clientId;
    private final String clientSecret;

    public ServiceClient(HttpClient httpClient, String clientId, String clientSecret) {
        this.httpClient = httpClient;
        this.clientId = clientId;
        this.clientSecret = clientSecret;
    }

    public Object invokeService(String serviceUrl, String method, Object input) throws Exception {
        // Logic to authenticate and invoke the service
    }
}
```
x??",1073,"Preface Microservices are an approach to distributed systems that promote the use of finely grained services with their own lifecycles, which collaborate together. Because microservices are primarily ...",qwen2.5:latest,2025-11-02 04:24:26,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Navigating This Book,Conway’s Law and System Design,"#### Conway’s Law and System Design
Explanation of how organizational structure influences system architecture.

:p How does Conway's Law relate to microservices?
??x
Conway's Law states that ""organizations which design systems ... are constrained by the communication structures of those organizations."" In the context of microservices, this means that the architectural design will reflect the organizational structure. The chapter discusses how aligning system design with team structure can lead to better outcomes.
```java
// Example of organizing teams based on services
public class ServiceTeam {
    private final List<ServiceDeveloper> developers;

    public ServiceTeam(List<ServiceDeveloper> developers) {
        this.developers = developers;
    }

    public void manageDevelopment() {
        // Logic to manage the development process
    }
}
```
x??",867,"Preface Microservices are an approach to distributed systems that promote the use of finely grained services with their own lifecycles, which collaborate together. Because microservices are primarily ...",qwen2.5:latest,2025-11-02 04:24:26,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Navigating This Book,Microservices at Scale,"#### Microservices at Scale
Explanation of challenges and solutions for scaling microservices.

:p What are some key issues discussed in managing large numbers of services?
??x
Key issues include handling increased failure rates, managing traffic volume, ensuring high availability, and maintaining system reliability. The chapter discusses strategies such as circuit breakers, load balancing, and fault tolerance.
```java
// Example of a simple circuit breaker implementation
public class CircuitBreaker {
    private final int maxFailures;
    private volatile boolean open;

    public CircuitBreaker(int maxFailures) {
        this.maxFailures = maxFailures;
        this.open = false;
    }

    public void onFailure() {
        if (!open && failureCount >= maxFailures) {
            open = true;
        }
    }

    public boolean shouldInvokeService() {
        return !open || !failureCount >= maxFailures;
    }
}
```
x??",933,"Preface Microservices are an approach to distributed systems that promote the use of finely grained services with their own lifecycles, which collaborate together. Because microservices are primarily ...",qwen2.5:latest,2025-11-02 04:24:26,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Navigating This Book,Bringing It All Together,"#### Bringing It All Together
Summary of microservices principles and key points from the book.

:p What are some core takeaways from this chapter?
??x
The chapter summarizes seven key microservices principles, including loose coupling, high cohesion, decentralized data management, autonomous deployment, smart endpoints, dumb pipes, and implicit state. These principles provide a framework for understanding what makes microservices different.
```java
// Example of a principle: Smart Endpoints, Dumb Pipes
public class ServiceEndpoint {
    public Object invokeService(ServiceRequest request) throws Exception {
        // Logic to handle the service request
        return new ServiceResponse();
    }
}
```
x??",715,"Preface Microservices are an approach to distributed systems that promote the use of finely grained services with their own lifecycles, which collaborate together. Because microservices are primarily ...",qwen2.5:latest,2025-11-02 04:24:26,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Technology Heterogeneity,Typographical Conventions,"#### Typographical Conventions

Background context: This section outlines the typographical conventions used throughout the book to distinguish different types of content. These conventions help readers quickly identify and differentiate between various elements such as new terms, user input, and more.

:p What are the different typographical styles used in this book?
??x
The different typographical styles used in this book include:

- **Italic**: Used for new terms, URLs, email addresses, filenames, and file extensions.
- **Constant width**: Used for program listings, variable or function names, databases, data types, environment variables, statements, and keywords.
- **Constant width bold**: Indicates text that should be typed literally by the user.
- **Constant width italic**: Shows text that should be replaced with user-supplied values or determined by context.

For example:
```text
This is an example of **italic** for new terms. This is a piece of code: `constantWidth` and this is **constant width italic** to indicate placeholders.
```
x??",1060,"Conventions Used in This Book The following typographical conventions are used in this book: Italic Indicates new terms, URLs, email addresses, filenames, and file extensions. Constant width Used for ...",qwen2.5:latest,2025-11-02 04:25:04,2
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Technology Heterogeneity,Safari Books Online,"#### Safari Books Online

Background context: This section introduces Safari Books Online, an on-demand digital library that offers technology and business content in both book and video formats from leading authors.

:p What is Safari Books Online?
??x
Safari Books Online is a digital library providing technology and business content in both book and video formats from top authors. It serves as the primary resource for research, problem-solving, learning, and certification training for professionals like technology workers, software developers, web designers, etc.

Members have access to thousands of books, videos, and pre-publication manuscripts through a fully searchable database from publishers such as O'Reilly Media, Addison-Wesley Professional, Microsoft Press, Prentice Hall Professional, and more.

```java
// Example code snippet for accessing Safari Books Online API (pseudocode)
public class SafariBooksAPI {
    public static void accessLibrary(String username, String password) {
        // Code to authenticate user and provide access to the library
    }
}
```
x??",1089,"Conventions Used in This Book The following typographical conventions are used in this book: Italic Indicates new terms, URLs, email addresses, filenames, and file extensions. Constant width Used for ...",qwen2.5:latest,2025-11-02 04:25:04,2
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Technology Heterogeneity,Contact Information,"#### Contact Information

Background context: This section provides contact details for the book's publisher, including how readers can reach out with comments or technical questions.

:p How can readers contact the publisher of this book?
??x
Readers can contact the publisher by sending email to `bookquestions@oreilly.com` or addressing comments and questions through their official website. The address is:

O'Reilly Media, Inc.
1005 Gravenstein Highway North
Sebastopol, CA 95472

Phone: 
- 800-998-9938 (in the United States or Canada)
- 707-829-0515 (international or local)
- 707-829-0104 (fax)

Additional resources are available at:
- Web page: `http://bit.ly/building-microservices`
- Facebook: `http://facebook.com/oreilly`
- Twitter: `http://twitter.com/oreillymedia`
- YouTube: `http://www.youtube.com/oreillymedia`

x??",834,"Conventions Used in This Book The following typographical conventions are used in this book: Italic Indicates new terms, URLs, email addresses, filenames, and file extensions. Constant width Used for ...",qwen2.5:latest,2025-11-02 04:25:04,2
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Technology Heterogeneity,Acknowledgments,"#### Acknowledgments

Background context: This section acknowledges the contributions of individuals and organizations that helped in the creation of this book.

:p Who are the key contributors acknowledged in this book?
??x
The key contributors acknowledged in this book include:

- Lindy Stephens, who encouraged the author to start on this journey and provided continuous support.
- Howard Newman (the author's father), for always being there for him.
- Ben Christensen, Vivek Subramaniam, and Martin Fowler, who provided detailed feedback throughout the writing process.
- James Lewis, with whom the author has discussed many ideas presented in the book over beers.

These contributions significantly shaped the content of this book, making it what it is today.

x??

---",775,"Conventions Used in This Book The following typographical conventions are used in this book: Italic Indicates new terms, URLs, email addresses, filenames, and file extensions. Constant width Used for ...",qwen2.5:latest,2025-11-02 04:25:04,2
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Technology Heterogeneity,Background on Microservices Evolution,"#### Background on Microservices Evolution
Microservices have emerged as a result of several technological advancements and practices over the years. This includes domain-driven design, continuous delivery, virtualization, infrastructure automation, small autonomous teams, and the experiences from large organizations like Amazon, Google, and Netflix.

:p What key factors contributed to the development of microservices?
??x
Several key factors have contributed to the development of microservices:
1. **Domain-Driven Design**: Eric Evans’s book helped in understanding how to model our systems based on real-world domains.
2. **Continuous Delivery**: This approach emphasizes that every code change should be testable and releasable.
3. **Virtualization Platforms**: Tools like VMs allowed for flexible machine provisioning and resizing.
4. **Infrastructure Automation**: Technologies such as Ansible, Chef, and Puppet enabled scaling of services.
5. **Small Autonomous Teams**: Encouraging teams to own the entire service lifecycle.
6. **Scalable Systems**: Companies like Netflix demonstrated how to build resilient systems at scale.

:x??",1144,"Specifically, I would like to thank (in no particular order) Kane Venables, Anand Krishnaswamy, Kent McNeil, Charles Haynes, Chris Ford, Aidy Lewis, Will Thames, Jon Eaves, Rolf Russell, Badrinath Jan...",qwen2.5:latest,2025-11-02 04:25:24,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Technology Heterogeneity,Characteristics of Microservices,"#### Characteristics of Microservices
Microservices are small, autonomous services that work together to form a larger application. They are designed to be independent and easily replaceable without affecting other parts of the system.

:p What defines microservices?
??x
Microservices are characterized by their:
1. **Small Size**: Individual services handle specific business functions.
2. **Autonomy**: Each service operates independently, making decisions based on its own data.
3. **Decoupling**: Services communicate through well-defined APIs.
4. **Replaceability**: If a service fails or needs to be updated, it can be done without affecting others.

:x??",662,"Specifically, I would like to thank (in no particular order) Kane Venables, Anand Krishnaswamy, Kent McNeil, Charles Haynes, Chris Ford, Aidy Lewis, Will Thames, Jon Eaves, Rolf Russell, Badrinath Jan...",qwen2.5:latest,2025-11-02 04:25:24,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Technology Heterogeneity,Benefits of Microservices,"#### Benefits of Microservices
By embracing microservices, organizations can deliver software faster and more flexibly, allowing them to adapt quickly to changing requirements and technologies.

:p What are the benefits of using microservices?
??x
The key benefits of microservices include:
1. **Faster Development**: Small teams can work independently on specific services.
2. **Scalability**: Each service can be scaled based on demand.
3. **Resilience**: Services fail independently, isolating failures and improving overall system stability.
4. **Flexibility**: New technologies can be adopted more easily without impacting the entire system.

:x??",652,"Specifically, I would like to thank (in no particular order) Kane Venables, Anand Krishnaswamy, Kent McNeil, Charles Haynes, Chris Ford, Aidy Lewis, Will Thames, Jon Eaves, Rolf Russell, Badrinath Jan...",qwen2.5:latest,2025-11-02 04:25:24,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Technology Heterogeneity,Collaboration Across Teams,"#### Collaboration Across Teams
Microservices enable better collaboration among teams by breaking down large systems into smaller, manageable services that can be owned and developed independently.

:p How do microservices facilitate team collaboration?
??x
Microservices facilitate team collaboration through:
1. **Independence**: Each team owns a specific service.
2. **Communication**: Services communicate via APIs, fostering clear interfaces.
3. **Ownership**: Teams are responsible for their services from development to deployment and maintenance.
4. **Flexibility**: Different teams can use different technologies or processes based on the needs of their service.

:x??",677,"Specifically, I would like to thank (in no particular order) Kane Venables, Anand Krishnaswamy, Kent McNeil, Charles Haynes, Chris Ford, Aidy Lewis, Will Thames, Jon Eaves, Rolf Russell, Badrinath Jan...",qwen2.5:latest,2025-11-02 04:25:24,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Technology Heterogeneity,Challenges in Implementing Microservices,"#### Challenges in Implementing Microservices
While microservices offer numerous benefits, they also introduce challenges such as complexity in orchestration, monitoring, and data management across services.

:p What are some challenges associated with implementing microservices?
??x
Some key challenges include:
1. **Orchestration**: Coordinating multiple services can be complex.
2. **Monitoring**: Ensuring the health of distributed systems is more difficult.
3. **Data Management**: Services often need to manage their own databases, leading to data consistency issues.
4. **Security**: Protecting individual microservices while maintaining overall security.

:x??",669,"Specifically, I would like to thank (in no particular order) Kane Venables, Anand Krishnaswamy, Kent McNeil, Charles Haynes, Chris Ford, Aidy Lewis, Will Thames, Jon Eaves, Rolf Russell, Badrinath Jan...",qwen2.5:latest,2025-11-02 04:25:24,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Technology Heterogeneity,Example Code for a Microservice,"#### Example Code for a Microservice
Here’s an example of how a simple REST API might look in Java using Spring Boot:

:p What is the basic structure of a RESTful microservice in Java?
??x
A basic RESTful microservice in Java using Spring Boot might look like this:
```java
import org.springframework.web.bind.annotation.GetMapping;
import org.springframework.web.bind.annotation.RestController;

@RestController
public class HelloWorldController {
    @GetMapping(""/hello"")
    public String sayHello() {
        return ""Hello, World!"";
    }
}
```
This code sets up a simple controller that handles GET requests to `/hello` and returns a string response.

:x??

---",667,"Specifically, I would like to thank (in no particular order) Kane Venables, Anand Krishnaswamy, Kent McNeil, Charles Haynes, Chris Ford, Aidy Lewis, Will Thames, Jon Eaves, Rolf Russell, Badrinath Jan...",qwen2.5:latest,2025-11-02 04:25:24,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Technology Heterogeneity,Cohesion and Codebase Management,"#### Cohesion and Codebase Management
Background context explaining the importance of cohesion and how it relates to monolithic systems. Discuss Robert C. Martin’s Single Responsibility Principle (SRP) and its relevance to microservices.

:p What is the concept of cohesion in software development, and why is it important?
??x
Cohesion refers to the degree to which a module's elements are related to each other and to their common purpose. High cohesion means that functions within a module are closely related, making the code easier to understand, maintain, and test. The Single Responsibility Principle (SRP) by Robert C. Martin emphasizes grouping related functionalities together so that changes affecting one responsibility do not impact unrelated responsibilities.

For example:
- A function should only have one reason to change.
```java
public class User {
    private String name;
    
    public void setName(String name) { this.name = name; }
    
    // other methods
}
```
In the above code, `setName` is responsible for setting the user's name and nothing else. This adheres to SRP.

x??",1104,"Small, and Focused on Doing One Thing Well Codebases grow as we write code to add new features. Over time, it can be difficult to know where a change needs to be made because the codebase is so large....",qwen2.5:latest,2025-11-02 04:25:51,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Technology Heterogeneity,Microservices Boundaries,"#### Microservices Boundaries
Background context explaining how microservices boundaries align with business boundaries. Discuss the benefits of keeping services focused on a specific boundary and avoiding large monolithic systems.

:p How do microservices boundaries differ from traditional monolithic applications, and why is this approach beneficial?
??x
Microservices boundaries are defined based on business capabilities rather than technical components, which makes it clear where code lives for a given piece of functionality. This approach helps in maintaining smaller, more manageable services that are easier to develop, test, and deploy.

Benefits include:
- Improved modularity: Easier to understand and modify specific parts of the application.
- Fault isolation: A failure in one service does not affect others.
- Scalability: Services can be scaled independently based on demand.

For example, a monolithic e-commerce system might have all functionalities (user management, product catalog, order processing) in one large codebase. In contrast, microservices would separate these into smaller services like `UserManagementService`, `CatalogService`, and `OrderProcessingService`.

```java
public interface UserService {
    User getUserById(String id);
}

public class UserServiceImpl implements UserService {
    // Implementation details
}
```
x??",1364,"Small, and Focused on Doing One Thing Well Codebases grow as we write code to add new features. Over time, it can be difficult to know where a change needs to be made because the codebase is so large....",qwen2.5:latest,2025-11-02 04:25:51,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Technology Heterogeneity,Determining Service Size,"#### Determining Service Size
Background context explaining the challenges of determining the size of a microservice, including factors like team management and complexity. Discuss Jon Eaves' rule of thumb for microservices.

:p How do you determine the appropriate size of a microservice?
??x
Determining the appropriate size of a microservice is challenging due to varying factors such as the number of lines of code (LOC), dependencies, and domain complexity. A useful heuristic is that a microservice should be small enough to be rewritten in two weeks by a team. This aligns with Jon Eaves' rule at RealEstate.com.au.

Another guideline is ""small enough and no smaller,"" which suggests that the size should feel manageable for the development team, balancing simplicity and functionality.

```java
public class SmallService {
    // Implementation details
}
```
x??",870,"Small, and Focused on Doing One Thing Well Codebases grow as we write code to add new features. Over time, it can be difficult to know where a change needs to be made because the codebase is so large....",qwen2.5:latest,2025-11-02 04:25:51,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Technology Heterogeneity,Service Size and Complexity,"#### Service Size and Complexity
Background context explaining the trade-offs between service size and complexity. Discuss how reducing service size can increase interdependence benefits but also introduce more moving parts.

:p What are the benefits and downsides of making microservices smaller?
??x
Making microservices smaller increases cohesion, making them easier to manage and maintain. Smaller services also improve interdependence by allowing finer-grained communication between components.

However, reducing service size can lead to increased complexity:
- More moving parts: Increased number of services means more potential points of failure.
- Coordination overhead: Services need to communicate more frequently, leading to additional complexity in managing these interactions.

For example, consider a `PaymentService` and an `InventoryService`. Smaller services might require more frequent interactions between them, increasing the overall system complexity.

```java
public class PaymentService {
    public void processPayment(String paymentDetails) { /* ... */ }
}

public class InventoryService {
    public void updateInventory(String productId, int quantity) { /* ... */ }
}
```
x??",1204,"Small, and Focused on Doing One Thing Well Codebases grow as we write code to add new features. Over time, it can be difficult to know where a change needs to be made because the codebase is so large....",qwen2.5:latest,2025-11-02 04:25:51,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Technology Heterogeneity,Organizational Alignment and Microservices,"#### Organizational Alignment and Microservices
Background context explaining the importance of aligning microservices with team structures. Discuss how this can help in breaking down large systems.

:p How does organizational alignment impact the design and implementation of microservices?
??x
Organizational alignment is crucial for successful microservices architecture because it ensures that each service corresponds to a specific business capability managed by a dedicated team. This helps in breaking down large, monolithic systems into smaller, more manageable pieces.

When teams are aligned with services, they can own and manage the codebase more effectively, leading to better quality and faster development cycles.

For example, if you have a `SalesTeam` responsible for managing user interactions, their microservice might handle customer queries and sales processing. The `TechnicalTeam` would be responsible for building and maintaining the `SalesService`.

```java
public class SalesService {
    public void processQuery(String query) { /* ... */ }
    
    public void processSale(SaleDetails sale) { /* ... */ }
}
```
x??

---",1147,"Small, and Focused on Doing One Thing Well Codebases grow as we write code to add new features. Over time, it can be difficult to know where a change needs to be made because the codebase is so large....",qwen2.5:latest,2025-11-02 04:25:51,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Technology Heterogeneity,Autonomous Deployment of Microservices,"#### Autonomous Deployment of Microservices
Microservices are deployed as separate entities, often isolated on a platform as a service (PAAS) or their own operating system process. This isolation is crucial for maintaining simplicity and ease of reasoning about the distributed system.

:p What is meant by autonomous deployment in microservices?
??x
Autonomous deployment in microservices means that each service operates independently and can be deployed, scaled, and modified without affecting others. Services are treated as separate entities with their own lifecycle management. This isolation helps in managing changes more easily and allows services to use different technologies or tools suited for their specific tasks.

For example:
```java
// Example of a simple microservice deployment script in pseudo-code
public class MicroserviceDeployer {
    public void deployService(String serviceId, String platform) {
        if (platform.equals(""PAAS"")) {
            // Deploy on PAAS platform using appropriate APIs and commands
        } else {
            // Deploy as a standalone OS process
        }
    }
}
```
x??",1128,"Autonomous Our microservice is a separate entity. It might be deployed as an isolated service on a platform as a service (PAAS), or it might be its own operating system process. We try to avoid packin...",qwen2.5:latest,2025-11-02 04:26:13,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Technology Heterogeneity,Communication Between Microservices,"#### Communication Between Microservices
Communication between microservices is done via network calls to ensure separation of concerns and avoid tight coupling. This approach enforces loose coupling, making services more resilient and easier to maintain.

:p How does communication happen between microservices?
??x
Microservices communicate with each other using network calls or APIs over a network. This ensures that the services are loosely coupled, meaning changes in one service do not directly affect others. Network calls allow for greater flexibility and scalability as services can be developed, deployed, and scaled independently.

For example:
```java
// Pseudo-code for making an API call to another microservice
public class ServiceConsumer {
    public void requestServiceData(String apiUrl) {
        try {
            // Make a network call using HTTP/HTTPS client
            Response response = httpClient.get(apiUrl);
            if (response.isSuccessful()) {
                String data = response.body();
                // Process the received data
            }
        } catch (Exception e) {
            // Handle exceptions and errors
        }
    }
}
```
x??",1189,"Autonomous Our microservice is a separate entity. It might be deployed as an isolated service on a platform as a service (PAAS), or it might be its own operating system process. We try to avoid packin...",qwen2.5:latest,2025-11-02 04:26:13,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Technology Heterogeneity,Importance of Decoupled APIs,"#### Importance of Decoupled APIs
Decoupling APIs is crucial for ensuring that services can be modified independently without impacting other services. This approach helps in maintaining the autonomy of each service.

:p Why are decoupled APIs important?
??x
Decoupled APIs are essential because they allow services to evolve and change independently without breaking dependent services. By keeping internal representations hidden, we prevent tight coupling between services, which makes it easier to make changes or updates. The golden rule is that a change in one service should not require changing other services.

For example:
```java
// Example of an API endpoint definition in Java
public class UserService {
    @GetMapping(""/users/{userId}"")
    public User getUserDetails(@PathVariable Long userId) {
        // Logic to fetch user details from the database or cache
        return userService.getUserById(userId);
    }
}
```
x??",940,"Autonomous Our microservice is a separate entity. It might be deployed as an isolated service on a platform as a service (PAAS), or it might be its own operating system process. We try to avoid packin...",qwen2.5:latest,2025-11-02 04:26:13,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Technology Heterogeneity,Technology Heterogeneity,"#### Technology Heterogeneity
Technology heterogeneity allows different services within a system to use different technologies suited for their specific needs, rather than using a single, standardized technology stack.

:p What is technology heterogeneity in microservices?
??x
Technology heterogeneity refers to the ability of microservices to use different technologies and tools based on their specific requirements. This approach allows teams to choose the best tool for each job without being constrained by a one-size-fits-all solution. It enables better performance optimization, flexibility, and scalability.

For example:
```java
// Pseudo-code for selecting technology stacks based on service needs
public class TechnologySelector {
    public String selectTechnology(String serviceName) {
        if (serviceName.equals(""socialGraph"")) {
            return ""graphDB"";
        } else if (serviceName.equals(""posts"")) {
            return ""documentDB"";
        } else {
            throw new IllegalArgumentException(""Unknown service name"");
        }
    }
}
```
x??

---",1081,"Autonomous Our microservice is a separate entity. It might be deployed as an isolated service on a platform as a service (PAAS), or it might be its own operating system process. We try to avoid packin...",qwen2.5:latest,2025-11-02 04:26:13,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Technology Heterogeneity,Embracing Different Technologies with Microservices,"#### Embracing Different Technologies with Microservices
Background context: The passage discusses how microservices can facilitate a more flexible and safer approach to adopting new technologies. In a monolithic application, introducing a new technology often requires significant changes across the entire system, posing higher risks. Conversely, microservices allow for experimenting with new technologies in smaller, isolated services.
:p How does microservices support the adoption of different technologies?
??x
Microservices enable organizations to try out and adopt new technologies more safely by isolating these changes within individual services. This approach reduces the risk associated with introducing untested technology across the entire system. By starting with lower-risk services, teams can gradually assess and integrate new technologies without disrupting other parts of the application.
For example:
- A monolithic application might require extensive testing before adopting a new programming language or framework, impacting all components.
- In contrast, microservices allow for incremental adoption: you can start by applying a new technology in one service, evaluate its impact, and then gradually introduce it to others if successful.

```java
public class NewTechnologyService {
    // Service that uses a new technology like GoLang for processing requests
}
```
x??",1395,"Figure 1-1. Microservices can allow you to more easily embrace different technologies With microservices, we are also able to adopt technology more quickly, and understand how new advancements may hel...",qwen2.5:latest,2025-11-02 04:26:34,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Technology Heterogeneity,Risks and Constraints of Technology Choices in Microservices,"#### Risks and Constraints of Technology Choices in Microservices
Background context: The text mentions organizations like Netflix and Twitter that adopt specific technology stacks, such as the Java Virtual Machine (JVM), due to their familiarity with its reliability and performance. However, these organizations do not restrict themselves to a single stack for all services.
:p What are some strategies mentioned for managing the risks associated with multiple technologies in microservices?
??x
Netflix and Twitter use specific technology stacks like the JVM but allow flexibility by developing libraries and tooling that make it easier to scale within their chosen platform. This approach mitigates risks while still enabling them to embrace new technologies incrementally.

For example, Netflix might provide Java-based tools for monitoring and logging that are tightly integrated with its ecosystem:
```java
public class ServiceUtil {
    public static void setupMonitoring() {
        // Setup monitoring using Netflix's library
    }
}
```
x??",1051,"Figure 1-1. Microservices can allow you to more easily embrace different technologies With microservices, we are also able to adopt technology more quickly, and understand how new advancements may hel...",qwen2.5:latest,2025-11-02 04:26:34,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Technology Heterogeneity,Mitigating Risks through Rapid Iteration,"#### Mitigating Risks through Rapid Iteration
Background context: The passage emphasizes the advantage of microservices in rapidly iterating and adopting new technologies. If a team can quickly rewrite a microservice, they can better manage risks associated with new technology by limiting potential negative impacts.
:p How does rapid iteration help in managing the risks of using new technologies?
??x
Rapid iteration allows teams to experiment with new technologies in a controlled environment before fully integrating them into critical parts of their application. This approach ensures that any issues can be quickly identified and addressed, reducing the overall risk.

For example:
- A microservice written in Java could be rapidly rewritten in another language like Go if it requires significant performance improvements.
```java
public class LegacyService {
    // Original service implementation in Java
}

public class NewGoService {
    // Reimplemented service in Go for better performance
}
```
x??",1012,"Figure 1-1. Microservices can allow you to more easily embrace different technologies With microservices, we are also able to adopt technology more quickly, and understand how new advancements may hel...",qwen2.5:latest,2025-11-02 04:26:34,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Technology Heterogeneity,Evolving Technology Choices with Microservices,"#### Evolving Technology Choices with Microservices
Background context: The text highlights the importance of balancing technology choices and evolution. As mentioned, organizations like Netflix and Twitter adopt specific technology stacks but still allow services to evolve independently.
:p How do microservices enable independent technological evolution?
??x
Microservices enable independent technological evolution by allowing different services within a system to use various technologies without significant dependencies on each other. This independence means that teams can experiment with new languages, frameworks, or tools in smaller, isolated services before applying them more broadly.

For example:
- A microservice handling user authentication might be implemented in Java while another service handling real-time data streaming could use Node.js.
```java
public class AuthMicroservice {
    // Implementation using Java and Spring Security
}

public class StreamingMicroservice {
    // Implementation using Node.js for real-time data processing
}
```
x??",1070,"Figure 1-1. Microservices can allow you to more easily embrace different technologies With microservices, we are also able to adopt technology more quickly, and understand how new advancements may hel...",qwen2.5:latest,2025-11-02 04:26:34,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Technology Heterogeneity,Ensuring Service Independence through Integration,"#### Ensuring Service Independence through Integration
Background context: The passage concludes by suggesting that integration strategies in microservices should focus on ensuring services can evolve independently without excessive coupling.
:p How do organizations ensure that their microservices can evolve independently?
??x
Organizations must design and implement integration strategies that allow microservices to evolve independently. This involves minimizing dependencies between services so that changes in one service do not inadvertently impact others.

For example:
- Use event-driven architectures where services communicate through events or messages, reducing direct dependencies.
```java
public class UserService {
    public void addUser(User user) {
        // Publish an event instead of directly interacting with another service
        userServiceEvents.publish(user);
    }
}

public class NotificationService {
    @SubscribeEvent
    public void onUserAdded(User user) {
        sendNotification(user);
    }
}
```
x??",1042,"Figure 1-1. Microservices can allow you to more easily embrace different technologies With microservices, we are also able to adopt technology more quickly, and understand how new advancements may hel...",qwen2.5:latest,2025-11-02 04:26:34,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Modules,Resilience in Microservices,"#### Resilience in Microservices
Resilience is a crucial aspect of microservice architecture, where service boundaries act as bulkheads. In monolithic systems, if one component fails, it can bring down the entire system. However, with microservices, isolated failures are contained within specific services, allowing other parts of the system to continue functioning.
:p How do microservices enhance resilience compared to monolithic applications?
??x
Microservices enhance resilience by isolating individual components, meaning that if a service fails, it does not necessarily bring down the entire system. This isolation allows for better management and recovery from failures. For example:
```java
public class OrderService {
    public void processOrder(Order order) throws ServiceUnavailableException {
        try {
            // Code to process the order
        } catch (SomeException e) {
            throw new ServiceUnavailableException(""Order processing failed"");
        }
    }
}
```
This code ensures that if an exception occurs during order processing, it is caught and rethrown as a `ServiceUnavailableException`, which can be handled by the service's caller.
x??",1181,"Resilience A key concept in resilience engineering is the bulkhead. If one component of a system fails, but that failure doesn’t cascade, you can isolate the problem and the rest of the system can car...",qwen2.5:latest,2025-11-02 04:26:54,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Modules,Scaling in Microservices,"#### Scaling in Microservices
In monolithic applications, scaling is a challenge because all components scale together. However, with microservices, you can scale individual services based on their needs. This allows for more efficient use of hardware resources and better handling of traffic spikes.
:p How does scaling work differently in microservices compared to monolithic applications?
??x
In microservices, each service can be scaled independently based on its load. For instance, if an application has a user service and a payment service, you can scale the user service during peak hours while keeping the payment service at a more stable level. This targeted scaling is not possible in monolithic architectures.
```java
public class UserMicroservice {
    public void handleUserRequest() {
        // Code to process user requests
    }
}

public class PaymentMicroservice {
    public void handlePaymentRequest() {
        // Code to process payment requests
    }
}
```
In this example, you can scale the `UserMicroservice` independently of the `PaymentMicroservice`, allowing for more efficient resource utilization.
x??",1133,"Resilience A key concept in resilience engineering is the bulkhead. If one component of a system fails, but that failure doesn’t cascade, you can isolate the problem and the rest of the system can car...",qwen2.5:latest,2025-11-02 04:26:54,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Modules,Ease of Deployment in Microservices,"#### Ease of Deployment in Microservices
Deploying changes in a monolithic application requires redeploying the entire system. This can be risky and infrequent, leading to large deltas between deployments. In microservices, you can update individual services independently, reducing risk and accelerating deployment cycles.
:p How does ease of deployment differ between monolithic applications and microservices?
??x
In monolithic applications, a small change necessitates redeploying the entire application, which can be risky and time-consuming. With microservices, changes can be deployed to specific services without affecting others, reducing risk and allowing for more frequent deployments.
```java
public class UpdateUserService {
    public void applyPatch() {
        // Code to update user service
    }
}

public class UpdatePaymentService {
    public void applyPatch() {
        // Code to update payment service
    }
}
```
In this example, you can deploy updates to the `UpdateUserService` and `UpdatePaymentService` independently. This allows for smaller, more frequent deployments with reduced risk.
x??

---",1125,"Resilience A key concept in resilience engineering is the bulkhead. If one component of a system fails, but that failure doesn’t cascade, you can isolate the problem and the rest of the system can car...",qwen2.5:latest,2025-11-02 04:26:54,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Modules,Organizational Alignment,"#### Organizational Alignment
Background context explaining how large teams and large codebases can lead to productivity issues, especially with distributed teams. Microservices allow better alignment of architecture to organization size, optimizing team sizes for higher productivity.

:p How do microservices help align organizational structure?
??x
Microservices enable a more granular division of labor, where smaller teams focus on manageable pieces of the system. This helps minimize the number of people working on any one codebase, leading to optimal team size and increased productivity. By collocating ownership of services between teams, microservices facilitate better collaboration and reduce the complexity associated with large monolithic systems.

```java
// Example of a small service definition in Java
public class UserService {
    // Service methods for user-related operations
}
```
x??",908,"The technology in this space has changed greatly in the last couple of years, and we’ll be looking more deeply into the topic of deployment in a microservice world in  Chapter 6 . Organizational Align...",qwen2.5:latest,2025-11-02 04:27:09,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Modules,Composability,"#### Composability
Explanation of how microservices enhance functionality reuse across different platforms (Web, mobile, desktop). Microservices allow for flexibility and adaptability to changing customer engagement strategies.

:p How do microservices improve the way we think about software reuse?
??x
Microservices provide a flexible architecture that allows components of an application to be used in various contexts. This is particularly useful as consumer needs expand beyond traditional channels like desktop websites or mobile apps, including new platforms such as native applications, mobile web, tablets, and wearables. By decomposing the system into small, independently deployable services, we can more easily adapt our architecture to meet diverse customer engagement strategies.

```java
// Example of a service being used in different contexts
public class WeatherService {
    public String getWeatherData(String location) {
        // Logic to fetch weather data based on location
        return ""Weather Data for "" + location;
    }
}
```
x??",1061,"The technology in this space has changed greatly in the last couple of years, and we’ll be looking more deeply into the topic of deployment in a microservice world in  Chapter 6 . Organizational Align...",qwen2.5:latest,2025-11-02 04:27:09,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Modules,Optimizing for Replaceability,"#### Optimizing for Replaceability
Explanation of the challenges faced with large, legacy systems and how microservices can make it easier to replace or remove services without significant risk.

:p How do microservices help in replacing or removing components?
??x
Microservices are designed to be small and modular, making it much simpler to replace or delete them when necessary. In a monolithic application, the cost of rewriting or deleting large portions of code is high due to the interconnected nature of the system. With microservices, each service can be independently implemented, tested, and deployed. This makes it easier to introduce new services or remove old ones without disrupting other parts of the system.

```java
// Example of a service being replaced
public class LegacyUserService {
    public String getUserDetails(int userId) {
        // Legacy logic for getting user details
        return ""Legacy User Details"";
    }
}

public class NewUserService {
    public String getUserDetails(int userId) {
        // Updated logic for getting user details
        return ""New User Details"";
    }
}
```
x??

---",1132,"The technology in this space has changed greatly in the last couple of years, and we’ll be looking more deeply into the topic of deployment in a microservice world in  Chapter 6 . Organizational Align...",qwen2.5:latest,2025-11-02 04:27:09,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Modules,Service-Oriented Architecture (SOA),"#### Service-Oriented Architecture (SOA)
Background context: SOA is a design approach where multiple services collaborate to provide end-user capabilities. Each service typically runs as a separate process and communicates via network calls, promoting reusability and ease of maintenance. While theoretically sound, many challenges arise in implementing SOA effectively.

:p What are the key features of Service-Oriented Architecture (SOA)?
??x
Service-Oriented Architecture (SOA) emphasizes modularity through services that operate as separate processes. Communication between these services happens over a network, allowing for reusability and easier maintenance compared to monolithic applications.
x??",705,"When a codebase is just a few hundred lines long, it is difficult for people to become emotionally attached to it, and the cost of replacing it is pretty small. What About Service-Oriented Architectur...",qwen2.5:latest,2025-11-02 04:27:25,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Modules,Challenges with SOA Implementation,"#### Challenges with SOA Implementation
Background context: Despite its benefits, SOA implementation faces numerous challenges such as communication protocols (e.g., SOAP), vendor middleware, and lack of guidance on service granularity. These factors can undermine the goal of SOA.

:p What are some common issues that arise during the implementation of Service-Oriented Architecture (SOA)?
??x
Common issues include:
- Communication protocols like SOAP.
- Vendor middleware complicating integration.
- Lack of clear guidelines for determining service granularity.
- Over-coupling services leading to maintenance difficulties.
x??",630,"When a codebase is just a few hundred lines long, it is difficult for people to become emotionally attached to it, and the cost of replacing it is pretty small. What About Service-Oriented Architectur...",qwen2.5:latest,2025-11-02 04:27:25,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Modules,Microservices as a Specific Approach for SOA,"#### Microservices as a Specific Approach for SOA
Background context: Microservices represent a granular approach to SOA, focusing on breaking down large systems into smaller, independently deployable services. This aligns with the principles of SOA but addresses its challenges by providing practical guidance.

:p How do microservices differ from traditional SOA?
??x
Microservices take SOA principles and apply them more practically:
- Focus on granular decomposition.
- Allow for independent deployment and scaling.
- Provide clear guidelines for splitting systems into manageable services.
- Enhance modularity while mitigating risks associated with SOA, such as over-coupling.
x??",686,"When a codebase is just a few hundred lines long, it is difficult for people to become emotionally attached to it, and the cost of replacing it is pretty small. What About Service-Oriented Architectur...",qwen2.5:latest,2025-11-02 04:27:25,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Modules,Decompositional Techniques,"#### Decompositional Techniques
Background context: Microservices emerge from the need to decompose large systems effectively. Other techniques like shared libraries also offer ways to achieve similar benefits by breaking down codebases into reusable components.

:p What are some alternative decompositional techniques to microservices?
??x
Alternative decompositional techniques include:
- Shared Libraries: Breaking a codebase into multiple libraries that can be shared between teams and services.
- Code organization practices like modular programming or component-based design.
- Using containerization tools for better isolation and deployment flexibility.
x??",666,"When a codebase is just a few hundred lines long, it is difficult for people to become emotionally attached to it, and the cost of replacing it is pretty small. What About Service-Oriented Architectur...",qwen2.5:latest,2025-11-02 04:27:25,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Modules,Shared Libraries as a Decomposition Technique,"#### Shared Libraries as a Decomposition Technique
Background context: Shared libraries are a standard decompositional technique used in virtually any language. They allow functionality to be shared between teams and services, reducing redundancy and improving maintainability.

:p How do shared libraries contribute to codebase decomposition?
??x
Shared libraries help by:
- Allowing the sharing of common functionalities across different parts of an application.
- Reducing redundant code and promoting consistency.
- Facilitating team collaboration through modular design.
- Enabling easier maintenance and updates via a single source for shared functionality.

Example usage in Java:
```java
// SharedLibrary.java
public class SharedLibrary {
    public void performAction() {
        // Code to be shared across multiple services
    }
}

// Service1.java
import com.example.SharedLibrary;

public class Service1 {
    private final SharedLibrary sharedLib = new SharedLibrary();
    
    public void executeTask() {
        sharedLib.performAction();
    }
}
```
x??",1072,"When a codebase is just a few hundred lines long, it is difficult for people to become emotionally attached to it, and the cost of replacing it is pretty small. What About Service-Oriented Architectur...",qwen2.5:latest,2025-11-02 04:27:25,7
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Modules,Shared Libraries,"#### Shared Libraries

Background context explaining shared libraries. Discuss their utility and common use cases within organizations, particularly for reusing common tasks across different parts of an application or system.

:p What are shared libraries, and why might teams create them?

??x
Shared libraries are collections of precompiled code that can be reused in multiple applications or services. Teams often create these to avoid duplication of effort by implementing common utilities or functions once and then sharing them across the organization. This is particularly useful for tasks that aren’t specific to a single business domain.

For example, consider a library for mathematical operations, string manipulation, or date handling, which can be reused across various parts of an application.
```java
// Example Java shared library class
public class MathUtil {
    public static int add(int a, int b) {
        return a + b;
    }

    public static String formatDate(Date date) {
        SimpleDateFormat sdf = new SimpleDateFormat(""yyyy-MM-dd"");
        return sdf.format(date);
    }
}
```
x??",1112,"I might create a set of useful collection utilities, for example, or perhaps a statistics library that can be reused. Teams can organize themselves around these libraries, and the libraries themselves...",qwen2.5:latest,2025-11-02 04:27:48,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Modules,Drawbacks of Shared Libraries,"#### Drawbacks of Shared Libraries

Background context on the potential drawbacks of using shared libraries, including limitations in technology heterogeneity, scaling, and deployment.

:p What are some key drawbacks of using shared libraries?

??x
Some key drawbacks include:

1. **Technology Heterogeneity**: The library typically has to be in the same language or run on the same platform.
2. **Scalability**: Scaling parts of your system independently from each other is curtailed.
3. **Deployment**: Deploying a new version of a shared library often requires redeploying the entire process, reducing the ability to deploy changes in isolation.
4. **Architectural Safety Measures**: Shared libraries lack obvious seams for architectural safety measures, making it harder to ensure system resiliency.

For example, if you have a Java application and a C++ shared library both needing to be used, this might require significant integration efforts or even rewrites of parts of the application.
x??",999,"I might create a set of useful collection utilities, for example, or perhaps a statistics library that can be reused. Teams can organize themselves around these libraries, and the libraries themselves...",qwen2.5:latest,2025-11-02 04:27:48,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Modules,Modular Decomposition,"#### Modular Decomposition

Background context on modular decomposition techniques, including examples like OSGi and Erlang's approach. Discuss how different languages handle module lifecycle management.

:p What is modular decomposition, and what are some approaches to implementing it?

??x
Modular decomposition involves breaking down a system or application into smaller, more manageable pieces called modules. This can enhance maintainability, reduce coupling between components, and enable easier updates and scaling.

Different technologies offer different ways to implement this:

- **OSGi**: A framework that allows plug-ins to be installed in processes like the Eclipse IDE. It provides module lifecycle management, but often requires significant work from developers to ensure proper isolation.
- **Erlang**: Built-in capabilities for modules to be stopped, restarted, and upgraded without issues, allowing running multiple versions of a module at the same time.

Example Erlang code demonstrating module lifecycle:
```erlang
-module(my_module).
-export([start/0, stop/0]).

start() ->
    io:format(""Starting my_module~n""),
    % Start processes or services here

stop() ->
    io:format(""Stopping my_module~n""),
    % Stop processes or services here.
```
x??",1271,"I might create a set of useful collection utilities, for example, or perhaps a statistics library that can be reused. Teams can organize themselves around these libraries, and the libraries themselves...",qwen2.5:latest,2025-11-02 04:27:48,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Modules,Challenges with Modular Decomposition within a Single Process Boundary,"#### Challenges with Modular Decomposition within a Single Process Boundary

Background context on the challenges faced when implementing modular decomposition within a single process, including coupling and hygiene issues.

:p What are some challenges of implementing modular decomposition within a single process boundary?

??x
Some key challenges include:

- **Tight Coupling**: Even in well-managed teams, modules can become overly coupled, leading to integration problems.
- **Hygiene Issues**: Without clear boundaries (process separation), it is easier for code to become tightly integrated, reducing the benefits of modular decomposition.

For example, if multiple modules share common data structures or interfaces without clear separation, this can lead to complex interdependencies and maintenance issues.

```java
// Example of potential tight coupling in a monolithic application
public class ModuleA {
    private static ServiceB serviceB;

    public void doSomething() {
        // Use some functionality from serviceB
        serviceB.someMethod();
    }
}
```
x??

---",1086,"I might create a set of useful collection utilities, for example, or perhaps a statistics library that can be reused. Teams can organize themselves around these libraries, and the libraries themselves...",qwen2.5:latest,2025-11-02 04:27:48,7
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Inaccurate Comparisons,No Silver Bullet: Microservices Complexity,"#### No Silver Bullet: Microservices Complexity

Background context explaining that microservices, despite their potential benefits, come with significant complexities. These include distributed system challenges and the need for improved deployment, testing, monitoring, scaling, and resilience.

:p What are some of the associated complexities introduced by using microservices?
??x
Microservices introduce several complexities typical of distributed systems:
- Deployment: Managing multiple services requires careful planning.
- Testing: Ensuring each service works correctly in isolation as well as with others is challenging.
- Monitoring: Keeping an eye on the performance and health of individual services can be daunting.
- Scaling: Determining how to scale different services independently is complex.
- Resilience: Building systems that can handle failures gracefully requires advanced strategies.

x??",912,"No Silver Bullet Before we finish, I should call out that microservices are no free lunch or silver bullet, and make for a bad choice as a golden hammer. They have all the associated complexities of d...",qwen2.5:latest,2025-11-02 04:28:06,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Inaccurate Comparisons,The Evolutionary Architect,"#### The Evolutionary Architect

Background context explaining the role shift in microservices architecture, emphasizing the need for architects to adapt their roles due to faster change and more fluid environments. This includes decisions on technology stack, team autonomy, and service boundaries.

:p How do the roles of architects evolve with microservices?
??x
Architects must now focus more on guiding and adapting rather than dictating a single vision. Key changes include:
- Embracing diversity in technologies and programming idioms.
- Facilitating autonomous teams to make their own decisions.
- Handling service boundaries flexibly based on evolving requirements.

x??",679,"No Silver Bullet Before we finish, I should call out that microservices are no free lunch or silver bullet, and make for a bad choice as a golden hammer. They have all the associated complexities of d...",qwen2.5:latest,2025-11-02 04:28:06,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Inaccurate Comparisons,Inaccurate Comparisons,"#### Inaccurate Comparisons

Background context highlighting the misuse of terms like ""architect"" or ""engineer"" in software development, explaining the unique challenges faced by developers compared to established professions. Discusses the lack of historical depth and rigorous training required for IT roles versus traditional professions.

:p Why are comparisons between software architects/engineers and traditional architects/engineers inaccurate?
??x
Comparisons are inaccurate because:
- Software development is a relatively new field with limited historical context.
- Traditional architecture/engineering have stringent qualifications and long-standing bodies of knowledge.
- Software systems can change rapidly, unlike physical structures which adhere to immutable laws.

x??",785,"No Silver Bullet Before we finish, I should call out that microservices are no free lunch or silver bullet, and make for a bad choice as a golden hammer. They have all the associated complexities of d...",qwen2.5:latest,2025-11-02 04:28:06,2
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Inaccurate Comparisons,Challenges for Architects,"#### Challenges for Architects

Background context discussing the critical role architects play in ensuring technical vision but also highlighting common criticisms. Emphasizes the importance of recognizing that architects significantly influence system quality and organizational adaptability despite frequent missteps.

:p What are some criticisms faced by software architects?
??x
Software architects face several criticisms:
- Overly prescriptive approaches leading to rigid systems.
- Lack of understanding about the dynamic nature of software development.
- Poor handling of evolving requirements and team autonomy.

x??",626,"No Silver Bullet Before we finish, I should call out that microservices are no free lunch or silver bullet, and make for a bad choice as a golden hammer. They have all the associated complexities of d...",qwen2.5:latest,2025-11-02 04:28:06,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Inaccurate Comparisons,Redefining Architecture,"#### Redefining Architecture

Background context suggesting a redefinition of what ""architecture"" means in the context of software development, emphasizing flexibility and adaptability. Suggests architects should focus on guiding rather than dictating final designs.

:p How should the role of an architect be redefined?
??x
The role of an architect should be redefined as:
- A guide to maintaining a cohesive technical vision.
- Facilitating decisions by teams based on evolving needs and technologies.
- Ensuring adaptability over rigidity in system design.

x??

---",569,"No Silver Bullet Before we finish, I should call out that microservices are no free lunch or silver bullet, and make for a bad choice as a golden hammer. They have all the associated complexities of d...",qwen2.5:latest,2025-11-02 04:28:06,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,An Evolutionary Vision for the Architect,Town Planner Analogy for IT Architects,"#### Town Planner Analogy for IT Architects
Background context explaining the concept. The author draws a parallel between IT architects and town planners to highlight the need for flexibility and long-term planning rather than detailed design up front.
:p How does the role of an IT architect compare to that of a town planner?
??x
The comparison emphasizes that just like a town planner, an IT architect should focus on broad strategic planning and flexible frameworks. They do not prescribe every detail but establish zones or guidelines where development can happen based on user needs and future requirements.
```java
// Example pseudocode for setting up basic framework (simplified)
public class FrameworkSetup {
    public void setupZones() {
        // Define different zones like industrial, residential, etc.
        defineZone(""industrial"");
        defineZone(""residential"");
        // Ensure the system is adaptable and can grow over time
        ensureAdaptability();
    }
    
    private void defineZone(String zone) {
        // Logic to set up specific rules for each zone
        System.out.println(""Setting "" + zone + "" zone."");
    }
    
    private void ensureAdaptability() {
        // Code to enforce adaptability of the system
        System.out.println(""Ensuring the system can evolve over time."");
    }
}
```
x??",1344,An Evolutionary Vision for the Architect Our requirements shift more rapidly than they do for people who design and build buildings — as do the tools and techniques at our disposal. The things we crea...,qwen2.5:latest,2025-11-02 04:28:31,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,An Evolutionary Vision for the Architect,Long-Term Planning and Flexibility in Architectural Design,"#### Long-Term Planning and Flexibility in Architectural Design
Background context explaining the concept. The text stresses the importance of designing software frameworks that can grow and change with user needs, rather than creating a fixed product.
:p Why is it important for IT architects to focus on long-term planning and flexibility?
??x
It is crucial because once software is deployed into production, its requirements will continue to evolve based on actual usage. Fixed designs do not account for the dynamic nature of real-world use cases, making flexible frameworks essential for accommodating change.
```java
// Example pseudocode for a flexible design approach
public class FlexibleDesign {
    public void createFramework() {
        // Define core components that can be extended or modified
        defineCoreComponents();
        // Ensure the system allows for future enhancements
        ensureFutureEnhancements();
    }
    
    private void defineCoreComponents() {
        System.out.println(""Defining core components to support growth."");
    }
    
    private void ensureFutureEnhancements() {
        System.out.println(""Ensuring the framework can be extended in the future."");
    }
}
```
x??",1222,An Evolutionary Vision for the Architect Our requirements shift more rapidly than they do for people who design and build buildings — as do the tools and techniques at our disposal. The things we crea...,qwen2.5:latest,2025-11-02 04:28:31,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,An Evolutionary Vision for the Architect,Ensuring User and Developer Happiness,"#### Ensuring User and Developer Happiness
Background context explaining the concept. The text underscores the need for IT architects to create systems that are not only user-friendly but also developer-friendly, promoting a harmonious environment.
:p How should an IT architect ensure both users and developers are happy with the system?
??x
IT architects should design systems that meet current needs while providing a platform for future development. They must balance usability for end-users with maintainability and scalability for developers to foster a collaborative and productive environment.
```java
// Example pseudocode for user and developer satisfaction
public class UserDeveloperHappiness {
    public void ensureSatisfaction() {
        // Ensure the system is user-friendly
        ensureUserFriendly();
        // Ensure the system is developer-friendly
        ensureDeveloperFriendly();
    }
    
    private void ensureUserFriendly() {
        System.out.println(""Ensuring the system is easy to use for end-users."");
    }
    
    private void ensureDeveloperFriendly() {
        System.out.println(""Ensuring the system supports ease of development and maintenance."");
    }
}
```
x??",1207,An Evolutionary Vision for the Architect Our requirements shift more rapidly than they do for people who design and build buildings — as do the tools and techniques at our disposal. The things we crea...,qwen2.5:latest,2025-11-02 04:28:31,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,An Evolutionary Vision for the Architect,Anticipating Changes in User Behavior,"#### Anticipating Changes in User Behavior
Background context explaining the concept. The text highlights that IT architects must anticipate how users will interact with software over time, allowing for changes without rigid planning.
:p Why is it important to anticipate user behavior changes?
??x
Anticipating user behavior changes helps in designing systems that can adapt to evolving needs and usage patterns. By preparing for potential future changes, architects can create more resilient and flexible solutions.
```java
// Example pseudocode for change anticipation
public class ChangeAnticipation {
    public void prepareForChanges() {
        // Analyze current usage trends
        analyzeUsageTrends();
        // Plan for potential future changes based on analysis
        planForFutureChanges();
    }
    
    private void analyzeUsageTrends() {
        System.out.println(""Analyzing current user behavior to predict future needs."");
    }
    
    private void planForFutureChanges() {
        System.out.println(""Preparing the system to accommodate future usage patterns."");
    }
}
```
x??",1106,An Evolutionary Vision for the Architect Our requirements shift more rapidly than they do for people who design and build buildings — as do the tools and techniques at our disposal. The things we crea...,qwen2.5:latest,2025-11-02 04:28:31,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,An Evolutionary Vision for the Architect,Enforcing Corrective Actions in Architectural Planning,"#### Enforcing Corrective Actions in Architectural Planning
Background context explaining the concept. The text explains that while architects should be flexible, they must also have mechanisms to correct deviations from their plans.
:p How do IT architects ensure adherence to their design frameworks?
??x
IT architects need to set up clear guidelines and zones within which development can occur. While they provide broad direction, they must also have mechanisms to enforce these rules when necessary, ensuring the system evolves as intended.
```java
// Example pseudocode for enforcing design framework
public class EnforceDesignFramework {
    public void enforceRules() {
        // Define the framework rules
        defineRules();
        // Monitor and correct deviations from the plan
        monitorAndCorrectDeviations();
    }
    
    private void defineRules() {
        System.out.println(""Defining clear rules for development zones."");
    }
    
    private void monitorAndCorrectDeviations() {
        System.out.println(""Monitoring system developments to ensure adherence to framework rules."");
    }
}
```
x??

---",1135,An Evolutionary Vision for the Architect Our requirements shift more rapidly than they do for people who design and build buildings — as do the tools and techniques at our disposal. The things we crea...,qwen2.5:latest,2025-11-02 04:28:31,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Zoning,Service Zones and Boundaries,"#### Service Zones and Boundaries
In the context of architecting systems, service zones or boundaries represent coarse-grained groups of services. The primary focus as an architect should be on how these services communicate with each other rather than diving into the internal details of each zone.
:p What are the main concerns for architects regarding service zones?
??x
The main concern is ensuring that services can effectively communicate and interact, while also being mindful of monitoring the overall health of the system. This involves thinking about inter-service communication protocols, error handling, and resilience mechanisms.
For example, in a microservices architecture, an architect might focus on defining clear APIs between services rather than deeply understanding how each service works internally.
```java
public class ServiceA {
    @GetMapping(""/api/data"")
    public ResponseEntity<Data> fetchData() {
        // Logic to fetch data
    }
}
```
x??",975,"Zoning So, to continue the metaphor of the architect as town planner for a moment, what are our zones? These are our service boundaries, or perhaps coarse-grained groups of services. As architects, we...",qwen2.5:latest,2025-11-02 04:28:52,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Zoning,Microservices and Team Autonomy,"#### Microservices and Team Autonomy
Microservices allow teams within an organization to have autonomy in choosing the best technology stack for their services. This approach is particularly useful when teams need flexibility and can operate independently.
:p How do microservices enable team autonomy?
??x
Microservices empower teams by allowing them to choose different technologies, data stores, and development methodologies tailored to their specific needs. This promotes innovation and adaptability within the organization.

However, this independence must be balanced with considerations such as shared infrastructure, tooling, and data management.
```java
public class TeamB {
    private final String techStack = ""Node.js"";
    private final String datastore = ""MongoDB"";

    public void developService() {
        // Logic for developing a service using Node.js and MongoDB
    }
}
```
x??",900,"Zoning So, to continue the metaphor of the architect as town planner for a moment, what are our zones? These are our service boundaries, or perhaps coarse-grained groups of services. As architects, we...",qwen2.5:latest,2025-11-02 04:28:52,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Zoning,Inter-Service Communication Standards,"#### Inter-Service Communication Standards
Effective inter-service communication is crucial in microservices architectures. Using consistent protocols can simplify integration, whereas varied protocols may complicate the development process.
:p What are the implications of inconsistent communication standards between services?
??x
Inconsistent communication standards can lead to complex and error-prone integration points. Different services might use different protocols (e.g., REST over HTTP, protocol buffers, Java RMI), making it challenging for consumers to handle these varying styles.

For instance:
```java
public class ServiceA {
    @GetMapping(""/api/data"")
    public ResponseEntity<Data> fetchData() {
        // Logic to fetch data using REST over HTTP
    }
}

public class ServiceB {
    @RpcService
    public void processData(Data data) {
        // Logic for processing data using Java RMI
    }
}
```
This inconsistency can cause difficulties in maintaining a cohesive system and may increase the complexity of debugging and troubleshooting.
x??",1067,"Zoning So, to continue the metaphor of the architect as town planner for a moment, what are our zones? These are our service boundaries, or perhaps coarse-grained groups of services. As architects, we...",qwen2.5:latest,2025-11-02 04:28:52,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Zoning,Architectural Involvement with Teams,"#### Architectural Involvement with Teams
Architects need to balance their involvement in teams. While architects should not micro-manage internal decisions, they must remain involved enough to understand the team's development practices and issues.

Regular interactions can help maintain a good relationship and ensure that architectural principles are followed.
:p How often should an architect spend time with each team?
??x
The frequency of interaction depends on the size of the teams. For instance, if working with four teams, spending half a day with each team every four weeks could be effective.

This routine ensures that architects stay informed about ongoing projects and can provide timely feedback.
```java
public class Architect {
    public void spendTimeWithTeams(List<Team> teams) {
        for (Team team : teams) {
            // Spend time with the team
            team.interact(this);
            // Check in after a predefined interval
        }
    }
}
```
x??

---",991,"Zoning So, to continue the metaphor of the architect as town planner for a moment, what are our zones? These are our service boundaries, or perhaps coarse-grained groups of services. As architects, we...",qwen2.5:latest,2025-11-02 04:28:52,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Tailored Service Template,Strategic Goals,"#### Strategic Goals
Background context explaining the importance of strategic goals for an organization. These high-level objectives should align with where the company is heading and how it aims to satisfy its customers or achieve its mission.

:p What are strategic goals, and why are they important for architects?
??x
Strategic goals are overarching objectives that speak to a company's future direction and how it intends to meet customer needs. They help align technological decisions with broader business objectives. For example, ""Expand into Southeast Asia to unlock new markets"" or ""Let the customer achieve as much as possible using self-service."" Architects need to ensure technology choices support these goals.

```java
public class Example {
    // Imagine a method that checks if current technology strategy supports strategic goals
    public boolean isTechnologyAligned(String goal, String[] technologies) {
        for (String tech : technologies) {
            if (tech.contains(""expansion"") || tech.contains(""self-service"")) {
                return true;
            }
        }
        return false;
    }
}
```
x??",1139,"A Principled Approach Rules are for the obedience of fools and the guidance of wise men. Generally attributed to Douglas Bader Making decisions in system design is all about trade-offs, and microservi...",qwen2.5:latest,2025-11-02 04:29:14,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Tailored Service Template,Principles,"#### Principles
Explanation of principles as rules defined to align actions with larger goals. These can change over time and are fewer in number compared to constraints, usually less than 10.

:p What is a principle, and how does it differ from a constraint?
??x
A principle is a rule you establish to ensure decisions align with your organization's broader objectives. For example, if the goal is to reduce time-to-market for new features, you might decide that delivery teams can ship software independently whenever they are ready. Principles are more flexible than constraints; constraints often represent behaviors or requirements that are difficult to change.

```java
public class Example {
    // Defining a principle as part of a method
    public boolean adhereToPrinciple(String teamName, String decision) {
        if (teamName.equals(""Delivery Team"")) {
            return decision.startsWith(""Ship "");
        }
        return false;
    }
}
```
x??",964,"A Principled Approach Rules are for the obedience of fools and the guidance of wise men. Generally attributed to Douglas Bader Making decisions in system design is all about trade-offs, and microservi...",qwen2.5:latest,2025-11-02 04:29:14,7
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Tailored Service Template,Heroku's 12 Factors,"#### Heroku's 12 Factors
Explanation of the Heroku design principles and their relevance to creating applications that work well on Heroku. These factors can be adapted for other contexts as well.

:p What are Heroku’s 12 Factors, and how might they apply in a broader context?
??x
Heroku’s 12 Factors are guidelines structured around helping developers create applications that perform well on the Heroku platform. They include both principles and constraints. For example:

- **The twelve-factor app is configured via environment variables** - This principle can be adapted to other contexts where configuration should not be hard-coded.
- **松耦合的系统设计** - 这种约束可能在其他环境中也很有用，确保系统的各个部分尽可能独立工作。

```java
public class Example {
    // Using an environment variable in a method
    public int calculateFactor(String envVar) {
        return Integer.parseInt(System.getenv(envVar));
    }
}
```
x??",892,"A Principled Approach Rules are for the obedience of fools and the guidance of wise men. Generally attributed to Douglas Bader Making decisions in system design is all about trade-offs, and microservi...",qwen2.5:latest,2025-11-02 04:29:14,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Tailored Service Template,Trade-offs in Microservices,"#### Trade-offs in Microservices
Explanation of the trade-offs involved in microservice architectures, including decisions about data storage and technology stacks.

:p What are some common trade-offs when designing a system with microservices?
??x
When designing a microservice architecture, several trade-offs come into play. For example:
- **Choosing a less familiar datastore for better scaling** - vs. sticking to something more tried-and-tested.
- **Deciding on technology stack diversity** - Is it okay to have two different stacks? Three?

```java
public class Example {
    // Pseudocode to decide on a data storage choice
    public DataStore chooseDataStore(String experience, String scalability) {
        if (experience < 5 && scalability > 3) return new NoSQL();
        else return new SQL();
    }
}
```
x??",823,"A Principled Approach Rules are for the obedience of fools and the guidance of wise men. Generally attributed to Douglas Bader Making decisions in system design is all about trade-offs, and microservi...",qwen2.5:latest,2025-11-02 04:29:14,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Tailored Service Template,Decision Making with Incomplete Information,"#### Decision Making with Incomplete Information
Explanation of how framing and principles can help make decisions when information is incomplete.

:p How do principles and framing aid in making decisions with limited information?
??x
Principles provide a framework for guiding decision-making, especially when the information available is incomplete. They allow you to align choices with your organization's goals even without full data. For example:
- If one goal is rapid release cycles, you might decide that delivery teams have full control over their software lifecycles.
- If another goal is global expansion, you may ensure all systems are portable.

```java
public class Example {
    // Method to apply principles in decision-making
    public boolean makeDecision(String[] principles, String situation) {
        for (String principle : principles) {
            if (principle.contains(""rapid release"") && situation.equals(""need quick deployment"")) return true;
            if (principle.contains(""global expansion"") && situation.equals(""deploy in new regions"")) return true;
        }
        return false;
    }
}
```
x??

---",1139,"A Principled Approach Rules are for the obedience of fools and the guidance of wise men. Generally attributed to Douglas Bader Making decisions in system design is all about trade-offs, and microservi...",qwen2.5:latest,2025-11-02 04:29:14,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Tailored Service Template,Practices Definition and Role,"#### Practices Definition and Role
Background context explaining what practices are, their purpose, and how they relate to principles. Practices ensure that principles are being carried out through detailed, task-oriented guidance. They often reflect technology-specific requirements and can change more frequently than principles.

:p What are practices in the context described?
??x
Practices are detailed, practical guidelines for performing tasks, ensuring that overarching principles are followed in a technical manner. They are specific to technologies and processes used by developers.
x??",596,"Practices Our practices are how we ensure our principles are being carried out. They are a set of detailed, practical guidance for performing tasks. They will often be technology-specific, and should ...",qwen2.5:latest,2025-11-02 04:29:34,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Tailored Service Template,Example of Practices,"#### Example of Practices
An example was provided about how a principle (delivery teams controlling the full lifecycle) can be underpinned by a practice (deploying services into isolated AWS accounts). This illustrates how practices provide concrete steps that align with broader principles.

:p Can you give an example of how a principle is translated into a practice?
??x
Sure, a principle stating that ""delivery teams control the full lifecycle of their systems"" could be translated into a practice such as ""all services are deployed into isolated AWS accounts."" This practice ensures self-service management and isolation from other teams, directly supporting the overarching principle.

For example:
```java
public class ServiceDeploymentManager {
    public void deployService(String serviceName, String awsAccount) {
        // Code to ensure service is deployed in an isolated AWS account
    }
}
```
x??",912,"Practices Our practices are how we ensure our principles are being carried out. They are a set of detailed, practical guidance for performing tasks. They will often be technology-specific, and should ...",qwen2.5:latest,2025-11-02 04:29:34,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Tailored Service Template,Combining Principles and Practices,"#### Combining Principles and Practices
The text suggests that for a smaller group or single team, combining principles and practices might be acceptable. However, larger organizations may need different sets of practices due to varying technologies and working practices, as long as they align with common principles.

:p How should large organizations handle the combination of principles and practices?
??x
Large organizations can have distinct sets of practices for different teams or technologies (e.g., .NET team vs. Java team) while maintaining a unified set of overarching principles. This approach ensures consistency in core values while allowing flexibility in implementation details.
x??",699,"Practices Our practices are how we ensure our principles are being carried out. They are a set of detailed, practical guidance for performing tasks. They will often be technology-specific, and should ...",qwen2.5:latest,2025-11-02 04:29:34,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Tailored Service Template,Real-World Example,"#### Real-World Example
A real-world example provided by Evan Bottcher shows how goals, principles, and practices interact. Practices change more frequently than principles over time.

:p Can you describe the importance of a diagram illustrating principles and practices?
??x
A diagram like the one described is crucial for visualizing how high-level goals map to specific principles and then down to detailed practices. It helps in communicating complex ideas clearly and ensuring everyone understands their roles and responsibilities.
x??",540,"Practices Our practices are how we ensure our principles are being carried out. They are a set of detailed, practical guidance for performing tasks. They will often be technology-specific, and should ...",qwen2.5:latest,2025-11-02 04:29:34,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Tailored Service Template,The Required Standard,"#### The Required Standard
Identifying what constitutes a ""good citizen"" service is key to maintaining system manageability. This involves defining required standards for services to ensure they do not bring down the entire system.

:p What is meant by ""good citizen"" in this context?
??x
A ""good citizen"" service refers to one that adheres to well-defined standards and behaviors, ensuring it does not negatively impact the overall stability and manageability of the system. This includes having necessary capabilities like self-management and isolation from other services.
x??

---",584,"Practices Our practices are how we ensure our principles are being carried out. They are a set of detailed, practical guidance for performing tasks. They will often be technology-specific, and should ...",qwen2.5:latest,2025-11-02 04:29:34,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Tailored Service Template,Monitoring System Health,"#### Monitoring System Health
Background context explaining the importance of monitoring system health. It's essential to have a cohesive view across services rather than just service-specific views for effective system-wide diagnosis and trend analysis.

:p How can we ensure coherent, cross-service views of our system health?
??x
We need to establish standardized metrics and monitoring practices that provide a holistic view of the system. This involves ensuring all services emit health and general monitoring-related metrics in the same way. For instance, you might choose a push mechanism where each service pushes its data into a central location like Graphite for metrics or Nagios for health checks.

```java
// Example of emitting metrics using a simplified model
public class ServiceMetrics {
    public void sendMetric(String metricName, double value) {
        // Code to send the metric to a centralized monitoring system
    }
}
```
x??",952,"And, as with people, what a good citizen is in one context does not reflect what it looks like somewhere else. Nonetheless, there are some common characteristics of well-behaved services that I think ...",qwen2.5:latest,2025-11-02 04:29:55,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Tailored Service Template,Interface Consistency Across Services,"#### Interface Consistency Across Services
Background context explaining the importance of standardizing interfaces across services for better integration and maintainability. Using too many different integration styles can lead to complexity and maintenance issues.

:p How does picking a small number of defined interface technologies benefit microservices?
??x
Picking a few well-defined interface technologies, such as HTTP/REST with consistent conventions (e.g., using verbs or nouns), helps in integrating new consumers more easily. It reduces the learning curve for developers and ensures consistency across services. For instance, if you decide to use HTTP/REST, define clear rules around handling pagination of resources and versioning endpoints.

```java
// Example of defining REST API conventions
public class ResourceHandler {
    public String handleRequest(String verb, String resource) {
        // Code to handle different verbs (GET, POST, PUT, DELETE) on a given resource
        return ""Handling request with method: "" + verb + "" and resource: "" + resource;
    }
}
```
x??",1093,"And, as with people, what a good citizen is in one context does not reflect what it looks like somewhere else. Nonetheless, there are some common characteristics of well-behaved services that I think ...",qwen2.5:latest,2025-11-02 04:29:55,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Tailored Service Template,Architectural Safety in Microservices,"#### Architectural Safety in Microservices
Background context explaining the importance of ensuring services are resilient against failures from downstream calls. This involves using techniques like connection pools, circuit breakers, and proper handling of HTTP response codes.

:p How can we ensure that one badly behaved service doesn't ruin the system's stability?
??x
To maintain system stability, we need to mandate architectural safety measures such as each microservice having its own connection pool and possibly using a circuit breaker pattern. This prevents a single failure from cascading into multiple services. Additionally, response codes should be handled correctly; for example, HTTP 2XX should not be used for errors, and the difference between 4XX and 5XX status codes must be respected.

```java
// Example of implementing a basic circuit breaker pattern
public class CircuitBreaker {
    private boolean isClosed;
    
    public void handleRequest() throws Exception {
        if (isClosed) {
            throw new ServiceUnavaliableException(""Circuit breaker open"");
        } else {
            // Proceed with the service request
        }
    }
}
```
x??",1180,"And, as with people, what a good citizen is in one context does not reflect what it looks like somewhere else. Nonetheless, there are some common characteristics of well-behaved services that I think ...",qwen2.5:latest,2025-11-02 04:29:55,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Tailored Service Template,Importance of Clear Interface Definitions,"#### Importance of Clear Interface Definitions
Background context explaining that picking a few well-defined interface technologies helps in integrating new consumers and maintaining system consistency. The number of different integration styles should be kept to a minimum for simplicity.

:p What is the impact of having multiple integration styles among microservices?
??x
Having too many different integration styles among microservices can lead to increased complexity, making it harder to maintain and integrate with new services. It's better to have one or two well-defined standards that everyone adheres to. For instance, if you choose HTTP/REST as your primary interface technology, define clear conventions around resource handling and versioning.

```java
// Example of defining REST API versioning
public class VersionedResource {
    public String getVersionedUrl(String resourceName) {
        // Return the URL with the correct version in the path or query parameters
        return ""/api/v1/"" + resourceName;
    }
}
```
x??

---",1046,"And, as with people, what a good citizen is in one context does not reflect what it looks like somewhere else. Nonetheless, there are some common characteristics of well-behaved services that I think ...",qwen2.5:latest,2025-11-02 04:29:55,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Tailored Service Template,Exemplars,"#### Exemplars
Exemplars are real-world services that demonstrate best practices and can be used as a reference for developers. The purpose is to encourage others to follow established guidelines without deviating too far.

:p What are exemplars, and why are they useful?
??x
Exemplars are concrete implementations of your system's best practices that serve as practical references for developers. They help ensure that new services adhere to the same standards by providing a clear example of what ""correct"" implementation looks like in practice.

By using real-world services as exemplars, you can build confidence among team members that following these guidelines will lead to robust and maintainable systems. This approach is more compelling than just writing down rules because developers prefer seeing actual code they can run and explore.
x??",850,"If our services play fast and loose with these rules, we end up with a more vulnerable system. Governance Through Code Getting together and agreeing on how things can be done is a good idea. But spend...",qwen2.5:latest,2025-11-02 04:30:16,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Tailored Service Template,Service Templates,"#### Service Templates
Service templates are pre-configured frameworks designed to streamline the implementation of services according to a set of predefined standards or best practices.

:p How do service templates help in implementing guidelines?
??x
Service templates provide a starting point for developers, reducing the initial effort required to implement core functionalities and aligning them with your organization's standards. By using tailored service templates, you can ensure that most of the code necessary for common services is already in place, allowing teams to focus on specific business logic rather than boilerplate code.

For example, if you use Dropwizard or Karyon as a base, these frameworks come with features like health checking and HTTP serving. By tailoring such templates, you can enforce additional requirements such as circuit breakers (e.g., integrating Hystrix) or centralized metrics collection (e.g., using Metrics from Dropwizard).

Here's an example of adding Hystrix to a Dropwizard service template:

```java
public class MyService extends Application<MyConfiguration> {
    private final HystrixDashboard hystrixDashboard;

    @Override
    public void initialize(Configuration config) throws Exception {
        super.initialize(config);
        
        // Initialize Hystrix Dashboard
        hystrixDashboard = new HystrixDashboard(config.getHystrixDashboardPort());
        hystrixDashboard.start();
    }

    @Override
    public String getName() {
        return ""MyService"";
    }

    @Override
    public void run(MyConfiguration configuration, Environment environment) throws Exception {
        // Existing Dropwizard services code...
        
        // Add Hystrix commands and metrics to the health check path
        environment.healthChecks().register(""myServiceHealthCheck"", new MyHealthCheck());
    }
}
```
x??",1874,"If our services play fast and loose with these rules, we end up with a more vulnerable system. Governance Through Code Getting together and agreeing on how things can be done is a good idea. But spend...",qwen2.5:latest,2025-11-02 04:30:16,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Tailored Service Template,Real-World vs. Perfect Examples,"#### Real-World vs. Perfect Examples
Exemplars should be real-world services that have proven successful, rather than artificially perfect examples.

:p Why should exemplars be based on real-world services and not artificial examples?
??x
Exemplars should ideally come from actual production environments where they have been tested and proven effective over time. Artificially perfect examples can give a false sense of security or practicality because they might lack the nuances that arise in real-world scenarios.

Using real-world services as exemplars ensures that developers are exposed to solutions that have faced real challenges and have found ways to overcome them. This approach makes it more likely that new services will be robust and well-optimized for production use, rather than overly simplistic or theoretical.

For instance, if you choose a service template that has been used in multiple projects and has encountered various issues and resolved them, developers can learn from these experiences.
x??",1020,"If our services play fast and loose with these rules, we end up with a more vulnerable system. Governance Through Code Getting together and agreeing on how things can be done is a good idea. But spend...",qwen2.5:latest,2025-11-02 04:30:16,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Tailored Service Template,Technological Constraints,"#### Technological Constraints
Tailoring service templates to specific technologies may subtly constrain language choices among team members.

:p How might tailoring service templates impact the choice of programming languages?
??x
Tailoring service templates to specific technologies like Dropwizard or Karyon for Java projects can influence developers' decisions on which technology stack to use. If these templates support only a particular technology, such as Java with Dropwizard or Karyon, developers may be less likely to choose alternative stacks that require more effort to integrate.

For example, if the in-house service template only supports Java and Dropwizard, and a developer is considering using a different framework like Spring Boot (which might not have an equivalent pre-configured template), they might find it too cumbersome or time-consuming. This can lead to a bias towards sticking with familiar tools, even when there might be better alternatives available.

To mitigate this, you could consider providing multiple templates for different technology stacks and allowing developers some flexibility in choosing the best tool for their task.
x??

---",1175,"If our services play fast and loose with these rules, we end up with a more vulnerable system. Governance Through Code Getting together and agreeing on how things can be done is a good idea. But spend...",qwen2.5:latest,2025-11-02 04:30:16,7
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Tailored Service Template,Fault Tolerance and Service Design at Netflix,"---
#### Fault Tolerance and Service Design at Netflix
Netflix places a high emphasis on fault tolerance, which is crucial to ensure that outages in one part of their system do not bring down everything. This is achieved through robust client libraries for JVMs, which provide essential tools for maintaining service behavior.
:p How does Netflix ensure fault tolerance across its services?
??x
To achieve fault tolerance, Netflix uses client libraries on the JVM to help teams implement reliable services. They also use sidecar services that communicate locally with a JVM using appropriate libraries. This decentralized approach allows for better resilience and easier maintenance compared to a monolithic framework.
```java
// Example of a basic fault-tolerant service implementation
public class FaultTolerantService {
    private ClientLibrary clientLibrary;

    public FaultTolerantService(ClientLibrary clientLibrary) {
        this.clientLibrary = clientLibrary;
    }

    public void handleRequest() {
        try {
            clientLibrary.execute();
        } catch (Exception e) {
            // Log and handle exception
        }
    }
}
```
x??",1161,"Netflix, for example, is especially concerned with aspects like fault tolerance, to ensure that the outage of one part of its system cannot take everything down. To handle this, a large amount of work...",qwen2.5:latest,2025-11-02 04:30:36,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Tailored Service Template,Sidecar Services in Netflix Architecture,"#### Sidecar Services in Netflix Architecture
Netflix employs sidecar services to enhance fault tolerance. These services communicate locally with a JVM using appropriate libraries, ensuring that the failure of one component does not bring down the entire system.
:p What is a sidecar service and how does it work?
??x
A sidecar service acts as an intermediary between the application and the library, providing local communication to ensure fault tolerance. It helps maintain the reliability of services by allowing them to interact with libraries in a resilient manner without directly relying on networked components.
```java
// Example of a sidecar service implementation
public class SidecarService {
    private ClientLibrary clientLibrary;

    public SidecarService(ClientLibrary clientLibrary) {
        this.clientLibrary = clientLibrary;
    }

    public void communicateWithClient() {
        // Local communication with the JVM using appropriate libraries
        clientLibrary.performLocalOperation();
    }
}
```
x??",1032,"Netflix, for example, is especially concerned with aspects like fault tolerance, to ensure that the outage of one part of its system cannot take everything down. To handle this, a large amount of work...",qwen2.5:latest,2025-11-02 04:30:36,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Tailored Service Template,Centralized vs. Distributed Practices in Service Templates,"#### Centralized vs. Distributed Practices in Service Templates
Netflix's approach to service templates is decentralized, promoting collective responsibility among teams. This avoids centralizing control over implementation details, which can otherwise be detrimental to team morale and productivity.
:p How does Netflix handle the use of service templates?
??x
Netflix encourages a distributed approach where each team defines and updates its own service template collaboratively. This decentralization helps maintain team autonomy and reduces the risk of imposing overly strict frameworks that could stifle innovation and developer morale.
```java
// Example of a decentralized service template management
public class TeamServiceTemplate {
    private static void updateTemplate() {
        // Collectively decide on changes to be made to the template
        System.out.println(""Updating team-specific service template..."");
    }
}
```
x??",944,"Netflix, for example, is especially concerned with aspects like fault tolerance, to ensure that the outage of one part of its system cannot take everything down. To handle this, a large amount of work...",qwen2.5:latest,2025-11-02 04:30:36,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Tailored Service Template,Risks and Mitigations in Service Template Management,"#### Risks and Mitigations in Service Template Management
Netflix mitigates risks associated with service templates by ensuring ease of use for developers. The goal is to balance code reuse with the need to avoid overly coupled systems, which can introduce vulnerabilities.
:p What are some key considerations when using a tailored service template?
??x
Key considerations include making the use of the template optional and prioritizing developer ease of use. Centralized frameworks should be designed to be flexible and intuitive, minimizing complexity to prevent overwhelming developers.
```java
// Example of an easy-to-use service template
public class EasyToUseServiceTemplate {
    public void applyTemplate() {
        // Simplified configuration options for developers
        System.out.println(""Applying simplified template..."");
    }
}
```
x??",856,"Netflix, for example, is especially concerned with aspects like fault tolerance, to ensure that the outage of one part of its system cannot take everything down. To handle this, a large amount of work...",qwen2.5:latest,2025-11-02 04:30:36,7
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Tailored Service Template,Coupling Risks in Shared Code,"#### Coupling Risks in Shared Code
The potential for coupling between services increases when shared code is used carelessly. Netflix mitigates this by either manually copying the service template into each service or treating it as a binary dependency, ensuring that each service remains isolated.
:p How does Netflix manage to avoid coupling issues with its service templates?
??x
Netflix avoids coupling by either manually copying the service template code into each service to ensure independence or using shared binaries while preventing developers from enforcing DRY (Don't Repeat Yourself) principles too strictly. This helps maintain isolation and resilience across services.
```java
// Example of manual copy approach for avoiding coupling
public class ManualCopyService {
    public void applyTemplate() {
        // Manually copied template code
        System.out.println(""Manually applied service template..."");
    }
}
```
x??

---",945,"Netflix, for example, is especially concerned with aspects like fault tolerance, to ensure that the outage of one part of its system cannot take everything down. To handle this, a large amount of work...",qwen2.5:latest,2025-11-02 04:30:36,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Governance and Leading from the Center,Technical Debt,"#### Technical Debt
Background context: Technical debt is a term used to describe shortcuts or compromises made during software development that can impact the system's long-term maintainability and stability. The concept is based on the analogy of financial debt, where immediate gains are sacrificed for future costs.

:p What is technical debt in software development?
??x
Technical debt refers to the shortcuts or compromises taken during software development that can lead to increased maintenance costs and decreased system quality over time. Just like financial debt, it incurs ongoing costs and should be managed and paid down.
x??",639,"Technical Debt We are often put in situations where we cannot follow through to the letter on our technical vision. Often, we need to make a choice to cut a few corners to get some urgent features out...",qwen2.5:latest,2025-11-02 04:30:56,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Governance and Leading from the Center,Exception Handling,"#### Exception Handling
Background context: Exceptions are a common occurrence in software development where unexpected situations arise that require special handling. Effective exception handling ensures the robustness of the application by dealing with these errors gracefully.

:p What is an example of when to handle exceptions?
??x
An example is deciding whether to use MySQL for most storage requirements and Cassandra for highly scalable storage based on expected growth volumes. If enough such decisions are made, it may eventually make sense to change the principle or practice.
x??",591,"Technical Debt We are often put in situations where we cannot follow through to the letter on our technical vision. Often, we need to make a choice to cut a few corners to get some urgent features out...",qwen2.5:latest,2025-11-02 04:30:56,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Governance and Leading from the Center,Governance and Leading from the Center,"#### Governance and Leading from the Center
Background context: Governance in software development ensures that enterprise objectives are met by setting direction, prioritizing tasks, making decisions, and monitoring performance against agreed-upon goals. Architects play a crucial role in ensuring technical vision alignment.

:p What is governance in the context of IT?
??x
Governance in IT ensures that enterprise objectives are achieved through evaluating stakeholder needs, setting direction via prioritization and decision-making, and monitoring performance, compliance, and progress against agreed-on directions and objectives.
x??",638,"Technical Debt We are often put in situations where we cannot follow through to the letter on our technical vision. Often, we need to make a choice to cut a few corners to get some urgent features out...",qwen2.5:latest,2025-11-02 04:30:56,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Governance and Leading from the Center,Architect's Responsibilities,"#### Architect's Responsibilities
Background context: Architects have a wide range of responsibilities including ensuring technical vision, guiding development principles, managing trade-offs, and leading teams. They must balance technical innovation with practical constraints.

:p What are the main roles of an architect?
??x
The main roles of an architect include ensuring there is a set of guiding principles for development, matching these principles to organizational strategy, avoiding practices that make developers miserable, staying updated on new technologies, making appropriate trade-offs, and carrying people along with them in their decisions.
x??",662,"Technical Debt We are often put in situations where we cannot follow through to the letter on our technical vision. Often, we need to make a choice to cut a few corners to get some urgent features out...",qwen2.5:latest,2025-11-02 04:30:56,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Governance and Leading from the Center,Decision-Making in Governance Groups,"#### Decision-Making in Governance Groups
Background context: Governance groups help distribute the burden of decision-making and ensure broader buy-in. Architects must lead these groups effectively while allowing teams to contribute.

:p How does an architect handle governance group decisions?
??x
An architect should generally go with the group's decision, understanding that a collective decision is often wiser than individual judgment. However, sometimes architects may overrule the group when necessary, such as when a critical issue (like veering into traffic or a duck pond) arises.
x??

---",600,"Technical Debt We are often put in situations where we cannot follow through to the letter on our technical vision. Often, we need to make a choice to cut a few corners to get some urgent features out...",qwen2.5:latest,2025-11-02 04:30:56,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Shared and Hidden Models,Vision and Technical Leadership,"#### Vision and Technical Leadership
Technical leadership involves not only making technology decisions but also ensuring that team members understand and contribute to the technical vision. This includes helping them grow professionally by understanding the vision and taking ownership of parts of the system.
:p What is a key responsibility of the technical leader in terms of the team's professional development?
??x
A key responsibility is helping team members understand and own parts of the technical vision. By providing opportunities for individuals to take ownership of specific services, leaders can help their teams grow professionally while also distributing the workload more effectively.
```java
public class OwnershipExample {
    public void assignServiceOwner(String service) {
        // Logic to assign a specific microservice to a team member
        System.out.println(""Assigning "" + service + "" to Team Member X"");
    }
}
```
x??",952,Building a Team Being the main point person responsible for the technical vision of your system and ensuring that you’re executing on this vision isn’t just about making technology decisions. It’s the...,qwen2.5:latest,2025-11-02 04:31:20,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Shared and Hidden Models,Empathy in Decision-Making,"#### Empathy in Decision-Making
Empathy is crucial for a technical leader as it involves understanding the impact of decisions on customers and colleagues. This ensures that choices are made with broader stakeholder considerations in mind.
:p Why is empathy important for a technical leader?
??x
Empathy is essential because it helps leaders make decisions that not only meet technological requirements but also consider the real-world impacts on users, stakeholders, and team members. It fosters better communication and collaboration, leading to more holistic and successful projects.
```java
public class EmpathyExample {
    public void demonstrateEmpathy(String impact) {
        // Code to simulate understanding an impact statement
        System.out.println(""Understanding that the decision will "" + impact);
    }
}
```
x??",832,Building a Team Being the main point person responsible for the technical vision of your system and ensuring that you’re executing on this vision isn’t just about making technology decisions. It’s the...,qwen2.5:latest,2025-11-02 04:31:20,2
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Shared and Hidden Models,Collaboration and Peer Engagement,"#### Collaboration and Peer Engagement
Collaboration involves actively engaging with peers and colleagues to define, refine, and execute the technical vision. This ensures a shared understanding and commitment across the team.
:p How does collaboration support the technical vision?
??x
Collaboration supports the technical vision by ensuring that all stakeholders are involved in the planning and decision-making process. This leads to better alignment and buy-in from the entire team, making it more likely that the vision will be successfully implemented and maintained.
```java
public class CollaborationExample {
    public void engagePeers(String issue) {
        // Code to simulate engaging peers on an issue
        System.out.println(""Discussing "" + issue + "" with our peer team"");
    }
}
```
x??",807,Building a Team Being the main point person responsible for the technical vision of your system and ensuring that you’re executing on this vision isn’t just about making technology decisions. It’s the...,qwen2.5:latest,2025-11-02 04:31:20,7
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Shared and Hidden Models,Adaptability in the Technical Vision,"#### Adaptability in the Technical Vision
Adaptability is about being flexible and responsive to changes in customer or organizational requirements. This ensures that the technical vision remains relevant over time.
:p What does adaptability mean for a technical leader?
??x
Adaptability means that a technical leader must be willing to change the technical vision as circumstances evolve, whether due to new business needs, technological advancements, or changing user expectations. Flexibility is key to ensuring long-term success and relevance of the project.
```java
public class AdaptabilityExample {
    public void adjustVision(String requirement) {
        // Code to simulate adjusting the technical vision based on a new requirement
        System.out.println(""Adapting our vision to include "" + requirement);
    }
}
```
x??",835,Building a Team Being the main point person responsible for the technical vision of your system and ensuring that you’re executing on this vision isn’t just about making technology decisions. It’s the...,qwen2.5:latest,2025-11-02 04:31:20,7
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Shared and Hidden Models,Autonomy for Teams,"#### Autonomy for Teams
Autonomy involves finding the right balance between standardizing processes and enabling team members to make decisions about their specific parts of the system. This empowers teams but also requires clear guidelines.
:p How does autonomy benefit a microservices architecture?
??x
Autonomy benefits a microservices architecture by allowing different services to be developed, maintained, and scaled independently. However, it's crucial to establish standards or governance frameworks to ensure consistency across services while empowering teams to innovate and adapt as needed.
```java
public class AutonomyExample {
    public void defineStandards(String standard) {
        // Code to simulate defining a standard for microservices
        System.out.println(""Defining "" + standard + "" for our microservices"");
    }
}
```
x??",852,Building a Team Being the main point person responsible for the technical vision of your system and ensuring that you’re executing on this vision isn’t just about making technology decisions. It’s the...,qwen2.5:latest,2025-11-02 04:31:20,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Shared and Hidden Models,Governance in Microservices,"#### Governance in Microservices
Governance ensures that the implementation of the system aligns with the overall technical vision. This includes setting standards, policies, and procedures to maintain consistency and quality across services.
:p What is governance in the context of microservices?
??x
Governance in the context of microservices involves establishing a framework for managing the development, deployment, and maintenance of individual services while ensuring they collectively contribute to the overall technical vision. This includes setting standards for architecture, security, performance, and other critical aspects.
```java
public class GovernanceExample {
    public void enforceGovernance(String policy) {
        // Code to simulate enforcing a governance policy on microservices
        System.out.println(""Enforcing "" + policy + "" across all services"");
    }
}
```
x??

---",901,Building a Team Being the main point person responsible for the technical vision of your system and ensuring that you’re executing on this vision isn’t just about making technology decisions. It’s the...,qwen2.5:latest,2025-11-02 04:31:20,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Shared and Hidden Models,Concept of Microservices and MusicCorp Background,"#### Concept of Microservices and MusicCorp Background
Background context: The provided text introduces MusicCorp, a brick-and-mortar retailer that has shifted its focus to online sales. It is described as being slightly behind current trends but with ambitious plans for growth through microservices.
:p What background information is given about MusicCorp?
??x
MusicCorp was originally a physical retail store but has pivoted towards an online business model due to the decline of gramophone records. Despite being somewhat late in adapting to digital trends, such as online music streaming (Spotify), the company aims to leverage microservices to enable rapid and easy changes.
x??",684,"You’re probably eager now to go and start making them, right? But where to start? In this chapter, we’ll look at how to think about the boundaries of your microservices that will hopefully maximize th...",qwen2.5:latest,2025-11-02 04:31:44,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Shared and Hidden Models,What Makes a Good Service,"#### What Makes a Good Service
Background context: The text emphasizes the importance of loose coupling and high cohesion when designing microservices for MusicCorp. These principles are crucial in ensuring that changes to one service do not necessitate altering other parts of the system.
:p What two key concepts should be considered when defining good services?
??x
The two key concepts are **loose coupling** and **high cohesion**. Loose coupling means a change in one service should not require modifications in another, while high cohesion ensures that each service is tightly integrated with its own internal components but loosely coupled with other services.
x??",671,"You’re probably eager now to go and start making them, right? But where to start? In this chapter, we’ll look at how to think about the boundaries of your microservices that will hopefully maximize th...",qwen2.5:latest,2025-11-02 04:31:44,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Shared and Hidden Models,Loose Coupling,"#### Loose Coupling
Background context: Loose coupling is critical for microservices to operate independently without affecting each other when changes are made. Tight coupling can hinder the agility of deploying new features or fixing bugs in one service, impacting others.
:p What does loose coupling mean in the context of microservices?
??x
Loose coupling means a change in one service should not require modifications in another. In microservices architecture, this is essential to ensure that any changes made to a single service can be deployed independently without affecting other services.
x??",603,"You’re probably eager now to go and start making them, right? But where to start? In this chapter, we’ll look at how to think about the boundaries of your microservices that will hopefully maximize th...",qwen2.5:latest,2025-11-02 04:31:44,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Shared and Hidden Models,Avoiding Tight Coupling,"#### Avoiding Tight Coupling
Background context: The text warns against common mistakes like picking integration styles that tightly bind services, leading to a situation where changes inside one service require corresponding changes in its consumers. This tight coupling is detrimental to the agility of microservices.
:p What are some causes of tight coupling between services?
??x
Tight coupling can be caused by choosing an integration style that binds one service too closely to another. For example, if Service A frequently makes calls to Service B and those calls are tightly integrated, changes in Service A could necessitate updates in Service B.
x??",659,"You’re probably eager now to go and start making them, right? But where to start? In this chapter, we’ll look at how to think about the boundaries of your microservices that will hopefully maximize th...",qwen2.5:latest,2025-11-02 04:31:44,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Shared and Hidden Models,Limiting Chatty Communication,"#### Limiting Chatty Communication
Background context: Excessive communication between services can lead to tight coupling. The text suggests limiting the number of different types of calls from one service to another to maintain loose coupling and avoid performance issues.
:p How does excessive chatty communication between services impact microservices?
??x
Excessive communication, often referred to as ""chatty"" communication, can lead to tight coupling and make it harder to change or deploy individual services. It also might introduce performance bottlenecks because of the increased number of calls.
x??",611,"You’re probably eager now to go and start making them, right? But where to start? In this chapter, we’ll look at how to think about the boundaries of your microservices that will hopefully maximize th...",qwen2.5:latest,2025-11-02 04:31:44,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Shared and Hidden Models,Example of Chatty Communication in Code,"#### Example of Chatty Communication in Code
Background context: The text suggests that limiting the types of calls between services helps maintain loose coupling. Here, an example is provided to illustrate a chatty communication pattern.
:p Provide an example of a chatty communication pattern between two microservices.
??x
```java
// Poorly designed service A that makes many small and frequent requests to Service B
public class ServiceA {
    private final RestTemplate restTemplate;

    public ServiceA(RestTemplate restTemplate) {
        this.restTemplate = restTemplate;
    }

    public void processRequest() {
        for (int i = 0; i < 1000; i++) { // Simulating 1000 small requests
            String response = restTemplate.getForObject(""http://serviceB/api/data"", String.class);
            // Process the data in some way...
        }
    }
}
```
x??",869,"You’re probably eager now to go and start making them, right? But where to start? In this chapter, we’ll look at how to think about the boundaries of your microservices that will hopefully maximize th...",qwen2.5:latest,2025-11-02 04:31:44,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Shared and Hidden Models,Conclusion of Flashcards for Microservices Introduction,"#### Conclusion of Flashcards for Microservices Introduction
These flashcards cover key concepts around defining and designing microservices, focusing on loose coupling and high cohesion. Understanding these principles is crucial for implementing a successful microservices architecture at MusicCorp.
---",304,"You’re probably eager now to go and start making them, right? But where to start? In this chapter, we’ll look at how to think about the boundaries of your microservices that will hopefully maximize th...",qwen2.5:latest,2025-11-02 04:31:44,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Shared and Hidden Models,High Cohesion,"#### High Cohesion
Background context explaining the importance of high cohesion. This principle emphasizes that related behavior should be grouped together to facilitate easier and quicker changes, reducing deployment risks.
:p Why is it important to have high cohesion in a system?
??x
High cohesion ensures that related behaviors are grouped in one place, making changes faster and minimizing deployment risks by limiting the scope of affected components. By grouping related functionality, you reduce the complexity of individual modules, making them more manageable and easier to test.
x??",594,"High Cohesion We want related behavior to sit together, and unrelated behavior to sit elsewhere. Why? Well, if we want to change behavior, we want to be able to change it in one place, and release tha...",qwen2.5:latest,2025-11-02 04:32:11,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Shared and Hidden Models,Bounded Context (Concept),"#### Bounded Context (Concept)
Background context explaining bounded contexts as introduced by Eric Evans in Domain-Driven Design. The concept helps define boundaries within a domain where models are specific to that context, ensuring loose coupling between different parts of a system.
:p What is the concept of a bounded context?
??x
A bounded context is a specific responsibility enforced by explicit boundaries, encapsulating related behaviors and data models. Each bounded context has its own language and logic, but can communicate with other contexts through well-defined interfaces.
x??",594,"High Cohesion We want related behavior to sit together, and unrelated behavior to sit elsewhere. Why? Well, if we want to change behavior, we want to be able to change it in one place, and release tha...",qwen2.5:latest,2025-11-02 04:32:11,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Shared and Hidden Models,Explicit Interfaces in Bounded Contexts,"#### Explicit Interfaces in Bounded Contexts
Background context explaining the importance of explicit interfaces between bounded contexts. These interfaces ensure that communication is clear and controlled, reducing complexity and potential errors.
:p How do bounded contexts communicate with each other?
??x
Bounded contexts communicate via explicit interfaces where they define which models can be shared externally. Communication occurs through these defined boundaries using well-structured models to avoid sharing unnecessary details or complex logic.
x??",560,"High Cohesion We want related behavior to sit together, and unrelated behavior to sit elsewhere. Why? Well, if we want to change behavior, we want to be able to change it in one place, and release tha...",qwen2.5:latest,2025-11-02 04:32:11,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Shared and Hidden Models,Cell Analogy for Bounded Contexts,"#### Cell Analogy for Bounded Contexts
Background context explaining the cell analogy, where bounded contexts are like cells with membranes defining internal and external interactions.
:p What is the cell analogy in domain-driven design?
??x
The cell analogy likens a bounded context to a biological cell. Just as a cell's membrane defines what is inside and outside, separating content and processes, bounded contexts separate parts of a system by their specific responsibilities and shared interfaces.
x??",507,"High Cohesion We want related behavior to sit together, and unrelated behavior to sit elsewhere. Why? Well, if we want to change behavior, we want to be able to change it in one place, and release tha...",qwen2.5:latest,2025-11-02 04:32:11,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Shared and Hidden Models,Finance Department Bounded Context Example,"#### Finance Department Bounded Context Example
Background context explaining how the finance department can be considered a bounded context within MusicCorp. This example illustrates the concept with practical details.
:p How does the finance department exemplify a bounded context?
??x
The finance department at MusicCorp is a clear example of a bounded context because it has specific responsibilities like payroll, accounting, and report generation. These tasks are encapsulated within this context, separate from other parts of the organization such as the warehouse, yet still interact with them through defined interfaces.
x??",633,"High Cohesion We want related behavior to sit together, and unrelated behavior to sit elsewhere. Why? Well, if we want to change behavior, we want to be able to change it in one place, and release tha...",qwen2.5:latest,2025-11-02 04:32:11,4
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Shared and Hidden Models,Warehouse Bounded Context Example,"#### Warehouse Bounded Context Example
Background context explaining how the warehouse can be considered a bounded context within MusicCorp. This example illustrates the concept with practical details.
:p How does the warehouse exemplify a bounded context?
??x
The warehouse at MusicCorp is another clear bounded context, managing tasks like order management, stock handling, and forklift operations. These activities are encapsulated within this context and interact with other parts of the business through well-defined interfaces.
x??",537,"High Cohesion We want related behavior to sit together, and unrelated behavior to sit elsewhere. Why? Well, if we want to change behavior, we want to be able to change it in one place, and release tha...",qwen2.5:latest,2025-11-02 04:32:11,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Shared and Hidden Models,Shared vs Hidden Models in Bounded Contexts,"#### Shared vs Hidden Models in Bounded Contexts
Background context explaining shared models (visible to external contexts) versus hidden models (internal to a bounded context).
:p What is the difference between shared and hidden models in a bounded context?
??x
Shared models are those that are communicated externally, while hidden models remain internal. For example, payroll data might be shared with other finance-related bounded contexts, but forklift truck details would likely stay within the warehouse context.
x??",523,"High Cohesion We want related behavior to sit together, and unrelated behavior to sit elsewhere. Why? Well, if we want to change behavior, we want to be able to change it in one place, and release tha...",qwen2.5:latest,2025-11-02 04:32:11,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Shared and Hidden Models,Importance of Context Boundaries,"#### Importance of Context Boundaries
Background context explaining why defining clear boundaries is crucial for managing complexity and ensuring modularity in software systems.
:p Why are context boundaries important?
??x
Defining clear boundaries helps manage complexity by isolating parts of a system that can be developed, tested, and deployed independently. This isolation reduces the risk of unintended side effects and makes it easier to make changes without affecting unrelated parts of the system.
x??

---",515,"High Cohesion We want related behavior to sit together, and unrelated behavior to sit elsewhere. Why? Well, if we want to change behavior, we want to be able to change it in one place, and release tha...",qwen2.5:latest,2025-11-02 04:32:11,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Shared and Hidden Models,Shared Model Between Contexts,"#### Shared Model Between Contexts
Background context explaining that different departments (finance and warehouse) need to share some information but not all. The finance department needs stock level information for accurate accounting, while internal details like storage locations remain hidden.

:p What is a shared model between the finance department and the warehouse?
??x
A shared model refers to information that both contexts need but are only exposed in a simplified form suitable for the external context. For example, the finance department needs to know stock levels, but it does not need to know about internal storage locations or specific processes like picking orders.

```java
public class StockItem {
    private int quantity; // Exposed quantity for finance
    private String warehouseLocation; // Not exposed to finance

    public void updateQuantity(int newQuantity) {
        this.quantity = newQuantity;
    }
}
```
x??",946,"Now the finance department doesn’t need to know about the detailed inner workings of the warehouse. It does need to know some things, though — for example it needs to know about stock levels to keep t...",qwen2.5:latest,2025-11-02 04:32:38,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Shared and Hidden Models,Internal vs. External Representations,"#### Internal vs. External Representations
Explanation that internal representations and external representations can differ significantly, with the latter being simplified or tailored for external use.

:p How do internal and external representations of stock items differ?
??x
Internal representations include detailed information such as where a stock item is stored within the warehouse (e.g., `warehouseLocation`). In contrast, the external representation shared with finance only includes basic details like current quantity (`quantity`).

```java
public class StockItemInternal {
    private int quantity; // For internal use
    private String warehouseLocation; // For internal use

    public void updateQuantity(int newQuantity) {
        this.quantity = newQuantity;
    }
}

public class StockItemExternal {
    private int quantity; // Exposed to external contexts like finance

    public void updateQuantity(int newQuantity) {
        this.quantity = newQuantity;
    }
}
```
x??",995,"Now the finance department doesn’t need to know about the detailed inner workings of the warehouse. It does need to know some things, though — for example it needs to know about stock levels to keep t...",qwen2.5:latest,2025-11-02 04:32:38,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Shared and Hidden Models,Contextual Variance in Concepts,"#### Contextual Variance in Concepts
Explanation that the same term can have different meanings across various contexts, such as ""return"" in customer and warehouse contexts.

:p How does the concept of a return differ between the customer and warehouse contexts?
??x
In the customer context, a return involves processes like printing shipping labels, dispatching packages, and waiting for refunds. In contrast, within the warehouse, a return signifies incoming stock that needs to be processed, which often triggers additional tasks such as generating restock requests.

```java
public class ReturnCustomer {
    public void printShippingLabel() { /* code */ }
    public void dispatchPackage() { /* code */ }
}

public class ReturnWarehouse {
    public void generateRestockRequest() { /* code */ }
}
```
x??",809,"Now the finance department doesn’t need to know about the detailed inner workings of the warehouse. It does need to know some things, though — for example it needs to know about stock levels to keep t...",qwen2.5:latest,2025-11-02 04:32:38,7
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Shared and Hidden Models,Internal and External Concerns,"#### Internal and External Concerns
Explanation that internal concerns within a context are separate from the shared models used by external contexts.

:p What does it mean when we say ""internal only representation"" in the context of stock items?
??x
The internal only representation refers to detailed information about how stock items are managed within the warehouse, such as specific storage locations or detailed picking processes. This is not exposed to external systems like finance but remains an internal concern.

```java
public class InternalStockItem {
    private int quantity;
    private String warehouseLocation;

    public void updateQuantity(int newQuantity) {
        this.quantity = newQuantity;
    }
}
```
x??

---",737,"Now the finance department doesn’t need to know about the detailed inner workings of the warehouse. It does need to know some things, though — for example it needs to know about stock levels to keep t...",qwen2.5:latest,2025-11-02 04:32:38,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Turtles All the Way Down,Modularity and Bounded Contexts,"---
#### Modularity and Bounded Contexts
In domain-driven design, it is crucial to define clear boundaries within a domain through bounded contexts. This approach helps in reducing tight coupling between different parts of the system while ensuring high cohesion within each context. By identifying these bounded contexts early on and modeling them as modules or services, we can later leverage them for microservices.

:p What are bounded contexts and why are they important?
??x
Bounded contexts define a domain model's boundaries within which a set of models (entities, value objects) is consistent and unambiguous. They help in reducing tight coupling between different parts of the system by clearly defining where one context ends and another begins. This approach ensures that each context can be developed independently with its own specialized language.

Code examples:
```java
public class InventoryContext {
    // Models related to inventory management
}

public class OrderFulfillmentContext {
    // Models related to order fulfillment processes
}
```
x??",1069,"Modules and Services By thinking clearly about what models should be shared, and not sharing our internal representations, we avoid one of the potential pitfalls that can result in tight coupling (the...",qwen2.5:latest,2025-11-02 04:32:59,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Turtles All the Way Down,Service Boundaries and Bounded Contexts Alignment,"#### Service Boundaries and Bounded Contexts Alignment
Once the bounded contexts are identified, these should be modeled as separate modules or services. This alignment ensures that each microservice represents a specific business capability within its bounded context, leading to loose coupling and strong cohesion.

:p How do service boundaries align with bounded contexts?
??x
Service boundaries should be aligned with bounded contexts to ensure that the system is organized around business capabilities rather than technical components. Each microservice should represent a bounded context, encapsulating all related models, services, and data access logic within it.

Code examples:
```java
public class SnapCiService {
    // Models, services, and repositories for SnapCI service
}

public class GoCdService {
    // Models, services, and repositories for GoCd service
}
```
x??",884,"Modules and Services By thinking clearly about what models should be shared, and not sharing our internal representations, we avoid one of the potential pitfalls that can result in tight coupling (the...",qwen2.5:latest,2025-11-02 04:32:59,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Turtles All the Way Down,Premature Decomposition,"#### Premature Decomposition
Premature decomposition of a system into microservices can lead to increased costs and maintenance challenges. It is often better to develop the initial system as a monolith and understand its boundaries more thoroughly before moving towards microservices.

:p What are the risks associated with premature decompositions?
??x
Prematurely splitting a system into microservices without thorough understanding of the domain and service boundaries can result in high costs due to frequent changes across services. It may also lead to over-engineering, as initial assumptions about boundaries might not hold up under real-world usage.

Code examples:
```java
public class MonolithicApplication {
    // Initial monolithic implementation before decomposition
}
```
x??",791,"Modules and Services By thinking clearly about what models should be shared, and not sharing our internal representations, we avoid one of the potential pitfalls that can result in tight coupling (the...",qwen2.5:latest,2025-11-02 04:32:59,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Turtles All the Way Down,Business Capabilities vs Data Models,"#### Business Capabilities vs Data Models
When identifying bounded contexts, focus on the business capabilities they provide rather than just data models. This approach ensures that services expose meaningful operations (like getting a current stock list or setting up payroll) rather than simple CRUD operations.

:p How should we think about bounded contexts in terms of business capabilities?
??x
Bounded contexts should be identified based on the business capabilities they provide, such as getting a current stock list from the warehouse context or managing payroll for finance. This approach ensures that services are designed to support real-world use cases rather than just abstracting data.

Code examples:
```java
public class WarehouseContext {
    public List<StockItem> getCurrentStockList() {
        // Logic to get current stock list
    }
}

public class FinanceContext {
    public void setupPayrollForNewRecruit(String recruit) {
        // Logic to set up payroll for a new recruit
    }
}
```
x??",1017,"Modules and Services By thinking clearly about what models should be shared, and not sharing our internal representations, we avoid one of the potential pitfalls that can result in tight coupling (the...",qwen2.5:latest,2025-11-02 04:32:59,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Turtles All the Way Down,Nested Bounded Contexts,"#### Nested Bounded Contexts
Bounded contexts can contain further bounded contexts, allowing for nested decomposition. This approach helps in managing complexity by breaking down larger contexts into smaller, more manageable units.

:p How do you handle nested bounded contexts?
??x
Nested bounded contexts allow finer-grained management of domain models and services within a larger context. For example, a warehouse might be decomposed into order fulfillment, inventory management, and goods receiving. These sub-contexts can be represented as separate microservices if they are managed by different teams.

Code examples:
```java
public class WarehouseContext {
    public OrderFulfillmentService getFulfillmentService() {
        return new OrderFulfillmentService();
    }

    public InventoryManagementService getInventoryService() {
        return new InventoryManagementService();
    }
}

public class OrderFulfillmentService {
    // Logic related to order fulfillment
}
```
x??

---",994,"Modules and Services By thinking clearly about what models should be shared, and not sharing our internal representations, we avoid one of the potential pitfalls that can result in tight coupling (the...",qwen2.5:latest,2025-11-02 04:32:59,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Technical Boundary,Bounded Contexts and Microservices,"#### Bounded Contexts and Microservices
Background context: The changes to a system are often driven by business needs. When decomposing systems into microservices, these services should be aligned with bounded contexts—areas within the domain that have their own language, rules, and data. This approach helps in isolating changes to specific microservices, reducing complexity.
:p How does aligning microservices with bounded contexts help in managing system changes?
??x
Aligning microservices with bounded contexts ensures that each service handles a specific part of the business logic, making it easier to isolate and manage changes. This alignment also reflects real-world business terms within the interfaces, enhancing communication clarity among teams.

For example:
- If you have an e-commerce domain, one bounded context might be ""Orders,"" while another could be ""Inventory.""
```java
public interface OrderService {
    // Methods related to orders
}

public interface InventoryService {
    // Methods related to inventory
}
```
x??",1045,Communication in Terms of Business Concepts The changes we implement to our system are often about changes the business wants to make to how the system behaves. We are changing functionality — capabil...,qwen2.5:latest,2025-11-02 04:33:20,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Technical Boundary,Communication Through Business Concepts,"#### Communication Through Business Concepts
Background context: Communicating between microservices should mimic the language and concepts used within the business. This approach ensures that teams understand each other better, reducing misunderstandings.
:p Why is it important for interfaces to reflect the same terms and ideas shared in an organization?
??x
It's crucial because it aligns technical communication with real-world business processes, making collaboration smoother and more effective. Using consistent terminology helps avoid misinterpretations and ensures that developers are working towards the same goals as the business.

For example:
- A form for a customer registration might be represented in both services using the term ""CustomerProfile.""
```java
public class CustomerRegistrationForm {
    private String name;
    private String email;
    
    // Constructor, getters, setters
}

public interface RegistrationService {
    void processRegistration(CustomerRegistrationForm form);
}
```
x??",1019,Communication in Terms of Business Concepts The changes we implement to our system are often about changes the business wants to make to how the system behaves. We are changing functionality — capabil...,qwen2.5:latest,2025-11-02 04:33:20,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Technical Boundary,Technical Boundary Issues,"#### Technical Boundary Issues
Background context: The example provided discusses a situation where services were modeled incorrectly along technical boundaries rather than business ones. This led to performance and maintenance issues.
:p What are the consequences of modeling service boundaries incorrectly?
??x
Incorrectly modeling service boundaries can lead to several problems, such as increased brittleness, performance overhead, and difficulty in maintaining code. For instance, making method calls overly frequent or complex can degrade system performance.

For example:
- Imagine a scenario where a customer profile update triggers multiple RPC calls.
```java
public class CustomerProfileService {
    public void updateCustomerProfile(String customerId, String newName) {
        // Update customer name
        customerRepository.updateName(customerId, newName);
        
        // Notify billing service (unnecessary if not strictly required)
        billingService.notifyUpdate();
        
        // Send email to user (if email is part of the profile)
        emailService.sendEmailToCustomer(customerId);
    }
}
```
x??",1137,Communication in Terms of Business Concepts The changes we implement to our system are often about changes the business wants to make to how the system behaves. We are changing functionality — capabil...,qwen2.5:latest,2025-11-02 04:33:20,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Technical Boundary,Onion Architecture,"#### Onion Architecture
Background context: The term ""onion architecture"" was used to describe a system with multiple layers, each representing different concerns. This structure can be difficult to maintain and modify.
:p What does the ""onion architecture"" refer to in this context?
??x
The ""onion architecture"" refers to a layered system where services have many layers (like an onion), making it complex to navigate and modify. Each layer represents a specific concern, such as data access or business logic.

For example:
- A typical layered structure might look like this: Presentation -> Application -> Domain -> Infrastructure.
```java
public class OrderController {
    private final OrderService orderService;
    
    public void placeOrder(OrderRequest request) {
        // Validate and process the request
        orderService.placeOrder(request);
    }
}
```
x??

---",881,Communication in Terms of Business Concepts The changes we implement to our system are often about changes the business wants to make to how the system behaves. We are changing functionality — capabil...,qwen2.5:latest,2025-11-02 04:33:20,7
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Shared Database,Bounded Contexts and Microservices,"#### Bounded Contexts and Microservices
Background context: This concept explains how bounded contexts are crucial in identifying seams within a problem space, allowing for microservices that maintain high cohesion and loose coupling. These boundaries help ensure that changes in one part of the system do not adversely affect another.
:p What is a bounded context?
??x
A bounded context refers to a specific domain or subject area where certain rules and definitions apply. In software development, it helps identify natural divisions within a complex application, enabling teams to focus on particular aspects without worrying about implementation details in other parts of the system. This leads to better maintainability and scalability.
x??",745,"Summary In this chapter, you’ve learned a bit about what makes a good service, and how to find seams in our problem space that give us the dual benefits of both loose coupling and high cohesion. Bound...",qwen2.5:latest,2025-11-02 04:33:42,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Shared Database,MusicCorp Example Domain,"#### MusicCorp Example Domain
Background context: The example domain, MusicCorp, is used throughout the book to illustrate various concepts related to microservices architecture. It helps readers understand practical applications of theoretical ideas.
:p What is the purpose of using the MusicCorp example?
??x
The purpose of using the MusicCorp example is to provide a real-world context for explaining complex architectural and design principles. By using this domain, readers can better grasp how different components interact and how bounded contexts can be applied in practice.
x??",586,"Summary In this chapter, you’ve learned a bit about what makes a good service, and how to find seams in our problem space that give us the dual benefits of both loose coupling and high cohesion. Bound...",qwen2.5:latest,2025-11-02 04:33:42,7
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Shared Database,Domain-Driven Design (DDD),"#### Domain-Driven Design (DDD)
Background context: DDD provides useful tools for finding sensible boundaries between microservices by focusing on the core domain logic and entities. The ideas from DDD help ensure that services remain autonomous, making it easier to change and release them independently.
:p What is the role of DDD in identifying service boundaries?
??x
Domain-Driven Design (DDD) helps identify natural boundaries within a complex system through concepts like bounded contexts. By focusing on core domain logic, entities, and value objects, DDD ensures that services are aligned with business needs rather than technical considerations.
x??",659,"Summary In this chapter, you’ve learned a bit about what makes a good service, and how to find seams in our problem space that give us the dual benefits of both loose coupling and high cohesion. Bound...",qwen2.5:latest,2025-11-02 04:33:42,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Shared Database,Ideal Integration Technology Considerations,"#### Ideal Integration Technology Considerations
Background context: Choosing the right technology for microservice communication is crucial to maintain autonomy and avoid breaking changes or tangled systems. Factors like avoiding breaking changes and ensuring API agnosticism are key in selecting an integration technology.
:p What are some important factors when choosing an integration technology?
??x
When choosing an integration technology, it's important to consider:
1. Avoiding Breaking Changes: Ensure that changes do not impact existing consumers.
2. Technology-Agnostic APIs: Design APIs that can be easily adapted if the underlying technology stack changes in the future.
3. Flexibility and Adaptability: Choose a technology that supports evolving requirements over time.
x??",787,"Summary In this chapter, you’ve learned a bit about what makes a good service, and how to find seams in our problem space that give us the dual benefits of both loose coupling and high cohesion. Bound...",qwen2.5:latest,2025-11-02 04:33:42,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Shared Database,Integration Technologies,"#### Integration Technologies
Background context: There are various options for integrating microservices, including SOAP, XML-RPC, REST, and Protocol Buffers. Each has its strengths and weaknesses, but it's crucial to choose one that aligns with the goals of minimizing breaking changes and ensuring API agnosticism.
:p What are some common integration technologies?
??x
Some common integration technologies include:
1. **SOAP**: A standard protocol for exchanging structured data in the implementation of web services in computer networks.
2. **XML-RPC**: An XML-based remote procedure call protocol, but it's less flexible compared to REST.
3. **REST**: Representational State Transfer, a set of constraints and architectural principles for designing networked applications.
4. **Protocol Buffers**: A binary serialization format that allows efficient data transmission over the wire.

Each has its own use cases and trade-offs, so the choice depends on specific requirements and context.
x??",995,"Summary In this chapter, you’ve learned a bit about what makes a good service, and how to find seams in our problem space that give us the dual benefits of both loose coupling and high cohesion. Bound...",qwen2.5:latest,2025-11-02 04:33:42,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Shared Database,Avoiding Breaking Changes,"#### Avoiding Breaking Changes
Background context: When making changes to microservices, it's crucial to ensure that consumers are not adversely affected. Techniques like introducing new fields without breaking existing consumers can help maintain system stability.
:p How do you ensure your microservice changes don't break existing consumers?
??x
To avoid breaking changes:
1. **Versioning APIs**: Use API versioning (e.g., /v1/users, /v2/users) to allow both old and new versions of an API to coexist temporarily.
2. **Backward Compatibility**: Ensure that adding new fields or features does not impact existing consumers by keeping the old logic intact for backward compatibility.
3. **Graceful Degradation**: Design APIs in a way that they can handle optional parameters, allowing older systems to continue functioning without issues.

By implementing these strategies, you can ensure that changes are made without breaking existing integrations.
x??",955,"Summary In this chapter, you’ve learned a bit about what makes a good service, and how to find seams in our problem space that give us the dual benefits of both loose coupling and high cohesion. Bound...",qwen2.5:latest,2025-11-02 04:33:42,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Shared Database,Technology-Agnostic APIs,"#### Technology-Agnostic APIs
Background context: As the IT industry evolves rapidly, it's important for APIs to remain agnostic of specific technologies. This ensures that services can be easily migrated or replaced without impacting other parts of the system.
:p Why is technology-agnosticism important in API design?
??x
Technology-agnosticism in API design is crucial because:
1. **Future-Proofing**: It allows your service to evolve and adapt to new tools, frameworks, and languages as they emerge.
2. **Scalability**: Services that are not tightly coupled to a specific technology stack can be more easily scaled or replaced without significant disruptions.
3. **Innovation**: Encourages experimentation with different technologies to improve productivity and performance.

By designing APIs that do not depend on specific implementation details, you ensure greater flexibility and resilience in your system.
x??

---",923,"Summary In this chapter, you’ve learned a bit about what makes a good service, and how to find seams in our problem space that give us the dual benefits of both loose coupling and high cohesion. Bound...",qwen2.5:latest,2025-11-02 04:33:42,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Shared Database,Keeping Options Open with Microservices,"#### Keeping Options Open with Microservices
Background context: The text discusses why microservices are preferred for keeping options open, particularly focusing on technology-agnostic communication between services. It emphasizes making services simple and accessible to consumers while hiding internal implementation details.
:p Why is it important to keep APIs used for communication between microservices technology-agnostic?
??x
It is crucial to keep APIs technology-agnostic because this approach allows the microservice architecture to remain flexible and independent of specific technology stacks. This flexibility ensures that changes in one service do not force consumers or other services to update their technology, thereby maintaining loose coupling and reducing the complexity and cost of integration.
```java
public interface CustomerService {
    void createCustomer(Customer customer);
    Customer getCustomer(String customerId);
}
```
x??",959,"I am a big fan of keeping my options open, which is why I am such a fan of microservices. It is also why I think it is very important to ensure that you keep the APIs used for communication between mi...",qwen2.5:latest,2025-11-02 09:49:00,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Shared Database,Making Services Easy for Consumers,"#### Making Services Easy for Consumers
Background context: The text emphasizes making microservices easy for consumers to use by providing full freedom in their technology choice. It suggests that while client libraries can ease adoption, they may increase coupling and thus should be used judiciously.
:p How do we make it easy for consumers to use our services?
??x
We make services easy for consumers by allowing them full freedom in their technology choices and providing simple APIs. However, we must balance this with the need to keep internal implementation details hidden. Client libraries can ease adoption but may increase coupling. Therefore, careful consideration is needed to ensure that any library provided does not introduce unnecessary dependencies or coupling.
```java
public class CustomerClient {
    public void createCustomer(Customer customer) {
        // Implementation using a technology-agnostic API
    }
}
```
x??",943,"I am a big fan of keeping my options open, which is why I am such a fan of microservices. It is also why I think it is very important to ensure that you keep the APIs used for communication between mi...",qwen2.5:latest,2025-11-02 09:49:00,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Shared Database,Hiding Internal Implementation Details,"#### Hiding Internal Implementation Details
Background context: The text stresses the importance of hiding internal implementation details to avoid breaking consumers if changes are made within the microservice. This is crucial for maintaining loose coupling and reducing technical debt.
:p Why is it important to hide internal implementation detail in a microservice?
??x
Hiding internal implementation details is important because exposing these details can make it harder to change or update the service without affecting its consumers. By keeping the interface clean and technology-agnostic, we reduce the risk of breaking changes and increase our ability to evolve the service independently.
```java
public class CustomerService {
    private final Database db;
    
    public CustomerService(Database db) {
        this.db = db;
    }
    
    public void createCustomer(Customer customer) {
        // Implementation that interacts with database internally but exposes a clean API
    }
}
```
x??",1004,"I am a big fan of keeping my options open, which is why I am such a fan of microservices. It is also why I think it is very important to ensure that you keep the APIs used for communication between mi...",qwen2.5:latest,2025-11-02 09:49:00,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Shared Database,Interfacing with Customers: The Shared Database Approach,"#### Interfacing with Customers: The Shared Database Approach
Background context: The text discusses the shared database approach, which is one of the most common forms of integration in microservices. It highlights the simplicity and speed of getting started but also notes potential drawbacks such as tight coupling.
:p What are some challenges of using a shared database for customer interactions?
??x
Using a shared database can lead to several challenges, including tight coupling between services, potential performance bottlenecks, and difficulty in scaling or maintaining data consistency. Additionally, changes in the schema or logic of one service can inadvertently break other services that rely on the shared database.
```java
public class CustomerService {
    public void createCustomer(Customer customer) {
        // SQL operations to insert into the shared database
    }
}
```
x??

---",903,"I am a big fan of keeping my options open, which is why I am such a fan of microservices. It is also why I think it is very important to ensure that you keep the APIs used for communication between mi...",qwen2.5:latest,2025-11-02 09:49:00,7
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Shared Database,Exposing Internal Implementation Details through Database Integration,"#### Exposing Internal Implementation Details through Database Integration
Background context: The passage discusses a common practice where services share data by directly accessing and modifying a database. This approach can lead to several issues, including breaking changes due to schema modifications and tight coupling between consumers and the database implementation.

:p What are the main drawbacks of using direct database access for sharing customer information among different services?
??x
The main drawbacks include:
1. **Breaking Changes**: Changing the internal database schema could break existing consumer logic.
2. **Tight Coupling**: Consumers become tightly coupled to the specific technology and structure of the database, making it difficult to change storage strategies over time.
3. **Decoupling Loss**: Both strong cohesion (uniformity in behavior) and loose coupling (independence of components) are compromised.

To illustrate, consider a scenario where you have three different services (warehouse, registration UI, call center UI) editing customer information directly through the database:
```java
// Pseudocode for direct database interaction
public void updateCustomerInfo(Customer customer) {
    // Directly modifying the database record using specific DB driver methods
    dbDriver.update(customer);
}
```
x??",1346,"This is a common enough pattern, but it’s one fraught with difficulties. Figure 4-1. Using DB integration to access and change customer information First, we are allowing external parties to view and ...",qwen2.5:latest,2025-11-02 09:49:25,2
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Shared Database,Shared API Fragility due to Database Changes,"#### Shared API Fragility due to Database Changes
Background context: The passage highlights that a shared database acts as a brittle shared API, making it challenging to change the underlying schema without breaking other services.

:p How does changing the database schema impact consumers using direct access?
??x
Changing the database schema can break existing consumers because they are tightly bound to the current structure. For example:
- If you decide to add a new field in the customer table or remove an old one, all services that interact with this data need to be updated.
- Any regression testing is required to ensure no part of the application breaks due to these changes.

Example scenario:
```java
// Original database schema
public class Customer {
    private String name;
    private int age;
}

// After modification: adding a new field
public class UpdatedCustomer {
    private String name;
    private int age;
    private boolean isVIP;
}
```
x??",972,"This is a common enough pattern, but it’s one fraught with difficulties. Figure 4-1. Using DB integration to access and change customer information First, we are allowing external parties to view and ...",qwen2.5:latest,2025-11-02 09:49:25,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Shared Database,Tying Consumers to Specific Technology Choices,"#### Tying Consumers to Specific Technology Choices
Background context: Direct database access binds consumers to the specific technology and structure of the database, making it hard to switch storage strategies without impacting these consumers.

:p What are the implications of tightly coupling services with a specific database technology?
??x
Tightly coupling services with a specific database technology means that if you need to change the underlying data store (e.g., from SQL to NoSQL), all consuming services must be updated. This can lead to significant rework and increased complexity.

Example:
```java
// Pseudocode for using a relational database driver
public void registerCustomer(Customer customer) {
    // Using specific JDBC methods to insert a new customer
    jdbcTemplate.update(""INSERT INTO customers (name, age) VALUES (?, ?)"", 
                         customer.getName(), customer.getAge());
}
```
x??",929,"This is a common enough pattern, but it’s one fraught with difficulties. Figure 4-1. Using DB integration to access and change customer information First, we are allowing external parties to view and ...",qwen2.5:latest,2025-11-02 09:49:25,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Shared Database,Losing Cohesion and Strong Coupling,"#### Losing Cohesion and Strong Coupling
Background context: The passage points out that direct database access leads to scattered logic across different services, compromising cohesion and strong coupling.

:p How does direct database access affect the distribution of business logic among services?
??x
Direct database access spreads critical business logic across multiple services. For instance:
- If editing customer information involves complex operations like updating related orders or notifications, each service that needs to edit a customer must implement this logic.
- This can lead to redundancy and inconsistent behavior if updates are not synchronized.

Example scenario:
```java
// Pseudocode for handling customer edits in three different services
public void editCustomerInWarehouse(Customer customer) {
    // Update warehouse-specific information
}

public void editCustomerInRegistrationUI(Customer customer) {
    // Update UI-related information
}

public void editCustomerInCallCenterUI(Customer customer) {
    // Update call center-specific information
}
```
x??",1088,"This is a common enough pattern, but it’s one fraught with difficulties. Figure 4-1. Using DB integration to access and change customer information First, we are allowing external parties to view and ...",qwen2.5:latest,2025-11-02 09:49:25,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Shared Database,Avoiding Database Integration for Microservices,"#### Avoiding Database Integration for Microservices
Background context: The passage concludes by advising against using direct database access in microservices architecture due to the issues it introduces.

:p What is the recommendation given regarding database integration in a microservices architecture?
??x
The recommendation is to avoid direct database access. Instead, services should collaborate through APIs or other means that do not expose internal implementation details, ensuring loose coupling and strong cohesion.

Example:
```java
// Service-to-service interaction via API
public void updateCustomer(Customer customer) {
    customerService.update(customer);
}
```
x??

---",689,"This is a common enough pattern, but it’s one fraught with difficulties. Figure 4-1. Using DB integration to access and change customer information First, we are allowing external parties to view and ...",qwen2.5:latest,2025-11-02 09:49:25,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Orchestration Versus Choreography,Synchronous Communication,"#### Synchronous Communication
Background context explaining synchronous communication. Synchronous communication involves a call to a remote server, which blocks until the operation completes. This method is straightforward and easy to reason about since we know when things have completed successfully or not.
:p What is synchronous communication?
??x
Synchronous communication refers to a mode of interaction where a client waits for an operation to complete before proceeding. If a request is made to a remote server, it will block until the response is received. This approach ensures that the calling code does not continue execution until it receives the result.
x??",673,"Synchronous Versus Asynchronous Before we start diving into the specifics of different technology choices, we should discuss one of the most important decisions we can make in terms of how services co...",qwen2.5:latest,2025-11-02 09:49:50,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Orchestration Versus Choreography,Asynchronous Communication,"#### Asynchronous Communication
Background context explaining asynchronous communication. In contrast to synchronous calls, with asynchronous communication, the caller doesn’t wait for the operation to complete before returning. This model is particularly useful for long-running jobs and maintaining low latency in applications where blocking a call would degrade performance.
:p What is asynchronous communication?
??x
Asynchronous communication allows a client to initiate an operation without waiting for it to complete. The caller can proceed with other tasks while the server processes the request. Once completed, the server can notify the client through callbacks or events. This approach helps maintain application responsiveness and manage long-running operations efficiently.
x??",790,"Synchronous Versus Asynchronous Before we start diving into the specifics of different technology choices, we should discuss one of the most important decisions we can make in terms of how services co...",qwen2.5:latest,2025-11-02 09:49:50,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Orchestration Versus Choreography,Request/Response Collaboration,"#### Request/Response Collaboration
Background context explaining the request/response model in collaboration. This model is closely aligned with synchronous communication where a client initiates a request and waits for a response before proceeding. It can also work for asynchronous communication, using callbacks or event-driven mechanisms to handle responses.
:p What is the request/response collaboration model?
??x
The request/response collaboration model involves a client sending a request to a server and waiting for a response. In synchronous scenarios, this means blocking until the operation completes. For asynchronous implementations, it can involve registering callbacks that get invoked once the server has processed the request.
x??",749,"Synchronous Versus Asynchronous Before we start diving into the specifics of different technology choices, we should discuss one of the most important decisions we can make in terms of how services co...",qwen2.5:latest,2025-11-02 09:49:50,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Orchestration Versus Choreography,Event-Based Collaboration,"#### Event-Based Collaboration
Background context explaining event-based collaboration. This approach inverts the traditional model by having services emit events and allowing other parties to react to them. Events are typically used for handling long-running processes or low-latency scenarios where immediate responses from a central authority are not practical.
:p What is event-based collaboration?
??x
Event-based collaboration allows services to notify others of events that have occurred without directly commanding actions. Other services can subscribe to these events and react accordingly. This model promotes loose coupling and asynchronous communication, making it suitable for complex business processes across multiple services.
x??",746,"Synchronous Versus Asynchronous Before we start diving into the specifics of different technology choices, we should discuss one of the most important decisions we can make in terms of how services co...",qwen2.5:latest,2025-11-02 09:49:50,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Orchestration Versus Choreography,Orchestration vs. Choreography,"#### Orchestration vs. Choreography
Background context explaining the differences between orchestration and choreography in managing complex logic spanning service boundaries. Orchestration relies on a central authority to guide and drive processes, while choreography distributes responsibility among collaborating services, allowing them to handle their parts independently.
:p What is orchestration?
??x
Orchestration involves having a central brain (like the customer service in the example) that coordinates and manages the flow of operations. This centralized approach ensures that each step in a process is tracked and monitored, providing visibility into the overall state of the operation.
x??",702,"Synchronous Versus Asynchronous Before we start diving into the specifics of different technology choices, we should discuss one of the most important decisions we can make in terms of how services co...",qwen2.5:latest,2025-11-02 09:49:50,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Orchestration Versus Choreography,Orchestration Example: Customer Creation,"#### Orchestration Example: Customer Creation
Background context providing an example of orchestration for handling complex processes like customer creation. In this scenario, the customer service acts as the central brain, initiating requests to other services (loyalty points bank, email service, and postal system) through request/response calls.
:p What is the orchestration approach in handling customer creation?
??x
The orchestration approach handles customer creation by having a centralized service (customer service) coordinate all actions. For example, when creating a new customer, it might talk to the loyalty points bank, email service, and postal system using synchronous request/response calls. The central brain can track each step's completion status.
```java
// Pseudocode for orchestration approach in Java
public void createCustomer(Customer customer) {
    // Step 1: Create a record in the loyalty points bank
    LoyaltyPointsBank.createRecord(customer);
    
    // Step 2: Send out a welcome pack via postal system
    PostalService.sendWelcomePack(customer);
    
    // Step 3: Send a welcome email to the customer
    EmailService.sendEmail(customer);
}
```
x??",1190,"Synchronous Versus Asynchronous Before we start diving into the specifics of different technology choices, we should discuss one of the most important decisions we can make in terms of how services co...",qwen2.5:latest,2025-11-02 09:49:50,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Orchestration Versus Choreography,Choreography Example: Customer Creation,"#### Choreography Example: Customer Creation
Background context providing an example of choreography for handling complex processes like customer creation. In this scenario, services emit events (e.g., ""Customer created"") and other services subscribe to these events to react accordingly.
:p What is the choreographed approach in handling customer creation?
??x
The choreographed approach handles customer creation by having the customer service emit an event without directly managing subsequent actions. Other services can subscribe to this event and handle their respective tasks independently.
```java
// Pseudocode for choreography approach in Java
public void createCustomer(Customer customer) {
    // Emit an event that other services can react to
    EventPublisher.emit(""Customer created"", customer);
}
```
x??

---",825,"Synchronous Versus Asynchronous Before we start diving into the specifics of different technology choices, we should discuss one of the most important decisions we can make in terms of how services co...",qwen2.5:latest,2025-11-02 09:49:50,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Brittleness,Remote Procedure Calls (RPC),"#### Remote Procedure Calls (RPC)
Background context: Remote procedure call (RPC) is a protocol that allows a program to execute a function on a different machine. It enables a local call to appear as if it were remote, making communication between systems seamless and transparent. RPC technologies can vary in how they handle interface definitions, networking protocols, and the coupling between client and server.

:p What are some types of RPC technology mentioned in the text?
??x
Some types of RPC technology include Java RMI, SOAP, Thrift, Protocol Buffers. These technologies differ in their approach to interface definition, networking protocols used, and how tightly they couple clients with servers.
x??",714,Remote Procedure Calls Remote procedure call  refers to the technique of making a local call and having it execute on a remote service somewhere. There are a number of different types of RPC technolog...,qwen2.5:latest,2025-11-02 09:50:22,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Brittleness,Interface Definition for Remote Services,"#### Interface Definition for Remote Services
Background context: For some RPC technologies like SOAP, Thrift, or Protocol Buffers, the use of an interface definition is crucial. This helps in generating client and server stubs that can be implemented across different technology stacks.

:p What are the advantages of using a separate interface definition for RPC services?
??x
The main advantage of using a separate interface definition is the ease of generating client and server stubs for various technology stacks, such as Java on one end and .NET on the other. This means that developers can leverage existing tools to create these stubs from a common interface description, simplifying cross-language integration.

Example: Using Thrift to generate client and server stubs:
```bash
thrift --gen java service.thrift
```
This command generates Java code for both the client and server, based on the `service.thrift` file.
x??",930,Remote Procedure Calls Remote procedure call  refers to the technique of making a local call and having it execute on a remote service somewhere. There are a number of different types of RPC technolog...,qwen2.5:latest,2025-11-02 09:50:22,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Brittleness,Technology Coupling in RPC,"#### Technology Coupling in RPC
Background context: Some RPC mechanisms are tightly coupled to a specific platform or technology stack. For instance, Java RMI is closely tied to the JVM environment.

:p What are some downsides of tight coupling in RPC technologies?
??x
Tight coupling can limit flexibility and interoperability between clients and servers that use different programming languages or frameworks. For example, using Java RMI means both the client and server must run on JVMs, which may not be feasible if one side needs to use a different runtime environment.

Example: Tight coupling in Java RMI:
```java
// Client code
Remote stub = ...;
CustomerRemote customerService = (CustomerRemote) PortableRemoteObject.narrow(stub, CustomerRemote.class);
Customer customer = customerService.findCustomer(""id"");

// Server code
public class MyServiceImpl implements CustomerRemote {
    public Customer findCustomer(String id) throws RemoteException {
        // Implementation...
    }
}
```
x??",1002,Remote Procedure Calls Remote procedure call  refers to the technique of making a local call and having it execute on a remote service somewhere. There are a number of different types of RPC technolog...,qwen2.5:latest,2025-11-02 09:50:22,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Brittleness,Performance Considerations for RPC,"#### Performance Considerations for RPC
Background context: While RPC can make remote calls appear local, there are significant performance differences between making a local call and a networked one. The overhead of marshalling/unmarshalling payloads and sending data over the network can be substantial.

:p Why is it important to consider the difference between local and remote method calls in API design?
??x
It's crucial because local method calls (in-process) are generally faster due to minimal overhead, while remote method calls involve network latency, serialization/deserialization costs, and potential failure points. Developers should optimize remote interfaces differently from local ones, considering factors like performance and reliability.

Example: Comparing local vs. remote call performance:
```java
// Local call (in-process)
public Customer findCustomerLocal(String id) {
    return customers.get(id);
}

// Remote call (networked)
public Customer findCustomerRemote(String id) throws RemoteException {
    // Network latency, serialization, and deserialization overhead
}
```
x??",1104,Remote Procedure Calls Remote procedure call  refers to the technique of making a local call and having it execute on a remote service somewhere. There are a number of different types of RPC technolog...,qwen2.5:latest,2025-11-02 09:50:22,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Brittleness,Brittleness in RPC Implementation,"#### Brittleness in RPC Implementation
Background context: Some popular RPC implementations can lead to brittle systems due to the tight coupling of interfaces. Changes in the interface can require updates across multiple clients and servers.

:p What are some ways changes in an RPC interface can cause brittleness?
??x
Changes like adding new methods or fields, removing existing ones, or restructuring data types can propagate throughout the system if not managed carefully. For example, modifying a Java RMI interface may necessitate regenerating stubs for all clients and ensuring server implementations are updated.

Example: Brittleness due to method addition in Java RMI:
```java
// Original interface
public interface CustomerRemote extends Remote {
    public Customer findCustomer(String id) throws RemoteException;
}

// New method added
public interface CustomerRemote extends Remote {
    public Customer findCustomer(String id) throws RemoteException;
    public Customer createCustomer(String emailAddress) throws RemoteException;
}
```
x??",1056,Remote Procedure Calls Remote procedure call  refers to the technique of making a local call and having it execute on a remote service somewhere. There are a number of different types of RPC technolog...,qwen2.5:latest,2025-11-02 09:50:22,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Brittleness,Network Reliability and Resiliency,"#### Network Reliability and Resiliency
Background context: The first fallacy of distributed computing is that ""the network is reliable."" However, networks can fail or behave unpredictably. Developers must design systems to handle these failures gracefully.

:p What are some considerations for designing resilient RPC services?
??x
Developers should consider the possibility of network unreliability and implement strategies such as retries, timeouts, fallbacks, and circuit breakers. These mechanisms help in maintaining service availability even when network issues occur.

Example: Implementing a retry mechanism:
```java
public Customer findCustomer(String id) {
    int attempts = 3;
    for (int i = 0; i < attempts; i++) {
        try {
            return customerService.findCustomer(id);
        } catch (RemoteException e) {
            if (i == attempts - 1) throw e; // Final attempt, rethrow exception
            log.error(""Failed to find customer"", e);
        }
    }
}
```
x??

---",999,Remote Procedure Calls Remote procedure call  refers to the technique of making a local call and having it execute on a remote service somewhere. There are a number of different types of RPC technolog...,qwen2.5:latest,2025-11-02 09:50:22,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Hypermedia As the Engine of Application State,RPC (Remote Procedure Call),"#### RPC (Remote Procedure Call)
Background context explaining RPC and its common implementations. Highlight the challenges associated with RMI (Remote Method Invocation) and how modern mechanisms like Protocol Buffers or Thrift mitigate some of these issues by avoiding lock-step releases.

:p What are the main characteristics of RPC in terms of implementation?
??x
RPC, particularly older implementations such as RMI, can pose several challenges. One significant issue is that client and server code need to be developed synchronously, leading to potential versioning problems and tight coupling between them. Modern mechanisms like Protocol Buffers or Thrift provide a more flexible approach by abstracting away the serialization/deserialization process of messages, thus reducing the need for synchronous releases.

```java
// Example using Protocol Buffers (protobuf)
message Customer {
  string name = 1;
  int32 id = 2;
}

Service CustomerService {
  rpc GetCustomerById(CustomerId) returns (Customer);
}
```
x??",1020,"Is RPC Terrible? Despite its shortcomings, I wouldn’t go so far as to call RPC terrible. Some of the common implementations that I have encountered can lead to the sorts of problems I have outlined he...",qwen2.5:latest,2025-11-02 09:50:48,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Hypermedia As the Engine of Application State,Potential Pitfalls of RPC,"#### Potential Pitfalls of RPC
Background context on common issues that arise with RPC, including how to avoid making remote calls opaque to clients and ensuring the ability to evolve server interfaces without requiring simultaneous client upgrades.

:p What should developers be cautious about when using RPC?
??x
Developers should ensure that their remote calls are not abstracted in such a way that it's impossible for clients to understand they involve network operations. This means avoiding layers of abstraction that obscure network dependencies, which can lead to hidden latency and performance issues. Additionally, the server interface should be designed with future changes in mind, allowing updates without forcing all client code to be upgraded simultaneously.

```java
// Example of a poorly abstracted remote call (bad practice)
public Customer getCustomerById(int id) {
    return customerService.getCustomer(id);
}

// A better approach would be:
public Customer getCustomerById(int id) throws RemoteCallException {
    try {
        return customerService.getCustomer(id);
    } catch (IOException e) {
        throw new RemoteCallException(""Network error"", e);
    }
}
```
x??",1195,"Is RPC Terrible? Despite its shortcomings, I wouldn’t go so far as to call RPC terrible. Some of the common implementations that I have encountered can lead to the sorts of problems I have outlined he...",qwen2.5:latest,2025-11-02 09:50:48,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Hypermedia As the Engine of Application State,REST (REpresentational State Transfer),"#### REST (REpresentational State Transfer)
Background context on REST and its architectural principles. Emphasize the concept of resources, representations, and how HTTP verbs can be used to interact with these resources.

:p What does REST stand for and what are the key concepts it introduces?
??x
REST stands for REpresentational State Transfer, an architectural style inspired by the World Wide Web. The core idea is that a client interacts with server resources through different representations (e.g., JSON, XML). Each resource can be accessed using HTTP verbs like GET, POST, PUT, DELETE to manipulate its state.

```java
// Example of RESTful service interaction
@GetMapping(""/customers/{id}"")
public ResponseEntity<Customer> getCustomer(@PathVariable Long id) {
    Customer customer = customerRepository.findById(id);
    if (customer == null) {
        return new ResponseEntity<>(HttpStatus.NOT_FOUND);
    }
    return new ResponseEntity<>(customer, HttpStatus.OK);
}

@PostMapping(""/customers"")
public ResponseEntity<Void> createCustomer(@RequestBody Customer customer) {
    Customer savedCustomer = customerRepository.save(customer);
    HttpHeaders headers = new HttpHeaders();
    headers.setLocation(URI.create(""/customers/"" + savedCustomer.getId()));
    return new ResponseEntity<>(headers, HttpStatus.CREATED);
}
```
x??",1343,"Is RPC Terrible? Despite its shortcomings, I wouldn’t go so far as to call RPC terrible. Some of the common implementations that I have encountered can lead to the sorts of problems I have outlined he...",qwen2.5:latest,2025-11-02 09:50:48,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Hypermedia As the Engine of Application State,Comparison of REST and HTTP,"#### Comparison of REST and HTTP
Background context on the relationship between REST and HTTP. Explain how HTTP verbs provide a natural fit for implementing RESTful services.

:p What are the advantages of using HTTP for implementing REST?
??x
HTTP provides several built-in features that simplify implementing RESTful services, such as predefined verbs (GET, POST, PUT, DELETE) with well-defined semantics. These verbs can be used to perform common CRUD operations on resources. For example, GET is typically used to retrieve a resource, POST for creating new ones, PUT for updating existing ones, and DELETE for removing them.

```java
// Example of using HTTP methods in RESTful service interactions
@GetMapping(""/customers/{id}"")
public ResponseEntity<Customer> getCustomer(@PathVariable Long id) {
    Customer customer = customerRepository.findById(id);
    if (customer == null) {
        return new ResponseEntity<>(HttpStatus.NOT_FOUND);
    }
    return new ResponseEntity<>(customer, HttpStatus.OK);
}

@PostMapping(""/customers"")
public ResponseEntity<Void> createCustomer(@RequestBody Customer customer) {
    Customer savedCustomer = customerRepository.save(customer);
    HttpHeaders headers = new HttpHeaders();
    headers.setLocation(URI.create(""/customers/"" + savedCustomer.getId()));
    return new ResponseEntity<>(headers, HttpStatus.CREATED);
}
```
x??

---",1379,"Is RPC Terrible? Despite its shortcomings, I wouldn’t go so far as to call RPC terrible. Some of the common implementations that I have encountered can lead to the sorts of problems I have outlined he...",qwen2.5:latest,2025-11-02 09:50:48,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Hypermedia As the Engine of Application State,GET vs POST Methods,"#### GET vs POST Methods
HTTP methods are used to interact with resources on a server. The GET method retrieves information from the server, while the POST method is used for creating new resources or submitting data to be processed by the server.
:p What does the GET method do?
??x
The GET method retrieves a resource in an idempotent way. This means that making multiple identical requests will have the same effect as making one request, which helps avoid unnecessary resource creation and updates.
```java
// Example of using GET in Java
public class HttpExample {
    public String retrieveCustomer(String customerId) {
        // HTTP GET call to retrieve customer details
        return ""GET /customer/"" + customerId;
    }
}
```
x??",741,"GET retrieves a resource in an idempotent way, and POST creates a new resource. This means we can avoid lots of different  createCustomer  or  editCustomer  methods. Instead, we can simply POST a cust...",qwen2.5:latest,2025-11-02 09:51:10,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Hypermedia As the Engine of Application State,Using POST for Creating Resources,"#### Using POST for Creating Resources
Using the POST method allows us to create a new resource on the server without needing separate methods like `createCustomer` or `editCustomer`.
:p How does using POST help in managing resources?
??x
By using POST, we can send a customer representation to the server, which will then create a new resource. This simplifies our API design and reduces the number of required methods.
```java
// Example of using POST to create a customer in Java
public class CustomerService {
    public void createCustomer(Customer customer) {
        // HTTP POST call to create a new customer
        System.out.println(""POST /customer with data: "" + customer);
    }
}
```
x??",701,"GET retrieves a resource in an idempotent way, and POST creates a new resource. This means we can avoid lots of different  createCustomer  or  editCustomer  methods. Instead, we can simply POST a cust...",qwen2.5:latest,2025-11-02 09:51:10,7
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Hypermedia As the Engine of Application State,Benefits of Using HTTP,"#### Benefits of Using HTTP
HTTP offers several benefits, including caching proxies like Varnish and load balancers like mod_proxy. It also provides security controls and monitoring tools out of the box.
:p What are some advantages of using HTTP?
??x
Some key advantages include:
- Caching: Tools like Varnish can cache responses to reduce server load.
- Load Balancing: Technologies such as mod_proxy can distribute traffic efficiently.
- Security: Features like basic auth, client certificates, and other security protocols make it easier to secure communications.
- Monitoring: Many monitoring tools have built-in support for HTTP.

```java
// Example of using Varnish cache in Java (pseudocode)
public class CachingService {
    public String getCustomer(String customerId) {
        // Check if customer is cached
        return varnishCache.get(customerId);
    }
}
```
x??",879,"GET retrieves a resource in an idempotent way, and POST creates a new resource. This means we can avoid lots of different  createCustomer  or  editCustomer  methods. Instead, we can simply POST a cust...",qwen2.5:latest,2025-11-02 09:51:10,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Hypermedia As the Engine of Application State,Hypermedia as the Engine of Application State (HATEOAS),"#### Hypermedia as the Engine of Application State (HATEOAS)
HATEOAS suggests that clients should navigate and interact with resources via hyperlinks, rather than hardcoding URIs.
:p What does HATEOAS stand for?
??x
Hypermedia As the Engine of Application State (HATEOAS) is a principle in RESTful design where interactions are driven by links embedded within responses. Instead of clients knowing specific URI formats, they follow hyperlinks to navigate and perform actions.

```java
// Example of HATEOAS in Java (pseudocode)
public class CustomerController {
    public String getCustomer(String customerId) {
        // Fetch customer details from the server with a link
        return ""GET /customer/"" + customerId;
    }

    public void processLink(Link link) {
        // Follow the link to perform an action
        System.out.println(""Following link: "" + link.getHref());
    }
}
```
x??

---",902,"GET retrieves a resource in an idempotent way, and POST creates a new resource. This means we can avoid lots of different  createCustomer  or  editCustomer  methods. Instead, we can simply POST a cust...",qwen2.5:latest,2025-11-02 09:51:10,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Hypermedia As the Engine of Application State,Hypermedia Controls Overview,"#### Hypermedia Controls Overview
Hypermedia controls are a way to provide instructions and navigation within web APIs, similar to how humans interact with shopping carts on websites. This allows clients to discover available actions by following links provided in API responses.

:p What is hypermedia control?
??x
Hypermedia controls are mechanisms within an API that guide the client's interaction by providing links to resources and operations, much like a human user navigating through a website's features such as viewing items in a shopping cart. These controls help clients discover available actions without needing prior knowledge of the exact URIs or APIs.
x??",671,"The location of the shopping cart has changed over time. The graphic has changed. The link has changed. But as humans we are smart enough to still see a shopping cart, know what it is, and interact wi...",qwen2.5:latest,2025-11-02 09:51:30,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Hypermedia As the Engine of Application State,Understanding Hypermedia Controls,"#### Understanding Hypermedia Controls
In Example 4-2, hypermedia controls are represented within an XML document for an album listing. The `link` elements with specific relations like `/artist` and `/instantpurchase` indicate where to navigate.

:p What does a `link` element in the provided example signify?
??x
A `link` element in the provided example signifies navigation points or actions available on the API resource. For instance, `<link rel=""/artist"" href=""/artist/theBrakes""/>` indicates that navigating to `/artist/theBrakes` will provide information about the artist associated with the album. Similarly, `<link rel=""/instantpurchase"" href=""/instantPurchase/1234""/>` suggests a path for purchasing the album.

```xml
<album>
    <name> Give Blood </name>
    <link rel=""/artist"" href=""/artist/theBrakes""/>
    <description> Awesome, short, brutish, funny and loud. Must buy. </description>
    <link rel=""/instantpurchase"" href=""/instantPurchase/1234""/>
</album>
```
x??",982,"The location of the shopping cart has changed over time. The graphic has changed. The link has changed. But as humans we are smart enough to still see a shopping cart, know what it is, and interact wi...",qwen2.5:latest,2025-11-02 09:51:30,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Hypermedia As the Engine of Application State,Decoupling Client from Server,"#### Decoupling Client from Server
Hypermedia controls enable decoupling between the client and server by abstracting the underlying details of API implementations. Changes in how hypermedia controls are displayed or structured should not affect clients as long as the semantics remain consistent.

:p How does hypermedia control help in decoupling a client from a server?
??x
Hypermedia controls help in decoupling a client from a server by abstracting the underlying implementation details of API resources. Clients need to understand that certain links represent specific actions (e.g., `/artist` for artist information, `/instantpurchase` for purchasing) rather than knowing precise URIs or complex protocols. This abstraction means that changes in how these controls are displayed or structured do not necessarily break clients as long as the semantics of each control remain consistent.

For example, a shopping cart might change from being a simple link to a more complex JavaScript widget, but the client still follows the same control logic.
x??",1054,"The location of the shopping cart has changed over time. The graphic has changed. The link has changed. But as humans we are smart enough to still see a shopping cart, know what it is, and interact wi...",qwen2.5:latest,2025-11-02 09:51:30,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Hypermedia As the Engine of Application State,Benefits and Trade-offs,"#### Benefits and Trade-offs
Using hypermedia controls provides significant benefits like progressive discovery of API endpoints and reduced coupling. However, it can be chatty as clients need to follow multiple links to find specific operations.

:p What are some trade-offs when using hypermedia controls?
??x
When using hypermedia controls, one of the trade-offs is increased chattiness or more network requests because clients often need to follow multiple links to find and perform specific actions. Despite this, the benefits include significant decoupling between the client and server, progressive discovery capabilities, and reduced coupling that can be advantageous over time.

```java
public class ClientExample {
    public void navigateAlbum() throws IOException {
        // Follow link to artist info
        String artistInfoUrl = ""http://example.com/artist/theBrakes"";
        
        // Follow another link for purchasing
        String purchaseUrl = ""http://example.com/instantPurchase/1234"";
        
        // Perform operations based on links found
    }
}
```
x??",1088,"The location of the shopping cart has changed over time. The graphic has changed. The link has changed. But as humans we are smart enough to still see a shopping cart, know what it is, and interact wi...",qwen2.5:latest,2025-11-02 09:51:30,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Hypermedia As the Engine of Application State,Initial Upfront Work and Long-term Benefits,"#### Initial Upfront Work and Long-term Benefits
While hypermedia controls require some initial work to set up, they offer long-term benefits such as flexibility in API changes without breaking existing clients.

:p What are the initial challenges with implementing hypermedia controls?
??x
The initial challenge with implementing hypermedia controls is that it requires upfront work to understand and integrate these controls into client applications. However, this initial effort often pays off over time by providing greater flexibility in how APIs can evolve without disrupting existing clients. This approach allows for progressive discovery of API endpoints and reduced coupling between the client and server.

```java
public class HypermediaSetup {
    public void setupHypermediaControls() throws IOException {
        // Logic to set up hypermedia controls and understand their semantics
    }
}
```
x??

---",917,"The location of the shopping cart has changed over time. The graphic has changed. The link has changed. But as humans we are smart enough to still see a shopping cart, know what it is, and interact wi...",qwen2.5:latest,2025-11-02 09:51:30,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Downsides to REST Over HTTP,JSON vs XML,"#### JSON vs XML
Background context explaining the differences and usage between JSON and XML. JSON is simpler and more lightweight, but lacks certain standards like hypermedia controls that are available in XML.

:p What is the difference between JSON and XML?
??x
JSON (JavaScript Object Notation) is a lightweight data-interchange format inspired by JavaScript object literal notation. It is easier to write and read for humans compared to XML. JSON uses key-value pairs enclosed in curly braces, making it simpler and more compact than XML. However, JSON does not natively support hypermedia controls like link elements that are easily handled in XML.

XML (eXtensible Markup Language) on the other hand, is a markup language designed for transporting and storing data. It uses tags to wrap around text to identify different types of information. XML supports more complex structures and has built-in mechanisms for handling hypermedia controls using `link` elements or other custom elements.

Example JSON:
```json
{
  ""name"": ""John"",
  ""age"": 30,
  ""city"": ""New York""
}
```

Example XML with link element:
```xml
<root>
  <name>John</name>
  <age>30</age>
  <link rel=""self"" href=""/john""/>
  <city>New York</city>
</root>
```
x??",1235,"JSON, XML, or Something Else? The use of standard textual formats gives clients a lot of flexibility as to how they consume resources, and REST over HTTP lets us use a variety of formats. The examples...",qwen2.5:latest,2025-11-02 09:51:58,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Downsides to REST Over HTTP,HAL (Hypertext Application Language),"#### HAL (Hypertext Application Language)
Background context explaining the need for a standard way to handle hypermedia controls in JSON, and how HAL addresses this.

:p What is HAL and why is it important?
??x
HAL (Hypertext Application Language) is a specification that aims to provide a consistent way of handling hypermedia controls within JSON responses. It addresses the lack of native support for link elements similar to XML's `<link>` tag in JSON. HAL provides a standard format for representing links and other metadata, which can help in creating more maintainable and flexible RESTful services.

Example HAL:
```json
{
  ""_links"": {
    ""self"": { ""href"": ""/resource/1"" },
    ""collection"": { ""href"": ""/resources"" }
  }
}
```
x??",741,"JSON, XML, or Something Else? The use of standard textual formats gives clients a lot of flexibility as to how they consume resources, and REST over HTTP lets us use a variety of formats. The examples...",qwen2.5:latest,2025-11-02 09:51:58,4
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Downsides to REST Over HTTP,XML Tool Support,"#### XML Tool Support
Background context explaining the advantages of using XML, particularly in terms of tool support for handling payloads.

:p What are some advantages of using XML over JSON?
??x
XML has several advantages when compared to JSON:

1. **Tool Support**: XML is often better supported by existing tools and libraries.
2. **XPath and CSS Selectors**: XML supports XPath, a powerful query language that allows you to extract specific parts of the payload easily. Additionally, CSS selectors can be used as an alternative or complement to XPath, making it even easier for developers who are familiar with HTML.
3. **Standardization**: XML has been around longer and is more standardized, meaning there are more established tools and practices.

Example using XPATH in XML:
```xml
<xsl:stylesheet version=""1.0"" xmlns:xsl=""http://www.w3.org/1999/XSL/Transform"">
  <xsl:template match=""/"">
    <html>
      <body>
        <h2>Users</h2>
        <table border=""1"">
          <tr bgcolor=""#9acd32"">
            <th>Name</th>
            <th>Email</th>
          </tr>
          <xsl:for-each select=""users/user"">
            <tr>
              <td><xsl:value-of select=""name""/></td>
              <td><xsl:value-of select=""email""/></td>
            </tr>
          </xsl:for-each>
        </table>
      </body>
    </html>
  </xsl:template>
</xsl:stylesheet>
```
x??",1375,"JSON, XML, or Something Else? The use of standard textual formats gives clients a lot of flexibility as to how they consume resources, and REST over HTTP lets us use a variety of formats. The examples...",qwen2.5:latest,2025-11-02 09:51:58,3
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Downsides to REST Over HTTP,REST Frameworks and Overhead,"#### REST Frameworks and Overhead
Background context explaining the trade-offs when using REST frameworks, including potential coupling issues.

:p What are some downsides to using REST frameworks like Spring Boot?
??x
While REST frameworks can simplify development and reduce boilerplate code, they often come with a set of assumptions that can lead to tight coupling between the service implementation and its external interface. For example, some frameworks might promote directly exposing database representations as API endpoints without proper abstraction.

This approach can cause problems later on when you need to change the internal data structures or storage mechanisms, as it locks you into specific implementations too early in the development process.

Example of tight coupling:
```java
@RestController
public class UserResource {
    @Autowired
    private UserRepository userRepository;

    @GetMapping(""/users"")
    public List<User> getUsers() {
        return userRepository.findAll();
    }
}
```
In this example, the `UserRepository` directly interacts with a database, making changes to storage mechanisms harder.

A better approach would be to abstract data access and expose only necessary interfaces:
```java
@RestController
public class UserResource {
    @Autowired
    private UserService userService;

    @GetMapping(""/users"")
    public List<UserDto> getUsers() {
        return userService.getAllUsers();
    }
}
```
x??",1454,"JSON, XML, or Something Else? The use of standard textual formats gives clients a lot of flexibility as to how they consume resources, and REST over HTTP lets us use a variety of formats. The examples...",qwen2.5:latest,2025-11-02 09:51:58,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Downsides to REST Over HTTP,HTTP Overhead,"#### HTTP Overhead
Background context explaining the potential overhead of using REST over HTTP, especially in scenarios requiring low latency.

:p Why might REST over HTTP be a poor choice for low-latency communications?
??x
REST over HTTP can introduce significant overhead due to its reliance on the full HTTP protocol stack. Each request incurs a round-trip time that includes:

1. **HTTP Handshake**: Establishing a connection and negotiating headers.
2. **Serialization/Deserialization Overhead**: Converting between different data formats (e.g., JSON, XML).
3. **Security Headers**: Including authentication and authorization checks.

These overheads can be substantial in scenarios where extremely low-latency or small message sizes are critical.

For example, consider a WebSocket scenario:
```java
@ServerEndpoint(""/websocket"")
public class WebSocketHandler {
    @OnOpen
    public void onOpen(Session session) {
        System.out.println(""New client connected: "" + session.getId());
    }

    @OnMessage
    public String handleMessage(String message) {
        // Process and return the message
        return ""Echo: "" + message;
    }
}
```
In this case, WebSockets provide a more efficient way to stream data directly between client and server without the overhead of HTTP.

x??

---",1300,"JSON, XML, or Something Else? The use of standard textual formats gives clients a lot of flexibility as to how they consume resources, and REST over HTTP lets us use a variety of formats. The examples...",qwen2.5:latest,2025-11-02 09:51:58,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Technology Choices,Message Broker Overview,"#### Message Broker Overview
Message brokers like RabbitMQ serve as intermediaries between producers and consumers of events. They handle subscriptions, ensuring that consumers are informed when an event arrives. Brokers can also manage state, such as tracking which messages a consumer has already seen.

:p What is the role of message brokers in asynchronous communication?
??x
Message brokers act as intermediaries for publishing and subscribing to events. They help ensure that producers can send events and consumers can be notified about those events efficiently. Additionally, they handle state management, keeping track of what messages have been consumed by each subscriber.

For example, consider a scenario where an order is placed in an e-commerce system:
- A message broker could be used for the order service to emit an event when an order is created.
- Other services (such as inventory or email notification) can subscribe to this event and take appropriate actions.

```java
// Pseudocode example of emitting an event using a message broker
public class OrderService {
    private MessageBroker messageBroker;

    public void createOrder(Order order) {
        // Emit the event through the message broker
        messageBroker.publish(""order.created"", order);
    }
}

// Example subscription in another service to consume events
public class InventoryService {
    private MessageBroker messageBroker;
    private Set<String> seenOrders = new HashSet<>();

    public void subscribeToOrderEvents() {
        // Subscribe to specific event types
        messageBroker.subscribe(""order.created"", this::handleOrderCreated);
    }

    private void handleOrderCreated(Order order) {
        if (!seenOrders.contains(order.getId())) {
            // Process the order only once
            seenOrders.add(order.getId());
            // Update inventory or perform other actions
        }
    }
}
```
x??",1918,"Implementing Asynchronous Event-Based Collaboration We’ve talked for a bit about some technologies that can help us implement request/response patterns. What about event-based, asynchronous communicat...",qwen2.5:latest,2025-11-02 09:52:25,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Technology Choices,ATOM for Event Propagation,"#### ATOM for Event Propagation
ATOM is a REST-compliant specification that defines semantics for publishing feeds of resources. It allows services to publish events and consumers to poll the feed for updates.

:p Can you describe how ATOM can be used for event propagation?
??x
ATOM can be used to propagate events by allowing services to publish updates as resources in a feed, which can then be consumed by other services that poll this feed. This approach leverages HTTP, making it easy to integrate with existing systems and benefiting from its scalability.

For instance, if the customer service changes (e.g., a new customer is added), the customer service could publish an event to a feed. Consumers could periodically check this feed for any changes using simple HTTP requests.

```java
// Pseudocode example of publishing events using ATOM
public class CustomerService {
    private HttpClient httpClient;
    private String atomFeedUrl;

    public void addCustomer(Customer customer) {
        // Create the Atom entry for the new customer
        String xmlEntry = generateAtomEntry(customer);
        
        // Post the XML to the feed URL
        Response response = httpClient.post(atomFeedUrl, xmlEntry);
        if (response.isSuccess()) {
            System.out.println(""Customer added successfully."");
        } else {
            System.out.println(""Failed to add customer: "" + response.getErrorMessage());
        }
    }

    private String generateAtomEntry(Customer customer) {
        // Generate the Atom XML entry for the customer
        return ""<entry><title>"" + customer.getName() + ""</title><content type='text'>"" + customer.getDescription() + ""</content></entry>"";
    }
}

// Pseudocode example of consuming events from ATOM feed
public class ConsumerService {
    private HttpClient httpClient;
    private String atomFeedUrl;

    public void checkForNewCustomers() {
        // Fetch the Atom feed and parse for new entries
        Response response = httpClient.get(atomFeedUrl);
        if (response.isSuccess()) {
            String xmlContent = response.getContent();
            List<Customer> newCustomers = parseAtomEntries(xmlContent);
            processNewCustomers(newCustomers);
        } else {
            System.out.println(""Failed to fetch the feed: "" + response.getErrorMessage());
        }
    }

    private List<Customer> parseAtomEntries(String xmlContent) {
        // Parse the Atom XML content and extract customer entries
        // Return a list of new customers found in the feed
        return new ArrayList<>();
    }

    private void processNewCustomers(List<Customer> newCustomers) {
        for (Customer customer : newCustomers) {
            System.out.println(""Processing new customer: "" + customer.getName());
            // Perform any necessary actions with the new customer
        }
    }
}
```
x??",2879,"Implementing Asynchronous Event-Based Collaboration We’ve talked for a bit about some technologies that can help us implement request/response patterns. What about event-based, asynchronous communicat...",qwen2.5:latest,2025-11-02 09:52:25,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Technology Choices,Competing Consumer Pattern in ATOM,"#### Competing Consumer Pattern in ATOM
The Competing Consumer pattern involves bringing up multiple worker instances to compete for messages. This approach is useful for scaling the number of workers to handle a list of independent jobs, but it introduces complexity if shared state needs to be managed.

:p How does the Competing Consumer pattern work with ATOM events?
??x
In the Competing Consumer pattern, you bring up multiple worker instances that can all compete for messages. This is useful when you need to scale the number of workers to handle a list of independent jobs efficiently. However, if two or more consumers see the same message, it could lead to redundant processing.

With ATOM, this means managing shared state among all the competing consumers to reduce the likelihood of duplicate work. For example, each consumer needs to keep track of which messages have already been processed and avoid reprocessing them.

```java
// Pseudocode example of implementing Competing Consumer with ATOM
public class WorkerService {
    private MessageBroker messageBroker;
    private Set<String> seenMessages = new HashSet<>();

    public void startConsuming() {
        // Subscribe to the event feed
        messageBroker.subscribe(""event.feed"", this::processEvent);
    }

    private void processEvent(Event event) {
        if (!seenMessages.contains(event.getMessageId())) {
            // Mark as processed before handling
            seenMessages.add(event.getMessageId());
            handleEvent(event);
        }
    }

    private void handleEvent(Event event) {
        // Process the event
        System.out.println(""Handling event: "" + event.getType() + "", ID: "" + event.getMessageId());
        // Perform necessary actions for this event
    }
}

// Example of how workers are started and compete for events
public class Application {
    public static void main(String[] args) {
        List<WorkerService> workers = new ArrayList<>();
        
        for (int i = 0; i < numberOfWorkers; i++) {
            WorkerService worker = new WorkerService();
            // Start the worker to consume events
            worker.startConsuming();
            workers.add(worker);
        }
    }
}
```
x??

---",2232,"Implementing Asynchronous Event-Based Collaboration We’ve talked for a bit about some technologies that can help us implement request/response patterns. What about event-based, asynchronous communicat...",qwen2.5:latest,2025-11-02 09:52:25,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Complexities of Asynchronous Architectures,Asynchronous Architecture Complexity,"---
#### Asynchronous Architecture Complexity
Background context: Asynchronous architectures, such as event-driven systems, can offer decoupled and scalable solutions but introduce complexities that need careful management. These complexities include handling long-running requests, managing message queues, and ensuring reliable processing of messages.

:p What are some challenges associated with asynchronous architectures?
??x
Challenges in asynchronous architectures include:
- Managing long-running async requests: Determining what to do when a response comes back, especially if the original node is no longer available.
- Handling competing consumer patterns: Workers may crash or fail to process messages correctly, leading to potential deadlocks and retry issues.
- Ensuring reliable message processing: Implementing mechanisms like dead letter queues (DLQ) for failed messages.

Code examples:
```java
// Pseudocode for handling a long-running async request
public void handleRequestAsync(String request) {
    try {
        // Logic to initiate the async process
        Future<String> result = processAsync(request);
        
        // Logic to handle the response when it comes back
        String response = result.get();
        log.info(""Received response: "" + response);
    } catch (ExecutionException | InterruptedException e) {
        log.error(""Error processing request"", e);
    }
}
```
x??",1415,"Complexities of Asynchronous Architectures Some of this asynchronous stuff seems fun, right? Event-driven architectures seem to lead to significantly more decoupled, scalable systems. And they can. Bu...",qwen2.5:latest,2025-11-02 09:52:42,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Complexities of Asynchronous Architectures,Catastrophic Failover Example,"#### Catastrophic Failover Example
Background context: The author provides a real-world example from 2006 involving a bank's pricing system, which demonstrates the challenges of asynchronous architectures. A bug caused workers to crash repeatedly due to unhandled messages in a transacted queue.

:p What is a catastrophic failover and how did it manifest in this case?
??x
A catastrophic failover refers to a situation where a message processing system fails repeatedly, leading to continuous retries that can cause more issues. In the example provided:
- A bug caused certain pricing requests to crash workers.
- Transacted queues led to locks timing out and messages being re-added to the queue.
- Other workers would then pick up these failed messages and crash again, creating a cycle.

The issue was resolved by fixing the bug, setting retry limits, and implementing a message hospital (dead letter queue) for failed messages:
```java
// Pseudocode for handling dead letter queues
public void handleFailedMessage(String message) {
    // Check if maximum retries have been reached
    if (!messageProcessor.reachedMaxRetries(message)) {
        messageHospital.enqueue(message);
        log.warn(""Failed to process message, enqueuing in hospital: "" + message);
    } else {
        log.error(""Exceeded max retries for message: "" + message);
    }
}
```
x??",1362,"Complexities of Asynchronous Architectures Some of this asynchronous stuff seems fun, right? Event-driven architectures seem to lead to significantly more decoupled, scalable systems. And they can. Bu...",qwen2.5:latest,2025-11-02 09:52:42,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Complexities of Asynchronous Architectures,Importance of Monitoring and Correlation IDs,"#### Importance of Monitoring and Correlation IDs
Background context: The author emphasizes the importance of having good monitoring tools and using correlation IDs to trace requests across different processes. This is crucial in complex, asynchronous systems.

:p Why are monitoring tools and correlation IDs important in asynchronous architectures?
??x
Monitoring tools help track system performance, detect failures, and ensure that all components are functioning correctly. Correlation IDs allow tracing of individual requests through a system, even when they span multiple services or processes.

For example:
- Monitoring can help identify patterns in failures, resource usage, and latency.
- Correlation IDs can be used to log and trace the flow of messages across different components.

```java
// Example of using correlation IDs for logging
public void processRequest(String request) {
    String corrId = UUID.randomUUID().toString();
    log.info(""Processing request [id: "" + corrId + "", request: "" + request + ""]"");
    
    try {
        // Process the request asynchronously
        processAsync(request, corrId);
    } catch (Exception e) {
        log.error(""Failed to process request [id: "" + corrId + ""]"", e);
    }
}
```
x??

---",1249,"Complexities of Asynchronous Architectures Some of this asynchronous stuff seems fun, right? Event-driven architectures seem to lead to significantly more decoupled, scalable systems. And they can. Bu...",qwen2.5:latest,2025-11-02 09:52:42,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Access by Reference,Services as State Machines,"#### Services as State Machines
Background context: The core idea of services being state machines is crucial for microservices architecture. Each service owns its bounded context and handles all logic related to that context, ensuring coherence. This approach helps manage state changes effectively by centralizing decision-making within the respective service.

:p What does the concept of a ""service as a state machine"" imply in microservices?
??x
This implies that each service manages its own state and lifecycle events based on the bounded context it owns. The service decides whether to accept or reject requests, ensuring all related logic is encapsulated within the service itself.

Example: A customer service handles all operations related to customers.
```java
public class CustomerService {
    public boolean updateCustomerDetails(CustomerRequest request) {
        // Check if the customer exists and can be updated
        if (customerExistsAndNotRemoved(request.customerId)) {
            // Perform the update
            updateCustomer(request);
            return true;
        }
        return false;
    }

    private boolean customerExistsAndNotRemoved(String customerId) {
        // Logic to check if the customer is still active
    }

    private void updateCustomer(CustomerRequest request) {
        // Logic to update customer details
    }
}
```
x??",1381,"Services as State Machines Whether you choose to become a REST ninja, or stick with an RPC-based mechanism like SOAP, the core concept of the service as a state machine is powerful. We’ve spoken befor...",qwen2.5:latest,2025-11-02 09:52:58,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Access by Reference,Reactive Extensions (Rx),"#### Reactive Extensions (Rx)
Background context: Reactive extensions are a mechanism for composing and reacting to multiple calls in an observable manner. They help manage asynchronous operations and handle concurrent calls more efficiently, making the code easier to reason about.

:p What is the main purpose of using reactive extensions in distributed systems?
??x
The main purpose is to abstract away the details of how calls are made and allow developers to focus on reacting to changes rather than managing asynchronous call chains. This makes handling multiple service calls for a single operation much simpler and more manageable.

Example: Observing the result of a downstream service call.
```java
import io.reactivex.rxjava3.core.Observable;

Observable<String> observeServiceCall(String serviceUrl) {
    return Observable.fromCallable(() -> makeServiceCall(serviceUrl));
}

private String makeServiceCall(String url) {
    // Simulated network call
    try {
        Thread.sleep(1000);
    } catch (InterruptedException e) {
        Thread.currentThread().interrupt();
    }
    return ""Response from service"";
}
```
x??",1135,"Services as State Machines Whether you choose to become a REST ninja, or stick with an RPC-based mechanism like SOAP, the core concept of the service as a state machine is powerful. We’ve spoken befor...",qwen2.5:latest,2025-11-02 09:52:58,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Access by Reference,DRY and Code Reuse in Microservices,"#### DRY and Code Reuse in Microservices
Background context: The DRY principle, or don't repeat yourself, is crucial for maintaining a clean and maintainable codebase. In microservices, it's essential to avoid duplicating behavior and knowledge across services while ensuring that the system remains cohesive.

:p How does the DRY principle apply differently in a microservice architecture?
??x
In microservices, DRY applies by avoiding duplicated logic within different services. Instead of replicating functionality in multiple services, common operations can be abstracted into shared libraries or functions. This ensures that changes are made in one place and propagated across services without duplicating effort.

Example: Reusing a function to check customer existence.
```java
public class CustomerService {
    private final boolean checkCustomerExists(String customerId) {
        // Logic to check if the customer exists
    }

    public boolean updateCustomerDetails(CustomerRequest request) {
        if (checkCustomerExists(request.customerId)) {
            // Proceed with updating details
        } else {
            return false;
        }
        return true;
    }
}
```
x??

---",1201,"Services as State Machines Whether you choose to become a REST ninja, or stick with an RPC-based mechanism like SOAP, the core concept of the service as a state machine is powerful. We’ve spoken befor...",qwen2.5:latest,2025-11-02 09:52:58,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Access by Reference,DRY Principle in Microservices,"#### DRY Principle in Microservices
Background context explaining the importance of DRY (Don't Repeat Yourself) principle and how it leads to reusability. However, it also mentions potential pitfalls when used incorrectly in microservice architectures.

:p What does DRY stand for and why is it important?
??x
The DRY principle stands for ""Don't Repeat Yourself."" It emphasizes the importance of reducing duplication in code to avoid errors due to inconsistent changes. When applied correctly, this leads to more maintainable and efficient code by abstracting common behaviors into reusable components.

However, in a microservice architecture, applying DRY across all services can lead to tight coupling between services. This is because any change in shared code may require updates in multiple services, leading to unnecessary disruptions and potential bugs.
x??",865,"When you want to change behavior, and that behavior is duplicated in many parts of your system, it is easy to forget everywhere you need to make a change, which can lead to bugs. So using DRY as a man...",qwen2.5:latest,2025-11-02 09:53:15,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Access by Reference,Coupling in Microservices,"#### Coupling in Microservices
Explanation of the dangers of too much coupling between microservices, especially when shared common domain objects or libraries are used.

:p What can happen if shared common domain objects or libraries are not managed carefully in a microservice architecture?
??x
If shared common domain objects or libraries are not managed carefully, changes to these shared components can cause cascading updates across multiple services. This can lead to issues such as:
- Services needing to be updated and redeployed.
- Message queues becoming invalid due to changed message formats.
- Potential bugs arising from forgotten updates.

For example, if a common domain object in a library is changed, all services using this library will need to be updated, which can disrupt the system's operation and introduce new bugs if not done properly.
x??",866,"When you want to change behavior, and that behavior is duplicated in many parts of your system, it is easy to forget everywhere you need to make a change, which can lead to bugs. So using DRY as a man...",qwen2.5:latest,2025-11-02 09:53:15,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Access by Reference,Client Libraries in Microservices,"#### Client Libraries in Microservices
Explanation of the pros and cons of creating client libraries for microservices, especially regarding logic leakage and technology constraints.

:p What are the potential problems with creating client libraries for your services?
??x
Creating client libraries can lead to several issues:
- **Logic Leakage:** If the same team creates both server APIs and client libraries, there is a risk that logic intended for the server starts leaking into the client library. This can cause maintenance issues where changes need to be rolled out in multiple clients.
- **Technology Constraints:** Mandating the use of specific client libraries can limit technology choices. For instance, if a client library must be used, developers might be forced to stick with certain programming languages or frameworks.

For example, if a server API method requires complex logic that should remain on the server side but is implemented in the client library, changes to this logic would need to be rolled out in all clients, causing unnecessary disruptions.
x??",1077,"When you want to change behavior, and that behavior is duplicated in many parts of your system, it is easy to forget everywhere you need to make a change, which can lead to bugs. So using DRY as a man...",qwen2.5:latest,2025-11-02 09:53:15,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Access by Reference,AWS Model for Client Libraries,"#### AWS Model for Client Libraries
Explanation of Amazon Web Services (AWS) approach towards client libraries and its benefits.

:p What model does AWS use for client libraries, and why is it beneficial?
??x
Amazon Web Services (AWS) uses a community-driven SDK approach. The underlying SOAP or REST web service calls can be made directly, but most users prefer to use one of the various software development kits (SDKs) provided by AWS.

These SDKs offer abstractions over the underlying API and are developed independently of the server API team. This separation ensures that:
- Server logic remains on the server side.
- Client libraries do not become overly complex or tightly coupled with the server.
- Flexibility in technology choices is maintained.

For instance, using an AWS SDK for Java or Python allows developers to leverage rich abstractions while keeping their codebase clean and maintainable. This approach minimizes the risk of logic leakage and maintains a clear separation between client and server responsibilities.
x??

---",1045,"When you want to change behavior, and that behavior is duplicated in many parts of your system, it is easy to forget everywhere you need to make a change, which can lead to bugs. So using DRY as a man...",qwen2.5:latest,2025-11-02 09:53:15,7
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Access by Reference,Client Library Approach and Its Pitfalls,"#### Client Library Approach and Its Pitfalls

Background context: The client library approach is a method used to manage service communication in distributed systems, particularly at large scales like those seen by Netflix. It involves encapsulating common functionalities such as service discovery, failure handling, logging, etc., within the client code. This ensures that these aspects are consistently managed across different services, avoiding individual re-implementations and promoting reliability and scalability.

If applicable, add code examples with explanations:
```java
// Pseudocode for a simple service client library
public class ServiceClient {
    public void upgradeService() { /* logic to handle upgrades */ }
    public boolean handleFailureMode() { /* logic to manage failures */ }
}
```

:p What are the key benefits of using a client library in distributed systems?
??x
Using a client library in distributed systems ensures reliability and scalability by handling common functionalities such as service discovery, failure modes, logging, etc., consistently across different services. This approach minimizes code duplication and enhances maintainability.
x??",1184,"This degree of separation seems to work, and avoids some of the pitfalls of client libraries. Part of the reason this works so well is that the client is in charge of when the upgrade happens. If you ...",qwen2.5:latest,2025-11-02 09:53:34,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Access by Reference,Client Library vs. Client Code Independence,"#### Client Library vs. Client Code Independence

Background context: The separation of client code from the underlying transport protocol is crucial to ensure that services can be upgraded independently without affecting each other. Netflix places significant emphasis on maintaining this independence, as over time, tight coupling between clients and servers can lead to problematic dependencies.

:p Why is it important for clients to manage when they upgrade their libraries?
??x
Clients should manage when they upgrade their client libraries to ensure that services can be released independently of each other. This separation allows for more flexible deployment strategies and reduces the risk of cascading failures during updates.
x??",741,"This degree of separation seems to work, and avoids some of the pitfalls of client libraries. Part of the reason this works so well is that the client is in charge of when the upgrade happens. If you ...",qwen2.5:latest,2025-11-02 09:53:34,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Access by Reference,Passing Domain Entities by Reference,"#### Passing Domain Entities by Reference

Background context: In microservices architecture, domain entities such as `Customer` should have their lifecycle managed within a single service to maintain consistency and integrity. When retrieving data, it is important to consider the possibility that other services might have modified the entity since the last retrieval.

:p How can you ensure your system has up-to-date information about a domain entity?
??x
To ensure your system has up-to-date information about a domain entity, pass around references to the original resources instead of complete copies. Include this reference in any entity representations so that if changes are needed, the latest state can be retrieved from the source service.

For example:
```java
public class CustomerReference {
    private String customerId;
    public CustomerReference(String customerId) { this.customerId = customerId; }
    public String getCustomerId() { return customerId; }
}
```
x??",986,"This degree of separation seems to work, and avoids some of the pitfalls of client libraries. Part of the reason this works so well is that the client is in charge of when the upgrade happens. If you ...",qwen2.5:latest,2025-11-02 09:53:34,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Access by Reference,Managing Memory and State Consistency,"#### Managing Memory and State Consistency

Background context: In microservices, maintaining the integrity of domain entities is crucial. When you retrieve an entity from a service, it's important to understand that other services might have updated this entity since your last retrieval. Keeping a memory of what the entity looked like when you requested it can be useful but risks becoming stale.

:p What should you do to ensure you have the most recent state of a domain entity?
??x
To ensure you have the most recent state of a domain entity, pass around references to the original resources rather than complete copies. Include this reference in any entity representations so that if changes are needed, the latest state can be retrieved from the source service.

For example:
```java
public class Customer {
    private String id;
    private String name;
    public Customer(String id, String name) { this.id = id; this.name = name; }
    
    // Getter for ID to pass as a reference
    public String getId() { return id; }
}
```
x??

---",1048,"This degree of separation seems to work, and avoids some of the pitfalls of client libraries. Part of the reason this works so well is that the client is in charge of when the upgrade happens. If you ...",qwen2.5:latest,2025-11-02 09:53:34,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Access by Reference,Resource Referencing vs. Including Full Data,"#### Resource Referencing vs. Including Full Data
In scenarios where services need to interact, it's often debated whether to send full data or just a reference. The decision hinges on performance, coupling, and the freshness of data.

:p How does sending a URI for Customer and Order resources instead of including all details affect service interactions?
??x
Sending a URI can reduce the load on the originating service by deferring the lookup of detailed information to the time when it is actually needed. This approach minimizes the amount of data passed in initial requests, which can be beneficial especially if dealing with many concurrent updates.

Example: 
If an email service is triggered upon shipping an order and needs customer details to compose an email, sending a URI for the Customer resource allows the email service to fetch the latest details when it's ready to send the email. This reduces the initial request payload but requires additional queries later.
```java
public class EmailService {
    public void sendEmailWhenOrderShipped(String customerId) {
        // Fetch customer details and order status from their respective services
        CustomerDetails customer = customerService.getCustomer(customerId);
        OrderStatus orderStatus = orderService.getOrderStatus(customerId);
        
        // Compose the email content using customer name, etc.
        String emailContent = ""Dear "" + customer.getName() + "", your order has been shipped. Details: "" + orderStatus;
        sendEmail(customer.getEmail(), emailContent);
    }

    private void sendEmail(String emailAddress, String content) {
        // Logic to send the email
    }
}
```
x??",1680,"Let’s consider the example where we ask the email service to send an email when an order has been shipped. Now we could send in the request to the email service with the customer’s email address, name...",qwen2.5:latest,2025-11-02 09:53:57,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Access by Reference,Event-Based Collaboration and Entity Updates,"#### Event-Based Collaboration and Entity Updates
When using event-based collaboration, it's crucial to understand not only that something has happened but also what exactly changed. This approach requires careful handling of entity states.

:p Why is it important to know both ""this happened"" and ""what happened"" in an event-driven system?
??x
In an event-driven architecture, simply knowing that a certain event occurred might not be sufficient. The state or context under which the event took place is also critical. For instance, if a customer's address changes, it’s important to know what the previous address was and when this change happened.

Example: 
Consider an event triggered by a customer changing their payment method. While knowing that the event occurred is important, understanding the specifics of the change (e.g., from card to PayPal) can help in managing user experience or performing additional checks.
```java
public class CustomerEvent {
    private String customerId;
    private EventTimestamp timestamp;
    private ChangeDetails changeDetails;

    public CustomerEvent(String customerId, EventTimestamp timestamp, ChangeDetails changeDetails) {
        this.customerId = customerId;
        this.timestamp = timestamp;
        this.changeDetails = changeDetails;
    }

    // Getters and setters
}

public class ChangeDetails {
    private String previousPaymentMethod;
    private String newPaymentMethod;

    public ChangeDetails(String previousPaymentMethod, String newPaymentMethod) {
        this.previousPaymentMethod = previousPaymentMethod;
        this.newPaymentMethod = newPaymentMethod;
    }
}
```
x??",1647,"Let’s consider the example where we ask the email service to send an email when an order has been shipped. Now we could send in the request to the email service with the customer’s email address, name...",qwen2.5:latest,2025-11-02 09:53:57,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Access by Reference,Caching and Freshness of Data,"#### Caching and Freshness of Data
Caching is a common solution to reduce the load on services by storing recent data. However, managing cache freshness can be complex.

:p How does HTTP support caching in a service-oriented architecture?
??x
HTTP provides several mechanisms for controlling and managing cache freshness through headers such as `Cache-Control`, `ETag`, and `Last-Modified`. These headers allow services to specify how long cached content should remain fresh and under what conditions the cache should be invalidated.

Example: 
Using HTTP Cache Control Headers:
```java
import javax.servlet.http.HttpServletResponse;

public class CustomerController {
    @GetMapping(""/customer/{id}"")
    public ResponseEntity<Customer> getCustomer(@PathVariable Long id) {
        // Retrieve customer from database or service
        Customer customer = customerService.getCustomerById(id);
        
        // Set cache control headers to allow caching for 10 minutes
        HttpServletResponse response = (HttpServletResponse) HttpContext.current().getResponse();
        response.setHeader(""Cache-Control"", ""public, max-age=600""); // Cache for 10 minutes
        
        return ResponseEntity.ok(customer);
    }
}
```
x??",1231,"Let’s consider the example where we ask the email service to send an email when an order has been shipped. Now we could send in the request to the email service with the customer’s email address, name...",qwen2.5:latest,2025-11-02 09:53:57,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Access by Reference,Reducing Coupling with Dumb Services,"#### Reducing Coupling with Dumb Services
In some architectures, dumb services are preferred over smart services that handle more logic. This can reduce coupling and simplify service interactions.

:p Why might an email service be designed to be ""dumb"" instead of receiving a full customer profile?
??x
Designing the email service to be ""dumb"" means it receives only necessary data like the customer’s name and email address, without detailed information about the customer. This approach minimizes coupling between services and reduces the risk of one service becoming dependent on another.

Example: 
An email service that receives minimal details:
```java
public class EmailService {
    public void sendEmail(String toAddress, String subject, String body) {
        // Logic to send an email using the provided information
    }
}

// Usage in another service or controller
public class OrderShipmentService {
    private final EmailService emailService;

    public OrderShipmentService(EmailService emailService) {
        this.emailService = emailService;
    }

    public void handleOrderShipped(Order order, Customer customer) {
        String subject = ""Your Order Has Been Shipped"";
        String body = ""Dear "" + customer.getName() + "", your order with ID "" + order.getId() + "" has been shipped."";
        
        // Send the email using minimal data
        emailService.sendEmail(customer.getEmail(), subject, body);
    }
}
```
x??

---",1454,"Let’s consider the example where we ask the email service to send an email when an order has been shipped. Now we could send in the request to the email service with the customer’s email address, name...",qwen2.5:latest,2025-11-02 09:53:57,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Defer It for as Long as Possible,Defer Breaking Changes as Long as Possible,"---
#### Defer Breaking Changes as Long as Possible
Background context: The best way to reduce the impact of making breaking changes is to avoid them altogether. This can be achieved by picking the right integration technology and encouraging good behavior from clients.

:p How can you defer a breaking change in a service?
??x
To defer a breaking change, you should design your service interfaces to be as flexible as possible so that even when internal structures or data fields are altered, external consumers are unaffected. This is achieved by using technologies that allow for loose coupling between services and clients.

For example, instead of tightly binding all fields from the database response, use a reader (like XPath in XML) that can selectively extract only necessary fields.
```xml
// Example of an XPATH query to extract needed fields
String xpathQuery = ""/customer/(firstname | lastname | email)"";
```
x??",926,"Versioning In every single talk I have ever done about microservices, I get asked  how do you do versioning?  People have the legitimate concern that eventually they will have to make a change to the ...",qwen2.5:latest,2025-11-02 09:54:17,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Defer It for as Long as Possible,Tolerant Reader Pattern,"#### Tolerant Reader Pattern
Background context: A tolerant reader is designed to ignore changes it doesn't care about, allowing the service provider to modify internal structures without breaking external consumers. This pattern aligns with Martin Fowler's principle of building services that can tolerate changes in their input/output.

:p What is a tolerant reader and how does it work?
??x
A tolerant reader is a design pattern where the consumer reads data from a service in such a way that it gracefully handles any changes to the format or structure of the data provided by the service. The idea is to make the reader as flexible as possible, so it can continue functioning even if the provider introduces new fields or restructures existing ones.

For instance, consider an email sending service that needs only `firstname`, `lastname`, and `email` from a customer's data. Instead of expecting these specific elements in a fixed location, use XPath (or equivalent) to dynamically locate them.
```xml
// Example using XPath
String xml = ""<customer>    <firstname> Sam </firstname>    <lastname> Newman </lastname>    <email> sam@magpiebrain.com </email>    <telephoneNumber> 555-1234-5678 </telephoneNumber></customer>"";
XPath xPath = XPathFactory.newInstance().newXPath();
String firstname = xPath.evaluate(""//firstname"", xml);
String lastname = xPath.evaluate(""//lastname"", xml);
String email = xPath.evaluate(""//email"", xml);
```
x??",1443,"Versioning In every single talk I have ever done about microservices, I get asked  how do you do versioning?  People have the legitimate concern that eventually they will have to make a change to the ...",qwen2.5:latest,2025-11-02 09:54:17,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Defer It for as Long as Possible,Postel's Law (Robustness Principle),"#### Postel's Law (Robustness Principle)
Background context: This principle, also known as the robustness principle or ""Be conservative in what you do, be liberal in what you accept from others,"" is a fundamental guideline for designing and interacting with systems that can tolerate changes. It applies particularly well to RESTful APIs where services might evolve over time.

:p What does Postel's Law state and how can it help manage versioning?
??x
Postel's Law states that software should be as conservative as possible in its behavior (i.e., strictly adhere to the defined protocol or specification) while being as liberal as possible in interpreting data received from others. In the context of managing microservices, this means designing services and clients to handle unexpected changes gracefully.

By following Postel’s Law, a service can remain robust even if an external client introduces new fields or modifies existing ones unexpectedly. This flexibility helps manage versioning more effectively by reducing the likelihood of breaking changes affecting downstream consumers.
```java
// Example of a Java method that follows Postel's Law
public void sendEmail(String customerId) {
    Customer customer = customerService.getCustomerById(customerId);
    String firstName = extractValue(customer, ""firstname"");
    String lastName = extractValue(customer, ""lastname"");
    String email = extractValue(customer, ""email"");

    // Send the email using the extracted values
}
```
x??

---",1499,"Versioning In every single talk I have ever done about microservices, I get asked  how do you do versioning?  People have the legitimate concern that eventually they will have to make a change to the ...",qwen2.5:latest,2025-11-02 09:54:17,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Coexist Different Endpoints,Catch Breaking Changes Early,"#### Catch Breaking Changes Early
Background context: It is essential to detect breaking changes early in a service's lifecycle. Consumer-driven contracts and running tests with client libraries can help identify these issues before they affect consumers.

:p How can you catch breaking changes early?
??x
To catch breaking changes early, use consumer-driven contracts as described in Chapter 7. Additionally, run integration tests using each supported client library against the latest service to ensure compatibility.
??x",523,"Catch Breaking Changes Early It’s crucial to make sure we pick up changes that will break consumers as soon as possible, because even if we choose the best possible technology, breaks can still happen...",qwen2.5:latest,2025-11-02 09:54:32,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Coexist Different Endpoints,Use Semantic Versioning,"#### Use Semantic Versioning
Background context: Semantic versioning is a specification that helps clients understand what changes will be backward compatible with their applications. The version number format (MAJOR.MINOR.PATCH) provides clear expectations.

:p What is semantic versioning and how does it work?
??x
Semantic versioning specifies the MAJOR, MINOR, and PATCH version numbers to indicate types of changes:
- Major: Backward incompatible changes.
- Minor: New functionality that should be backward compatible.
- Patch: Bug fixes for existing functionality.

Example use case:
If our helpdesk application works with customer service v1.2.0, it can safely upgrade to v1.3.0 (new minor version) but not v1.1.0 (backward incompatible changes). It may need changes when upgrading to v2.0.0.
??x",803,"Catch Breaking Changes Early It’s crucial to make sure we pick up changes that will break consumers as soon as possible, because even if we choose the best possible technology, breaks can still happen...",qwen2.5:latest,2025-11-02 09:54:32,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Coexist Different Endpoints,Coexist Different Endpoints,"#### Coexist Different Endpoints
Background context: Coexisting different versions of endpoints allows gradual migration of consumers while maintaining the ability to release microservices independently.

:p How can you handle coexistence of different endpoint versions?
??x
To handle coexistence, deploy a new version of the service that exposes both old and new interfaces. Gradually transition consumers from using the old interface to the new one. Once all consumers are no longer using the old endpoint, remove it.
??x",523,"Catch Breaking Changes Early It’s crucial to make sure we pick up changes that will break consumers as soon as possible, because even if we choose the best possible technology, breaks can still happen...",qwen2.5:latest,2025-11-02 09:54:32,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Coexist Different Endpoints,Transforming Requests for Coexisting Endpoints,"#### Transforming Requests for Coexisting Endpoints
Background context: When coexisting different endpoint versions, a transformation layer can help manage transitions smoothly.

:p How does transforming requests work in coexisting endpoints?
??x
Transforming requests involves mapping older API calls to newer ones. For example, internally transform all V1 requests to V2 and then to the final version as consumers migrate.
??x",428,"Catch Breaking Changes Early It’s crucial to make sure we pick up changes that will break consumers as soon as possible, because even if we choose the best possible technology, breaks can still happen...",qwen2.5:latest,2025-11-02 09:54:32,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Coexist Different Endpoints,URI vs Header for Routing Requests,"#### URI vs Header for Routing Requests
Background context: Deciding between using URIs or request headers for routing requests can impact design choices based on how clients interact with services.

:p How do you route requests when coexisting different endpoint versions?
??x
For HTTP, use both version numbers in request headers (e.g., `X-API-Version`) and in the URI path (e.g., `/v1/customer/` or `/v2/customer/`). For RPC, methods can be namespaced differently (e.g., `v1.createCustomer` and `v2.createCustomer`).
??x",523,"Catch Breaking Changes Early It’s crucial to make sure we pick up changes that will break consumers as soon as possible, because even if we choose the best possible technology, breaks can still happen...",qwen2.5:latest,2025-11-02 09:54:32,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Use Multiple Concurrent Service Versions,Multiple Concurrent Service Versions,"#### Multiple Concurrent Service Versions
Background context: This versioning strategy involves running different versions of a service simultaneously to support both old and new consumers. It is used by Netflix for legacy devices or when the cost of changing older consumers is too high.

:p What are the challenges associated with using multiple concurrent service versions?
??x
The challenges include:
- Needing to fix and deploy two sets of services for internal bug fixes, which can require branching the codebase—always problematic.
- The necessity to implement smart routing mechanisms to direct requests to the appropriate version of the service. This often results in additional complexity, with smarts sitting in middleware or nginx scripts.
- Handling persistent state management; customers created by either version need visibility to all services regardless of which version was used.

Code examples are not directly applicable here as they pertain more to architectural and deployment strategies rather than specific coding tasks:
```java
// Pseudocode for branching logic (simplified)
if (consumerVersion == ""old"") {
    routeTo(oldService);
} else if (consumerVersion == ""new"") {
    routeTo(newService);
}
```
x??",1230,"Use Multiple Concurrent Service Versions Another versioning solution often cited is to have different versions of the service live at once, and for older consumers to route their traffic to the older ...",qwen2.5:latest,2025-11-02 09:54:53,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Use Multiple Concurrent Service Versions,Blue/Green Deployments,"#### Blue/Green Deployments
Background context: This is a deployment strategy where both the old and new versions of an application run in parallel, with traffic directed to one or the other. It allows for a seamless transition without downtime.

:p What are the key characteristics of blue/green deployments?
??x
Blue/green deployments involve:
- Running two identical environments side by side (blue and green).
- Initially directing all production traffic to the ""green"" environment, which is considered live.
- Slowly shifting traffic from the ""blue"" to the ""green"" environment as testing progresses.
- Once fully validated, switching all traffic to the new ""green"" version while marking the old one as decommissioned.

Code examples are not directly applicable here but can be conceptual:
```java
// Pseudocode for blue/green deployment logic (simplified)
if (environment == ""blue"") {
    useBlueEnvironment();
} else if (environment == ""green"") {
    useGreenEnvironment();
}
```
x??",989,"Use Multiple Concurrent Service Versions Another versioning solution often cited is to have different versions of the service live at once, and for older consumers to route their traffic to the older ...",qwen2.5:latest,2025-11-02 09:54:53,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Use Multiple Concurrent Service Versions,Canary Releases,"#### Canary Releases
Background context: This is a form of controlled release where a new version of an application is first deployed to a small subset of users or nodes. Feedback from this group helps in determining whether the release can be expanded.

:p What are the key characteristics of canary releases?
??x
Canary releases involve:
- Deploying the new version to a small, manageable segment of users or nodes.
- Monitoring the behavior and performance of these users/nodes closely.
- Gradually increasing the proportion of users on the new version based on observed results.
- Allowing quick rollback if issues are identified.

Code examples can be used to illustrate the logic:
```java
// Pseudocode for canary release logic (simplified)
if (userBelongsToCanaryGroup) {
    serveNewVersion();
} else {
    serveOldVersion();
}
```
x??

---",848,"Use Multiple Concurrent Service Versions Another versioning solution often cited is to have different versions of the service live at once, and for older consumers to route their traffic to the older ...",qwen2.5:latest,2025-11-02 09:54:53,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,UI Fragment Composition,Evolution of User Interfaces,"#### Evolution of User Interfaces

Background context: The evolution of user interfaces (UIs) from desktop applications to web and mobile UIs has seen significant changes. Initially, UIs were built using technologies like Motif and Swing for desktop applications with some server-side components. With the rise of the Web, UIs became more ""thin"" with more logic on the server side handling interactions via GET and POST requests.

:p How did early desktop application user interfaces differ from web-based UIs in terms of interaction logic?
??x
In early desktop application development using technologies like Motif and Swing, most of the logic was handled locally. The client (desktop) would interact directly with the user interface without heavy reliance on server-side processing. This meant that interactions were mostly managed by event handling within the application itself.

For example:
```java
// Example in Java using Swing to handle mouse click events
public class MyFrame extends JFrame {
    public MyFrame() {
        addMouseListener(new MouseAdapter() {
            @Override
            public void mouseClicked(MouseEvent e) {
                System.out.println(""Mouse clicked at: "" + e.getX() + "", "" + e.getY());
            }
        });
    }
}
```
x??",1275,"User Interfaces So far, we haven’t really touched on the world of the user interface. A few of you out there might just be providing a cold, hard, clinical API to your customers, but many of us find o...",qwen2.5:latest,2025-11-02 09:55:14,2
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,UI Fragment Composition,Web-based User Interfaces,"#### Web-based User Interfaces

Background context: Early web-based UIs relied heavily on server-side rendering, where the entire page was generated and sent to the client browser. Over time, with the rise of JavaScript, more dynamic behavior could be added to web applications, making them feel ""fatter"" or more similar to desktop applications.

:p How did the role of JavaScript change in web-based user interfaces?
??x
JavaScript's role evolved from being just a tool for enhancing form validations and simple client-side interactions to becoming an essential component for creating dynamic and interactive web applications. It allowed developers to handle complex UI interactions directly on the browser, reducing the load on the server.

For example:
```javascript
// Example of using JavaScript to dynamically update content based on user interaction
document.getElementById(""myButton"").addEventListener(""click"", function() {
    var element = document.getElementById(""content"");
    if (element.style.display === ""none"") {
        element.style.display = ""block"";
    } else {
        element.style.display = ""none"";
    }
});
```
x??",1141,"User Interfaces So far, we haven’t really touched on the world of the user interface. A few of you out there might just be providing a cold, hard, clinical API to your customers, but many of us find o...",qwen2.5:latest,2025-11-02 09:55:14,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,UI Fragment Composition,Towards a Holistic Digital Experience,"#### Towards a Holistic Digital Experience

Background context: The shift towards digital experiences involves considering multiple touchpoints such as desktop applications, mobile devices, wearables, and physical stores. Microservices enable flexibility in combining services to cater to these diverse interaction points.

:p How does the adoption of microservices contribute to creating a holistic digital experience?
??x
Microservices allow for modular service design, where each service can be independently developed, scaled, and updated. By exposing granular APIs through microservices, organizations can combine different capabilities in various ways to create tailored experiences across multiple touchpoints.

For example:
```java
// Pseudocode to illustrate combining services from microservices
public class DigitalExperienceService {
    private OrderService orderService;
    private CustomerProfileService customerProfileService;

    public void createOrder(int userId, String product) {
        // Get customer profile data
        Map<String, Object> customerData = customerProfileService.getCustomerProfile(userId);

        // Place an order with additional details
        OrderRequest request = new OrderRequest();
        request.setProduct(product);
        request.setCustomerName((String) customerData.get(""name""));
        orderService.placeOrder(request);
    }
}
```
x??",1398,"User Interfaces So far, we haven’t really touched on the world of the user interface. A few of you out there might just be providing a cold, hard, clinical API to your customers, but many of us find o...",qwen2.5:latest,2025-11-02 09:55:14,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,UI Fragment Composition,Constraints in User Interface Design,"#### Constraints in User Interface Design

Background context: Understanding and addressing constraints such as browser compatibility, screen resolution, network bandwidth, battery life, etc., is crucial for designing a robust user interface. These constraints can significantly impact the performance and usability of web applications on different devices.

:p What are some common constraints to consider when designing a web-based user interface?
??x
Common constraints in designing web-based UIs include browser compatibility, screen resolution, network bandwidth, battery life, and data usage. Each of these factors can affect how an application performs and uses resources.

For example:
- **Browser Compatibility**: Ensuring that the application works across different browsers (e.g., Chrome, Firefox, Safari).
- **Screen Resolution**: Designing layouts that adapt to various screen sizes.
- **Network Bandwidth**: Optimizing content delivery to ensure fast loading times even on slower networks.
```java
// Example of using media queries in CSS for responsive design
@media only screen and (max-width: 600px) {
    /* Styles for small screens */
}
```
x??",1163,"User Interfaces So far, we haven’t really touched on the world of the user interface. A few of you out there might just be providing a cold, hard, clinical API to your customers, but many of us find o...",qwen2.5:latest,2025-11-02 09:55:14,2
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,UI Fragment Composition,Tablet Interaction Constraints,"#### Tablet Interaction Constraints
Background context: The nature of user interactions varies across different devices. For instance, right-clicking is not as straightforward on a tablet compared to traditional desktop computers. This necessitates adapting interfaces for various constraints such as single-handed operation or limited bandwidth access.

:p How does the interaction differ between tablets and traditional desktops?
??x
The interaction differs because tablets lack some of the mouse functionalities like right-click, which are easily accessible on desktops. Tablet interactions might require touch gestures or other forms of input that do not rely on a mouse.
x??",679,"The nature of interactions changes, too. I can’t easily right-click on a tablet. On a mobile phone, I may want to design my interface to be used mostly one-handed, with most operations being controlle...",qwen2.5:latest,2025-11-02 09:55:34,4
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,UI Fragment Composition,Mobile Interaction Constraints,"#### Mobile Interaction Constraints
Background context: On mobile phones, user interfaces need to be designed for one-handed use and primarily controlled by a thumb. This design constraint is crucial to ensure usability and accessibility.

:p How should the interface be designed for mobile devices?
??x
The interface should be optimized for one-handed use, placing key controls within easy reach of the thumb. Design elements like buttons, menus, and input fields need to be easily accessible without requiring multiple taps or complex gestures.
x??",550,"The nature of interactions changes, too. I can’t easily right-click on a tablet. On a mobile phone, I may want to design my interface to be used mostly one-handed, with most operations being controlle...",qwen2.5:latest,2025-11-02 09:55:34,3
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,UI Fragment Composition,SMS as an Interface in Developing Regions,"#### SMS as an Interface in Developing Regions
Background context: In regions with limited bandwidth, using SMS as a user interface can be more effective than relying on high-bandwidth services. This approach ensures that users can still access essential services even when internet connectivity is poor.

:p Why might SMS be used as an interface in developing regions?
??x
SMS is used as an interface in developing regions due to its reliability and low bandwidth requirements. It allows users to interact with services through text messages, which are less dependent on stable or high-speed internet connections.
x??",618,"The nature of interactions changes, too. I can’t easily right-click on a tablet. On a mobile phone, I may want to design my interface to be used mostly one-handed, with most operations being controlle...",qwen2.5:latest,2025-11-02 09:55:34,4
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,UI Fragment Composition,API Composition for User Interfaces,"#### API Composition for User Interfaces
Background context: When designing user interfaces, it’s important to leverage existing APIs that services use for communication. This approach can make development more efficient and ensure consistency in data retrieval.

:p How can a web-based UI interact with APIs?
??x
A web-based UI can interact with APIs by using JavaScript GET or POST requests to retrieve or modify data. For example, the UI could send an HTTP request to fetch customer records from a service API and display them appropriately on the page.
```javascript
// Example of making an HTTP GET request in JavaScript
fetch('https://api.example.com/customers/123')
  .then(response => response.json())
  .then(data => {
    // Update UI with fetched data
  });
```
x??",776,"The nature of interactions changes, too. I can’t easily right-click on a tablet. On a mobile phone, I may want to design my interface to be used mostly one-handed, with most operations being controlle...",qwen2.5:latest,2025-11-02 09:55:34,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,UI Fragment Composition,UI Fragment Composition,"#### UI Fragment Composition
Background context: Instead of the UI interacting directly with APIs and mapping responses to controls, services can provide pre-constructed UI fragments that are combined by the UI. This approach reduces complexity in handling API responses.

:p How does UI fragment composition work?
??x
UI fragment composition involves services providing specific parts (fragments) of the user interface which are then integrated into a larger UI. For example, a recommendation service might provide a widget that can be embedded within other controls to create a comprehensive user experience.
```java
// Pseudocode for integrating a recommendation fragment in Java
public class UserInterface {
    private RecommendationWidget widget;

    public UserInterface() {
        // Initialize and add the recommendation widget
        this.widget = new RecommendationWidget();
    }
}
```
x??",904,"The nature of interactions changes, too. I can’t easily right-click on a tablet. On a mobile phone, I may want to design my interface to be used mostly one-handed, with most operations being controlle...",qwen2.5:latest,2025-11-02 09:55:34,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,UI Fragment Composition,API Gateway as an Intermediate Layer,"#### API Gateway as an Intermediate Layer
Background context: An API gateway can help manage and optimize communication between services and the user interface, especially when dealing with multiple APIs. It aggregates requests to reduce the number of direct calls, thereby improving efficiency.

:p What role does an API gateway play?
??x
An API gateway acts as a middleware that receives requests from the UI, processes them by aggregating multiple service calls, and then sends back a unified response. This helps in reducing the load on mobile devices and optimizing network usage.
```java
// Example of an API Gateway handling multiple requests
public class ApiGateway {
    public Response handleRequest(Request request) {
        // Process the request to aggregate necessary data from services
        List<ServiceResponse> responses = service1.fetchData() + service2.fetchData();
        
        return new Response(responses);
    }
}
```
x??

---",958,"The nature of interactions changes, too. I can’t easily right-click on a tablet. On a mobile phone, I may want to design my interface to be used mostly one-handed, with most operations being controlle...",qwen2.5:latest,2025-11-02 09:55:34,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,UI Fragment Composition,Coarse-Grained Fragments Serving UI Components,"#### Coarse-Grained Fragments Serving UI Components

Background context explaining the concept. This approach involves serving up coarser-grained parts of a user interface (UI) from server-side applications, which then make appropriate API calls to assemble these fragments on the client side. The key is that each fragment can be managed by different teams based on their area of responsibility.

:p What is the main idea behind serving coarse-grained UI components?
??x
The main idea involves breaking down the user interface into larger, more manageable parts (panes or pages) that are served up from server-side applications. Each part can be developed and maintained independently by a specific team according to their area of expertise.
```java
// Example of a simple server-side method serving an order management page
public String getOrderManagementPage() {
    // Logic to fetch necessary data for the page
    // ...
    
    // Return the HTML content as a string or render it directly
    return ""<h1>Order Management</h1><p>...</p>"";
}
```
x??",1057,It might get rendered as a box on a web page along with other content. A variation of this approach that can work well is to assemble a series of coarser-grained parts of a UI. So rather than creating...,qwen2.5:latest,2025-11-02 09:56:09,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,UI Fragment Composition,Assembly Layer,"#### Assembly Layer

Background context explaining the concept. Since each coarse-grained fragment can come from different server-side applications, an assembly layer is necessary to combine these fragments into a complete user interface. This could be achieved through server-side templating or smart URI routing.

:p What does the assembly layer do in this approach?
??x
The assembly layer combines individual UI components (coarse-grained parts) served by different server-side applications. It ensures that these components are integrated seamlessly to form a cohesive user experience, often using techniques like server-side templates or intelligent URL routing.
```java
// Example of a simple server-side template engine integration
public String assembleUI(String headerTemplate, String contentTemplate) {
    // Render the header and content using the provided templates
    String header = renderTemplate(headerTemplate);
    String content = renderTemplate(contentTemplate);
    
    // Assemble the complete UI page
    return ""<html><head>...</head><body>"" + header + content + ""</body></html>"";
}
```
x??",1117,It might get rendered as a box on a web page along with other content. A variation of this approach that can work well is to assemble a series of coarser-grained parts of a UI. So rather than creating...,qwen2.5:latest,2025-11-02 09:56:09,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,UI Fragment Composition,Ensuring Consistency of User Experience,"#### Ensuring Consistency of User Experience

Background context explaining the concept. While serving coarse-grained fragments allows for better team ownership and faster updates, it can lead to inconsistencies in the user experience if not managed properly.

:p How do we address the issue of inconsistent user experiences?
??x
To ensure a consistent user experience despite using different server-side applications to serve UI components, techniques like living style guides are employed. These guidebooks or tools provide shared assets such as HTML components, CSS stylesheets, and images that help maintain uniformity across the interface.

```java
// Example of a simple living style guide concept
public class StyleGuide {
    public String getComponent(String componentName) {
        // Return pre-defined HTML component with consistent styling
        return ""<div class='component'>"" + componentName + ""</div>"";
    }
}
```
x??",938,It might get rendered as a box on a web page along with other content. A variation of this approach that can work well is to assemble a series of coarser-grained parts of a UI. So rather than creating...,qwen2.5:latest,2025-11-02 09:56:09,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,UI Fragment Composition,Native Applications and Hybrid Approaches,"#### Native Applications and Hybrid Approaches

Background context explaining the concept. The approach of serving UI components through server-side applications may not be suitable for native applications or thick clients, as these require more control over the user interface.

:p What challenges arise when using this approach in native applications?
??x
Challenges include the inability to serve UI components directly due to the nature of native applications. A hybrid approach might involve serving HTML components from a web-based application and integrating them into the native app environment, but this can introduce complexities and performance issues.

```java
// Example of hybrid approach (simplified)
public void integrateWebComponentIntoNativeApp(String componentHTML) {
    // Logic to embed or render the provided HTML in the native app UI
    // ...
}
```
x??",878,It might get rendered as a box on a web page along with other content. A variation of this approach that can work well is to assemble a series of coarser-grained parts of a UI. So rather than creating...,qwen2.5:latest,2025-11-02 09:56:09,7
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,UI Fragment Composition,Cross-Cutting Interactions,"#### Cross-Cutting Interactions

Background context explaining the concept. In some cases, interactions are so cross-cutting that they do not fit neatly into widgets or pages managed by server-side applications.

:p Why does a service's capabilities sometimes not fit into widgets or pages?
??x
When an interaction is too complex or pervasive to be contained within a single widget or page, it may not align well with the coarse-grained UI component approach. For instance, dynamic recommendations that need to update in real-time across multiple points of interaction (like auto-suggestions during searches) are challenging to implement using this method.

```java
// Example of handling cross-cutting interactions (pseudo-code)
public void handleDynamicRecommendation(String userQuery) {
    // Fetch relevant recommendations based on the query
    List<String> recommendations = fetchRecommendations(userQuery);
    
    // Update UI elements that trigger these recommendations in real-time
    updateUI(recommendations);
}
```
x??

---",1039,It might get rendered as a box on a web page along with other content. A variation of this approach that can work well is to assemble a series of coarser-grained parts of a UI. So rather than creating...,qwen2.5:latest,2025-11-02 09:56:09,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Backends for Frontends,Backends for Frontends (BFFs),"#### Backends for Frontends (BFFs)
Background context: A common approach to manage interactions between front-end interfaces and back-end services is through an API gateway. However, this can lead to complications when these gateways become too complex, leading to issues with maintaining and updating them independently.

The objective of BFFs is to provide a more granular and isolated approach where each specific user interface has its dedicated backend service. This allows teams working on different UIs to handle their own server-side components without interfering with others.

:p What are the main benefits of using Backends for Frontends (BFFs)?
??x
The main benefits include maintaining isolation between different front-end interfaces, allowing for independent deployment and updates of each UI component. It also ensures that business logic remains within services themselves rather than in complex gateways, making it easier to manage.

For example:
- If a mobile app needs specific API calls, a BFF can be created just for the mobile application.
- A desktop version might need different APIs or additional functionalities, which would have its own BFF.
```java
public class MobileBff {
    public String fetchUserDetails(String userId) {
        // Logic to fetch user details from backend and format them for the mobile app
    }
}

public class DesktopBff {
    public String fetchUserSettings(String userId) {
        // Logic to fetch settings tailored for desktop users
    }
}
```
x??",1507,"Backends for Frontends A common solution to the problem of chatty interfaces with backend services, or the need to vary content for different types of devices, is to have a server-side aggregation end...",qwen2.5:latest,2025-11-02 09:56:36,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Backends for Frontends,Monolithic Gateway Issue,"#### Monolithic Gateway Issue
Background context: Using a single monolithic gateway can lead to several problems, including tightly coupling all services and making it difficult to update or modify individual components independently.

:p What problem does the use of a single monolithic gateway typically cause?
??x
The use of a single monolithic gateway often leads to a situation where everything is thrown into one giant layer, leading to a lack of isolation between different user interfaces. This makes it challenging to release updates for each interface independently without affecting others.

For example:
- A change in authentication logic could impact all services behind the same gateway.
```java
public class MonolithicGateway {
    public void handleRequest(String request) {
        // Complex logic involving multiple backend calls and transformations
    }
}
```
x??",884,"Backends for Frontends A common solution to the problem of chatty interfaces with backend services, or the need to vary content for different types of devices, is to have a server-side aggregation end...",qwen2.5:latest,2025-11-02 09:56:36,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Backends for Frontends,Dedicated Backends for Frontends (BFFs),"#### Dedicated Backends for Frontends (BFFs)
Background context: To avoid the pitfalls of monolithic gateways, a better approach is to use dedicated backends for each front-end interface. This method allows teams working on specific user interfaces to handle their server-side components more effectively.

:p How does using dedicated backends for front-ends improve development and maintenance?
??x
Using dedicated backends for front-ends improves development and maintenance by providing a clear separation of concerns. Each BFF can be tailored specifically to the needs of its corresponding UI, making it easier to update or modify without affecting other interfaces.

For example:
- A web application might have one set of APIs, while a mobile app has another.
```java
public class WebBff {
    public String getUserProfile(String userId) {
        // Logic specific to fetching user profile for the web interface
    }
}

public class MobileBff {
    public String getPushNotifications(String userId) {
        // Logic specific to handling push notifications for the mobile app
    }
}
```
x??",1099,"Backends for Frontends A common solution to the problem of chatty interfaces with backend services, or the need to vary content for different types of devices, is to have a server-side aggregation end...",qwen2.5:latest,2025-11-02 09:56:36,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Backends for Frontends,Authentication and Authorization Layer,"#### Authentication and Authorization Layer
Background context: In a system using BFFs, there might be a need for an additional layer between the BFFs and the front-end interfaces to handle authentication and authorization. This layer ensures that only authorized users can access specific functionalities.

:p What is typically placed between BFFs and front-end interfaces?
??x
An API authentication and authorization layer is typically placed between BFFs and front-end interfaces. This layer ensures secure communication by verifying user credentials and authorizing access to appropriate resources.

For example:
- A JWT (JSON Web Token) can be used for authentication.
```java
public class AuthLayer {
    public String authenticateUser(String username, String password) {
        // Logic to validate the user's credentials and issue a token if valid
        return ""token123"";
    }
}
```
x??

---",904,"Backends for Frontends A common solution to the problem of chatty interfaces with backend services, or the need to vary content for different types of devices, is to have a server-side aggregation end...",qwen2.5:latest,2025-11-02 09:56:36,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,On Your Own Terms,Fragment-Based Assembly for Websites vs Backends-for-Frontends for Mobile Apps,"#### Fragment-Based Assembly for Websites vs Backends-for-Frontends for Mobile Apps
This concept involves leveraging different architectural approaches depending on the use case. The key is to maintain cohesion and ensure that logic associated with core operations lives within dedicated services, rather than being scattered throughout the system.
:p How can an organization effectively utilize a hybrid approach when developing both websites and mobile applications?
??x
An organization can adopt fragment-based assembly for building a website, focusing on modular components. Simultaneously, it can use a backends-for-frontends approach for its mobile application to optimize performance and user experience specific to the device. This way, the logic associated with core functionalities like ordering music or updating customer details remains encapsulated within their respective services.
??x
The organization benefits from maintaining clear boundaries between components, ensuring that the website's front-end is assembled from reusable fragments while the mobile app communicates efficiently with backend APIs tailored for mobile devices.
```java
// Pseudocode for fragment-based assembly in a web application
public class WebsiteBuilder {
    public void assembleWebPage(Fragment[] fragments) {
        // Assemble web page using provided fragments
    }
}

// Pseudocode for backends-for-frontends approach in a mobile app
public class MobileAppServiceClient {
    private String backendUrl;

    public MobileAppServiceClient(String url) {
        this.backendUrl = url;
    }

    public void fetchCustomerDetails() {
        // Make request to the backend API to fetch customer details
    }
}
```
x??",1715,"A Hybrid Approach Many of the aforementioned options don’t need to be one-size-fits-all. I could see an organization adopting the approach of fragment-based assembly to create a website, but using a b...",qwen2.5:latest,2025-11-02 09:57:01,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,On Your Own Terms,Integrating with Commercial Off-the-Shelf Software (COTS),"#### Integrating with Commercial Off-the-Shelf Software (COTS)
The text discusses integrating existing systems that are under control and those that aren't, like COTS or SaaS. The advice is to build software if it's unique and strategic; otherwise, buy off-the-shelf solutions.
:p What factors should an organization consider when deciding whether to build custom software or integrate with commercial products?
??x
Organizations should weigh the uniqueness of their needs against cost-effectiveness. If a tool’s capabilities aren't uniquely aligned with the business strategy, it is often more cost-effective to purchase an existing product rather than building from scratch.
??x
The decision hinges on whether the custom software can offer strategic advantages or if standard commercial offerings will suffice. For instance, payroll systems are generally standardized, whereas content management systems (CMS) might be built in-house for a newspaper’s unique needs.
```java
// Pseudocode to evaluate 'build vs buy' decision
public class BuildVsBuyEvaluator {
    public String evaluate(String toolName, boolean isUnique, boolean strategicAsset) {
        if (isUnique && strategicAsset) return ""Build"";
        else return ""Buy"";
    }
}
```
x??",1247,"A Hybrid Approach Many of the aforementioned options don’t need to be one-size-fits-all. I could see an organization adopting the approach of fragment-based assembly to create a website, but using a b...",qwen2.5:latest,2025-11-02 09:57:01,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,On Your Own Terms,Lack of Control in Integrating with COTS Products,"#### Lack of Control in Integrating with COTS Products
Integration challenges arise when dealing with third-party tools that have already made technical decisions. These include language constraints, versioning issues, and customization limitations.
:p What are the main integration challenges faced when working with COTS products?
??x
The primary challenges include:
1. **Language Constraints**: The programming languages supported by a product might be limited or non-standard.
2. **Customization Limitations**: Customizing existing tools can be complex and expensive, often more costly than building something bespoke.
3. **Versioning Issues**: Upgrading the tool can break customizations due to changes in underlying infrastructure.
4. **Control Loss**: You are effectively ceding some level of control to an outside party.
??x
These challenges highlight the importance of choosing tools that align with your development practices and are open to customization. Tools like Salesforce, which have proprietary ecosystems, can be particularly problematic for integration.
```java
// Pseudocode to evaluate COTS tool limitations
public class ToolEvaluation {
    public boolean isCustomizationFeasible(String toolName) {
        // Check if the tool supports continuous integration and has a good API
        return toolSupportsCI(toolName) && toolHasGoodAPI(toolName);
    }

    private boolean toolSupportsCI(String toolName) {
        // Logic to check if the tool supports CI
        return false; // Example placeholder
    }

    private boolean toolHasGoodAPI(String toolName) {
        // Logic to check if the tool's API is user-friendly
        return false; // Example placeholder
    }
}
```
x??",1709,"A Hybrid Approach Many of the aforementioned options don’t need to be one-size-fits-all. I could see an organization adopting the approach of fragment-based assembly to create a website, but using a b...",qwen2.5:latest,2025-11-02 09:57:01,7
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,On Your Own Terms,Integration Spaghetti and Proprietary Protocols,"#### Integration Spaghetti and Proprietary Protocols
The integration process can become overly complex when tools use different protocols or allow direct access to their data stores, leading to coupling issues.
:p What challenges arise from the use of proprietary protocols and direct database access in integrating with software products?
??x
Challenges include:
1. **Protocol Incompatibility**: Tools might use various communication protocols (e.g., proprietary binary, SOAP, XML-RPC), making integration difficult and potentially requiring custom solutions for each.
2. **Direct Database Access**: Allowing direct access to the database can lead to tight coupling between your application and the product’s internal structure, increasing maintenance complexity.
3. **Lack of Standardization**: Without a standard protocol or API, integrating multiple tools can result in spaghetti code, making the system hard to maintain and extend.
??x
These issues emphasize the need for standardized integration methods and careful consideration when selecting tools that will be integrated into your ecosystem.
```java
// Pseudocode to illustrate potential integration complexity
public class IntegrationManager {
    public void integrateTool(String toolName) {
        if (toolSupportsStandardProtocol(toolName)) {
            // Use standard protocol
        } else if (toolHasCustomAPI(toolName)) {
            // Use custom API
        } else if (toolAllowsDirectDBAccess(toolName)) {
            // Handle direct DB access with caution
        }
    }

    private boolean toolSupportsStandardProtocol(String toolName) {
        return true; // Example placeholder
    }

    private boolean toolHasCustomAPI(String toolName) {
        return false; // Example placeholder
    }

    private boolean toolAllowsDirectDBAccess(String toolName) {
        return false; // Example placeholder
    }
}
```
x??",1901,"A Hybrid Approach Many of the aforementioned options don’t need to be one-size-fits-all. I could see an organization adopting the approach of fragment-based assembly to create a website, but using a b...",qwen2.5:latest,2025-11-02 09:57:01,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Example The multirole CRM system,CMS as a Service,"#### CMS as a Service
The provided text explains that Content Management Systems (CMS) are often used by enterprises to create and manage dynamic content for their websites. However, these systems typically have poor development environments for custom code and are designed primarily for content creation and management rather than complex application logic.

Often, organizations want to integrate the CMS with internal services to pull in dynamic content like customer records or product offerings. The challenge is that doing this within the CMS itself can lead to complex and tightly coupled code.

To address these challenges, the text suggests creating a custom service that fronts the CMS, handling all external requests and integrating with the CMS only for content retrieval. This allows for more control over scaling, templating systems, and API interactions.

:p How does fronting a CMS with a custom service solve the development environment issues?
??x
Fronting a CMS with a custom service solves the development environment issues by allowing developers to write their own code that integrates with the CMS in a controlled manner. This approach keeps complex logic out of the CMS, which is better suited for content management rather than application-specific business rules.

The custom service can be designed and scaled according to the specific needs of the organization, using more robust development tools and methodologies. For example, it can handle page layout, integrate with other services, and use templating systems that are better suited for dynamic content generation.

```java
public class CustomService {
    private final CMS cms;

    public CustomService(CMS cms) {
        this.cms = cms;
    }

    // Method to fetch and format content from the CMS
    public String getContentForPage(String pageId) {
        Content content = cms.getContent(pageId);
        return formatContent(content);
    }

    // Internal method for formatting content
    private String formatContent(Content content) {
        // Logic to apply templates, add dynamic data, etc.
        return ""<html>"" + content.getBody() + ""</html>"";
    }
}
```
x??",2166,"On Your Own Terms COTS and SAAS products absolutely have their place, and it isn’t feasible (or sensible) for most of us to build everything from scratch. So how do we resolve these challenges? The ke...",qwen2.5:latest,2025-11-02 09:57:22,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Example The multirole CRM system,Multirole CRM System,"#### Multirole CRM System
The text discusses the challenges of using Customer Relationship Management (CRM) tools in an enterprise environment. These tools often try to do everything and can become a single point of failure due to their extensive feature sets.

Over time, such tools can lead to a tangled web of dependencies between various internal systems that use the CRM APIs for integration. This can make it difficult to control the direction and choices made around the system, which are typically dictated by the vendor rather than the enterprise itself.

To regain some control over the system, the text suggests identifying core domain concepts currently handled by the CRM and creating façade services that abstract these concepts. These services expose resources in a RESTful manner, making it easier for external systems to integrate with them.

By using façade services, internal systems can be decoupled from the CRM's APIs, allowing for more flexible and maintainable integration points. This approach lays the groundwork for potential migrations or replacements of the CRM tool.

:p How does creating façade services help in managing dependencies on a multirole CRM system?
??x
Creating façade services helps in managing dependencies on a multirole CRM system by abstracting away the underlying complexities and exposing only necessary domain concepts as RESTful resources. This decouples internal systems from the CRM's APIs, making it easier to integrate with them.

The advantage is that each internal system can interact with these façade services using standard HTTP requests, reducing the complexity of integration. Additionally, if a different CRM tool needs to be chosen or replaced in the future, the change can be implemented by updating the façades rather than rewriting all integrations directly against the new system's APIs.

```java
public class ProjectService {
    private final CRM crm;

    public ProjectService(CRM crm) {
        this.crm = crm;
    }

    // Method to fetch project details from the CRM
    @GetMapping(""/projects/{projectId}"")
    public ResponseEntity<ProjectResource> getProject(@PathVariable String projectId) {
        Project project = crm.getProject(projectId);
        return new ResponseEntity<>(new ProjectResource(project), HttpStatus.OK);
    }

    // Internal method for converting project model to resource
    private ProjectResource convertToResource(Project project) {
        return new ProjectResource(
            project.getId(),
            project.getName(),
            project.getDescription()
        );
    }
}
```
x??

---",2608,"On Your Own Terms COTS and SAAS products absolutely have their place, and it isn’t feasible (or sensible) for most of us to build everything from scratch. So how do we resolve these challenges? The ke...",qwen2.5:latest,2025-11-02 09:57:22,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Its All About Seams,Strangler Application Pattern,"#### Strangler Application Pattern
The Strangler Application Pattern is a useful approach when dealing with legacy or third-party systems that are not fully under our control. This pattern allows us to gradually replace existing functionality without requiring a full rewrite, making it easier to manage and transition away from older, less maintainable systems.
:p What is the Strangler Application Pattern used for?
??x
The Strangler Application Pattern is used to capture and intercept calls to old legacy or third-party systems. By routing these calls through new code, you can replace functionality over time, allowing for a gradual migration rather than a complete rewrite. This approach helps in reducing risk and complexity during the transition.
```java
// Example of intercepting calls using a proxy class
public class LegacySystemProxy {
    private final LegacySystem legacySystem;

    public LegacySystemProxy() {
        this.legacySystem = new LegacySystem();
    }

    public void performAction(String input) {
        // Check if we can route the call to new code or old system
        boolean isNewCodeAvailable = checkForNewCode(input);
        
        if (isNewCodeAvailable) {
            // Direct the call to new code
            newCode.performAction(input);
        } else {
            // Route the call to legacy system
            legacySystem.performAction(input);
        }
    }

    private boolean checkForNewCode(String input) {
        // Logic to determine if new code is available for this input
        return false;
    }
}
```
x??",1573,"The Strangler Pattern When it comes to legacy or even COTS platforms that aren’t totally under our control, we also have to deal with what happens when we want to remove them or at least move away fro...",qwen2.5:latest,2025-11-02 09:57:49,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Its All About Seams,Microservices Interception with Proxies,"#### Microservices Interception with Proxies
When dealing with microservices, a single monolithic application intercepting all calls to an existing legacy system may not be practical. Instead, you can use a series of microservices to capture and redirect original calls. This approach might require the use of proxies to handle capturing and routing these calls.
:p How do microservices interception using proxies work?
??x
Microservices interception using proxies involves creating multiple small services that collectively handle the redirection of requests from legacy systems to new code. Proxies act as intermediaries, intercepting the original calls and deciding whether to route them to existing legacy code or new microservices. This method allows for a more modular and scalable approach compared to a single monolithic application.
```java
// Example of using a proxy service to intercept calls in microservices architecture
public class LegacyCallProxy {
    private final MicroserviceA microserviceA;
    private final MicroserviceB microserviceB;

    public LegacyCallProxy(MicroserviceA microserviceA, MicroserviceB microserviceB) {
        this.microserviceA = microserviceA;
        this.microserviceB = microserviceB;
    }

    public void processRequest(String input) {
        // Determine which service to route the request to based on input
        if (input.startsWith(""A"")) {
            microserviceA.handle(input);
        } else if (input.startsWith(""B"")) {
            microserviceB.handle(input);
        }
    }
}
```
x??",1552,"The Strangler Pattern When it comes to legacy or even COTS platforms that aren’t totally under our control, we also have to deal with what happens when we want to remove them or at least move away fro...",qwen2.5:latest,2025-11-02 09:57:49,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Its All About Seams,Avoiding Database Integration in Microservices,"#### Avoiding Database Integration in Microservices
Database integration can tightly couple microservices, making it harder to evolve and maintain them independently. To avoid this, it's recommended to use API-based communication where possible. This approach helps keep services decoupled, as changes in one service do not necessarily require updates in others.
:p Why should you avoid database integration in microservices?
??x
Avoiding database integration in microservices is crucial because it tightly couples the services, making them harder to evolve and maintain independently. Instead of directly accessing databases, microservices should communicate through APIs or other message-passing mechanisms. This approach ensures that changes in one service do not necessarily require updates in others.
```java
// Example of API-based communication instead of database integration
public class OrderService {
    public void createOrder(Order order) {
        // Instead of directly inserting into the database, send a request to PaymentService
        PaymentService paymentService = new PaymentService();
        paymentService.createPaymentFor(order);
    }
}

public class PaymentService {
    public void createPaymentFor(Order order) {
        // Logic to handle creating payment for the order through an API call
    }
}
```
x??",1338,"The Strangler Pattern When it comes to legacy or even COTS platforms that aren’t totally under our control, we also have to deal with what happens when we want to remove them or at least move away fro...",qwen2.5:latest,2025-11-02 09:57:49,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Its All About Seams,Choreography Over Orchestration in Microservices,"#### Choreography Over Orchestration in Microservices
Choreography and orchestration are two different ways of managing interactions between microservices. In choreography, each service independently handles its responsibilities, and communication is based on events or messages. In contrast, orchestration centralizes control over the flow of operations.
:p What's the difference between choreography and orchestration in microservices?
??x
Choreography and orchestration are two different paradigms for managing interactions between microservices.

- **Orchestration**: Centralizes control over the flow of operations. A single service dictates the sequence of actions that need to be performed by other services.
- **Choreography**: Each service independently handles its responsibilities, and communication is based on events or messages. There is no centralized authority dictating the interactions between services.

Choreography is generally more flexible but requires more coordination among services, while orchestration provides better control over the flow of operations but can become complex as the number of services increases.
```java
// Example of choreography (each service handles its own logic independently)
public class NotificationService {
    public void sendNotification(Order order) {
        // Logic to send notification for a new order
    }
}

public class PaymentService {
    public void processPayment(Order order) {
        // Logic to handle payment processing
        NotificationService notificationService = new NotificationService();
        notificationService.sendNotification(order);
    }
}
```
x??",1641,"The Strangler Pattern When it comes to legacy or even COTS platforms that aren’t totally under our control, we also have to deal with what happens when we want to remove them or at least move away fro...",qwen2.5:latest,2025-11-02 09:57:49,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Its All About Seams,Postel's Law and Tolerant Readers in Microservices,"#### Postel's Law and Tolerant Readers in Microservices
Postel's Law, also known as the robustness principle, suggests that software should be conservative in its sending and liberal in its receiving. In microservices, this translates to tolerant readers: services should be designed to handle unexpected data gracefully without breaking.
:p What is Postel's Law and why is it important in microservices?
??x
Postel's Law, also known as the robustness principle, suggests that software should be conservative in its sending and liberal in its receiving. In microservices, this means designing services to handle unexpected data or messages gracefully without breaking.

- **Conservative in Sending**: Ensure that your service adheres strictly to the protocol specifications when sending data.
- **Liberal in Receiving**: Be prepared to handle unexpected data formats or malformed messages from other services.

This approach helps ensure robustness and resilience, as services can continue functioning even if they encounter issues with incoming data.
```java
// Example of a tolerant reader design following Postel's Law
public class OrderProcessor {
    public void processOrder(Order order) {
        try {
            // Logic to process the order
        } catch (IllegalArgumentException e) {
            // Handle invalid input gracefully
            System.out.println(""Received an invalid order: "" + e.getMessage());
        }
    }
}
```
x??",1451,"The Strangler Pattern When it comes to legacy or even COTS platforms that aren’t totally under our control, we also have to deal with what happens when we want to remove them or at least move away fro...",qwen2.5:latest,2025-11-02 09:57:49,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Its All About Seams,Decomposing Monolithic Applications into Microservices,"#### Decomposing Monolithic Applications into Microservices
Monolithic applications grow over time and can become unwieldy, making it difficult to evolve their design. To address this, you can decompose the monolith into smaller microservices, each handling a specific domain or functionality.
:p How do you handle decomposing monolithic applications into microservices?
??x
Decomposing monolithic applications into microservices involves breaking down the large application into smaller, independent services that focus on specific functionalities. This approach allows for better scalability, maintainability, and easier evolution of individual components.

To start this process:
1. **Identify bounded contexts**: Determine the different domains or subdomains within your application.
2. **Design microservices**: Create small, loosely coupled services based on these bounded contexts.
3. **Ensure loose coupling**: Services should communicate through APIs rather than sharing databases directly.
4. **Gradual migration**: Start with small changes and gradually replace functionality in the monolith with new microservices.

By following these steps, you can effectively decompose a monolithic application into more manageable and scalable microservices.
```java
// Example of identifying bounded contexts for decomposition
public class BoundedContext {
    public static void main(String[] args) {
        // Identify different domains or subdomains (bounded contexts)
        String contextA = ""User Management"";
        String contextB = ""Order Processing"";

        // Decompose into microservices based on these contexts
        MicroserviceA microserviceA = new MicroserviceA(contextA);
        MicroserviceB microserviceB = new MicroserviceB(contextB);

        // Gradually replace functionality in the monolith with these services
    }
}

class MicroserviceA {
    public MicroserviceA(String context) {
        System.out.println(""Initializing "" + context);
    }

    public void handleUserRequest(UserRequest request) {
        // Logic to handle user requests
    }
}
```
x??",2092,"The Strangler Pattern When it comes to legacy or even COTS platforms that aren’t totally under our control, we also have to deal with what happens when we want to remove them or at least move away fro...",qwen2.5:latest,2025-11-02 09:57:49,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Breaking Apart MusicCorp,Identifying Bounded Contexts as Seams,"#### Identifying Bounded Contexts as Seams
Background context: The monolithic structure of services often leads to a lack of cohesion and tight coupling, making it difficult to make changes without affecting other parts of the system. Michael Feathers' concept of seams is introduced as a way to address this issue by defining clear boundaries in code that can be worked on independently.

:p What are bounded contexts in the context of identifying seams?
??x
Bounded contexts are defined as cohesive and loosely coupled boundaries within an organization, which make excellent seams for breaking apart monolithic services. They represent areas of functionality that can be separated and treated as distinct units.
x??",717,It’s All About Seams We discussed in  Chapter 3  that we want our services to be highly cohesive and loosely coupled. The problem with the monolith is that all too often it is the opposite of both. Ra...,qwen2.5:latest,2025-11-02 09:58:15,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Breaking Apart MusicCorp,Creating Packages Based on Bounded Contexts,"#### Creating Packages Based on Bounded Contexts
Background context: To effectively use bounded contexts as seams, the first step is to identify them in your codebase by creating packages or namespaces that reflect these contexts.

:p How do you create packages for bounded contexts?
??x
You can create packages using namespace concepts provided by programming languages. For example, in Java, you use `package` statements. In Python, you would use module imports. The goal is to group related code into logical units that align with the identified bounded contexts.
```java
// Example of creating a package for the Catalog context
package com.musiccorp.catalog;
public class Item {
    // Catalog-related methods and classes here
}
```
x??",740,It’s All About Seams We discussed in  Chapter 3  that we want our services to be highly cohesive and loosely coupled. The problem with the monolith is that all too often it is the opposite of both. Ra...,qwen2.5:latest,2025-11-02 09:58:15,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Breaking Apart MusicCorp,Refactoring Code into Bounded Context Packages,"#### Refactoring Code into Bounded Context Packages
Background context: After identifying packages, you need to move existing code into these new packages. Modern IDEs can help automate this process through refactorings.

:p How can modern IDEs aid in moving code between packages?
??x
Modern Integrated Development Environments (IDEs) offer refactoring tools that can automatically move code from one package to another. These tools reduce the manual effort required and minimize potential errors during the reorganization of code.
For example, in a Java IDE like IntelliJ IDEA:
```java
// Before refactoring
package com.musiccorp.backend;

public class FinanceReport {
    // Code here
}

// After refactoring
package com.musiccorp.finance;

public class FinanceReport {
    // Code here
}
```
x??",799,It’s All About Seams We discussed in  Chapter 3  that we want our services to be highly cohesive and loosely coupled. The problem with the monolith is that all too often it is the opposite of both. Ra...,qwen2.5:latest,2025-11-02 09:58:15,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Breaking Apart MusicCorp,Identifying Leftover Code and New Contexts,"#### Identifying Leftover Code and New Contexts
Background context: As you move code into packages, leftover code might indicate the presence of new bounded contexts or missing ones.

:p What should be done with leftover code?
??x
Leftover code that doesn't fit neatly into any existing package should be investigated to determine if it represents a new bounded context. This process can help in identifying additional areas within your system that need separation.
```java
// Example of identifying leftover code
package com.musiccorp.catalog;
public class LeftoverCode {
    // Code here, possibly indicating a missing bounded context
}
```
x??",646,It’s All About Seams We discussed in  Chapter 3  that we want our services to be highly cohesive and loosely coupled. The problem with the monolith is that all too often it is the opposite of both. Ra...,qwen2.5:latest,2025-11-02 09:58:15,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Breaking Apart MusicCorp,Analyzing Dependencies Between Packages,"#### Analyzing Dependencies Between Packages
Background context: Tools like Structure 101 can help visualize and analyze dependencies between packages to ensure they align with real-world organizational interactions.

:p How do you use tools like Structure 101?
??x
Tools like Structure 101 provide graphical representations of package dependencies, allowing developers to visually inspect the relationships between different parts of their codebase. This can help identify discrepancies where the actual organization of the system differs from the logical structure represented in the code.
For example:
```java
// Pseudocode for checking package dependencies using a tool like Structure 101
Tool.checkDependencies(""com.musiccorp.warehouse"", ""com.musiccorp.finance"");
```
x??",776,It’s All About Seams We discussed in  Chapter 3  that we want our services to be highly cohesive and loosely coupled. The problem with the monolith is that all too often it is the opposite of both. Ra...,qwen2.5:latest,2025-11-02 09:58:15,7
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Breaking Apart MusicCorp,Incremental Refactoring Process,"#### Incremental Refactoring Process
Background context: The refactoring process is not a one-time, big-bang event but can be done incrementally over time. It allows for gradual improvement and minimizes disruptions.

:p What are the benefits of incremental refactoring?
??x
Incremental refactoring allows you to make changes in small steps, reducing the risk of introducing bugs or breaking existing functionality. This approach also makes it easier to manage dependencies and integrate new features without overwhelming your development team.
```java
// Example of an incremental change
public class Item {
    private int stockLevel;
    
    public void updateStock(int quantity) {
        // Incremental logic here
    }
}
```
x??

---",740,It’s All About Seams We discussed in  Chapter 3  that we want our services to be highly cohesive and loosely coupled. The problem with the monolith is that all too often it is the opposite of both. Ra...,qwen2.5:latest,2025-11-02 09:58:15,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Getting to Grips with the Problem,Pace of Change,"#### Pace of Change
Background context explaining why the pace of change is a critical factor when deciding to split monoliths. It often involves upcoming changes that will make it beneficial to have separate services for quicker and easier management.

:p How does understanding the pace of change influence decisions on splitting a monolithic application?
??x
Understanding the pace of change can guide the decision-making process by identifying areas where changes are expected to be frequent or significant. For instance, if there is an upcoming overhaul in inventory management, separating that functionality into its own service could facilitate faster development and deployment cycles.

```java
public class InventoryService {
    public void updateInventory() {
        // Logic for updating the inventory system
    }
}
```
x??",837,The Reasons to Split the Monolith Deciding that you’d like a monolithic service or application to be smaller is a good start. But I would strongly advise you to chip away at these systems. An incremen...,qwen2.5:latest,2025-11-02 09:58:42,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Getting to Grips with the Problem,Team Structure,"#### Team Structure
Context around team structure, highlighting geographical distribution as a key driver for splitting monoliths. This can lead to better ownership and autonomy among development teams.

:p How does the geographic split of a delivery team influence decisions on splitting a monolithic application?
??x
Geographic splits in a delivery team can drive the decision to separate parts of the codebase that are frequently worked on by different regions. For example, if one team based in Hawaii works more with warehouse management, separating this functionality could allow them to take full ownership and make changes independently.

```java
public class WarehouseManagementService {
    public void manageInventory() {
        // Logic for managing inventory operations
    }
}
```
x??",799,The Reasons to Split the Monolith Deciding that you’d like a monolithic service or application to be smaller is a good start. But I would strongly advise you to chip away at these systems. An incremen...,qwen2.5:latest,2025-11-02 09:58:42,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Getting to Grips with the Problem,Security Considerations,"#### Security Considerations
Discussion on security audits leading to the need for separation of sensitive data handling, ensuring better protection through isolation in a microservices architecture.

:p How can splitting parts of a monolithic application based on security needs improve overall system security?
??x
Splitting parts of a monolithic application that handle sensitive data can enhance security by isolating these components. For instance, if the finance-related code manages sensitive information, separating this into its own service can provide additional protections such as better monitoring and secure handling of data at rest and in transit.

```java
public class FinanceService {
    public void processFinanceData() {
        // Logic for processing financial transactions
    }
}
```
x??",811,The Reasons to Split the Monolith Deciding that you’d like a monolithic service or application to be smaller is a good start. But I would strongly advise you to chip away at these systems. An incremen...,qwen2.5:latest,2025-11-02 09:58:42,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Getting to Grips with the Problem,Technology Drivers,"#### Technology Drivers
Exploration of technology drivers, like the adoption of new programming languages or frameworks, that can drive the decision to split a monolithic application into microservices.

:p How does adopting a new technology, such as Clojure for recommendation algorithms, influence decisions on splitting a monolithic application?
??x
Adopting new technologies, such as Clojure for implementing recommendation algorithms, can drive the decision to separate specific parts of the monolith. This allows for easier experimentation and testing of alternative implementations without disrupting other services.

```java
public class RecommendationService {
    public void applyNewAlgorithm() {
        // Logic for applying a new recommendation algorithm in Clojure
    }
}
```
x??",795,The Reasons to Split the Monolith Deciding that you’d like a monolithic service or application to be smaller is a good start. But I would strongly advise you to chip away at these systems. An incremen...,qwen2.5:latest,2025-11-02 09:58:42,7
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Getting to Grips with the Problem,Tangled Dependencies,"#### Tangled Dependencies
Discussion on identifying tangled dependencies, particularly with the database, as a critical step in splitting monoliths. Highlighting the importance of understanding how code interacts with the database to avoid complex refactoring.

:p How does analyzing dependencies help in deciding which parts of the application should be split?
??x
Analyzing dependencies is crucial for understanding which parts of the application can be separated without causing significant disruptions. By examining how different components interact with the database, you can identify seams that are less entangled and easier to split out.

```java
public class RepositoryLayer {
    private final EntityManagerFactory entityManagerFactory;

    public RepositoryLayer(EntityManagerFactory entityManagerFactory) {
        this.entityManagerFactory = entityManagerFactory;
    }

    // Methods for interacting with the database
}
```
x??",942,The Reasons to Split the Monolith Deciding that you’d like a monolithic service or application to be smaller is a good start. But I would strongly advise you to chip away at these systems. An incremen...,qwen2.5:latest,2025-11-02 09:58:42,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Getting to Grips with the Problem,Database Splitting,"#### Database Splitting
Explanation of how to split out database access code, particularly focusing on the repository layer and tools like SchemaSpy for visualizing database relationships.

:p How can you use SchemaSpy to visualize database dependencies when splitting a monolithic application?
??x
SchemaSpy is a useful tool for generating graphical representations of database relationships. By using it, you can better understand the coupling between tables that span different bounded contexts and identify which parts of the database are used by specific code components.

```java
// Example usage of SchemaSpy
new SchemaSpy(new File(""output""), ""jdbc:mysql://localhost/database"", ""username"", ""password"").execute();
```
x??

---",732,The Reasons to Split the Monolith Deciding that you’d like a monolithic service or application to be smaller is a good start. But I would strongly advise you to chip away at these systems. An incremen...,qwen2.5:latest,2025-11-02 09:58:42,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Example Breaking Foreign Key Relationships,Breaking Foreign Key Relationships,"#### Breaking Foreign Key Relationships
Background context: In the example, there's a foreign key relationship between the catalog and finance packages where the finance code uses data from the line item table stored in the catalog package. This creates a dependency that needs to be addressed when these become separate services.

:p What is the issue with having the finance code reach into the line item table?
??x
The issue is that database integration happens between different parts of the application, which can lead to coupling and potential performance issues as services are separated. To address this, the solution involves exposing data via an API call in the catalog package, making sure the finance code interacts through a service boundary rather than directly accessing the database.

```java
// Example of an API method in the CatalogService
public class CatalogService {
    public String getAlbumTitleBySku(String sku) {
        // Logic to fetch album title by SKU from the line item table
        return ""Bruce Springsteen’s Greatest Hits"";
    }
}
```
x??",1077,"Example: Breaking Foreign Key Relationships In this example, our catalog code uses a generic line item table to store information about an album. Our finance code uses a ledger table to track financia...",qwen2.5:latest,2025-11-02 09:59:11,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Example Breaking Foreign Key Relationships,Exposing Data via API Call,"#### Exposing Data via API Call
Background context: To avoid direct database access and ensure loose coupling between services, an API call is introduced. This allows the finance package to request necessary information from the catalog package.

:p How does exposing data through a service boundary help in managing foreign key relationships?
??x
Exposing data through a service boundary helps by decoupling the finance code from direct database access. It ensures that changes or updates in the line item table managed by the catalog do not affect the finance package directly. This separation also allows for better performance and scalability as services can manage their own databases more efficiently.

```java
// Example of how the finance package might make an API call to CatalogService
public class FinanceService {
    private final CatalogService catalogService;

    public FinanceService(CatalogService catalogService) {
        this.catalogService = catalogService;
    }

    public String generateReport() {
        // Assuming some ledger data is available here
        String sku = ""12345"";
        String albumTitle = catalogService.getAlbumTitleBySku(sku);
        return ""We sold 400 copies of "" + albumTitle + "" and made $1,300."";
    }
}
```
x??",1269,"Example: Breaking Foreign Key Relationships In this example, our catalog code uses a generic line item table to store information about an album. Our finance code uses a ledger table to track financia...",qwen2.5:latest,2025-11-02 09:59:11,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Example Breaking Foreign Key Relationships,Managing Foreign Key Relationships in Separate Services,"#### Managing Foreign Key Relationships in Separate Services
Background context: After removing the foreign key relationship, the finance package needs to handle data relationships across services. This might involve making multiple database calls or implementing custom consistency checks.

:p What changes are needed when separating services that previously had a foreign key constraint?
??x
When separating services that previously had a foreign key constraint, you need to manage dependencies and ensure data integrity through API calls instead of direct database access. You may end up making additional database calls or implementing custom consistency checks across services. For example, if an order service references catalog items, removing the item could leave orders with invalid references.

```java
// Example of handling an invalid reference in the OrderService
public class OrderService {
    public void updateOrder(List<Long> catalogItemIds) {
        // Check for valid IDs and handle any inconsistencies
        for (Long id : catalogItemIds) {
            CatalogService catalog = new CatalogService();
            if (!catalog.itemExists(id)) {
                throw new RuntimeException(""Invalid catalog ID: "" + id);
            }
        }
    }
}
```
x??",1279,"Example: Breaking Foreign Key Relationships In this example, our catalog code uses a generic line item table to store information about an album. Our finance code uses a ledger table to track financia...",qwen2.5:latest,2025-11-02 09:59:11,7
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Example Breaking Foreign Key Relationships,Performance Considerations in Service Separation,"#### Performance Considerations in Service Separation
Background context: While separating services, performance is a critical concern. The objective is to determine whether the trade-off of slower direct database access for better service independence and scalability is acceptable.

:p How do you evaluate the impact on performance when considering separation of services?
??x
To evaluate the impact on performance, you should first measure the current performance of your system. Understand what constitutes good performance based on testing and user needs. If these requirements are met, separating services by using API calls can be a reasonable trade-off. It ensures that each service is self-contained and scalable.

```java
// Example of profiling code to measure current performance
public class PerformanceProfiler {
    public long getCurrentPerformanceMetrics() {
        // Logic to fetch current performance metrics (e.g., response time, throughput)
        return 500; // Example latency in ms
    }
}
```
x??",1024,"Example: Breaking Foreign Key Relationships In this example, our catalog code uses a generic line item table to store information about an album. Our finance code uses a ledger table to track financia...",qwen2.5:latest,2025-11-02 09:59:11,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Example Breaking Foreign Key Relationships,Handling Inconsistencies Across Services,"#### Handling Inconsistencies Across Services
Background context: After separating services and removing foreign key constraints, inconsistencies can arise if one service deletes data that another service references. This requires implementing custom logic to maintain data integrity.

:p What steps are necessary to handle data inconsistencies across separated services?
??x
Handling data inconsistencies involves implementing custom consistency checks or triggers for cleanup actions in the services. For instance, if a catalog item is deleted, related orders should be updated or cleaned up to prevent invalid references.

```java
// Example of handling item deletion and updating related order service
public class CatalogService {
    public void deleteItem(Long id) {
        // Delete logic
        for (OrderService os : getAllOrderServices()) {
            os.updateOrdersForRemovedItem(id);
        }
    }

    private void updateOrdersForRemovedItem(Long id) {
        // Logic to check and update orders referencing the removed item
    }
}
```
x??

---",1066,"Example: Breaking Foreign Key Relationships In this example, our catalog code uses a generic line item table to store information about an album. Our finance code uses a ledger table to track financia...",qwen2.5:latest,2025-11-02 09:59:11,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Example Shared Static Data,Storing Shared Static Data in Databases,"#### Storing Shared Static Data in Databases
Background context: The passage discusses the common practice of storing shared static data, such as country codes, in databases within various systems. This approach is often used across different services that need this information, but it raises questions about consistency and maintenance.

If multiple services read from a single database table for shared static data, like country codes, there can be challenges when updating the data. For instance, if one service updates its database with new information (e.g., ""Newmantopia"" as a new country), another service might not get this update immediately, leading to inconsistencies.

:p What are some potential issues with storing shared static data in a single database table across multiple services?
??x
There can be several issues:
1. **Inconsistent Data**: If one service updates the database with new information but other services do not receive or apply these changes, it leads to inconsistent data.
2. **Maintenance Overhead**: Frequent updates require careful management and testing, especially when updating a shared resource.
3. **Latency in Updates**: There might be delays between the time an update is made in one service and when it's reflected across all services.

These issues can impact the reliability and accuracy of the data used by different parts of the system.

x??",1389,Example: Shared Static Data I have seen perhaps as many country codes stored in databases (shown in  Figure 5-4 ) as I have written  StringUtils  classes for in-house Java projects. This seems to impl...,qwen2.5:latest,2025-11-02 09:59:34,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Example Shared Static Data,Duplicating Shared Static Data Tables,"#### Duplicating Shared Static Data Tables
Background context: One approach to address the inconsistency issue mentioned earlier is to duplicate the shared static data table for each package or service. This ensures that every service has its own copy, reducing the risk of inconsistent updates.

:p What are the potential challenges with duplicating a shared static data table across different services?
??x
The primary challenge with duplicating shared static data tables is maintaining consistency:
1. **Maintenance Overhead**: Each service would need to be updated independently whenever changes occur.
2. **Complexity in Updates**: Ensuring that all instances of the duplicated table are kept synchronized can be complex and error-prone.

For example, if a new country code needs to be added, this change must be manually propagated to each instance of the duplicated table across all services.

x??",904,Example: Shared Static Data I have seen perhaps as many country codes stored in databases (shown in  Figure 5-4 ) as I have written  StringUtils  classes for in-house Java projects. This seems to impl...,qwen2.5:latest,2025-11-02 09:59:34,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Example Shared Static Data,Treating Shared Static Data as Code,"#### Treating Shared Static Data as Code
Background context: Another approach is to treat shared static data as part of the application’s code or configuration. This could involve storing it in a property file or using enums within the codebase, making changes easier and more controlled compared to altering a live database table.

:p How might you implement treating shared static data (like country codes) as code?
??x
You can implement this by:
1. **Using Property Files**: Store the data in a properties file that is deployed with each service.
2. **Enums or Constants**: Define enums or constants within your application code for easy access and maintainability.

Example using property files:

```properties
# config.properties
countryCodes=AU,US,CA
```

Example using Java enums:

```java
public enum CountryCode {
    AUSTRALIA(""AU""),
    UNITED_STATES(""US""),
    CANADA(""CA"");

    private final String code;

    CountryCode(String code) {
        this.code = code;
    }

    public String getCode() {
        return code;
    }
}
```

In both cases, changes are easier to manage and less prone to database-related issues.

x??",1139,Example: Shared Static Data I have seen perhaps as many country codes stored in databases (shown in  Figure 5-4 ) as I have written  StringUtils  classes for in-house Java projects. This seems to impl...,qwen2.5:latest,2025-11-02 09:59:34,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Example Shared Static Data,Pushing Static Data into a Separate Service,"#### Pushing Static Data into a Separate Service
Background context: In some scenarios, the complexity and volume of static reference data might necessitate pushing this data into its own dedicated service. This approach can help manage large volumes and complex rules associated with the static data, ensuring that other services depend on reliable and up-to-date information.

:p When would it be appropriate to push static data into a separate service?
??x
It is appropriate to push static data into a separate service when:
1. **Volume and Complexity**: The amount of static data or the rules governing its usage are extensive.
2. **Rules Management**: There are complex business logic or validation rules that need to be applied to the static data.
3. **Reliability**: Ensuring high reliability and consistency across all services that depend on this data.

For example, if you have a large number of countries with varying tax rates or regulations, having a dedicated service for managing these codes can simplify the architecture and ensure consistent updates.

x??

---",1077,Example: Shared Static Data I have seen perhaps as many country codes stored in databases (shown in  Figure 5-4 ) as I have written  StringUtils  classes for in-house Java projects. This seems to impl...,qwen2.5:latest,2025-11-02 09:59:34,7
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Example Shared Data,Shared Data and Implicit Models,"#### Shared Data and Implicit Models
Background context explaining shared mutable data, where both finance and warehouse systems update a common customer record table. This leads to potential issues like race conditions and inconsistent states.

:p What is the problem with shared mutable data in this scenario?
??x
The problem arises from having multiple parts of the system (finance and warehouse) updating the same piece of data (customer records). This can lead to race conditions, where one part of the system might overwrite changes made by another, or inconsistencies if both parts read outdated data.

Code examples:
```java
// Pseudocode for finance code update
void updatePayment(Customer customer) {
    // Read current payment status from database
    String currentStatus = getPaymentStatusFromDatabase(customer.id);
    
    // Update the payment status in the database
    setPaymentStatusInDatabase(customer.id, ""PAID"");
}

// Pseudocode for warehouse code update
void dispatchOrder(Customer customer) {
    // Read current order status from database
    String currentStatus = getOrderStatusFromDatabase(customer.id);
    
    // Dispatch the order and update its status in the database
    setOrderStatusInDatabase(customer.id, ""DISPATCHED"");
}
```
x??",1270,"Example: Shared Data Now let’s dive into a more complex example, but one that can be a common problem when you’re trying to tease apart systems: shared mutable data. Our finance code tracks payments m...",qwen2.5:latest,2025-11-02 09:59:56,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Example Shared Data,Domain Concepts and Code Organization,"#### Domain Concepts and Code Organization
Background context about needing to model domain concepts explicitly rather than implicitly. The current setup where both finance and warehouse systems use a generic customer record table without clear boundaries leads to confusion.

:p Why is it important to model the Customer as a distinct entity in this scenario?
??x
It is crucial because modeling the Customer as a distinct entity helps clarify ownership and responsibility of data, making the code more modular and maintainable. By doing so, we can better encapsulate logic related to customers within specific services or packages.

Code examples:
```java
// Pseudocode for creating a new Customer service
package com.example.customer;

public class CustomerService {
    public void updatePayment(Customer customer) {
        // Specific logic for updating payments
    }
    
    public void dispatchOrder(Customer customer) {
        // Specific logic for dispatching orders
    }
}

// Example of how to use the Customer Service in another package
package com.example.finance;

import com.example.customer.CustomerService;

public class FinanceSystem {
    private final CustomerService customerService;
    
    public FinanceSystem(CustomerService customerService) {
        this.customerService = customerService;
    }
    
    void processPayment() {
        // Use customer service to update payment status
        customerService.updatePayment(customer);
    }
}
```
x??",1482,"Example: Shared Data Now let’s dive into a more complex example, but one that can be a common problem when you’re trying to tease apart systems: shared mutable data. Our finance code tracks payments m...",qwen2.5:latest,2025-11-02 09:59:56,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Example Shared Data,Bounded Contexts and Service Layer Separation,"#### Bounded Contexts and Service Layer Separation
Background context about recognizing bounded contexts as distinct domains with their own rules and models. The example illustrates how separating the Customer domain into its own service helps in managing changes and responsibilities.

:p What is a bounded context, and why is it important to recognize them?
??x
A bounded context refers to a specific part of a larger system where certain rules apply and are meaningful. Recognizing bounded contexts is important because different parts of the system might have different views on the same data or processes. By separating these contexts into distinct services or packages, we can ensure that each service operates within its own defined scope, reducing complexity and making the codebase more manageable.

Code examples:
```java
// Example of a Customer bounded context
package com.example.customer;

public class CustomerService {
    public void updatePayment(Customer customer) {
        // Specific logic for updating payments
    }
    
    public void dispatchOrder(Customer customer) {
        // Specific logic for dispatching orders
    }
}

// Example of a Finance system using the Customer service
package com.example.finance;

import com.example.customer.CustomerService;

public class FinanceSystem {
    private final CustomerService customerService;
    
    public FinanceSystem(CustomerService customerService) {
        this.customerService = customerService;
    }
    
    void processPayment() {
        // Use customer service to update payment status
        customerService.updatePayment(customer);
    }
}
```
x??

---",1646,"Example: Shared Data Now let’s dive into a more complex example, but one that can be a common problem when you’re trying to tease apart systems: shared mutable data. Our finance code tracks payments m...",qwen2.5:latest,2025-11-02 09:59:56,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Transactional Boundaries,Refactoring Databases,"#### Refactoring Databases
Background context: The example discusses the need to refactor a database schema by separating concerns that were previously conflated. This is part of a broader process of breaking down monolithic applications into microservices, each with its own bounded context and database schema.

:p What is the primary reason for refactoring databases in this scenario?
??x
The primary reason for refactoring databases is to separate concerns that were previously conflated within a single table. By doing so, it becomes clear that different parts of the application have distinct needs, which can be better served by separate tables or schemas.
x??",667,"Example: Shared Tables Figure 5-7  shows our last example. Our catalog needs to store the name and price of the records we sell, and the warehouse needs to keep an electronic record of inventory. We d...",qwen2.5:latest,2025-11-02 10:00:13,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Transactional Boundaries,Splitting Tables into Separate Entities,"#### Splitting Tables into Separate Entities
Background context: The example suggests splitting a shared line item table into two separate entities—one for catalog details and another for warehouse inventory. This separation helps in managing data more effectively within different contexts.

:p Why is it beneficial to split the shared line item table?
??x
Splitting the shared line item table benefits the application by allowing each context (e.g., catalog and warehouse) to manage its specific requirements independently. This leads to cleaner, more maintainable code and databases that better reflect real-world relationships and responsibilities.
x??",656,"Example: Shared Tables Figure 5-7  shows our last example. Our catalog needs to store the name and price of the records we sell, and the warehouse needs to keep an electronic record of inventory. We d...",qwen2.5:latest,2025-11-02 10:00:13,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Transactional Boundaries,Staging the Break for Schema Separation,"#### Staging the Break for Schema Separation
Background context: After refactoring the database schema, it is recommended to keep the application code together before fully separating the services into microservices. This staged approach helps in managing potential issues arising from breaking transactional integrity.

:p Why is it advised to stage the separation of schemas before splitting the service?
??x
It is advised to stage the separation of schemas by keeping the application code together because this allows for easier testing and troubleshooting without affecting consumers of the service. If problems arise, the change can be reverted or adjusted more easily.
x??",678,"Example: Shared Tables Figure 5-7  shows our last example. Our catalog needs to store the name and price of the records we sell, and the warehouse needs to keep an electronic record of inventory. We d...",qwen2.5:latest,2025-11-02 10:00:13,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Transactional Boundaries,Transactional Boundaries in Monolithic Schema vs. Separated Schemas,"#### Transactional Boundaries in Monolithic Schema vs. Separated Schemas
Background context: The example illustrates how transactions work within a monolithic schema versus when schemas are separated. In a monolithic schema, all operations can be performed within a single transaction; however, once the schemas are separated, this safety is lost.

:p How does separating database schemas affect transactional integrity?
??x
Separating database schemas affects transactional integrity because operations that were previously contained within a single transaction now span multiple schemas. This means that if one part of an operation succeeds and another fails, it can leave the system in an inconsistent state unless special measures are taken.
x??",749,"Example: Shared Tables Figure 5-7  shows our last example. Our catalog needs to store the name and price of the records we sell, and the warehouse needs to keep an electronic record of inventory. We d...",qwen2.5:latest,2025-11-02 10:00:13,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Transactional Boundaries,Example of Transactional Boundaries,"#### Example of Transactional Boundaries
Background context: The example uses a MusicCorp scenario to illustrate how transactions work when separating database schemas. It shows that operations that need to be atomic across different databases now span two separate transaction boundaries, which can lead to partial failures.

:p What happens if the insert into the order table succeeds but the insert into the picking table fails?
??x
If the insert into the order table succeeds but the insert into the picking table fails, the system will leave the order in a partially completed state. This inconsistency could result in an incorrect inventory count and possible delays in order fulfillment.

To handle this, you might need to implement compensating transactions or other mechanisms to ensure that either both operations succeed or neither does.
x??

---",857,"Example: Shared Tables Figure 5-7  shows our last example. Our catalog needs to store the name and price of the records we sell, and the warehouse needs to keep an electronic record of inventory. We d...",qwen2.5:latest,2025-11-02 10:00:13,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Reporting Database,Retrying Operations,"#### Retrying Operations
Background context: Sometimes, operations that fail to insert into a warehouse’s picking table can be retried later. This approach is often used when dealing with long-lived business operations, ensuring eventual consistency rather than strict transactional consistency upon failure.

:p What is the retry strategy for failed operations?
??x
The retry strategy involves queuing up the operation and attempting it again at a later date. This approach leverages eventual consistency, where the system gets into a consistent state eventually without immediate guarantees.
```java
// Pseudocode example of retry mechanism
public void processOrder(Order order) {
    boolean success = false;
    
    while (!success && shouldRetry(order)) {
        try {
            insertIntoPickingTable(order);
            success = true;
        } catch (Exception e) {
            logError(""Failed to process order: "" + order, e);
            queueForRetryingLater(order);
            Thread.sleep(retryDelay); // Wait before retrying
        }
    }
    
    if (!success) {
        handleFailure(order);
    }
}
```
x??",1131,"Try Again Later The fact that the order was captured and placed might be enough for us, and we may decide to retry the insertion into the warehouse’s picking table at a later date. We could queue up t...",qwen2.5:latest,2025-11-02 10:00:35,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Reporting Database,Compensating Transactions,"#### Compensating Transactions
Background context: When an operation fails and cannot be retried, compensating transactions are used to undo the effects of a failed transaction. This ensures that the system remains in a consistent state despite individual failures.

:p What is a compensating transaction?
??x
A compensating transaction is a mechanism used to reverse or undo the changes made by a failed operation, bringing the system back into a consistent state. For example, if an order was placed but the corresponding pick instruction could not be inserted, issuing a `DELETE` statement on the order would compensate for the failure.

```java
// Pseudocode example of compensating transaction
public void handleOrderFailure(Order order) {
    try {
        // Simulate picking operation
        insertIntoPickingTable(order);
    } catch (Exception e) {
        logError(""Failed to process order: "" + order, e);
        
        // Compensate by removing the order from the database
        deleteFromOrderTable(order);
        
        notifyUserOfFailure();
    }
}
```
x??",1081,"Try Again Later The fact that the order was captured and placed might be enough for us, and we may decide to retry the insertion into the warehouse’s picking table at a later date. We could queue up t...",qwen2.5:latest,2025-11-02 10:00:35,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Reporting Database,Distributed Transactions and Two-Phase Commit,"#### Distributed Transactions and Two-Phase Commit
Background context: For operations involving multiple systems or databases, distributed transactions can ensure consistency across them. The two-phase commit (2PC) protocol is a common method used in this scenario.

:p What is the two-phase commit process?
??x
The two-phase commit process involves a preparatory phase and a commitment phase:
1. **Preparation Phase**: Each participant (e.g., databases or services) votes to either prepare for the transaction or abort.
2. **Commit Phase**: If all participants voted to prepare, they proceed with committing; otherwise, the transaction is aborted.

```java
// Pseudocode example of two-phase commit
public void performTwoPhaseCommit(TransactionManager manager) {
    // Preparation phase
    if (manager.prepare()) {
        // Commit phase
        if (manager.commit()) {
            logInfo(""Transaction committed successfully."");
        } else {
            logError(""Transaction failed during the commit phase."");
        }
    } else {
        logError(""Transaction was not prepared."");
    }
}
```
x??

---",1114,"Try Again Later The fact that the order was captured and placed might be enough for us, and we may decide to retry the insertion into the warehouse’s picking table at a later date. We could queue up t...",qwen2.5:latest,2025-11-02 10:00:35,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Reporting Database,Distributed Transaction Voting Process,"#### Distributed Transaction Voting Process
Background context: In a distributed transaction, each participant (cohort) communicates with the central transaction manager to decide whether it can proceed. The transaction manager aggregates these decisions and either commits or rolls back all transactions based on the outcome.
:p How does the voting process work in distributed transactions?
??x
In the voting process, every cohort (participant) in a distributed transaction independently decides if it thinks its local transaction can go ahead. This decision is sent to the central transaction manager. If all cohorts vote 'yes', the transaction manager commits all participants; if any participant votes 'no', the transaction manager rolls back all transactions.
The voting process ensures that only when consensus is reached among all participants does a commit occur, thus maintaining transaction integrity in a distributed system.

```java
// Pseudocode for Voting Process
public class TransactionManager {
    public void startVoting() {
        // Assume cohorts are stored as a list of Cohort objects
        List<Cohort> cohorts = getParticipants();
        
        // Each cohort makes its decision and sends to the manager
        Map<String, Boolean> votes = new HashMap<>();
        for (Cohort cohort : cohorts) {
            boolean voteResult = cohort.decide(); // Simulates voting logic
            votes.put(cohort.getName(), voteResult);
        }
        
        // Aggregate results from all participants
        if (votes.values().stream().allMatch(vote -> vote)) {
            commitTransactions(could);
        } else {
            rollbackTransactions();
        }
    }
}
```
x??",1707,This is where each participant (also called a  cohort  in this context) in the distributed transaction tells the transaction manager whether it thinks its local transaction can go ahead. If the transa...,qwen2.5:latest,2025-11-02 10:01:03,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Reporting Database,Central Coordination and Outages in Distributed Transactions,"#### Central Coordination and Outages in Distributed Transactions
Background context: The voting process relies on central coordination, meaning that all parties must halt until the transaction manager tells them to proceed. This reliance can lead to issues during outages or when participants fail to respond.
:p What are the risks associated with relying on a single transaction manager for distributed transactions?
??x
The primary risk of relying on a single transaction manager is system availability and reliability. If the transaction manager fails or goes down, all pending transactions that require coordination will be stuck in an indefinite state, neither committed nor rolled back.

Furthermore, if any cohort fails to respond during the voting phase, the entire process can block, leading to potential deadlocks or inconsistent states across the distributed system. Additionally, even after a commit vote is given, failure of a participant to complete its transaction can leave the overall transaction in an uncertain state, violating the ACID properties.

```java
// Pseudocode for Handling Outages
public class TransactionManager {
    public void handleOutage() {
        if (isTransactionManagerDown()) {
            // Try to manually manage pending transactions or fallback to a simpler strategy
            // such as compensating retries or optimistic concurrency control.
            System.out.println(""Transaction Manager is down, attempting manual recovery."");
            
            // Example of compensating retry logic
            for (Cohort cohort : cohorts) {
                try {
                    if (!cohort.isCompleted()) {
                        retry(cohort);
                    }
                } catch (Exception e) {
                    log.error(""Failed to recover transaction"", e);
                }
            }
        }
    }

    private void retry(Cohort cohort) {
        // Logic to attempt another round of voting or manual intervention
    }
}
```
x??",2012,This is where each participant (also called a  cohort  in this context) in the distributed transaction tells the transaction manager whether it thinks its local transaction can go ahead. If the transa...,qwen2.5:latest,2025-11-02 10:01:03,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Reporting Database,Complexity and Scalability Issues in Distributed Transactions,"#### Complexity and Scalability Issues in Distributed Transactions
Background context: Distributed transactions introduce significant complexity due to the need for central coordination, which can lead to performance bottlenecks, lock contention, and overall system scalability issues. The use of compensating retry logic adds further layers of complexity.
:p Why are distributed transactions difficult to scale and maintain?
??x
Distributed transactions are inherently complex because they require centralized coordination among all participants. This coordination introduces several challenges:

1. **Centralization Bottleneck**: A single transaction manager handles the voting process, making it a critical point of failure that can become a bottleneck as the system scales.
2. **Lock Contention**: During the voting phase and execution, cohorts might hold locks on resources, leading to contention when multiple transactions attempt to access the same resource simultaneously. This can significantly degrade performance and make scaling difficult.
3. **Compensating Retry Logic**: In cases where a transaction fails after the initial commit vote, compensating retry logic must be implemented to fix inconsistencies. This adds complexity and can lead to more complex error handling mechanisms.

```java
// Pseudocode for Compensating Retry Logic
public class TransactionManager {
    public void handleCompensation() {
        Map<String, Boolean> votes = getVotes();
        
        if (votes.values().stream().anyMatch(vote -> !vote)) { // Check if any participant voted 'no'
            rollbackTransactions(); // Rollback all transactions
            
            for (Cohort cohort : cohorts) {
                tryCompensate(cohort);
            }
        } else {
            commitTransactions();
        }
    }

    private void tryCompensate(Cohort cohort) {
        // Logic to attempt compensation based on the state of the transaction
        if (!cohort.isCommitted()) {
            cohort.compensate(); // Attempt to roll back changes in case of failure
        }
    }
}
```
x??",2098,This is where each participant (also called a  cohort  in this context) in the distributed transaction tells the transaction manager whether it thinks its local transaction can go ahead. If the transa...,qwen2.5:latest,2025-11-02 10:01:03,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Reporting Database,Alternatives and Best Practices for Distributed Transactions,"#### Alternatives and Best Practices for Distributed Transactions
Background context: When considering distributed transactions, it's crucial to evaluate whether the operations truly require such complexity. Simplifying or avoiding the splitting of state can lead to more scalable and maintainable systems.
:p How should one approach designing a system that needs distributed transactions?
??x
When encountering business operations that currently occur within a single transaction, assess whether they genuinely need to be part of a distributed transaction. Consider if these operations can be broken down into smaller, local transactions that operate under the principle of eventual consistency.

This approach leverages simpler, more scalable mechanisms and reduces complexity by avoiding the pitfalls associated with centralized coordination. If state must remain consistent across multiple systems, ensure it is not fragmented to begin with. Create a concrete concept or entity (e.g., an ""in-process-order"") that represents the transaction end-to-end, providing a clear structure for managing transactions and their compensating actions.

```java
// Example of In-Process-Order Concept
public class OrderManager {
    public void processOrder(Order order) throws Exception {
        try {
            // Local transaction within one system or microservice
            orderService.placeOrder(order);
            
            // Simulate a long-running operation that might fail
            if (Math.random() < 0.1) {
                throw new RuntimeException(""Simulated failure during processing"");
            }
            
            // Further local operations
            billingService.chargeCustomer(order);
        } catch (Exception e) {
            // Handle exception and retry logic if necessary
            handleFailure(order, e);
        }
    }

    private void handleFailure(Order order, Exception cause) {
        // Log the failure and attempt to rollback or compensate as needed
        tryCompensate(order, cause);
    }

    private void tryCompensate(Order order, Exception cause) {
        if (order.isCommitted()) {
            orderService.rollbackOrder(order);
            billingService.refundCustomer(order);
        }
    }
}
```
x??

---",2275,This is where each participant (also called a  cohort  in this context) in the distributed transaction tells the transaction manager whether it thinks its local transaction can go ahead. If the transa...,qwen2.5:latest,2025-11-02 10:01:03,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Reporting Database,Reporting in Microservices Architecture,"#### Reporting in Microservices Architecture
Background context: When splitting a service into smaller parts, we also need to consider how data is stored and managed. This becomes crucial for reporting use cases since traditional monolithic architectures store all data in one database, making it easier to generate reports.

:p What are the challenges faced when moving from a monolithic architecture to a microservices architecture regarding data storage and reporting?
??x
The challenges include:
1. **Shared Schema**: Changes in schema need careful management.
2. **Limited Optimization Options**: The database is optimized for either the live system or the reporting system, but not both.
3. **Database Choice Limitations**: Being constrained to a single type of database limits exploration of new options like graph databases (Neo4j) and column-oriented databases (Cassandra).

For example:
- In a monolithic architecture: 
```sql
SELECT gl.transaction_id, c.product_name, p.price
FROM general_ledger gl
JOIN catalog c ON gl.product_id = c.id
JOIN purchases p ON gl.purchase_id = p.id;
```
This SQL query joins data from the general ledger with descriptions and prices.

In a microservices architecture:
- You might need to use read replicas or even different databases for each service, leading to more complex reporting logic.
x??",1338,"Reporting As we’ve already seen, in splitting a service into smaller parts, we need to also potentially split up how and where data is stored. This creates a problem, however, when it comes to one vit...",qwen2.5:latest,2025-11-02 10:01:26,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Reporting Database,Reporting Database in Monolithic Architecture,"#### Reporting Database in Monolithic Architecture
Background context: In traditional monolithic architectures, all data is stored in one database, making it easier to generate reports by joining data from various sources. Typically, these reports are run on a read replica of the main database.

:p What is the typical setup for reporting systems in monolithic architectures?
??x
In monolithic architectures, reporting systems usually use a read replica of the main database to avoid impacting the performance of the live system.
```java
// Pseudo code for setting up a read replica connection in Java
import java.sql.Connection;
import java.sql.DriverManager;

public class ReportingDB {
    public Connection getReadReplicaConnection() throws SQLException {
        String url = ""jdbc:mysql://read-replica-host:3306/mydatabase"";
        return DriverManager.getConnection(url);
    }
}
```
This setup ensures that the reporting queries do not interfere with the main application's performance.

x??",1001,"Reporting As we’ve already seen, in splitting a service into smaller parts, we need to also potentially split up how and where data is stored. This creates a problem, however, when it comes to one vit...",qwen2.5:latest,2025-11-02 10:01:26,4
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Reporting Database,Challenges in Microservices Reporting,"#### Challenges in Microservices Reporting
Background context: When moving to a microservices architecture, data is split across multiple databases. This requires a different approach for generating reports since each service might have its own database schema and storage solution.

:p What are the downsides of using read replicas in monolithic architectures for reporting?
??x
The main downsides include:
1. **Shared Schema Impediment**: Changes to the shared schema can be difficult to coordinate.
2. **Limited Optimization**: The database cannot be optimized efficiently for both live use and reporting, often resulting in a compromise schema that is suboptimal for both purposes.

For example, if you have different services with distinct schemas:
```sql
// Service 1: General Ledger
SELECT * FROM general_ledger;

// Service 2: Catalog
SELECT * FROM catalog;
```
Generating a report that requires joining data from these two services becomes complex and potentially inefficient.
x??",989,"Reporting As we’ve already seen, in splitting a service into smaller parts, we need to also potentially split up how and where data is stored. This creates a problem, however, when it comes to one vit...",qwen2.5:latest,2025-11-02 10:01:26,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Reporting Database,Alternatives to Traditional Reporting Databases,"#### Alternatives to Traditional Reporting Databases
Background context: As data storage in microservices architectures diversifies, traditional approaches to reporting may no longer be the best solution. There are multiple alternatives available to bring together data for reporting purposes.

:p What are some viable alternatives to the standard reporting database model?
??x
Some viable alternatives include:
1. **Unified Data Lake**: Centralize all data in a unified storage system like Hadoop or AWS S3, then use analytics tools like Apache Spark for querying.
2. **Event-Driven Architecture**: Use event streams (e.g., Kafka) to capture and aggregate events from different services for reporting.
3. **Data Mesh**: Implement a microservices approach where each service has its own data store but also shares a common data catalog.

For example, using a unified data lake:
```python
# Pseudo code for reading from a data lake in Python
from pyspark.sql import SparkSession

def read_data_lake():
    spark = SparkSession.builder.appName(""ReportingApp"").getOrCreate()
    df = spark.read.parquet(""/path/to/data/lake"")
    return df
```
x??

---",1148,"Reporting As we’ve already seen, in splitting a service into smaller parts, we need to also potentially split up how and where data is stored. This creates a problem, however, when it comes to one vit...",qwen2.5:latest,2025-11-02 10:01:26,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Data Retrieval via Service Calls,Data Retrieval for Reporting Systems,"#### Data Retrieval for Reporting Systems
Background context: This section discusses challenges and approaches to data retrieval for reporting systems. Key issues include the inefficiency of making many API calls, especially when dealing with large volumes of data. The text also mentions the need for accurate historical data and how traditional APIs might not be optimized for such use cases.

:p What are some key challenges in retrieving data from microservices for a reporting system?
??x
The main challenges include:
- Making multiple API calls to gather necessary data, which can be inefficient.
- Ensuring that all required data is up-to-date, especially historic data.
- APIs not being designed for bulk data retrieval or reporting use cases.

Code examples are less relevant here as the focus is on explaining context and challenges. However, an example of making multiple API calls could look like:
```java
for (CustomerID customer : getAllCustomers()) {
    CustomerDetails customerDetails = fetchCustomerDetails(customer);
}
```
x??",1045,"Data Retrieval via Service Calls There are many variants of this model, but they all rely on pulling the required data from the source systems via API calls. For a very simple reporting system, like a...",qwen2.5:latest,2025-11-02 10:01:48,7
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Data Retrieval via Service Calls,Local Data Caching vs. Real-Time Data Retrieval,"#### Local Data Caching vs. Real-Time Data Retrieval
Background context: The text explains the dangers of keeping a local copy of data in a reporting system due to potential changes even in historic data. This necessitates fetching real-time data from source systems.

:p Why is keeping a local copy of historical data dangerous for reporting?
??x
Keeping a local copy of historical data is risky because:
- The source systems might update the data even after it has been retrieved, leading to discrepancies.
- Ensuring data integrity and accuracy can become complex with frequent updates.

Code example:
```java
// Example showing why local caching might be problematic
public CustomerDetails fetchCustomerData(CustomerID id) {
    // Local cache retrieval
    if (cache.containsKey(id)) return cache.get(id);
    
    // Real-time API call to update cache
    CustomerDetails details = customerService.fetchDetailsById(id);
    cache.put(id, details);
    return details;
}
```
x??",983,"Data Retrieval via Service Calls There are many variants of this model, but they all rely on pulling the required data from the source systems via API calls. For a very simple reporting system, like a...",qwen2.5:latest,2025-11-02 10:01:48,7
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Data Retrieval via Service Calls,Batch APIs for Reporting Systems,"#### Batch APIs for Reporting Systems
Background context: The text suggests that traditional APIs might not be optimized for reporting needs. A potential solution is using batch APIs where a list of IDs can be passed to retrieve multiple records in one go.

:p How do batch APIs help with data retrieval for reporting?
??x
Batch APIs improve efficiency by allowing the calling system to pass a list of IDs and receive all relevant data in a single call, reducing the number of API requests. This is particularly useful when dealing with large volumes of data.

Example code:
```java
public class CustomerService {
    public void exportCustomers(List<CustomerID> ids, String location) {
        // Process batch request to fetch customers from the list and save file at given location
        File exportedData = new File(location);
        for (CustomerID id : ids) {
            CustomerDetails customer = fetchCustomerDetails(id);
            exportedData.append(customer.toString());
        }
    }
}
```
x??",1013,"Data Retrieval via Service Calls There are many variants of this model, but they all rely on pulling the required data from the source systems via API calls. For a very simple reporting system, like a...",qwen2.5:latest,2025-11-02 10:01:48,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Data Retrieval via Service Calls,Polling Mechanism for Batch Requests,"#### Polling Mechanism for Batch Requests
Background context: The text describes a scenario where batch requests are sent to a service, and the calling system polls until it receives confirmation that the data is ready.

:p What is a polling mechanism used for in the context of reporting systems?
??x
A polling mechanism is used when a request is made to export large amounts of data, and the calling system needs to check periodically (poll) until the service has processed the request and the data is available. This method ensures that the calling system can handle the asynchronous nature of the data retrieval process.

Example code:
```java
public void waitForExportCompletion(String requestUrl) {
    while (true) {
        HttpURLConnection conn = (HttpURLConnection) new URL(requestUrl).openConnection();
        conn.setRequestMethod(""GET"");
        
        int responseCode = conn.getResponseCode();
        if (responseCode == 201) break; // Request fulfilled, proceed to fetch data
        Thread.sleep(5000); // Wait for 5 seconds before polling again
    }
}
```
x??

---",1088,"Data Retrieval via Service Calls There are many variants of this model, but they all rely on pulling the required data from the source systems via API calls. For a very simple reporting system, like a...",qwen2.5:latest,2025-11-02 10:01:48,7
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Data Pumps,Data Pump Overview,"#### Data Pump Overview
Background context explaining why using a data pump can be beneficial. The method involves a standalone program that accesses the database of the service and pushes data into a reporting database, which helps avoid overhead from frequent HTTP calls.

:p What is a data pump and how does it differ from traditional methods of pulling data?
??x
A data pump is a standalone program designed to directly access the internal database of a service and push its data into a separate reporting database. This approach reduces the overhead associated with frequent HTTP calls, as it minimizes the need for creating APIs solely for reporting purposes.

The main difference lies in reducing the latency and resource usage by pushing data rather than pulling it. This can be particularly beneficial when dealing with large volumes of data that would otherwise result in a high number of HTTP requests.

```java
public class DataPump {
    private final DatabaseService databaseService;
    private final ReportingDatabase reportingDatabase;

    public DataPump(DatabaseService databaseService, ReportingDatabase reportingDatabase) {
        this.databaseService = databaseService;
        this.reportingDatabase = reportingDatabase;
    }

    public void pumpData() {
        List<DataRecord> records = databaseService.fetchRecords();
        for (DataRecord record : records) {
            reportingDatabase.save(record);
        }
    }
}
```
x??",1462,"Data Pumps Rather than have the reporting system pull the data, we could instead have the data pushed to the reporting system. One of the downsides of retrieving the data by standard HTTP calls is the...",qwen2.5:latest,2025-11-02 10:02:10,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Data Pumps,Team Management and Deployment,"#### Team Management and Deployment
Explanation of the importance of having the same team manage both the service and its data pump. This ensures consistency and reduces coupling issues by version controlling both.

:p How should a data pump be managed to minimize integration challenges?
??x
The data pump should be built and managed by the same team that manages the service. This can be done as a command-line program triggered via Cron, or another scheduling mechanism. To minimize coupling issues, it's recommended to version-control the code for both the service and the data pump together. Additionally, builds of the data pump should be an artifact created during the build process of the service.

This approach ensures that changes in the service schema are reflected consistently in the reporting database, reducing the complexity of managing multiple systems independently.

```java
public class DataPumpBuild {
    private final ServiceProject serviceProject;
    private final ReportingDatabaseProject reportingDatabaseProject;

    public DataPumpBuild(ServiceProject serviceProject, ReportingDatabaseProject reportingDatabaseProject) {
        this.serviceProject = serviceProject;
        this.reportingDatabaseProject = reportingDatabaseProject;
    }

    public void createDataPumpArtifact() {
        // Build the service and data pump artifacts
        ServiceVersion serviceVersion = serviceProject.build();
        ReportingDatabaseVersion reportingVersion = reportingDatabaseProject.build(serviceVersion);

        // Package both versions together as an artifact
        Artifact artifact = new Artifact(serviceVersion, reportingVersion);
        artifact.save();
    }
}
```
x??",1705,"Data Pumps Rather than have the reporting system pull the data, we could instead have the data pushed to the reporting system. One of the downsides of retrieving the data by standard HTTP calls is the...",qwen2.5:latest,2025-11-02 10:02:10,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Data Pumps,Materialized Views for Aggregation,"#### Materialized Views for Aggregation
Explanation of using materialized views to reduce coupling and improve performance in relational databases. This technique allows the creation of a single monolithic reporting schema by exposing only the necessary data.

:p How can materialized views be used to mitigate database integration challenges?
??x
Materialized views can be utilized to create an aggregated view of data from multiple services within a single schema, thereby mitigating traditional DB integration challenges. By having one schema in the reporting database for each service and using materialized views, only the necessary data is exposed to the customer data pump.

This approach reduces coupling by isolating the reporting schema as a published API that is harder to change. However, its effectiveness depends on the capabilities of the chosen database.

```java
public class MaterializedView {
    private final ReportingDatabase reportingDatabase;
    private final ServiceSchema serviceSchema;

    public MaterializedView(ReportingDatabase reportingDatabase, ServiceSchema serviceSchema) {
        this.reportingDatabase = reportingDatabase;
        this.serviceSchema = serviceSchema;
    }

    public void createMaterializedView() {
        String query = ""CREATE MATERIALIZED VIEW agg_data AS SELECT * FROM service_schema"";
        reportingDatabase.execute(query);
    }
}
```
x??

---",1411,"Data Pumps Rather than have the reporting system pull the data, we could instead have the data pushed to the reporting system. One of the downsides of retrieving the data by standard HTTP calls is the...",qwen2.5:latest,2025-11-02 10:02:10,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Mapping Continuous Integration to Microservices,JSON File Population Using AWS S3,"#### JSON File Population Using AWS S3

Background context: In one project, data was populated into JSON files stored in AWS S3 to masquerade as a giant data mart. This approach worked well but faced scalability issues that necessitated a change.

:p How did the initial system populate data using AWS S3?
??x
The initial system used data pumps to populate JSON files directly into AWS S3, treating it like a central repository or data mart.
```java
// Example pseudocode for populating JSON files in S3 using a data pump
public void populateS3WithJSONData(String fileName, String jsonData) {
    // Code to write jsonData into an S3 bucket as fileName
}
```
x??",662,"Alternative Destinations On one project I was involved with, we used a series of data pumps to populate JSON files in AWS S3, effectively using S3 to masquerade as a giant data mart. This approach wor...",qwen2.5:latest,2025-11-02 10:02:28,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Mapping Continuous Integration to Microservices,Event Data Pump,"#### Event Data Pump

Background context: Microservices emit events based on state changes of managed entities. These events can be used by external subscribers, like a reporting system, to populate databases without coupling directly to the source microservice's internal workings.

:p How does an event data pump differ from a traditional data pump in terms of architecture?
??x
An event data pump decouples the process of populating the reporting database from the internal workings of the service emitting events. Instead of relying on scheduled data pumping, it listens for specific events and processes them as they occur.

Example pseudocode:
```java
public class EventDataPump {
    private Set<String> processedEvents;

    public void subscribeToEventStream(EventEmitter emitter) {
        emitter.on(""event-created"", (data) -> processNewEventData(data));
        emitter.on(""event-updated"", (data) -> processUpdatedEventData(data));
        emitter.on(""event-deleted"", (data) -> processDeletedEventData(data));
    }

    private void processNewEventData(String data) {
        // Process and insert new event into the reporting system
    }

    private void processUpdatedEventData(String data) {
        // Update existing records in the reporting system based on the event
    }

    private void processDeletedEventData(String data) {
        // Handle deletion events to reflect changes in the reporting system
    }
}
```
x??",1443,"Alternative Destinations On one project I was involved with, we used a series of data pumps to populate JSON files in AWS S3, effectively using S3 to masquerade as a giant data mart. This approach wor...",qwen2.5:latest,2025-11-02 10:02:28,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Mapping Continuous Integration to Microservices,Backup Data Pump,"#### Backup Data Pump

Background context: This approach leverages existing backup solutions to handle large volumes of data, especially useful for systems like Netflix that face significant scale issues. It acts somewhat like a special case of a traditional data pump but is distinct due to its use of backups.

:p How does the backup data pump solution address scalability challenges?
??x
The backup data pump solution addresses scalability by utilizing existing backup mechanisms, which are designed to handle large volumes of data efficiently. This approach minimizes the need for additional infrastructure and processing power typically required by traditional data pumps.

Example pseudocode:
```java
public class BackupDataPump {
    public void startBackup() {
        // Code to initiate a backup process using available tools
    }

    public void processBackedUpData(String backupFilePath) {
        // Code to read and process the backed-up data, ideally integrating it into the reporting system
    }
}
```
x??

---",1029,"Alternative Destinations On one project I was involved with, we used a series of data pumps to populate JSON files in AWS S3, effectively using S3 to masquerade as a giant data mart. This approach wor...",qwen2.5:latest,2025-11-02 10:02:28,4
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Mapping Continuous Integration to Microservices,Netflix's Data Backup Strategy,"#### Netflix's Data Backup Strategy
Background context: Netflix uses Cassandra as its database and has adopted a specific strategy to back up this data. The SSTables, which are the actual storage format for Cassandra data, are copied to Amazon S3 for safekeeping.

:p How does Netflix ensure data backup for its Cassandra databases?
??x
Netflix ensures data backup by making copies of SSTable files (the actual storage format in Cassandra) and storing them in Amazon S3. This approach leverages S3's durability guarantees.
```java
// Pseudocode to illustrate the backup process
public class BackupSSTables {
    public void backupToS3(String localPath, String s3BucketName) {
        // Code to copy SSTable files from local path to S3 bucket
        // Example using AWS SDK for Java
        AmazonS3 s3Client = AmazonS3ClientBuilder.defaultClient();
        File file = new File(localPath);
        String objectKey = ""backups/"" + file.getName();
        ObjectMetadata metadata = new ObjectMetadata();
        s3Client.putObject(new PutObjectRequest(s3BucketName, objectKey, file));
    }
}
```
x??",1101,"Netflix has decided to standardize on Cassandra as the backing store for its services, of which there are many. Netflix has invested significant time in building tools to make Cassandra easy to work w...",qwen2.5:latest,2025-11-02 10:02:49,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Mapping Continuous Integration to Microservices,Netflix's Data Reporting Pipeline,"#### Netflix's Data Reporting Pipeline
Background context: Netflix uses Hadoop to process SSTable backups for reporting across large amounts of data. They have open-sourced this approach as the Aegisthus project.

:p What tool does Netflix use to report across all its Cassandra data, and what is one notable outcome of using this tool?
??x
Netflix uses Hadoop to report across all its Cassandra data by processing SSTable backups. The notable outcome is that they have open-sourced their solution as the Aegisthus project.
```java
// Pseudocode illustrating the use of Hadoop for processing SSTables
public class DataProcessingPipeline {
    public void processSSTables(String inputPath, String outputPath) throws IOException {
        // Code to read SSTable files from input path and write processed data to output path using Hadoop MapReduce
        Configuration conf = new Configuration();
        FileInputFormat.addInputPath(conf, new Path(inputPath));
        FileOutputFormat.setOutputPath(conf, new Path(outputPath));
        
        Job job = Job.getInstance(conf);
        job.setJarByClass(DataProcessingPipeline.class);
        job.setMapperClass(MySSTableMapper.class);
        job.setReducerClass(MySSTableReducer.class);
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(IntWritable.class);
        
        job.waitForCompletion(true);
    }
}
```
x??",1396,"Netflix has decided to standardize on Cassandra as the backing store for its services, of which there are many. Netflix has invested significant time in building tools to make Cassandra easy to work w...",qwen2.5:latest,2025-11-02 10:02:49,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Mapping Continuous Integration to Microservices,Generic Eventing Systems for Real-Time Data,"#### Generic Eventing Systems for Real-Time Data
Background context: The approach of using a single location for reporting is being reconsidered due to the variety of use cases with different accuracy and timeliness requirements. Netflix is moving towards generic eventing systems that can route data to multiple places as needed.

:p Why are generic eventing systems becoming more important in modern data processing?
??x
Generic eventing systems are becoming more important because they allow for flexibility in handling data across various use cases, each with different accuracy and timeliness requirements. This enables routing data to multiple destinations based on specific needs.
```java
// Pseudocode illustrating a simple event routing system
public class EventRoutingSystem {
    public void routeEvent(Event event) {
        if (event.isRealTime()) {
            realTimeProcessor.process(event);
        } else if (event.needsDashboards()) {
            dashboardPublisher.publish(event);
        } else if (event.needAlerts()) {
            alertingSystem.sendAlert(event);
        }
    }
}
```
x??",1113,"Netflix has decided to standardize on Cassandra as the backing store for its services, of which there are many. Netflix has invested significant time in building tools to make Cassandra easy to work w...",qwen2.5:latest,2025-11-02 10:02:49,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Mapping Continuous Integration to Microservices,Cost of Change in Data Management,"#### Cost of Change in Data Management
Background context: Making small, incremental changes is promoted to better understand and mitigate the impact of each change, reducing the risk of mistakes. However, certain operations like splitting databases or rewriting APIs can be complex.

:p What are some reasons why splitting apart a database or re-writing an API can be risky?
??x
Splitting apart a database or re-writing an API can be risky because these operations are much more complex and harder to roll back compared to moving code within a codebase. These actions require significant effort, making them increasingly risky.
```java
// Pseudocode illustrating the complexity of splitting a database
public class DatabaseSplitter {
    public void splitDatabase(Database db1, Database db2) throws Exception {
        // Code to carefully move data and reconfigure schemas between databases
        for (Table table : db1.getTables()) {
            // Move data from old schema to new one
            if (!table.isCritical()) {
                table.moveTo(db2);
            }
        }
        // Additional steps like updating references, indexes, etc.
    }
}
```
x??

---",1177,"Netflix has decided to standardize on Cassandra as the backing store for its services, of which there are many. Netflix has invested significant time in building tools to make Cassandra easy to work w...",qwen2.5:latest,2025-11-02 10:02:49,7
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Mapping Continuous Integration to Microservices,Service Decomposition and Boundaries,"#### Service Decomposition and Boundaries
Background context: In this section, we discuss the process of decomposing large services into smaller, more manageable ones. This is done by identifying seams where service boundaries can emerge incrementally. The goal is to make it easier to maintain and evolve the system over time.
:p How do you identify suitable seams for splitting a large service?
??x
To identify suitable seams for splitting a large service, we start by finding areas of functionality that could operate independently. We use techniques like class-responsibility-collaboration (CRC) cards to understand the responsibilities and collaborations within each potential service. This helps in visualizing how different parts of the system interact with one another.
??x
To illustrate this, let's consider a music shop scenario where customers can search for records, register on the website, or purchase albums. We would create CRC cards for each relevant component to understand its responsibilities and interactions.

```pseudocode
// Example CRC card for a Customer Service
class CustomerService {
    Responsibilities:
        - Handle customer registration
        - Process purchases
        - Search for records
    
    Collaborators:
        - PaymentGateway
        - RecordRepository
}
```
x??

#### Root Causes of Large Services
Background context: The text discusses why large services grow beyond a reasonable size and the challenges in splitting them. It emphasizes that growing a service is acceptable but must be done before it becomes too costly.
:p Why do services tend to grow so large, and what are the challenges in splitting them?
??x
Services tend to grow large because as they handle more functionality and scale with the system's requirements, their complexity increases over time. This growth often occurs incrementally without a clear plan for modularity, leading to tightly coupled components that are difficult to isolate.
The main challenges in splitting large services include:
1. **Finding Starting Points**: Identifying where to start decomposition can be challenging since the service might have many interdependencies.
2. **Cost of Splitting**: Resources like infrastructure, development, and testing need to be reallocated, which can be time-consuming.

To address these challenges, libraries, lightweight frameworks, and self-service provision tools can help reduce the cost and complexity of splitting services.
??x
```java
// Example Code for Provisioning a New Service Instance
public class ServiceProvisioner {
    public void createNewServiceInstance(String serviceName) throws Exception {
        // Code to allocate resources, set up new service environment
        System.out.println(""Creating instance for "" + serviceName);
    }
}
```
x??

#### Incremental Approach to Decomposition
Background context: The text suggests that decomposing a system should be done incrementally. This means making small changes over time rather than attempting a major overhaul all at once.
:p How can we implement an incremental approach to decompose our services?
??x
An incremental approach involves identifying seams or areas of the service where it is relatively easy to isolate functionality. For example, in a music shop scenario:
1. **Identify Functional Areas**: Break down responsibilities like customer registration, record search, and album purchase.
2. **Create CRC Cards**: Use CRC cards to define the responsibilities and collaborations for each new service.
3. **Implement Gradually**: Start by refactoring one area at a time, ensuring that each change is thoroughly tested before moving on.

Here’s an example of how this could be implemented:

```pseudocode
// Step 1: Define Responsibilities
class CustomerRegistrationService {
    Responsibilities:
        - Handle customer sign-up process
    
    Collaborators:
        - Database
}

class RecordSearchService {
    Responsibilities:
        - Search for records based on customer queries
    
    Collaborators:
        - RecordRepository
}
```
x??

#### Cost of Splitting Services
Background context: The text highlights the costs associated with splitting large services, such as finding new infrastructure and setting up a new service stack. It suggests ways to reduce these costs through investment in tools and platforms.
:p What are some ways to reduce the cost of splitting services?
??x
Reducing the cost of splitting services can be achieved by:
1. **Investment in Libraries and Frameworks**: Using established libraries and lightweight frameworks can save development time and effort.
2. **Self-Service Provision Tools**: Providing access to self-service virtual machines or platform as a service (PaaS) can help with quick setup and testing of new services.

For instance, using a PaaS like AWS Elastic Beanstalk can simplify setting up a new environment for the service:
```java
// Example Code Using AWS Elastic Beanstalk
public class ServiceDeployer {
    public void deployServiceToElasticBeanstalk(String serviceName) throws Exception {
        // Code to deploy the service to AWS Elastic Beanstalk
        System.out.println(""Deploying "" + serviceName + "" to Elastic Beanstalk"");
    }
}
```
x??

#### Summary of Decomposition and Evolution
Background context: The text concludes by summarizing the importance of decomposing services incrementally, reducing the cost of splitting, and allowing for easier maintenance and evolution over time.
:p What is the overall message about service decomposition in this chapter?
??x
The overall message emphasizes that it's acceptable to grow a service as long as we plan for eventual decomposition. The key is to identify seams where functionality can be isolated incrementally. By doing so, we can keep services manageable and reduce the costs associated with splitting them into smaller, more focused services.
This approach allows us to evolve our systems in an incremental fashion, adapting to new requirements and maintaining a clean architecture.
??x
```java
// Example Code for Incremental Service Decomposition
public class ServiceDecomposer {
    public void decomposeService(String serviceName) throws Exception {
        // Step 1: Identify responsibilities
        identifyResponsibilities(serviceName);
        
        // Step 2: Create CRC cards
        createCRCCards(serviceName);
        
        // Step 3: Implement new services gradually
        implementNewServices(serviceName);
    }
    
    private void identifyResponsibilities(String serviceName) {
        System.out.println(""Identifying responsibilities for "" + serviceName);
    }
    
    private void createCRCCards(String serviceName) {
        System.out.println(""Creating CRC cards for "" + serviceName);
    }
    
    private void implementNewServices(String serviceName) {
        System.out.println(""Implementing new services for "" + serviceName);
    }
}
```
x??
---",6929,"Sketch out your proposed design. See what happens when you run use cases across what you think your service boundaries will be. For our music shop, for example, imagine what happens when a customer se...",qwen2.5:latest,2025-11-02 10:03:15,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Mapping Continuous Integration to Microservices,Continuous Integration (CI),"#### Continuous Integration (CI)
Continuous integration is a practice that involves integrating code changes from multiple contributors into a shared repository frequently, typically several times a day. This helps to detect integration issues early and ensures that all contributions are compatible with each other.

The core goal of CI is to keep everyone in sync by ensuring that newly checked-in code properly integrates with existing code. A CI server detects code commits, checks out the code, and performs verification steps like compiling the code and running tests.

:p What is continuous integration (CI) and what is its main objective?
??x
The main objective of continuous integration is to keep everyone in sync by ensuring that newly checked-in code properly integrates with existing code. It involves integrating code changes from multiple contributors into a shared repository frequently, typically several times a day. This helps to detect integration issues early and ensures compatibility among all contributions.
x??",1035,We have many more moving parts to get into production now. So next up we’ll dive into the world of deployment. Chapter 6. Deployment Deploying a monolithic application is a fairly straightforward proc...,qwen2.5:latest,2025-11-02 10:03:27,4
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Mapping Continuous Integration to Microservices,Artifacts for Microservices,"#### Artifacts for Microservices
In the context of microservices deployment, we often create artifacts that are used for further validation. These artifacts can include compiled code, binaries, or running services that can be tested.

To enable these artifacts to be reused, they are placed in a repository where they can be accessed and deployed consistently across different environments.

:p What are artifacts in the context of microservices deployment?
??x
Artifacts in the context of microservices deployment refer to compiled code, binaries, or running services that are created as part of the CI process. These artifacts are used for further validation and placed in a repository to be accessed and deployed consistently across different environments.
x??",763,We have many more moving parts to get into production now. So next up we’ll dive into the world of deployment. Chapter 6. Deployment Deploying a monolithic application is a fairly straightforward proc...,qwen2.5:latest,2025-11-02 10:03:27,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Mapping Continuous Integration to Microservices,Benefits of Continuous Integration (CI),"#### Benefits of Continuous Integration (CI)
Continuous integration provides several benefits:
- Fast feedback on code quality
- Automation of binary artifact creation
- Version control of all code required to build the artifact
- Traceability from a deployed artifact back to the code, including details of tests run

:p What are some key benefits of continuous integration?
??x
Key benefits of continuous integration include fast feedback on code quality, automation of binary artifact creation, version control of all code required to build the artifact, and traceability from a deployed artifact back to the code, which includes details of tests run.
x??",658,We have many more moving parts to get into production now. So next up we’ll dive into the world of deployment. Chapter 6. Deployment Deploying a monolithic application is a fairly straightforward proc...,qwen2.5:latest,2025-11-02 10:03:27,7
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Mapping Continuous Integration to Microservices,CI vs. CI Tool Adoption,"#### CI vs. CI Tool Adoption
It's important to distinguish between using a CI tool and adopting the practice of continuous integration. A CI tool is just an enabling mechanism for the approach.

:p Why is there a difference between using a CI tool and adopting the practice of continuous integration?
??x
There is a difference because simply using a CI tool does not necessarily mean that you are following the practices of continuous integration. The practice involves integrating code changes frequently, ensuring compatibility through automated tests, and maintaining version control for all build artifacts. A CI tool enables these practices but on its own does not guarantee their effective implementation.
x??

---",720,We have many more moving parts to get into production now. So next up we’ll dive into the world of deployment. Chapter 6. Deployment Deploying a monolithic application is a fairly straightforward proc...,qwen2.5:latest,2025-11-02 10:03:27,5
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Mapping Continuous Integration to Microservices,Jez Humble’s Three Questions for CI,"#### Jez Humble’s Three Questions for CI
Jez Humble proposed three questions to test understanding of Continuous Integration (CI): 
1. Do you check in to mainline once per day? Frequent integration helps ensure that your code integrates well with others' changes.
2. Do you have a suite of tests to validate your changes? Tests are necessary for ensuring that the behavior of the system is not broken.
3. When the build is broken, is it the #1 priority of the team to fix it? A passing green build means safe integration; a red build indicates potential issues.

:p What do Jez Humble's three questions test about CI?
??x
Jez Humble's three questions assess whether you understand the fundamental principles of CI:
- Frequent integration (daily check-ins) ensures smooth merging with others' changes.
- A suite of tests verifies that your code doesn't break existing functionality.
- The importance of fixing a broken build immediately to maintain a green status.

The first question ensures regular and frequent integrations, reducing the complexity and risk associated with integrating large sets of changes. The second guarantees that changes are validated before integration to prevent breaking the system. The third emphasizes prioritizing build fixes, ensuring continuous delivery and preventing the accumulation of integration issues.
x??",1345,I really like Jez Humble’s three questions he asks people to test if they really understand what CI is about: Do you check in to mainline once per day? You need to make sure your code integrates. If y...,qwen2.5:latest,2025-11-02 10:03:48,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Mapping Continuous Integration to Microservices,Microservices CI Mapping,"#### Microservices CI Mapping
When implementing CI for microservices, it’s important to map individual services to their own builds rather than a single monolithic one.

:p How should microservices be mapped to CI builds?
??x
Microservices should each have dedicated CI builds. This approach allows isolated changes in one service without affecting others, ensuring independent deployment and testing of each component.

For example:
- Each microservice has its own repository.
- Each repository triggers a separate build process.
- The build validates the service independently before deploying.

This setup ensures that developers can make small, incremental changes to individual services without impacting the entire system. It also facilitates easier debugging and quicker troubleshooting when issues arise.

```java
// Pseudocode for CI pipeline for a microservice

public class MicroserviceCI {
    public void runTests() {
        // Run unit tests
        // Run integration tests
        // Run end-to-end tests
        if (allTestsPass) {
            System.out.println(""Build is green."");
        } else {
            System.out.println(""Build is red. Fix the issues."");
        }
    }
}
```
x??",1208,I really like Jez Humble’s three questions he asks people to test if they really understand what CI is about: Do you check in to mainline once per day? You need to make sure your code integrates. If y...,qwen2.5:latest,2025-11-02 10:03:48,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Mapping Continuous Integration to Microservices,Single Repository vs. Multiple Repositories for CI,"#### Single Repository vs. Multiple Repositories for CI
A single repository can be used for all microservices, but this approach might not always be ideal.

:p How does a single repository with one build process impact CI?
??x
Using a single repository and a unified build process may simplify the initial setup, as it reduces the number of repositories to manage. However, this approach could lead to inefficiencies when making small changes:

- Frequent integrations (daily check-ins) are required.
- All services get tested even if only one service has been changed.
- This can increase wait times and resource usage unnecessarily.

For example:
If you have multiple microservices in a single repository:
```java
// Pseudocode for unified CI build

public class UnifiedCI {
    public void runAllTests() {
        // Run tests for all services
        if (allTestsPass) {
            System.out.println(""Build is green."");
        } else {
            System.out.println(""Build is red. Fix the issues."");
        }
    }
}
```

In this setup, every check-in triggers a full build and test of all microservices, which might be unnecessary for minor changes.

To optimize, consider using separate repositories or multi-repository CI systems that focus on individual services.
x??

---",1285,I really like Jez Humble’s three questions he asks people to test if they really understand what CI is about: Do you check in to mainline once per day? You need to make sure your code integrates. If y...,qwen2.5:latest,2025-11-02 10:03:48,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Mapping Continuous Integration to Microservices,Cycle Time and Deployment Complexity,"#### Cycle Time and Deployment Complexity
Background context: The text discusses challenges related to deploying changes efficiently, particularly when working with microservices. Key points include managing cycle time (the speed of moving a change from development to live) and determining which services need to be deployed.

:p What are the main challenges in deploying changes in a microservices architecture?
??x
The primary challenges involve accurately identifying which services should be deployed based on small, individual changes. This can lead to inefficiencies if you end up redeploying everything together or face build failures that block other deployments.
??x",676,"This impacts our cycle time, the speed at which we can move a single change from development to live. More troubling, though, is knowing what artifacts should or shouldn’t be deployed. Do I now need t...",qwen2.5:latest,2025-11-02 10:04:04,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Mapping Continuous Integration to Microservices,Build Services and Deployment,"#### Build Services and Deployment
Background context: The text explores different approaches to managing builds in microservices environments. These include deploying all services together, using a monolithic source control system with multiple CI builds, and having one CI build per microservice.

:p What is the risk of deploying everything together?
??x
Deploying everything together can lead to significant delays if there's an issue that requires fixing before other changes can proceed. This approach also doesn't allow for fine-grained control over which services are updated.
??x",588,"This impacts our cycle time, the speed at which we can move a single change from development to live. More troubling, though, is knowing what artifacts should or shouldn’t be deployed. Do I now need t...",qwen2.5:latest,2025-11-02 10:04:04,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Mapping Continuous Integration to Microservices,Monolithic Source Control and Build Process,"#### Monolithic Source Control and Build Process
Background context: The text contrasts a monolithic source tree with multiple CI builds versus having one source code repository per microservice, each with its own CI build.

:p What is the main downside of using a single source repo with subdirectories mapped to independent builds?
??x
The risk here is that developers might get into the habit of checking in changes for multiple services at once, potentially leading to tightly coupled services.
??x",502,"This impacts our cycle time, the speed at which we can move a single change from development to live. More troubling, though, is knowing what artifacts should or shouldn’t be deployed. Do I now need t...",qwen2.5:latest,2025-11-02 10:04:04,7
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Mapping Continuous Integration to Microservices,Microservice-Specific CI Builds,"#### Microservice-Specific CI Builds
Background context: The preferred approach discussed is having one CI build per microservice. This allows for rapid and targeted deployment.

:p Why is a single CI build per microservice advantageous?
??x
It provides clear ownership of the service by the respective team, simplifies testing, and enables faster validation before production deployment.
??x",392,"This impacts our cycle time, the speed at which we can move a single change from development to live. More troubling, though, is knowing what artifacts should or shouldn’t be deployed. Do I now need t...",qwen2.5:latest,2025-11-02 10:04:04,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Mapping Continuous Integration to Microservices,Test Automation in Microservices,"#### Test Automation in Microservices
Background context: The text emphasizes that tests should be part of each microservice's source code repository to ensure accurate test coverage.

:p How can tests for a microservice be managed effectively?
??x
Tests for a microservice should reside within the same source control as its code. This ensures that developers run the correct set of tests every time they make changes.
??x",423,"This impacts our cycle time, the speed at which we can move a single change from development to live. More troubling, though, is knowing what artifacts should or shouldn’t be deployed. Do I now need t...",qwen2.5:latest,2025-11-02 10:04:04,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Mapping Continuous Integration to Microservices,Continuous Delivery Integration,"#### Continuous Delivery Integration
Background context: The text moves beyond CI to discuss continuous delivery, indicating an end-to-end process from development to production.

:p What is the relationship between CI and CD in this context?
??x
Continuous Delivery (CD) extends the benefits of CI by automating not just builds but also deployments. It ensures that code changes can be safely delivered to production at any time.
??x",434,"This impacts our cycle time, the speed at which we can move a single change from development to live. More troubling, though, is knowing what artifacts should or shouldn’t be deployed. Do I now need t...",qwen2.5:latest,2025-11-02 10:04:04,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Build Pipelines and Continuous Delivery,Build Pipelines and Continuous Delivery,"#### Build Pipelines and Continuous Delivery
Continuous integration (CI) is a method where developers merge their work frequently, usually several times a day, into a shared repository. This allows for multiple stages within a build to be introduced, particularly useful when different types of tests are involved. The idea behind this is to ensure that fast, small-scoped tests provide quick feedback before slow, large-scoped tests run.
:p What is the purpose of having multiple stages in a build pipeline?
??x
The purpose of having multiple stages in a build pipeline is to separate fast, small-scoped tests from slower, larger-scoped tests. This allows for quicker feedback on failing fast tests without waiting for slower tests to complete. If the fast tests fail, there's no need to run slow tests, which saves time and resources.
x??",840,"Build Pipelines and Continuous Delivery Very early on in using continuous integration, we realized the value in sometimes having multiple stages inside a build. Tests are a very common case where this...",qwen2.5:latest,2025-11-02 10:04:33,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Build Pipelines and Continuous Delivery,Build Pipelines,"#### Build Pipelines
Build pipelines are a way of organizing stages in a build process where each stage runs specific tasks or tests. This helps in tracking the progress of software as it clears through different environments, providing insights into its quality before release.

:p How does a build pipeline help in managing different types of tests?
??x
A build pipeline helps manage different types of tests by segregating them into distinct stages. For instance, you can have one stage for fast unit tests and another for slower integration or end-to-end tests. This separation ensures that fast feedback is available early on, reducing the time wasted waiting for slow tests to complete if fast tests fail.
x??",715,"Build Pipelines and Continuous Delivery Very early on in using continuous integration, we realized the value in sometimes having multiple stages inside a build. Tests are a very common case where this...",qwen2.5:latest,2025-11-02 10:04:33,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Build Pipelines and Continuous Delivery,Continuous Delivery,"#### Continuous Delivery
Continuous delivery (CD) builds upon continuous integration by treating each check-in as a potential release candidate. It involves modeling all processes from development to production and tracking the readiness of software versions.

:p What is the main difference between continuous integration and continuous delivery?
??x
The main difference between continuous integration and continuous delivery is that CI focuses on frequent merging of code into a shared repository, while CD goes further by treating every commit as a potential release candidate. CD involves modeling all processes from development to production, ensuring each version is ready for deployment.
x??",698,"Build Pipelines and Continuous Delivery Very early on in using continuous integration, we realized the value in sometimes having multiple stages inside a build. Tests are a very common case where this...",qwen2.5:latest,2025-11-02 10:04:33,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Build Pipelines and Continuous Delivery,Multistage Build Pipelines,"#### Multistage Build Pipelines
Multistage build pipelines extend the concept of continuous integration by including multiple stages that an artifact must pass through to be considered production-ready. This helps in tracking the quality and progress of software as it moves towards release.

:p How do multistage build pipelines differ from traditional CI pipelines?
??x
Multistage build pipelines differ from traditional CI pipelines by incorporating more stages that represent different environments or processes (e.g., testing, UAT, staging) before a final production release. This adds visibility and ensures each stage is validated before moving to the next.
x??",668,"Build Pipelines and Continuous Delivery Very early on in using continuous integration, we realized the value in sometimes having multiple stages inside a build. Tests are a very common case where this...",qwen2.5:latest,2025-11-02 10:04:33,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Build Pipelines and Continuous Delivery,Standard Release Process as a Build Pipeline,"#### Standard Release Process as a Build Pipeline
A standard release process modeled as a build pipeline includes various stages such as development, testing, user acceptance testing (UAT), staging, and finally deployment to production.

:p What does a typical build pipeline include?
??x
A typical build pipeline includes several stages: 
1. Development: Where code is committed.
2. Testing: Various automated tests run here.
3. UAT: Manual user acceptance testing.
4. Staging: Pre-production environment for final validation.
5. Deployment: Release to the production environment.

This structure helps in tracking the progress and quality of software as it moves through each stage before being released.
x??",710,"Build Pipelines and Continuous Delivery Very early on in using continuous integration, we realized the value in sometimes having multiple stages inside a build. Tests are a very common case where this...",qwen2.5:latest,2025-11-02 10:04:33,7
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Build Pipelines and Continuous Delivery,Model All Processes,"#### Model All Processes
In continuous delivery, all processes from check-in to production are modeled to track the readiness of every version of the software.

:p Why is it important to model all processes in a build pipeline?
??x
It is important to model all processes in a build pipeline because it ensures visibility into the entire release process. By modeling each stage, you can track the quality and status of your code as it moves through different environments, from development to production. This helps in identifying bottlenecks and issues early on.
x??",566,"Build Pipelines and Continuous Delivery Very early on in using continuous integration, we realized the value in sometimes having multiple stages inside a build. Tests are a very common case where this...",qwen2.5:latest,2025-11-02 10:04:33,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Build Pipelines and Continuous Delivery,Artifacts in Build Pipelines,"#### Artifacts in Build Pipelines
Artifacts are created at various stages in a build pipeline and move through these stages, providing confidence that the software will work in production.

:p What is an artifact in the context of build pipelines?
??x
An artifact in the context of build pipelines refers to the output produced by different stages of the pipeline. For example, it could be compiled code, package files, or any other deliverables generated during development, testing, and deployment. The artifact moves through each stage, and its successful completion at each stage increases confidence that the software will work in production.
x??",651,"Build Pipelines and Continuous Delivery Very early on in using continuous integration, we realized the value in sometimes having multiple stages inside a build. Tests are a very common case where this...",qwen2.5:latest,2025-11-02 10:04:33,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Build Pipelines and Continuous Delivery,Microservices World,"#### Microservices World
In a microservices world, where services are released independently, one pipeline per service is recommended to ensure proper release management.

:p How does the concept of build pipelines apply differently in a microservices architecture?
??x
In a microservices architecture, each microservice has its own pipeline. This ensures that individual services can be released and updated independently without affecting others. Each pipeline focuses on the specific logic and dependencies of that service, providing fine-grained control over release processes.
x??

---",590,"Build Pipelines and Continuous Delivery Very early on in using continuous integration, we realized the value in sometimes having multiple stages inside a build. Tests are a very common case where this...",qwen2.5:latest,2025-11-02 10:04:33,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Custom Images,Initial Service Boundaries for Greenfield Projects,"#### Initial Service Boundaries for Greenfield Projects
When a team is starting out with a new project, especially one that’s completely new, there will be significant churn as they figure out where service boundaries should lie. During this initial phase, it's often beneficial to keep services larger and in the same build because changes across these boundaries are frequent.
:p Why might it make sense to keep all services in a single build during the early stages of a project?
??x
During the early stages, keeping services together can reduce the overhead of cross-service changes, making development and debugging easier. However, this approach is meant as a transitionary measure until service boundaries stabilize.
x??",727,"And the Inevitable Exceptions As with all good rules, there are exceptions we need to consider too. The “one microservice per build” approach is absolutely something you should aim for, but are there ...",qwen2.5:latest,2025-11-02 10:05:03,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Custom Images,Platform-Specific Artifacts,"#### Platform-Specific Artifacts
Most technology stacks have their own first-class artifacts like JAR files in Java or Ruby gems. These are typically used to package and deploy applications but may not be sufficient for all deployment scenarios due to the need for additional software and configuration.
:p How might a platform-specific artifact differ from what is needed for complete deployment of an application?
??x
For example, while a Java JAR file can run as an executable, Ruby or Python applications often require running inside process managers like Apache or Nginx. This necessitates using tools like Puppet or Chef to manage and configure additional software.
x??",675,"And the Inevitable Exceptions As with all good rules, there are exceptions we need to consider too. The “one microservice per build” approach is absolutely something you should aim for, but are there ...",qwen2.5:latest,2025-11-02 10:05:03,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Custom Images,Operating System Artifacts,"#### Operating System Artifacts
To avoid the issues associated with technology-specific artifacts, creating native operating system artifacts can be a better approach. These artifacts are designed to work directly on the OS level, potentially simplifying deployment across different technologies.
:p Why might operating system artifacts provide an advantage over technology-specific ones?
??x
Operating system artifacts can help in standardizing deployments regardless of the underlying technology stack. For example, creating binary packages that can be installed and managed via package managers like apt or yum can simplify cross-platform deployment.
x??",657,"And the Inevitable Exceptions As with all good rules, there are exceptions we need to consider too. The “one microservice per build” approach is absolutely something you should aim for, but are there ...",qwen2.5:latest,2025-11-02 10:05:03,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Custom Images,Transitionary Build Strategies,"#### Transitionary Build Strategies
While it's ideal to keep each microservice in its own build for better isolation and flexibility, there may be times when keeping all services in one build is practical. This approach reduces the complexity of cross-service changes during initial development phases but should be used as a temporary measure.
:p In what situation might keeping all services in one build be beneficial?
??x
Keeping all services in one build can reduce the overhead of managing multiple builds, especially when service boundaries are still fluid and frequently changing. This approach is suitable for transitional periods until microservices stabilize.
x??",673,"And the Inevitable Exceptions As with all good rules, there are exceptions we need to consider too. The “one microservice per build” approach is absolutely something you should aim for, but are there ...",qwen2.5:latest,2025-11-02 10:05:03,7
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Custom Images,Automated Deployment Tools,"#### Automated Deployment Tools
Tools like Puppet, Chef, and Ansible can help manage and configure additional software needed to deploy microservices that use different artifact formats. These tools can also standardize the deployment process across multiple technologies.
:p How do automated configuration management tools aid in managing deployments?
??x
Automated tools like Puppet or Chef allow for the creation of scripts that define how services should be configured and deployed, abstracting away differences between artifacts. This helps ensure consistency and reduces manual errors.
x??",595,"And the Inevitable Exceptions As with all good rules, there are exceptions we need to consider too. The “one microservice per build” approach is absolutely something you should aim for, but are there ...",qwen2.5:latest,2025-11-02 10:05:03,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Custom Images,Different Artifact Formats,"#### Different Artifact Formats
Depending on the technology stack, artifacts can vary significantly (e.g., JAR files in Java, gems in Ruby). When multiple technologies are involved, managing these different formats can become complex and error-prone.
:p How do different artifact formats pose challenges during deployment?
??x
Different artifact formats require different deployment mechanisms. For example, a JAR file might run directly, while a Python application needs to be managed by a process manager. This complexity can make deployments more difficult and error-prone if not managed properly.
x??

---",609,"And the Inevitable Exceptions As with all good rules, there are exceptions we need to consider too. The “one microservice per build” approach is absolutely something you should aim for, but are there ...",qwen2.5:latest,2025-11-02 10:05:03,7
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Custom Images,OS-Specific Artifacts,"#### OS-Specific Artifacts
Background context explaining how creating and using OS-specific artifacts (like RPMs, deb packages, or MSI) can simplify deployment and management. These tools help with installation, uninstallation, dependency resolution, and package repositories.

:p What are the advantages of using OS-specific artifacts for deployment?
??x
Using OS-specific artifacts offers several benefits:
1. **Simplified Deployment**: Native tools handle the installation process, reducing complexity.
2. **Dependency Management**: The OS tools can automatically install dependencies, ensuring that all necessary components are available.
3. **Uninstallation and Maintenance**: Package managers provide easy ways to manage installations, including uninstallation and updates.

For example, in Linux, you can define package dependencies using a `spec` file (for RPMs) or a `control` file (for DEB packages), which the OS tools will then handle during installation:
```shell
# Example of defining a dependency in a spec file for an RPM-based system
Requires: libxyz

%install
cp -r ./src /usr/local/lib/xyz-1.0.0
```
x??",1122,"For example, for a RedHat– or CentOS-based system, I might build RPMs; for Ubuntu, I might build a deb package; or for Windows, an MSI. The advantage of using OS-specific artifacts is that from a depl...",qwen2.5:latest,2025-11-02 10:05:31,4
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Custom Images,FPM Package Manager Tool,"#### FPM Package Manager Tool
Background context on the `FPM` package manager tool, which provides an abstraction for creating Linux OS packages and converting from tarball-based deployments to OS-based ones.

:p How does the `FPM` package manager tool simplify the creation of Linux packages?
??x
The `FPM` (Fast Package Manager) simplifies the creation of Linux packages by providing a higher-level interface that abstracts away many of the complexities involved in creating traditional RPM or DEB files. It allows developers to create packages from various sources, such as tarballs, directories, or even scripts.

Example usage:
```shell
fpm -s dir -t rpm -n myapp -v 1.0.0 /path/to/app
```
This command creates an RPM package named `myapp` with version `1.0.0` from the directory `/path/to/app`.

x??",805,"For example, for a RedHat– or CentOS-based system, I might build RPMs; for Ubuntu, I might build a deb package; or for Windows, an MSI. The advantage of using OS-specific artifacts is that from a depl...",qwen2.5:latest,2025-11-02 10:05:31,4
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Custom Images,Chocolatey Package Manager for Windows,"#### Chocolatey Package Manager for Windows
Background context on how tools like `Chocolatey`, a package manager for Windows, provide functionality similar to Linux package managers but can be less common due to historical preferences for manual installations.

:p How does Chocolatey improve the deployment of tools and services in a Windows environment?
??x
Chocolatey improves the deployment of tools and services in a Windows environment by providing functionalities akin to those found in Linux package managers. It simplifies the process of installing, updating, and managing software packages on Windows systems.

For example, you can install a tool like `Git` using Chocolatey with:
```shell
choco install git -y
```
This command will automatically handle dependencies and provide an easy way to manage Git on your system.

x??",835,"For example, for a RedHat– or CentOS-based system, I might build RPMs; for Ubuntu, I might build a deb package; or for Windows, an MSI. The advantage of using OS-specific artifacts is that from a depl...",qwen2.5:latest,2025-11-02 10:05:31,2
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Custom Images,Multi-OS Deployment Challenges,"#### Multi-OS Deployment Challenges
Background context discussing the challenges of deploying software across multiple operating systems, especially in terms of artifact management and complexity.

:p What are the downsides of deploying software onto multiple different operating systems?
??x
Deploying software onto multiple different operating systems can present several downsides:
1. **Artifact Management**: Managing different packages for each OS can be cumbersome.
2. **Complexity**: Increased variability in behavior across different OSes can lead to more complex deployment scripts and troubleshooting.
3. **Maintenance Overhead**: More frequent updates and maintenance are required, which can be resource-intensive.

For example, if you need to deploy a Java application on both Ubuntu and Windows machines, you would have to manage two separate package types (DEB for Linux and MSI for Windows), leading to increased overhead:
```shell
# Example of deploying a Java app on different OSes
# For Linux
sudo apt-get install java-package
fpm -s dir -t deb -n myapp -v 1.0.0 /path/to/app

# For Windows
choco install jre -y
```
x??",1137,"For example, for a RedHat– or CentOS-based system, I might build RPMs; for Ubuntu, I might build a deb package; or for Windows, an MSI. The advantage of using OS-specific artifacts is that from a depl...",qwen2.5:latest,2025-11-02 10:05:31,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Custom Images,Custom Images for Automation,"#### Custom Images for Automation
Background context on the challenges of using automated configuration management tools like Puppet, Chef, and Ansible, particularly in terms of provisioning servers.

:p What is a challenge with using automated configuration management systems like Puppet or Ansible during server provisioning?
??x
A significant challenge when using automated configuration management systems like Puppet, Chef, or Ansible during server provisioning is the time taken to run the scripts on a machine. For example, if you are provisioning an Ubuntu server and configuring it for Java application deployment:

```shell
# Example of provisioning with Puppet
sudo puppet apply /path/to/manifest.pp
```
This command can take considerable time depending on the complexity of the manifest file.

x??

---",815,"For example, for a RedHat– or CentOS-based system, I might build RPMs; for Ubuntu, I might build a deb package; or for Windows, an MSI. The advantage of using OS-specific artifacts is that from a depl...",qwen2.5:latest,2025-11-02 10:05:31,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Custom Images,Provisioning Time for Java Application,"#### Provisioning Time for Java Application

Background context: When setting up a machine to run a Java application, initial setup can take around five minutes, with most of that time spent provisioning and installing the Oracle JVM. This process may need to repeat as other software like collectd, Logstash, and nagios are installed.

:p How long does it usually take to set up an environment for running a Java application?
??x
It typically takes about 5 minutes, including machine provisioning and installing the Oracle JVM. Additional time is required for installing other necessary tools such as collectd, Logstash, and components of Nagios.
x??",651,"The first thing I need to do is install the Oracle JVM to run my Java application. I’ve seen this simple process take around five minutes, with a couple of minutes taken up by the machine being provis...",qwen2.5:latest,2025-11-02 10:05:52,4
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Custom Images,Configuration Management Tools,"#### Configuration Management Tools

Background context: Configuration management tools like Puppet, Chef, Ansible can manage software installations efficiently by avoiding reinstallation if the software already exists on the machine. However, these processes can still be time-consuming due to checks and installations.

:p What are some benefits of using configuration management tools?
??x
Some benefits include smart handling of software installation where the tool will not reinstall software that is already present, ensuring consistency across machines, and reducing manual intervention. However, running all checks and potentially installing new software can still be time-consuming.
x??",695,"The first thing I need to do is install the Oracle JVM to run my Java application. I’ve seen this simple process take around five minutes, with a couple of minutes taken up by the machine being provis...",qwen2.5:latest,2025-11-02 10:05:52,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Custom Images,Time-Varying Dependencies,"#### Time-Varying Dependencies

Background context: Over time, as more tools are added to a machine's dependencies, the time needed for provisioning increases. This can become a bottleneck when deploying changes frequently.

:p Why is managing long-term dependency installation problematic?
??x
Managing long-term dependency installations becomes problematic because the process of installing new software every time can take significant time, which can delay deployments and provide slow feedback during development or CI cycles.
x??",534,"The first thing I need to do is install the Oracle JVM to run my Java application. I’ve seen this simple process take around five minutes, with a couple of minutes taken up by the machine being provis...",qwen2.5:latest,2025-11-02 10:05:52,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Custom Images,Custom Virtual Machine Images,"#### Custom Virtual Machine Images

Background context: Creating a custom virtual machine (VM) image that includes common dependencies can significantly reduce setup time. This is especially useful for frequent deployments.

:p What are the key benefits of using custom VM images?
??x
The key benefits include faster deployment as new instances do not require reinstallation of dependencies, reduced downtime during updates, and consistent environments across multiple deployments.
x??",485,"The first thing I need to do is install the Oracle JVM to run my Java application. I’ve seen this simple process take around five minutes, with a couple of minutes taken up by the machine being provis...",qwen2.5:latest,2025-11-02 10:05:52,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Custom Images,Toolchain Variability,"#### Toolchain Variability

Background context: Historically, building VM images varied by platform (VMWare, AWS, Rackspace), making it difficult to maintain a unified approach.

:p What challenges did different toolchains pose for image creation?
??x
Different toolchains posed challenges because the process of creating VM images differed between platforms like VMWare and AWS. This variability required separate procedures for each environment, complicating cross-platform consistency.
x??",492,"The first thing I need to do is install the Oracle JVM to run my Java application. I’ve seen this simple process take around five minutes, with a couple of minutes taken up by the machine being provis...",qwen2.5:latest,2025-11-02 10:05:52,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Custom Images,Packer Tool,"#### Packer Tool

Background context: Packer is a tool that simplifies the process of building VM images across multiple platforms from a single configuration script.

:p How does Packer simplify image creation?
??x
Packer simplifies image creation by allowing the use of configuration scripts (supporting tools like Chef, Ansible, and Puppet) to build images for different platforms from a unified configuration. This means you can create production and development images with identical settings but for different environments.
x??

---",538,"The first thing I need to do is install the Oracle JVM to run my Java application. I’ve seen this simple process take around five minutes, with a couple of minutes taken up by the machine being provis...",qwen2.5:latest,2025-11-02 10:05:52,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Environments,Virtual Machine Images as Artifacts,"#### Virtual Machine Images as Artifacts
Background context explaining how virtual machine images can be used to bake dependencies and services, leading to faster spin-up times. Netflix's adoption of AWS AMIs is mentioned as a reference.

:p How do virtual machine images contribute to speeding up service deployment?
??x
Virtual machine images allow for the baking in of dependencies and entire services directly into the image itself. This means that when a new instance is launched, the service is already configured and ready to run, significantly reducing spin-up time. This approach is particularly beneficial in DevOps practices where rapid deployment and iteration are crucial.

For example, instead of just installing an operating system and then manually setting up dependencies and services, you can create a VM image that includes all these elements pre-configured. Here’s a simplified pseudo-code to illustrate this:

```pseudo
// Pseudo-code for creating a virtual machine image with a service baked in
function buildVMImage(serviceName) {
    vm = new VirtualMachine();
    // Install OS and dependencies
    installOS(vm);
    installDependencies(vm);

    // Bake the service into the VM
    bakeServiceIntoVM(vm, serviceName);

    return vm;
}

// Function to simulate installing an operating system on a VM
function installOS(vm) {
    vm.setOperatingSystem(""Ubuntu 20.04"");
}

// Function to simulate installing dependencies on a VM
function installDependencies(vm) {
    vm.installPackage(""git"");
    vm.installPackage(""python3"");
}

// Function to bake the service into the VM
function bakeServiceIntoVM(vm, serviceName) {
    vm.getServiceContainer().installService(serviceName);
}
```

x??",1714,"Images as Artifacts So we can create virtual machine images that bake in dependencies to speed up feedback, but why stop there? We could go further, bake our service into the image itself, and adopt t...",qwen2.5:latest,2025-11-02 10:06:19,7
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Environments,Immutable Servers,"#### Immutable Servers
Background context explaining how storing configuration in source control ensures reproducibility. The problem of ""configuration drift"" is highlighted as a common issue that needs to be addressed.

:p What does the immutable server pattern aim to achieve?
??x
The immutable server pattern aims to ensure that no changes are made directly on running servers; all changes must go through a build pipeline, creating new machines with updated configurations. This approach helps avoid ""configuration drift,"" where the configuration of a running host diverges from what is stored in source control.

To implement this, you might disable SSH access during image creation to prevent manual changes:

```pseudo
// Pseudo-code for disabling SSH on a VM image
function createImmutableVMImage() {
    vm = new VirtualMachine();
    // Disable SSH access
    disableSSHAccess(vm);

    return vm;
}

// Function to simulate disabling SSH access
function disableSSHAccess(vm) {
    vm.disableRemoteAccess();
}
```

x??",1028,"Images as Artifacts So we can create virtual machine images that bake in dependencies to speed up feedback, but why stop there? We could go further, bake our service into the image itself, and adopt t...",qwen2.5:latest,2025-11-02 10:06:19,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Environments,Different Environments for Deployments,"#### Different Environments for Deployments
Background context explaining the various stages and environments a microservice might go through in a CD pipeline, such as test, UAT, performance, and production. The differences between these environments are highlighted.

:p How do different environments impact software deployments?
??x
Different environments serve distinct purposes and may require different configurations or setups. For instance, a production environment might have multiple load-balanced hosts across data centers, while a test environment could run everything on a single machine. These differences can introduce challenges such as configuration drift if not properly managed.

To ensure consistency, you need to automate the creation of environments that mimic production settings where possible, but also allow for faster feedback in less complex setups:

```pseudo
// Pseudo-code for deploying software across different environments
function deploySoftware(environmentType) {
    switch (environmentType) {
        case ""test"":
            deployToSingleMachine();
            break;
        case ""uat"":
            deployToMultipleMachines();
            break;
        case ""performance"":
            deployWithLoadBalancing();
            break;
        case ""production"":
            deployWithHighAvailability();
            break;
    }
}

// Example function to simulate deployment on a single machine
function deployToSingleMachine() {
    vm = new VirtualMachine();
    installSoftware(vm);
    startServiceOnVM(vm);
}
```

x??

---",1564,"Images as Artifacts So we can create virtual machine images that bake in dependencies to speed up feedback, but why stop there? We could go further, bake our service into the image itself, and adopt t...",qwen2.5:latest,2025-11-02 10:06:19,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Multiple Services Per Host,Service Configuration Management,"#### Service Configuration Management
Background context explaining how service configuration should be handled to avoid problems. Discusses the issues with building one artifact per environment and suggests managing configuration separately.

:p How should we handle different configurations for various environments as part of our deployment process?
??x
We should create a single artifact that includes core functionality, while keeping sensitive or environment-specific configurations managed separately. This can be done through properties files specific to each environment or by passing in parameters during the installation process. Using a dedicated configuration system is also recommended for managing complex and varying environments.

For example:
```properties
# app.properties (example)
database.username=myuser
database.password=mypassword
```

Or using command-line arguments:
```sh
./install.sh --env=production --db-username=admin --db-password=secretpassword
```
x??",986,"Service Configuration Our services need some configuration. Ideally, this should be a small amount, and limited to those features that change from one environment to another, such as  what username an...",qwen2.5:latest,2025-11-02 10:06:41,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Multiple Services Per Host,Service-to-Host Mapping,"#### Service-to-Host Mapping
Background context on the concept of hosts in a microservices environment, explaining how multiple services can be run on a single host for easier management and cost savings. Discusses challenges such as monitoring, side effects, and deployment complexity.

:p How many services per host should we have?
??x
Multiple services per host (up to 10-30) is often preferred due to simpler host management and reduced costs. However, it comes with challenges like increased difficulty in monitoring, potential resource contention between services, and more complex deployments.

For instance:
```java
// Example of a simple service deployment script that deploys two services on the same host.
public class ServiceDeployment {
    public void deployServicesOnHost(String hostName) {
        Service service1 = new Service(""service1"");
        Service service2 = new Service(""service2"");
        
        // Deploy both services to the same host
        service1.deployTo(hostName);
        service2.deployTo(hostName);
    }
}
```
x??",1057,"Service Configuration Our services need some configuration. Ideally, this should be a small amount, and limited to those features that change from one environment to another, such as  what username an...",qwen2.5:latest,2025-11-02 10:06:41,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Multiple Services Per Host,Multiple Services Per Host Model,"#### Multiple Services Per Host Model
Background context on how deploying multiple microservices on a single host can simplify deployment and reduce costs, but also poses challenges like increased monitoring complexity and resource contention. Discusses the trade-offs between this model and alternatives.

:p Why is having multiple services per host attractive?
??x
Having multiple services per host is attractive because it simplifies host management for different teams and reduces overall costs. It aligns with practices in application containerization, where many services share a single container, making deployment simpler for developers.

For example:
```java
// Example of deploying two microservices to the same host.
public class MicroserviceDeployment {
    public void deployMicroservicesOnHost(String hostName) {
        Microservice service1 = new Microservice(""service1"");
        Microservice service2 = new Microservice(""service2"");
        
        // Deploy both services to the same host
        service1.deployTo(hostName);
        service2.deployTo(hostName);
    }
}
```
x??",1098,"Service Configuration Our services need some configuration. Ideally, this should be a small amount, and limited to those features that change from one environment to another, such as  what username an...",qwen2.5:latest,2025-11-02 10:06:41,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Multiple Services Per Host,Challenges with Multiple Services Per Host Model,"#### Challenges with Multiple Services Per Host Model
Background context on the complexities introduced by deploying multiple services on a single host, such as increased monitoring complexity, resource contention, and deployment challenges. Discusses the importance of maintaining independent release strategies.

:p What are some challenges associated with having multiple microservices per host?
??x
Some challenges include difficulty in monitoring individual services' performance independently, potential side effects from load imbalances (one service under heavy load affecting others), increased complexity in analyzing host failures and their impact, and complexities in deploying updates without affecting other services.

For example:
```java
// Example of a scenario where deployment of one microservice affects another.
public class ServiceDeployment {
    public void deployServicesWithImpact(String hostName) {
        Microservice service1 = new Microservice(""service1"");
        Microservice service2 = new Microservice(""service2"");
        
        // Deploying service1 first might impact the resources available to service2
        service1.deployTo(hostName);
        service2.deployTo(hostName);
    }
}
```
x??",1232,"Service Configuration Our services need some configuration. Ideally, this should be a small amount, and limited to those features that change from one environment to another, such as  what username an...",qwen2.5:latest,2025-11-02 10:06:41,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Multiple Services Per Host,Impact Analysis and Deployment Complexity,"#### Impact Analysis and Deployment Complexity
Background context on how having multiple services per host can complicate impact analysis during failures and deployment, as well as limiting autonomy of teams and options for image-based deployments.

:p What are the potential downsides of using a single host for multiple microservices?
??x
Potential downsides include difficulty in analyzing the impact of host failures (since one host failure affects all services), increased complexity in deploying updates to individual services without affecting others, reduced autonomy for different teams managing their own hosts, and limitations on deployment artifact options like image-based deployments or immutable servers.

For example:
```java
// Example of a deployment scenario where multiple services are tied together.
public class ServiceDeployment {
    public void deployServicesTogether(String hostName) {
        Microservice service1 = new Microservice(""service1"");
        Microservice service2 = new Microservice(""service2"");
        
        // Deploying both services in one step
        service1.deployTo(hostName);
        service2.deployTo(hostName);
    }
}
```
x??",1181,"Service Configuration Our services need some configuration. Ideally, this should be a small amount, and limited to those features that change from one environment to another, such as  what username an...",qwen2.5:latest,2025-11-02 10:06:41,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Application Containers,Application Containers Overview,"#### Application Containers Overview
Background context explaining the traditional setup where multiple distinct services or applications sit inside a single application container, which runs on a single host. This model leverages benefits such as improved manageability and reduced overhead of language runtimes.

:p What are the key features of application containers mentioned in this text?
??x
The key features include clustering support for grouping instances together, monitoring tools, and reduced overhead of language runtimes due to sharing one JVM instance across multiple services. However, these benefits come with constraints on technology choices and potential limitations on automation and management options.

```java
public class ExampleContainer {
    public void deployServices() {
        // Code to deploy Java services inside a single JVM
        System.out.println(""Deploying 5 Java services in one JVM"");
    }
}
```
x??",944,"Application Containers If you’re familiar with deploying .NET applications behind IIS or Java applications into a servlet container, you will be well acquainted with the model where multiple distinct ...",qwen2.5:latest,2025-11-02 10:07:05,7
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Application Containers,Technological Constraints of Application Containers,"#### Technological Constraints of Application Containers
Background context discussing how application containers can constrain technology choices, leading to the need for a specific technology stack that limits options for implementation and automation.

:p How do application containers impact technology choices?
??x
Application containers can force you to adopt a particular technology stack, which restricts both the choice of implementing technologies for your services and the options available for automated management. This constraint could be detrimental if it limits flexibility in choosing tools or frameworks that better suit specific service needs.

```java
public class TechnologyStackChoice {
    public void chooseTechnology() {
        // Code to demonstrate choosing a technology stack
        String techStack = ""JVM with embedded Jetty"";
        System.out.println(""Choosing: "" + techStack);
    }
}
```
x??",928,"Application Containers If you’re familiar with deploying .NET applications behind IIS or Java applications into a servlet container, you will be well acquainted with the model where multiple distinct ...",qwen2.5:latest,2025-11-02 10:07:05,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Application Containers,Overhead and Resource Management,"#### Overhead and Resource Management
Background context highlighting the overhead associated with managing multiple hosts, and how automation can help address these challenges. Discusses potential downsides of using application containers like increased complexity in lifecycle management and slower spin-up times.

:p What are some of the downsides of using application containers?
??x
Some downsides include constrained technology choices, increased complexity in lifecycle management (as compared to simply restarting a JVM), slower spin-up times affecting developer feedback cycles, and added resource overhead from commercial technologies. These factors can make managing multiple services less efficient.

```java
public class LifecycleManagement {
    public void manageLifecycle() {
        // Code to demonstrate complex lifecycle management
        System.out.println(""Managing application lifecycle is more complex than restarting a JVM."");
    }
}
```
x??",968,"Application Containers If you’re familiar with deploying .NET applications behind IIS or Java applications into a servlet container, you will be well acquainted with the model where multiple distinct ...",qwen2.5:latest,2025-11-02 10:07:05,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Application Containers,Self-Contained Deployable Microservices,"#### Self-Contained Deployable Microservices
Background context on the approach of using self-contained deployable microservices as artifacts, highlighting examples like Nancy for .NET and Jetty embedded container in Java. Discusses their potential to operate at scale.

:p What is recommended instead of application containers?
??x
Instead of application containers, it’s recommended to use self-contained deployable microservices. Examples include using tools like Nancy for .NET or the Jetty embedded container in Java, which can run multiple services independently and are designed to operate at scale without the constraints of a single JVM.

```java
public class SelfContainedMicroservice {
    public void createMicroservice() {
        // Code to demonstrate creating self-contained microservices
        System.out.println(""Creating a self-contained deployable microservice using embedded Jetty."");
    }
}
```
x??",923,"Application Containers If you’re familiar with deploying .NET applications behind IIS or Java applications into a servlet container, you will be well acquainted with the model where multiple distinct ...",qwen2.5:latest,2025-11-02 10:07:05,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Application Containers,Resource Overhead and Management Complexity,"#### Resource Overhead and Management Complexity
Background context on the resource overhead associated with application containers, including commercial costs and added complexity in analyzing resource use and threads due to multiple applications sharing the same process.

:p What are some of the resource-related challenges with application containers?
??x
Resource-related challenges include additional costs for commercial tools, higher resource overhead from using these technologies, increased complexity in analyzing resource use and thread management when multiple applications share a single process. These factors can impact performance and scalability negatively.

```java
public class ResourceManagement {
    public void manageResources() {
        // Code to demonstrate managing resources with shared JVMs
        System.out.println(""Managing resources becomes complex with multiple services sharing the same JVM."");
    }
}
```
x??",948,"Application Containers If you’re familiar with deploying .NET applications behind IIS or Java applications into a servlet container, you will be well acquainted with the model where multiple distinct ...",qwen2.5:latest,2025-11-02 10:07:05,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Application Containers,Conclusion on Application Containers,"#### Conclusion on Application Containers
Background context summarizing that while application containers offer some benefits, their constraints and downsides suggest reevaluating their necessity in modern microservices architectures.

:p What is the author's recommendation regarding application containers?
??x
The author recommends strongly considering self-contained deployable microservices as artifacts over application containers due to reduced technology constraints, improved manageability, and better automation options. Application containers are seen as potentially outdated given current technological advancements and requirements of a microservices architecture.

```java
public class Conclusion {
    public void evaluateContainers() {
        // Code to demonstrate evaluating the necessity of application containers
        System.out.println(""Evaluate if application containers are really required in modern microservices architectures."");
    }
}
```
x??
---",979,"Application Containers If you’re familiar with deploying .NET applications behind IIS or Java applications into a servlet container, you will be well acquainted with the model where multiple distinct ...",qwen2.5:latest,2025-11-02 10:07:05,7
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Single Service Per Host,Single Service Per Host Model,"#### Single Service Per Host Model
This model aims to simplify monitoring, remediation, and scaling by ensuring each host runs only a single microservice. It helps reduce single points of failure and eases security management.

:p What are the benefits of using a single-service-per-host model in microservices architecture?
??x
The benefits include simplified monitoring and remediation, easier scalability of individual services, reduced complexity, and improved security by focusing on specific services and hosts. This approach also allows for more flexible deployment techniques like image-based deployments or immutable server patterns.

For example:
```java
public class ServiceA {
    public void handleRequest() {
        // Handle request logic for service A
    }
}

public class ServiceB {
    public void handleRequest() {
        // Handle request logic for service B
    }
}
```
In this scenario, each microservice runs on its own host, making it easier to manage and scale.

x??",994,"Single Service Per Host With a single-service-per-host model shown in  Figure 6-8 , we avoid side effects of multiple hosts living on a single host, making monitoring and remediation much simpler. We ...",qwen2.5:latest,2025-11-02 10:07:30,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Single Service Per Host,Monitoring and Remediation Simplicity,"#### Monitoring and Remediation Simplicity
By running a single service per host, monitoring the health of services becomes more straightforward. Each host can be monitored independently, simplifying the process of identifying issues and applying fixes.

:p How does running a single service on each host simplify monitoring?
??x
Running a single service on each host makes it easier to monitor because you only need to check one service per host. This isolation allows for simpler and more targeted remediation efforts when an issue arises.

For instance, if `ServiceA` is running on Host1 and `ServiceB` on Host2, issues can be isolated to their respective hosts, making troubleshooting more efficient.

x??",708,"Single Service Per Host With a single-service-per-host model shown in  Figure 6-8 , we avoid side effects of multiple hosts living on a single host, making monitoring and remediation much simpler. We ...",qwen2.5:latest,2025-11-02 10:07:30,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Single Service Per Host,Scale and Failure Design,"#### Scale and Failure Design
Designing for scale and failure becomes more manageable with the single-service-per-host model. This approach helps in identifying which service has failed when an outage occurs.

:p How does a single-service-per-host design aid in scale and failure management?
??x
This design aids in scale and failure management by ensuring that outages affect only specific services, not the entire system. You can independently scale each service as needed without affecting others. For example, if `ServiceA` experiences an outage, it impacts only its host and does not bring down other services.

```java
public class ServiceManager {
    public void handleFailure(ServiceType service) {
        switch (service) {
            case A:
                // Handle failure for ServiceA
                break;
            case B:
                // Handle failure for ServiceB
                break;
        }
    }
}
```
Here, each `Service` is managed independently, making it easier to isolate and address failures.

x??",1038,"Single Service Per Host With a single-service-per-host model shown in  Figure 6-8 , we avoid side effects of multiple hosts living on a single host, making monitoring and remediation much simpler. We ...",qwen2.5:latest,2025-11-02 10:07:30,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Single Service Per Host,Cost Implications of More Servers,"#### Cost Implications of More Servers
While the single-service-per-host model provides benefits in terms of monitoring and management, there are downsides like increased server management overhead and potential cost implications for running more distinct hosts.

:p What are the potential downsides of using a single-service-per-host model?
??x
The potential downsides include:
- Increased complexity in managing a larger number of servers.
- Higher costs associated with operating multiple hosts.

For example, if you previously had 3 hosts and now need to scale to 10, there will be more resources required for management tools, additional hardware, and possibly increased cloud costs.

x??",693,"Single Service Per Host With a single-service-per-host model shown in  Figure 6-8 , we avoid side effects of multiple hosts living on a single host, making monitoring and remediation much simpler. We ...",qwen2.5:latest,2025-11-02 10:07:30,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Single Service Per Host,Alternative Deployment Techniques,"#### Alternative Deployment Techniques
The single-service-per-host model opens up the potential to use alternative deployment techniques such as image-based deployments or immutable server patterns. These methods can simplify updates and rollbacks but require careful planning.

:p How does a single-service-per-host architecture support alternative deployment techniques?
??x
A single-service-per-host architecture supports alternative deployment techniques like:
- **Image-Based Deployments**: You can create and maintain consistent images for each service, making deployments more predictable.
- **Immutable Server Patterns**: Each host runs a fixed image, reducing the risk of state corruption during updates.

For instance:
```java
public class DeploymentManager {
    public void deployNewVersion(ServiceType service) {
        switch (service) {
            case A:
                updateServiceAImage();
                break;
            case B:
                updateServiceBImage();
                break;
        }
    }

    private void updateServiceAImage() {
        // Logic to update ServiceA's image
    }

    private void updateServiceBImage() {
        // Logic to update ServiceB's image
    }
}
```
This ensures that each service can be updated independently without affecting others.

x??

---",1318,"Single Service Per Host With a single-service-per-host model shown in  Figure 6-8 , we avoid side effects of multiple hosts living on a single host, making monitoring and remediation much simpler. We ...",qwen2.5:latest,2025-11-02 10:07:30,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Traditional Virtualization,PaaS Overview,"#### PaaS Overview
PaaS allows developers to work at a higher level of abstraction, focusing on application development rather than infrastructure management. Providers like Heroku handle running applications and even services such as databases with minimal effort from the developer.
:p What is Platform as a Service (PaaS)?
??x
PaaS simplifies software deployment and operation by abstracting away much of the underlying infrastructure, allowing developers to focus on their application logic rather than server management. Providers like Heroku offer hosted solutions that automatically provision and run applications, often scaling them up or down based on demand.
```python
# Example usage in Python for a PaaS-based deployment script
def deploy_app(app):
    print(f""Deploying {app} using the PaaS provider..."")
```
x??",825,"Platform as a Service When using a platform as a service (PaaS), you are working at a higher-level abstraction than at a single host. Most of these platforms rely on taking a technology-specific artif...",qwen2.5:latest,2025-11-02 10:07:52,4
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Traditional Virtualization,Hosted vs. Self-Hosted PaaS Solutions,"#### Hosted vs. Self-Hosted PaaS Solutions
Hosted PaaS solutions are fully managed by third-party providers, offering robust features and support but often limiting control. In contrast, self-hosted PaaS requires more maintenance effort from the user.
:p What distinguishes hosted from self-hosted PaaS solutions?
??x
Hosted PaaS solutions provide a managed service where the provider handles all aspects of infrastructure management, scalability, and updates. Self-hosted PaaS solutions require users to manage their own infrastructure but offer greater flexibility and control over configuration and deployment processes.
```bash
# Example command for deploying an application on Heroku (hosted)
heroku create myapp
git push heroku master
```
x??",748,"Platform as a Service When using a platform as a service (PaaS), you are working at a higher-level abstraction than at a single host. Most of these platforms rely on taking a technology-specific artif...",qwen2.5:latest,2025-11-02 10:07:52,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Traditional Virtualization,Challenges with Smart PaaS Solutions,"#### Challenges with Smart PaaS Solutions
Smart PaaS solutions that attempt to handle complex scaling can sometimes fail due to their generic approach, which may not fit the specific needs of a non-standard application. This often leads to suboptimal performance or even downtime.
:p What are some challenges associated with smart PaaS solutions?
??x
The challenge lies in the fact that smart PaaS solutions often use heuristics and predefined scaling rules that may not align well with the unique requirements of certain applications. These generic strategies can lead to poor performance, over-provisioning or under-provisioning resources, and potential downtime.
```java
// Example of a flawed autoscaling logic in Java
public class Autoscaler {
    public void scaleUp(int currentLoad) {
        if (currentLoad > 50) {
            // Scaling up is overly aggressive based on the heuristic
            System.out.println(""Scaling up to handle load."");
        }
    }
}
```
x??",981,"Platform as a Service When using a platform as a service (PaaS), you are working at a higher-level abstraction than at a single host. Most of these platforms rely on taking a technology-specific artif...",qwen2.5:latest,2025-11-02 10:07:52,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Traditional Virtualization,Importance of Automation in DevOps,"#### Importance of Automation in DevOps
Automation can significantly reduce the overhead associated with managing multiple servers and services, making it easier to scale without increasing workload proportionally. Tools like Ansible or Jenkins facilitate this automation.
:p Why is automation important in managing a large number of servers and services?
??x
Automation is crucial because it allows for consistent and repeatable processes across many machines, reducing human error and freeing up developers' time to focus on development rather than manual tasks. Automation tools can handle tasks such as deployment, configuration management, and monitoring.
```yaml
# Example Ansible playbook for deploying an application
- name: Deploy web application
  hosts: web_servers
  tasks:
    - name: Ensure the application is deployed
      copy:
        src: /path/to/app.jar
        dest: /opt/myapp/
```
x??

---",913,"Platform as a Service When using a platform as a service (PaaS), you are working at a higher-level abstraction than at a single host. Most of these platforms rely on taking a technology-specific artif...",qwen2.5:latest,2025-11-02 10:07:52,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Traditional Virtualization,Automation in Microservice Architectures,"#### Automation in Microservice Architectures
Background context explaining how automation is crucial for managing microservices, including provisioning services, deployment processes, and monitoring. The chapter emphasizes that automation helps maintain productivity among developers by enabling self-service for provisioning and deploying services.

:p What are some key aspects of automation mentioned in the text?
??x
Automation involves several critical areas:
1. **Provisioning Services**: Developers should be able to provision individual or groups of services easily.
2. **Deployment Processes**: Software deployment must be automated, ensuring that changes can be deployed without manual intervention.
3. **Database Changes**: Deployment of database changes should also be automated to reduce human error and increase efficiency.

x??",843,"That means multiple deployments to handle, services to monitor, logs to collect. Automation is essential. Automation is also how we can make sure that our developers still remain productive. Giving th...",qwen2.5:latest,2025-11-02 10:08:12,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Traditional Virtualization,Case Study: RealEstate.com.au,"#### Case Study: RealEstate.com.au
Background context explaining the journey of REA towards a distributed microservices architecture. The case study highlights how automation was essential in transitioning from monolithic architectures to microservices, reducing development time significantly over 18 months.

:p What challenges did REA face during its transition to a microservices-based architecture?
??x
REA faced significant initial challenges due to the complexity of setting up the necessary tooling. The company had to spend considerable time on getting services provisioned, deploying code, and monitoring them initially. This front-loading of work impacted the development pace early on.

For example:
- In the first three months: Just two new microservices were moved into production.
- Next three months saw 10-15 microservices being deployed.
- By the end of 18 months: Over 60-70 microservices were live.

x??",923,"That means multiple deployments to handle, services to monitor, logs to collect. Automation is essential. Automation is also how we can make sure that our developers still remain productive. Giving th...",qwen2.5:latest,2025-11-02 10:08:12,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Traditional Virtualization,Case Study: Gilt,"#### Case Study: Gilt
Background context explaining Gilt's move from a monolithic Rails application to multiple microservices. The case study illustrates how automation, particularly in tooling for developers, drove the rapid adoption and growth of microservices within the company.

:p How did Gilt's approach to microservices differ initially compared to later stages?
??x
Initially, Gilt had a monolithic Rails application which was difficult to scale. In 2009, they started decomposing this into multiple microservices due to automation, especially in tooling for developers. Over time, the use of automated processes significantly increased their microservice count:
- After one year: Around 10 microservices.
- By 2012: Over 100 microservices.
- In 2014: More than 450 microservices.

This rapid growth was attributed to automation and developer-friendly tooling, making it easier for developers to handle the build, deployment, and support of services.

x??",964,"That means multiple deployments to handle, services to monitor, logs to collect. Automation is essential. Automation is also how we can make sure that our developers still remain productive. Giving th...",qwen2.5:latest,2025-11-02 10:08:12,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Traditional Virtualization,Moving from Physical to Virtual Machines,"#### Moving from Physical to Virtual Machines
Background context explaining how traditional virtualization tools like VMware or AWS reduce host management overhead. The text emphasizes that these tools are key in managing a large number of hosts by breaking them into smaller parts.

:p What is the benefit of using virtual machines according to the text?
??x
The main benefit of using virtual machines, as per the text, is reducing the overhead associated with managing physical hosts. Traditional virtualization solutions like VMware or those provided by AWS are highlighted for their efficiency in this regard.

For example:
- Virtualization helps in efficiently utilizing resources.
- It allows chunking up existing physical machines into smaller parts, improving overall management and scalability.

x??

---",813,"That means multiple deployments to handle, services to monitor, logs to collect. Automation is essential. Automation is also how we can make sure that our developers still remain productive. Giving th...",qwen2.5:latest,2025-11-02 10:08:12,4
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Traditional Virtualization,Physical Server Cost,"#### Physical Server Cost
Background context explaining why having lots of hosts is expensive. Discuss the need for a physical server per host and challenges with scaling.

:p Why are many organizations looking to reduce the number of physical servers?
??x
The cost associated with maintaining multiple physical servers can be significant, especially considering hardware costs, energy consumption, and maintenance overhead. Each physical server requires its own power supply, cooling systems, and space. Reducing the number of hosts can lead to more efficient use of resources and lower operational costs.

In Java, you might see this in terms of managing a cluster of microservices hosted on different servers:
```java
public class ServerManager {
    public void manageServers(List<String> serverList) {
        // Code for managing multiple physical servers
    }
}
```
x??",877,"However, there have been some new advances in this space that are well worth exploring, as they can open up even more interesting possibilities for dealing with our microservice architecture. Traditio...",qwen2.5:latest,2025-11-02 10:08:35,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Traditional Virtualization,Virtualization Overview,"#### Virtualization Overview
Explain the concept of virtualization and its benefits, focusing on how it allows slicing up a single physical server into separate hosts. Mention the trade-offs between traditional VMs and lightweight containers.

:p What is virtualization and what does it allow us to do?
??x
Virtualization involves creating multiple virtual environments (virtual machines or VMs) from a single physical host. This approach allows for better resource utilization, easier management, and flexibility in deploying applications. However, it introduces additional overhead due to the hypervisor layer that manages resources.

In terms of code, you can simulate this with a simple class representing a virtual machine:
```java
public class VirtualMachine {
    private String name;
    private OperatingSystem os;

    public VirtualMachine(String name) {
        this.name = name;
        this.os = new OperatingSystem();
    }

    // Methods for managing VM resources and operations
}
```
x??",1005,"However, there have been some new advances in this space that are well worth exploring, as they can open up even more interesting possibilities for dealing with our microservice architecture. Traditio...",qwen2.5:latest,2025-11-02 10:08:35,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Traditional Virtualization,Type 2 vs. Type 1 Virtualization,"#### Type 2 vs. Type 1 Virtualization
Explain the difference between type 2 (hosted) virtualization and type 1 (bare metal) virtualization, highlighting their respective advantages and disadvantages.

:p What is the key difference between type 2 and type 1 virtualization?
??x
Type 2 virtualization runs a VM on top of an existing host operating system. This means that instead of directly accessing hardware resources, VMs run in guest mode within a host OS like Linux or Windows. Examples include VMware Workstation and VirtualBox.

Type 1 virtualization (bare metal) refers to hypervisors that run directly on the host's hardware without an additional host OS. This approach allows for more direct access to underlying resources but requires specialized hardware support. Examples include KVM, Xen, and VMWare ESXi.

In code, you might represent these concepts with classes:
```java
public class VirtualizationType {
    enum Type { TYPE_2, TYPE_1 }

    public static void main(String[] args) {
        if (VirtualizationType.Type.TYPE_1 == VirtualizationType.Type.TYPE_1) {
            System.out.println(""Running in bare metal mode"");
        } else {
            System.out.println(""Running on a host OS"");
        }
    }
}
```
x??",1239,"However, there have been some new advances in this space that are well worth exploring, as they can open up even more interesting possibilities for dealing with our microservice architecture. Traditio...",qwen2.5:latest,2025-11-02 10:08:35,2
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Traditional Virtualization,Hypervisor Overhead,"#### Hypervisor Overhead
Discuss the overhead introduced by the hypervisor, including its role in managing resources and isolating virtual machines.

:p What is the impact of hypervisor overhead in virtualization?
??x
The hypervisor acts as an intermediary layer between physical hardware and virtualized environments. It allocates and manages CPU, memory, storage, and network resources among multiple VMs. However, this adds a significant overhead that can reduce overall efficiency. The more VMs the hypervisor needs to manage, the more resources it consumes.

For example:
```java
public class Hypervisor {
    private int cpuCores;
    private Memory memory;

    public void allocateResources(int vmCount) {
        // Allocate CPU and memory considering the overhead
    }
}
```
x??",789,"However, there have been some new advances in this space that are well worth exploring, as they can open up even more interesting possibilities for dealing with our microservice architecture. Traditio...",qwen2.5:latest,2025-11-02 10:08:35,7
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Traditional Virtualization,Diminishing Returns in Slicing VMs,"#### Diminishing Returns in Slicing VMs
Explain why there are diminishing returns when slicing a physical server into smaller VMs, highlighting resource allocation issues.

:p Why do we face diminishing returns when trying to slice up our physical infrastructure further?
??x
As you create more VMs on a single physical server, the overhead of the hypervisor increases. This means that each VM gets fewer resources because a portion of the CPU, memory, and other resources are consumed by the hypervisor itself. Eventually, adding more VMs leads to less available resources for applications running inside them.

This can be illustrated with an example:
```java
public class ResourceAllocation {
    private int totalCores;
    private List<VirtualMachine> vmList;

    public void allocateResources() {
        for (VirtualMachine vm : vmList) {
            // Calculate effective resource allocation after considering hypervisor overhead
        }
    }
}
```
x??",965,"However, there have been some new advances in this space that are well worth exploring, as they can open up even more interesting possibilities for dealing with our microservice architecture. Traditio...",qwen2.5:latest,2025-11-02 10:08:35,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Traditional Virtualization,Lightweight Containers,"#### Lightweight Containers
Introduce the concept of lightweight containers as an alternative to traditional VMs, discussing their advantages and disadvantages compared to virtualization.

:p What are lightweight containers and how do they compare to traditional VMs?
??x
Lightweight containers, such as Docker or LXC, run directly on the host's operating system without a full-fledged hypervisor. They isolate applications by using namespaces and control groups (cgroups) but share the kernel with other processes running on the same host. This approach minimizes overhead and allows for more efficient use of resources.

For example:
```java
public class ContainerManager {
    private List<Container> containerList;

    public void startContainers() {
        // Start containers without hypervisor overhead
    }
}
```
x??

---",832,"However, there have been some new advances in this space that are well worth exploring, as they can open up even more interesting possibilities for dealing with our microservice architecture. Traditio...",qwen2.5:latest,2025-11-02 10:08:35,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Linux Containers,Vagrant Overview,"#### Vagrant Overview
Vagrant is a powerful deployment platform that simplifies the creation and management of virtual development environments. It uses a standard virtualization system, typically VirtualBox but also compatible with other platforms like VMware. By defining VMs in a configuration file (usually `Vagrantfile`), it allows developers to set up production-like environments locally on their machines.

:p What is Vagrant used for?
??x
Vagrant is primarily used for development and testing purposes rather than production, as it provides virtualized environments that can be easily managed and shared among team members. This setup helps in creating consistent and reproducible environments across different machines.
x??",733,"Vagrant Vagrant is a very useful deployment platform, which is normally used for dev and test rather than production. Vagrant provides you with a virtual cloud on your laptop. Underneath, it uses a st...",qwen2.5:latest,2025-11-02 10:09:03,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Linux Containers,Benefits of Using Vagrant,"#### Benefits of Using Vagrant
Running multiple VMs through Vagrant can be taxing on a typical development machine due to the overhead involved in managing each one. However, for teams using on-demand cloud platforms like AWS, Vagrant offers faster turnaround times, which is beneficial for development cycles.

:p What are some benefits of using Vagrant?
??x
Some key benefits include:
- Ability to spin up multiple VMs quickly and easily.
- Simulating production-like environments locally.
- Facilitating the testing of failure modes by shutting down individual VMs.
- Mapping VM directories to local directories, allowing immediate reflection of changes.

Code Example: A simple `Vagrantfile` snippet
```ruby
Vagrant.configure(""2"") do |config|
  config.vm.define ""web"" do |web|
    web.vm.box = ""ubuntu/trusty64""
    web.vm.network ""private_network"", ip: ""192.168.33.10""
  end

  config.vm.define ""db"" do |db|
    db.vm.box = ""ubuntu/trusty64""
    db.vm.network ""private_network"", ip: ""192.168.33.11""
  end
end
```
x??",1021,"Vagrant Vagrant is a very useful deployment platform, which is normally used for dev and test rather than production. Vagrant provides you with a virtual cloud on your laptop. Underneath, it uses a st...",qwen2.5:latest,2025-11-02 10:09:03,5
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Linux Containers,Linux Containers Overview,"#### Linux Containers Overview
Linux containers provide an alternative to full virtualization, allowing for process isolation and resource management at a finer granularity compared to VMs. They share the same kernel but can run different operating systems.

:p What is a key difference between Vagrant and Linux Containers?
??x
A key difference is that Vagrant uses full virtualization (e.g., VirtualBox) where each VM has its own complete OS, whereas Linux containers use the host's kernel to isolate processes without emulating a full OS. This makes containers much lighter in terms of resource usage.

Code Example: Simple LXC Container Creation
```bash
lxc-start -n my_container -d
```
This command starts an LXC container named `my_container` in the background.
x??",771,"Vagrant Vagrant is a very useful deployment platform, which is normally used for dev and test rather than production. Vagrant provides you with a virtual cloud on your laptop. Underneath, it uses a st...",qwen2.5:latest,2025-11-02 10:09:03,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Linux Containers,Benefits and Drawbacks of Linux Containers,"#### Benefits and Drawbacks of Linux Containers
Linux containers offer faster provisioning times (seconds vs. minutes for VMs) and finer-grained resource control, making them more cost-effective and efficient than running services in separate VMs.

:p What are some benefits of using Linux Containers?
??x
Benefits include:
- Faster startup: Containers can start within seconds compared to many minutes for full virtual machines.
- Improved hardware utilization: More containers can run on the same hardware due to their lightweight nature.
- Enhanced resource control: Better ability to allocate and manage resources per container.

Code Example: Starting a Container with Resources Allocated
```bash
lxc-start -n my_container -d --setcpucfs 2048 --setmemcfs 1G
```
This command starts the `my_container` allocating 2GB of memory and 2048 CPU shares.
x??",855,"Vagrant Vagrant is a very useful deployment platform, which is normally used for dev and test rather than production. Vagrant provides you with a virtual cloud on your laptop. Underneath, it uses a st...",qwen2.5:latest,2025-11-02 10:09:03,7
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Linux Containers,Security Considerations with Linux Containers,"#### Security Considerations with Linux Containers
Despite their advantages, Linux containers have limitations regarding isolation. Processes within a container can potentially interact with other containers or the host due to shared kernel resources.

:p What are some security concerns with using Linux Containers?
??x
Security concerns include:
- Potential for processes from one container to interact with others or the host.
- Need for careful management of permissions and resource allocation.
- Risk of bugs or design limitations allowing processes to escape isolation.

Mitigation strategies might involve thorough testing, using established best practices, and monitoring for unusual activity.
x??",706,"Vagrant Vagrant is a very useful deployment platform, which is normally used for dev and test rather than production. Vagrant provides you with a virtual cloud on your laptop. Underneath, it uses a st...",qwen2.5:latest,2025-11-02 10:09:03,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Linux Containers,Hybrid Approach: LXC on AWS,"#### Hybrid Approach: LXC on AWS
LXC can be used effectively with full-fat virtualization too. For example, running LXC containers on an EC2 instance allows leveraging the on-demand compute power of cloud platforms while maintaining local flexibility.

:p How can Linux Containers be used in conjunction with AWS?
??x
Linux Containers can be run on top of a large EC2 instance to take advantage of on-demand compute resources while using lightweight, isolated processes. This hybrid approach combines the benefits of cloud scalability with the efficiency and control provided by containers.
x??

---",599,"Vagrant Vagrant is a very useful deployment platform, which is normally used for dev and test rather than production. Vagrant provides you with a virtual cloud on your laptop. Underneath, it uses a st...",qwen2.5:latest,2025-11-02 10:09:03,7
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Docker,Docker Overview,"#### Docker Overview
Docker is a platform that simplifies container management by providing tools and abstractions to handle container creation, deployment, networking, and registry storage. It allows developers to package applications along with their dependencies into lightweight, portable containers.

:p What does Docker provide for managing applications in the context of containerization?
??x
Docker provides a platform for creating, deploying, and running packaged applications (referred to as Docker apps or images) in lightweight containers. This includes handling container provisioning, networking, and registry management.
x??",639,"Docker Docker is a platform built on top of lightweight containers. It handles much of the work around handling containers for you. In Docker, you create and deploy  apps , which are synonymous with i...",qwen2.5:latest,2025-11-02 10:09:28,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Docker,VM vs. Container Abstraction,"#### VM vs. Container Abstraction
In traditional virtual machine (VM) environments, developers deal with full operating systems, whereas Docker abstracts away the underlying OS, focusing on application-level packaging.

:p How does Docker simplify application deployment compared to using VMs?
??x
Docker simplifies application deployment by hiding the underlying OS details and focusing on lightweight containerization. Instead of managing a full OS for each application (as with VMs), Docker allows multiple applications to share the host OS, leading to more efficient use of resources.
x??",592,"Docker Docker is a platform built on top of lightweight containers. It handles much of the work around handling containers for you. In Docker, you create and deploy  apps , which are synonymous with i...",qwen2.5:latest,2025-11-02 10:09:28,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Docker,Docker App Abstraction,"#### Docker App Abstraction
Docker abstracts away the underlying technology used to implement services, similar to how VM images work.

:p What is the benefit of Docker's app abstraction for developers?
??x
The benefit of Docker's app abstraction is that it shields developers from the complexities of the underlying OS and container management. Developers can focus on building their applications without worrying about the intricacies of deployment infrastructure.
x??",470,"Docker Docker is a platform built on top of lightweight containers. It handles much of the work around handling containers for you. In Docker, you create and deploy  apps , which are synonymous with i...",qwen2.5:latest,2025-11-02 10:09:28,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Docker,Docker Registry Concept,"#### Docker Registry Concept
Docker introduces its own registry concept, which allows users to store and version Docker images.

:p How does Docker's registry help in managing applications?
??x
Docker's registry helps in storing and versioning application images. This means developers can maintain different versions of their applications, tag them for easy identification, and push these images to a central repository for sharing or deployment.
x??",451,"Docker Docker is a platform built on top of lightweight containers. It handles much of the work around handling containers for you. In Docker, you create and deploy  apps , which are synonymous with i...",qwen2.5:latest,2025-11-02 10:09:28,7
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Docker,Docker for Development and Testing,"#### Docker for Development and Testing
Using Docker can reduce the complexity of setting up multiple services locally by running a single VM that hosts a Docker instance.

:p How does Docker simplify local development and testing?
??x
Docker simplifies local development and testing by allowing developers to run a single VM with Docker, rather than multiple VMs. This reduces setup time and overhead, making it faster to develop and test applications.
x??",457,"Docker Docker is a platform built on top of lightweight containers. It handles much of the work around handling containers for you. In Docker, you create and deploy  apps , which are synonymous with i...",qwen2.5:latest,2025-11-02 10:09:28,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Docker,CoreOS Integration,"#### CoreOS Integration
CoreOS is an operating system designed specifically for running Docker containers efficiently.

:p What makes CoreOS suitable for running Docker containers?
??x
CoreOS is suitable for running Docker containers because it is a lightweight Linux distribution that provides only the essential services needed to run Docker. This minimalistic approach allows more resources to be dedicated to the containers themselves, enhancing efficiency.
x??",465,"Docker Docker is a platform built on top of lightweight containers. It handles much of the work around handling containers for you. In Docker, you create and deploy  apps , which are synonymous with i...",qwen2.5:latest,2025-11-02 10:09:28,7
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Docker,Scheduling Layer Requirements,"#### Scheduling Layer Requirements
For managing multiple Docker instances across machines, additional tools like Kubernetes or CoreOS's cluster technology are required.

:p What is a key requirement for running Docker in a multi-machine environment?
??x
A key requirement for running Docker in a multi-machine environment is the need for a scheduling layer that can allocate and manage containers. Tools like Kubernetes from Google or CoreOS’s cluster technology help in this regard by providing ways to request and run containers across multiple machines.
x??",560,"Docker Docker is a platform built on top of lightweight containers. It handles much of the work around handling containers for you. In Docker, you create and deploy  apps , which are synonymous with i...",qwen2.5:latest,2025-11-02 10:09:28,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Docker,Deis as a PaaS Solution,"#### Deis as a PaaS Solution
Deis is an attempt to provide a Heroku-like platform using Docker, focusing on container-based deployments.

:p How does Deis fit into the Docker ecosystem?
??x
Deis fits into the Docker ecosystem by providing a Platform-as-a-Service (PaaS) solution similar to Heroku but based entirely on containers. It aims to simplify application deployment and management while leveraging Docker for efficient resource utilization.
x??",452,"Docker Docker is a platform built on top of lightweight containers. It handles much of the work around handling containers for you. In Docker, you create and deploy  apps , which are synonymous with i...",qwen2.5:latest,2025-11-02 10:09:28,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Docker,CaaS (Containers as a Service),"#### CaaS (Containers as a Service)
Docker, combined with appropriate scheduling layers, is positioned between Infrastructure-as-a-Service (IaaS) and Platform-as-a-Service (PaaS).

:p How does Docker fit into the broader cloud computing stack?
??x
Docker fits into the broader cloud computing stack by offering Containers as a Service (CaaS), which sits between IaaS and PaaS. It provides an efficient way to manage containers while abstracting away much of the underlying complexity, making it suitable for various deployment scenarios.
x??

---",546,"Docker Docker is a platform built on top of lightweight containers. It handles much of the work around handling containers for you. In Docker, you create and deploy  apps , which are synonymous with i...",qwen2.5:latest,2025-11-02 10:09:28,7
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,A Deployment Interface,Deployment Interface Overview,"#### Deployment Interface Overview
In order to ensure uniform deployment mechanisms from development to production, a single parameterizable command-line call is recommended. This approach allows for consistency and ease of use across different environments. The command typically requires three parameters: artifact name (microservice), version, and environment.
:p What are the key elements of a unified deployment interface?
??x
The key elements include:
1. A known entity or microservice name.
2. The version of the artifact to deploy, which could be locally, latest green build from an artifact repository, or an exact build for testing/bugs.
3. The environment where the service should be deployed.

This setup ensures that deployments are consistent and can be easily triggered via scripts or manually in various scenarios like local development, CI testing, and production environments.
??x",898,"A Deployment Interface Whatever underlying platform or artifacts you use, having a uniform interface to deploy a given service is vital. We’ll want to trigger deployment of a microservice on demand in...",qwen2.5:latest,2025-11-02 10:10:02,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,A Deployment Interface,Example Deployment Command,"#### Example Deployment Command
The deployment command is parameterized to include artifact name, version, and environment. This allows for flexibility in deploying different versions of a microservice into various environments.

Example command format:
```
$ deploy artifact=<microservice_name> environment=<environment_name> version=<version>
```

:p How would you deploy the catalog service locally using the provided deployment command?
??x
The command to deploy the catalog service locally would be:

```sh
$ deploy artifact=catalog environment=local version=local
```
This command specifies that the `catalog` microservice should be deployed in the local environment with the same version as on the local machine.
??x",723,"A Deployment Interface Whatever underlying platform or artifacts you use, having a uniform interface to deploy a given service is vital. We’ll want to trigger deployment of a microservice on demand in...",qwen2.5:latest,2025-11-02 10:10:02,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,A Deployment Interface,CI/CD Trigger Example,"#### CI/CD Trigger Example
The CI build service can pick up changes and trigger deployments automatically. The latest build artifact from a successful CI run is passed through the pipeline to subsequent stages.

Example command for CI stage:
```
$ deploy artifact=catalog environment=ci version=b456
```

:p How would you deploy the catalog service into an integrated test environment using the latest build artifact?
??x
To deploy the catalog service into an integrated test environment with the latest build artifact, you would use the following command:

```sh
$ deploy artifact=catalog environment=integrated_qa version=latest
```
This command deploys the `catalog` microservice in the integrated QA environment using the most recent build from the CI/CD pipeline.
??x",772,"A Deployment Interface Whatever underlying platform or artifacts you use, having a uniform interface to deploy a given service is vital. We’ll want to trigger deployment of a microservice on demand in...",qwen2.5:latest,2025-11-02 10:10:02,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,A Deployment Interface,Deployment Automation with Fabric and Boto,"#### Deployment Automation with Fabric and Boto
Fabric is a Python library that allows mapping command-line calls to functions. It supports tasks like SSH into remote machines, making it suitable for deploying services across cloud environments.

:p How can you use Fabric and Boto together to automate deployments on AWS?
??x
To automate deployments on AWS using Fabric and Boto, you would typically create a script that leverages both tools. Here is an example of how this might be structured:

```python
from fabric import Connection
import boto3

def deploy_microservice(connection, artifact_name, version, environment):
    # SSH into the target machine
    conn = Connection(host='your_remote_host', user='ubuntu')

    # Use Boto to interact with AWS services if needed
    ec2 = boto3.resource('ec2')
    
    # Deploy logic here
    conn.run(f'deploy artifact={artifact_name} environment={environment} version={version}')
```

This script sets up a connection to an EC2 instance and uses Fabric's `Connection` object to run the deployment command on that machine. Boto3 can be used for more complex interactions with AWS services if necessary.
??x",1156,"A Deployment Interface Whatever underlying platform or artifacts you use, having a uniform interface to deploy a given service is vital. We’ll want to trigger deployment of a microservice on demand in...",qwen2.5:latest,2025-11-02 10:10:02,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,A Deployment Interface,Alternative Deployment Tools,"#### Alternative Deployment Tools
For different technology stacks, alternative tools like Capistrano (Ruby) or PowerShell (Windows) can also be effective.

:p What are some alternatives to Fabric for deploying applications?
??x
Some alternatives to Fabric for deploying applications include:
- **Capistrano**: A Ruby-based deployment tool that is similar to Fabric in its functionality.
- **PowerShell**: For Windows environments, PowerShell scripts can be used to automate deployments.
These tools offer similar capabilities and can be configured to fit the specific needs of different development stacks and environments.
??x
---",631,"A Deployment Interface Whatever underlying platform or artifacts you use, having a uniform interface to deploy a given service is vital. We’ll want to trigger deployment of a microservice on demand in...",qwen2.5:latest,2025-11-02 10:10:02,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Environment Definition,Environment Definition Overview,"#### Environment Definition Overview
Background context: The environment definition is a YAML file that maps microservices to specific compute, network, and storage resources for different environments. It allows specifying varying resource requirements based on the environment type (e.g., development vs production) and can include credentials management.

:p What are the key components of an environment definition?
??x
The key components of an environment definition include:
- **Nodes**: Specifies the instance types (AMI ID, size), region, and credentials for each node.
- **Services**: Lists which services run on each node.
- **Number of Nodes**: Indicates how many instances to launch in certain environments.
- **Credentials Management**: Allows specifying different credentials for different environments, often stored separately for security reasons.

For example:
```yaml
development:
  nodes:
    - ami_id: ami-e1e1234
      size: t1.micro
      credentials_name: eu-west-ssh
      services: [catalog-service]
  region: eu-west-1

production:
  nodes:
    - ami_id: ami-e1e1234
      size: m3.xlarge
      credentials_name: prod-credentials
      services: [catalog-service]
  number: 5
```
x??",1209,"Environment Definition Clearly, for this to work, we need to have some way of defining what our environments look like, and what our service looks like in a given environment. You can think of an envi...",qwen2.5:latest,2025-11-02 10:10:34,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Environment Definition,Service Configuration in Environments,"#### Service Configuration in Environments
Background context: Services configurations remain consistent across different environments, but the resources (nodes, load balancers) can vary. The configuration details are stored separately to focus on service-specific requirements.

:p What is the key information stored for a microservice in an environment definition?
??x
The key information stored for a microservice in an environment definition includes:
- **Puppet Manifest**: Specifies the Puppet file to run for configuring the service.
- **Connectivity Details**: Defines protocols, ports, and access control rules.

For example:
```yaml
catalog-service:
  puppet_manifest: catalog.pp
  connectivity:
    - protocol: tcp
      ports: [8080, 8081]
      allowed: [WORLD]
```
x??",782,"Environment Definition Clearly, for this to work, we need to have some way of defining what our environments look like, and what our service looks like in a given environment. You can think of an envi...",qwen2.5:latest,2025-11-02 10:10:34,7
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Environment Definition,Resource Management Across Environments,"#### Resource Management Across Environments
Background context: The system uses different resources (instance sizes, number of nodes) for various environments based on cost-effectiveness and testing needs. By default, load balancers are automatically created if a service has more than one instance.

:p How does the system manage resource scaling across environments?
??x
The system manages resource scaling by:
- Specifying different instance types (sizes and AMI IDs) in each environment.
- Configuring the number of nodes for certain environments to optimize cost or testing efficiency.
- Automatically creating load balancers if a service is deployed on multiple instances.

For example, comparing development and production settings:
```yaml
development:
  nodes: 
    - ami_id: ami-e1e1234
      size: t1.micro
      services: [catalog-service]
  region: eu-west-1

production:
  nodes:
    - ami_id: ami-e1e1234
      size: m3.xlarge
      number: 5
      services: [catalog-service]
```
x??",1000,"Environment Definition Clearly, for this to work, we need to have some way of defining what our environments look like, and what our service looks like in a given environment. You can think of an envi...",qwen2.5:latest,2025-11-02 10:10:34,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Environment Definition,Credentials and Security Management,"#### Credentials and Security Management
Background context: To ensure security, credentials for sensitive environments are stored separately in specific source code repositories accessible only to selected personnel. This separation helps manage access controls effectively.

:p How is the credentials management handled across different environments?
??x
Credentials management is handled by:
- Storing credentials for sensitive environments separately.
- Limiting access to these credentials through controlled repository permissions.
- Using unique credentials names in environment definitions to reference the correct repositories or secrets managers.

For example, defining unique credential names for different environments:
```yaml
development:
  nodes: 
    - ami_id: ami-e1e1234
      size: t1.micro
      credentials_name: eu-west-ssh

production:
  nodes:
    - ami_id: ami-e1e1234
      size: m3.xlarge
      credentials_name: prod-credentials
```
x??",964,"Environment Definition Clearly, for this to work, we need to have some way of defining what our environments look like, and what our service looks like in a given environment. You can think of an envi...",qwen2.5:latest,2025-11-02 10:10:34,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Environment Definition,Example Environment Definitions for Different Environments,"#### Example Environment Definitions for Different Environments
Background context: The environment definitions allow specifying varying resource requirements and configurations based on the environment type. This ensures cost-effectiveness, ease of management, and security.

:p What are some key differences between development and production environment definitions?
??x
Some key differences between development and production environment definitions include:
- **Instance Size**: Development uses smaller instances (e.g., `t1.micro`), while production might use larger ones (e.g., `m3.xlarge`).
- **Number of Nodes**: Production environments typically have more nodes than development for redundancy and performance.
- **Credentials Management**: Different credentials are used to ensure security, with sensitive information stored separately.

For example:
```yaml
development:
  nodes: 
    - ami_id: ami-e1e1234
      size: t1.micro
      services: [catalog-service]
  region: eu-west-1

production:
  nodes:
    - ami_id: ami-e1e1234
      size: m3.xlarge
      number: 5
      services: [catalog-service]
```
x??",1121,"Environment Definition Clearly, for this to work, we need to have some way of defining what our environments look like, and what our service looks like in a given environment. You can think of an envi...",qwen2.5:latest,2025-11-02 10:10:34,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Environment Definition,Puppet Configuration for Microservices,"#### Puppet Configuration for Microservices
Background context: The Puppet configuration system is used to manage the deployment of microservices, ensuring consistency and ease of updates across environments. This approach leverages conventions such as standardizing port usage.

:p How does Puppet manifest management work in environment definitions?
??x
Puppet manifest management works by:
- Storing the name or path to a Puppet manifest file for each service.
- Using Puppet configurations to manage the deployment and configuration of services consistently across environments.

For example, defining a Puppet manifest for a microservice:
```yaml
catalog-service:
  puppet_manifest: catalog.pp
```
x??",706,"Environment Definition Clearly, for this to work, we need to have some way of defining what our environments look like, and what our service looks like in a given environment. You can think of an envi...",qwen2.5:latest,2025-11-02 10:10:34,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Environment Definition,Load Balancing Strategy Across Environments,"#### Load Balancing Strategy Across Environments
Background context: The system automatically creates load balancers when multiple instances of a service are deployed. This helps distribute traffic efficiently and ensures high availability.

:p How does the system handle load balancing for microservices?
??x
The system handles load balancing by:
- Automatically creating load balancers if more than one instance is configured for a service.
- Using the region specified in the environment definition to target the correct AWS regions or other cloud services.

For example, configuring multiple nodes with an auto-created load balancer:
```yaml
production:
  nodes: 
    - ami_id: ami-e1e1234
      size: m3.xlarge
      number: 5
      services: [catalog-service]
```
x??",773,"Environment Definition Clearly, for this to work, we need to have some way of defining what our environments look like, and what our service looks like in a given environment. You can think of an envi...",qwen2.5:latest,2025-11-02 10:10:34,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Environment Definition,Future Tooling and Terraform,"#### Future Tooling and Terraform
Background context: Tools like Terraform are emerging to simplify the management of environments and resources. Although these tools are new, they offer promising capabilities for managing infrastructure as code.

:p What is Terraform, and how does it fit into environment definition management?
??x
Terraform is a tool from HashiCorp that helps manage and provision infrastructure as code. It allows defining and automatically applying the infrastructure you need across different environments efficiently.

For example, using Terraform to define an environment:
```hcl
resource ""aws_instance"" ""example"" {
  ami           = ""ami-e1e1234""
  instance_type = ""t1.micro""

  tags = {
    Name = ""catalog-service-dev""
  }
}
```
x??

---",765,"Environment Definition Clearly, for this to work, we need to have some way of defining what our environments look like, and what our service looks like in a given environment. You can think of an envi...",qwen2.5:latest,2025-11-02 10:10:34,7
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Types of Tests,Service Independence and CI Builds,"#### Service Independence and CI Builds
Maintaining the ability to release one microservice independently from another is crucial. This requires using a technology that supports independent deployment of services, such as separating each service into its own repository and ensuring there's a CI build per microservice for separate deployment.

:p What are the key points regarding service independence in microservices?
??x
The key points include maintaining the ability to release one service independently from another. This requires using a technology that supports independent deployment of services, such as separating each service into its own repository and ensuring there's a CI build per microservice for separate deployment.
x??",739,"Summary We’ve covered a lot of ground here, so a recap is in order. First, focus on maintaining the ability to release one service independently from another, and make sure that whatever technology yo...",qwen2.5:latest,2025-11-02 10:11:07,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Types of Tests,Single-Service Per Host/Container,"#### Single-Service Per Host/Container
It is beneficial to move towards running each microservice on a single host or container to simplify management. Technologies like LXC or Docker can be used to make this process cheaper and easier, provided that the culture of automation is strong.

:p Why should we aim for a single-service per host/container?
??x
Aiming for a single-service per host/container simplifies management by isolating each microservice in its own environment. This makes it easier to manage dependencies, updates, and scaling individually. Technologies like LXC or Docker can be used to facilitate this setup, but the key is ensuring that automation is deeply ingrained.

Using Docker as an example:
```bash
# Example of creating a container for a service
docker run -d --name my-service-container -p 8080:8080 my-service-image
```
x??",854,"Summary We’ve covered a lot of ground here, so a recap is in order. First, focus on maintaining the ability to release one service independently from another, and make sure that whatever technology yo...",qwen2.5:latest,2025-11-02 10:11:07,7
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Types of Tests,Automation and Platform Use,"#### Automation and Platform Use
Automation is crucial in managing microservices. Using platforms like AWS can provide significant benefits due to the extensive automation features these platforms offer.

:p Why is automation important for managing microservices?
??x
Automation is essential because it allows for consistent, reliable, and repeatable processes in deploying, scaling, and maintaining microservices. Platforms like AWS facilitate this by offering built-in automation tools that simplify tasks such as provisioning resources, deploying applications, and managing infrastructure.

Using AWS Lambda as an example:
```python
# Example of a simple Lambda function in Python
def lambda_handler(event, context):
    # Handle the event and perform necessary operations
    return {
        'statusCode': 200,
        'body': json.dumps('Hello from Lambda!')
    }
```
x??",878,"Summary We’ve covered a lot of ground here, so a recap is in order. First, focus on maintaining the ability to release one service independently from another, and make sure that whatever technology yo...",qwen2.5:latest,2025-11-02 10:11:07,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Types of Tests,Self-Service Deployment Tools,"#### Self-Service Deployment Tools
Creating tools that allow developers to self-service deploy any given service into different environments can greatly enhance productivity. This reduces the burden on DevOps teams and ensures consistency in deployment across various stages.

:p Why are self-service deployment tools important for microservices?
??x
Self-service deployment tools are crucial because they empower developers to manage their services independently, reducing the need for frequent manual intervention by DevOps teams. This results in faster deployment cycles, improved collaboration between development and operations teams, and ensures consistency across different environments like staging and production.

Example of a self-service deployment tool:
```bash
# Example command for deploying a service using a self-service tool
deploy-service my-service --stage staging
```
x??",892,"Summary We’ve covered a lot of ground here, so a recap is in order. First, focus on maintaining the ability to release one service independently from another, and make sure that whatever technology yo...",qwen2.5:latest,2025-11-02 10:11:07,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Types of Tests,Continuous Delivery (CD) Books Recommendation,"#### Continuous Delivery (CD) Books Recommendation
For more in-depth knowledge on microservices, Continuous Delivery by Jez Humble and David Farley is highly recommended. It covers topics like pipeline design and artifact management.

:p What book would you recommend for deeper understanding of microservices?
??x
The book ""Continuous Delivery: Reliable Software Releases through Build, Test, and Deployment Automation"" by Jez Humble and David Farley is highly recommended. It provides detailed insights into continuous delivery practices, including pipeline design and artifact management.
x??",595,"Summary We’ve covered a lot of ground here, so a recap is in order. First, focus on maintaining the ability to release one service independently from another, and make sure that whatever technology yo...",qwen2.5:latest,2025-11-02 10:11:07,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Types of Tests,Testing Microservices,"#### Testing Microservices
Testing in a distributed system adds complexity. Tools like Brian Marick's testing quadrant can help categorize different types of tests to balance speed and quality.

:p What does the testing quadrant by Brian Marick differentiate?
??x
Brian Marick’s testing quadrant helps categorize different types of tests into four quadrants:
1. Bottom-left: Technology-facing tests (unit tests, performance tests)
2. Top-left: Business-facing tests that help non-technical stakeholders understand system behavior (acceptance tests)
3. Top-right: End-to-end tests (integration and acceptance tests) and manual testing
4. Bottom-right: Exploratory testing

This categorization helps in understanding the balance between different types of tests to ensure both speed and quality.
x??",797,"Summary We’ve covered a lot of ground here, so a recap is in order. First, focus on maintaining the ability to release one service independently from another, and make sure that whatever technology yo...",qwen2.5:latest,2025-11-02 10:11:07,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Types of Tests,Types of Tests for Microservices,"#### Types of Tests for Microservices
Understanding test types is crucial. Common categories include technology-facing tests, business-facing tests, end-to-end tests, and exploratory testing.

:p What are the main types of automated tests mentioned for microservices?
??x
The main types of automated tests mentioned for microservices include:
1. Technology-facing tests: Unit tests, performance tests (automated)
2. Business-facing tests: Large-scoped, end-to-end tests like acceptance tests and manual user testing

These tests help in understanding how the system works from both technical and non-technical perspectives.
x??

---",632,"Summary We’ve covered a lot of ground here, so a recap is in order. First, focus on maintaining the ability to release one service independently from another, and make sure that whatever technology yo...",qwen2.5:latest,2025-11-02 10:11:07,7
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Test Scope,Test Pyramid Overview,"#### Test Pyramid Overview
Background context: Mike Cohn, in his book ""Succeeding with Agile,"" outlines a model called the Test Pyramid to guide the types and proportions of automated tests needed. The pyramid aims to help teams think about test coverage scope as well as the relative amounts of different types of tests.
:p What is the Test Pyramid?
??x
The Test Pyramid is a conceptual framework proposed by Mike Cohn in his book ""Succeeding with Agile"" that helps in understanding and balancing the types and proportions of automated tests, particularly unit, service, and UI (now called end-to-end) tests. This model encourages teams to focus on smaller and more granular unit tests at the base of the pyramid, with fewer but broader integration or service tests above, and even fewer end-to-end tests at the top.
x??",821,"Test Scope In his book  Succeeding with Agile  (Addison-Wesley), Mike Cohn outlines a model called the Test Pyramid to help explain what types of automated tests you need. The pyramid helps us think a...",qwen2.5:latest,2025-11-02 10:11:35,4
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Test Scope,Unit Tests,"#### Unit Tests
Background context: Unit tests are a type of automated test that typically checks individual units or functions in isolation. Cohn’s original model referred to these as ""unit"" tests, though this term can be ambiguous since it is often used interchangeably with other terms like component testing.
:p What defines a unit test according to the Test Pyramid?
??x
A unit test, according to the Test Pyramid, typically checks individual units or functions in isolation. This means that each test focuses on a small piece of functionality without considering its interactions with other parts of the system. In Cohn’s original model, these are referred to as ""unit"" tests.
x??",686,"Test Scope In his book  Succeeding with Agile  (Addison-Wesley), Mike Cohn outlines a model called the Test Pyramid to help explain what types of automated tests you need. The pyramid helps us think a...",qwen2.5:latest,2025-11-02 10:11:35,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Test Scope,Service Tests,"#### Service Tests
Background context: Service tests, according to Mike Cohn, are a type of automated test that checks multiple functions or classes working together in an integrated manner but not involving the user interface (UI). This layer is often misunderstood and can be confused with other types of integration testing.
:p What are service tests in the Test Pyramid?
??x
Service tests, as defined by Mike Cohn in the Test Pyramid, check multiple functions or classes working together in an integrated manner without involving the UI. These tests are broader than unit tests but narrower than end-to-end (UI) tests. They simulate interactions between different components of the system.
x??",697,"Test Scope In his book  Succeeding with Agile  (Addison-Wesley), Mike Cohn outlines a model called the Test Pyramid to help explain what types of automated tests you need. The pyramid helps us think a...",qwen2.5:latest,2025-11-02 10:11:35,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Test Scope,End-to-End Tests,"#### End-to-End Tests
Background context: End-to-end tests were originally referred to as UI tests by Mike Cohn in his Test Pyramid model, but he now prefers this term for clarity and consistency with broader software testing practices.
:p What are end-to-end (UI) tests?
??x
End-to-end (UI) tests, previously called UI tests by Mike Cohn, simulate the entire user journey from start to finish. These tests cover interactions from the front end all the way through to back-end services and databases. They provide high-level coverage of how different parts of the application interact in real-world scenarios.
x??",613,"Test Scope In his book  Succeeding with Agile  (Addison-Wesley), Mike Cohn outlines a model called the Test Pyramid to help explain what types of automated tests you need. The pyramid helps us think a...",qwen2.5:latest,2025-11-02 10:11:35,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Test Scope,Example Scenario,"#### Example Scenario
Background context: The provided text describes a helpdesk application, main website, customer service, loyalty points bank, and their interactions. This scenario is used to illustrate how tests can be structured according to the Test Pyramid.
:p Can you provide an example of testing different layers in the music shop system?
??x
Certainly! Let's consider the music shop system described:
- **Unit Tests**: Testing individual functions like validating a customer ID or checking loyalty point balances.
```java
public class CustomerService {
    @Test
    public void testValidateCustomerId() {
        // Test logic here
    }
}
```
- **Service Tests**: Testing interactions between components, such as retrieving and updating customer details from both the helpdesk application and the main website.
```java
public class ServiceLayerTests {
    @Test
    public void testRetrieveCustomerDetails() {
        // Test logic to simulate interaction between helpdesk app and main site
    }
}
```
- **End-to-End Tests**: Simulating a full user journey, such as a customer interacting with the website, being redirected to the helpdesk application for further assistance, and then the system updating the loyalty points bank.
```java
public class EndToEndTests {
    @Test
    public void testFullCustomerInteraction() {
        // Test logic that covers interactions across multiple layers
    }
}
```
x??",1425,"Test Scope In his book  Succeeding with Agile  (Addison-Wesley), Mike Cohn outlines a model called the Test Pyramid to help explain what types of automated tests you need. The pyramid helps us think a...",qwen2.5:latest,2025-11-02 10:11:35,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Those Tricky End-to-End Tests,Unit Tests,"#### Unit Tests
Background context explaining unit tests. Unit tests typically test a single function or method call and are used in test-driven design (TDD) and property-based testing. The goal is to provide very fast feedback about the functionality of code snippets in isolation.
:p What are unit tests?
??x
Unit tests are designed to test individual functions or methods in isolation, providing quick feedback on the correctness of small parts of the codebase. They help catch bugs early in the development process and support refactoring by ensuring that changes do not break existing functionality.

```java
public void testAddition() {
    Calculator calculator = new Calculator();
    int result = calculator.add(1, 2);
    assertEquals(3, result);
}
```
x??",766,"Unit Tests These are tests that typically test a single function or method call. The tests generated as a side effect of  test-driven design  (TDD) will fall into this category, as do the sorts of tes...",qwen2.5:latest,2025-11-02 10:12:00,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Those Tricky End-to-End Tests,Service Tests,"#### Service Tests
Background context explaining service tests. Service tests are used to test individual services within a system or monolithic application by stubbing out external collaborators, allowing for isolated testing of service capabilities.
:p What are service tests?
??x
Service tests bypass the user interface and focus on testing individual services in isolation. This is achieved by stubbing out all external dependencies, ensuring that only the service itself is tested. Service tests help improve the isolation of tests, making it easier to find and fix issues.

```java
public void testUserService() {
    UserService userService = new UserService();
    MockUserRepository userRepository = new MockUserRepository();
    userService.setUserRepository(userRepository);
    
    User user = new User(""john"", ""password"");
    userService.registerUser(user);
    assertEquals(1, userRepository.users.size());
}
```
x??",932,"Unit Tests These are tests that typically test a single function or method call. The tests generated as a side effect of  test-driven design  (TDD) will fall into this category, as do the sorts of tes...",qwen2.5:latest,2025-11-02 10:12:00,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Those Tricky End-to-End Tests,End-to-End Tests,"#### End-to-End Tests
Background context explaining end-to-end tests. These tests simulate the entire workflow of a system from start to finish, often driving a GUI through a browser or mimicking other user interactions.
:p What are end-to-end tests?
??x
End-to-end (E2E) tests cover the complete flow of a system, including interactions with databases and external services. They provide high confidence that the code will work in production environments but can be slow and more complex to set up and maintain.

```java
@bdd.scenario(""User registration"", ""Testing end-to-end flow"")
public void userRegistration() {
    WebDriver driver = new ChromeDriver();
    
    // Navigate to the registration page
    driver.get(""http://example.com/register"");
    
    // Fill in form fields
    WebElement usernameField = driver.findElement(By.name(""username""));
    usernameField.sendKeys(""testuser"");
    
    WebElement passwordField = driver.findElement(By.name(""password""));
    passwordField.sendKeys(""testpass123"");
    
    // Submit the form
    WebElement submitButton = driver.findElement(By.cssSelector(""input[type='submit']""));
    submitButton.click();
    
    // Check if the user is registered successfully
    WebElement successMessage = driver.findElement(By.id(""success-message""));
    assertEquals(""Registration successful!"", successMessage.getText());
}
```
x??",1377,"Unit Tests These are tests that typically test a single function or method call. The tests generated as a side effect of  test-driven design  (TDD) will fall into this category, as do the sorts of tes...",qwen2.5:latest,2025-11-02 10:12:00,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Those Tricky End-to-End Tests,Trade-Offs in Testing,"#### Trade-Offs in Testing
Background context explaining trade-offs between different types of tests. The pyramid model illustrates that as test scope increases, so does the confidence level and complexity.
:p What are the key trade-offs when considering unit, service, and end-to-end tests?
??x
The key trade-offs involve balancing the speed of feedback with the confidence in functionality:

- **Unit Tests**: Fast but limited to small scopes; catch local bugs early.
- **Service Tests**: Faster than E2E, isolated environment for services; harder to pinpoint issues if they fail.
- **End-to-End Tests**: Provide full system confidence, slow and resource-intensive.

For example:
```java
// Unit Test - Fast and focused on a small function
public void testAddition() {
    Calculator calculator = new Calculator();
    int result = calculator.add(1, 2);
    assertEquals(3, result);
}

// Service Test - Isolated environment for testing services
public void testUserService() {
    UserService userService = new UserService();
    MockUserRepository userRepository = new MockUserRepository();
    userService.setUserRepository(userRepository);
    
    User user = new User(""john"", ""password"");
    userService.registerUser(user);
    assertEquals(1, userRepository.users.size());
}

// End-to-End Test - Full system simulation
@bdd.scenario(""User registration"", ""Testing end-to-end flow"")
public void userRegistration() {
    WebDriver driver = new ChromeDriver();
    
    // Navigate to the registration page
    driver.get(""http://example.com/register"");
    
    // Fill in form fields and submit, then verify success message
}
```
x??

---",1647,"Unit Tests These are tests that typically test a single function or method call. The tests generated as a side effect of  test-driven design  (TDD) will fall into this category, as do the sorts of tes...",qwen2.5:latest,2025-11-02 10:12:00,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Those Tricky End-to-End Tests,Broken Functionality Detection,"#### Broken Functionality Detection
Background context: The text discusses how broken functionality detection is crucial for maintaining software quality. It mentions that finding and fixing broken functionality faster leads to more efficient development processes.

:p How can a team ensure they detect broken functionality faster?
??x
A team can ensure they detect broken functionality faster by using a combination of unit tests, service tests, and end-to-end tests in their continuous integration builds. This approach helps in quickly identifying which part of the code has caused an issue.
???x",600,"We find broken functionality faster, our continuous integration builds are faster, and we are less likely to move on to a new task before finding out we have broken something. When those smaller-scope...",qwen2.5:latest,2025-11-02 10:12:20,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Those Tricky End-to-End Tests,Continuous Integration Builds,"#### Continuous Integration Builds
Background context: The text highlights that faster continuous integration (CI) builds contribute to quicker feedback cycles, thereby reducing the likelihood of moving on to new tasks without verifying the stability of existing ones.

:p How do faster CI builds help in development processes?
??x
Faster CI builds allow developers to catch and fix issues earlier in the development cycle. This ensures that they do not proceed with new features or tasks until the current code is confirmed to be stable, thus reducing the risk of introducing more bugs.
???x",592,"We find broken functionality faster, our continuous integration builds are faster, and we are less likely to move on to a new task before finding out we have broken something. When those smaller-scope...",qwen2.5:latest,2025-11-02 10:12:20,7
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Those Tricky End-to-End Tests,Feedback Cycles Improvement,"#### Feedback Cycles Improvement
Background context: The text suggests that feedback cycles are crucial for efficient development. It mentions that smaller-scoped tests provide quicker feedback on what exactly broke.

:p How can a team improve their feedback cycles?
??x
A team can improve their feedback cycles by writing more unit tests and fewer end-to-end or service tests, as unit tests provide faster feedback on specific parts of the codebase. The team should also aim to replace broader-scoped tests with smaller-scoped ones where possible.
???x",553,"We find broken functionality faster, our continuous integration builds are faster, and we are less likely to move on to a new task before finding out we have broken something. When those smaller-scope...",qwen2.5:latest,2025-11-02 10:12:20,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Those Tricky End-to-End Tests,Test Pyramid Balance,"#### Test Pyramid Balance
Background context: The text explains that different types of automated tests serve different purposes and suggests a general rule of thumb for balancing them.

:p How many tests are recommended in each layer of the test pyramid?
??x
A good rule of thumb is to have an order of magnitude more unit tests than service tests, and an order of magnitude more service tests than end-to-end tests. This means if you have 100 unit tests, you might aim for around 10 service tests, and 1 or fewer end-to-end tests.
???x",537,"We find broken functionality faster, our continuous integration builds are faster, and we are less likely to move on to a new task before finding out we have broken something. When those smaller-scope...",qwen2.5:latest,2025-11-02 10:12:20,7
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Those Tricky End-to-End Tests,Test Snow Cone,"#### Test Snow Cone
Background context: The text describes a common anti-pattern known as the test snow cone, where there are few small-scoped tests but many large-scoped ones.

:p What is the issue with a test snow cone structure?
??x
A test snow cone structure has too many end-to-end and service tests, leading to slow feedback cycles. This setup can significantly impact development speed because builds run slower and issues might remain unresolved for longer periods.
???x",478,"We find broken functionality faster, our continuous integration builds are faster, and we are less likely to move on to a new task before finding out we have broken something. When those smaller-scope...",qwen2.5:latest,2025-11-02 10:12:20,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Those Tricky End-to-End Tests,Implementing Service Tests,"#### Implementing Service Tests
Background context: The text discusses the implementation of service tests, which aim to test a slice of functionality across an entire service while stubbing out collaborators.

:p How do you implement service tests in a system?
??x
To implement service tests, you need to create instances of services and stub out any downstream dependencies. For example, if testing the customer service from Figure 7-3, deploy the customer service instance and stub out its downstream service (like the loyalty points bank). Configure these stubs to return expected responses.
???x",600,"We find broken functionality faster, our continuous integration builds are faster, and we are less likely to move on to a new task before finding out we have broken something. When those smaller-scope...",qwen2.5:latest,2025-11-02 10:12:20,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Those Tricky End-to-End Tests,Stubbing Downstream Collaborators,"#### Stubbing Downstream Collaborators
Background context: The text explains how to mock or stub collaborators in a test environment.

:p How do you handle faking downstream collaborators for service tests?
??x
To fake downstream collaborators, you can create a stub service that responds with predefined responses. For example, if testing the customer service, you might configure the stub loyalty points bank to return known points balances for certain customers.
???x",470,"We find broken functionality faster, our continuous integration builds are faster, and we are less likely to move on to a new task before finding out we have broken something. When those smaller-scope...",qwen2.5:latest,2025-11-02 10:12:20,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Those Tricky End-to-End Tests,Stub vs Mock Approach,"#### Stub vs Mock Approach

Background context: The text discusses the use of stubs and mocks in testing, explaining their differences and when to use each. It also introduces tools like Mountebank for creating mock services.

:p What is the main difference between a stub and a mock in testing?

??x
A stub does not care about how many times it is called or if any calls are made, whereas a mock verifies that specific methods or actions were performed during the test. Stubs return fixed responses to simplify tests, while mocks track interactions to ensure they occur as expected.
x??",587,"For example, I might tell my stub points bank that when asked for the balance of customer 123, it should return 15,000. The test doesn’t care if the stub is called 0, 1, or 100 times. A variation on t...",qwen2.5:latest,2025-11-02 10:12:46,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Those Tricky End-to-End Tests,Use of Mountebank for Stub Services,"#### Use of Mountebank for Stub Services

Background context: The text mentions using tools like Mountebank to create stub services programmatically via HTTP commands.

:p How does Mountebank help in creating test doubles?

??x
Mountebank helps by allowing you to programmatically define and manage stubs through HTTP commands. You can specify the port, protocol, and responses for each endpoint. This makes it easier to simulate various scenarios without writing custom code.
x??",480,"For example, I might tell my stub points bank that when asked for the balance of customer 123, it should return 15,000. The test doesn’t care if the stub is called 0, 1, or 100 times. A variation on t...",qwen2.5:latest,2025-11-02 10:12:46,7
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Those Tricky End-to-End Tests,Stubbing vs Mocking in Service Tests,"#### Stubbing vs Mocking in Service Tests

Background context: The text highlights the difference between using stubs versus mocks in service tests, explaining that stubs are used more frequently.

:p In what situations might you prefer a mock over a stub?

??x
You would prefer a mock when you need to verify interactions with a collaborator. For example, if you want to ensure that a specific method is called during the test. Stubs are useful when you just want to provide canned responses without tracking the number or nature of calls.
x??",544,"For example, I might tell my stub points bank that when asked for the balance of customer 123, it should return 15,000. The test doesn’t care if the stub is called 0, 1, or 100 times. A variation on t...",qwen2.5:latest,2025-11-02 10:12:46,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Those Tricky End-to-End Tests,Implementing a Stub Service,"#### Implementing a Stub Service

Background context: The text discusses different methods for implementing stub services, mentioning personal experiences and the introduction of Mountebank.

:p How can you implement a stub service using Mountebank?

??x
To use Mountebank as a stub service:
1. Launch it specifying the port and protocol.
2. Define the expected requests and their responses via HTTP commands.
3. Optionally set expectations to track interactions.
4. Use these endpoints in your tests.

For example, to create a simple HTTP stub:
```
curl -X POST http://localhost:2525/stubs --data '{""protocol"": ""http"", ""port"": 8081, ""methods"": [""GET""], ""responses"": [{""statusCode"": 200, ""body"": ""{'balance': 15000}""}]}'
```

x??",729,"For example, I might tell my stub points bank that when asked for the balance of customer 123, it should return 15,000. The test doesn’t care if the stub is called 0, 1, or 100 times. A variation on t...",qwen2.5:latest,2025-11-02 10:12:46,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Those Tricky End-to-End Tests,Testing Service Interactions,"#### Testing Service Interactions

Background context: The text discusses the importance of testing interactions between different services and not just individual components.

:p How can you ensure that changes in one service do not break others?

??x
You should test the interaction between services, such as running integration tests where your customer service communicates with a stubbed or mocked version of the loyalty points bank. After ensuring these internal tests pass, consider testing the external interactions by simulating how real clients (like helpdesk and web shop) would interact with your customer service.
x??

---",635,"For example, I might tell my stub points bank that when asked for the balance of customer 123, it should return 15,000. The test doesn’t care if the stub is called 0, 1, or 100 times. A variation on t...",qwen2.5:latest,2025-11-02 10:12:46,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Those Tricky End-to-End Tests,End-to-End Testing in Microservices,"#### End-to-End Testing in Microservices
Background context explaining the concept: In a microservice architecture, end-to-end tests are crucial for ensuring that all services interact correctly when deployed together. Mike Cohn's testing pyramid highlights their importance by placing them at the top due to their broad coverage of the system.
:p What is the role of end-to-end tests in a microservice system?
??x
End-to-end tests simulate user interactions through multiple services, providing an overview of how these services work together. They help ensure that all parts of the system behave as expected when deployed in production-like environments.
```java
// Pseudocode for an end-to-end test scenario
public class EndToEndTest {
    @Test
    public void testCustomerService() {
        // Deploy and configure all services (customer, helpdesk, webshop)
        CustomerService customerService = deployAndConfigureServices();
        
        // Perform actions that a user might do through the UI
        String customerId = createCustomer(customerService);
        
        // Verify interactions with other services
        boolean isHelpDeskAvailable = checkAvailability(customerId, helpdeskService);
        assertTrue(isHelpDeskAvailable);
    }
}
```
x??",1271,"Of course, we have forgotten the important tests at the top of the pyramid: the end-to-end tests. Those Tricky End-to-End Tests In a microservice system, the capabilities we expose via our user interf...",qwen2.5:latest,2025-11-02 10:13:18,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Those Tricky End-to-End Tests,Deployment Strategy for End-to-End Tests,"#### Deployment Strategy for End-to-End Tests
Background context explaining the concept: When deploying a new version of one service in a microservice system, it's essential to test its interaction with other services. This involves running end-to-end tests that require multiple service deployments.
:p How should we manage deployment and testing of new versions across services?
??x
We can use a pipeline design where each service change triggers an end-to-end test stage. This approach ensures that every time a service changes, the entire system is tested to catch potential integration issues early.

This can be achieved by having multiple pipelines fan in to a single end-to-end test stage. Here’s how this setup works:
```java
// Pseudocode for a pipeline configuration
public class PipelineConfig {
    public void configurePipelines() {
        // Service A pipeline
        serviceAPipeline = new ServicePipeline();
        serviceAPipeline.onBuild().then(testServiceAAndDependencies());
        
        // Service B pipeline
        serviceBPipeline = new ServicePipeline();
        serviceBPipeline.onBuild().then(testServiceBAndDependencies());
        
        // End-to-end test stage
        endToEndTestStage = new EndToEndTestStage();
        serviceAPipeline.fanIn(endToEndTestStage);
        serviceBPipeline.fanIn(endToEndTestStage);
    }
    
    private void testServiceAAndDependencies() {
        deployServices(); // Deploy all services including A, B, etc.
        runEndToEndTests(); // Run end-to-end tests against the deployed services
    }
}
```
x??",1584,"Of course, we have forgotten the important tests at the top of the pyramid: the end-to-end tests. Those Tricky End-to-End Tests In a microservice system, the capabilities we expose via our user interf...",qwen2.5:latest,2025-11-02 10:13:18,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Those Tricky End-to-End Tests,Fan-In Pipeline Model,"#### Fan-In Pipeline Model
Background context explaining the concept: The fan-in model is a strategy where multiple pipelines contribute to a single end-to-end test stage. This ensures that every change in any service triggers the execution of comprehensive integration tests.
:p What is the fan-in pipeline model, and why is it beneficial?
??x
The fan-in pipeline model involves setting up separate pipelines for each microservice. Each pipeline runs its specific tests but then feeds into a common end-to-end test stage where all services are deployed and tested together.

This approach is beneficial because:
- It ensures that changes in any service trigger comprehensive system-wide testing.
- It avoids redundant testing by sharing the deployment of services across multiple pipelines.
```java
// Pseudocode for fan-in pipeline model
public class PipelineManager {
    public void managePipelines() {
        List<ServicePipeline> pipelines = new ArrayList<>();
        
        // Add individual service pipelines
        ServiceA pipelineA = new ServiceAPipeline();
        pipelines.add(pipelineA);
        
        ServiceB pipelineB = new ServiceBPipeline();
        pipelines.add(pipelineB);
        
        // Common end-to-end test stage
        EndToEndTestStage endToEndTestStage = new EndToEndTestStage();
        
        for (ServicePipeline pipeline : pipelines) {
            pipeline.fanIn(endToEndTestStage); // Fan in to the common test stage
        }
    }
}
```
x??",1493,"Of course, we have forgotten the important tests at the top of the pyramid: the end-to-end tests. Those Tricky End-to-End Tests In a microservice system, the capabilities we expose via our user interf...",qwen2.5:latest,2025-11-02 10:13:18,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Those Tricky End-to-End Tests,Handling Different Service Versions,"#### Handling Different Service Versions
Background context explaining the concept: When deploying a new version of a service, it's crucial to decide whether to test against production versions or new development versions. This decision affects the reliability and speed of testing.
:p How do we handle different service versions during end-to-end testing?
??x
Handling different service versions involves deciding whether to use the current production versions or the latest development versions for end-to-end tests. This can be managed by setting up a flexible pipeline that allows for both options:
- Test against production versions to ensure compatibility with live systems.
- Test against new development versions to catch integration issues early.

This flexibility is achieved through configurable settings in the CI/CD pipelines, ensuring that different environments (staging vs. production) are tested appropriately.
```java
// Pseudocode for handling different service versions
public class PipelineConfigurator {
    public void configurePipelines(String environment) {
        if (""production"".equals(environment)) {
            // Use production services for end-to-end tests
            deployProductionServices();
        } else {
            // Use development services for end-to-end tests
            deployDevelopmentServices();
        }
        
        runEndToEndTests(); // Run the same end-to-end test against selected versions
    }
}
```
x??",1470,"Of course, we have forgotten the important tests at the top of the pyramid: the end-to-end tests. Those Tricky End-to-End Tests In a microservice system, the capabilities we expose via our user interf...",qwen2.5:latest,2025-11-02 10:13:18,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Those Tricky End-to-End Tests,Redundancy in Testing,"#### Redundancy in Testing
Background context explaining the concept: While end-to-end tests are essential, they can sometimes overlap with other testing methods. Managing this redundancy is crucial to avoid unnecessary work and ensure efficient deployment.
:p How do we prevent redundancy in our test suite?
??x
Preventing redundancy involves carefully designing your CI/CD pipeline so that different types of tests (unit, integration, end-to-end) are not duplicating each other's efforts.

For example, if you have unit tests covering basic functionalities and integration tests ensuring service interactions, end-to-end tests should focus on higher-level scenarios rather than retesting lower levels.
```java
// Pseudocode for preventing redundancy
public class TestStrategy {
    public void configureTests() {
        // Unit tests cover individual components
        List<UnitTest> unitTests = new ArrayList<>();
        
        // Integration tests ensure services interact correctly
        List<IntegrationTest> integrationTests = new ArrayList<>();
        
        // End-to-end tests cover complex scenarios involving multiple services
        List<EndToEndTest> endToEndTests = new ArrayList<>();
        
        run(unitTests, ""unit""); // Run unit tests
        run(integrationTests, ""integration""); // Run integration tests
        run(endToEndTests, ""end-to-end""); // Run end-to-end tests
        
        // Ensure no overlap by checking test coverage and dependencies
    }
    
    private void run(List<Test> tests, String testType) {
        for (Test test : tests) {
            if (!test.getCoveredByOtherTests(testType)) { // Custom logic to check redundancy
                test.run();
            }
        }
    }
}
```
x??",1752,"Of course, we have forgotten the important tests at the top of the pyramid: the end-to-end tests. Those Tricky End-to-End Tests In a microservice system, the capabilities we expose via our user interf...",qwen2.5:latest,2025-11-02 10:13:18,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Consumer-Driven Tests to the Rescue,Flaky and Brittle Tests,"---
#### Flaky and Brittle Tests
Background context: As test scope increases, so too do the number of moving parts. These can introduce failures that are not related to the functionality under test. Temporary issues like network glitches or race conditions can cause tests to fail despite the core functionality being correct.

:p What is a flaky test?
??x
A test that sometimes fails and sometimes passes, providing little useful information about the system's state. These tests are often brittle because they may depend on external factors such as service availability, network conditions, or race conditions.
x??",616,"Downsides to End-to-End Testing There are, unfortunately, many disadvantages to end-to-end testing. Flaky and Brittle Tests As test scope increases, so too do the number of moving parts. These moving ...",qwen2.5:latest,2025-11-02 10:13:41,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Consumer-Driven Tests to the Rescue,Normalization of Deviance,"#### Normalization of Deviance
Background context: Over time, if we accept flaky tests as normal, it can lead to a situation where broken functionality is tolerated, leading to potential issues in the application. This phenomenon is described by Diane Vaughan.

:p What does the normalization of deviance refer to?
??x
It refers to the human tendency to accept something as normal over time even if it's problematic initially. In testing terms, this means that flaky tests can become accepted as part of the standard, potentially leading to a disregard for actual issues in the system.
x??",589,"Downsides to End-to-End Testing There are, unfortunately, many disadvantages to end-to-end testing. Flaky and Brittle Tests As test scope increases, so too do the number of moving parts. These moving ...",qwen2.5:latest,2025-11-02 10:13:41,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Consumer-Driven Tests to the Rescue,Test Ownership and Maintenance,"#### Test Ownership and Maintenance
Background context: For end-to-end tests involving multiple services, determining who should write and maintain these tests is crucial. Initially, one might think each service's team owns their respective tests. However, when dealing with shared end-to-end tests, deciding on ownership becomes more complex.

:p Who writes the end-to-end tests in a multi-team environment?
??x
In a multi-team environment, the end-to-end tests should ideally be maintained by a cross-functional team that includes representatives from each service's owning team. Alternatively, these tests might be owned by a dedicated test or quality assurance (QA) team.
x??",679,"Downsides to End-to-End Testing There are, unfortunately, many disadvantages to end-to-end testing. Flaky and Brittle Tests As test scope increases, so too do the number of moving parts. These moving ...",qwen2.5:latest,2025-11-02 10:13:41,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Consumer-Driven Tests to the Rescue,Strategies to Mitigate Flaky Tests,"#### Strategies to Mitigate Flaky Tests
Background context: To address flaky tests, various strategies can be employed such as rewriting the tests to avoid multi-threading issues, improving environment stability, or replacing flaky tests with smaller, more deterministic ones.

:p How should flaky tests be handled?
??x
Flaky tests should be identified and tracked down. If immediate fixes are not possible, they should be removed from the test suite. Efforts can include rewriting the tests to avoid multi-threading issues, making the underlying environment more stable, or replacing them with smaller, less problematic tests.
x??",631,"Downsides to End-to-End Testing There are, unfortunately, many disadvantages to end-to-end testing. Flaky and Brittle Tests As test scope increases, so too do the number of moving parts. These moving ...",qwen2.5:latest,2025-11-02 10:13:41,7
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Consumer-Driven Tests to the Rescue,Example of Flaky Test Handling,"#### Example of Flaky Test Handling
Background context: Martin Fowler advocates for removing flaky tests and finding ways to make them deterministic.

:p What advice does Martin Fowler give regarding flaky tests?
??x
Martin Fowler suggests tracking down flaky tests. If immediate fixes are not possible, these tests should be removed from the suite. Rewriting tests to avoid multi-threading issues, improving environment stability, or replacing flaky tests with smaller tests can help.
x??",489,"Downsides to End-to-End Testing There are, unfortunately, many disadvantages to end-to-end testing. Flaky and Brittle Tests As test scope increases, so too do the number of moving parts. These moving ...",qwen2.5:latest,2025-11-02 10:13:41,7
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Consumer-Driven Tests to the Rescue,Code Example for Flaky Test Identification,"#### Code Example for Flaky Test Identification
Background context: To identify and track down flaky tests, it's useful to log detailed information about test failures.

:p How can we log detailed failure information in a test?
??x
We can log detailed failure information by capturing logs during test execution. Here’s an example of how this might be done using Java:

```java
public class TestLogger {
    private static final Logger logger = LoggerFactory.getLogger(TestLogger.class);

    public void runTest() {
        try {
            // Test logic here
            if (/* test fails */) {
                logger.error(""Test failed with stack trace: "", e);
                throw new RuntimeException(""Test failed"");
            }
        } catch (Exception e) {
            logger.error(""Test caught an exception: "", e);
        }
    }
}
```
x??

---",859,"Downsides to End-to-End Testing There are, unfortunately, many disadvantages to end-to-end testing. Flaky and Brittle Tests As test scope increases, so too do the number of moving parts. These moving ...",qwen2.5:latest,2025-11-02 10:13:41,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Consumer-Driven Tests to the Rescue,Free-for-All Test Environment,"---

#### Free-for-All Test Environment
Background context: The issue arises when all teams have unrestricted access to add tests without a clear understanding of the overall test suite health. This leads to an uncontrolled increase in test cases, potentially causing the ""test snow cone"" phenomenon where the number and complexity of tests overwhelm the system.

:p What are the key issues with having an uncontrolled environment for adding end-to-end tests?
??x
The main issues include:
1. **Explosion of Test Cases**: Teams might add redundant or irrelevant tests without a cohesive strategy.
2. **Health Ignored**: Lack of clear ownership means that test results may be ignored, and when they break, teams assume it's someone else’s problem.
3. **Increased Cycle Times**: Developers wait for dedicated testing teams to write end-to-end tests, increasing development cycles.

??x",882,"I have seen a number of anti-patterns caused here. These tests become a free-for-all, with all teams granted access to add tests without any understanding of the health of the whole suite. This can of...",qwen2.5:latest,2025-11-02 10:14:06,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Consumer-Driven Tests to the Rescue,Dedicated Team Ownership vs. Joint Ownership,"#### Dedicated Team Ownership vs. Joint Ownership
Background context: A common organizational response is to centralize test writing in a dedicated team, but this can lead to decreased involvement from the original service developers, causing delays and reduced knowledge of the tests' specifics.

:p Why is joint ownership between teams and their respective test suites beneficial?
??x
Joint ownership ensures:
1. **Proximity**: Teams remain closely involved with the code they write, reducing distance and increasing accountability.
2. **Speed**: Faster response to failures as the developers can quickly diagnose issues without waiting for a separate team.
3. **Quality**: Higher quality tests as developers have more context on their service's functionality.

??x",767,"I have seen a number of anti-patterns caused here. These tests become a free-for-all, with all teams granted access to add tests without any understanding of the health of the whole suite. This can of...",qwen2.5:latest,2025-11-02 10:14:06,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Consumer-Driven Tests to the Rescue,Slowness of End-to-End Test Suites,"#### Slowness of End-to-End Test Suites
Background context: The issue arises when end-to-end test suites are slow to run, often due to lack of optimization and comprehensive coverage, leading to delays in development cycles and reduced developer engagement.

:p Why is the slowness of end-to-end tests a significant problem?
??x
The main problems include:
1. **Delayed Feedback**: Developers may take hours or even days to identify broken functionality.
2. **Context Switches**: Frequent interruptions as developers need to shift focus from coding to debugging test failures.

??x",580,"I have seen a number of anti-patterns caused here. These tests become a free-for-all, with all teams granted access to add tests without any understanding of the health of the whole suite. This can of...",qwen2.5:latest,2025-11-02 10:14:06,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Consumer-Driven Tests to the Rescue,Parallel Execution and Test Suite Optimization,"#### Parallel Execution and Test Suite Optimization
Background context: Running tests in parallel can mitigate the slowness issue, but it's not a substitute for actively optimizing the suite by removing redundant or flaky tests. Tools like Selenium Grid facilitate parallel execution.

:p How can we improve the performance of end-to-end test suites?
??x
Improvements include:
1. **Parallel Execution**: Using tools like Selenium Grid to run tests in parallel.
2. **Optimization**: Regularly reviewing and refining the suite to reduce redundancy and flakiness.
3. **Contextual Relevance**: Ensuring that every test is relevant and contributes meaningfully to coverage.

??x",673,"I have seen a number of anti-patterns caused here. These tests become a free-for-all, with all teams granted access to add tests without any understanding of the health of the whole suite. This can of...",qwen2.5:latest,2025-11-02 10:14:06,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Consumer-Driven Tests to the Rescue,Flakiness of End-to-End Tests,"#### Flakiness of End-to-End Tests
Background context: Flaky tests can significantly hamper development efficiency, as they may break for reasons unrelated to the actual code changes. This requires careful management and continuous optimization.

:p What challenges do flaky end-to-end tests present?
??x
Challenges include:
1. **False Positives/Negatives**: Tests that fail or pass intermittently due to external factors.
2. **Delayed Feedback**: Developers may not get accurate feedback on their code changes, leading to misdirection in debugging.

??x",554,"I have seen a number of anti-patterns caused here. These tests become a free-for-all, with all teams granted access to add tests without any understanding of the health of the whole suite. This can of...",qwen2.5:latest,2025-11-02 10:14:06,7
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Consumer-Driven Tests to the Rescue,Joint Ownership as the Best Balance,"#### Joint Ownership as the Best Balance
Background context: The ideal approach is a shared ownership model where multiple teams contribute to and are responsible for the end-to-end test suite. This balances centralization with decentralization, ensuring both accountability and flexibility.

:p How does joint ownership improve the management of end-to-end tests?
??x
Joint ownership improves:
1. **Shared Responsibility**: Multiple teams take responsibility, reducing the burden on any single team.
2. **Cohesive Strategy**: A shared understanding of test priorities and coverage ensures a more coherent suite.
3. **Developer Involvement**: Developers remain engaged with their code's tests, enhancing quality and responsiveness.

??x
---",740,"I have seen a number of anti-patterns caused here. These tests become a free-for-all, with all teams granted access to add tests without any understanding of the health of the whole suite. This can of...",qwen2.5:latest,2025-11-02 10:14:06,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Consumer-Driven Tests to the Rescue,Balancing Test Value and Burden,"#### Balancing Test Value and Burden
Background context: Discusses the trade-off between maintaining a large number of tests to ensure comprehensive coverage and the risk associated with removing or refactoring tests, which can lead to missed bugs.

:p How do you balance the value added by tests against the burden they entail?
??x
The question focuses on understanding how to manage the cost-benefit analysis in testing. Given that each test removed might prevent a bug from being caught but could also save time and effort, it's crucial to have a clear risk assessment framework.

In practice, identifying redundant or less valuable tests can improve efficiency without compromising too much on coverage. However, this is challenging due to the inherent difficulties humans have in assessing risk accurately.
x??",815,It is hard to have a balanced conversation about the value something adds versus the burden it entails. It can also be a difficult risk/reward trade-off. Do you get thanked if you remove a test? Maybe...,qwen2.5:latest,2025-11-02 10:14:29,7
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Consumer-Driven Tests to the Rescue,Long Feedback Cycles and Pile-Ups,"#### Long Feedback Cycles and Pile-Ups
Background context: Describes how long feedback cycles from end-to-end tests impact developer productivity and deployment processes, leading to pile-ups where multiple changes accumulate during test failures.

:p How does a long feedback cycle affect the deployment process?
??x
A long feedback cycle means that fixing issues in end-to-end tests takes significant time. This delays the availability of deployable services because only software that passes all tests can be deployed. Consequently, while waiting for tests to pass, other changes from upstream teams might pile up, increasing the complexity and risk of deployments.

For instance, if a seven-hour-long build is broken, developers cannot check in code until it's fixed, which can lead to delays in deploying new features.
x??",827,It is hard to have a balanced conversation about the value something adds versus the burden it entails. It can also be a difficult risk/reward trade-off. Do you get thanked if you remove a test? Maybe...,qwen2.5:latest,2025-11-02 10:14:29,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Consumer-Driven Tests to the Rescue,Metaversion and Microservices,"#### Metaversion and Microservices
Background context: Discusses the concept of ""metaversion"" where multiple services are versioned together, leading to coupled systems that lose the advantage of independent deployment.

:p What is the concept of metaversion?
??x
Metaversion refers to the practice of treating a system as a whole unit for versioning and releasing updates. This means changing and deploying all related microservices at once rather than independently.

The main issue with metaversion is that it can lead to services becoming tightly coupled, which was one of the key benefits of using microservices (i.e., being able to deploy changes in isolation).

For example:
```java
public class MicroserviceA {
    public void updateService() {
        // Code for updating Service A
    }
}

public class MicroserviceB {
    public void updateService() {
        // Code for updating Service B
    }
}
```
Metaversion would involve bundling these updates together, potentially leading to issues if one service fails during deployment.
x??

---",1052,It is hard to have a balanced conversation about the value something adds versus the burden it entails. It can also be a difficult risk/reward trade-off. Do you get thanked if you remove a test? Maybe...,qwen2.5:latest,2025-11-02 10:14:29,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Consumer-Driven Tests to the Rescue,End-to-End Test Journeys vs. Stories,"#### End-to-End Test Journeys vs. Stories
Background context: The passage discusses the challenges and limitations of end-to-end (E2E) tests when dealing with multiple services, highlighting the potential for test suite bloat and inefficiency.

:p What is a key issue with using E2E tests for multiple services?
??x
The key issue with using E2E tests for multiple services is that they can quickly become bloated, leading to an explosion in scenarios under test. This situation worsens if a new end-to-end test is added for every piece of functionality introduced, resulting in poor feedback cycles and overlapping test coverage.

Example: In a system with 10 services, adding one E2E test per service could result in 100 individual tests that might overlap significantly.
x??",776,"This is bad. Test Journeys, Not Stories Despite the disadvantages just outlined, for many users end-to-end tests can still be manageable with one or two services, and in these situations still make a ...",qwen2.5:latest,2025-11-02 10:14:48,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Consumer-Driven Tests to the Rescue,Core Journeys vs. Individual Functionalities,"#### Core Journeys vs. Individual Functionalities
Background context: The passage suggests focusing on core journeys rather than individual functionalities to manage the complexity of end-to-end tests.

:p How can you manage complex systems with end-to-end tests without making them bloated?
??x
By focusing on a small number of core journeys that cover high-value interactions, you can reduce the downsides of integration tests. These core journeys need to be mutually agreed upon and jointly owned by relevant teams. For example, in a music shop system, key journeys might include ordering a CD, returning a product, or creating a new customer.

Example: If each journey covers a critical interaction like ""order a CD,"" you can ensure that the core functionality is tested without overwhelming the test suite with individual functional tests.
x??",848,"This is bad. Test Journeys, Not Stories Despite the disadvantages just outlined, for many users end-to-end tests can still be manageable with one or two services, and in these situations still make a ...",qwen2.5:latest,2025-11-02 10:14:48,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Consumer-Driven Tests to the Rescue,Consumer-Driven Contracts (CDCs),"#### Consumer-Driven Contracts (CDCs)
Background context: The passage introduces consumer-driven contracts as an alternative to traditional end-to-end testing, focusing on ensuring service interoperability.

:p What is the purpose of using consumer-driven contracts in software development?
??x
The purpose of using consumer-driven contracts (CDCs) is to ensure that when a new service is deployed to production, it won’t break its consumers. CDCs define the expectations of consumers on a service and capture these expectations as tests, which are then run against the producer.

Example: For a customer service with two consumers—helpdesk and web shop—the tests would validate how each consumer uses the service.
x??",718,"This is bad. Test Journeys, Not Stories Despite the disadvantages just outlined, for many users end-to-end tests can still be manageable with one or two services, and in these situations still make a ...",qwen2.5:latest,2025-11-02 10:14:48,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Consumer-Driven Tests to the Rescue,Consumer-Driven Tests in Practice,"#### Consumer-Driven Tests in Practice
Background context: The passage explains how to implement and benefit from consumer-driven tests, focusing on their integration into CI/CD pipelines.

:p How are consumer-driven tests run against a single producer?
??x
Consumer-driven tests (CDCs) are run against the producer service by itself with any downstream dependencies stubbed out. This allows these tests to be isolated and faster compared to end-to-end tests. For example, in our customer service scenario, both the helpdesk’s and web shop’s expectations would be tested separately but independently of each other.

Example: 
```java
// Pseudocode for a CDC test
public void testCustomerServiceWithHelpdesk() {
    // Stub out all dependencies except those relevant to the helpdesk
    // Run tests that validate how the customer service behaves as seen by the helpdesk
}

public void testCustomerServiceWithWebShop() {
    // Stub out all dependencies except those relevant to the web shop
    // Run tests that validate how the customer service behaves as seen by the web shop
}
```
x??",1088,"This is bad. Test Journeys, Not Stories Despite the disadvantages just outlined, for many users end-to-end tests can still be manageable with one or two services, and in these situations still make a ...",qwen2.5:latest,2025-11-02 10:14:48,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Consumer-Driven Tests to the Rescue,Integration into Test Pyramid,"#### Integration into Test Pyramid
Background context: The passage describes where consumer-driven tests fit in the test pyramid and their unique focus.

:p How do consumer-driven tests sit in relation to other types of tests in a test pyramid?
??x
Consumer-driven tests (CDCs) sit at the same level as service-level tests in the test pyramid, but with a different focus. They are focused on validating how a consumer will use the service and can be run against the service independently of its downstream dependencies.

Example:
```java
// Pseudocode for placing CDCs in the test pyramid
public class ServiceTests {
    @Test
    public void testCustomerServiceWithHelpdesk() {
        // Test logic here
    }

    @Test
    public void testCustomerServiceWithWebShop() {
        // Test logic here
    }
}

// Higher-level integration tests would be located further up the pyramid, testing multiple services together.
```
x??

---",933,"This is bad. Test Journeys, Not Stories Despite the disadvantages just outlined, for many users end-to-end tests can still be manageable with one or two services, and in these situations still make a ...",qwen2.5:latest,2025-11-02 10:14:48,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Pact,Overview of Pact,"#### Overview of Pact
Pact is a consumer-driven testing tool that was originally developed for Ruby but now supports JVM and .NET. It allows consumers to define expectations about producer behavior, creating JSON-based pact files that are used to verify the producer's API. This approach ensures that both parties have a clear understanding of their interaction.
:p What does Pact enable in software development?
??x
Pact enables consumer-driven testing by allowing developers to define what they expect from an API and then verify these expectations against the actual implementation. This process helps ensure that changes in one component do not break the functionality expected by other components. The use of JSON specifications makes it language-agnostic, meaning different parts of a system can work with the same pact files.
??x",836,"Pact Pact  is a consumer-driven testing tool that was originally developed in-house at RealEstate.com.au, but is now open source, with Beth Skurrie driving most of the development. Originally just for...",qwen2.5:latest,2025-11-02 10:15:13,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Pact,Pact Workflow Overview,"#### Pact Workflow Overview
In the Pact workflow, the consumer starts by defining expectations using a Ruby DSL. These expectations are then tested against a local mock server to create a pact file. On the producer side, this JSON pact file is used to verify that the API behaves as expected. This process requires both the consumer and producer to be in different builds.
:p How does Pact work between the consumer and producer?
??x
Pact works by having the consumer define expectations using a Ruby DSL, which are then tested against a mock server to generate a pact file. On the producer side, this JSON pact file is used to drive API calls and verify responses. The key steps are:
1. Consumer defines expectations.
2. Mock server tests these expectations.
3. Pact file is created.
4. Producer uses the pact file to ensure compliance.
```ruby
# Example of defining a contract in Ruby using Pact DSL
contract = Pact.new('consumer', 'provider') do
  upon_receiving(""a request for data"") do
    with_stubs(path: '/data', method: :get) do |stubbed_request|
      will_respond_with(
        status: 200,
        body: { ""key"" => ""value"" },
        headers: { 'Content-Type' => 'application/json' }
      )
    end
  end
end

contract.provide_pact to_file: 'pact.json'
```
x??",1273,"Pact Pact  is a consumer-driven testing tool that was originally developed in-house at RealEstate.com.au, but is now open source, with Beth Skurrie driving most of the development. Originally just for...",qwen2.5:latest,2025-11-02 10:15:13,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Pact,Pact Specification File,"#### Pact Specification File
The pact file is a JSON specification that formalizes the interaction between consumer and producer. It can be hand-coded but using the language API like Pact’s Ruby DSL makes it easier, especially for testing purposes.
:p What is the purpose of a pact file in Pact?
??x
The purpose of a pact file in Pact is to document and validate the expected interactions between a consumer and a provider. This JSON-based specification ensures that both parties have clear expectations about API behavior, which can be verified at different stages of development.

Using the language API (like Pact’s Ruby DSL) makes it easier to generate these specifications dynamically during tests.
??x",707,"Pact Pact  is a consumer-driven testing tool that was originally developed in-house at RealEstate.com.au, but is now open source, with Beth Skurrie driving most of the development. Originally just for...",qwen2.5:latest,2025-11-02 10:15:13,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Pact,Using Pact for Multi-Language Projects,"#### Using Pact for Multi-Language Projects
Pact's ability to use a JSON-based pact file across multiple languages is particularly useful. For example, you can define expectations in Ruby but verify them against a Java implementation using the JVM port of Pact.
:p How does Pact support multi-language projects?
??x
Pact supports multi-language projects by allowing developers to create and validate API contracts using language-specific tools (like Pact’s Ruby DSL) while ensuring compatibility across different languages. The JSON-based pact files are portable, meaning they can be used with different programming languages without needing to rewrite the contract definitions.

For instance:
- Define expectations in a Ruby consumer.
- Use these expectations to generate a pact file.
- Verify this pact file against a Java producer using Pact’s JVM port.
```java
// Example of verifying a pact file in Java
PactBroker pactBroker = new PactBroker(""http://localhost:1234"");
Consumer consumer = new Consumer(""ExampleConsumer"", ""1.0"").hasPactsWith(pactBroker);
Provider provider = new Provider(""ExampleProvider"", ""1.0"");

consumer.given(""some condition"")
  .uponReceiving(""a request for data"")
  .withHeaders(Map.of(""Accept"", ""application/json""))
  .willRespondWith()
    .status(200)
    .body(""{\""key\"":\""value\""}"")
    .headers(Map.of(""Content-Type"", ""application/json""));

provider.verifyPact(consumer);
```
x??",1413,"Pact Pact  is a consumer-driven testing tool that was originally developed in-house at RealEstate.com.au, but is now open source, with Beth Skurrie driving most of the development. Originally just for...",qwen2.5:latest,2025-11-02 10:15:13,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Pact,Pacto vs. Pact,"#### Pacto vs. Pact
Pacto is another Ruby tool for consumer-driven testing that records interactions to generate expectations. While it simplifies creating initial contracts, these are static and not regenerated with each build as in Pact.
:p How does Pacto differ from Pact?
??x
Pacto differs from Pact primarily in its approach to contract generation and verification:
- **Static Contracts:** Once generated, the expectations in Pacto remain static. They do not change with new builds or versions of the consumer.
- **Dynamic Regeneration:** With Pact, the consumer regenerates the expectations in every build, ensuring that they reflect the latest requirements.

Pacto is useful for quickly setting up initial contracts by recording interactions, while Pact ensures that these contracts are dynamic and continuously validated against evolving requirements.
??x
---",867,"Pact Pact  is a consumer-driven testing tool that was originally developed in-house at RealEstate.com.au, but is now open source, with Beth Skurrie driving most of the development. Originally just for...",qwen2.5:latest,2025-11-02 10:15:13,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Separating Deployment from Release,Agile Stories and Conversations,"#### Agile Stories and Conversations
Agile stories are placeholders for conversations that define what a service API should look like. CDCs (Conversations Driven Contracts) codify these discussions and serve as triggers for evolving APIs when they break.
:p What is the purpose of agile stories?
??x
Agile stories act as placeholders for detailed discussions about what an API should do, ensuring that all stakeholders understand the requirements before development starts. They are a way to define user needs in a collaborative manner.
x??",540,"It’s About Conversations In agile, stories are often referred to as a placeholder for a conversation. CDCs are just like that. They become the codification of a set of discussions about what a service...",qwen2.5:latest,2025-11-02 10:15:37,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Separating Deployment from Release,CDCs and Communication,"#### CDCs and Communication
CDCs require good communication and trust between consumer and producing services. In intra-team or same-person scenarios, this is easier but in third-party or large-scale public API consumption, frequent communication and trust might be lacking.
:p What challenges can arise when implementing CDCs for third-party services?
??x
Implementing CDCs for third-party services can be challenging due to the lack of direct communication channels and mutual trust. This can lead to misalignments in API expectations and difficulties in maintaining consistent contract evolution.
x??",603,"It’s About Conversations In agile, stories are often referred to as a placeholder for a conversation. CDCs are just like that. They become the codification of a set of discussions about what a service...",qwen2.5:latest,2025-11-02 10:15:37,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Separating Deployment from Release,End-to-End Tests vs. CDCs,"#### End-to-End Tests vs. CDCs
While end-to-end tests are valuable, they have significant drawbacks as the number of moving parts increases. Many organizations prefer CDCs for intra-team or internal APIs but use end-to-end tests with semantic monitoring to catch issues before production.
:p Why might an organization choose not to use end-to-end tests exclusively?
??x
An organization may opt against exclusive reliance on end-to-end tests because they introduce overhead, are slow, and do not scale well with increasing complexity. CDCs and improved monitoring often provide better insights into API contract integrity without the downsides of comprehensive end-to-end testing.
x??",683,"It’s About Conversations In agile, stories are often referred to as a placeholder for a conversation. CDCs are just like that. They become the codification of a set of discussions about what a service...",qwen2.5:latest,2025-11-02 10:15:37,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Separating Deployment from Release,Semantic Monitoring,"#### Semantic Monitoring
Semantic monitoring involves using pre-existing end-to-end tests to monitor production systems after deployment. This approach helps catch issues in real-world scenarios that might not be covered by unit or integration tests.
:p What is semantic monitoring and how does it differ from traditional end-to-end testing?
??x
Semantic monitoring uses existing end-to-end tests to continuously monitor a live production system, providing insights into actual usage and performance. Unlike traditional end-to-end tests run before deployment, semantic monitoring focuses on the real-world usage patterns post-deployment.
x??",641,"It’s About Conversations In agile, stories are often referred to as a placeholder for a conversation. CDCs are just like that. They become the codification of a set of discussions about what a service...",qwen2.5:latest,2025-11-02 10:15:37,7
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Separating Deployment from Release,Blue/Green Deployments,"#### Blue/Green Deployments
Blue/green deployments involve deploying two versions of an application simultaneously, where one receives live traffic while the other is tested in situ. This technique helps detect and fix issues before they affect all users.
:p How does a blue/green deployment work?
??x
In a blue/green deployment, both production environments (blue and green) run identical code but receive different traffic. The new version (green) is tested thoroughly while live traffic continues to flow through the old version (blue). Once confirmed as stable, all traffic switches to the new version.
```java
public class BlueGreenDeployment {
    private Map<String, String> dnsEntries;
    private LoadBalancer lb;

    public void deployNewVersion(String newVersion) {
        // Update DNS entries and load balancer to direct traffic to blue environment
        updateTraffic(newVersion, ""blue"");

        // Perform testing on the green version
        testGreenEnvironment(newVersion);

        // Switch all traffic to the new green version if tests pass
        updateTraffic(newVersion, ""green"");
    }

    private void updateTraffic(String newVersion, String environment) {
        dnsEntries.put(environment, newVersion);
        lb.updateLoadBalancingConfiguration(dnsEntries);
    }
}
```
x??",1312,"It’s About Conversations In agile, stories are often referred to as a placeholder for a conversation. CDCs are just like that. They become the codification of a set of discussions about what a service...",qwen2.5:latest,2025-11-02 10:15:37,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Separating Deployment from Release,Canary Releasing,"#### Canary Releasing
Canary releasing is a deployment strategy where a small percentage of users are directed to the new version of an application. This approach helps detect issues without impacting all users and provides feedback for gradual rollouts.
:p What is canary releasing?
??x
Canary releasing involves deploying a new version of an application to a small, controlled group of users (typically 1% or less) before full-scale release. It allows teams to gather feedback and ensure the new version works as expected in a production-like environment.
x??

---",566,"It’s About Conversations In agile, stories are often referred to as a placeholder for a conversation. CDCs are just like that. They become the codification of a set of discussions about what a service...",qwen2.5:latest,2025-11-02 10:15:37,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Performance Tests,Canary Releasing Overview,"#### Canary Releasing Overview
Canary releasing is a deployment strategy where a new software version is gradually introduced to a small subset of production traffic. This allows for real-world performance verification and risk management before full-scale rollout. The goal is to ensure that the new version performs as expected, covering both functional and nonfunctional aspects.

The process involves deploying a baseline cluster representing the current production version alongside the new version. A controlled amount of live traffic is directed through the canary version, while monitoring various metrics such as response times, error rates, and business outcomes like sales conversion rate.

:p What are the key benefits and steps involved in canary releasing?
??x
The key benefits include real-world testing without affecting all users, allowing for gradual rollouts based on performance data. Steps involve deploying both versions, directing a controlled subset of traffic to the new version, monitoring metrics, and deciding whether to proceed with full rollout.

To implement this, you might use code like:
```java
public class TrafficRouter {
    private final Map<String, ServiceVersion> versions = new HashMap<>();
    
    public void routeTraffic(String userId) {
        if (isCanary(userId)) {
            versions.get(""newVersion"").handleRequest();
        } else {
            versions.get(""baseline"").handleRequest();
        }
    }

    boolean isCanary(String userId) {
        // Logic to determine canary traffic
    }
}
```
x??",1557,"Canary Releasing With canary releasing, we are verifying our newly deployed software by directing amounts of production traffic against the system to see if it performs as expected. “Performing as exp...",qwen2.5:latest,2025-11-02 10:16:09,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Performance Tests,Canary Releasing vs. Blue/Green Deployment,"#### Canary Releasing vs. Blue/Green Deployment
Canary releasing and blue/green deployment are both strategies for reducing the risk of software deployments, but they differ in their approach. In canary releasing, new versions are gradually introduced to a small subset of production traffic, allowing detailed monitoring and adjustments before full-scale rollout. This method allows for extended coexistence of different versions.

In contrast, blue/green deployment involves maintaining two identical environments (blue and green) and switching the live traffic between them. The environments are updated independently, but the entire environment switch happens quickly, minimizing downtime. Canary releasing typically requires more complex setup due to the need for sophisticated traffic routing and monitoring.

:p How do canary releasing and blue/green deployment differ in terms of their approach?
??x
Canary releasing involves gradually introducing new versions to a small subset of production traffic for detailed monitoring and adjustments before full rollout, allowing extended coexistence. Blue/green deployment maintains two identical environments and switches live traffic between them quickly, minimizing downtime but requiring a simpler setup.

Example code for canary deploying might look like:
```java
public class CanaryRouter {
    private final ServiceVersion baseline;
    private final ServiceVersion newVersion;

    public CanaryRouter(ServiceVersion baseline, ServiceVersion newVersion) {
        this.baseline = baseline;
        this.newVersion = newVersion;
    }

    public void routeTraffic(String userId) {
        if (userId.startsWith(""canary-"")) {
            newVersion.handleRequest();
        } else {
            baseline.handleRequest();
        }
    }
}
```
x??",1803,"Canary Releasing With canary releasing, we are verifying our newly deployed software by directing amounts of production traffic against the system to see if it performs as expected. “Performing as exp...",qwen2.5:latest,2025-11-02 10:16:09,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Performance Tests,Traffic Shadowing in Canary Releasing,"#### Traffic Shadowing in Canary Releasing
In canary releasing, traffic shadowing is a technique where production traffic is mirrored and directed to the canary version. This method ensures that both versions see identical requests but only the production results are exposed externally, reducing the risk of customer impact if something goes wrong during testing.

:p How does traffic shadowing work in the context of canary releasing?
??x
Traffic shadowing involves mirroring production traffic and directing it to a canary version. Both versions see the same incoming requests, but only the production version's responses are seen by external users. This allows for detailed comparison without exposing potential issues.

Example code might involve:
```java
public class TrafficShredder {
    public void shadowTraffic(String userId) {
        if (userId.startsWith(""shadow-"")) {
            // Process as canary request
        } else {
            // Process normally and send to production version
        }
    }
}
```
x??",1029,"Canary Releasing With canary releasing, we are verifying our newly deployed software by directing amounts of production traffic against the system to see if it performs as expected. “Performing as exp...",qwen2.5:latest,2025-11-02 10:16:09,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Performance Tests,Coexistence of Different Service Versions,"#### Coexistence of Different Service Versions
In a canary release, different versions of services coexist for an extended period. This approach allows for thorough testing in the live environment before fully committing to the new version. However, it also means tying up more hardware resources and requiring sophisticated traffic routing mechanisms.

:p What are the implications of using canary releasing for longer-term service version coexistence?
??x
Using canary releasing for longer-term service version coexistence implies extended use of multiple versions in production. This approach helps in thorough testing but requires managing additional hardware, monitoring tools, and complex traffic routing to ensure smooth transitions between versions.

Example code for managing traffic might include:
```java
public class VersionManager {
    private final Map<String, ServiceVersion> activeVersions = new HashMap<>();

    public void addVersion(String version) {
        activeVersions.put(version, new ServiceVersion(version));
    }

    public void routeTraffic(String userId) {
        String bestMatch = findBestMatchingVersion(userId);
        if (bestMatch != null) {
            activeVersions.get(bestMatch).handleRequest();
        } else {
            // Default to a fallback version
        }
    }

    private String findBestMatchingVersion(String userId) {
        // Logic to determine the most suitable version based on user or request characteristics
    }
}
```
x??",1494,"Canary Releasing With canary releasing, we are verifying our newly deployed software by directing amounts of production traffic against the system to see if it performs as expected. “Performing as exp...",qwen2.5:latest,2025-11-02 10:16:09,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Performance Tests,Benefits and Considerations of Canary Releasing,"#### Benefits and Considerations of Canary Releasing
Canary releasing provides real-world testing with minimal risk, allowing for detailed monitoring before full-scale deployment. It requires more complex setup and planning compared to other methods like blue/green deployments but offers greater flexibility and insight into the new version's performance.

:p What are some benefits and considerations when implementing canary releasing?
??x
Benefits of canary releasing include real-world testing with minimal risk, detailed monitoring before full-scale deployment, and greater flexibility in assessing the new version. Considerations involve more complex setup and planning, extended coexistence of different versions, and higher hardware resource usage.

Example code for traffic routing might be:
```java
public class CanaryRoutingService {
    private final Map<String, ServiceVersion> activeVersions = new HashMap<>();

    public void addVersion(String version) {
        activeVersions.put(version, new ServiceVersion(version));
    }

    public void routeTraffic(Map<String, String> headers) {
        // Logic to determine the best version based on header information
        if (headers.containsKey(""canary"")) {
            activeVersions.get(headers.get(""canary"")).handleRequest();
        } else {
            // Default to a fallback version
        }
    }
}
```
x??

---",1388,"Canary Releasing With canary releasing, we are verifying our newly deployed software by directing amounts of production traffic against the system to see if it performs as expected. “Performing as exp...",qwen2.5:latest,2025-11-02 10:16:09,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Performance Tests,Mean Time Between Failures (MTBF) and Mean Time to Repair (MTTR),"---
#### Mean Time Between Failures (MTBF) and Mean Time to Repair (MTTR)
Background context: In web operations, there is a trade-off between optimizing for MTBF and MTTR. MTBF measures the average time between failures of a system, while MTTR measures the average time it takes to recover from those failures.
If we can spot issues early in production and roll back quickly, we reduce the impact on our customers. Blue/green deployments are one such technique that helps achieve this by deploying new versions without downtime and testing them before directing all users.
:p What is the trade-off between MTBF and MTTR in web operations?
??x
The trade-off involves balancing the average time a system operates without failure (MTBF) against the average time required to repair a failure once it occurs (MTTR). The goal is often to reduce both, but resources might be limited. For example, a fast rollback coupled with good monitoring can help minimize MTTR and thus reduce the impact on customers.
??x",1002,"In the web operations world, this is often referred to as the trade-off between optimizing for  mean time between failures (MTBF)  and  mean time to repair (MTTR) . Techniques to reduce the time to re...",qwen2.5:latest,2025-11-02 10:16:30,7
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Performance Tests,Blue/Green Deployment,"#### Blue/Green Deployment
Background context: A blue/green deployment involves deploying a new version of software in an environment that is identical to production. Once testing is complete, traffic is directed from the old (blue) environment to the new (green) one. This approach ensures no downtime and allows for quick rollbacks if necessary.
:p What is a blue/green deployment?
??x
A blue/green deployment involves deploying a new version of software in an environment identical to production, ensuring that traffic can be switched between environments without downtime. If issues arise, you can quickly switch back to the old (blue) environment.
??x",656,"In the web operations world, this is often referred to as the trade-off between optimizing for  mean time between failures (MTBF)  and  mean time to repair (MTTR) . Techniques to reduce the time to re...",qwen2.5:latest,2025-11-02 10:16:30,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Performance Tests,Nonfunctional Requirements (NFRs),"#### Nonfunctional Requirements (NFRs)
Background context: NFRs cover characteristics like acceptable latency, user support capacity, accessibility for people with disabilities, and data security. These are often more complex than functional requirements and typically need testing in a production-like environment rather than just unit tests.
:p What are nonfunctional requirements (NFRs)?
??x
Nonfunctional requirements include aspects such as performance, scalability, usability, and security of the system. They describe how the system behaves under certain conditions but cannot be implemented like normal features. Examples include acceptable latency for web pages, user support capacity, accessibility for people with disabilities, and data security.
??x",761,"In the web operations world, this is often referred to as the trade-off between optimizing for  mean time between failures (MTBF)  and  mean time to repair (MTTR) . Techniques to reduce the time to re...",qwen2.5:latest,2025-11-02 10:16:30,7
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Performance Tests,Cross-Functional Requirements (CFRs),"#### Cross-Functional Requirements (CFRs)
Background context: CFRs are a term preferred over NFRs by some, emphasizing that these requirements emerge from cross-cutting work across multiple aspects of the system. They often require testing in production to ensure they are met.
:p What is cross-functional requirement (CFR)?
??x
Cross-functional requirements are a term used to describe nonfunctional requirements that result from cross-cutting work and can only be fully tested in a production-like environment. This term highlights that these system behaviors emerge from multiple aspects of the development process rather than being simple functional features.
??x
---",671,"In the web operations world, this is often referred to as the trade-off between optimizing for  mean time between failures (MTBF)  and  mean time to repair (MTTR) . Techniques to reduce the time to re...",qwen2.5:latest,2025-11-02 10:16:30,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Performance Tests,Trade-offs Based on Service Durability Requirements,"#### Trade-offs Based on Service Durability Requirements
Background context: When designing and evolving a microservice-based system, it is important to consider different service level requirements for various services. This allows for more tailored design decisions where some services might require higher durability while others can tolerate more downtime without negatively impacting the core business.
:p What are trade-offs based on service durability requirements in a microservices architecture?
??x
Trade-offs involve balancing the reliability and availability of services within a system. For instance, you may decide that the payment service must have very high uptime due to its critical nature for your business operations, while the music recommendation service can tolerate some downtime as it does not significantly impact the core functionality.
For example:
- Payment Service: Require 99.9% availability
- Music Recommendation Service: Can handle up to 5 minutes of downtime

This decision impacts how tests are structured and what performance levels are expected from each service.
x??",1105,"For some CFRs, you may want to track them at an individual service level. For example, you may decide that the durability of service you require from your payment service is significantly higher, but ...",qwen2.5:latest,2025-11-02 10:16:53,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Performance Tests,Fine-grained Nature of Microservices in Designing Trade-offs,"#### Fine-grained Nature of Microservices in Designing Trade-offs
Background context: The fine-grained nature of microservices allows for detailed trade-offs between different services. Each service can be optimized based on its specific requirements, leading to a more efficient and scalable system design.
:p How does the fine-grained nature of microservices impact designing trade-offs?
??x
The fine-grained nature of microservices enables designers to make nuanced decisions about each individual service’s performance and reliability needs. For example:
- Payment Service: High durability with frequent tests to ensure robustness
- Music Recommendation Service: Lower durability but more relaxed testing intervals

This approach ensures that critical services are highly reliable while allowing non-critical ones to be designed for lower maintenance overhead.
x??",868,"For some CFRs, you may want to track them at an individual service level. For example, you may decide that the durability of service you require from your payment service is significantly higher, but ...",qwen2.5:latest,2025-11-02 10:16:53,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Performance Tests,Test Pyramid for Cross-functional Requirements (CFRs),"#### Test Pyramid for Cross-functional Requirements (CFRs)
Background context: The test pyramid concept helps in structuring the types of tests needed to ensure a robust system. For CFRs, there should be a mix of end-to-end and unit tests. End-to-end tests are crucial but can be complemented by smaller-scoped tests that target specific issues.
:p How does the test pyramid apply to cross-functional requirements (CFRs)?
??x
The test pyramid suggests a hierarchical structure for testing where:
- Unit tests form the base, covering small units of functionality
- Integration tests are in the middle, ensuring interactions between components
- End-to-end tests are at the top, simulating complete user journeys

For CFRs, you might start with end-to-end load tests and then write smaller-scoped tests to catch specific issues. For example:
```python
# Example of a small-scoped test in Python
def test_performance_bottleneck():
    # Simulate user actions
    for i in range(100):
        simulate_user_action()
        assert response_time < 500  # Check performance against a threshold

```
x??",1096,"For some CFRs, you may want to track them at an individual service level. For example, you may decide that the durability of service you require from your payment service is significantly higher, but ...",qwen2.5:latest,2025-11-02 10:16:53,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Performance Tests,Performance Tests and Network Boundaries,"#### Performance Tests and Network Boundaries
Background context: As microservices increase network boundaries, performance testing becomes more critical. It is important to track down sources of latency in call chains with multiple synchronous calls.
:p Why are performance tests crucial in microservices?
??x
Performance tests are essential because they help identify bottlenecks in a microservice-based architecture where multiple network calls can significantly impact overall system speed. For instance, if an operation previously involved one database call but now involves several across network boundaries, the total latency increases.
To address this:
```java
// Example performance test in Java using JMeter or a similar tool
public class PerformanceTest {
    @Test
    public void testMultipleServiceCalls() throws InterruptedException {
        int numUsers = 100;
        for (int i = 0; i < numUsers; i++) {
            // Simulate user interaction
            performUserInteraction();
            Thread.sleep(50); // Simulate time between requests
        }
    }

    private void performUserInteraction() {
        // Simulate calls to services and database
    }
}
```
x??",1193,"For some CFRs, you may want to track them at an individual service level. For example, you may decide that the durability of service you require from your payment service is significantly higher, but ...",qwen2.5:latest,2025-11-02 10:16:53,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Performance Tests,Early Consideration of Cross-functional Requirements (CFRs),"#### Early Consideration of Cross-functional Requirements (CFRs)
Background context: It is crucial to identify and review cross-functional requirements early in the development process. This ensures that all relevant stakeholders are aligned on expectations.
:p Why should cross-functional requirements be considered early?
??x
Early consideration of cross-functional requirements helps ensure alignment among teams, reducing the risk of miscommunication and ensuring that all aspects of the system meet business needs. For example:
- Accessibility features for HTML markup can be identified early to integrate them into development cycles.
By reviewing CFRs regularly, you can make informed decisions about trade-offs and design choices.

```python
# Example of a quick check in Python
def check_accessibility_features(html):
    # Quick check function
    errors = []
    if not html_has_correct_a11y_attributes(html):
        errors.append(""Missing accessibility attributes"")
    return errors

def html_has_correct_a11y_attributes(html):
    # Check for correct ARIA labels, roles, etc.
    pass
```
x??",1107,"For some CFRs, you may want to track them at an individual service level. For example, you may decide that the durability of service you require from your payment service is significantly higher, but ...",qwen2.5:latest,2025-11-02 10:16:53,7
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Performance Tests,Importance of Regular Performance Testing,"#### Importance of Regular Performance Testing
Background context: The text emphasizes the importance of regularly performing performance tests to ensure that your system's behavior under load closely mimics production conditions. This helps in identifying potential bottlenecks and ensuring that the results are indicative of what you can expect on live systems.

:p Why is it important to perform regular performance testing?
??x
Regularly conducting performance tests allows you to assess how your application behaves with increasing load, helping identify and mitigate potential bottlenecks before they impact production environments. This practice ensures that the test environment closely mirrors the production setup, providing more accurate insights into system performance.

Code examples are not typically used here as this is a conceptual topic.
x??",860,"This allows you to see how latency of calls varies with increasing load. This means that performance tests can take a while to run. In addition, you’ll want the system to match production as closely a...",qwen2.5:latest,2025-11-02 10:17:16,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Performance Tests,Challenges in Making Performance Environment Production-Like,"#### Challenges in Making Performance Environment Production-Like
Background context: The text highlights the challenges of making your performance testing environment as similar to the production environment as possible. These challenges include acquiring enough data and using comparable infrastructure, which can be resource-intensive.

:p What are some challenges in creating a production-like performance testing environment?
??x
Some key challenges include:
- Acquiring sufficient production-like data for realistic testing.
- Using hardware and software configurations that closely match the production environment.
- Ensuring that tests accurately reflect real-world scenarios and conditions.

Code examples are not typically used here as this is a conceptual topic.
x??",778,"This allows you to see how latency of calls varies with increasing load. This means that performance tests can take a while to run. In addition, you’ll want the system to match production as closely a...",qwen2.5:latest,2025-11-02 10:17:16,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Performance Tests,Frequency of Performance Testing,"#### Frequency of Performance Testing
Background context: The text suggests running performance tests regularly, but acknowledges that it may not be feasible to do so with every code commit. Instead, teams often run a subset daily and a larger set weekly.

:p How frequently should performance testing be conducted?
??x
Performance testing should be conducted as frequently as possible, ideally daily for a subset of changes and weekly for a broader range. This approach helps in identifying newly introduced issues early without overburdening the development process with too many tests per commit.
```java
// Example code to illustrate how one might set up performance test schedules
public class PerformanceTestScheduler {
    public void scheduleTests() {
        // Schedule daily small tests
        performDailyTests();
        
        // Schedule weekly larger tests
        performWeeklyTests();
    }
    
    private void performDailyTests() {
        // Code for performing smaller, more frequent tests
    }
    
    private void performWeeklyTests() {
        // Code for performing larger, less frequent but comprehensive tests
    }
}
```
x??",1159,"This allows you to see how latency of calls varies with increasing load. This means that performance tests can take a while to run. In addition, you’ll want the system to match production as closely a...",qwen2.5:latest,2025-11-02 10:17:16,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Performance Tests,Importance of Setting Performance Targets,"#### Importance of Setting Performance Targets
Background context: The text stresses the importance of setting performance targets to ensure that the test results can be used effectively. These targets help in determining when a build should pass or fail based on the performance metrics.

:p Why is it important to set performance targets for tests?
??x
Setting performance targets helps in defining what acceptable performance looks like, making it easier to determine whether your application meets these standards. This allows you to automate the test results into the CI/CD pipeline so that a failing build can be immediately addressed by the development team.

Code examples are not typically used here as this is a conceptual topic.
x??",743,"This allows you to see how latency of calls varies with increasing load. This means that performance tests can take a while to run. In addition, you’ll want the system to match production as closely a...",qwen2.5:latest,2025-11-02 10:17:16,7
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Performance Tests,Visualizing System Behavior,"#### Visualizing System Behavior
Background context: The text recommends using the same tools in performance testing environments for visualizing system behavior as those used in production. This ensures consistency and ease of comparison between test results and actual production data.

:p Why should you use the same visualization tools in both your performance testing environment and production?
??x
Using the same visualization tools in both your performance testing environment and production makes it easier to compare and contrast the system behavior, ensuring that any issues identified during testing can be quickly validated or ruled out in real-world conditions. This consistency helps in maintaining a unified approach to monitoring and troubleshooting.

Code examples are not typically used here as this is a conceptual topic.
x??

---",850,"This allows you to see how latency of calls varies with increasing load. This means that performance tests can take a while to run. In addition, you’ll want the system to match production as closely a...",qwen2.5:latest,2025-11-02 10:17:16,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Correlation IDs,Holistic Approach to Testing,"#### Holistic Approach to Testing
Background context: The passage outlines a comprehensive strategy for testing systems, emphasizing fast feedback loops and the separation of test types. It also introduces consumer-driven contracts as an alternative to end-to-end tests.

:p What is the primary focus when implementing a holistic approach to testing?
??x
The primary focus is on optimizing for fast feedback by separating different types of tests and utilizing consumer-driven contracts where applicable. This ensures quicker identification and resolution of issues before they impact production.
x??",600,"Summary Bringing this all together, what I have outlined here is a holistic approach to testing that hopefully gives you some general guidance on how to proceed when testing your own systems. To reite...",qwen2.5:latest,2025-11-02 10:17:36,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Correlation IDs,Consumer-Driven Contracts,"#### Consumer-Driven Contracts
Background context: The text suggests using consumer-driven contracts as a means to reduce the need for extensive end-to-end testing, thereby improving collaboration between teams.

:p How can consumer-driven contracts aid in team communication?
??x
Consumer-driven contracts provide focus points for conversations between teams by ensuring that each service's implementation is driven by its consumers' requirements. This helps in aligning development efforts and ensures that services are built with a clear understanding of their intended use.
x??",581,"Summary Bringing this all together, what I have outlined here is a holistic approach to testing that hopefully gives you some general guidance on how to proceed when testing your own systems. To reite...",qwen2.5:latest,2025-11-02 10:17:36,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Correlation IDs,Trade-Offs Between Testing Efforts,"#### Trade-Offs Between Testing Efforts
Background context: The passage mentions the importance of understanding the trade-offs between putting more effort into testing to detect issues faster versus minimizing downtime.

:p What factors should be considered when deciding how much effort to put into testing?
??x
Factors to consider include the speed at which issues can be detected, the impact on mean time between failures (MTBF) and mean time to recovery (MTTR). Balancing these factors helps in optimizing test efforts for better overall system reliability.
x??",566,"Summary Bringing this all together, what I have outlined here is a holistic approach to testing that hopefully gives you some general guidance on how to proceed when testing your own systems. To reite...",qwen2.5:latest,2025-11-02 10:17:36,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Correlation IDs,Monitoring Microservice Systems,"#### Monitoring Microservice Systems
Background context: The text discusses the challenges of monitoring microservice-based systems due to their complex nature compared to monolithic applications.

:p Why is monitoring a microservice-based system more challenging than that of a monolithic application?
??x
Monitoring a microservice-based system is more challenging because it involves multiple servers, log files, and potential network latency issues. Unlike monolithic applications where the source of problems can be easily identified (e.g., slow website = monolith), microservices require a more intricate approach to pinpoint issues.
x??",642,"Summary Bringing this all together, what I have outlined here is a holistic approach to testing that hopefully gives you some general guidance on how to proceed when testing your own systems. To reite...",qwen2.5:latest,2025-11-02 10:17:36,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Correlation IDs,Example Monitoring Scenario,"#### Example Monitoring Scenario
Background context: The passage uses an example of a Friday afternoon system failure to illustrate the complexity of monitoring fine-grained systems.

:p What does the example scenario highlight about the challenges of monitoring microservice-based systems?
??x
The example highlights that in monolithic applications, it is straightforward to identify what has gone wrong. However, with microservices, issues can originate from various services and their interactions, making diagnosis more complex. This underscores the need for robust monitoring strategies.
x??

---",601,"Summary Bringing this all together, what I have outlined here is a holistic approach to testing that hopefully gives you some general guidance on how to proceed when testing your own systems. To reite...",qwen2.5:latest,2025-11-02 10:17:36,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Correlation IDs,Monitoring a Single Node,"---
#### Monitoring a Single Node
Background context: We start by monitoring a single service running on a single host. The goal is to detect issues early so they can be addressed promptly.

:p What metrics should you monitor for a single node?

??x
For a single node, we need to monitor the following:
- CPU usage: To ensure it does not exceed the threshold and cause performance degradation.
- Memory usage: To check if memory is being used efficiently without running out of space.
- Log files: To capture any errors or warnings that might indicate an issue.

The thresholds for these metrics can be set based on historical data. For example, you may want to alert when CPU usage exceeds 80% and memory usage goes over 90%.

```java
public class NodeMonitor {
    public void checkCPUUsage(double cpuUsage) {
        if (cpuUsage > 80) {
            // Alert system
        }
    }

    public void checkMemoryUsage(double memoryUsage) {
        if (memoryUsage > 90) {
            // Alert system
        }
    }
}
```
x??",1026,"The answer here is pretty straightforward: monitor the small things, and use aggregation to see the bigger picture. To see how, we’ll start with the simplest system we can: a single node. Single Servi...",qwen2.5:latest,2025-11-02 10:17:58,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Correlation IDs,Scaling to Multiple Servers,"#### Scaling to Multiple Servers
Background context: As loads increase, we may need to scale our service to multiple servers. This introduces the complexity of monitoring across different hosts.

:p How do you monitor a service running on multiple servers?

??x
When scaling to multiple servers, you need to:
- Monitor host-level metrics (CPU, memory) for all instances.
- Aggregate these metrics so you can see trends and compare them across all servers.
- Isolate the problem by checking if issues are present on all servers or specific ones.

You can use Nagios to group hosts and set up alerts. For application monitoring, you can track response times and error rates.

```java
public class MultiServerMonitor {
    public void checkHostMetrics(List<Double> cpuUsageList, List<Double> memoryUsageList) {
        double avgCPU = calculateAverage(cpuUsageList);
        double avgMemory = calculateAverage(memoryUsageList);

        if (avgCPU > 80 || avgMemory > 90) {
            // Alert system
        }
    }

    private double calculateAverage(List<Double> usageList) {
        return usageList.stream().mapToDouble(Double::doubleValue).average().orElse(0.0);
    }
}
```
x??",1184,"The answer here is pretty straightforward: monitor the small things, and use aggregation to see the bigger picture. To see how, we’ll start with the simplest system we can: a single node. Single Servi...",qwen2.5:latest,2025-11-02 10:17:58,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Correlation IDs,Log Management and Analysis,"#### Log Management and Analysis
Background context: Logs are crucial for diagnosing issues, especially when they are spread across multiple servers.

:p How do you manage logs for a service running on multiple hosts?

??x
For managing logs across multiple hosts:
- Use tools like `ssh-multiplexers` to run commands on all hosts simultaneously.
- Implement log rotation to avoid filling up disk space with old logs.
- Search through logs using tools that can aggregate and filter results, such as `grep`.

Example command:
```bash
ssh -t user@host1 ""cat /var/log/app.log | grep 'Error'""
```

You might also want to set up centralized logging systems like ELK Stack or Graylog.

x??",681,"The answer here is pretty straightforward: monitor the small things, and use aggregation to see the bigger picture. To see how, we’ll start with the simplest system we can: a single node. Single Servi...",qwen2.5:latest,2025-11-02 10:17:58,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Correlation IDs,Load Balancer Considerations,"#### Load Balancer Considerations
Background context: With a load balancer distributing requests, monitoring becomes more complex as issues may appear on different servers.

:p What additional steps are needed when using a load balancer for monitoring?

??x
When using a load balancer:
- Monitor both the host-level metrics and application response times.
- Aggregate metrics across all nodes to detect global trends.
- Isolate local issues by examining individual node logs and metrics separately from aggregated data.

```java
public class LoadBalancerMonitor {
    public void checkApplicationResponseTimes(List<Double> responseTimes) {
        double avgResponseTime = calculateAverage(responseTimes);

        if (avgResponseTime > 200) { // Threshold in milliseconds
            // Alert system
        }
    }

    private double calculateAverage(List<Double> times) {
        return times.stream().mapToDouble(Double::doubleValue).average().orElse(0.0);
    }
}
```
x??

---",982,"The answer here is pretty straightforward: monitor the small things, and use aggregation to see the bigger picture. To see how, we’ll start with the simplest system we can: a single node. Single Servi...",qwen2.5:latest,2025-11-02 10:17:58,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Correlation IDs,Load Balancer Aggregation and Health Monitoring,"#### Load Balancer Aggregation and Health Monitoring
Background context: For tasks like response time tracking, we can gather some aggregation for free by monitoring at the load balancer itself. However, it's crucial to monitor the load balancer as well since its misbehavior can cause issues. A healthy service configuration helps in removing unhealthy nodes from our application.
:p How do you ensure that your load balancer is functioning correctly and how does this affect your service health?
??x
To ensure the load balancer functions correctly, continuous monitoring of response times, error rates, and availability should be implemented. When a node becomes unhealthy, the load balancer configuration typically removes it from active use to prevent further issues.
```java
// Example pseudo-code for removing an unhealthy node in a load balancer setup
public void removeUnhealthyNode(String nodeID) {
    if (isNodeHealthy(nodeID)) {
        return; // Node is healthy, do nothing
    }
    // Logic to update the configuration and remove the node from active use
}
```
x??",1080,"For tasks like response time tracking, we can get some of the aggregation for free by tracking at the load balancer itself. But we need to track the load balancer as well, of course; if that misbehave...",qwen2.5:latest,2025-11-02 10:18:16,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Correlation IDs,Central Log Aggregation Using Logstash,"#### Central Log Aggregation Using Logstash
Background context: As the number of hosts increases, manual log retrieval becomes impractical. Specialized systems like Logstash can parse logs in various formats and forward them for centralized analysis using tools like Kibana.
:p How does Logstash help in managing logs across multiple hosts?
??x
Logstash centralizes logging by parsing different types of logs and forwarding them to a central repository such as Elasticsearch, making it easier to analyze logs from multiple sources.

```java
// Example pseudo-code for configuring Logstash pipeline
public void configureLogstashPipeline(String inputPath, String outputPath) {
    logstash.addInput(inputPath); // Define where to get the logs from
    logstash.addFilter(""grok"", ""{\""match\"":{\""message\"":\""%{SYSLOG}`"", ""target\"":\""message""});
    logstash.addOutput(outputPath); // Define where to send processed logs
}
```
x??",925,"For tasks like response time tracking, we can get some of the aggregation for free by tracking at the load balancer itself. But we need to track the load balancer as well, of course; if that misbehave...",qwen2.5:latest,2025-11-02 10:18:16,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Correlation IDs,Metric Tracking Across Services,"#### Metric Tracking Across Services
Background context: In complex environments, understanding system behavior requires collecting and analyzing metrics from multiple services. Metrics can help in identifying issues such as sudden increases in CPU load or error rates.
:p Why is it important to gather metrics for a more complex system?
??x
Gathering metrics is crucial because it helps in identifying patterns that indicate potential issues. For example, an increase in HTTP 4XX errors per second could signal problems with client requests, while rising CPU load on the catalog service might indicate performance degradation or resource contention.
```java
// Example pseudo-code for metric collection and aggregation
public void collectAndAggregateMetrics(String serviceName) {
    metricsCollector.addMetric(serviceName, ""cpuLoad"", getCurrentCpuLoad());
    metricsCollector.addMetric(serviceName, ""httpErrors"", getHttpErrorCount());
    // Logic to aggregate metrics across multiple instances of the same service or the entire system
}
```
x??

---",1053,"For tasks like response time tracking, we can get some of the aggregation for free by tracking at the load balancer itself. But we need to track the load balancer as well, of course; if that misbehave...",qwen2.5:latest,2025-11-02 10:18:16,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Correlation IDs,Graphite Overview,"---
#### Graphite Overview
Graphite is a system that simplifies the process of sending and querying metrics. It provides a simple API for real-time metric submission, storage optimization through resolution reduction over time, and flexible querying capabilities.

:p What is Graphite used for?
??x
Graphite is used to collect, store, and visualize time-series data such as metrics from various systems or services. It allows you to send metrics in real-time and query them to produce charts and other displays.
x??",515,Graphite is one such system that makes this very easy. It exposes a very simple API and allows you to send metrics in real time. It then allows you to query those metrics to produce charts and other d...,qwen2.5:latest,2025-11-02 10:18:33,7
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Correlation IDs,Metric Resolution Reduction,"#### Metric Resolution Reduction
To handle large volumes of data, Graphite reduces the resolution of older metrics by aggregating them over time. For example:
- Record CPU usage every 10 seconds for the last 10 minutes.
- Aggregate the samples into a single metric per minute for the next day.
- Further reduce to one sample every 30 minutes for several years.

:p How does Graphite handle data volume?
??x
Graphite handles large volumes of data by reducing the resolution of older metrics. This means it aggregates the data over time, ensuring that storage requirements remain manageable while still retaining historical trends. For instance, you might record CPU usage every 10 seconds for recent times, then aggregate these into one-minute intervals for a day's worth of data, and finally reduce to one sample every 30 minutes for longer-term data.
x??",855,Graphite is one such system that makes this very easy. It exposes a very simple API and allows you to send metrics in real time. It then allows you to query those metrics to produce charts and other d...,qwen2.5:latest,2025-11-02 10:18:33,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Correlation IDs,Aggregation Across Samples,"#### Aggregation Across Samples
Graphite allows aggregation across different samples. This means you can view metrics at various granularities: from the entire system, down to specific service groups or individual instances.

:p How does Graphite allow us to aggregate metrics?
??x
Graphite enables aggregation by allowing you to drill down into specific series while also viewing the overall aggregated data for your system. For example, you can see response times for your whole system, a group of services, or focus on a single instance. This flexibility is useful for understanding how different parts of your system contribute to performance.
x??",651,Graphite is one such system that makes this very easy. It exposes a very simple API and allows you to send metrics in real time. It then allows you to query those metrics to produce charts and other d...,qwen2.5:latest,2025-11-02 10:18:33,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Correlation IDs,Service Metrics,"#### Service Metrics
Operating systems and supporting subsystems like Nginx or Varnish provide useful metrics such as response times or cache hit rates. Additionally, it's recommended that custom services also expose their own metrics.

:p Why should services expose their own metrics?
??x
Services should expose their own metrics to gain deeper insights into the behavior of specific components. This is crucial for understanding usage patterns and identifying unused features. For example, an accounts service might want to track customer activity such as viewing past orders, while a web shop could monitor revenue generated in real-time.

Example code for exposing metrics:
```python
def get_customer_order_views():
    # Code to retrieve the number of times customers view their past orders
    return 12345

def get_webshop_revenue_today():
    # Code to calculate daily revenue from sales
    return 9876.54
```
x??

---",927,Graphite is one such system that makes this very easy. It exposes a very simple API and allows you to send metrics in real time. It then allows you to query those metrics to produce charts and other d...,qwen2.5:latest,2025-11-02 10:18:33,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Correlation IDs,User Behavior Metrics,"#### User Behavior Metrics
Background context explaining how metrics inform system improvements based on user behavior. Discusses the example of increased searches by genre on a catalog service and whether this is problematic or expected.
:p How can metrics help improve a system based on user behavior?
??x
Metrics can provide insights into how users are interacting with the system, allowing us to make informed decisions about where to focus improvements. For instance, if there's an increase in searches by genre on a catalog service, we might need to optimize search algorithms or database queries related to that genre to ensure performance remains satisfactory.
For example:
```java
// Pseudocode for tracking search trends
public class SearchMetrics {
    private Map<String, AtomicInteger> genreSearchCount;

    public void logGenreSearch(String genre) {
        int currentCount = genreSearchCount.getOrDefault(genre, new AtomicInteger(0)).getAndIncrement();
        if (currentCount > THRESHOLD) { // Define a threshold for significant changes
            notifyImprovementNeeded(genre);
        }
    }

    private void notifyImprovementNeeded(String genre) {
        System.out.println(""Genre search "" + genre + "" has exceeded expected levels. Investigate!"");
    }
}
```
x??",1290,"Second, we are getting better than ever at reacting to how our users are using our system to work out how to improve it. Metrics that inform us of how our systems behave can only help us here. We push...",qwen2.5:latest,2025-11-02 10:18:55,7
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Correlation IDs,Metrics Library: Codahale's Metrics,"#### Metrics Library: Codahale's Metrics
Background on the use of metrics libraries, specifically mentioning Codahale’s Metrics library for Java applications. Explain its functionality and how it supports various types of metrics.
:p What is Codahale's Metrics library used for?
??x
Codahale's Metrics library is a powerful tool for collecting and managing performance metrics in Java applications. It allows you to store metrics as counters, timers, or gauges; time-box metrics (e.g., ""number of orders in the last five minutes""); and supports sending data to Graphite and other aggregating and reporting systems.
For example:
```java
import com.yammer.metrics.Metrics;
import com.yammer.metrics.Timer;

public class MetricsExample {
    private Timer timer = Metrics.newTimer(Timer.class);

    public void performTask() {
        long start = System.currentTimeMillis();
        try {
            // Perform some task
        } finally {
            timer.update(System.currentTimeMillis() - start);
        }
    }
}
```
x??",1028,"Second, we are getting better than ever at reacting to how our users are using our system to work out how to improve it. Metrics that inform us of how our systems behave can only help us here. We push...",qwen2.5:latest,2025-11-02 10:18:55,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Correlation IDs,Synthetic Monitoring,"#### Synthetic Monitoring
Explanation of synthetic monitoring, which involves simulating user interactions to determine if a service is healthy. Discusses the limitations and benefits compared to traditional monitoring.
:p What does synthetic monitoring entail?
??x
Synthetic monitoring involves creating fake events or requests that mimic real user actions to check the health of services. This can help determine if a system is working as expected, even when real users' behavior is complex and difficult to predict.
For example:
```java
import org.apache.commons.exec.CommandLine;
import org.apache.commons.exec.DefaultExecuteResultHandler;

public class SyntheticMonitoring {
    public void runSyntheticChecks() throws Exception {
        CommandLine cmd = new CommandLine(""/path/to/nagios/job"");
        DefaultExecuteResultHandler resultHandler = new DefaultExecuteResultHandler();
        int exitValue = execCommand(cmd, resultHandler);
        if (exitValue != 0) {
            throw new RuntimeException(""Nagios job failed: "" + resultHandler.getExitCode());
        }
    }

    private int execCommand(CommandLine cmd, DefaultExecuteResultHandler handler) throws Exception {
        // Execute the command and wait for it to complete
        return cmd.execute(handler);
    }
}
```
x??",1298,"Second, we are getting better than ever at reacting to how our users are using our system to work out how to improve it. Metrics that inform us of how our systems behave can only help us here. We push...",qwen2.5:latest,2025-11-02 10:18:55,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Correlation IDs,Fake Event Generation for Monitoring,"#### Fake Event Generation for Monitoring
Explanation of generating fake events to simulate user interactions during system monitoring. Discusses an example from a project with tight deadlines where synthetic monitoring was used.
:p How were fake events generated in the described project?
??x
In the described project, fake events were generated using Nagios at regular intervals to price part of the portfolio that was not booked into downstream systems. This allowed the team to monitor the system's response time and performance under simulated user conditions without affecting actual user data.
For example:
```java
public class FakeEventGenerator {
    public void generateFakeEvents() {
        while (true) {
            try {
                Thread.sleep(60 * 1000); // Wait for a minute
                String event = ""fake_event""; // Simulate an event
                executeNagiosCommand(event);
            } catch (InterruptedException e) {
                Thread.currentThread().interrupt();
                break;
            }
        }
    }

    private void executeNagiosCommand(String event) {
        try {
            ProcessBuilder pb = new ProcessBuilder(""/path/to/nagios/job"", event);
            pb.start();
        } catch (IOException e) {
            e.printStackTrace();
        }
    }
}
```
x??

---",1333,"Second, we are getting better than ever at reacting to how our users are using our system to work out how to improve it. Metrics that inform us of how our systems behave can only help us here. We push...",qwen2.5:latest,2025-11-02 10:18:55,7
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Correlation IDs,Synthetic Transactions and Semantic Monitoring,"#### Synthetic Transactions and Semantic Monitoring
Background context: The passage discusses using synthetic transactions for semantic monitoring to ensure that systems behave as expected. This method differs from alerting on lower-level metrics, which provides more detailed diagnostics but can be noisy. 
:p What is a synthetic transaction used for in system monitoring?
??x
A synthetic transaction is used to simulate real-world events or scenarios within the system to verify that it behaves correctly under various conditions and configurations. This helps in identifying semantic issues rather than just technical failures.
x??",634,"Our system picked it up and ran all the various calculations just like any other job, except the results appeared in the  junk book, which was used only for testing. If a re-pricing wasn’t seen within...",qwen2.5:latest,2025-11-02 10:19:17,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Correlation IDs,Semantic Monitoring vs Alerting on Lower-Level Metrics,"#### Semantic Monitoring vs Alerting on Lower-Level Metrics
Background context: The text contrasts using synthetic transactions for semantic monitoring against relying solely on alerting on lower-level metrics, arguing that the former provides better overall indicators of system health because it focuses on end-to-end behavior. 
:p Why might semantic monitoring be preferred over just alerting on lower-level metrics?
??x
Semantic monitoring is preferred because it checks if the system behaves correctly from a user perspective (end-to-end), which can catch issues not detected by lower-level metrics that only monitor individual components or services. Lower-level alerts are useful for detailed diagnostics but can be overwhelming and less indicative of actual user experiences.
x??",787,"Our system picked it up and ran all the various calculations just like any other job, except the results appeared in the  junk book, which was used only for testing. If a re-pricing wasn’t seen within...",qwen2.5:latest,2025-11-02 10:19:17,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Correlation IDs,Implementing Semantic Monitoring,"#### Implementing Semantic Monitoring
Background context: The passage suggests leveraging existing end-to-end tests to implement semantic monitoring, as these tests already provide the necessary hooks to launch and check results. However, care must be taken regarding data requirements and potential side effects.
:p How can existing end-to-end tests be repurposed for semantic monitoring?
??x
Existing end-to-end tests can be repurposed by running a subset of them on an ongoing basis as part of the monitoring strategy. This leverages the system's built-in hooks to launch these tests and check results, providing a more holistic view of system behavior.
x??",660,"Our system picked it up and ran all the various calculations just like any other job, except the results appeared in the  junk book, which was used only for testing. If a re-pricing wasn’t seen within...",qwen2.5:latest,2025-11-02 10:19:17,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Correlation IDs,Data Requirements for Tests,"#### Data Requirements for Tests
Background context: When using end-to-end tests for semantic monitoring, it is crucial to manage data requirements effectively. Tests may need to adapt to changing live data or use different sources of data.
:p What challenges arise from managing data in semantic monitoring tests?
??x
Challenges include ensuring that test data remains consistent with real-world scenarios and adapting to changes in live data over time. This might require setting up a separate source of known, controlled data for testing purposes.
x??",554,"Our system picked it up and ran all the various calculations just like any other job, except the results appeared in the  junk book, which was used only for testing. If a re-pricing wasn’t seen within...",qwen2.5:latest,2025-11-02 10:19:17,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Correlation IDs,Correlation IDs and Call Tracing,"#### Correlation IDs and Call Tracing
Background context: The text highlights the importance of traceability when diagnosing issues across multiple services, especially with complex interactions where an error might not be immediately obvious in logs.
:p Why are correlation IDs important in system diagnostics?
??x
Correlation IDs are essential because they help track and correlate events across different services. By assigning a unique identifier to each initiating request and propagating it through downstream calls, you can trace the call chain and understand the flow of interactions leading to an error or issue.
x??",625,"Our system picked it up and ran all the various calculations just like any other job, except the results appeared in the  junk book, which was used only for testing. If a re-pricing wasn’t seen within...",qwen2.5:latest,2025-11-02 10:19:17,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Correlation IDs,Example Code for Correlation IDs,"#### Example Code for Correlation IDs
Background context: The passage mentions using correlation IDs to trace call chains, which is crucial for diagnosing issues in complex systems. 
:p How can a simple example be implemented in Java to illustrate passing correlation IDs through multiple service calls?
??x
Here's an example of how you might implement and pass a correlation ID across multiple services in Java:

```java
public class CorrelationIdUtil {
    private static final ThreadLocal<String> contextMap = new ThreadLocal<>();

    public static String getCorrelationId() {
        return contextMap.get();
    }

    public static void setCorrelationId(String id) {
        contextMap.set(id);
    }
}

public class CustomerRegistrationService {
    public void registerCustomer(Customer customer) {
        String correlationId = CorrelationIdUtil.getCorrelationId();

        // Check credit card details with payment service
        PaymentService.checkCreditCardDetails(customer.getCreditCardInfo(), correlationId);

        // Send welcome pack using postal service
        PostalService.sendWelcomePack(customer.getAddress(), correlationId);
        
        // Send welcome email using email service
        EmailService.sendWelcomeEmail(customer.getEmail(), correlationId);
    }
}
```

This example uses a `ThreadLocal` variable to pass the correlation ID through multiple services, ensuring that each call maintains traceability.
x??",1451,"Our system picked it up and ran all the various calculations just like any other job, except the results appeared in the  junk book, which was used only for testing. If a re-pricing wasn’t seen within...",qwen2.5:latest,2025-11-02 10:19:17,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Correlation IDs,Correlation IDs for Tracking Call Chains,"#### Correlation IDs for Tracking Call Chains

Correlation IDs are used to track events and calls across multiple services, especially useful in complex distributed systems where it's hard to trace the flow of an event or a request. They help in diagnosing issues by providing a unique identifier that can be traced through various logs.

Background context: In a system with multiple components, understanding the sequence and behavior of requests is crucial for debugging and performance analysis. Correlation IDs ensure that each service invocation can be uniquely identified and linked back to its origin.

:p What are correlation IDs used for in distributed systems?
??x
Correlation IDs are used to track events or calls across multiple services by assigning a unique identifier to each request, allowing the entire call chain to be traced through logs. This is particularly useful for diagnosing issues and understanding the flow of requests in complex systems.

```java
public class ServiceCaller {
    private String correlationId;

    public ServiceCaller(String correlationId) {
        this.correlationId = correlationId;
    }

    public void makeRequest() {
        // Make a request with the correlation ID
        System.out.println(""Request made with Correlation ID: "" + correlationId);
    }
}
```
x??",1320,"One approach that can be useful here is to use correlation IDs. When the first call is made, you generate a GUID for the call. This is then passed along to all subsequent calls, as seen in  Figure 8-5...",qwen2.5:latest,2025-11-02 10:19:47,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Correlation IDs,Enforcing the Use of Correlation IDs,"#### Enforcing the Use of Correlation IDs

Ensuring that each service knows to pass on the correlation ID is critical. Standardization and enforcement across the system are necessary to maintain a consistent traceability mechanism.

Background context: Once you have implemented correlation IDs, it's important to enforce their use consistently across all services to avoid losing track of request sequences. This can be challenging but essential for maintaining the integrity of call chains.

:p How do you ensure that each service passes on the correlation ID?
??x
To ensure that each service passes on the correlation ID, you need to standardize and enforce its usage across your system. This often involves creating a common library or tooling that handles the passing of the correlation ID in headers, query parameters, or other means.

For example, if using HTTP as the protocol, you can wrap a standard HTTP client library and add code to propagate the correlation IDs in the headers:

```java
public class HttpClientWrapper {
    private final OkHttpClient httpClient;
    private final String correlationId;

    public HttpClientWrapper(String correlationId) {
        this.correlationId = correlationId;
        this.httpClient = new OkHttpClient();
    }

    public Response makeRequest(String url, RequestBody requestBody) throws IOException {
        // Add the correlation ID to the request headers
        Headers headers = Headers.of(""Correlation-Id"", correlationId);
        Request request = new Request.Builder()
                .url(url)
                .headers(headers)
                .post(requestBody)
                .build();

        return httpClient.newCall(request).execute();
    }
}
```
x??",1725,"One approach that can be useful here is to use correlation IDs. When the first call is made, you generate a GUID for the call. This is then passed along to all subsequent calls, as seen in  Figure 8-5...",qwen2.5:latest,2025-11-02 10:19:47,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Correlation IDs,Log Aggregation and Visualization,"#### Log Aggregation and Visualization

Log aggregation tools can help in tracing events through the system, making it easier to diagnose issues. Tools like Zipkin provide detailed tracing of interservice calls.

Background context: With log aggregation tools, you can trace an event across multiple services and understand its behavior throughout your system. This is valuable for debugging and performance analysis. Tools like Zipkin offer a UI to visualize these traces.

:p What role do log aggregation tools play in tracking service interactions?
??x
Log aggregation tools help in tracing events through the system by collecting logs from various services and presenting them in a unified manner. They enable you to trace an event's journey across multiple services, making it easier to diagnose issues and understand the flow of requests.

For instance, Zipkin is a distributed tracing system that can be used to track interservice calls with detailed UIs for visualization:

```java
// Example of using Zipkin to trace service interactions
public class TraceService {
    private final Tracer tracer;

    public TraceService() {
        this.tracer = new SimpleTracer(new ZipkinSender()); // Assuming a Zipkin sender is configured
    }

    public void makeRequest(String serviceName) throws IOException {
        Span currentSpan = tracer.newChildBuilder()
                .withOperationName(""makeRequest"")
                .start();

        try (Scope scope = tracer.withSpan(currentSpan)) {
            // Simulate making a request to another service
            String response = callAnotherService(serviceName);
            System.out.println(response);

            // Finish the span when done
            currentSpan.finish();
        }
    }

    private String callAnotherService(String serviceName) throws IOException {
        // Code to make a request to another service, with tracing enabled
        return ""Response from "" + serviceName;
    }
}
```
x??",1977,"One approach that can be useful here is to use correlation IDs. When the first call is made, you generate a GUID for the call. This is then passed along to all subsequent calls, as seen in  Figure 8-5...",qwen2.5:latest,2025-11-02 10:19:47,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Correlation IDs,Using Zipkin for Tracing,"#### Using Zipkin for Tracing

Zipkin is a distributed tracing system that can trace interservice calls across multiple systems. It provides detailed visualization and is based on Google's Dapper.

Background context: Zipkin helps in tracking the flow of requests between services, providing a UI to visualize these traces. This is particularly useful when dealing with complex, distributed systems where request flows are not straightforward.

:p What is Zipkin used for?
??x
Zipkin is used for tracing interservice calls across multiple systems in complex, distributed architectures. It provides detailed visualization tools to help understand the flow of requests and diagnose issues that might be hard to pinpoint otherwise.

For example, using Zipkin involves setting up a sender (like `ZipkinSender`) and a tracer to collect spans representing each service call:

```java
public class TraceSetup {
    private final Tracer tracer;

    public TraceSetup() {
        this.tracer = new SimpleTracer(new ZipkinSender()); // Assuming a Zipkin sender is configured
    }

    public void startTrace(String serviceName) {
        Span currentSpan = tracer.newChildBuilder()
                .withOperationName(""startTrace"")
                .start();

        try (Scope scope = tracer.withSpan(currentSpan)) {
            System.out.println(""Starting trace for "" + serviceName);
            // Simulate making a request to another service
            callAnotherService(serviceName);

            currentSpan.finish();
        }
    }

    private String callAnotherService(String serviceName) {
        // Code to make a request to another service, with tracing enabled
        return ""Trace received by "" + serviceName;
    }
}
```
x??

---",1741,"One approach that can be useful here is to use correlation IDs. When the first call is made, you generate a GUID for the call. This is then passed along to all subsequent calls, as seen in  Figure 8-5...",qwen2.5:latest,2025-11-02 10:19:47,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Summary,Understanding Cascade Cascading Failures,"#### Understanding Cascade Cascading Failures
Background context explaining the scenario where network connections between services fail, even though individual services are healthy. This can lead to hidden issues that synthetic monitoring and visibility into integration points help uncover.

:p What is a cascade cascading failure, and why is it important to monitor service integrations?
??x
A cascade cascading failure occurs when there's an issue in the network connection between two or more services, making them unable to communicate despite their internal health. To detect such issues, monitoring individual service health alone may not suffice; synthetic monitoring can help identify problems by mimicking real user interactions (e.g., a customer searching for a song). Monitoring integration points and tracking downstream dependencies is crucial.

To illustrate this concept with an example:
```java
public class ServiceHealthChecker {
    public void checkServiceIntegration() {
        // Code to mimic a service interaction, e.g., database call or API request.
        try {
            // Simulate network call between services.
            String response = performNetworkCall();
            if (response == null || response.isEmpty()) {
                // Log failure
                System.out.println(""Failed to get valid response from downstream service."");
            }
        } catch (Exception e) {
            // Handle exceptions, possibly marking the integration as unhealthy.
            System.err.println(""Error in network call: "" + e.getMessage());
        }
    }

    private String performNetworkCall() {
        // Simulated method that performs a network request to another service.
        return null; // For demonstration purposes, it returns null.
    }
}
```
x??",1806,The Cascade Cascading failures can be especially perilous. Imagine a situation where the network connection between our music shop website and the catalog service goes down. The services themselves ap...,qwen2.5:latest,2025-11-02 10:20:18,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Summary,Importance of Standardization in Monitoring,"#### Importance of Standardization in Monitoring
Background context explaining the need for standardization across services to ensure consistency and ease of analysis. Highlighting issues like differing metric names even when they represent the same data point.

:p Why is standardization important in monitoring services?
??x
Standardization in monitoring helps maintain uniformity in how metrics are collected, logged, and reported across different services. This ensures that similar performance indicators have consistent naming conventions and formats, making it easier to compare data and draw meaningful insights. For instance, using the same metric name like ""ResponseTime"" or ""RspTimeSecs"" for measuring response times can lead to confusion when analyzing trends.

Example of non-standardized metrics:
```java
// Example showing non-standardized naming conventions.
public class ServiceMetrics {
    public int getResponseTime() { return 10; }
    public long getRspTimeSecs() { return 10L; } // Different naming but same functionality.
}
```
x??",1055,The Cascade Cascading failures can be especially perilous. Imagine a situation where the network connection between our music shop website and the catalog service goes down. The services themselves ap...,qwen2.5:latest,2025-11-02 10:20:18,7
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Summary,Monitoring Response Time and Error Rates,"#### Monitoring Response Time and Error Rates
Background context explaining the importance of tracking response times and error rates as basic metrics to understand service performance. Mentioning tools like Hystrix that can help manage these aspects effectively.

:p Why is monitoring response time and error rate essential in service health checks?
??x
Monitoring response time and error rates are fundamental for assessing the health and performance of services. These metrics provide insights into how quickly a service responds to requests and whether those responses contain errors, helping to pinpoint where potential bottlenecks or issues might be occurring.

Example using Hystrix for circuit breaker functionality:
```java
import com.netflix.hystrix.HystrixCommand;
import com.netflix.hystrix.HystrixCommandGroupKey;

public class ServiceHealthMonitor extends HystrixCommand<Void> {
    public static void main(String[] args) {
        // Create a command with the group name ""CatalogService"".
        new ServiceHealthMonitor(HystrixCommandGroupKey.Factory.asKey(""CatalogService""))
                .execute();
    }

    @Override
    protected Void run() throws Exception {
        // Simulate network request to another service.
        performNetworkRequest();
        return null;
    }

    private void performNetworkRequest() {
        try {
            Thread.sleep(100); // Simulating a delay.
        } catch (InterruptedException e) {
            throw new RuntimeException(e);
        }
        System.out.println(""Service response received."");
    }

    @Override
    protected Void getFallback() {
        return null; // Return a fallback if the command fails.
    }
}
```
x??",1703,The Cascade Cascading failures can be especially perilous. Imagine a situation where the network connection between our music shop website and the catalog service goes down. The services themselves ap...,qwen2.5:latest,2025-11-02 10:20:18,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Summary,Consideration of Audience and Data Use Cases,"#### Consideration of Audience and Data Use Cases
Background context explaining how different types of data need to be presented in ways that are relevant to their intended audience. Highlighting the differences between immediate alerts for support teams versus long-term trends for capacity planning.

:p How should data be tailored based on its audience?
??x
Data should be tailored differently depending on who will be using it and when they need it. Immediate alerts, such as failed synthetic monitoring tests, require quick action by the support team. Conversely, long-term trends like weekly CPU load increases are more relevant for capacity planning or performance optimization.

Example of alerting different audiences:
```java
import org.springframework.boot.actuate.metrics.CounterService;

public class AlertManager {
    private final CounterService counterService;

    public AlertManager(CounterService counterService) {
        this.counterService = counterService;
    }

    public void notifySupportTeam(String message) {
        // Send alert to support team immediately.
        sendImmediateAlert(message);
    }

    public void reportToCapacityPlanners() {
        // Collect and analyze data for long-term trends.
        String trendReport = generateTrendReport();
        storeAndArchive(trendReport);
    }

    private void sendImmediateAlert(String message) {
        // Code to send immediate alert via SMS, email, etc.
    }

    private String generateTrendReport() {
        return ""CPU usage increased by 2% over the last week."";
    }

    private void storeAndArchive(String report) {
        // Code to store and archive trend reports for future reference.
    }
}
```
x??",1710,The Cascade Cascading failures can be especially perilous. Imagine a situation where the network connection between our music shop website and the catalog service goes down. The services themselves ap...,qwen2.5:latest,2025-11-02 10:20:18,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Summary,Unification of Metrics Collection,"#### Unification of Metrics Collection
Background context explaining the shift towards unified event processing systems that can handle both business metrics (e.g., orders, revenue) and operational metrics (e.g., response times, errors). Highlighting tools like Riemann and Suro that facilitate this unification.

:p How is the future direction in metric collection moving?
??x
The future trend in metric collection is moving towards unified event processing systems that can handle both business and operational metrics. This shift aims to provide a more holistic view of system performance, enabling real-time analysis and quicker responses to issues. Tools like Riemann and Suro are designed to unify these events by aggregating them from various sources and routing them appropriately for analysis.

Example using Riemann to route events:
```java
import com.twitter.heron.api.spout.RiakSpout;

public class EventRouter {
    public void processEvent(String event) {
        // Route the event based on its type.
        RiakSpout riakSpout = new RiakSpout();
        riakSpout.emit(event);
    }
}
```
x??",1109,The Cascade Cascading failures can be especially perilous. Imagine a situation where the network connection between our music shop website and the catalog service goes down. The services themselves ap...,qwen2.5:latest,2025-11-02 10:20:18,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Summary,Summary of Monitoring Advice,"#### Summary of Monitoring Advice
Background context summarizing key points for effective monitoring, including tracking response times, error rates, and application-level metrics. Emphasizing the importance of standardization and aggregation.

:p What are some key advice for implementing effective monitoring?
??x
Key advice for effective monitoring includes:

- Track basic metrics like inbound response time at a minimum.
- Follow with error rates and then focus on application-level metrics.
- Monitor the health of all downstream responses, including response times and error rates.
- Use libraries like Hystrix to help manage network calls effectively.

For standardization:
- Collect logs into a standard location in a consistent format.
- Aggregate host-level metrics (e.g., CPU) with application-level metrics.
- Ensure your metric storage tool supports aggregation at various levels.
- Maintain data long enough for trend analysis.
- Use a single, queryable tool for aggregating and storing logs.
- Consider using correlation IDs to facilitate tracking.
- Understand what requires immediate action, structure alerting and dashboards accordingly.

Unified metrics collection tools like Riemann or Suro can simplify architecture by handling both business and operational data in an integrated manner.

x??

---",1319,The Cascade Cascading failures can be especially perilous. Imagine a situation where the network connection between our music shop website and the catalog service goes down. The services themselves ap...,qwen2.5:latest,2025-11-02 10:20:18,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Single Sign-On Gateway,Single Sign-On (SSO) Overview,"#### Single Sign-On (SSO) Overview
Background context: In the modern era of distributed systems, managing user authentication across different services can become a complex challenge. A single sign-on (SSO) solution is designed to address this issue by allowing users to authenticate once and access multiple applications or services without needing to re-authenticate.
:p What are some common Single Sign-On Implementations?
??x
Common SSO solutions include SAML and OpenID Connect. SAML is the standard for enterprise environments, while OpenID Connect has emerged as an implementation of OAuth 2.0 and offers a simpler REST-based approach.

SAML works by directing users to authenticate with an identity provider (IdP) before allowing access to service providers (SPs). OpenID Connect uses more straightforward HTTP requests.
??x",832,"Chapter 9.  Security We’ve become familiar with stories about security breaches of large-scale systems resulting in our data being exposed to all sorts of dodgy characters. But more recently, events l...",qwen2.5:latest,2025-11-02 10:20:42,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Single Sign-On Gateway,Authentication Process,"#### Authentication Process
Background context: The process of confirming that a user is who they claim to be is known as authentication. In traditional scenarios, this usually involves providing a username and password, but modern systems can use alternative methods like biometric data (fingerprint).
:p What is the difference between authentication and authorization?
??x
Authentication is about verifying identity, while authorization determines what actions a user is permitted to perform based on their identity.
??x",522,"Chapter 9.  Security We’ve become familiar with stories about security breaches of large-scale systems resulting in our data being exposed to all sorts of dodgy characters. But more recently, events l...",qwen2.5:latest,2025-11-02 10:20:42,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Single Sign-On Gateway,Authorization Mechanism,"#### Authorization Mechanism
Background context: Once authenticated, users are granted permissions to specific actions or resources. This mapping from an authenticated user (principal) to the allowed actions is known as authorization.
:p How does Django handle authentication and authorization?
??x
Django provides built-in mechanisms for managing users and handling authentication and authorization within single applications. However, in distributed systems, more complex SSO solutions like OpenID Connect or custom gateway implementations are required.
??x",559,"Chapter 9.  Security We’ve become familiar with stories about security breaches of large-scale systems resulting in our data being exposed to all sorts of dodgy characters. But more recently, events l...",qwen2.5:latest,2025-11-02 10:20:42,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Single Sign-On Gateway,Single Sign-On Gateway Implementation,"#### Single Sign-On Gateway Implementation
Background context: In a microservices architecture, using a shared gateway to handle all SSO interactions can centralize the process and reduce redundancy. This approach uses a proxy that handles authentication with an identity provider and then forwards authenticated users to their intended services.
:p What is Shibboleth in this context?
??x
Shibboleth is a tool used for implementing single sign-on solutions, particularly with SAML-based identity providers. It can be integrated with web servers like Apache to handle the redirections and authentications required for SSO.
??x",626,"Chapter 9.  Security We’ve become familiar with stories about security breaches of large-scale systems resulting in our data being exposed to all sorts of dodgy characters. But more recently, events l...",qwen2.5:latest,2025-11-02 10:20:42,7
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Single Sign-On Gateway,Security Considerations,"#### Security Considerations
Background context: Ensuring data security involves protecting it both in transit (using secure protocols) and at rest (using encryption). Additionally, securing underlying systems and networks is crucial. The human element, including user authentication and authorization, also plays a significant role.
:p What are the main security measures mentioned?
??x
The text mentions network perimeter protection, subnet isolation, firewalls, machine-level security, operating system security, and hardware security as layers of defense in depth. These can be implemented to secure data both in transit and at rest.
??x",641,"Chapter 9.  Security We’ve become familiar with stories about security breaches of large-scale systems resulting in our data being exposed to all sorts of dodgy characters. But more recently, events l...",qwen2.5:latest,2025-11-02 10:20:42,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Single Sign-On Gateway,Defense in Depth,"#### Defense in Depth
Background context: The concept of defense in depth involves implementing multiple layers of security measures to protect systems from various types of threats. This approach ensures that even if one layer fails, others can still provide protection.
:p How does a gateway contribute to the security strategy?
??x
A gateway can centralize SSO and authentication processes, reducing redundancy and improving manageability. However, it must be designed carefully to avoid becoming a single point of failure, which could compromise overall system security.
??x
---",582,"Chapter 9.  Security We’ve become familiar with stories about security breaches of large-scale systems resulting in our data being exposed to all sorts of dodgy characters. But more recently, events l...",qwen2.5:latest,2025-11-02 10:20:42,7
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,API Keys,Fine-Grained Authorization for Microservices,"#### Fine-Grained Authorization for Microservices
Fine-grained authorization goes beyond coarse-grained authentication by providing nuanced access control based on user attributes. For instance, a helpdesk application might allow only staff members with specific roles to view customer details or issue refunds, but limit the amount of refund in certain scenarios.
:p How does fine-grained authorization differ from coarse-grained authentication?
??x
Fine-grained authorization involves making more detailed decisions about what actions are allowed based on additional attributes extracted during authentication. Coarse-grained authentication, on the other hand, simply checks if a user is logged in and grants access to certain resources based on high-level roles like ""STAFF"" or ""ADMIN."" Fine-grained authorization allows for more precise control over operations within an application.
x??",891,"Fine-Grained Authorization A gateway may be able to provide fairly effective coarse-grained authentication. For example, it could prevent access to any non-logged-in user to the helpdesk application. ...",qwen2.5:latest,2025-11-02 10:21:07,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,API Keys,Group-Based Access Control,"#### Group-Based Access Control
In fine-grained authorization, groups or roles are often used to grant permissions. For example, a CALL_CENTER group might have limited access to view customer details but can issue refunds of up to $50. A CALL_CENTER_TEAM_LEADER would have similar permissions but could issue higher amounts.
:p How can group-based access control be implemented in an application?
??x
Group-based access control is typically implemented by associating users with roles or groups, and then defining permissions based on these roles. Here's a simplified example using Java:

```java
public class PermissionService {
    public boolean canViewCustomerDetails(User user) {
        return user.getRole().equals(""CALL_CENTER"") || user.getGroup().contains(""CALL_CENTER"");
    }

    public boolean canIssueRefund(User user, double amount) {
        if (user.getRole().equals(""CALL_CENTER"")) {
            return amount <= 50;
        } else if (user.getRole().equals(""CALL_CENTER_TEAM_LEADER"")) {
            return true; // No specific limit for team leaders
        }
        return false;
    }
}
```
x??",1116,"Fine-Grained Authorization A gateway may be able to provide fairly effective coarse-grained authentication. For example, it could prevent access to any non-logged-in user to the helpdesk application. ...",qwen2.5:latest,2025-11-02 10:21:07,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,API Keys,Service-to-Service Authentication and Authorization,"#### Service-to-Service Authentication and Authorization
Service-to-service authentication involves programs or services authenticating with each other, which is different from human-computer interactions. One common approach is to assume that all internal service calls are implicitly trusted.
:p What is the main difference between human-computer interaction and service-to-service communication in terms of authentication?
??x
In human-computer interaction, the focus is on verifying the identity of a user through mechanisms like login credentials or biometric data. However, in service-to-service communication, the entities involved are programs or services, which often communicate over network boundaries. The main difference lies in the level of trust and the need for explicit authentication between systems.
x??",822,"Fine-Grained Authorization A gateway may be able to provide fairly effective coarse-grained authentication. For example, it could prevent access to any non-logged-in user to the helpdesk application. ...",qwen2.5:latest,2025-11-02 10:21:07,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,API Keys,Allowing Everything Inside the Perimeter,"#### Allowing Everything Inside the Perimeter
An organization might decide to assume that all traffic originating from within its perimeter is trusted. This approach can be risky as it relies on securing the perimeter rather than protecting internal communications.
:p What are the potential risks of assuming all internal service calls are implicitly trusted?
??x
Assuming all internal service calls are implicitly trusted poses several risks:
1. **Man-in-the-Middle (MITM) Attacks**: An attacker who gains access to the network can intercept and manipulate data without being detected.
2. **Data Exposure**: Sensitive information might be exposed if not properly secured, even within the network perimeter.
3. **Lack of Fine-Grained Control**: Without proper authorization checks, services may inadvertently expose sensitive operations or data.

To mitigate these risks, it is crucial to implement robust authentication and authorization mechanisms at all points where services communicate, regardless of their location in the network.
x??

---",1046,"Fine-Grained Authorization A gateway may be able to provide fairly effective coarse-grained authentication. For example, it could prevent access to any non-logged-in user to the helpdesk application. ...",qwen2.5:latest,2025-11-02 10:21:07,7
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,API Keys,HTTP Basic Authentication,"#### HTTP Basic Authentication
Background context: This authentication method sends a username and password over HTTP headers, allowing the server to validate these credentials. While it is well-understood and supported, using it without HTTPS poses significant security risks as the credentials can be intercepted.

:p What are the main issues with using HTTP Basic Authentication?
??x
The main issues include:
- Lack of encryption: The username and password are sent in plain text over HTTP, making them vulnerable to interception by any intermediary.
- Security concerns: Without HTTPS, any party on the network path between client and server can access the credentials.

```java
// Example of HTTP Basic Authentication without HTTPS (not secure)
public class InsecureBasicAuth {
    public String getCredentials(String username, String password) {
        // Encode username and password in base64 format for header
        return ""Basic "" + Base64.getEncoder().encodeToString((username + "":"" + password).getBytes());
    }
}
```
x??",1037,"This is by far the most common form of inside-perimeter trust I see in organizations. They may decide to run this traffic over HTTPS, but they don’t do much else. I’m not saying that is a good thing. ...",qwen2.5:latest,2025-11-02 10:21:30,2
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,API Keys,HTTPS Considerations,"#### HTTPS Considerations
Background context: While HTTP Basic Authentication can be used over HTTPS, the use of HTTPS provides additional security benefits such as encrypting the communication and preventing eavesdropping.

:p What are the advantages of using HTTPS with HTTP Basic Authentication?
??x
The advantages include:
- Encryption: HTTPS ensures that the username and password are transmitted securely.
- Integrity: HTTPS prevents man-in-the-middle attacks by verifying that the server is who it claims to be.
- Data confidentiality: The payload cannot be tampered with or read by eavesdroppers.

```java
// Example of using HTTP Basic Authentication over HTTPS (secure)
public class SecureBasicAuth {
    public String getCredentials(String username, String password) throws IOException {
        // Use SSL/TLS for secure communication
        URL url = new URL(""https://example.com/api"");
        HttpsURLConnection connection = (HttpsURLConnection) url.openConnection();
        connection.setRequestProperty(""Authorization"", ""Basic "" + Base64.getEncoder().encodeToString((username + "":"" + password).getBytes()));
        // Proceed with request
    }
}
```
x??",1174,"This is by far the most common form of inside-perimeter trust I see in organizations. They may decide to run this traffic over HTTPS, but they don’t do much else. I’m not saying that is a good thing. ...",qwen2.5:latest,2025-11-02 10:21:30,7
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,API Keys,SSL Certificate Management Challenges,"#### SSL Certificate Management Challenges
Background context: Manually managing SSL certificates can be complex, especially when multiple machines need to use them. Organizations often have their own certificate issuing processes, which require additional administrative overhead.

:p What are the challenges in managing SSL certificates?
??x
The main challenges include:
- Administrative burden: Issuing and renewing certificates for multiple services.
- Self-signed certificates: They are not easily revokable and require careful planning to handle disaster scenarios.
- Lack of automated tools: Management processes can be cumbersome without mature automation tools.

```java
// Example of handling SSL certificates (not automatically managed)
public class CertificateManagement {
    public void manageCertificates(String host, String certPath) throws Exception {
        // Manually load and install certificate for HTTPS connection
        KeyStore keyStore = KeyStore.getInstance(""JKS"");
        FileInputStream keyIn = new FileInputStream(certPath);
        keyStore.load(keyIn, ""changeit"".toCharArray());
        HttpsURLConnection.setDefaultKeyManager(new DefaultKeyManager() {
            @Override
            public X509Certificate[] getAcceptedIssuers() { return null; }
        });
    }
}
```
x??",1313,"This is by far the most common form of inside-perimeter trust I see in organizations. They may decide to run this traffic over HTTPS, but they don’t do much else. I’m not saying that is a good thing. ...",qwen2.5:latest,2025-11-02 10:21:30,4
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,API Keys,Caching with SSL,"#### Caching with SSL
Background context: Using SSL can prevent caching by reverse proxies like Varnish or Squid. To enable caching, traffic must be terminated at the load balancer before being cached.

:p How can you enable caching when using SSL?
??x
To enable caching:
- Terminate SSL traffic at a load balancer.
- Cache content behind the load balancer.

```java
// Example of terminating SSL and caching (hypothetical)
public class LoadBalancerConfig {
    public void configureLoadBalancer(String lbHost, String sslCertPath) throws Exception {
        // Configure load balancer to terminate SSL
        LoadBalancer lb = new LoadBalancer(lbHost);
        lb.configureTermination(sslCertPath);
        // Cache content behind the load balancer
        lb.cacheContentBehindProxy();
    }
}
```
x??",803,"This is by far the most common form of inside-perimeter trust I see in organizations. They may decide to run this traffic over HTTPS, but they don’t do much else. I’m not saying that is a good thing. ...",qwen2.5:latest,2025-11-02 10:21:30,7
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,API Keys,SAML or OpenID Connect Integration,"#### SAML or OpenID Connect Integration
Background context: If an organization already uses SAML or OpenID Connect for authentication, integrating these methods with service-to-service interactions can simplify credential management and reduce redundancy.

:p What are the benefits of using SAML or OpenID Connect for service-to-service authentication?
??x
The benefits include:
- Centralized identity management: Use existing SSO solutions to handle service-to-service authentication.
- Reduced duplication: Leverage the same credentials, reducing the risk of duplicated behavior and improving consistency.

```java
// Example of integrating with an Identity Provider using OpenID Connect (hypothetical)
public class ServiceIntegration {
    public void authenticateWithSAML(String clientId, String clientSecret) throws Exception {
        // Obtain tokens from identity provider via OpenID Connect
        TokenResponse token = OAuth2Client.getToken(clientId, clientSecret);
        // Use the token for authentication
    }
}
```
x??",1036,"This is by far the most common form of inside-perimeter trust I see in organizations. They may decide to run this traffic over HTTPS, but they don’t do much else. I’m not saying that is a good thing. ...",qwen2.5:latest,2025-11-02 10:21:30,4
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,API Keys,Centralized Service Access Control via Directory Server,"#### Centralized Service Access Control via Directory Server
Background context: This approach leverages an existing infrastructure and a centralized directory server for managing service access controls. It ensures that all service interactions are authenticated through this central point, making it easier to manage permissions and access across multiple services.

:p What is the main advantage of using a centralized directory server for service access control?
??x
The primary benefit is the ability to centrally manage and enforce authentication and authorization policies, reducing the complexity and effort required to maintain multiple independent systems. This setup also allows for fine-grained control over permissions and easier revocation or modification of credentials.
x??",789,"The advantage here is that you’re making use of existing infrastructure, and get to centralize all your service access controls in a central directory server. We’d still need to route this over HTTPS ...",qwen2.5:latest,2025-11-02 10:21:53,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,API Keys,Service Accounts for Authentication,"#### Service Accounts for Authentication
Background context: In this approach, clients have a set of credentials they use to authenticate themselves with an identity provider, which then grants the service necessary information for authentication decisions.

:p Why are service accounts recommended in this context?
??x
Service accounts help in maintaining secure and granular access control. By assigning specific credentials to each microservice, it becomes easier to manage and revoke access if a set of credentials is compromised.
x??",538,"The advantage here is that you’re making use of existing infrastructure, and get to centralize all your service access controls in a central directory server. We’d still need to route this over HTTPS ...",qwen2.5:latest,2025-11-02 10:21:53,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,API Keys,Secure Storage of Credentials,"#### Secure Storage of Credentials
Background context: Storing credentials securely is crucial to prevent unauthorized access. The challenge lies in ensuring that the client has a secure way to store these credentials.

:p How can clients securely store their credentials?
??x
Clients need to employ robust methods to store credentials, such as using environment variables, secret management services, or encrypted storage mechanisms. These methods help mitigate risks associated with insecure storage.
x??",506,"The advantage here is that you’re making use of existing infrastructure, and get to centralize all your service access controls in a central directory server. We’d still need to route this over HTTPS ...",qwen2.5:latest,2025-11-02 10:21:53,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,API Keys,Authentication Protocols: SAML and OpenID Connect,"#### Authentication Protocols: SAML and OpenID Connect
Background context: Various protocols like SAML and OpenID Connect are used for authentication. While OpenID Connect is simpler to implement, it may not be as widely supported yet.

:p What are the strengths of OpenID Connect?
??x
OpenID Connect offers a simplified workflow compared to SAML, making it easier to integrate into applications. It provides more user-friendly and flexible authentication experiences while maintaining robust security.
x??",506,"The advantage here is that you’re making use of existing infrastructure, and get to centralize all your service access controls in a central directory server. We’d still need to route this over HTTPS ...",qwen2.5:latest,2025-11-02 10:21:53,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,API Keys,Client Certificates for Authentication,"#### Client Certificates for Authentication
Background context: Using client certificates with TLS can provide strong guarantees about the identity of clients. However, managing certificates involves significant operational challenges.

:p What are the main drawbacks of using client certificates?
??x
The primary issues include complex certificate management and troubleshooting, as well as the difficulty in revoking or reissuing certificates when needed. Additionally, managing a greater number of certificates can be cumbersome.
x??",536,"The advantage here is that you’re making use of existing infrastructure, and get to centralize all your service access controls in a central directory server. We’d still need to route this over HTTPS ...",qwen2.5:latest,2025-11-02 10:21:53,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,API Keys,HMAC Over HTTP for Authentication,"#### HMAC Over HTTP for Authentication
Background context: HMAC (Hash-based Message Authentication Code) is used to ensure data integrity by generating a short, fixed-length message digest.

:p What are the key benefits of using HMAC over plain HTTP?
??x
HMAC provides better security than Basic Auth because it uses a hash function and shared secret keys, which helps prevent attacks like man-in-the-middle. It also reduces the risk of credentials being compromised.
x??",471,"The advantage here is that you’re making use of existing infrastructure, and get to centralize all your service access controls in a central directory server. We’d still need to route this over HTTPS ...",qwen2.5:latest,2025-11-02 10:21:53,7
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,API Keys,Summary of Flashcards,"#### Summary of Flashcards

- **Centralized Service Access Control via Directory Server**: Focuses on centralizing authentication and authorization policies for multiple services.
- **Service Accounts for Authentication**: Emphasizes using specific credentials for each microservice to manage access more efficiently.
- **Secure Storage of Credentials**: Discusses methods to securely store client credentials, ensuring they are not easily accessible by unauthorized parties.
- **Authentication Protocols: SAML and OpenID Connect**: Compares the strengths and limitations of different authentication protocols.
- **Client Certificates for Authentication**: Highlights the operational challenges associated with managing client certificates.
- **HMAC Over HTTP for Authentication**: Explains how HMAC can be used to secure data integrity over HTTP.",847,"The advantage here is that you’re making use of existing infrastructure, and get to centralize all your service access controls in a central directory server. We’d still need to route this over HTTPS ...",qwen2.5:latest,2025-11-02 10:21:53,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,API Keys,HMAC-Based Request Signing,"#### HMAC-Based Request Signing
Background context explaining the HMAC-based request signing approach. This method is extensively used by Amazon’s S3 APIs for AWS and parts of the OAuth specification to ensure the integrity and authenticity of requests. The idea involves hashing the request body along with a private key, sending only the hash (and not the private key) in the request.

HMAC stands for Hash-based Message Authentication Code. It uses a cryptographic hash function like SHA-256 combined with a secret key to create a unique signature of the message.

:p How does HMAC ensure the security and integrity of requests?
??x
The server generates a hash using its copy of the private key and re-creates the hash from the received request body. If the two hashes match, it confirms that no tampering occurred during transit and that the request is authentic.

For example, in Java, you can use the `HMACSHA256` method from the `Mac` class in the `javax.crypto` package:
```java
import javax.crypto.Mac;
import javax.crypto.spec.SecretKeySpec;

public class HMACExample {
    public static String generateHMAC(String data, byte[] key) throws Exception {
        Mac sha256_Hmac = Mac.getInstance(""HmacSHA256"");
        SecretKeySpec secret_key = new SecretKeySpec(key, ""HmacSHA256"");
        sha256_Hmac.init(secret_key);
        return Base64.getEncoder().encodeToString(sha256_Hmac.doFinal(data.getBytes()));
    }
}
```
x??",1434,"An alternative approach, as used extensively by Amazon’s S3 APIs for AWS and in parts of the OAuth specification, is to use a  hash-based messaging code (HMAC)  to sign the request. With HMAC the body...",qwen2.5:latest,2025-11-02 10:22:23,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,API Keys,API Keys for Microservices Communication,"#### API Keys for Microservices Communication
Background context explaining the use of API keys in microservice-to-microservice communication. Unlike traditional authentication mechanisms, API keys are designed to be simple and easy to implement but come with their own set of challenges.

API keys allow services to identify who is making a call and often include rate-limiting or other access control measures. They can be implemented using shared keys or public-private key pairs.

:p What is the primary purpose of using API keys in microservice-to-microservice communication?
??x
The main purposes are to identify the caller, enforce access controls, and manage service usage (e.g., rate limiting). For instance, a gateway model might use API keys to authenticate calls and limit the number of requests from a particular key.

Here’s an example of checking an API key in Java:
```java
public class ApiKeyValidator {
    private final Map<String, String> apiKeyMap;

    public ApiKeyValidator(Map<String, String> apiKeyMap) {
        this.apiKeyMap = apiKeyMap;
    }

    public boolean validateApiKey(String apiKey) {
        return apiKeyMap.containsKey(apiKey);
    }
}
```
x??",1186,"An alternative approach, as used extensively by Amazon’s S3 APIs for AWS and in parts of the OAuth specification, is to use a  hash-based messaging code (HMAC)  to sign the request. With HMAC the body...",qwen2.5:latest,2025-11-02 10:22:23,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,API Keys,JSON Web Tokens (JWT),"#### JSON Web Tokens (JWT)
Background context explaining JWTs and their use in microservices communication. JWT is a compact, URL-safe means of representing claims to be transferred between two parties.

.JWT consists of three parts: Header, Payload, and Signature. The header typically contains the type of token and the signing algorithm used. The payload can contain various claims about the user or entity holding the token. The signature ensures that the JWT has not been tampered with during transit.

:p What are JSON Web Tokens (JWTs) primarily used for in microservices?
??x
JSON Web Tokens (JWTs) are primarily used to transmit information between parties as a JSON object, typically containing the user's identity and other claims. They offer a stateless way of managing sessions and can be easily cached.

Here’s an example of creating a JWT:
```java
import io.jsonwebtoken.Claims;
import io.jsonwebtoken.Jwts;
import io.jsonwebtoken.SignatureAlgorithm;

public class JwtExample {
    public static String createJwt(String subject, long ttlMillis) {
        SignatureAlgorithm signatureAlgorithm = SignatureAlgorithm.HS256;
        byte[] apiKeySecretBytes = DatatypeConverter.parseBase64Binary(""secret"");
        Key signingKey = new SecretKeySpec(apiKeySecretBytes, signatureAlgorithm.getJcaName());
        return Jwts.builder()
                .setSubject(subject)
                .signWith(signatureAlgorithm, signingKey)
                .compact();
    }
}
```
x??",1482,"An alternative approach, as used extensively by Amazon’s S3 APIs for AWS and in parts of the OAuth specification, is to use a  hash-based messaging code (HMAC)  to sign the request. With HMAC the body...",qwen2.5:latest,2025-11-02 10:22:23,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,API Keys,Gateway Models for API Management,"#### Gateway Models for API Management
Background context explaining the role of gateway models in managing and securing microservices. A gateway model acts as a central point of control over all external requests to a system, providing features like authentication, rate limiting, logging, and more.

:p What is the role of a gateway model in microservice architecture?
??x
A gateway model serves as an entry point for all external requests to a system, acting as a single point of control. It can handle tasks such as API key validation, rate limiting, logging, authentication (e.g., SAML), and more. This approach simplifies the complexity of managing security and service access across multiple microservices.

For example, a gateway might use Spring Cloud Gateway or Kong to manage traffic:
```java
import org.springframework.cloud.gateway.filter.GatewayFilterChain;
import org.springframework.cloud.gateway.filter.factory.RateLimiterGatewayFilter;
import org.springframework.http.server.reactive.ServerHttpRequest;
import org.springframework.web.server.ServerWebExchange;

@Bean
public RouteLocator customRouteLocator(RouteLocatorBuilder builder) {
    return builder.routes()
            .route(r -> r.path(""/api/v1/**"")
                    .filters(f -> f.filter(new RateLimiterGatewayFilterFactory().apply(ServerWebExchange)))
                    .uri(""lb://service-name""))
            .build();
}
```
x??

---",1419,"An alternative approach, as used extensively by Amazon’s S3 APIs for AWS and in parts of the OAuth specification, is to use a  hash-based messaging code (HMAC)  to sign the request. With HMAC the body...",qwen2.5:latest,2025-11-02 10:22:23,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Deputy Problem,Confused Deputy Problem,"#### Confused Deputy Problem
Background context explaining the concept. The confused deputy problem refers to a situation where a malicious party can trick a service (the ""deputy"") into making unauthorized calls on its behalf, even if the deputy itself is trusted.

:p What is the confused deputy problem?
??x
The confused deputy problem arises when a trusted service or application (the ""deputy"") is manipulated by a malicious actor to perform actions that it shouldn't be able to. For example, in an online shopping scenario, a logged-in user might trick the online shop's UI into making requests for sensitive information about other users.

This can occur even if the deputy has implicit trust within its perimeter or uses certificates/API keys to verify itself. However, such measures alone may not be sufficient to prevent unauthorized actions from being performed.
x??",875,The Deputy Problem Having a principal authenticate with a given microserservice is simple enough. But what happens if that service then needs to make additional calls to complete an operation? Take a ...,qwen2.5:latest,2025-11-02 10:22:49,7
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Deputy Problem,Implicit Trust and Authentication,"#### Implicit Trust and Authentication
Background context explaining the concept. In scenarios where a service makes calls internally (within its own network), it might adopt an approach of implicit trust. This means that calls from within the same network perimeter are assumed to be legitimate without further authentication.

However, this can lead to vulnerabilities if a malicious party manages to trick the service into performing unauthorized actions on behalf of another entity.

:p Should downstream services accept calls from the online shop under an implicit trust model?
??x
Implicit trust models assume that all internal calls are legitimate. However, this approach is risky because it doesn't verify the identity or context of each caller. A malicious party could exploit this by tricking the service into making unauthorized requests.

For example, if a user logs in and then tricks the online shop's UI into requesting sensitive information about another user, this request might be accepted under an implicit trust model.
x??",1042,The Deputy Problem Having a principal authenticate with a given microserservice is simple enough. But what happens if that service then needs to make additional calls to complete an operation? Take a ...,qwen2.5:latest,2025-11-02 10:22:49,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Deputy Problem,Verifying Caller Identity,"#### Verifying Caller Identity
Background context explaining the concept. To mitigate the confused deputy problem, one approach is to verify the identity of the caller before performing any operations.

This can involve passing the original principal's credentials down the call chain and validating them at each step. However, this approach can be complex and may require additional layers of authentication protocols like SAML or OpenID Connect.

:p How can we ensure that downstream services correctly identify the caller?
??x
To ensure that downstream services correctly identify the caller, you can implement a system where the online shop service passes along the original principal's credentials (such as through SAML assertions) when making calls to other services. This way, each service can verify the identity of the caller independently.

For instance, if the online shop wants to fetch an order status for a specific user, it could include the user’s credentials in its request:

```java
// Pseudocode example
public OrderStatus getOrderStatus(String orderId, String userId) {
    // Fetch order from order service with userId and other necessary information
    OrderServiceClient client = new OrderServiceClient();
    
    // Pass along user's credentials for verification
    OrderRequest request = new OrderRequest(orderId, userId);
    OrderResponse response = client.getOrder(request);

    return parseOrderResponse(response);
}
```

This ensures that even if the online shop is compromised, other services can still verify the caller’s identity.
x??",1571,The Deputy Problem Having a principal authenticate with a given microserservice is simple enough. But what happens if that service then needs to make additional calls to complete an operation? Take a ...,qwen2.5:latest,2025-11-02 10:22:49,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Deputy Problem,Routing Requests Directly,"#### Routing Requests Directly
Background context explaining the concept. Another approach to handle service-to-service communication securely is to route requests directly from the UI to the target service and validate the request there.

This method avoids trusting internal calls implicitly but introduces complexity in managing authentication and authorization across multiple services, which can be cumbersome if not properly managed.

:p Can we avoid implicit trust by routing requests directly?
??x
By routing requests directly from the UI to the order service, you bypass the need for implicit trust within your network. This approach allows the order service itself to validate the request based on its own policies and authentication mechanisms.

For example:

```java
// Pseudocode example
public OrderStatus getOrderStatus(String orderId) {
    // Directly route the request from UI to order service
    OrderServiceClient client = new OrderServiceClient();
    
    // Pass along the necessary information for validation
    OrderRequest request = new OrderRequest(orderId);
    OrderResponse response = client.getOrder(request);

    return parseOrderResponse(response);
}
```

This method ensures that even if internal services are compromised, the order service can still verify the authenticity of each request.
x??",1332,The Deputy Problem Having a principal authenticate with a given microserservice is simple enough. But what happens if that service then needs to make additional calls to complete an operation? Take a ...,qwen2.5:latest,2025-11-02 10:22:49,7
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Deputy Problem,Passing Credentials Downstream,"#### Passing Credentials Downstream
Background context explaining the concept. In some cases, authentication schemes allow passing down credentials to downstream services. However, this approach is complex and often impractical with protocols like SAML, which require nested assertions.

:p Can we pass down original principal's credentials using SAML?
??x
Passing down original principal's credentials using SAML can be technically possible but highly complex due to the need for nested SAML assertions. While theoretically feasible, this approach is rarely implemented in practice because of its complexity and overhead.

Instead, other methods like API keys or custom token-based systems are often preferred for practicality.
x??",732,The Deputy Problem Having a principal authenticate with a given microserservice is simple enough. But what happens if that service then needs to make additional calls to complete an operation? Take a ...,qwen2.5:latest,2025-11-02 10:22:49,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Deputy Problem,Complexity of Validation,"#### Complexity of Validation
Background context explaining the concept. In scenarios where multiple services make downstream calls, the validation process can become increasingly complex. Each service might need to validate not only its own identity but also ensure that subsequent calls are valid.

:p How does the complexity increase with each additional call in a chain?
??x
The complexity increases exponentially as you add more layers of service-to-service communication. Each service must verify the legitimacy of the caller at multiple levels, potentially requiring nested authentication checks and complex logic to manage trust between services.

For example:

```java
// Pseudocode example
public OrderStatus getOrderStatus(String orderId) {
    // Service A makes a request to Service B which then requests from Service C
    OrderServiceClient client = new OrderServiceClient();
    
    // Pass along the necessary information for validation
    OrderRequest request = new OrderRequest(orderId);
    OrderResponse responseFromB = client.getOrder(request);  // Service B processes and forwards
    
    // Now, Service B needs to validate its own credentials AND verify the request from Service A
    if (validateRequestFromA(responseFromB)) {
        // Forward validated request to final service C
        ShippingServiceClient shippingClient = new ShippingServiceClient();
        OrderResponse responseFromC = shippingClient.getShippingInfo(orderId);
        
        return parseOrderResponse(responseFromC);
    } else {
        throw new UnauthorizedAccessException(""Request validation failed."");
    }
}
```

This example shows how each step in the chain needs to validate its own identity and ensure that it is acting on behalf of a legitimate caller.
x??

---",1781,The Deputy Problem Having a principal authenticate with a given microserservice is simple enough. But what happens if that service then needs to make additional calls to complete an operation? Take a ...,qwen2.5:latest,2025-11-02 10:22:49,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,A Worked Example,Use Well-Known Implementations for Encryption,"---
#### Use Well-Known Implementations for Encryption
Background context: When securing data at rest, it's crucial to use well-tested and widely adopted encryption algorithms. Implementing your own encryption or even someone else’s can lead to security vulnerabilities.

:p What are the reasons to choose a well-known implementation of AES-128 or AES-256?
??x
Using well-known implementations like AES-128 or AES-256 ensures that you benefit from extensive testing and regular updates. Popular runtimes such as Java and .NET provide built-in libraries for these algorithms, which are highly likely to be secure and regularly maintained.

For example, the Java Cryptography Architecture (JCA) includes providers like SunJCE, which implement standard encryption algorithms including AES.
```java
import javax.crypto.Cipher;
import javax.crypto.spec.SecretKeySpec;

public class EncryptionExample {
    private static final String ALGORITHM = ""AES"";
    
    public static void main(String[] args) throws Exception {
        byte[] keyBytes = new byte[16];
        // Initialize the key with your 128-bit (16 bytes) or 256-bit (32 bytes) key
        SecretKeySpec secretKey = new SecretKeySpec(keyBytes, ALGORITHM);
        
        Cipher cipher = Cipher.getInstance(""AES"");
        cipher.init(Cipher.ENCRYPT_MODE, secretKey);
        
        byte[] encryptedData = cipher.doFinal(""Sensitive data"".getBytes());
        System.out.println(new String(encryptedData));
    }
}
```
x??",1482,"Securing Data at Rest Data lying about is a liability, especially if it is sensitive. Hopefully we’ve done everything we can to ensure attackers cannot breach our network, and also that they cannot br...",qwen2.5:latest,2025-11-02 10:23:16,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,A Worked Example,Importance of Salted Password Hashing,"#### Importance of Salted Password Hashing
Background context: To protect passwords, it’s essential to use techniques that make it difficult for attackers to reverse-engineer the original password. One such technique is salting, where a random value (salt) is added to the password before hashing.

:p Why is salted password hashing important?
??x
Salted password hashing protects against attacks by adding a unique, random value (the ""salt"") to each password before hashing it. This means that even if two users have the same password, their hashes will be different, making dictionary and brute-force attacks much more difficult.

For example, you can use Java’s `MessageDigest` class with a salt to hash passwords.
```java
import java.security.MessageDigest;
import java.security.NoSuchAlgorithmException;

public class SaltedHashingExample {
    public static String getSalt() {
        // Generate a random 16-byte salt (you can make it longer or shorter as needed)
        return ""RandomSaltValue""; 
    }

    public static void main(String[] args) throws NoSuchAlgorithmException, Exception {
        String password = ""password"";
        String salt = getSalt();
        MessageDigest digest = MessageDigest.getInstance(""SHA-256"");
        
        // Create a byte array to hold the new hash value
        byte[] hashValue = digest.digest((salt + password).getBytes());
        
        // Convert byte array into signum representation
        StringBuilder hexString = new StringBuilder(32);
        for (byte b : hashValue) {
            String hex = Integer.toHexString(0xff & b);
            if(hex.length() == 1) hexString.append('0');
            hexString.append(hex);
        }
        
        System.out.println(""Salted Hash: "" + hexString.toString());
    }
}
```
x??",1788,"Securing Data at Rest Data lying about is a liability, especially if it is sensitive. Hopefully we’ve done everything we can to ensure attackers cannot breach our network, and also that they cannot br...",qwen2.5:latest,2025-11-02 10:23:16,7
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,A Worked Example,Key Management and Storage,"#### Key Management and Storage
Background context: The security of data encryption relies heavily on the management and storage of keys. If an attacker can access the key, they can decrypt the data.

:p Where should keys be stored to ensure data security?
??x
Keys should be stored in secure, separate locations from where the encrypted data is stored. Using a dedicated security appliance or a key vault service can help manage keys securely and provide controlled access mechanisms.

For example, Azure Key Vault provides a managed service for storing keys.
```java
// Pseudocode to demonstrate key management with Azure Key Vault (simplified)
import com.azure.identity.DefaultAzureCredentialBuilder;
import com.azure.keyvault.keys.KeyClient;
import com.azure.keyvault.keys.models.KeyProperties;

public class KeyVaultExample {
    public static void main(String[] args) throws Exception {
        // Initialize the client using your Azure subscription details
        String vaultUrl = ""https://your-key-vault.vault.azure.net"";
        
        DefaultAzureCredential credential = new DefaultAzureCredentialBuilder().build();
        KeyClient keyClient = new KeyClientBuilder()
            .vaultUrl(vaultUrl)
            .credential(credential)
            .buildClient();

        // Generate a new key
        KeyProperties keyProps = new KeyProperties().withKeyType(""AsymmetricRSA"").withKeySize(2048);
        
        keyClient.createKey(""YourKeyName"", keyProps);

        // Retrieve the generated key
        byte[] key = keyClient.getKey(""YourKeyName"").getKeyMaterial();
        
        System.out.println(new String(key));
    }
}
```
x??

---",1658,"Securing Data at Rest Data lying about is a liability, especially if it is sensitive. Hopefully we’ve done everything we can to ensure attackers cannot breach our network, and also that they cannot br...",qwen2.5:latest,2025-11-02 10:23:16,7
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,A Worked Example,Avoid Implementing Your Own Encryption,"---
#### Avoid Implementing Your Own Encryption
Background context: When dealing with data security, it's often recommended to leverage established and well-tested encryption solutions rather than implementing your own. Custom encryption schemes can introduce vulnerabilities that are hard to detect and fix.

:p What is the reason for avoiding custom implementations of encryption?
??x
Avoiding custom implementations helps prevent introducing security vulnerabilities that may be difficult to identify and address. Established cryptographic libraries have undergone extensive testing and scrutiny, making them more reliable and secure.
x??",641,"Avoid implementing your own, and do some good research. Pick Your Targets Assuming everything should be encrypted can simplify things somewhat. There is no guesswork about what should or should not be...",qwen2.5:latest,2025-11-02 10:23:32,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,A Worked Example,Pick Your Targets,"#### Pick Your Targets
Background context: Encrypting all data might simplify some aspects of security management but can significantly increase computational overhead. Instead, it's important to carefully assess which parts of the system need encryption based on sensitivity.

:p What is a balanced approach for encryption in a system?
??x
A balanced approach involves identifying critical data stores that require strong protection and encrypting them selectively. Fine-grained services can help in segmenting sensitive data from less critical ones, ensuring only necessary data is encrypted.
x??",598,"Avoid implementing your own, and do some good research. Pick Your Targets Assuming everything should be encrypted can simplify things somewhat. There is no guesswork about what should or should not be...",qwen2.5:latest,2025-11-02 10:23:32,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,A Worked Example,Decrypt on Demand,"#### Decrypt on Demand
Background context: Encrypting data at the point of access (on demand) helps minimize storage overhead while maintaining security. Storing decrypted data increases the risk of exposure if a breach occurs.

:p What are the benefits of decrypting data on demand?
??x
Benefits include reduced storage costs and decreased computational load, as sensitive data is only decrypted when necessary. This approach minimizes the exposure surface and enhances overall system security.
x??",499,"Avoid implementing your own, and do some good research. Pick Your Targets Assuming everything should be encrypted can simplify things somewhat. There is no guesswork about what should or should not be...",qwen2.5:latest,2025-11-02 10:23:32,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,A Worked Example,Encrypt Backups,"#### Encrypt Backups
Background context: Encryption should be applied to backups to ensure that even if backup files are compromised, the sensitive information remains protected.

:p Why is it important to encrypt backups?
??x
Encrypting backups ensures data integrity and confidentiality. It prevents unauthorized access to critical information stored in backup files, which can be a significant security risk.
x??",415,"Avoid implementing your own, and do some good research. Pick Your Targets Assuming everything should be encrypted can simplify things somewhat. There is no guesswork about what should or should not be...",qwen2.5:latest,2025-11-02 10:23:32,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,A Worked Example,Defense in Depth,"#### Defense in Depth
Background context: Implementing multiple layers of security helps mitigate risks by providing redundancy and ensuring that breaches are detected and contained.

:p What is the principle behind defense in depth?
??x
The principle behind defense in depth involves layering various security measures to protect against different types of threats. This approach ensures that if one layer fails, others can still provide protection.
x??",454,"Avoid implementing your own, and do some good research. Pick Your Targets Assuming everything should be encrypted can simplify things somewhat. There is no guesswork about what should or should not be...",qwen2.5:latest,2025-11-02 10:23:32,7
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,A Worked Example,Firewalls,"#### Firewalls
Background context: Firewalls are used to control access to a network and prevent unauthorized traffic from entering or leaving.

:p What is the purpose of using multiple firewalls in a system?
??x
Using multiple firewalls provides additional layers of security. Local firewalls on hosts can tailor rules to specific services, while perimeter firewalls manage broader access controls, creating a robust defense against external threats.
x??",455,"Avoid implementing your own, and do some good research. Pick Your Targets Assuming everything should be encrypted can simplify things somewhat. There is no guesswork about what should or should not be...",qwen2.5:latest,2025-11-02 10:23:32,2
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,A Worked Example,Logging,"#### Logging
Background context: Logging helps in detecting and responding to security incidents after they occur.

:p What is the importance of logging sensitive information?
??x
Logging should be done with care to avoid storing sensitive data. Logs can help in post-incident analysis but must not contain critical information that could be exploited by attackers.
x??",369,"Avoid implementing your own, and do some good research. Pick Your Targets Assuming everything should be encrypted can simplify things somewhat. There is no guesswork about what should or should not be...",qwen2.5:latest,2025-11-02 10:23:32,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,A Worked Example,Intrusion Detection Systems (IDS) and Intrusion Prevention Systems (IPS),"#### Intrusion Detection Systems (IDS) and Intrusion Prevention Systems (IPS)
Background context: IDS and IPS are security systems that monitor networks or hosts for suspicious behavior, providing alerts or taking action to prevent potential threats. IDS is typically more passive, alerting administrators to issues, while IPS can actively stop potential attacks.

:p What are the key differences between IDS and IPS?
??x
IDS primarily monitors networks or hosts for suspicious activities and reports them, whereas IPS takes a step further by stopping suspicious activities from occurring. The main difference lies in their operational mode—IDS is more passive (heuristic-based), while IPS can actively intervene to prevent threats.
x??",736,"Intrusion Detection (and Prevention) System Intrusion detection systems (IDS)  can monitor networks or hosts for suspicious behavior, reporting problems when it sees them. Intrusion prevention systems...",qwen2.5:latest,2025-11-02 10:23:57,2
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,A Worked Example,Network Segregation with Microservices,"#### Network Segregation with Microservices
Background context: In a monolithic architecture, network segmentation might be limited, but microservices allow for finer-grained control over how services interact by placing them in different network segments. AWS provides VPCs that can further enhance security by isolating networks and allowing controlled communication between them.

:p How does using microservices facilitate better network segregation?
??x
Using microservices enables better network segmentation because each service can be placed in its own network segment, reducing the attack surface and improving isolation. AWS’s VPC allows for virtual subnets where services can run independently, with rules defining which segments can communicate. This setup also supports multi-layer security measures by creating multiple perimeters.
x??",849,"Intrusion Detection (and Prevention) System Intrusion detection systems (IDS)  can monitor networks or hosts for suspicious behavior, reporting problems when it sees them. Intrusion prevention systems...",qwen2.5:latest,2025-11-02 10:23:57,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,A Worked Example,Operating System Security,"#### Operating System Security
Background context: The operating system is a critical layer in any application stack that could introduce vulnerabilities. Properly securing the OS involves minimizing permissions for running services and regularly patching to mitigate known issues.

:p How can you secure an operating system from potential vulnerabilities?
??x
To secure an operating system, run services with minimal permissions to limit damage if compromised. Additionally, ensure regular patching of your software to address security vulnerabilities. Tools like Microsoft’s SCCM or RedHat’s Spacewalk can help in automating updates and monitoring the status of patches.

```java
// Example of a simple script using Ansible to update systems
- name: Ensure all packages are up to date
  apt:
    name: '*'
    state: latest
```
x??

---",838,"Intrusion Detection (and Prevention) System Intrusion detection systems (IDS)  can monitor networks or hosts for suspicious behavior, reporting problems when it sees them. Intrusion prevention systems...",qwen2.5:latest,2025-11-02 10:23:57,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,A Worked Example,AppArmor Overview,"#### AppArmor Overview
AppArmor allows you to define how your application is expected to behave, with the kernel keeping an eye on it. If the application starts doing something it shouldn't, the kernel steps in.
:p What does AppArmor do?
??x
AppArmor defines a set of rules for applications to follow, ensuring they only perform actions that are permitted by these rules. The kernel monitors these actions and stops the application if it attempts to do anything outside its defined behavior.
x??",495,"AppArmour, for example, allows you to define how your application is expected to behave, with the kernel keeping an eye on it. If it starts doing something it shouldn’t, the kernel steps in. AppArmour...",qwen2.5:latest,2025-11-02 10:24:13,5
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,A Worked Example,SELinux Overview,"#### SELinux Overview
SELinux (Security-Enhanced Linux) is a security module for the Linux kernel that enforces mandatory access controls on all operating system resources.
:p What is SELinux?
??x
SELinux is a security-enhancing mechanism that uses labels to enforce strict access control policies, ensuring that only authorized entities can interact with sensitive data or services.
x??",387,"AppArmour, for example, allows you to define how your application is expected to behave, with the kernel keeping an eye on it. If it starts doing something it shouldn’t, the kernel steps in. AppArmour...",qwen2.5:latest,2025-11-02 10:24:13,4
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,A Worked Example,Grsecurity Overview,"#### Grsecurity Overview
GrSecurity aims to be simpler to use than AppArmor and SELinux while also offering expanded capabilities. It requires a custom kernel to work effectively.
:p What is Grsecurity?
??x
Grsecurity is an enhanced security module that provides additional protection mechanisms on top of the standard Linux kernel, aiming for ease of use with advanced features. To utilize its full capabilities, a custom kernel is required.
x??",446,"AppArmour, for example, allows you to define how your application is expected to behave, with the kernel keeping an eye on it. If it starts doing something it shouldn’t, the kernel steps in. AppArmour...",qwen2.5:latest,2025-11-02 10:24:13,3
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,A Worked Example,Security Layering in MusicCorp Architecture,"#### Security Layering in MusicCorp Architecture
In the context of MusicCorp's architecture, different parts of the system should have varying levels of security depending on their sensitivity and importance.
:p How can we apply security layering to MusicCorp’s architecture?
??x
By implementing differentiated security measures based on the sensitivity of data and the criticality of services. For instance, using HTTPS for secure transactions, client certificates for third-party services, and HTTP for less sensitive content.
x??",532,"AppArmour, for example, allows you to define how your application is expected to behave, with the kernel keeping an eye on it. If it starts doing something it shouldn’t, the kernel steps in. AppArmour...",qwen2.5:latest,2025-11-02 10:24:13,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,A Worked Example,Secure Communication with Third-Party Royalty Payment System,"#### Secure Communication with Third-Party Royalty Payment System
For secure communication with a third-party system handling royalty payments, ensure all data is transmitted over an encrypted channel.
:p How should we handle data transmission to the third-party royalty payment system?
??x
Use client certificates and HTTPS (or other strong encryption methods) to securely transmit sensitive data. This ensures that only authorized parties can access the information and minimizes the risk of interception or tampering.
x??",524,"AppArmour, for example, allows you to define how your application is expected to behave, with the kernel keeping an eye on it. If it starts doing something it shouldn’t, the kernel steps in. AppArmour...",qwen2.5:latest,2025-11-02 10:24:13,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,A Worked Example,Data Security for Public WiFi Users,"#### Data Security for Public WiFi Users
For users accessing MusicCorp's services over public WiFi, provide an additional layer of security through HTTPS to protect their sessions from potential eavesdropping.
:p What should we do for customers using public WiFi?
??x
Implement HTTPS for all customer-facing web pages to encrypt data in transit. This prevents unauthorized parties from intercepting sensitive information such as login credentials or payment details.
x??",470,"AppArmour, for example, allows you to define how your application is expected to behave, with the kernel keeping an eye on it. If it starts doing something it shouldn’t, the kernel steps in. AppArmour...",qwen2.5:latest,2025-11-02 10:24:13,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,A Worked Example,Internal Collaboration Services Security,"#### Internal Collaboration Services Security
For internal services used only within the network perimeter, ensure that they are not exposed to external threats by limiting their access to trusted environments.
:p How should we secure internal collaborating services?
??x
Implement strict access controls and use encrypted channels for communication between internal services. Ensure that these services do not communicate with the internet or untrusted networks to prevent unauthorized access.
x??

---",503,"AppArmour, for example, allows you to define how your application is expected to behave, with the kernel keeping an eye on it. If it starts doing something it shouldn’t, the kernel steps in. AppArmour...",qwen2.5:latest,2025-11-02 10:24:13,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,A Worked Example,API Key Usage,"#### API Key Usage
Background context: The company wants to share catalog data widely but prevent abuse. Using API keys can help track usage and control access.

:p How does using API keys contribute to sharing catalog data while preventing abuse?
??x
Using API keys allows the company to track who is using their data, ensuring that it is not abused or misused by unauthorized parties. Each user of the data would need a unique key, which can be monitored for suspicious activity and revoked if necessary.

```java
public class ApiKeyManager {
    private Map<String, User> apiKeys = new HashMap<>();

    public void addApiKey(String apiKey, User user) {
        apiKeys.put(apiKey, user);
    }

    public boolean validateApiKey(String apiKey) {
        return apiKeys.containsKey(apiKey);
    }
}
```
x??",809,"For the feeds of catalog data, we want this information shared as widely as possible to allow people to easily buy music from us. However, we don’t want this abused, and we’d like some idea of who is ...",qwen2.5:latest,2025-11-02 10:24:41,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,A Worked Example,Network Perimeter Hardening,"#### Network Perimeter Hardening
Background context: The company decides to focus on hardening the network perimeter initially. This includes setting up a firewall and security appliances to detect malicious traffic.

:p What are some key steps in hardening the network perimeter?
??x
Key steps include configuring a properly configured firewall, selecting an appropriate hardware or software security appliance for intrusion detection (e.g., port scanning or DDoS attacks), and ensuring that all incoming and outgoing traffic is monitored and filtered according to defined rules.

```java
public class FirewallConfigurator {
    public void configureFirewall() {
        // Code to set up firewall rules, allow necessary services, block unwanted ones.
        System.out.println(""Firewall configured for security perimeter."");
    }
}
```
x??",843,"For the feeds of catalog data, we want this information shared as widely as possible to allow people to easily buy music from us. However, we don’t want this abused, and we’d like some idea of who is ...",qwen2.5:latest,2025-11-02 10:24:41,2
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,A Worked Example,Data Encryption Strategy,"#### Data Encryption Strategy
Background context: Customer data is sensitive and requires strong protection. The company decides to encrypt the data at rest but decrypt it on read.

:p How does the strategy of encrypting data held by the customer service protect against data breaches?
??x
Encrypting customer data at rest ensures that even if an attacker gains access to the database, they cannot read or use the data without the encryption key. Decrypting data only when needed (on read) minimizes exposure and prevents bulk retrieval.

```java
public class DataEncryptionManager {
    private String encryptionKey;

    public void encryptData(String data) throws EncryptionException {
        // Encrypt data using encryptionKey
    }

    public String decryptData(String encryptedData) {
        // Decrypt data and return plaintext
        return ""DecryptedData"";
    }
}
```
x??",886,"For the feeds of catalog data, we want this information shared as widely as possible to allow people to easily buy music from us. However, we don’t want this abused, and we’d like some idea of who is ...",qwen2.5:latest,2025-11-02 10:24:41,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,A Worked Example,Client Certificates for Additional Security,"#### Client Certificates for Additional Security
Background context: If the system allowed bulk retrieval of customer data, client certificates could be used to further secure API access.

:p How do client certificates enhance security in this scenario?
??x
Client certificates provide an additional layer of authentication beyond API keys. By requiring clients to present valid certificates, the company can ensure that only authorized and trusted applications or devices have access to sensitive data. This makes it harder for attackers to gain unauthorized access even if they have obtained a valid API key.

```java
public class ClientCertificateAuthenticator {
    public boolean authenticateClientCertificate(String certificate) {
        // Code to validate client certificate
        return true;
    }
}
```
x??",820,"For the feeds of catalog data, we want this information shared as widely as possible to allow people to easily buy music from us. However, we don’t want this abused, and we’d like some idea of who is ...",qwen2.5:latest,2025-11-02 10:24:41,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,A Worked Example,Final Architecture Overview,"#### Final Architecture Overview
Background context: The final architecture balances sharing of catalog data with strong protection for customer data. Different technologies are used based on the sensitivity and usage needs.

:p What is the key takeaway from MusicCorp's more secure system?
??x
The key takeaway is that a security strategy must be tailored to the specific nature and sensitivity of the data being protected. For shared data, using API keys can track usage while preventing abuse. For sensitive customer data, encryption at rest with on-demand decryption provides robust protection against unauthorized access.

```java
public class SecurityArchitecture {
    private ApiKeyManager apiKeyManager;
    private FirewallConfigurator firewallConfigurator;
    private DataEncryptionManager encryptionManager;

    public void implementSecurity() {
        apiKeyManager = new ApiKeyManager();
        firewallConfigurator = new FirewallConfigurator();
        encryptionManager = new DataEncryptionManager();

        // Implement steps to secure both catalog data and customer data
    }
}
```
x??

---",1115,"For the feeds of catalog data, we want this information shared as widely as possible to allow people to easily buy music from us. However, we don’t want this abused, and we’d like some idea of who is ...",qwen2.5:latest,2025-11-02 10:24:41,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Adapting to Communication Pathways,Data Minimization,"#### Data Minimization
In today's world, storing large amounts of data has become easier due to advancements in technology and databases. However, this abundance of data can pose significant risks related to privacy and security. To mitigate these issues, it is essential to store only necessary information that is absolutely required for business operations or legal compliance.
:p Why should businesses consider minimizing the amount of personally identifiable information they store?
??x
To protect against potential data breaches, unauthorized access, and comply with various data protection regulations such as GDPR. By storing less sensitive data, the risk of exposure is reduced, thus decreasing the impact on individuals whose data might be compromised.
x??",766,"Be Frugal As disk space becomes cheaper and the capabilities of the databases improve, the ease with which bulk amounts of information can be captured and stored is improving rapidly. This data is val...",qwen2.5:latest,2025-11-02 10:25:04,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Adapting to Communication Pathways,Datensparsamkeit,"#### Datensparsamkeit
The German concept of ""Datensparsamkeit"" emphasizes the importance of only collecting and storing essential information that fulfills specific business requirements or legal obligations. This approach helps in reducing the potential for misuse of personal data and mitigates risks associated with data breaches.
:p What does the term ""Datensparsamkeit"" signify?
??x
It signifies the practice of storing as little personally identifiable information (PII) as possible, only keeping what is absolutely necessary to perform business functions or meet legal requirements. This approach aligns with privacy-focused policies and reduces the risk of data exposure.
x??",683,"Be Frugal As disk space becomes cheaper and the capabilities of the databases improve, the ease with which bulk amounts of information can be captured and stored is improving rapidly. This data is val...",qwen2.5:latest,2025-11-02 10:25:04,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Adapting to Communication Pathways,Access Control for Ex-Employees,"#### Access Control for Ex-Employees
Organizations must have robust processes in place to manage access credentials when employees leave. The potential damage a disgruntled ex-employee could cause by retaining access to sensitive systems necessitates strict control over credential revocation and monitoring.
:p How can organizations ensure that ex-employees do not retain access to company systems?
??x
Organizations should implement a formal process for revoking access credentials when an employee leaves. This includes deactivating accounts, removing permissions from systems, and conducting audits of user accounts to ensure they are up-to-date and accurate.
```java
// Pseudocode for revoking ex-employee's access
public void revokeAccess(String employeeID) {
    // Deactivate account
    deactivateAccount(employeeID);
    
    // Remove system permissions
    removePermissionsFromSystems(employeeID);
    
    // Audit user accounts
    auditUserAccounts();
}
```
x??",977,"Be Frugal As disk space becomes cheaper and the capabilities of the databases improve, the ease with which bulk amounts of information can be captured and stored is improving rapidly. This data is val...",qwen2.5:latest,2025-11-02 10:25:04,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Adapting to Communication Pathways,Use of Established Security Protocols,"#### Use of Established Security Protocols
Security is a critical aspect of data protection, and it is often advisable to use well-established cryptographic tools and protocols rather than developing custom solutions. Using established technologies ensures greater security and reduces the risk of errors.
:p Why should developers avoid writing their own cryptographic functions?
??x
Developers should avoid writing their own cryptographic functions because doing so can introduce significant vulnerabilities. Custom cryptographic implementations are prone to errors, especially for those without extensive experience in cryptography. Established tools like AES (Advanced Encryption Standard) have undergone rigorous testing and peer review, making them reliable choices.
```java
// Example of using AES encryption with Java
import javax.crypto.Cipher;
import javax.crypto.spec.SecretKeySpec;

public class CryptoExample {
    private static final String ALGORITHM = ""AES"";
    
    public void encrypt(String key, String value) throws Exception {
        SecretKeySpec secretKey = new SecretKeySpec(key.getBytes(), ALGORITHM);
        Cipher cipher = Cipher.getInstance(ALGORITHM);
        cipher.init(Cipher.ENCRYPT_MODE, secretKey);
        
        byte[] encrypted = cipher.doFinal(value.getBytes());
        System.out.println(""Encrypted: "" + bytesToHex(encrypted));
    }
    
    // Helper method to convert bytes to hex string
    private static String bytesToHex(byte[] bytes) {
        StringBuilder sb = new StringBuilder();
        for (byte b : bytes) {
            sb.append(String.format(""%02x"", b));
        }
        return sb.toString();
    }
}
```
x??

---",1677,"Be Frugal As disk space becomes cheaper and the capabilities of the databases improve, the ease with which bulk amounts of information can be captured and stored is improving rapidly. This data is val...",qwen2.5:latest,2025-11-02 10:25:04,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Adapting to Communication Pathways,Educating Developers on Security Concerns,"#### Educating Developers on Security Concerns
Background context: The text emphasizes the importance of educating developers about security concerns to raise awareness and reduce security issues early in the development process. Tools like OWASP Top Ten list and Security Testing Framework are suggested as starting points.

:p How can we educate developers about security concerns?
??x
Educating developers is crucial for reducing security issues from the start. Familiarizing them with tools such as the OWASP Top Ten list, which outlines common web application vulnerabilities, and the OWASP Security Testing Framework, which provides a structured approach to testing these vulnerabilities, can significantly enhance their awareness.

For example, you could organize training sessions where developers learn about different types of attacks like cross-site scripting (XSS) and SQL injection. This education helps them write more secure code from the beginning.
x??",968,"Baking Security In Just as with automated functional testing, we don’t want security to be left to a different set of people, nor do we want to leave everything to the last minute. Helping educate dev...",qwen2.5:latest,2025-11-02 10:25:32,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Adapting to Communication Pathways,Using Automated Tools for Security,"#### Using Automated Tools for Security
Background context: The text suggests using automated tools to probe systems for vulnerabilities, such as Zed Attack Proxy (ZAP), which can simulate malicious attacks. Other tools like Brakeman for Ruby use static analysis to detect common coding mistakes.

:p What are some examples of automated security testing tools?
??x
Automated security testing tools help identify potential vulnerabilities early in the development cycle. For instance, Zed Attack Proxy (ZAP) is an open-source tool that can simulate various types of attacks on web applications. It uses a combination of active scanning and passive monitoring to detect issues.

Another example is Brakeman, a static analysis tool for Ruby applications. It scans code for common security flaws such as SQL injection and unsafe method calls.
```java
// Example Brakeman usage in a hypothetical Java context
Brakeman brakeman = new Brakeman();
brakeman.scanProject(""path/to/project"");
```
x??",988,"Baking Security In Just as with automated functional testing, we don’t want security to be left to a different set of people, nor do we want to leave everything to the last minute. Helping educate dev...",qwen2.5:latest,2025-11-02 10:25:32,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Adapting to Communication Pathways,Integrating Security Testing into CI Builds,"#### Integrating Security Testing into CI Builds
Background context: The text suggests integrating security testing tools like ZAP or Brakeman directly into Continuous Integration (CI) builds to ensure regular and automated checks for vulnerabilities.

:p How can we integrate automated security testing into our CI pipeline?
??x
Integrating security testing tools into the CI pipeline ensures that security checks are performed automatically with every code commit. For example, you could configure Jenkins or Travis CI to run ZAP scans as part of the build process.

Here’s a simple example using Jenkins:
```groovy
pipeline {
    agent any
    stages {
        stage('Security Scan') {
            steps {
                sh 'zap.sh start -daemon'
                sh 'zap.sh spider http://your-app-url.com'
                sh 'zap.sh scan --scantimeout 10m'
                sh 'zap.sh report --format xml > security_report.xml'
                sh 'mvn install'
            }
        }
    }
}
```
x??",1003,"Baking Security In Just as with automated functional testing, we don’t want security to be left to a different set of people, nor do we want to leave everything to the last minute. Helping educate dev...",qwen2.5:latest,2025-11-02 10:25:32,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Adapting to Communication Pathways,Using Nessus for Security Scans,"#### Using Nessus for Security Scans
Background context: The text mentions that tools like Nessus can be used to scan systems for vulnerabilities, though this often requires human interpretation of the results. These scans are more involved but still automatable.

:p How does a tool like Nessus help with security testing?
??x
Nessus is an automated security scanner that can detect a wide range of vulnerabilities in your network and applications. While it requires manual review to interpret its findings, integrating it into your CI pipeline allows you to automate the initial scanning process.

For example:
```bash
# Running Nessus scan using command line
nessuscli scan create --policy-id 1234567890-abcde --target http://your-app-url.com
```
This command creates a new scan with a specific policy and target URL. You would then manually review the detailed reports generated by Nessus.
x??",897,"Baking Security In Just as with automated functional testing, we don’t want security to be left to a different set of people, nor do we want to leave everything to the last minute. Helping educate dev...",qwen2.5:latest,2025-11-02 10:25:32,2
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Adapting to Communication Pathways,Microsoft Security Development Lifecycle (SDL),"#### Microsoft Security Development Lifecycle (SDL)
Background context: The text references Microsoft's SDL, which provides models for integrating security into development processes. While it may feel overly waterfall in some aspects, taking parts of it can improve current workflows.

:p How does Microsoft’s Security Development Lifecycle (SDL) help with integrating security?
??x
Microsoft’s SDL offers a structured approach to building secure software by incorporating security practices throughout the entire development lifecycle. It provides guidelines on how to bake security into every phase of the SDLC, from threat modeling and coding standards to code reviews and testing.

To integrate SDL principles, you might:
1. Conduct regular threat modeling sessions.
2. Implement coding standards that enforce security best practices.
3. Perform code reviews with a focus on security.
4. Use automated tools for static and dynamic analysis.

Here’s an example of how to implement a basic SDL check in a CI pipeline using Jenkins:
```groovy
pipeline {
    agent any
    stages {
        stage('Threat Modeling') {
            steps {
                echo 'Running threat modeling checks'
                // Add steps for running automated tools like Fortify
            }
        }
        stage('Code Review') {
            steps {
                echo 'Performing code reviews with security focus'
                // Integration of static analysis tools like SonarQube or Brakeman
            }
        }
    }
}
```
x??",1526,"Baking Security In Just as with automated functional testing, we don’t want security to be left to a different set of people, nor do we want to leave everything to the last minute. Helping educate dev...",qwen2.5:latest,2025-11-02 10:25:32,2
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Adapting to Communication Pathways,External Verification and Penetration Testing,"#### External Verification and Penetration Testing
Background context: The text suggests conducting external assessments, such as penetration testing, to ensure real-world vulnerability scenarios. These tests help teams identify issues they might overlook due to familiarity.

:p Why is it important to have an external assessment done for security?
??x
External assessments, particularly penetration testing by third-party experts, are crucial because developers may not be able to see their own mistakes. These testers can provide a fresh perspective and ensure that the application is robust against real-world attacks.

For example:
```bash
# Example command to run a penetration test with Metasploit
msfconsole
use auxiliary/scanner/http/enum_http_methods
set RHOSTS 192.168.1.100
run
```
This command runs an enumeration scan using Metasploit to identify HTTP methods that could be exploited.
x??",902,"Baking Security In Just as with automated functional testing, we don’t want security to be left to a different set of people, nor do we want to leave everything to the last minute. Helping educate dev...",qwen2.5:latest,2025-11-02 10:25:32,7
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Adapting to Communication Pathways,Microservices and Security Considerations,"#### Microservices and Security Considerations
Background context: The text discusses how microservices can help reduce the impact of security breaches by decomposing systems into smaller, more manageable components. It also mentions trading off security complexity for performance based on data sensitivity.

:p How do microservices influence security considerations?
??x
Microservices architectures allow you to apply different security measures at various levels within your system. By isolating services, you limit the potential impact of a breach. For sensitive data, you can implement more complex and secure approaches, while using lighter-weight methods for lower-risk scenarios.

For example:
```java
// Example of securing a microservice with OAuth2 in Java using Spring Security
@Configuration
public class SecurityConfig extends WebSecurityConfigurerAdapter {
    @Override
    protected void configure(HttpSecurity http) throws Exception {
        http.authorizeRequests()
            .antMatchers(""/secured/**"").authenticated() // Secure specific endpoints
            .and()
            .oauth2Login(); // Use OAuth2 for authentication
    }
}
```
x??

---",1171,"Baking Security In Just as with automated functional testing, we don’t want security to be left to a different set of people, nor do we want to leave everything to the last minute. Helping educate dev...",qwen2.5:latest,2025-11-02 10:25:32,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Adapting to Communication Pathways,Defense in Depth and Security Best Practices,"#### Defense in Depth and Security Best Practices
Background context explaining the concept of defense in depth, its importance for security. Mention the need to regularly patch operating systems and use established cryptographic tools rather than creating custom ones.

:p What is the main principle behind ""defense in depth""?
??x
The main principle behind ""defense in depth"" is layering multiple security controls to protect a system or network from various types of attacks, ensuring that if one control fails, others can still provide protection. This approach helps in reducing the overall risk and improving resilience.
x??",629,"Finally, understand the importance of defense in depth, make sure you patch your operating systems, and even if you consider yourself a rock star, don’t try to implement your own cryptography. If you ...",qwen2.5:latest,2025-11-02 10:25:58,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Adapting to Communication Pathways,Open Web Application Security Project (OWASP),"#### Open Web Application Security Project (OWASP)
Background context on OWASP as an organization dedicated to improving software security. Discuss its Top 10 Security Risks document.

:p What is OWASP known for?
??x
OWASP, or the Open Web Application Security Project, is a nonprofit organization focused on enhancing the security of web applications and systems. It is well-known for its regularly updated ""Top 10 Security Risks"" document, which provides essential guidelines and best practices to protect against common vulnerabilities.
x??",543,"Finally, understand the importance of defense in depth, make sure you patch your operating systems, and even if you consider yourself a rock star, don’t try to implement your own cryptography. If you ...",qwen2.5:latest,2025-11-02 10:25:58,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Adapting to Communication Pathways,Cryptography Engineering Book,"#### Cryptography Engineering Book
Background context on the book by Niels Ferguson, Bruce Schneier, and Tadayoshi Kohno, focusing on cryptographic principles.

:p What does the book ""Cryptography Engineering"" cover?
??x
The book ""Cryptography Engineering"" covers fundamental principles of cryptography, including practical aspects of implementing secure systems. It provides insights into designing and deploying cryptographic solutions in real-world applications.
x??",469,"Finally, understand the importance of defense in depth, make sure you patch your operating systems, and even if you consider yourself a rock star, don’t try to implement your own cryptography. If you ...",qwen2.5:latest,2025-11-02 10:25:58,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Adapting to Communication Pathways,Key Length and Security,"#### Key Length and Security
Background context on key length's impact on security, mentioning AES-256 as an example but acknowledging potential concerns raised by experts.

:p How does key length affect the security of encryption?
??x
Key length significantly affects the security of encryption. Generally, longer keys increase resistance to brute-force attacks because they require more computational effort to crack. However, it's important to note that even with long keys like AES-256, there are potential implementation concerns and evolving cryptographic landscape factors to consider.
x??",596,"Finally, understand the importance of defense in depth, make sure you patch your operating systems, and even if you consider yourself a rock star, don’t try to implement your own cryptography. If you ...",qwen2.5:latest,2025-11-02 10:25:58,4
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Adapting to Communication Pathways,Conway’s Law,"#### Conway’s Law
Background context on Conway’s law as stated by Melvin Conway in his 1968 paper and its application in system design.

:p What is Conway's Law?
??x
Conway's Law states that the structure of a software organization will influence the structure of the systems it builds. In other words, an organization's communication patterns are mirrored in the design of the products or systems they develop.
x??",415,"Finally, understand the importance of defense in depth, make sure you patch your operating systems, and even if you consider yourself a rock star, don’t try to implement your own cryptography. If you ...",qwen2.5:latest,2025-11-02 10:25:58,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Adapting to Communication Pathways,Moore’s Law and Its Impact,"#### Moore’s Law and Its Impact
Background context on Moore's law regarding the doubling of transistors density every two years.

:p What is Moore's Law?
??x
Moore's Law predicts that the number of transistors in a dense integrated circuit doubles approximately every two years, leading to increased computational power and efficiency over time. This has driven significant advancements in technology.
x??",405,"Finally, understand the importance of defense in depth, make sure you patch your operating systems, and even if you consider yourself a rock star, don’t try to implement your own cryptography. If you ...",qwen2.5:latest,2025-11-02 10:25:58,2
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Adapting to Communication Pathways,Evidence for Conway’s Law,"#### Evidence for Conway’s Law
Background context on evidence supporting Conway’s law, including its widespread acceptance and applicability.

:p Can you provide an example of how Conway's Law applies?
??x
Conway's Law can be seen in many real-world scenarios where the organizational structure directly influences system design. For instance, if a company has four teams working independently on different modules of a compiler, it might result in a system with four separate passes instead of a more integrated solution.
x??

---",531,"Finally, understand the importance of defense in depth, make sure you patch your operating systems, and even if you consider yourself a rock star, don’t try to implement your own cryptography. If you ...",qwen2.5:latest,2025-11-02 10:25:58,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Adapting to Communication Pathways,Loose and Tightly Coupled Organizations,"#### Loose and Tightly Coupled Organizations
Background context: The concept of loose and tightly coupled organizations is explored to understand how organizational structure influences the design and quality of software systems. MacCormack, Rusnak, and Baldwin's study looked at different types of organizations, categorizing them as either loosely or tightly coupled.
:p What are some key differences between loosely and tightly coupled organizations?
??x
Loosely coupled organizations are typically more distributed and have less centralized control. They often represent open source communities where teams operate independently. Tightly coupled organizations, on the other hand, are usually colocated with strongly aligned visions and goals, typical of commercial product firms.
x??",787,"But you don’t have to take my word for it: since Conway’s original submission, a lot of work has been done in this area. A number of studies have been carried out to explore the interrelation of organ...",qwen2.5:latest,2025-11-02 10:26:24,7
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Adapting to Communication Pathways,Empirical Study by Microsoft (Windows Vista),"#### Empirical Study by Microsoft (Windows Vista)
Background context: Microsoft conducted an empirical study to understand how its organizational structure impacted the software quality of Windows Vista. The research involved analyzing various metrics related to software quality, such as code complexity, to determine which factors were statistically relevant.
:p What did Microsoft’s empirical study reveal about the impact of organizational structures on software quality?
??x
Microsoft's study found that the metrics associated with organizational structures were the most statistically relevant measures in determining error-proneness. This indicates that the organizational setup significantly influences system quality.
x??",730,"But you don’t have to take my word for it: since Conway’s original submission, a lot of work has been done in this area. A number of studies have been carried out to explore the interrelation of organ...",qwen2.5:latest,2025-11-02 10:26:24,4
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Adapting to Communication Pathways,Amazon and Netflix Example,"#### Amazon and Netflix Example
Background context: Amazon and Netflix are often cited as examples where organizational structure is closely aligned with architecture design. Both companies prioritize small, independent teams to ensure rapid changes and innovation in their systems.
:p How did Amazon and Netflix align their organizational structures with the desired architecture?
??x
Amazon structured its teams around ""two-pizza teams,"" ensuring that no team was too large to be fed by two pizzas. This allowed for smaller, more agile teams that could own the entire lifecycle of their services. Netflix followed a similar approach but emphasized even smaller and more independent teams from the beginning.
x??",713,"But you don’t have to take my word for it: since Conway’s original submission, a lot of work has been done in this area. A number of studies have been carried out to explore the interrelation of organ...",qwen2.5:latest,2025-11-02 10:26:24,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Adapting to Communication Pathways,Impact on System Design,"#### Impact on System Design
Background context: The alignment between organizational structure and system design is crucial for achieving desired qualities in software systems. Studies show that loosely coupled organizations tend to produce more modular, less coupled systems compared to tightly coupled organizations.
:p What evidence supports the claim that organizational structures significantly impact system design?
??x
Studies by MacCormack et al., empirical findings from Microsoft's Windows Vista project, and examples like Amazon and Netflix all support this claim. These demonstrate that the nature of a system created by an organization closely mirrors its structure.
x??",684,"But you don’t have to take my word for it: since Conway’s original submission, a lot of work has been done in this area. A number of studies have been carried out to explore the interrelation of organ...",qwen2.5:latest,2025-11-02 10:26:24,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Adapting to Communication Pathways,Conclusion on Organizational Structure and System Design,"#### Conclusion on Organizational Structure and System Design
Background context: The text concludes by emphasizing the importance of aligning organizational structures with desired architectural qualities to ensure higher quality systems and better software development processes.
:p What is the key takeaway from this section regarding organizational structure and system design?
??x
The key takeaway is that the nature and quality of the systems produced are strongly influenced by the organizational structure. Organizational structure should be designed to align with the desired architecture, leading to more modular, maintainable, and efficient systems.
x??

---",669,"But you don’t have to take my word for it: since Conway’s original submission, a lot of work has been done in this area. A number of studies have been carried out to explore the interrelation of organ...",qwen2.5:latest,2025-11-02 10:26:24,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Adapting to Communication Pathways,Impact of Organizational Structure on Service Design,"#### Impact of Organizational Structure on Service Design

Background context: This concept explains how an organization's structure, specifically communication pathways and geographical boundaries, can influence service design. It discusses the importance of balancing the frequency of communication within a team with the complexity of a service.

:p How does organizational structure impact service design in terms of communication?
??x
Organizational structure significantly impacts service design by influencing the ease and frequency of communication among team members. In a single, geolocated team, fine-grained communication is possible, making it easier to implement changes and refactorings within a service. However, when teams are geographically distributed, coarse-grained communication via tools like video conferencing and email becomes necessary. This can lead to higher costs of coordination and change, potentially resulting in less frequent updates or even the fragmentation of services.

```java
// Example of fine-grained communication for a single team
public class CatalogService {
    void updateItem() {
        // Fine-grained method calls within a service
        ItemRepository.update(item);
        PricingService.calculateNewPrice(item);
    }
}
```

x??",1285,So how does this understanding help us? Let’s look at a few different organizational situations and understand what impact each might have on our system design. Adapting to Communication Pathways Let’...,qwen2.5:latest,2025-11-02 10:26:50,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Adapting to Communication Pathways,Single Team Ownership,"#### Single Team Ownership

Background context: This concept describes how a single, colocated team can own and maintain a service efficiently. It highlights the benefits of frequent communication and shared ownership within such teams.

:p How does a single team's ownership affect the design and implementation of a service?
??x
A single, colocated team owning a service allows for frequent, fine-grained communication and refactoring. This setup is ideal because it keeps the cost of change low, ensuring that changes can be made confidently without significant delays or coordination issues.

```java
// Example of a simple refactoring within a service by a single team
public class CatalogService {
    void updateItem() {
        ItemRepository.update(item);
        PricingService.calculateNewPrice(item);
    }

    // Refactoring to improve performance or add features
    public void optimizeUpdateProcess() {
        item = ItemRepository.getLatestVersionOfItem();
        PricingService.calculateOptimizedPrice(item);
        saveChanges(item);
    }
}
```

x??",1073,So how does this understanding help us? Let’s look at a few different organizational situations and understand what impact each might have on our system design. Adapting to Communication Pathways Let’...,qwen2.5:latest,2025-11-02 10:26:50,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Adapting to Communication Pathways,Geographically Distributed Teams,"#### Geographically Distributed Teams

Background context: This concept explains the challenges and implications of having geographically distributed teams working on a service. It discusses how coarse-grained communication can affect the ease of making changes.

:p How does geographical distribution among team members impact the ease of making changes in a service?
??x
Geographical distribution introduces challenges for frequent, fine-grained communication, leading to higher costs of coordination and change. Teams may rely on coarse-grained communication tools like video conferencing and email, which can make it difficult for individual team members to confidently implement small changes.

```java
// Example of a refactoring in a geographically distributed team with coarse-grained communication
public class CatalogService {
    void updateItem() {
        // Coarse-grained approach due to communication constraints
        ItemRepository.update(item);
        PricingService.calculateNewPrice(item);
        sendUpdateRequest();
    }
}
```

x??",1059,So how does this understanding help us? Let’s look at a few different organizational situations and understand what impact each might have on our system design. Adapting to Communication Pathways Let’...,qwen2.5:latest,2025-11-02 10:26:50,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Adapting to Communication Pathways,Service Decomposition Based on Geographical Boundaries,"#### Service Decomposition Based on Geographical Boundaries

Background context: This concept explains how geographical boundaries can drive the decomposition of services. It suggests that teams should take ownership of parts of a service, allowing for easier and more frequent changes.

:p How do geographical boundaries influence the decision to decompose services?
??x
Geographical boundaries can be used as a guideline for decomposing services into smaller, manageable components. Teams in different locations can specialize in specific parts of a service, taking ownership of those sections. This helps reduce communication costs and allows teams to make changes more frequently without coordination issues.

```java
// Example of service decomposition based on geographical ownership
public class GlobalCatalogService {
    // UK team responsible for this part
    public void updateItemUK() {
        ItemRepositoryUK.update(item);
        PricingServiceUK.calculateNewPrice(item);
    }

    // India team responsible for this part
    public void updateItemIndia() {
        ItemRepositoryIndia.update(item);
        PricingServiceIndia.calculateNewPrice(item);
    }
}
```

x??",1187,So how does this understanding help us? Let’s look at a few different organizational situations and understand what impact each might have on our system design. Adapting to Communication Pathways Let’...,qwen2.5:latest,2025-11-02 10:26:50,7
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Adapting to Communication Pathways,Impact of Organizational Structure on System Modularity,"#### Impact of Organizational Structure on System Modularity

Background context: This concept highlights how the structure of an organization can influence the modularity and coupling of the systems it builds. It suggests that a more loosely coupled organizational structure tends to build more modular, less coupled systems.

:p How does the organizational structure impact the modularity and coupling of built systems?
??x
The organizational structure has a significant impact on the modularity and coupling of the systems being developed. A more loosely coupled organization, characterized by geographically distributed teams, is likely to produce more modular and decoupled systems due to the need for coarse-grained communication. Conversely, a single team that owns many services tends to lean toward tighter integration.

```java
// Example of a service built in a loosely coupled organization
public class GlobalCatalogService {
    // UK team responsible for this part
    public void updateItemUK() {
        ItemRepositoryUK.update(item);
        PricingServiceUK.calculateNewPrice(item);
    }

    // India team responsible for this part
    public void updateItemIndia() {
        ItemRepositoryIndia.update(item);
        PricingServiceIndia.calculateNewPrice(item);
    }
}
```

x??

---",1304,So how does this understanding help us? Let’s look at a few different organizational situations and understand what impact each might have on our system design. Adapting to Communication Pathways Let’...,qwen2.5:latest,2025-11-02 10:26:50,7
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Case Study RealEstate.com.au,Service Ownership,"---
#### Service Ownership
Service ownership means that a specific team is responsible for making changes to the service they own. This includes all aspects of the service from requirements gathering, building, deploying, and maintaining it. It's particularly common with microservices where small teams can take responsibility for smaller services.

This model promotes increased autonomy and speed in delivering updates. Teams have an incentive to create services that are easy to deploy, reducing the risk of ""throwing something over the wall"" since there is no one else to handle deployment.
:p What is service ownership?
??x
Service ownership refers to a team being fully responsible for a specific service, including making changes, building, deploying, and maintaining it. This model increases autonomy and promotes faster delivery by ensuring that teams are incentivized to create services that are easy to manage and deploy.
x??",937,"Service Ownership What do I mean by  service ownership ? In general, it means that the team owning a service is responsible for making changes to that service. The team should feel free to restructure...",qwen2.5:latest,2025-11-02 10:27:11,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Case Study RealEstate.com.au,Drivers for Shared Services,"#### Drivers for Shared Services
There are several reasons why some teams might choose shared service ownership over individual ownership. One common driver is the difficulty or perceived lack of value in splitting a large, monolithic system into smaller services. This can be particularly challenging when the cost of refactoring exceeds the potential benefits.

Another reason could be structural challenges within traditional IT organizations where team boundaries are often aligned with technical components rather than features.
:p What are drivers for shared service ownership?
??x
Drivers for shared service ownership include difficulties in splitting large monolithic systems into smaller services due to high costs, or a perception that such refactoring does not provide sufficient benefits. Another driver is structural challenges within traditional IT organizations where teams are often aligned around technical components rather than features.
x??",960,"Service Ownership What do I mean by  service ownership ? In general, it means that the team owning a service is responsible for making changes to that service. The team should feel free to restructure...",qwen2.5:latest,2025-11-02 10:27:11,7
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Case Study RealEstate.com.au,Feature Teams,"#### Feature Teams
Feature teams aim to drive the development of a set of related functionalities by working across different layers and boundaries. This approach aims to maintain focus on end results and ensure cohesive, well-integrated work.

In contrast to traditional IT structures, feature teams can operate in an environment where multiple teams handle UI, application logic, and databases separately, leading to less integrated development efforts.
:p What are feature teams?
??x
Feature teams are small groups of developers who collaborate to deliver a set of related functionalities by working across different layers (UI, application logic, database) and boundaries. This approach aims to maintain focus on the end result and ensure cohesive, well-integrated work.

In traditional IT structures, there might be separate teams for UI, application logic, and databases, leading to less integrated development efforts.
x??",929,"Service Ownership What do I mean by  service ownership ? In general, it means that the team owning a service is responsible for making changes to that service. The team should feel free to restructure...",qwen2.5:latest,2025-11-02 10:27:11,7
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Case Study RealEstate.com.au,Challenges with Large Monolithic Systems,"#### Challenges with Large Monolithic Systems
One of the main challenges in large monolithic systems is the difficulty or high cost associated with splitting them into smaller, more manageable services. This can make it harder for organizations to adopt microservices architectures effectively.

This challenge often leads teams to consider merging existing teams or finding alternative strategies to manage and refactor their systems.
:p What challenges do large monolithic systems face?
??x
Large monolithic systems face the challenge of high costs or difficulty in splitting them into smaller, more manageable services. This makes it challenging for organizations to adopt microservices architectures effectively.

This challenge often leads teams to consider merging existing teams or finding alternative strategies to manage and refactor their systems.
x??

---",866,"Service Ownership What do I mean by  service ownership ? In general, it means that the team owning a service is responsible for making changes to that service. The team should feel free to restructure...",qwen2.5:latest,2025-11-02 10:27:11,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Case Study RealEstate.com.au,Microservices and Business Domain Alignment,"#### Microservices and Business Domain Alignment
In a microservices architecture, services are modeled after business domains rather than technical ones. This approach aims to align teams with these business domains, enhancing their ability to retain customer focus and see features through to completion due to holistic understanding and ownership of technology associated with the service.
:p How does modeling microservices after business domains benefit team alignment?
??x
By aligning teams along business domains, the team is more likely to understand and own all aspects of a service, which helps in maintaining a customer focus and seeing features through development. This reduces the risk of misalignment between technology and business needs.
x??",757,"Everyone can change every service, every piece of code. The role of the service custodians here becomes much more complex, if the role exists at all. Unfortunately, I rarely see functioning custodians...",qwen2.5:latest,2025-11-02 10:27:31,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Case Study RealEstate.com.au,Delivery Bottlenecks and Shared Services,"#### Delivery Bottlenecks and Shared Services
A common reason for adopting microservices is to avoid delivery bottlenecks where one team holds up progress due to high workloads or production issues. In such scenarios, other teams might have to wait while changes are made, which can lead to delays in feature development.
:p What strategies can be used to avoid delivery bottlenecks without resorting to shared services?
??x
Strategies include:
1. **Waiting**: Moving on to other tasks if the feature is less critical or waiting out the backlog.
2. **Adding Resources**: Increasing the team size temporarily to help with the workload.
3. **Service Splitting**: Dividing large services into smaller, more focused ones that are easier to manage.
4. **Internal Open Source Model**: Encouraging collaboration and contribution within the organization for shared services.
x??",870,"Everyone can change every service, every piece of code. The role of the service custodians here becomes much more complex, if the role exists at all. Unfortunately, I rarely see functioning custodians...",qwen2.5:latest,2025-11-02 10:27:31,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Case Study RealEstate.com.au,Standardization vs. Flexibility,"#### Standardization vs. Flexibility
Standardizing a technology stack can improve cross-team collaboration by making it easier for team members to understand and modify each other’s code. However, this standardization comes with trade-offs such as reduced flexibility in adopting the best solution for specific tasks.
:p How does standardization affect the ability of teams to adopt the right solutions?
??x
Standardization helps ensure consistency across the organization, making it easier for team members to understand and modify each other’s code. However, it can also limit the team's ability to adopt the most appropriate technology or solution for a specific task, leading to inefficiencies.
x??",702,"Everyone can change every service, every piece of code. The role of the service custodians here becomes much more complex, if the role exists at all. Unfortunately, I rarely see functioning custodians...",qwen2.5:latest,2025-11-02 10:27:31,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Case Study RealEstate.com.au,Internal Open Source Model,"#### Internal Open Source Model
When internal open source is implemented, a small group of people are considered core committers who have full control over a project and can merge changes. This model encourages collaboration and sharing of knowledge across teams.
:p How does the internal open source model benefit team collaboration?
??x
The internal open source model benefits team collaboration by fostering a culture of shared ownership and contribution, which helps in managing shared services more effectively. Core committers act as stewards, ensuring that changes are made consistently and correctly.
x??",612,"Everyone can change every service, every piece of code. The role of the service custodians here becomes much more complex, if the role exists at all. Unfortunately, I rarely see functioning custodians...",qwen2.5:latest,2025-11-02 10:27:31,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Case Study RealEstate.com.au,Splitting Services to Avoid Bottlenecks,"#### Splitting Services to Avoid Bottlenecks
Splitting large services into smaller ones can help manage workloads and avoid bottlenecks. For example, if a service handles both music catalogs and ringtones, splitting it could allow specific teams to own certain functionalities, reducing the risk of delays.
:p How does splitting services into smaller components address delivery bottlenecks?
??x
Splitting large services into smaller ones ensures that workloads are distributed more evenly among teams. This reduces the likelihood of a single team becoming a bottleneck for overall progress, as different teams can focus on specific parts of the system without being held up by others.
x??

---",694,"Everyone can change every service, every piece of code. The role of the service custodians here becomes much more complex, if the role exists at all. Unfortunately, I rarely see functioning custodians...",qwen2.5:latest,2025-11-02 10:27:31,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Case Study RealEstate.com.au,Custodians of Code,"#### Custodians of Code
Background context explaining that within open source projects, there are committers who own and maintain the codebase. In an organization, similar patterns can be applied where trusted individuals (core team) have commit rights and untrusted contributors submit changes through pull requests or pairing up.
:p Who typically makes changes to an open source project?
??x
Core committers make changes, while others can request changes by submitting pull requests. These core committers are responsible for maintaining the code quality and consistency.
x??",577,"They are the custodians of the code. If you want a change to an open source project, you either ask one of the committers to make the change for you, or else you make the change yourself and send them...",qwen2.5:latest,2025-11-02 10:27:53,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Case Study RealEstate.com.au,Core Committers vs Untrusted Committers,"#### Core Committers vs Untrusted Committers
Explanation of how in a team or organization setting, separating trusted (core) committers from untrusted contributors helps maintain code quality and consistency. This separation is essential when changes need to be reviewed before they are integrated into the main codebase.
:p How does an organization differentiate between core committers and untrusted contributors?
??x
Core committers are typically the original developers or key members who have commit rights and are trusted to make significant changes. Untrusted contributors, on the other hand, submit changes through pull requests or by pairing up with trusted team members for code reviews.
x??",701,"They are the custodians of the code. If you want a change to an open source project, you either ask one of the committers to make the change for you, or else you make the change yourself and send them...",qwen2.5:latest,2025-11-02 10:27:53,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Case Study RealEstate.com.au,Role of Core Committers,"#### Role of Core Committers
Explanation that core committers need to ensure changes adhere to coding guidelines and maintain the overall quality and consistency of the codebase. They act as gatekeepers, reviewing and approving submitted patches.
:p What is the role of core committers in an internal open source model?
??x
Core committers are responsible for ensuring that any changes submitted by untrusted contributors follow the established coding standards and guidelines. They review and approve these changes to maintain code quality and consistency.
x??",561,"They are the custodians of the code. If you want a change to an open source project, you either ask one of the committers to make the change for you, or else you make the change yourself and send them...",qwen2.5:latest,2025-11-02 10:27:53,2
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Case Study RealEstate.com.au,Gatekeepers and Their Responsibilities,"#### Gatekeepers and Their Responsibilities
Explanation that gatekeepers need to spend time reviewing and approving patches, which can be time-consuming. Good gatekeepers communicate clearly and encourage good behavior from submitters, while bad gatekeepers may misuse their power or engage in unnecessary conflicts.
:p What does a gatekeeper do in an internal open source model?
??x
Gatekeepers review and approve patches submitted by untrusted contributors. They ensure the changes are consistent with the codebase's guidelines and quality standards. Good gatekeepers foster collaboration, while bad gatekeepers can misuse their power or cause friction.
x??",659,"They are the custodians of the code. If you want a change to an open source project, you either ask one of the committers to make the change for you, or else you make the change yourself and send them...",qwen2.5:latest,2025-11-02 10:27:53,7
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Case Study RealEstate.com.au,Maturity of Services,"#### Maturity of Services
Explanation that allowing contributions from outside the core team depends on the maturity of the service. Less mature services have a higher risk of accepting subpar changes, so they may limit external contributions until the core structure is stable.
:p When should untrusted committers be allowed to contribute to an internal open source model?
??x
Untrusted committers should be allowed to contribute when the service is sufficiently mature and its core structure is well-defined. This ensures that submitted patches are of high quality and do not disrupt existing functionalities.
x??",615,"They are the custodians of the code. If you want a change to an open source project, you either ask one of the committers to make the change for you, or else you make the change yourself and send them...",qwen2.5:latest,2025-11-02 10:27:53,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Case Study RealEstate.com.au,Tooling for Internal Open Source Model,"#### Tooling for Internal Open Source Model
Explanation that proper tooling, such as distributed version control systems with pull request features, is essential for managing contributions in an internal open source model. Inline commenting tools can facilitate discussions and improve the patch review process.
:p What kind of tooling is necessary for supporting an internal open source model?
??x
Distributed version control systems that support pull requests are crucial. These tools help manage code changes and provide a platform for inline comments and discussions, improving the review process.
x??

---",610,"They are the custodians of the code. If you want a change to an open source project, you either ask one of the committers to make the change for you, or else you make the change yourself and send them...",qwen2.5:latest,2025-11-02 10:27:53,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Case Study RealEstate.com.au,Bounded Contexts and Team Structures,"#### Bounded Contexts and Team Structures

Background context explaining that bounded contexts are crucial for defining service boundaries, which then influence team structures. This alignment helps teams understand domain concepts better and facilitates internal communication among services within a context.

If applicable, add code examples with explanations:
```java
public class ShoppingCartService {
    // Methods to handle add/remove from cart
}
```

:p How do bounded contexts impact the structure of development teams?
??x
Bounded contexts help define where specific domains or parts of a business logic exist. By aligning teams along these contexts, each team can focus on understanding and maintaining the domain-specific services more effectively.

In this example, if we have a `ShoppingCartService`, it is likely part of the consumer web sales bounded context. The team responsible for this service would be more familiar with its specific business logic, such as handling add and remove operations from the shopping cart.
x??",1042,"Finally, you’ll need to make it very easy for a committer to build and deploy your software, and make it available for others. Typically this involves having well- defined build and deployment pipelin...",qwen2.5:latest,2025-11-02 10:28:16,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Case Study RealEstate.com.au,Orphaned Services,"#### Orphaned Services

Background context explaining that smaller services are simpler and may not require frequent changes. However, even services that do not change often still need to have a de facto owner.

:p How should teams handle orphaned services in microservices architecture?
??x
Even if a service like `ShoppingCartService` does not need frequent updates, it still needs an owner who can make necessary changes when required. In the context of bounded contexts, this ownership is naturally assigned based on team alignment with specific domains.

For example, if we have a team aligned with consumer web sales, they would own and manage the `ShoppingCartService`. This ensures that even dormant services are taken care of in case any updates or maintenance are needed.
x??",785,"Finally, you’ll need to make it very easy for a committer to build and deploy your software, and make it available for others. Typically this involves having well- defined build and deployment pipelin...",qwen2.5:latest,2025-11-02 10:28:16,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Case Study RealEstate.com.au,RealEstate.com.au Case Study,"#### RealEstate.com.au Case Study

Background context explaining the multi-faceted nature of REA’s business and how it is divided into different lines of business (LOB) with their own IT delivery teams. Each LOB has a specific focus, such as residential property in Australia.

:p How does REA's structure differ from typical monolithic architectures?
??x
REA's architecture is more modular, broken down into multiple lines of business (LOB), each focusing on specific aspects of the real estate market. Unlike monolithic architectures where all functionalities are bundled together, REA’s approach allows for more targeted and specialized development teams.

For example, one team might handle residential property listings in Australia, while another handles commercial properties or overseas operations. This structure enables better resource allocation and specialization.
x??",880,"Finally, you’ll need to make it very easy for a committer to build and deploy your software, and make it available for others. Typically this involves having well- defined build and deployment pipelin...",qwen2.5:latest,2025-11-02 10:28:16,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Case Study RealEstate.com.au,Build and Deployment Pipelines,"#### Build and Deployment Pipelines

Background context explaining the importance of well-defined build and deployment pipelines and centralized artifact repositories for ease of integration and deployment.

:p What are the benefits of having well-defined build and deployment pipelines?
??x
Well-defined build and deployment pipelines provide a clear path from development to production, making it easier to integrate changes, test them thoroughly, and deploy without disruptions. Centralized artifact repositories ensure that all necessary components are available in one place, reducing the chances of version mismatches or dependency issues.

For example:
```java
public class BuildPipeline {
    public void executeBuild() {
        // Steps to compile code, run tests, and package artifacts
    }
}
```
x??

---",817,"Finally, you’ll need to make it very easy for a committer to build and deploy your software, and make it available for others. Typically this involves having well- defined build and deployment pipelin...",qwen2.5:latest,2025-11-02 10:28:16,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Case Study RealEstate.com.au,Team Rotation and Domain Awareness,"#### Team Rotation and Domain Awareness
People rotate between teams within a line of business (LOB) periodically, but tend to stay for extended periods. This ensures team members build strong domain awareness, improving communication with stakeholders. Each squad owns the entire lifecycle of services it creates, from building to decommissioning.
:p How does rotating people between teams while staying in one LOB support better communication and domain expertise?
??x
By rotating people between teams within a LOB, they can gain broader insights into different parts of the business while maintaining deep knowledge. This rotation allows for cross-pollination of ideas and enhances overall understanding of the entire domain. It fosters stronger relationships with stakeholders across various business areas.
```java
// Pseudocode to simulate team rotation process
public class TeamRotation {
    private Map<String, String> teamMembers = new HashMap<>();
    
    public void rotateTeamMember(String currentTeam, String memberName) {
        // Logic to rotate members between teams within the same LOB
        if (teamMembers.containsKey(memberName)) {
            teamMembers.put(memberName, null); // Remove from current team
            addTeamToMember(currentTeam, memberName);
        }
    }

    private void addTeamToMember(String newTeam, String memberName) {
        // Add to the new team
        teamMembers.put(memberName, newTeam);
    }
}
```
x??",1465,"People rotate between these teams every now and then, but  tend to stay within that line of business for extended periods, ensuring that the team members can build up a strong awareness of that part o...",qwen2.5:latest,2025-11-02 10:28:45,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Case Study RealEstate.com.au,Squad Ownership and Lifecycle Management,"#### Squad Ownership and Lifecycle Management
Each squad is responsible for the entire lifecycle of the services it creates. This includes building, testing, releasing, supporting, and even decommissioning them. A core delivery services team provides advice and guidance to these squads.
:p What are the responsibilities of a squad in REA's architecture?
??x
Squads at REA are responsible for the complete lifecycle of the services they create:
- Building: Design and develop new features or services.
- Testing: Ensure that all developed services meet quality standards through rigorous testing.
- Releasing: Deploy new versions of the service to production environments.
- Supporting: Provide ongoing support and maintenance after deployment.
- Decommissioning: Plan for and manage the end-of-life phase of a service when it is no longer needed.

```java
// Pseudocode illustrating squad responsibilities
public class Squad {
    private String serviceName;
    
    public void buildService() {
        // Logic to design and develop new features
    }
    
    public void testService() {
        // Logic for rigorous testing
    }
    
    public void releaseService() {
        // Deployment logic to production environments
    }
    
    public void supportService() {
        // Ongoing support and maintenance
    }
    
    public void decommissionService() {
        // Plan and manage end-of-life phase
    }
}
```
x??",1432,"People rotate between these teams every now and then, but  tend to stay within that line of business for extended periods, ensuring that the team members can build up a strong awareness of that part o...",qwen2.5:latest,2025-11-02 10:28:45,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Case Study RealEstate.com.au,Autonomous Communication within LOBs,"#### Autonomous Communication within LOBs
Within a line of business, services can communicate freely in any way they see fit. This autonomy is balanced by mandated asynchronous batch communication between different LOBs. This coarse-grained approach aligns with the overall business structure.
:p How does REA ensure both autonomy and standardized communication practices?
??x
REA ensures both autonomy and standardized communication practices through the following methods:
- **Autonomy within LOBs**: Services within a line of business can communicate freely using any method, as decided by their custodians (squad members).
- **Standardized Communication Between LOBs**: All communication between different lines of business is mandated to be asynchronous batch. This coarse-grained approach aligns with the overall structure and allows each LOB significant freedom in managing itself.

```java
// Pseudocode for communication patterns
public class CommunicationPattern {
    private String serviceName;
    
    public void communicateWithinLOB() {
        // Autonomy: Services can use any method they choose.
    }
    
    public void batchCommunicateBetweenLOBs() {
        // Standardized: Use asynchronous batch communication.
    }
}
```
x??",1252,"People rotate between these teams every now and then, but  tend to stay within that line of business for extended periods, ensuring that the team members can build up a strong awareness of that part o...",qwen2.5:latest,2025-11-02 10:28:45,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Case Study RealEstate.com.au,Architecture and Organizational Alignment,"#### Architecture and Organizational Alignment
The architecture of REA is aligned with the way the business operates. Each LOB has its own set of services, and all integration methods are decided by the squads who act as custodians of those services. Between LOBs, communication is mandated to be asynchronous batch.
:p How does REA ensure alignment between its architecture and organizational structure?
??x
REA ensures alignment between its architecture and organizational structure in the following ways:
- **LOB-Specific Services**: Each line of business (LOB) has a set of services tailored to its specific needs.
- **Squad Custodianship**: Squads act as custodians of their services, deciding on integration methods within their LOB.
- **Standardized Communication Between LOBs**: All communication between different LOBs is mandated to be asynchronous batch, ensuring coarse-grained communication that aligns with the overall business structure.

```java
// Pseudocode for architecture and organizational alignment
public class LOBArchitecture {
    private String serviceName;
    
    public void defineLOBServices() {
        // Define services specific to a line of business.
    }
    
    public void batchCommunicateBetweenLOBs() {
        // Ensure standardized asynchronous batch communication between LOBs.
    }
}
```
x??",1339,"People rotate between these teams every now and then, but  tend to stay within that line of business for extended periods, ensuring that the team members can build up a strong awareness of that part o...",qwen2.5:latest,2025-11-02 10:28:45,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Case Study RealEstate.com.au,Autonomous Service Delivery,"#### Autonomous Service Delivery
REA uses a structure that allows for significant autonomy in service delivery. This includes the ability to take services down whenever needed, as long as they can satisfy batch integration with other parts of the business and stakeholders.
:p How does REA's approach enable autonomous service management?
??x
REA’s approach enables autonomous service management by allowing each line of business (LOB) significant freedom in managing its services:
- **Ability to Take Services Down**: LOBs can take their services down whenever they need, as long as the batch integration with other parts of the business and stakeholders is maintained.
- **Batch Integration**: Ensures that even if services are taken down, they can still meet the required integration requirements through asynchronous batch communication.

```java
// Pseudocode for autonomous service management
public class AutonomousServiceManagement {
    private String serviceName;
    
    public void scheduleMaintenance() {
        // Logic to take a service down for maintenance.
    }
    
    public void ensureBatchIntegration() {
        // Ensure that the service satisfies batch integration requirements.
    }
}
```
x??",1222,"People rotate between these teams every now and then, but  tend to stay within that line of business for extended periods, ensuring that the team members can build up a strong awareness of that part o...",qwen2.5:latest,2025-11-02 10:28:45,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Case Study RealEstate.com.au,Rapid Growth and Adaptability,"#### Rapid Growth and Adaptability
From a few services a few years ago, REA now has hundreds of services with more than people. The ability to deliver change at this pace has helped the company achieve significant success locally and is now expanding overseas. The architecture and organizational structure are continuously evolving.
:p How does REA's adaptability impact its growth and success?
??x
REA’s adaptability impacts its growth and success by enabling rapid changes in service delivery:
- **Continuous Growth**: With hundreds of services and more being added, the company can scale and innovate quickly.
- **Local Success**: The ability to deliver change has contributed to significant local market success.
- **Global Expansion**: This adaptability is also driving the expansion into overseas markets.

```java
// Pseudocode for growth and adaptation
public class GrowthAndAdaptation {
    private int numberOfServices;
    
    public void scaleServiceDelivery() {
        // Logic to add more services and scale delivery.
    }
    
    public void expandIntoOverseasMarkets() {
        // Plan and implement expansion strategies.
    }
}
```
x??

---",1164,"People rotate between these teams every now and then, but  tend to stay within that line of business for extended periods, ensuring that the team members can build up a strong awareness of that part o...",qwen2.5:latest,2025-11-02 10:28:45,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,How Much Is Too Much,Conway’s Law in Reverse,"#### Conway’s Law in Reverse
Background context explaining how system design can influence organizational structure, and the anecdotal evidence provided by a client example. The original system was designed for a modest website with content sourced from third parties and later evolved as the business operations shifted towards its online presence.

:p How did the organization's structure evolve in response to changes in the company's primary business focus?
??x
In this case, as the print side of the business diminished and the digital presence grew, the original system design inadvertently laid the path for how the organization grew. The IT department was structured into three channels or divisions aligned with the input, core, and output parts of the business, each having separate delivery teams.

The organizational structure didn't predate the system but grew around it as the company's focus shifted towards its digital presence.
x??",948,"Conway’s Law in Reverse So far, we’ve spoken about how the organization impacts the system design. But what about the reverse? Namely, can a system design change the organization? While I haven’t been...",qwen2.5:latest,2025-11-02 10:29:09,7
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,How Much Is Too Much,Microservices and Developer Mindset,"#### Microservices and Developer Mindset
Background context explaining how microservices can change a developer’s mindset from thinking about code in isolation to considering network boundaries and operational concerns. It also discusses the challenges of moving developers from monolithic systems to microservices.

:p How might moving from a monolithic system to microservices affect a developer's role?
??x
Moving from a monolithic system, where developers may have used just one language and been oblivious to operational concerns, to a microservice environment requires a shift in mindset. Developers must now consider network boundaries, the implications of failure, and operational concerns more carefully.

For example, if a developer writes a function in a microservice that calls another service across a network boundary, they need to think about how failures or delays might impact their system.
x??",911,"Conway’s Law in Reverse So far, we’ve spoken about how the organization impacts the system design. But what about the reverse? Namely, can a system design change the organization? While I haven’t been...",qwen2.5:latest,2025-11-02 10:29:09,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,How Much Is Too Much,Organizational Structure Adaptation,"#### Organizational Structure Adaptation
Background context explaining the difficulty organizations face when trying to adapt their structure to fit new technologies or business models. The example provided discusses the challenges of aligning an organization’s structure with its evolving technology stack.

:p What is a common challenge in adapting organizational structures to align with technological advancements?
??x
A common challenge is that organizational structures often don’t change as quickly as technological and business requirements evolve. For instance, in the described scenario, even though the company shifted from a print-based to an online-focused operation, the existing organizational structure based on legacy systems did not naturally adapt to this new focus.

To overcome such challenges, significant changes in both technology and organization might be required simultaneously.
x??",909,"Conway’s Law in Reverse So far, we’ve spoken about how the organization impacts the system design. But what about the reverse? Namely, can a system design change the organization? While I haven’t been...",qwen2.5:latest,2025-11-02 10:29:09,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,How Much Is Too Much,People Problem,"#### People Problem
Background context citing Gerry Weinberg’s quote that it's always a people problem. The text discusses how microservices can create a more complex operational environment for developers.

:p How does the transition to microservices impact organizational culture according to the provided text?
??x
The transition to microservices is not just about technology but also involves significant cultural shifts within an organization. Developers must now be aware of network boundaries, failure modes, and other operational concerns that were previously irrelevant in monolithic systems.

This shift can be challenging for developers who are used to working in a simpler environment where they could focus solely on coding without worrying about such complexities.
x??

---",787,"Conway’s Law in Reverse So far, we’ve spoken about how the organization impacts the system design. But what about the reverse? Namely, can a system design change the organization? While I haven’t been...",qwen2.5:latest,2025-11-02 10:29:09,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,How Much Is Too Much,Empowering Development Teams,"#### Empowering Development Teams
Background context: The text emphasizes the importance of empowering development teams by increasing their autonomy. This approach can introduce challenges, particularly for teams accustomed to blaming others and potentially facing contractual barriers that prevent them from carrying support pagers.

:p How does increased autonomy impact development teams?
??x
Empowering development teams with more autonomy can increase productivity and innovation but may also lead to accountability issues if team members are not used to full responsibility. Additionally, there might be contractual barriers that hinder the ability of developers to carry support pagers for the systems they maintain.

```java
// Example of a developer taking responsibility in a microservices environment
public class DeveloperResponsibility {
    public void takeOwnership(String system) {
        System.out.println(""Developer is now fully accountable for "" + system);
    }
}
```
x??",994,"Likewise, pushing power into development teams to increase autonomy can be fraught. People who have in the past thrown work over the wall to someone else are accustomed to having someone else to blame...",qwen2.5:latest,2025-11-02 10:29:30,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,How Much Is Too Much,Understanding Staff Appitude to Change,"#### Understanding Staff Appitude to Change
Background context: The text stresses the importance of understanding how your staff feels about changes in the organization, especially when transitioning to microservices. It suggests that pushing change too fast can lead to resistance and failure.

:p How should you approach changing your current staff's practices?
??x
You should understand your staff’s appetite for change and avoid pushing them too quickly. Consider providing a transitional period where a separate team handles frontline support or deployment, allowing developers time to adjust to new practices. Recognize that some changes may require different types of people within the organization.

```java
// Example of phased transition
public class TransitionPlan {
    public void introduceMicroservices() {
        System.out.println(""Introducing microservices with a phased approach."");
        System.out.println(""Separate support team for initial period."");
    }
}
```
x??",990,"Likewise, pushing power into development teams to increase autonomy can be fraught. People who have in the past thrown work over the wall to someone else are accustomed to having someone else to blame...",qwen2.5:latest,2025-11-02 10:29:30,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,How Much Is Too Much,Aligning Service Ownership to Colocated Teams,"#### Aligning Service Ownership to Colocated Teams
Background context: The text discusses aligning service ownership to colocated teams and bounded contexts within the organization. This alignment is crucial for avoiding tension points in microservice architectures.

:p Why is it important to align service ownership with colocated teams?
??x
Aligning service ownership with colocated teams ensures that teams are responsible for their specific parts of the system, which can improve communication and reduce complexity. Bounded contexts help define clear boundaries around services, ensuring that teams understand their responsibilities and avoid overlap.

```java
// Example of defining bounded context in a microservices architecture
public class BoundedContext {
    public void defineContext(String serviceName) {
        System.out.println(""Defining bounded context for "" + serviceName);
    }
}
```
x??",910,"Likewise, pushing power into development teams to increase autonomy can be fraught. People who have in the past thrown work over the wall to someone else are accustomed to having someone else to blame...",qwen2.5:latest,2025-11-02 10:29:30,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,How Much Is Too Much,Challenges at Scale,"#### Challenges at Scale
Background context: The text highlights the challenges that arise as microservice architectures grow beyond simple examples. It mentions handling multiple service failures and managing a large number of services.

:p What are some challenges when dealing with complex microservice architectures?
??x
Challenges include handling the failure of multiple separate services, managing hundreds of services, and coping with the complexity introduced by more microservices than people can handle effectively. These issues require robust monitoring, testing, and management strategies to ensure system reliability and maintainability.

```java
// Example of error handling in a complex microservice architecture
public class ErrorHandling {
    public void handleFailures() {
        try {
            // Simulate service call
            String result = ""Service Result"";
            if (result == null) {
                throw new Exception(""Service failed"");
            }
        } catch (Exception e) {
            System.out.println(""Handling failure: "" + e.getMessage());
        }
    }
}
```
x??",1121,"Likewise, pushing power into development teams to increase autonomy can be fraught. People who have in the past thrown work over the wall to someone else are accustomed to having someone else to blame...",qwen2.5:latest,2025-11-02 10:29:30,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,How Much Is Too Much,Conway's Law and Service Design,"#### Conway's Law and Service Design
Background context: The text introduces Conway’s law, which suggests that the structure of an organization influences the design of its systems. It emphasizes aligning service ownership with organizational boundaries to avoid tension points.

:p How does Conway’s law impact microservice architecture?
??x
Conway’s law states that the structure of a system mirrors the communication structure within an organization. Therefore, designing services should reflect the natural boundaries and team structures of the organization to ensure effective collaboration and reduce conflicts.

```java
// Example of applying Conway's Law in service design
public class ServiceDesign {
    public void alignWithOrganization() {
        System.out.println(""Aligning microservices with organizational structure."");
    }
}
```
x??

---",857,"Likewise, pushing power into development teams to increase autonomy can be fraught. People who have in the past thrown work over the wall to someone else are accustomed to having someone else to blame...",qwen2.5:latest,2025-11-02 10:29:30,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,How Much Is Too Much,Embracing Failure at Scale,"#### Embracing Failure at Scale
Background context: In distributed systems, failure is an inevitable aspect that needs to be planned for and managed. Understanding this principle helps organizations make better trade-offs and design more resilient systems.

:p What are some reasons why embracing failure is important in large-scale systems?
??x
Embracing failure is crucial because it allows us to focus on making our system robust against failures rather than trying to prevent them entirely. By acknowledging that hardware can fail, we can adopt strategies like graceful degradation and planned outages, which are easier to handle than unexpected disruptions.

For example, consider a service that needs to be upgraded. Planning for the failure of an individual node means you can perform in-place upgrades more easily by simply bringing down one instance at a time without affecting the entire system's availability.

```java
// Pseudocode for graceful upgrade
public void upgradeService() {
    // Mark some nodes for upgrade
    List<Node> nodesToUpgrade = getNodesMarkedForUpgrade();

    for (Node node : nodesToUpgrade) {
        shutdownAndReplace(node);
    }
}
```
x??",1180,"And as anyone who has read the  fallacies of distributed computing  can tell you, we know that the network is unreliable. We can do our best to try to limit the causes of failure, but at a certain sca...",qwen2.5:latest,2025-11-02 10:29:57,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,How Much Is Too Much,Hard Drives and Reliability at Scale,"#### Hard Drives and Reliability at Scale
Background context: As the number of hard drives increases in a system, the likelihood of failure also increases. This is due to statistical certainty rather than individual reliability.

:p Why do hard drives fail more often as the number of hard drives increases?
??x
Hard drives fail based on their overall usage and lifespan. Even if each drive has a low probability of failure, with a large number of drives, the combined probability of at least one failing becomes significant due to the law of large numbers.

For instance, if you have 100 drives with a 1% chance of failure per year, the probability that none will fail is (0.99)^100 ≈ 36.7%. Conversely, the probability that at least one drive fails is about 1 - 0.367 = 63.3%.

```java
// Pseudocode for calculating failure probabilities
public double calculateFailureProbability(int drives) {
    double individualFailureRate = 0.01; // Example: 1% per year
    return 1 - Math.pow(1 - individualFailureRate, drives);
}
```
x??",1030,"And as anyone who has read the  fallacies of distributed computing  can tell you, we know that the network is unreliable. We can do our best to try to limit the causes of failure, but at a certain sca...",qwen2.5:latest,2025-11-02 10:29:57,4
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,How Much Is Too Much,Google's Approach to Server Failures,"#### Google's Approach to Server Failures
Background context: Google uses a robust approach to handle server failures by planning for them at the hardware level. This involves using bare motherboards and velcro attachments for hard drives.

:p How does Google’s method of handling server failures demonstrate resilience?
??x
Google’s use of bare motherboards and velcro hard drive attachments shows a pragmatic approach to hardware reliability. By not screwing in hard drives, they can quickly replace them when needed, reducing downtime and maintenance complexity.

This method reduces the time and effort required for planned outages during upgrades or maintenance, making the process more efficient and less disruptive.

```java
// Pseudocode for Google's server rack setup
public void setupServerRack() {
    List<Motherboard> motherboards = getMotherboards();
    for (Motherboard motherboard : motherboards) {
        attachHardDriveByVelcro(motherboard);
    }
}

private void attachHardDriveByVelcro(Motherboard motherboard) {
    // Attach hard drive using velcro
}
```
x??",1082,"And as anyone who has read the  fallacies of distributed computing  can tell you, we know that the network is unreliable. We can do our best to try to limit the causes of failure, but at a certain sca...",qwen2.5:latest,2025-11-02 10:29:57,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,How Much Is Too Much,Trade-offs in Resilience and Cost,"#### Trade-offs in Resilience and Cost
Background context: When building resilient systems, it's important to balance the cost of implementing high-reliability solutions against the potential benefits. Sometimes, simpler designs can be more effective if failure is accounted for.

:p How can assuming that hardware will fail help in making trade-off decisions?
??x
Assuming hardware will fail helps by allowing you to focus on designing systems that are resilient and can handle failures gracefully. This mindset can lead to cost savings because you don't need to invest heavily in highly redundant or fault-tolerant hardware.

For example, using cheaper components with simpler failure handling mechanisms (like velcro-attached hard drives) might be sufficient if the system is designed to tolerate individual node failures without significant impact on overall service availability.

```java
// Pseudocode for cost-effective design
public void designSystem(String systemType) {
    if (systemType.equals(""reporting"")) {
        useCheapComponents();
    } else {
        useExpensiveButReliableComponents();
    }
}

private void useCheapComponents() {
    // Implement using bare motherboards and velcro hard drives
}
```
x??",1228,"And as anyone who has read the  fallacies of distributed computing  can tell you, we know that the network is unreliable. We can do our best to try to limit the causes of failure, but at a certain sca...",qwen2.5:latest,2025-11-02 10:29:57,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,How Much Is Too Much,Cross-Functional Requirements in Distributed Systems,"#### Cross-Functional Requirements in Distributed Systems
Background context: Understanding cross-functional requirements helps in designing systems that meet durability, availability, throughput, and latency goals. However, the level of redundancy and fault tolerance required can vary based on the specific application.

:p How does understanding cross-functional requirements help in system design?
??x
Understanding cross-functional requirements is essential for designing robust distributed systems. By knowing exactly what levels of data durability, service availability, throughput, and acceptable latency are needed, you can make informed decisions about trade-offs between cost, complexity, and reliability.

For example, an application that only runs twice a month might not require the same level of redundancy as one that needs to be available 24/7. Autoscaling systems can be effective but might be overkill for applications with low demand patterns.

```java
// Pseudocode for implementing cross-functional requirements
public void configureSystem(Configuration config) {
    if (config.getRequirementLevel() == ""high"") {
        setupHighAvailability();
    } else if (config.getRequirementLevel() == ""medium"") {
        setupMediumAvailability();
    } else {
        setupBasicAvailability();
    }
}

private void setupHighAvailability() {
    // Implement high availability strategies
}
```
x??

---",1418,"And as anyone who has read the  fallacies of distributed computing  can tell you, we know that the network is unreliable. We can do our best to try to limit the causes of failure, but at a certain sca...",qwen2.5:latest,2025-11-02 10:29:57,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,How Much Is Too Much,Blue/Green Deployments,"---
#### Blue/Green Deployments
Blue/green deployments are a strategy to minimize downtime when updating or maintaining software. In this approach, two identical environments (blue and green) are maintained, with one environment serving live traffic while the other is updated or maintained.

:p How does blue/green deployment work?
??x
In blue/green deployment, you maintain two versions of your application: a current version (blue) that serves live traffic and a new version (green). The traffic can be switched between these environments without affecting users. This allows for smooth updates and maintenance.
```
// Example pseudocode to switch traffic from blue to green
function switchTraffic(newVersion) {
    if (newVersion === 'green') {
        // Redirect all requests to the green environment
        // Start scaling up the green environment gradually
    } else {
        // Continue using the blue environment for traffic
    }
}
```
x??",954,"Likewise, figuring out how to do blue/green deployments to eliminate downtime of a service might make sense for your online ecommerce system, but for your corporate intranet knowledge base it’s probab...",qwen2.5:latest,2025-11-02 10:30:25,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,How Much Is Too Much,User Requirements and Tolerances,"#### User Requirements and Tolerances
Understanding user requirements is crucial for designing a system that meets their needs. You need to identify how much downtime or latency users can tolerate, which depends on the nature of the service (e.g., ecommerce vs. corporate intranet).

:p What factors influence how much failure or latency a system can handle?
??x
The tolerance for failures and latency varies depending on the type of service:
- Ecommerce systems might require minimal downtime to avoid lost sales.
- Corporate intranets may tolerate more downtime since they are not as critical.

To determine these requirements, you need to ask questions about user expectations and help users understand the trade-offs between different levels of service. For instance, if a 90th percentile response time of 2 seconds is required with 200 concurrent connections per second, this is a specific requirement that needs to be documented.
```
// Example code snippet to measure response times
public class PerformanceMonitor {
    public void checkResponseTime(int connectionsPerSecond) {
        // Logic to simulate and measure response time under load
        long responseTime = simulateLoad(connectionsPerSecond);
        if (responseTime > 2000) { // Assuming 2 seconds is the threshold
            System.out.println(""Response time exceeded expected limits."");
        } else {
            System.out.println(""Performance within acceptable range."");
        }
    }

    private long simulateLoad(int connectionsPerSecond) {
        // Simulate load and return response time in milliseconds
        return 1500; // Example value
    }
}
```
x??",1648,"Likewise, figuring out how to do blue/green deployments to eliminate downtime of a service might make sense for your online ecommerce system, but for your corporate intranet knowledge base it’s probab...",qwen2.5:latest,2025-11-02 10:30:25,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,How Much Is Too Much,Response Time/Latency,"#### Response Time/Latency
Response times are critical for user satisfaction, especially on services with high traffic. Measuring these at different levels of concurrency helps understand how the system performs under various loads.

:p How can you measure response time effectively?
??x
To measure response time effectively:
1. Use a representative set of operations to test.
2. Test with varying numbers of concurrent users or connections.
3. Set targets based on percentiles, such as 90th percentile response times.
4. Document the expected performance under different load conditions.

For example, you might define: ""We expect the website to have a 90th percentile response time of 2 seconds when handling 200 concurrent connections per second.""

```java
public class ResponseTimeMonitor {
    public void measureResponseTime(int users) {
        long[] responseTimes = simulateLoad(users);
        
        // Calculate and print the 90th percentile response time
        double[] sortedResponseTimes = Arrays.stream(responseTimes).sorted().toArray();
        int index = (int) Math.round(sortedResponseTimes.length * 0.9);
        System.out.println(""90th Percentile Response Time: "" + sortedResponseTimes[index] + "" ms"");
    }
    
    private long[] simulateLoad(int users) {
        // Simulate load and return an array of response times
        long[] responses = new long[users];
        for (int i = 0; i < users; i++) {
            responses[i] = (long) Math.random() * 2500 + 1000;
        }
        return responses;
    }
}
```
x??",1549,"Likewise, figuring out how to do blue/green deployments to eliminate downtime of a service might make sense for your online ecommerce system, but for your corporate intranet knowledge base it’s probab...",qwen2.5:latest,2025-11-02 10:30:25,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,How Much Is Too Much,Availability,"#### Availability
Availability refers to the uptime of a service. It is important for critical services where downtime could have significant consequences.

:p How do you measure availability?
??x
Measuring availability involves determining how often a system is available to users and ensuring it meets expected reliability standards. While measuring periods of acceptable downtime can be useful from a historical reporting perspective, the primary focus should be on whether the service is accessible when needed.

For example:
- A 99.9% uptime guarantee means only 0.1% of time could be down.
- Historical metrics might show that your system has been available 99.85% of the time over a year.

```java
public class AvailabilityMonitor {
    public double calculateAvailability(long totalUptime, long totalDowntime) {
        return (totalUptime * 100.0 / (totalUptime + totalDowntime));
    }
    
    public void monitorAvailability() {
        // Simulate uptime and downtime
        long uptime = 8760; // Assume the service was up for a year
        long downtime = 24; // Example downtime in hours
        
        double availability = calculateAvailability(uptime, downtime);
        System.out.println(""Current Availability: "" + availability + ""%"");
    }
}
```
x??",1276,"Likewise, figuring out how to do blue/green deployments to eliminate downtime of a service might make sense for your online ecommerce system, but for your corporate intranet knowledge base it’s probab...",qwen2.5:latest,2025-11-02 10:30:25,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,How Much Is Too Much,Data Durability,"#### Data Durability
Data durability involves how much data can be lost and for how long it needs to be retained. The retention period can vary based on the nature of the data (e.g., financial transactions vs. session logs).

:p How do you determine the durability requirements for different types of data?
??x
To determine data durability:
- Financial records often require long-term storage to comply with regulatory requirements.
- Session logs might only need short-term retention, such as a year or less.

For example, your system might specify:
- Keep financial transaction records for 10 years.
- Store session logs for one year for space efficiency.

```java
public class DataDurabilityPolicy {
    public void defineDataRetentionPolicies() {
        // Define policies based on data type
        Map<String, Integer> retentionPeriods = new HashMap<>();
        retentionPeriods.put(""financial_transactions"", 10); // Years
        retentionPeriods.put(""session_logs"", 1); // Year
        
        for (Map.Entry<String, Integer> entry : retentionPeriods.entrySet()) {
            System.out.println(entry.getKey() + "" must be retained for: "" + entry.getValue() + "" years."");
        }
    }
}
```
x??

---",1213,"Likewise, figuring out how to do blue/green deployments to eliminate downtime of a service might make sense for your online ecommerce system, but for your corporate intranet knowledge base it’s probab...",qwen2.5:latest,2025-11-02 10:30:25,7
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Architectural Safety Measures,Degrading Functionality,"#### Degrading Functionality
Background context: The ability to safely degrade functionality is crucial when building a resilient system with microservices. Understanding which parts of the functionality are critical and knowing how to handle failures gracefully can significantly improve system resilience.

:p What is degrading functionality, and why is it important in a microservice architecture?
??x
Degrading functionality refers to designing your system so that if one or more microservices fail, you can still provide some level of service. This approach improves overall system resilience by ensuring that even if parts of the system are down, critical functionalities remain accessible. For example, if a shopping cart service is unavailable, you might still display product listings but hide or disable related UI elements.

In a monolithic application, system health is binary—either it's working or not. However, in microservice architectures, understanding and managing nuanced failure scenarios becomes essential.
x??",1032,"Degrading Functionality An essential part of building a resilient system, especially when your functionality is spread over a number of different microservices that may be up or down, is the ability t...",qwen2.5:latest,2025-11-02 10:30:49,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Architectural Safety Measures,Architectural Safety Measures,"#### Architectural Safety Measures
Background context: To ensure that failures do not cause widespread issues, architectural safety measures such as bulkheads, timeouts, and circuit breakers are crucial. These measures help prevent a single point of failure from cascading into the entire system.

:p What is the purpose of implementing architectural safety measures in a microservice architecture?
??x
The purpose of implementing architectural safety measures is to protect the overall health of the system by preventing the impact of failures from spreading. This includes techniques like setting timeouts, using bulkheads (separate connection pools), and implementing circuit breakers to avoid sending requests to unhealthy services.

By standardizing these practices, you can ensure that your application remains stable even when parts of it fail.
x??",855,"Degrading Functionality An essential part of building a resilient system, especially when your functionality is spread over a number of different microservices that may be up or down, is the ability t...",qwen2.5:latest,2025-11-02 10:30:49,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Architectural Safety Measures,Strangler Application Example,"#### Strangler Application Example
Background context: The example describes a scenario where an online classified ads website was strangling older legacy applications by gradually replacing them with new microservices. This process can introduce vulnerabilities if not managed carefully.

:p What is the risk of failing to manage dependencies properly during the strangler application process?
??x
The risk of failing to manage dependencies properly during the strangler application process is that a single point of failure in an older, poorly maintained service can bring down the entire system. This is because modern services often depend on multiple legacy services, and if one of these dependencies fails, it can cause a cascade of failures throughout the system.

In the example provided, the slow response time of a downstream ad system, which handled only 5% of traffic, brought down the whole site due to its impact on request handling.
x??",951,"Degrading Functionality An essential part of building a resilient system, especially when your functionality is spread over a number of different microservices that may be up or down, is the ability t...",qwen2.5:latest,2025-11-02 10:30:49,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Architectural Safety Measures,Timeout Configuration,"#### Timeout Configuration
Background context: The example highlights the importance of properly configuring timeouts in HTTP connection pools. Misconfigured timeouts can lead to excessive resource consumption and potential cascading failures.

:p What is the issue with misconfigured timeouts in an HTTP connection pool?
??x
Misconfigured timeouts in an HTTP connection pool can cause a build-up of blocked threads when requests are slow or fail to complete within the timeout period. This can lead to excessive resource usage, such as connections and memory, which can exhaust system resources and cause the application to crash.

In the example, setting incorrect timeouts caused the system to peak at around 800 connections in just five minutes, bringing it down.
x??",771,"Degrading Functionality An essential part of building a resilient system, especially when your functionality is spread over a number of different microservices that may be up or down, is the ability t...",qwen2.5:latest,2025-11-02 10:30:49,7
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Architectural Safety Measures,Bulkheads,"#### Bulkheads
Background context: Bulkheads are a method of isolating parts of your application so that failures in one part do not affect others. This is particularly useful when dealing with external dependencies that may be unreliable.

:p What is the role of bulkheads in managing microservice architectures?
??x
The role of bulkheads in managing microservice architectures is to separate different connection pools or service boundaries, ensuring that a failure in one part of the system does not affect other parts. By isolating services, you can control resource consumption and prevent cascading failures.

For example, using separate HTTP connection pools for different downstream services helps limit the impact of a failing service on others.
x??",758,"Degrading Functionality An essential part of building a resilient system, especially when your functionality is spread over a number of different microservices that may be up or down, is the ability t...",qwen2.5:latest,2025-11-02 10:30:49,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Architectural Safety Measures,Circuit Breaker,"#### Circuit Breaker
Background context: A circuit breaker is a pattern used to handle faults by breaking the circuit when a certain threshold of failures is reached. This prevents further requests from being sent to unhealthy services, allowing them to recover.

:p How does a circuit breaker work in the context of microservice architectures?
??x
A circuit breaker works by monitoring the success rate of service calls. When a defined threshold of failures is reached, it trips and stops sending requests to that service. This allows the failing service time to recover without overwhelming the system with more failed requests.

In the example, implementing a circuit breaker would have prevented traffic from being sent to the unhealthy downstream service, allowing it to recover.
x??

---",793,"Degrading Functionality An essential part of building a resilient system, especially when your functionality is spread over a number of different microservices that may be up or down, is the ability t...",qwen2.5:latest,2025-11-02 10:30:49,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Antifragile Organization,Antifragile Organization Concept,"#### Antifragile Organization Concept
Background context explaining that organizations like Netflix and Google have embraced a philosophy of building systems resilient to failure by regularly causing it. This is based on Nassim Taleb’s concept of antifragility, where systems benefit from disorder and failure.
:p What is the core idea behind an antifragile organization?
??x
The core idea is that organizations should be designed in such a way that they not only survive but thrive under conditions of uncertainty, stress, and disruption. This involves actively encouraging failure and building robust mechanisms to handle it. Organizations like Netflix and Google exemplify this by regularly causing system failures to ensure their systems are resilient.
x??",760,"The Antifragile Organization In his book  Antifragile  (Random House), Nassim Taleb talks about things that actually benefit from failure and disorder. Ariel Tseitlin used this concept to coin the con...",qwen2.5:latest,2025-11-02 10:31:15,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Antifragile Organization,Game Days for Simulating Failures,"#### Game Days for Simulating Failures
Background context on how some organizations, including Google during my time there, simulate server failure through game days where teams practice reacting to such events. This is a form of preparing the organization for real-world scenarios.
:p What are game days in the context of simulating failures?
??x
Game days are exercises where organizations simulate system failures and have their teams react to them as if it were a real incident. The goal is to prepare the organization for potential real-world disruptions by providing practice under controlled conditions.
x??",614,"The Antifragile Organization In his book  Antifragile  (Random House), Nassim Taleb talks about things that actually benefit from failure and disorder. Ariel Tseitlin used this concept to coin the con...",qwen2.5:latest,2025-11-02 10:31:15,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Antifragile Organization,DiRT (Disaster Recovery Test) Exercises,"#### DiRT (Disaster Recovery Test) Exercises
Background context on how Google conducts annual disaster recovery tests, including large-scale simulations like earthquakes, to test its resilience against significant disruptions.
:p What are DiRT exercises?
??x
DiRT stands for Disaster Recovery Tests. These are comprehensive exercises where Google simulates large-scale disasters such as earthquakes to test the robustness and readiness of its systems. The objective is to ensure that critical services can withstand major disruptions.
x??",538,"The Antifragile Organization In his book  Antifragile  (Random House), Nassim Taleb talks about things that actually benefit from failure and disorder. Ariel Tseitlin used this concept to coin the con...",qwen2.5:latest,2025-11-02 10:31:15,3
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Antifragile Organization,Chaos Monkey: A Simian Army Tool,"#### Chaos Monkey: A Simian Army Tool
Background context on Netflix's use of tools like Chaos Monkey, which randomly turns off machines during certain hours, ensuring developers are prepared for failures in production environments.
:p What is the Chaos Monkey?
??x
Chaos Monkey is a tool developed by Netflix that periodically and unpredictably terminates random instances within its infrastructure. It runs daily to simulate failure scenarios in production, forcing developers to build resilient systems capable of handling unexpected outages.
x??",548,"The Antifragile Organization In his book  Antifragile  (Random House), Nassim Taleb talks about things that actually benefit from failure and disorder. Ariel Tseitlin used this concept to coin the con...",qwen2.5:latest,2025-11-02 10:31:15,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Antifragile Organization,Latency Monkey: Simulating Network Delays,"#### Latency Monkey: Simulating Network Delays
Background context on how the Latency Monkey simulates slow network connectivity between machines, adding another layer of complexity and unpredictability to test system resilience.
:p What is the Latency Monkey?
??x
Latency Monkey is a tool that simulates network delays by introducing artificial latency between different nodes in the system. It helps test whether applications can handle degraded network conditions without failing catastrophically.
x??",503,"The Antifragile Organization In his book  Antifragile  (Random House), Nassim Taleb talks about things that actually benefit from failure and disorder. Ariel Tseitlin used this concept to coin the con...",qwen2.5:latest,2025-11-02 10:31:15,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Antifragile Organization,Open-Source Simian Army Tools,"#### Open-Source Simian Army Tools
Background context on Netflix making its Simian Army tools, including Chaos Monkey and Latency Monkey, available under an open-source license for other organizations to use and improve upon.
:p Why did Netflix make the Simian Army tools open source?
??x
Netflix made the Simian Army tools, like Chaos Monkey and Latency Monkey, open source to encourage wider adoption and improvement. By sharing these tools, they hope that more organizations can benefit from building resilient systems through regular failure simulation exercises.
x??",571,"The Antifragile Organization In his book  Antifragile  (Random House), Nassim Taleb talks about things that actually benefit from failure and disorder. Ariel Tseitlin used this concept to coin the con...",qwen2.5:latest,2025-11-02 10:31:15,7
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Antifragile Organization,Blameless Culture for Learning from Failures,"#### Blameless Culture for Learning from Failures
Background context on Netflix's emphasis on a blameless culture where mistakes are seen as opportunities for learning rather than punishment. This approach empowers developers and fosters continuous improvement.
:p What is the blameless culture in the context of system failures?
??x
The blameless culture in the context of system failures refers to an organizational mindset that views errors and failures as valuable learning experiences. Instead of punishing individuals, this approach focuses on understanding what went wrong and improving systems to prevent similar issues in the future.
x??",646,"The Antifragile Organization In his book  Antifragile  (Random House), Nassim Taleb talks about things that actually benefit from failure and disorder. Ariel Tseitlin used this concept to coin the con...",qwen2.5:latest,2025-11-02 10:31:15,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Antifragile Organization,Preparing for Distributed System Failures,"#### Preparing for Distributed System Failures
Background context on how distributed systems, due to their nature, are inherently more vulnerable to failure compared to centralized ones. The importance of preparing for such failures is highlighted by companies like Netflix and Google.
:p Why do distributed systems need special preparation?
??x
Distributed systems are more vulnerable to failures because they rely on multiple independent nodes that can fail independently. Unlike centralized systems, where a single point of failure might be easier to manage, distributed systems require robust mechanisms to handle individual node failures gracefully. Preparing for these failures ensures better system reliability and customer satisfaction.
x??

---",753,"The Antifragile Organization In his book  Antifragile  (Random House), Nassim Taleb talks about things that actually benefit from failure and disorder. Ariel Tseitlin used this concept to coin the con...",qwen2.5:latest,2025-11-02 10:31:15,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Circuit Breakers,Timeouts,"---
#### Timeouts
Timeouts are crucial for handling downstream system failures, ensuring that a system does not hang indefinitely while waiting for a response. The right timeout value balances between responsiveness and reliability.

If you wait too long before considering a call to have failed, your system may become unresponsive or slow down significantly. Conversely, if you set the timeout too low, legitimate requests might be considered failures prematurely, leading to incorrect handling of service calls.

To manage this effectively, it is recommended to:
1. **Set default timeouts**: Choose sensible defaults for timeouts that apply across your application.
2. **Log and adapt**: Monitor when timeouts occur and adjust the timeout values as necessary based on observed behavior.

:p What is the importance of setting appropriate timeouts in a downstream system?
??x
Appropriate timeouts are essential to ensure that your system does not hang indefinitely while waiting for responses from other services. Too long of a timeout can slow down your system, making it unresponsive or prone to delays. Conversely, too short of a timeout might incorrectly flag a call as failed before the actual response is received.

Setting default timeouts and monitoring their effectiveness through logging help in fine-tuning these values.
x??",1336,"Timeouts Timeouts are something it is easy to overlook, but in a downstream system they are important to get right. How long can I wait before I can consider a downstream system to actually be down? W...",qwen2.5:latest,2025-11-02 10:31:42,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Circuit Breakers,Circuit Breakers,"#### Circuit Breakers
Circuit breakers are inspired by electrical circuit protection mechanisms. They work by automatically stopping further requests to a downstream service that is experiencing issues, preventing cascading failures across dependent systems.

A common implementation strategy involves:
1. **Blowing the breaker**: When a certain number of consecutive requests fail (e.g., due to timeouts or errors), the circuit breaker trips and stops all subsequent calls.
2. **Graceful recovery**: After some time, the system sends out probes to check if the downstream service has recovered. If enough successful responses are received, the breaker is reset.

:p What is a circuit breaker and how does it work in software systems?
??x
A circuit breaker acts as a protection mechanism for software services by automatically stopping further requests when a downstream service starts experiencing issues. This prevents cascading failures that could otherwise affect multiple parts of your application.

Here’s a simplified example in pseudocode:
```pseudocode
circuitBreaker = new CircuitBreaker()

function sendRequest(url) {
    if (circuitBreaker.isOpen()) {
        return handleFailure()
    }
    
    try {
        response = makeHttpCall(url)
        circuitBreaker.success(response)
    } catch (TimeoutException | IOException e) {
        circuitBreaker.failure(e)
        if (breakerFailedThresholdReached()) {
            circuitBreaker.open()
        }
    }
}

function breakerFailedThresholdReached() {
    // Logic to check if the failure threshold has been reached
}
```

When a certain number of calls fail, the circuit breaker trips, and all subsequent requests are failed immediately without making actual calls. After some time, it attempts to reset by sending test requests.
x??

---",1807,"Timeouts Timeouts are something it is easy to overlook, but in a downstream system they are important to get right. How long can I wait before I can consider a downstream system to actually be down? W...",qwen2.5:latest,2025-11-02 10:31:42,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Bulkheads,Bulkheads Concept,"#### Bulkheads Concept
In software architecture, bulkheads are mechanisms to isolate parts of a system from failure. They help contain and limit the impact of an outage or error in one component to avoid cascading failures throughout the entire system. This is similar to how physical bulkheads on ships protect against flooding.
:p What is a bulkhead in the context of software architecture?
??x
A bulkhead in software architecture is a mechanism used to isolate parts of a system from failure, such as different connection pools or microservices, to limit the impact of an outage or error to that specific part. This helps prevent cascading failures.
x??",656,"Bulkheads In another pattern from  Release It. , Nygard introduces the concept of a  bulkhead  as a way to isolate yourself from failure. In shipping, a bulkhead is a part of the ship that can be seal...",qwen2.5:latest,2025-11-02 10:31:59,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Bulkheads,Connection Pooling for Bulkheads,"#### Connection Pooling for Bulkheads
Connection pooling is a technique where multiple connections are pre-established and managed in a pool so they can be reused rather than being created and destroyed on each request. This reduces overhead and improves performance, but it also means that if one connection pool becomes saturated or exhausted, other pools may also be affected.
:p How can separate connection pools for downstream services help implement bulkheads?
??x
Separate connection pools for downstream services ensure that if one connection pool gets exhausted due to resource constraints or high load, the others remain unaffected. This prevents a single point of failure from impacting the entire system and allows other parts to continue functioning normally.
x??",776,"Bulkheads In another pattern from  Release It. , Nygard introduces the concept of a  bulkhead  as a way to isolate yourself from failure. In shipping, a bulkhead is a part of the ship that can be seal...",qwen2.5:latest,2025-11-02 10:31:59,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Bulkheads,Separation of Concerns for Bulkheads,"#### Separation of Concerns for Bulkheads
Separating concerns into separate microservices can act as bulkheads by reducing the likelihood of an outage in one area affecting another. By teasing apart functionality, you create boundaries that limit the spread of failures.
:p How does separation of concerns help implement bulkheads?
??x
Separation of concerns helps implement bulkheads by creating distinct boundaries between different parts of a system. If one microservice fails or experiences issues, it is isolated from other services, preventing the failure from spreading and impacting the entire system.
x??",613,"Bulkheads In another pattern from  Release It. , Nygard introduces the concept of a  bulkhead  as a way to isolate yourself from failure. In shipping, a bulkhead is a part of the ship that can be seal...",qwen2.5:latest,2025-11-02 10:31:59,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Bulkheads,Circuit Breakers for Bulkheads,"#### Circuit Breakers for Bulkheads
Circuit breakers are an automatic mechanism that can be used to limit the spread of failures in real-time by shutting down a failing service before it causes further damage. They act as a bulkhead, protecting both the consumer and the downstream service from additional calls.
:p What is the role of circuit breakers in implementing bulkheads?
??x
Circuit breakers serve as automatic bulkheads that shut down a failing service to prevent it from causing further damage. They protect both the consumer (by preventing more requests) and the downstream service (from being overwhelmed by additional calls). This helps contain failures and prevents cascading issues.
x??",702,"Bulkheads In another pattern from  Release It. , Nygard introduces the concept of a  bulkhead  as a way to isolate yourself from failure. In shipping, a bulkhead is a part of the ship that can be seal...",qwen2.5:latest,2025-11-02 10:31:59,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Bulkheads,Load Shedding with Hystrix,"#### Load Shedding with Hystrix
Load shedding is a technique where, when resources become saturated, requests are rejected to prevent further overload. The Netflix Hystrix library provides mechanisms for implementing this, allowing bulkheads that reject requests under certain conditions to avoid resource saturation.
:p How does load shedding help in preventing cascading failures?
??x
Load shedding helps prevent cascading failures by rejecting additional requests when resources are saturated. This ensures that important systems don't become overwhelmed and act as bottlenecks for multiple upstream services, thereby maintaining overall system stability.
```java
// Example Hystrix command with fallback logic
import com.netflix.hystrix.HystrixCommand;
import com.netflix.hystrix.HystrixCommandGroupKey;

public class MyCommand extends HystrixCommand<String> {
    public MyCommand() {
        super(HystrixCommandGroupKey.Factory.asKey(""MyCommand""));
    }

    @Override
    protected String run() throws Exception {
        // Logic to execute the command and return a result
        return ""Result"";
    }
}
```
x??",1123,"Bulkheads In another pattern from  Release It. , Nygard introduces the concept of a  bulkhead  as a way to isolate yourself from failure. In shipping, a bulkhead is a part of the ship that can be seal...",qwen2.5:latest,2025-11-02 10:31:59,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Bulkheads,Summary of Bulkheads,"#### Summary of Bulkheads
Bulkheads are crucial in software architecture for containing failures, ensuring that one part of a system does not bring down another. Implementing bulkheads through separation of concerns, connection pooling, and circuit breakers helps maintain the robustness and reliability of complex systems.
:p What are the main benefits of implementing bulkheads in a system?
??x
Implementing bulkheads provides several key benefits: it contains failures to limit their impact, ensures that one part of the system does not bring down another, and prevents cascading failures. This leads to more robust and reliable software architectures capable of handling unexpected issues without total system failure.
x??

---",731,"Bulkheads In another pattern from  Release It. , Nygard introduces the concept of a  bulkhead  as a way to isolate yourself from failure. In shipping, a bulkhead is a part of the ship that can be seal...",qwen2.5:latest,2025-11-02 10:31:59,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Idempotency,Service Isolation,"---
#### Service Isolation
Service isolation refers to the technique of minimizing dependencies between services so that the failure or maintenance of one service does not significantly impact others. This can be achieved through various integration techniques that allow a downstream server to be offline while upstream services continue functioning without disruption.

:p How does increasing isolation between services help?
??x
Increasing isolation between services reduces the need for coordination, allowing teams more autonomy and flexibility in managing their respective services. When services depend less on each other being up, they are less affected by outages or planned downtime.
x??",697,"Isolation The more one service depends on another being up, the more the health of one impacts the ability of the other to do its job. If we can use integration techniques that allow a downstream serv...",qwen2.5:latest,2025-11-02 10:32:22,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Idempotency,Idempotency,"#### Idempotency
Idempotent operations ensure that performing an operation multiple times has no additional effect beyond performing it once. This is particularly useful for recovery scenarios where messages might be retransmitted.

:p What does idempotency mean in the context of service calls?
??x
In the context of service calls, idempotency means that making a call with identical parameters again and again will have no additional effect after the first successful execution. For example, if a call to add points is made multiple times, only one set of points should be added regardless of how many times the call was executed.

Example in XML for adding points:
```xml
<credit>
  <amount>100</amount>
  <forAccount>1234</account>
  <reason>
    <forPurchase>4567</forPurchase>
  </reason>
</credit>
```

This mechanism ensures that even if the message is processed multiple times, only one credit for a specific order will be recorded. This can prevent overprocessing or double-counting.
x??",997,"Isolation The more one service depends on another being up, the more the health of one impacts the ability of the other to do its job. If we can use integration techniques that allow a downstream serv...",qwen2.5:latest,2025-11-02 10:32:22,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Idempotency,HTTP Idempotent Verbs,"#### HTTP Idempotent Verbs
HTTP defines some verbs as idempotent (GET and PUT), meaning they should not change the state of the server when called more than once with the same parameters.

:p Why are certain HTTP verbs considered idempotent?
??x
Certain HTTP verbs like GET and PUT are defined in the HTTP specification to be idempotent, meaning that making multiple identical requests will have the same effect as a single request. For instance, sending a GET request twice will not change the state of the server or result in any additional side effects.

:p How do we ensure our services handle these verbs idempotently?
??x
To ensure HTTP verbs are handled idempotently in your service:
```java
public class IdempotentService {
    private Map<String, Boolean> processedRequests = new HashMap<>();

    public void processRequest(String requestIdentifier) {
        if (!processedRequests.containsKey(requestIdentifier)) {
            // Process the actual business logic here
            processedRequests.put(requestIdentifier, true);
        }
    }
}
```
This ensures that even if a GET or PUT request is sent multiple times, the service will only execute the necessary business logic once.
x??

---",1207,"Isolation The more one service depends on another being up, the more the health of one impacts the ability of the other to do its job. If we can use integration techniques that allow a downstream serv...",qwen2.5:latest,2025-11-02 10:32:22,7
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Load Balancing,Go Bigger,"---
#### Go Bigger
Background context: Vertical scaling involves using more powerful machines to improve performance and throughput. This can be achieved by upgrading a single server with faster CPUs, better I/O, or larger memory. However, this approach has limitations, such as increased costs and potential underutilization of resources.
If applicable, add code examples with explanations:
```java
// Example of scaling up a system by changing the instance type in cloud provider
String newInstanceType = ""m5.xlarge"";
ec2.updateInstanceTypes(instanceId, newInstanceType);
```
:p How does vertical scaling work?
??x
Vertical scaling involves increasing the power of a single machine to handle more load and reduce latency. It can be achieved by upgrading the CPU, memory, or I/O capabilities of an existing server. However, this approach might not always lead to better performance due to resource underutilization.
x??",920,"Scaling We scale our systems in general for one of two reasons. First, to help deal with failure: if we’re worried that something will fail, then having more of it will help, right? Second, we scale f...",qwen2.5:latest,2025-11-02 10:32:37,4
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Load Balancing,Splitting Workloads,"#### Splitting Workloads
Background context: In microservice architectures, it is often beneficial to move multiple services from a single host onto separate hosts to improve throughput and scaling. This can also increase system resiliency by reducing the impact of a single host failure.
:p What is an example of splitting workloads in microservices?
??x
An example of splitting workloads is separating critical functionalities like account management into one service and non-critical queries into another. For instance, if the accounts service handles both customer financial records and report generation, these could be split into separate services to reduce the load on the core functionality.
```java
// Pseudocode for splitting workloads
class AccountsService {
    void manageCustomerAccounts() { /* critical logic */ }
}

class AccountsReportingService {
    void generateReports() { /* non-critical logic */ }
}
```
x??",930,"Scaling We scale our systems in general for one of two reasons. First, to help deal with failure: if we’re worried that something will fail, then having more of it will help, right? Second, we scale f...",qwen2.5:latest,2025-11-02 10:32:37,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Load Balancing,Spreading Your Risk,"#### Spreading Your Risk
Background context: To enhance system resilience, it is important to distribute resources across multiple hosts rather than relying on a single point of failure. This can be achieved by deploying microservices across different servers.
:p How does spreading the risk help in scaling?
??x
Spreading the risk helps by distributing the load and potential failures across multiple hosts. If one host fails, the impact is reduced because other hosts can continue to function without interruption. This approach ensures that no single point of failure can bring down the entire system.
```java
// Example of deploying services on different hosts
List<String> serviceHosts = new ArrayList<>();
serviceHosts.add(""host1"");
serviceHosts.add(""host2"");
serviceHosts.add(""host3"");

for (String host : serviceHosts) {
    deployServiceOn(host);
}
```
x??

---",870,"Scaling We scale our systems in general for one of two reasons. First, to help deal with failure: if we’re worried that something will fail, then having more of it will help, right? Second, we scale f...",qwen2.5:latest,2025-11-02 10:32:37,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Load Balancing,Virtualization and Host Distribution,"#### Virtualization and Host Distribution
In modern computing, a host is often a virtual concept running on physical hardware. Ensuring services are distributed across different hosts can mitigate the risk of outages if one physical box goes down. This practice is crucial for maintaining service reliability.
:p What is a potential issue when all services run on virtual hosts on the same physical box?
??x
When a single physical host fails, multiple services running on its virtual machines (VMs) could go down simultaneously, leading to broader outages.
x??",560,"A simplistic example of this is making sure that you don’t have multiple services on one host, where an outage would impact multiple services. But let’s consider what  host  means. In most situations ...",qwen2.5:latest,2025-11-02 10:32:52,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Load Balancing,SAN and Storage Risks,"#### SAN and Storage Risks
SANs are commonly used in virtualization environments to manage storage. However, they can be a point of failure since they are expensive and designed not to fail but still have the potential for issues.
:p What is a critical risk associated with relying on a single SAN for VM storage?
??x
A single SAN failure can potentially take down all connected VMs, leading to significant downtime if the SAN is unavailable.
x??",446,"A simplistic example of this is making sure that you don’t have multiple services on one host, where an outage would impact multiple services. But let’s consider what  host  means. In most situations ...",qwen2.5:latest,2025-11-02 10:32:52,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Load Balancing,Availability Zones in AWS,"#### Availability Zones in AWS
AWS uses availability zones (AZs) as its equivalent of data centers. Services should be distributed across multiple AZs within a region to ensure high availability and resilience against failures at the data center level.
:p Why is it important to distribute services across multiple availability zones in AWS?
??x
Distributing services across multiple availability zones ensures that if one AZ fails, other instances can still handle the load, preventing complete service downtime. This approach leverages redundancy within a region.
x??",569,"A simplistic example of this is making sure that you don’t have multiple services on one host, where an outage would impact multiple services. But let’s consider what  host  means. In most situations ...",qwen2.5:latest,2025-11-02 10:32:52,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Load Balancing,Multi-Region Services,"#### Multi-Region Services
For critical applications, distributing services across multiple regions provides an additional layer of resilience against regional failures or disasters.
:p How does running services in multiple AWS regions enhance reliability?
??x
Running services in multiple AWS regions ensures that if one region experiences a failure (e.g., due to natural disaster), other regions can continue to provide service. This approach minimizes the impact of a regional outage on overall availability.
x??",515,"A simplistic example of this is making sure that you don’t have multiple services on one host, where an outage would impact multiple services. But let’s consider what  host  means. In most situations ...",qwen2.5:latest,2025-11-02 10:32:52,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Load Balancing,Load Balancing for Resilience,"#### Load Balancing for Resilience
Load balancing is essential for achieving resilience by distributing traffic across multiple instances, thereby avoiding single points of failure.
:p What is the primary benefit of using load balancers in a microservice architecture?
??x
The primary benefit of using load balancers is that they distribute incoming requests across multiple service instances, ensuring no single instance bears all the load and providing fault tolerance. This helps maintain service availability even if some instances fail.
x??

---",550,"A simplistic example of this is making sure that you don’t have multiple services on one host, where an outage would impact multiple services. But let’s consider what  host  means. In most situations ...",qwen2.5:latest,2025-11-02 10:32:52,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Load Balancing,Load Balancing Overview,"#### Load Balancing Overview
Background context: Load balancing is a technique used to distribute network traffic across multiple instances of a service, thereby improving performance and reliability. Different types of load balancers exist—hardware appliances and software-based solutions like mod_proxy—which share common capabilities such as distributing incoming requests and managing instance health.

:p What are the key features that all types of load balancers typically have?
??x
All types of load balancers distribute calls to one or more instances based on a specific algorithm, remove unhealthy instances, and add them back when they become healthy. They often provide additional useful features such as SSL termination.
x??",736,"Figure 11-4. An example of a load balancing approach to scale the number of customer service instances Load balancers come in all shapes and sizes, from big and expensive hardware appliances to softwa...",qwen2.5:latest,2025-11-02 10:33:16,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Load Balancing,SSL Termination,"#### SSL Termination
Background context: SSL (Secure Sockets Layer) termination is a common feature in load balancing where the load balancer handles the SSL decryption for inbound HTTPS requests before passing the traffic to backend instances, which can then communicate over HTTP. This reduces the overhead on individual hosts.

:p What is the primary benefit of SSL termination at a load balancer?
??x
The main advantage of SSL termination at a load balancer is that it simplifies the setup of individual hosts by offloading the SSL processing work to the load balancer, thus reducing the overhead and potential security risks associated with handling SSL on every host.
x??",677,"Figure 11-4. An example of a load balancing approach to scale the number of customer service instances Load balancers come in all shapes and sizes, from big and expensive hardware appliances to softwa...",qwen2.5:latest,2025-11-02 10:33:16,4
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Load Balancing,VLAN for Enhanced Security,"#### VLAN for Enhanced Security
Background context: Using Virtual Local Area Networks (VLANs) can enhance security by isolating microservices instances. A VLAN allows requests from outside it to come only via a router, which acts as an SSL-terminating load balancer.

:p How does using a VLAN improve the security of a microservice?
??x
Using a VLAN improves security by ensuring that all external communication to the microservice occurs over HTTPS, but internally, services communicate over HTTP. This approach limits exposure to potential man-in-the-middle attacks and keeps sensitive internal communications secure.
x??",623,"Figure 11-4. An example of a load balancing approach to scale the number of customer service instances Load balancers come in all shapes and sizes, from big and expensive hardware appliances to softwa...",qwen2.5:latest,2025-11-02 10:33:16,3
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Load Balancing,AWS Elastic Load Balancers (ELBs),"#### AWS Elastic Load Balancers (ELBs)
Background context: AWS offers elastic load balancers (ELBs) as a service for managing traffic distribution across multiple instances. These can be used with security groups or VPCs to implement VLAN-like isolation.

:p How does an AWS ELB work in the context of microservices?
??x
An AWS ELB works by distributing incoming requests from clients to one or more backend instances based on predefined rules. It can perform SSL termination, ensuring that external HTTPS traffic is decrypted and then forwarded as HTTP to the backend services running inside a specific VPC or security group.
x??",630,"Figure 11-4. An example of a load balancing approach to scale the number of customer service instances Load balancers come in all shapes and sizes, from big and expensive hardware appliances to softwa...",qwen2.5:latest,2025-11-02 10:33:16,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Load Balancing,Software vs. Hardware Load Balancers,"#### Software vs. Hardware Load Balancers
Background context: While hardware load balancers offer robust capabilities, they can be challenging to automate and manage. Software-based load balancers, such as mod_proxy, provide flexibility in configuration but may require more manual setup.

:p What is an advantage of using software load balancers over hardware ones?
??x
An advantage of using software load balancers is their ease of automation and reconfiguration. They allow teams to make changes without physical hardware constraints, providing greater flexibility and potentially reducing the risk of single points of failure.
x??",634,"Figure 11-4. An example of a load balancing approach to scale the number of customer service instances Load balancers come in all shapes and sizes, from big and expensive hardware appliances to softwa...",qwen2.5:latest,2025-11-02 10:33:16,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Load Balancing,Configuration Management for Load Balancers,"#### Configuration Management for Load Balancers
Background context: Properly managing configurations for load balancers ensures that they are stored in version control and can be applied automatically, maintaining consistency across environments.

:p Why is it important to store load balancer configurations in version control?
??x
Storing load balancer configurations in version control helps maintain a history of changes, facilitates collaboration among team members, and allows for the automatic application of configurations, ensuring consistent setup across different deployment stages.
x??",598,"Figure 11-4. An example of a load balancing approach to scale the number of customer service instances Load balancers come in all shapes and sizes, from big and expensive hardware appliances to softwa...",qwen2.5:latest,2025-11-02 10:33:16,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Load Balancing,Handling Single Point of Failure,"#### Handling Single Point of Failure
Background context: Even with load balancing, microservices may still have single points of failure if they rely on external services or persistent data stores. Ensuring that critical components are redundant is crucial.

:p How can a database be a single point of failure in a microservice architecture?
??x
A database can become a single point of failure because multiple instances of a microservice might access the same database, but there could be only one instance running the database service. If this single database fails, it can bring down all services relying on it.
x??

---",624,"Figure 11-4. An example of a load balancing approach to scale the number of customer service instances Load balancers come in all shapes and sizes, from big and expensive hardware appliances to softwa...",qwen2.5:latest,2025-11-02 10:33:16,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Caching in HTTP,Worker-Based Systems,"#### Worker-Based Systems
Worker-based systems involve a collection of instances that work on a shared backlog of tasks. This model is particularly effective for batch processing, asynchronous jobs, and peak load scenarios where additional instances can be spun up on demand.

These systems are well-suited for tasks such as image thumbnail processing, sending emails, or generating reports. They leverage existing software like message brokers (e.g., RabbitMQ) or state management tools (e.g., Zookeeper) to manage the work queue.

:p What is a key characteristic of worker-based systems?
??x
Worker-based systems are characterized by having multiple instances that process tasks from a shared backlog, often used for batch and asynchronous jobs. They can scale both throughput and resiliency.
x??",798,"Worker-Based Systems Load balancing isn’t the only way to have multiple instances of your service share load and reduce fragility. Depending on the nature of the operations, a worker-based system coul...",qwen2.5:latest,2025-11-02 10:33:42,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Caching in HTTP,Scaling Challenges,"#### Scaling Challenges
As systems grow, they may need to be rearchitected to handle increased load. Jeff Dean’s advice suggests designing for 10 times growth but planning to rewrite before reaching 100 times the original load.

Scaling issues often arise from monolithic applications that become too complex and slow as user traffic increases. A common approach is to break down a monolith into smaller services, choose better data stores, or adopt new architectural patterns like event-driven systems.

:p What should developers consider when scaling their applications?
??x
Developers should consider the need for rearchitecting their applications as they scale beyond certain thresholds. This might involve breaking a monolithic application into microservices, optimizing database usage, or switching to more scalable architectural styles such as event-based processing.
x??",878,"Worker-Based Systems Load balancing isn’t the only way to have multiple instances of your service share load and reduce fragility. Depending on the nature of the operations, a worker-based system coul...",qwen2.5:latest,2025-11-02 10:33:42,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Caching in HTTP,Gilt’s Scaling Experience,"#### Gilt’s Scaling Experience
Gilt started with a simple monolithic Rails application that served well for two years but eventually faced scaling issues when the business grew. To handle increased load, they had to redesign their application.

The redesign process often involves making significant changes, such as splitting an existing monolith into microservices or choosing new technologies and platforms.

:p What happened at Gilt regarding their initial application?
??x
Gilt initially used a simple monolithic Rails application that was effective for two years. However, with increased business success came higher load, which eventually required the company to redesign its application.
x??",699,"Worker-Based Systems Load balancing isn’t the only way to have multiple instances of your service share load and reduce fragility. Depending on the nature of the operations, a worker-based system coul...",qwen2.5:latest,2025-11-02 10:33:42,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Caching in HTTP,Redesign Scenarios,"#### Redesign Scenarios
Redesigns can involve various actions: splitting apart an existing monolith, using different data stores, or adopting new technologies and platforms. The goal is often to improve performance, scalability, and maintainability.

For instance, moving from synchronous request/response models to event-based systems can significantly enhance the system's ability to handle load.

:p What are some common actions taken during application redesigns?
??x
Common actions during application redesigns include splitting a monolithic application into microservices, using more efficient data stores, and adopting new architectural patterns such as event-driven systems. These changes aim to improve performance, scalability, and maintainability.
x??",762,"Worker-Based Systems Load balancing isn’t the only way to have multiple instances of your service share load and reduce fragility. Depending on the nature of the operations, a worker-based system coul...",qwen2.5:latest,2025-11-02 10:33:42,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Caching in HTTP,Rearchitecting for Massive Scale,"#### Rearchitecting for Massive Scale
There's a risk that seeing the need to rearchitect at certain scaling thresholds might lead people to build systems with massive scale from the beginning. However, this approach is often unnecessary unless absolutely required.

:p What is a potential pitfall in dealing with scaling?
??x
A potential pitfall is the tendency to over-engineer by building for massive scale right from the start when not necessary. This can be costly and time-consuming without providing immediate benefits.
x??

---",534,"Worker-Based Systems Load balancing isn’t the only way to have multiple instances of your service share load and reduce fragility. Depending on the nature of the operations, a worker-based system coul...",qwen2.5:latest,2025-11-02 10:33:42,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Caching in HTTP,Experimentation and Scaling Priorities,"#### Experimentation and Scaling Priorities
Background context: When starting a new project, it is crucial to prioritize rapid experimentation over upfront scaling efforts. Eric Ries emphasizes this by sharing an anecdote where he spent six months building a product with no demand, whereas he could have tested market interest before investing significant resources.
:p Why should we not focus on scaling up front during the early stages of a new project?
??x
It is important to prioritize understanding if there is any market interest in your product over upfront investments in scalability. By rapidly experimenting and gathering feedback, you can better align with user needs without wasting resources on features that may never be used.
x??",745,"This can be disastrous. At the start of a new project, we often don’t know exactly what we want to build, nor do we know if it will be successful. We need to be able to rapidly experiment, and underst...",qwen2.5:latest,2025-11-02 10:34:05,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Caching in HTTP,Eric Ries' Beach Example,"#### Eric Ries' Beach Example
Background context: Eric Ries suggests a scenario where instead of spending six months building a non-functional prototype, one could simply create a ""404"" link and collect feedback to understand if there was any demand. This approach can provide valuable insights without the overhead of full-scale development.
:p How does the story about Eric Ries and the 404 link illustrate the importance of rapid experimentation?
??x
The story highlights that by creating minimal viable products (MVPs) or simple placeholders, you can gather crucial user feedback early on. This helps in validating your ideas without investing heavily in scalable infrastructure upfront.
x??",695,"This can be disastrous. At the start of a new project, we often don’t know exactly what we want to build, nor do we know if it will be successful. We need to be able to rapidly experiment, and underst...",qwen2.5:latest,2025-11-02 10:34:05,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Caching in HTTP,Scaling Databases,"#### Scaling Databases
Background context: Scaling databases is a critical aspect of building microservices. Different types of databases offer various scaling capabilities, and choosing the right one based on specific use cases ensures efficient resource utilization.
:p Why might we need to consider different forms of database scalability?
??x
Different types of databases provide varying levels of scalability. For instance, some may excel in horizontal scaling (adding more nodes), while others might focus on vertical scaling (improving hardware performance). Understanding the requirements and choosing the appropriate technology can optimize both read and write operations.
x??",685,"This can be disastrous. At the start of a new project, we often don’t know exactly what we want to build, nor do we know if it will be successful. We need to be able to rapidly experiment, and underst...",qwen2.5:latest,2025-11-02 10:34:05,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Caching in HTTP,Availability vs Durability,"#### Availability vs Durability
Background context: Separating the concepts of service availability from data durability is essential for designing robust systems. Ensuring that data remains safe even when services fail requires different strategies than ensuring services remain available at all times.
:p How should we approach the separation between service availability and data durability?
??x
Service availability and data durability are two distinct goals. To ensure data safety, you might use techniques like resilient filesystems or standby databases. For maintaining service availability, focus on redundancy and failover mechanisms. Both aspects need careful planning to balance reliability with performance.
x??",723,"This can be disastrous. At the start of a new project, we often don’t know exactly what we want to build, nor do we know if it will be successful. We need to be able to rapidly experiment, and underst...",qwen2.5:latest,2025-11-02 10:34:05,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Caching in HTTP,Scaling for Reads,"#### Scaling for Reads
Background context: Many services primarily handle read operations, making it essential to efficiently scale these operations without overcomplicating the system architecture. Caching and read replicas are common strategies to achieve this.
:p How can we effectively scale reads in a database?
??x
To scale reads, consider implementing caching mechanisms or using read replicas. For example, in MySQL or PostgreSQL, data can be copied from a primary node to one or more replicas. This setup allows directing read requests to the replica nodes while writes continue on the primary.
```java
// Pseudocode for setting up read replicas in Java
public class DatabaseManager {
    private PrimaryNode primary;
    private List<ReadReplica> replicas;

    public void addReadReplica(ReadReplica replica) {
        this.replicas.add(replica);
    }

    public Object getFromPrimary(String key) {
        return primary.get(key);
    }

    public Object getFromReplica(String key, int index) {
        if (index < 0 || index >= replicas.size()) throw new IndexOutOfBoundsException();
        return replicas.get(index).get(key);
    }
}
```
x??

---",1165,"This can be disastrous. At the start of a new project, we often don’t know exactly what we want to build, nor do we know if it will be successful. We need to be able to rapidly experiment, and underst...",qwen2.5:latest,2025-11-02 10:34:05,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Caching in HTTP,Eventually Consistent Systems,"#### Eventually Consistent Systems

Background context explaining the concept. In distributed systems, achieving immediate consistency can be challenging due to network latency and node failures. An eventually consistent system allows for temporary inconsistency between nodes but guarantees that all nodes will reach a consistent state after some time.

:p What is an eventually consistent system?
??x
An eventually consistent system allows data to become temporarily inconsistent across nodes in a distributed system, ensuring eventual consistency. This approach helps scale systems by allowing nodes to operate independently and asynchronously.
x??",651,"Such a setup is called  eventually consistent , and if you can handle the temporary inconsistency it is a fairly easy and common way to help scale systems. We’ll look into this in more depth shortly w...",qwen2.5:latest,2025-11-02 10:34:26,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Caching in HTTP,Read Replicas for Scaling Reads,"#### Read Replicas for Scaling Reads

Background context explaining the concept. Using read replicas can help distribute read operations, reducing load on the primary database node and improving overall performance.

:p How does using read replicas work?
??x
Read replicas are copies of the primary database that receive data through asynchronous replication. This allows read requests to be distributed across multiple nodes, reducing the load on a single node and improving response times.
x??",495,"Such a setup is called  eventually consistent , and if you can handle the temporary inconsistency it is a fairly easy and common way to help scale systems. We’ll look into this in more depth shortly w...",qwen2.5:latest,2025-11-02 10:34:26,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Caching in HTTP,Sharding for Scaling Writes,"#### Sharding for Scaling Writes

Background context explaining the concept. Sharding involves splitting data across multiple database nodes based on some criteria (like hashing) to handle write operations efficiently.

:p What is sharding?
??x
Sharding is a technique where data is distributed across multiple database nodes, allowing writes and reads to be split and handled by different instances. This can improve performance and scale the system.
x??",455,"Such a setup is called  eventually consistent , and if you can handle the temporary inconsistency it is a fairly easy and common way to help scale systems. We’ll look into this in more depth shortly w...",qwen2.5:latest,2025-11-02 10:34:26,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Caching in HTTP,Handling Queries in Sharded Systems,"#### Handling Queries in Sharded Systems

Background context explaining the concept. In sharded systems, querying data across shards can be complex due to the distributed nature of the data.

:p How do you handle queries in a sharded system?
??x
In a sharded system, individual record lookups are straightforward as they can be routed to specific nodes based on hashing functions. However, queries spanning multiple nodes require handling through asynchronous mechanisms or cached results.
x??",493,"Such a setup is called  eventually consistent , and if you can handle the temporary inconsistency it is a fairly easy and common way to help scale systems. We’ll look into this in more depth shortly w...",qwen2.5:latest,2025-11-02 10:34:26,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Caching in HTTP,Adding Shards to an Existing Cluster,"#### Adding Shards to an Existing Cluster

Background context explaining the concept. Adding new shards can improve write scalability but requires careful handling to avoid downtime.

:p How do you add a shard to an existing cluster?
??x
Adding a shard to an existing cluster involves ensuring data is rebalanced without significant downtime. Modern systems like Cassandra support adding shards live, where the system handles background rebalancing of data.
x??",461,"Such a setup is called  eventually consistent , and if you can handle the temporary inconsistency it is a fairly easy and common way to help scale systems. We’ll look into this in more depth shortly w...",qwen2.5:latest,2025-11-02 10:34:26,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Caching in HTTP,Replication in Cassandra,"#### Replication in Cassandra

Background context explaining the concept. Cassandra provides additional capabilities for handling write volume and resiliency through replication.

:p What is replication in Cassandra?
??x
Replication in Cassandra involves replicating data across multiple nodes to ensure high availability and durability. This can be achieved using a ring, where data is distributed based on tokens.
x??",419,"Such a setup is called  eventually consistent , and if you can handle the temporary inconsistency it is a fairly easy and common way to help scale systems. We’ll look into this in more depth shortly w...",qwen2.5:latest,2025-11-02 10:34:26,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Caching in HTTP,Scaling Databases for Writes,"#### Scaling Databases for Writes

Background context explaining the concept. Scaling write volume often requires more complex strategies than read scaling due to the nature of write operations.

:p What are some challenges in scaling writes?
??x
Scaling writes can be challenging as it involves handling both individual record writes and queries spanning multiple nodes. Techniques like sharding, replication, and using distributed databases like Cassandra help manage these complexities.
x??

---",498,"Such a setup is called  eventually consistent , and if you can handle the temporary inconsistency it is a fairly easy and common way to help scale systems. We’ll look into this in more depth shortly w...",qwen2.5:latest,2025-11-02 10:34:26,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Caching in HTTP,Shared Database Infrastructure,"#### Shared Database Infrastructure
Shared database infrastructure allows one running database to host multiple, independent schemas for each microservice. This approach can reduce the number of machines needed but introduces a significant single point of failure. If the database goes down, it can impact multiple microservices simultaneously.

:p What is shared database infrastructure?
??x
Shared database infrastructure enables hosting multiple independent schemas from different microservices within one running database instance to minimize the need for multiple databases and reduce operational complexity.
This approach leverages a single database to serve data for various services but comes with high risk if that central point of failure occurs.",756,"Shared Database Infrastructure Some types of databases, such as the traditional RDBMS, separate the concept of the database itself and the schema. This means one running database could host multiple, ...",qwen2.5:latest,2025-11-02 10:34:56,7
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Caching in HTTP,CQRS Pattern (Command-Query Responsibility Segregation),"#### CQRS Pattern (Command-Query Responsibility Segregation)
CQRS is an architectural pattern where commands (modifications) and queries (read operations) are handled by separate models. This separation can help in scaling different aspects of the application independently.

:p What does CQRS stand for?
??x
CQRS stands for Command-Query Responsibility Segregation, which separates write (commands) and read (queries) responsibilities to optimize the performance and scalability of an application.
The key idea is that commands are used to modify state, while queries are used to retrieve state. These can be processed synchronously or asynchronously.",652,"Shared Database Infrastructure Some types of databases, such as the traditional RDBMS, separate the concept of the database itself and the schema. This means one running database could host multiple, ...",qwen2.5:latest,2025-11-02 10:34:56,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Caching in HTTP,Event Sourcing in CQRS,"#### Event Sourcing in CQRS
Event sourcing is a technique where commands are stored as events, capturing changes to the state of an application. This approach allows for reconstructing the current state by replaying these events.

:p How does event sourcing work?
??x
In event sourcing, every command issued modifies the system's state and is recorded as an event. These events can then be used to rebuild the current state at any point in time.
For example:
```java
public class OrderService {
    private List<Event> events = new ArrayList<>();

    public void placeOrder(Order order) {
        // Apply business logic
        OrderPlacedEvent placed = new OrderPlacedEvent(order);
        
        // Record the event
        events.add(placed);

        // Optionally, persist this to storage
    }
}
```
x??",813,"Shared Database Infrastructure Some types of databases, such as the traditional RDBMS, separate the concept of the database itself and the schema. This means one running database could host multiple, ...",qwen2.5:latest,2025-11-02 10:34:56,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Caching in HTTP,Query Projections in CQRS,"#### Query Projections in CQRS
Query projections involve creating a view of data based on stored events. These projections can be used for read operations and may differ from the command model.

:p What are query projections?
??x
Query projections are derived from stored events to provide an up-to-date view of application state for read operations, distinct from the command model which handles modifications.
They can be implemented in various ways, such as event sourcing or directly querying events to create a snapshot or projection of data.",547,"Shared Database Infrastructure Some types of databases, such as the traditional RDBMS, separate the concept of the database itself and the schema. This means one running database could host multiple, ...",qwen2.5:latest,2025-11-02 10:34:56,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Caching in HTTP,Caching Strategy,"#### Caching Strategy
Caching is used to store previous results of some operation so that subsequent requests can reuse this stored value without recalculating it. This technique often eliminates unnecessary round-trips to databases or other services for faster responses.

:p What is caching in the context of database operations?
??x
Caching stores the outcome of a previously performed computation or data retrieval, allowing future identical requests to use the cached result instead of re-executing the operation.
For instance:
```java
@Cacheable(value = ""products"", key = ""#product.id"")
public Product getProductById(Long id) {
    // Database query logic here
}
```
x??",676,"Shared Database Infrastructure Some types of databases, such as the traditional RDBMS, separate the concept of the database itself and the schema. This means one running database could host multiple, ...",qwen2.5:latest,2025-11-02 10:34:56,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Caching in HTTP,Benefits of Caching in CQRS,"#### Benefits of Caching in CQRS
In a CQRS setup, caching can provide similar benefits to read replicas but without requiring the backing store for the cache to be identical to the data store used for modifications. Different types of queries and reads can benefit from different caching strategies.

:p How does caching fit into a CQRS architecture?
??x
Caching in CQRS helps by storing results of query operations, reducing load on the database or service layer. It allows serving read-heavy workloads more efficiently.
For example, you might use different types of caches for different queries to improve performance and scalability.",636,"Shared Database Infrastructure Some types of databases, such as the traditional RDBMS, separate the concept of the database itself and the schema. This means one running database could host multiple, ...",qwen2.5:latest,2025-11-02 10:34:56,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Caching in HTTP,Scalability with Caching,"#### Scalability with Caching
Different caching strategies can be used based on the nature of the data being accessed, allowing for efficient handling of varying query loads without overloading the primary data store.

:p What are some ways to scale using caching?
??x
You can use various caching mechanisms like in-memory caches (e.g., Redis), distributed caches, or even simple file-based solutions depending on your specific needs. The key is to cache results effectively where they will be reused.
For instance:
```java
public class CacheManager {
    private final Map<String, Object> cache = new HashMap<>();

    public void cacheResult(String key, Object result) {
        cache.put(key, result);
    }

    public Optional<Object> getCachedResult(String key) {
        return Optional.ofNullable(cache.get(key));
    }
}
```
x??
---",841,"Shared Database Infrastructure Some types of databases, such as the traditional RDBMS, separate the concept of the database itself and the schema. This means one running database could host multiple, ...",qwen2.5:latest,2025-11-02 10:34:56,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Caching in HTTP,Client-Side Caching,"#### Client-Side Caching
Client-side caching allows the client to store cached results, deciding when and if it needs a fresh copy. The downstream service can provide hints for optimal behavior.

:p What is client-side caching?
??x
Client-side caching involves storing data on the client side, allowing the client to decide whether to retrieve a new copy of the data from the server or use the cached version. This can significantly reduce network calls and alleviate load from downstream services. The client gets control over cache invalidation and behavior.

```java
// Example Java code for managing client-side caching using HTTP headers
public class ClientCacheManager {
    public boolean shouldFetchData(String url, String lastModified) {
        // Check if the data is stale based on Last-Modified header or other criteria
        return true; // Assume always fetch for simplicity
    }
}
```
x??",907,"Used well, it can yield huge performance benefits. The reason that HTTP scales so well in handling large numbers of requests is that the concept of caching is built in. Even with a simple monolithic w...",qwen2.5:latest,2025-11-02 10:35:21,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Caching in HTTP,Proxy Caching,"#### Proxy Caching
Proxy caching uses a proxy server to cache responses between the client and the server. Examples include reverse proxies and CDNs like Varnish.

:p What is proxy caching?
??x
Proxy caching involves placing a proxy server between the client and the server to store cached copies of frequently accessed data. This can reduce network latency and improve overall performance by serving cached content directly from the cache instead of making additional requests to the origin server. Common examples include reverse proxies like Squid or Varnish, which can cache any HTTP traffic.

```java
// Example Java code for a simple proxy caching logic using HTTP headers
public class ProxyCacheManager {
    public boolean shouldServeFromCache(String url) {
        // Check if the request is cached based on ETag or other criteria
        return true; // Assume always serve from cache for simplicity
    }
}
```
x??",925,"Used well, it can yield huge performance benefits. The reason that HTTP scales so well in handling large numbers of requests is that the concept of caching is built in. Even with a simple monolithic w...",qwen2.5:latest,2025-11-02 10:35:21,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Caching in HTTP,Server-Side Caching,"#### Server-Side Caching
Server-side caching involves storing data within the server, often using specialized caching systems like Redis or Memcached. This method can simplify caching management but may introduce additional complexity in handling invalidation.

:p What is server-side caching?
??x
Server-side caching refers to storing cached data on the server side, utilizing systems such as Redis or Memcache for efficient storage and retrieval of frequently accessed data. This approach simplifies client-side logic since clients don't need to manage caching directly. However, it requires careful management of cache invalidation to ensure data freshness.

```java
// Example Java code using Redis for server-side caching
public class ServerCacheManager {
    private final Jedis jedis;

    public ServerCacheManager() {
        this.jedis = new Jedis(""localhost"");
    }

    public boolean shouldReturnFromCache(String key) {
        // Check if the data is in cache based on key
        return jedis.exists(key);
    }
}
```
x??

---",1042,"Used well, it can yield huge performance benefits. The reason that HTTP scales so well in handling large numbers of requests is that the concept of caching is built in. Even with a simple monolithic w...",qwen2.5:latest,2025-11-02 10:35:21,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Caching in HTTP,Cache-Control Directives,"#### Cache-Control Directives
Cache-control directives are used to control caching on the client side by specifying how long a resource can be cached. This information is sent as part of an HTTP response header.

:p What do cache-control directives tell clients about resources?
??x
Cache-control directives inform clients whether they should cache a resource and for how long, measured in seconds. For example:
```plaintext
Cache-Control: max-age=3600
```
This means the resource can be cached by the client for up to 3600 seconds (1 hour).

x??",546,"Caching in HTTP HTTP provides some really useful controls to help us cache either on the client side or server side, which are worth understanding even if you aren’t using HTTP itself. First, with HTT...",qwen2.5:latest,2025-11-02 10:35:39,7
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Caching in HTTP,Expires Header,"#### Expires Header
The Expires header is an alternative way to control caching. Instead of specifying a time-to-live, it sets an absolute expiration date and time.

:p What does the Expires header specify?
??x
The Expires header specifies a future date or timestamp after which a resource should be considered stale and fetched again from the server.

x??",356,"Caching in HTTP HTTP provides some really useful controls to help us cache either on the client side or server side, which are worth understanding even if you aren’t using HTTP itself. First, with HTT...",qwen2.5:latest,2025-11-02 10:35:39,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Caching in HTTP,Conditional GET Using ETags,"#### Conditional GET Using ETags
ETags are used to determine if a cached version of a resource is still valid. A conditional GET request can be made using an If-None-Match header with the current ETag value.

:p How does an ETag work in HTTP?
??x
An ETag is a unique identifier for a specific version of a resource. When making a conditional GET request, you can include the If-None-Match header to specify the ETag value of the cached resource. The server checks if this ETag matches the current version. If it does, the server responds with a 304 Not Modified status code, indicating that the client has the latest version.

x??",630,"Caching in HTTP HTTP provides some really useful controls to help us cache either on the client side or server side, which are worth understanding even if you aren’t using HTTP itself. First, with HTT...",qwen2.5:latest,2025-11-02 10:35:39,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Caching in HTTP,Reverse Proxies and CDNs,"#### Reverse Proxies and CDNs
Reverse proxies like Squid or Varnish can cache content transparently between clients and servers. CDNs like AWS CloudFront or Akamai route requests to geographically distributed caches to minimize latency.

:p What are reverse proxies used for in HTTP caching?
??x
Reverse proxies, such as Squid or Varnish, sit between the client and server. They can cache content locally and expire it based on cache-control directives or other rules. This helps reduce load on origin servers and improve performance by serving cached content faster.

x??",572,"Caching in HTTP HTTP provides some really useful controls to help us cache either on the client side or server side, which are worth understanding even if you aren’t using HTTP itself. First, with HTT...",qwen2.5:latest,2025-11-02 10:35:39,7
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Caching in HTTP,Client Caching,"#### Client Caching
HTTP client libraries and caches handle much of the caching work for us, ensuring that requests are optimized for speed and efficiency.

:p How do HTTP client libraries help with caching?
??x
HTTP client libraries manage cache control based on HTTP headers. They use ETags, Cache-Control directives, and expiration times to decide whether to fetch new content or serve a cached version. For example, the following Java code snippet demonstrates how an HTTP client might handle ETags:
```java
HttpRequest request = HttpRequest.newBuilder()
        .uri(URI.create(""https://example.com/resource""))
        .header(""If-None-Match"", ""etag_value"")
        .build();

HttpResponse<String> response = HttpClient.newHttpClient().send(request, BodyHandlers.ofString());
if (response.statusCode() == 304) {
    // Handle 304 Not Modified
} else if (response.statusCode() == 200) {
    String content = response.body();
    // Handle new resource version
}
```

x??

---",979,"Caching in HTTP HTTP provides some really useful controls to help us cache either on the client side or server side, which are worth understanding even if you aren’t using HTTP itself. First, with HTT...",qwen2.5:latest,2025-11-02 10:35:39,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Autoscaling,Write-Behind Caching for Performance Optimization,"#### Write-Behind Caching for Performance Optimization
Write-behind caching can be used to buffer and batch writes, reducing write operations to a canonical source. This is useful during bursts of writes or when there's a high chance of writing the same data multiple times.
:p What is the purpose of using write-behind caching?
??x
The purpose of using write-behind caching is to optimize performance by buffering writes before they are flushed to the downstream canonical source, which can be done at some later point. This approach reduces the load on the canonical source and allows for batching multiple write operations into fewer flushes.
```java
// Example code snippet demonstrating write-behind caching logic
public class WriteBehindCache {
    private BlockingQueue<WriteOperation> writeQueue = new LinkedBlockingQueue<>();

    public void write(String key, String value) {
        // Enqueue the write operation to be processed later
        writeQueue.add(new WriteOperation(key, value));
        // Simulate flushing of writes after a delay
        try {
            Thread.sleep(500); // Simulating cache flush delay
            processWriteQueue();
        } catch (InterruptedException e) {
            Thread.currentThread().interrupt();
        }
    }

    private void processWriteQueue() {
        while (!writeQueue.isEmpty()) {
            WriteOperation operation = writeQueue.poll();
            if (operation != null) {
                // Perform actual write to the canonical source
                System.out.println(""Writing "" + operation.getKey() + "" to canonical source"");
            }
        }
    }

    private static class WriteOperation {
        String key;
        String value;

        public WriteOperation(String key, String value) {
            this.key = key;
            this.value = value;
        }

        public String getKey() {
            return key;
        }
    }
}
```
x??",1933,"Caching for Writes Although you’ll find yourself using caching for reads more often, there are some use cases where caching for writes make sense. For example, if you make use of a write-behind cache,...",qwen2.5:latest,2025-11-02 10:36:07,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Autoscaling,Caching for Resilience in Case of Failure,"#### Caching for Resilience in Case of Failure
Client-side caching can be utilized to serve stale data if the downstream service is unavailable, ensuring availability even when fresh content isn't immediately available. This approach is particularly useful for systems where serving stale data is better than no data at all.
:p How does client-side caching enhance resilience during service failures?
??x
Client-side caching enhances resilience by allowing the system to serve cached but potentially stale data when the downstream service is unavailable. This ensures that users can still access content, albeit with a slight delay in freshness. The key idea is to trade off between availability and freshness of data.
```java
// Example code snippet demonstrating client-side caching logic
public class ClientSideCache {
    private Map<String, String> cache = new HashMap<>();

    public String getData(String key) {
        // Check if the data is available in the cache
        if (cache.containsKey(key)) {
            return cache.get(key);
        } else {
            // Simulate fetching data from the downstream service
            try {
                Thread.sleep(100); // Simulating delay to fetch fresh data
                String data = fetchDataFromDownstreamService(key);
                cache.put(key, data);
                return data;
            } catch (InterruptedException e) {
                Thread.currentThread().interrupt();
                return ""Unavailable""; // Return a placeholder when service is unavailable
            }
        }
    }

    private String fetchDataFromDownstreamService(String key) throws InterruptedException {
        // Simulating fetching fresh data from the downstream service
        Thread.sleep(100); // Delay to simulate network latency
        return ""Fresh Data for Key: "" + key;
    }
}
```
x??",1864,"Caching for Writes Although you’ll find yourself using caching for reads more often, there are some use cases where caching for writes make sense. For example, if you make use of a write-behind cache,...",qwen2.5:latest,2025-11-02 10:36:07,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Autoscaling,Hiding the Origin with Asynchronous Cache Repopulation,"#### Hiding the Origin with Asynchronous Cache Repopulation
In a scenario where an entire cache region fails, ensuring that requests do not hit the origin can be achieved by having the origin populate the cache asynchronously. This prevents overwhelming the origin and ensures stability.
:p How does hiding the origin work to protect against cache failures?
??x
Hiding the origin works by making the origin asynchronously repopulate the cache when needed. If a cache miss occurs, it triggers an event that the origin can use to fill the cache. This approach helps in preventing a thundering herd scenario where many requests hit the origin simultaneously, thus avoiding overwhelming it.
```java
// Example code snippet demonstrating hiding the origin with asynchronous cache repopulation
public class OriginServer {
    private Map<String, String> cache = new HashMap<>();
    private EventQueue eventQueue = new EventQueue();

    public void serveRequest(String key) {
        if (cache.containsKey(key)) {
            // Serve data from cache if available
            System.out.println(""Serving cached data for key: "" + key);
            return;
        } else {
            // Simulate a cache miss and trigger an event to repopulate the cache
            System.out.println(""Cache miss for key: "" + key + "", triggering cache repopulation"");
            eventQueue.enqueue(new CacheRepopulationEvent(key));
        }
    }

    private void processEventQueue() {
        while (!eventQueue.isEmpty()) {
            Event event = eventQueue.dequeue();
            if (event instanceof CacheRepopulationEvent) {
                CacheRepopulationEvent cRepEvent = (CacheRepopulationEvent) event;
                String key = cRepEvent.getKey();
                // Simulate repopulating the cache
                System.out.println(""Populating cache for key: "" + key);
                fetchAndStoreDataFromOrigin(key);
            }
        }
    }

    private void fetchAndStoreDataFromOrigin(String key) {
        // Simulate fetching data from the origin and storing it in the cache
        try {
            Thread.sleep(500); // Simulating delay to fetch fresh data
            String data = fetchDataFromOrigin(key);
            cache.put(key, data);
        } catch (InterruptedException e) {
            Thread.currentThread().interrupt();
        }
    }

    private String fetchDataFromOrigin(String key) throws InterruptedException {
        // Simulate fetching fresh data from the origin
        Thread.sleep(100); // Delay to simulate network latency
        return ""Fresh Data for Key: "" + key;
    }

    public static class CacheRepopulationEvent extends Event {
        private final String key;

        public CacheRepopulationEvent(String key) {
            this.key = key;
        }

        public String getKey() {
            return key;
        }
    }
}
```
x??

---",2896,"Caching for Writes Although you’ll find yourself using caching for reads more often, there are some use cases where caching for writes make sense. For example, if you make use of a write-behind cache,...",qwen2.5:latest,2025-11-02 10:36:07,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Autoscaling,Hiding Origin from Client and Populating Cache Asynchronously,"#### Hiding Origin from Client and Populating Cache Asynchronously
Background context: This approach involves hiding the origin of data from the client and populating caches asynchronously to ensure system resilience. By handling failures quickly, it minimizes resource usage and latency, preventing cascading failures that can occur in a cache.
:p How does this approach help in ensuring system resilience?
??x
This approach helps by rapidly failing requests when parts of the system fail, thus not consuming additional resources or increasing latency. This prevents issues in caches from propagating downstream and gives time to recover the system. For example, if a service fails but is quickly handled without impacting other services, the overall system remains available.
```java
// Example pseudo-code for handling failed requests asynchronously
public void handleRequestAsync(Request request) {
    try {
        // Process request normally
    } catch (Exception e) {
        // Log and fail fast
        log.error(""Failed to process request"", e);
        return;
    }
}
```
x??",1088,"Figure 11-7. Hiding the origin from the client and populating the cache asynchronously This sort of approach may not make sense for some situations, but it can be a way to ensure the system remains up...",qwen2.5:latest,2025-11-02 10:36:26,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Autoscaling,Caching in Microservices,"#### Caching in Microservices
Background context: When implementing caching in a microservice architecture, it is important to consider the complexity introduced by multiple services. The more caches there are between you and the source of fresh data, the harder it can be to ensure that data freshness is maintained.
:p Why should caching be kept simple in a microservice architecture?
??x
Caching should be kept simple because the more caches there are, the harder it becomes to maintain data freshness. In a microservice architecture, multiple services might be involved in handling requests, and each cache layer can introduce delays or inconsistencies. By sticking to one cache, you reduce complexity and improve visibility into where stale data might be coming from.
```java
// Example of a simple caching strategy with one cache layer
@Cacheable(value = ""simpleCache"")
public String fetchDataFromService() {
    // Fetch data logic here
    return data;
}
```
x??",970,"Figure 11-7. Hiding the origin from the client and populating the cache asynchronously This sort of approach may not make sense for some situations, but it can be a way to ensure the system remains up...",qwen2.5:latest,2025-11-02 10:36:26,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Autoscaling,Cache Poisoning: A Cautionary Tale,"#### Cache Poisoning: A Cautionary Tale
Background context: Cache poisoning is a scenario where stale or incorrect data is served to clients, potentially for an extended period. It can happen due to various reasons like bugs in cache header insertion code or misconfiguration of HTTP headers.
:p What is cache poisoning and how does it affect the system?
??x
Cache poisoning occurs when stale data is served indefinitely, leading to incorrect information being displayed to users despite updates occurring elsewhere. In the described scenario, a bug caused some pages to bypass proper cache headers, resulting in stale content being cached for an extended period. This can severely impact user experience and accuracy of information.
```java
// Pseudo-code for identifying and fixing cache poisoning issues
public void fixCachePoisoning() {
    // Identify affected URLs
    List<String> affectedUrls = getAffectedURLs();
    
    // Clear cache manually or through API
    clearCache(affectedUrls);
}
```
x??",1009,"Figure 11-7. Hiding the origin from the client and populating the cache asynchronously This sort of approach may not make sense for some situations, but it can be a way to ensure the system remains up...",qwen2.5:latest,2025-11-02 10:36:26,4
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Autoscaling,Managing Multiple Caches in Public Web Applications,"#### Managing Multiple Caches in Public Web Applications
Background context: In a public web application, multiple caches might be used between the server and the client. These include content delivery networks (CDNs), ISP-level caching, and browser cache. Controlling all these layers can be challenging.
:p How does managing multiple caches affect data freshness?
??x
Managing multiple caches affects data freshness because each layer of caching has its own mechanisms for storing and serving data. Ensuring that all these layers are properly updated is complex. For instance, CDNs might have their own cache invalidation policies, and browser caches can store data indefinitely unless explicitly cleared.
```java
// Example pseudo-code to handle multiple cache levels
public void serveContent() {
    // Check CDN cache first
    if (cdnCache.hasCachedContent(request)) {
        return cdnCache.getAndServeContent();
    }
    
    // Serve content from application server
    Content content = fetchFromApplicationServer(request);
    updateCaches(content, request);  // Update all relevant caches
    return content;
}
```
x??

---",1137,"Figure 11-7. Hiding the origin from the client and populating the cache asynchronously This sort of approach may not make sense for some situations, but it can be a way to ensure the system remains up...",qwen2.5:latest,2025-11-02 10:36:26,7
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Autoscaling,Autoscaling Overview,"#### Autoscaling Overview
Autoscaling involves automatically adjusting the number of active instances in a system based on real-time monitoring data. This is particularly useful for managing load variations and ensuring that resources are used efficiently, both in terms of cost and performance.

:p What does autoscaling involve?
??x
Autoscaling involves dynamically scaling your microservices by adding or removing virtual hosts as needed to manage load and ensure optimal resource utilization. It can be triggered based on known trends or reactive responses to observed changes.
x??",585,"Caching can be very powerful indeed, but you need to understand the full path of data that is cached from source to destination to really appreciate its complexities and what can go wrong. Autoscaling...",qwen2.5:latest,2025-11-02 10:36:51,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Autoscaling,Predictive Scaling,"#### Predictive Scaling
Predictive scaling uses historical data to anticipate when the system will experience increased load and scale resources accordingly. This approach helps in managing peak loads more efficiently by adjusting resources before the actual demand surges.

:p How does predictive scaling work?
??x
Predictive scaling relies on analyzing past usage patterns and trends to determine future resource needs. By using historical data, you can set up autoscaling rules that automatically adjust the number of instances based on expected load increases.
For example, if your system experiences a peak load between 9 a.m. and 5 p.m., you might scale up additional instances at 8:45 a.m. to prepare for this.

```java
public class Autoscaler {
    public void predictAndScale() {
        // Load historical data
        List<Long> pastLoad = getPastLoadData();
        
        // Analyze trends and calculate future load
        long predictedLoad = analyzeTrends(pastLoad);
        
        // Scale resources based on prediction
        if (predictedLoad > currentLoad) {
            launchAdditionalInstances(predictedLoad - currentLoad);
        } else {
            shutDownUnusedInstances(currentLoad - predictedLoad);
        }
    }
    
    private List<Long> getPastLoadData() {
        // Fetch historical load data from monitoring tools
        return Collections.emptyList();
    }
    
    private long analyzeTrends(List<Long> pastLoad) {
        // Implement trend analysis logic here
        return 0;
    }
    
    private void launchAdditionalInstances(long instancesToLaunch) {
        for (int i = 0; i < instancesToLaunch; i++) {
            VirtualHost virtualHost = new VirtualHost();
            virtualHost.launch();
        }
    }
    
    private void shutDownUnusedInstances(long instancesToShutDown) {
        for (int i = 0; i < instancesToShutDown; i++) {
            VirtualHost virtualHost = getUnusedInstance();
            if (virtualHost != null) {
                virtualHost.shutdown();
            }
        }
    }
}
```
x??",2077,"Caching can be very powerful indeed, but you need to understand the full path of data that is cached from source to destination to really appreciate its complexities and what can go wrong. Autoscaling...",qwen2.5:latest,2025-11-02 10:36:51,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Autoscaling,Reactive Scaling,"#### Reactive Scaling
Reactive scaling responds to real-time changes in load conditions, such as spikes or failures. This approach is useful for handling unexpected events and ensuring that the system can quickly adapt to varying demands.

:p How does reactive scaling work?
??x
Reactive scaling involves monitoring the current load and automatically scaling resources up or down based on immediate observed needs. This is particularly useful when you need to handle sudden increases in load, such as a news story breaking.
For example, if AWS detects an increase in traffic, it can launch additional instances to cope with the surge.

```java
public class ReactiveAutoscaler {
    public void reactToLoadChange(long currentLoad) {
        // Check for threshold conditions
        long targetLoad = 50; // Example threshold
        
        if (currentLoad > targetLoad) {
            int newInstancesNeeded = calculateNewInstances(currentLoad - targetLoad);
            launchAdditionalInstances(newInstancesNeeded);
        } else if (currentLoad < targetLoad && currentLoad > 30) { // Adjust based on your thresholds
            shutDownUnusedInstances(targetLoad - currentLoad);
        }
    }
    
    private int calculateNewInstances(long instancesNeeded) {
        // Implement logic to determine how many new instances are needed
        return (int)(instancesNeeded / 10); // Example calculation
    }
}
```
x??",1423,"Caching can be very powerful indeed, but you need to understand the full path of data that is cached from source to destination to really appreciate its complexities and what can go wrong. Autoscaling...",qwen2.5:latest,2025-11-02 10:36:51,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Autoscaling,Cost Optimization with Autoscaling,"#### Cost Optimization with Autoscaling
Cost optimization is a key benefit of using autoscaling. By scaling resources based on actual demand, you can reduce unnecessary costs associated with maintaining idle capacity during periods of low load.

:p How does cost optimization work in autoscaling?
??x
Cost optimization works by allowing the system to dynamically adjust its resource allocation according to real-time usage patterns. This means that when the load is low, fewer instances are running, and when the load increases, more instances can be launched automatically.
For example, on a news site with predictable daily trends, you might scale down resources at night when traffic is lower, saving money without compromising performance during peak hours.

```java
public class CostOptimizer {
    public void optimizeCosts() {
        // Fetch current load data
        long currentLoad = fetchCurrentLoad();
        
        // Determine optimal number of instances based on load and historical trends
        int optimalInstances = determineOptimalInstances(currentLoad);
        
        // Scale resources to match the optimal setting
        scaleResources(optimalInstances);
    }
    
    private long fetchCurrentLoad() {
        // Implement logic to retrieve current system load
        return 0;
    }
    
    private int determineOptimalInstances(long load) {
        if (load < 100) {
            return 5; // Example low load setting
        } else if (load >= 200 && load <= 300) {
            return 10; // Example medium load setting
        } else {
            return 20; // Example high load setting
        }
    }
    
    private void scaleResources(int instancesNeeded) {
        for (int i = currentInstances; i < instancesNeeded; i++) {
            launchInstance();
        }
        while (currentInstances > instancesNeeded) {
            shutDownInstance();
        }
    }
}
```
x??

---",1926,"Caching can be very powerful indeed, but you need to understand the full path of data that is cached from source to destination to really appreciate its complexities and what can go wrong. Autoscaling...",qwen2.5:latest,2025-11-02 10:36:51,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,CAP Theorem,CAP Theorem Overview,"---
#### CAP Theorem Overview
Background context: The CAP theorem, proposed by Eric Brewer and later mathematically proven, dictates that in a distributed system, we can only achieve two out of three desirable properties—consistency, availability, and partition tolerance—simultaneously. This theorem has significant implications for the design of distributed systems.

:p What is the CAP theorem about?
??x
The CAP theorem asserts that in a distributed system, it's impossible to simultaneously achieve consistency, availability, and partition tolerance. We can choose any two out of these three properties but not all three.
x??",630,"CAP Theorem We’d like to have it all, but unfortunately we know we can’t. And when it comes to distributed systems like those we build using microservice architectures, we even have a mathematical pro...",qwen2.5:latest,2025-11-02 10:37:12,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,CAP Theorem,Consistency in Distributed Systems,"#### Consistency in Distributed Systems
Background context: In a distributed system, consistency ensures that every read operation will return the most recent write operation made by any node. However, achieving strong consistency across a network where nodes might fail or be partitioned is challenging.

:p What does consistency mean in distributed systems?
??x
Consistency means that all nodes in a distributed system view the data as being up-to-date and correct, such that if one node has updated some data, any subsequent read operation from another node should return the latest version of that data. This property ensures there are no stale or inconsistent reads.
x??",675,"CAP Theorem We’d like to have it all, but unfortunately we know we can’t. And when it comes to distributed systems like those we build using microservice architectures, we even have a mathematical pro...",qwen2.5:latest,2025-11-02 10:37:12,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,CAP Theorem,Availability in Distributed Systems,"#### Availability in Distributed Systems
Background context: Availability refers to the system's ability to process requests at all times. In a highly available distributed system, every request should receive a response, even if some parts of the system fail.

:p What does availability mean in distributed systems?
??x
Availability means that the system can handle any number of requests without failing or significantly degrading performance. The goal is to ensure that users always get a response from the service, regardless of potential failures within the system.
x??",574,"CAP Theorem We’d like to have it all, but unfortunately we know we can’t. And when it comes to distributed systems like those we build using microservice architectures, we even have a mathematical pro...",qwen2.5:latest,2025-11-02 10:37:12,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,CAP Theorem,Partition Tolerance in Distributed Systems,"#### Partition Tolerance in Distributed Systems
Background context: Partition tolerance ensures that the system continues to operate and maintain consistency even when communication between nodes fails (i.e., network partitions occur).

:p What does partition tolerance mean in distributed systems?
??x
Partition tolerance means that a system should continue to function correctly and consistently, even if some parts of it are isolated from each other due to network failures or disconnections. The system must handle such partitions without compromising its consistency.
x??",576,"CAP Theorem We’d like to have it all, but unfortunately we know we can’t. And when it comes to distributed systems like those we build using microservice architectures, we even have a mathematical pro...",qwen2.5:latest,2025-11-02 10:37:12,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,CAP Theorem,Example Scenario with Two Data Centers,"#### Example Scenario with Two Data Centers
Background context: Consider an inventory service deployed across two data centers with databases in each center that synchronize via replication. This setup can help distribute the load and ensure high availability, but it introduces challenges related to consistency and partition tolerance.

:p In this scenario, what could happen if the network link between the two data centers fails?
??x
If the network link between the two data centers fails, writes made to one database might not be propagated to the other. This can lead to inconsistencies where reads from one node might return different versions of data than those read from another node due to unreplicated updates.
x??",725,"CAP Theorem We’d like to have it all, but unfortunately we know we can’t. And when it comes to distributed systems like those we build using microservice architectures, we even have a mathematical pro...",qwen2.5:latest,2025-11-02 10:37:12,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,CAP Theorem,Handling Partition Failures with Replication,"#### Handling Partition Failures with Replication
Background context: When partition failures occur in a distributed system, replication techniques are used to ensure that changes made in one part of the network can eventually be propagated to other parts. However, this introduces challenges in maintaining consistency across all nodes.

:p How does replication help handle partition failures?
??x
Replication helps by ensuring that data is copied and synchronized between multiple nodes. Even if a node becomes isolated (partitioned), it will still have access to the most recent data from its local or other replicated sources, thus maintaining availability and helping to recover consistency when communication is restored.
x??",731,"CAP Theorem We’d like to have it all, but unfortunately we know we can’t. And when it comes to distributed systems like those we build using microservice architectures, we even have a mathematical pro...",qwen2.5:latest,2025-11-02 10:37:12,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,CAP Theorem,Trade-offs in Distributed Systems,"#### Trade-offs in Distributed Systems
Background context: The CAP theorem highlights that achieving all three properties—consistency, availability, and partition tolerance—is not possible in a distributed system. Designers must choose which two of these properties are most critical for their specific use case.

:p What does the CAP theorem imply about designing distributed systems?
??x
The CAP theorem implies that when designing distributed systems, designers must make trade-offs between consistency, availability, and partition tolerance. For example, a system might prioritize strong consistency over availability in certain scenarios or choose high availability and eventual consistency to better handle network partitions.
x??

---",741,"CAP Theorem We’d like to have it all, but unfortunately we know we can’t. And when it comes to distributed systems like those we build using microservice architectures, we even have a mathematical pro...",qwen2.5:latest,2025-11-02 10:37:12,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,DNS,AP System,"---
#### AP System
Background context: In a scenario where we don’t shut down the inventory service entirely, changes made to data in one data center (DC1) may not be immediately reflected in another data center (DC2). This leads to potentially stale data being served by nodes in DC2.
:p What does an AP system imply?
??x
An AP system refers to a situation where availability and partition tolerance are prioritized over consistency. Changes made in one node might not be seen by other nodes due to network partitions, resulting in some users seeing outdated or inconsistent data.
??x",585,"Sacrificing Consistency Let’s assume that we don’t shut the inventory service down entirely. If I make a change now to the data in DC1, the database in DC2 doesn’t see it. This means any requests made...",qwen2.5:latest,2025-11-02 10:37:39,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,DNS,Eventually Consistent Systems,"#### Eventually Consistent Systems
Background context: To maintain high availability and partition tolerance (AP), systems often accept that data inconsistencies will exist temporarily. This means updates may take time to propagate across all nodes, leading to a state of eventual consistency.
:p What is the definition of an eventually consistent system?
??x
An eventually consistent system accepts that after a network partition heals, all nodes will eventually converge on the same state, but this synchronization might not happen immediately. Users might experience outdated data during the transition period.
??x",617,"Sacrificing Consistency Let’s assume that we don’t shut the inventory service down entirely. If I make a change now to the data in DC1, the database in DC2 doesn’t see it. This means any requests made...",qwen2.5:latest,2025-11-02 10:37:39,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,DNS,Sacrificing Availability for Consistency (CP System),"#### Sacrificing Availability for Consistency (CP System)
Background context: To ensure consistency across multiple nodes, nodes must communicate and coordinate updates. During a partition, if nodes cannot talk to each other, they lose the ability to enforce consistency rules, leading to potential inconsistency issues.
:p What happens when we need to maintain consistency but sacrifice availability?
??x
When maintaining consistency (C), nodes must stay in sync even during partitions. If nodes can't communicate, they might not be able to coordinate updates, resulting in an inability to serve requests until the partition heals and resynchronization occurs.
??x",665,"Sacrificing Consistency Let’s assume that we don’t shut the inventory service down entirely. If I make a change now to the data in DC1, the database in DC2 doesn’t see it. This means any requests made...",qwen2.5:latest,2025-11-02 10:37:39,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,DNS,Distributed Locking Challenges,"#### Distributed Locking Challenges
Background context: Ensuring consistent reads across multiple database nodes requires transactional reads that involve locking mechanisms. However, implementing and managing these locks can lead to significant performance issues and complications.
:p What are the challenges of distributed locking in a multi-node system?
??x
Distributed locking is challenging because it involves coordinating between different nodes, which can be slow due to network latency. Additionally, ensuring consistency during reads and writes requires complex logic that might block other operations, making it difficult to implement reliably across multiple nodes.
??x",682,"Sacrificing Consistency Let’s assume that we don’t shut the inventory service down entirely. If I make a change now to the data in DC1, the database in DC2 doesn’t see it. This means any requests made...",qwen2.5:latest,2025-11-02 10:37:39,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,DNS,Example of Distributed Locking in Java,"#### Example of Distributed Locking in Java
Background context: Implementing a distributed lock mechanism typically involves using external services like Redis or implementing custom solutions with care due to the complexity involved.
:p How can we illustrate the concept of distributed locking in Java?
??x
Implementing a distributed lock in Java might involve using an external service like Redis. Here is an example using Jedis, which provides a way to handle distributed locks:
```java
import redis.clients.jedis.Jedis;
import java.util.concurrent.TimeUnit;

public class DistributedLockExample {
    private final String lockKey = ""lockKey"";
    private static final int EXPIRE_TIME_IN_SECONDS = 30;

    public void acquireLock(Jedis jedis) throws InterruptedException {
        String result = jedis.set(lockKey, ""1"", ""NX"", ""EX"", EXPIRE_TIME_IN_SECONDS);
        if (result == null) {
            // Retry acquiring the lock after a brief delay
            Thread.sleep(500);  // Sleep for half a second before retrying
            acquireLock(jedis);
        }
    }

    public void releaseLock(Jedis jedis) {
        String script = ""if redis.call('get', KEYS[1]) == ARGV[1] then return redis.call('del', KEYS[1]) else return 0 end"";
        jedis.eval(script, 1, lockKey, ""1"");
    }
}
```
The `acquireLock` method attempts to set a key with an expiry time. If the lock is not acquired, it retries after a brief delay. The `releaseLock` method checks if the current client owns the lock before releasing it.
??x",1522,"Sacrificing Consistency Let’s assume that we don’t shut the inventory service down entirely. If I make a change now to the data in DC1, the database in DC2 doesn’t see it. This means any requests made...",qwen2.5:latest,2025-11-02 10:37:39,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,DNS,CAP Theorem Overview,"#### CAP Theorem Overview
Background context explaining the core idea of the CAP theorem. It states that in distributed computer systems, it is impossible to simultaneously guarantee all three of these properties: Consistency (C), Availability (A), and Partition Tolerance (P). Typically, a system can only have at most two of these guarantees.
:p What does the CAP theorem state about distributed systems?
??x
The CAP theorem states that in any distributed system, it is impossible to simultaneously guarantee all three properties: Consistency (C), Availability (A), and Partition Tolerance (P). A system must sacrifice one of these properties. For instance, a system can be both consistent and available but not partition-tolerant.
x??",737,Remember when we talked about distributed transactions in  Chapter 5 ? The core reason they are challenging is because of this problem with ensuring consistency across multiple nodes. Getting multinod...,qwen2.5:latest,2025-11-02 10:38:01,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,DNS,CP vs AP Systems,"#### CP vs AP Systems
Background context discussing the trade-offs between CP and AP systems in distributed environments. Consistent, Partition-Tolerant (CP) systems prioritize consistency over availability and partition tolerance. Available, Partition-Tolerant (AP) systems sacrifice consistency for availability and partition tolerance. The decision on which to choose depends heavily on the specific requirements of the application.
:p What are the trade-offs between CP and AP systems?
??x
The trade-offs between CP and AP systems involve balancing consistency with availability and partition tolerance. CP systems ensure data is consistent across all nodes, but this can reduce availability if a node fails or network partitions occur. AP systems guarantee high availability by allowing stale reads during network partitions but may return inconsistent data. The choice depends on the application's requirements.
x??",921,Remember when we talked about distributed transactions in  Chapter 5 ? The core reason they are challenging is because of this problem with ensuring consistency across multiple nodes. Getting multinod...,qwen2.5:latest,2025-11-02 10:38:01,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,DNS,Eventual Consistency,"#### Eventual Consistency
Background context explaining eventual consistency in distributed systems. In an eventually consistent system, all operations are processed and eventually completed successfully. Eventually, all nodes will reflect the same state; however, until that point, reads might return stale data.
:p What is eventual consistency?
??x
Eventual consistency ensures that all updates to a distributed database will be propagated across all replicas, but there may be a transient period where read operations can return inconsistent results due to concurrent writes. The system guarantees that if no new updates are made, eventually the state of all nodes will converge.
x??",686,Remember when we talked about distributed transactions in  Chapter 5 ? The core reason they are challenging is because of this problem with ensuring consistency across multiple nodes. Getting multinod...,qwen2.5:latest,2025-11-02 10:38:01,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,DNS,Choosing Between AP and CP,"#### Choosing Between AP and CP
Background context discussing how to decide between an AP or CP system based on business requirements. The decision depends on understanding whether a small amount of inconsistency is acceptable (AP) or if strong consistency is necessary at all times (CP).
:p How do you choose between AP and CP systems?
??x
You choose between AP and CP systems by assessing the impact of inconsistency in your application. If occasional stale reads are acceptable, such as in an inventory system where a few minutes of outdated data might be fine, then an AP system is suitable. For financial applications like banking, where every transaction must be consistent to avoid issues like double spending, a CP system is necessary.
x??",747,Remember when we talked about distributed transactions in  Chapter 5 ? The core reason they are challenging is because of this problem with ensuring consistency across multiple nodes. Getting multinod...,qwen2.5:latest,2025-11-02 10:38:01,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,DNS,Microservice Consistency,"#### Microservice Consistency
Background context on how different services within the same application can have varying consistency requirements. An entire system doesn't need to be uniformly CP or AP; individual microservices can have different guarantees based on their specific use cases.
:p How can you design microservices with varying consistency needs?
??x
Designing microservices with varying consistency needs involves identifying critical paths where strong consistency is required and less-critical areas that can tolerate eventual consistency. For example, a catalog service might be AP to allow for better scalability, while an inventory or financial transaction service must be CP to ensure transactions are consistent.
x??",737,Remember when we talked about distributed transactions in  Chapter 5 ? The core reason they are challenging is because of this problem with ensuring consistency across multiple nodes. Getting multinod...,qwen2.5:latest,2025-11-02 10:38:01,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,DNS,Sacrificing Partition Tolerance,"#### Sacrificing Partition Tolerance
Background context on the challenges of building systems without partition tolerance. A system without partition tolerance cannot operate over a network and must run as a single process locally, making it impractical for distributed applications.
:p Can a system sacrifice partition tolerance?
??x
A system can theoretically sacrifice partition tolerance by running as a single process on a local machine, but this makes the system unsuitable for distributed environments. Partition-tolerant systems are necessary when operations need to be performed across multiple nodes, ensuring availability and consistency in case of network partitions.
x??

---",688,Remember when we talked about distributed transactions in  Chapter 5 ? The core reason they are challenging is because of this problem with ensuring consistency across multiple nodes. Getting multinod...,qwen2.5:latest,2025-11-02 10:38:01,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,DNS,Trade-offs in CAP Theorem,"#### Trade-offs in CAP Theorem
Background context explaining the trade-offs between Consistency, Availability, and Partition Tolerance as described by the CAP theorem. It's important to understand that while a system can fully achieve either consistency or availability (and sometimes both) in the absence of partitioning, in the presence of partitions, the choice is forced.

In practice, systems often provide nuanced trade-offs within these constraints. For example, with Cassandra, you can make different decisions for individual calls regarding how strictly consistent they need to be.

:p What are the key aspects of the CAP theorem as discussed?
??x
The key aspects of the CAP theorem revolve around the choices that must be made between Consistency (C), Availability (A), and Partition Tolerance (P). In practice, systems can offer a range of trade-offs within these constraints. For instance, in Cassandra, you have flexibility to decide how consistency is enforced on a per-operation basis.

```java
// Example pseudocode for different levels of read consistency in Cassandra
public class CassandraReadConsistency {
    public void readWithStrongConsistency() {
        // Perform read that blocks until all replicas confirm the value
    }

    public void readWithQuorumConsistency() {
        // Perform read with a quorum of replicas confirming the value
    }

    public void readWithSingleNodeConsistency() {
        // Perform read without waiting for any other replica
    }
}
```
x??",1503,"Really, what we have done is push the trade-offs around the CAP theorem down to individual service capabilities. Another complexity is that neither consistency nor availability is all or nothing. Many...",qwen2.5:latest,2025-11-02 10:38:28,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,DNS,Nuanced Trade-offs in Cassandra,"#### Nuanced Trade-offs in Cassandra
Background context explaining how Cassandra allows for nuanced trade-offs between consistency and availability, offering different levels of quorum settings.

:p How can you make different trade-offs with Cassandra?
??x
You can tailor the level of consistency for each read operation by choosing from various quorum settings. For example:
- You can wait until all replicas confirm the value (strong consistency).
- Or, use a quorum setting that requires confirmation from a majority but is faster.
- Alternatively, you can perform reads without waiting for any other replica to respond.

Here's an illustration:

```java
// Example pseudocode for different read consistency settings in Cassandra
public class CassandraReadConsistency {
    public void readWithStrongConsistency() {
        // Perform read that blocks until all replicas confirm the value
    }

    public void readWithQuorumConsistency() {
        // Perform read with a quorum of replicas confirming the value
    }

    public void readWithSingleNodeConsistency() {
        // Perform read without waiting for any other replica
    }
}
```
x??",1150,"Really, what we have done is push the trade-offs around the CAP theorem down to individual service capabilities. Another complexity is that neither consistency nor availability is all or nothing. Many...",qwen2.5:latest,2025-11-02 10:38:28,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,DNS,Real-World vs. Electronic Systems,"#### Real-World vs. Electronic Systems
Background context explaining how electronic systems often reflect real-world scenarios, where consistency cannot account for all events.

:p How do electronic systems differ from real-world scenarios in terms of consistency?
??x
Electronic systems, while trying to represent the real world, can only approximate it due to their inherent limitations. Real-world events such as physical damage (e.g., an album breaking), are not directly accounted for by our systems unless explicitly coded into them. In contrast, electronic systems assume a controlled environment where every state change is known and recorded.

For example, in inventory management:
- The system might show 99 copies of ""Give Blood"" by The Brakes.
- However, a real-world event (an album breaking) could mean only 98 are actually available.

While an AP system would occasionally need to reconcile discrepancies with users, it is often simpler and more scalable than maintaining strict consistency.

```java
// Example pseudocode for inventory reconciliation in an AP system
public class InventoryReconciliation {
    public void updateInventoryAfterSale() {
        // Update the inventory count but allow for eventual consistency
    }

    public void notifyUserAboutOutOfStock() {
        // Send a notification to the user if their item is out of stock due to real-world events
    }
}
```
x??",1406,"Really, what we have done is push the trade-offs around the CAP theorem down to individual service capabilities. Another complexity is that neither consistency nor availability is all or nothing. Many...",qwen2.5:latest,2025-11-02 10:38:28,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,DNS,Service Discovery in Microservices,"#### Service Discovery in Microservices
Background context explaining how service discovery becomes critical as the number of microservices increases, highlighting its importance for monitoring and inter-service communication.

:p What is the primary challenge with managing multiple microservices?
??x
The primary challenge with managing multiple microservices lies in knowing where each service instance is running. This is crucial for tasks such as monitoring, load balancing, and ensuring that services can communicate effectively. With a growing number of microservices, this becomes increasingly complex.

For example:
- You might need to know which environment hosts the accounts service.
- Microservices that use the accounts service must have reliable information on where it resides.

```java
// Example pseudocode for basic service discovery
public class ServiceDiscovery {
    public String discoverServiceLocation(String serviceName) {
        // Logic to find and return the location of a given service
        return ""http://localhost:8080"";
    }
}
```
x??

---",1077,"Really, what we have done is push the trade-offs around the CAP theorem down to individual service capabilities. Another complexity is that neither consistency nor availability is all or nothing. Many...",qwen2.5:latest,2025-11-02 10:38:28,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,DNS,DNS Service Discovery,"#### DNS Service Discovery
Background context: DNS (Domain Name System) is a fundamental part of the internet, allowing us to map human-readable domain names to IP addresses. In microservices architecture, using DNS for service discovery can help developers quickly locate services without knowing their current IP addresses.

Relevant formulas or data: DNS has Time To Live (TTL) values which determine how long a client can cache an entry before checking if it has been updated. 

:p What is the main advantage of using DNS for service discovery in microservices?
??x
The main advantage of using DNS for service discovery is its simplicity and widespread support across different technology stacks. It allows services to be referred to by names rather than IP addresses, making it easier to manage instances that might change over time.

```java
// Example Java code snippet to resolve a domain name to an IP address
import java.net.InetAddress;
InetAddress ipAddress = InetAddress.getByName(""accounts.musiccorp.com"");
```
x??",1028,"Or perhaps you just want to make it easy for developers in your organization to know what APIs are available so they don’t reinvent the wheel. Broadly speaking, all of these use cases fall under the b...",qwen2.5:latest,2025-11-02 10:38:50,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,DNS,Dynamic DNS Entries for Microservices,"#### Dynamic DNS Entries for Microservices
Background context: In microservices environments, services are often created and destroyed frequently. This requires dynamic updates to the DNS entries so that the latest service instances can be discovered.

Relevant formulas or data: The TTL value in DNS entries affects how long a client can cache an IP address before checking if it has been updated.

:p How does the TTL value affect DNS-based service discovery?
??x
The TTL (Time To Live) value in DNS entries determines how long a client can consider the entry fresh. If the TTL is set too low, clients may frequently request updates from the DNS server, which can increase network traffic. Conversely, if the TTL is set too high, clients might continue to use stale IP addresses for longer than necessary.

```java
// Example Java code snippet to update a DNS entry with a higher TTL
import java.net.InetAddress;
InetAddress ipAddress = InetAddress.getByName(""accounts.musiccorp.com"");
// Update DNS record with a higher TTL value
```
x??",1040,"Or perhaps you just want to make it easy for developers in your organization to know what APIs are available so they don’t reinvent the wheel. Broadly speaking, all of these use cases fall under the b...",qwen2.5:latest,2025-11-02 10:38:50,6
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,DNS,Using Load Balancers with DNS,"#### Using Load Balancers with DNS
Background context: To mitigate the issues of stale entries and frequent updates, one can use DNS to point to a load balancer. The load balancer then manages routing traffic to individual instances.

Relevant formulas or data: Load balancing algorithms distribute incoming requests among available backend servers.

:p How does using a load balancer with DNS improve service discovery in microservices?
??x
Using a load balancer with DNS improves service discovery by decoupling the IP addresses of service instances from their domain names. The load balancer handles routing traffic to individual instances, which can be dynamically managed and updated without affecting the DNS entries.

```java
// Example Java code snippet for configuring a load balancer
import com.example.LoadBalancerConfig;

LoadBalancerConfig config = new LoadBalancerConfig();
config.addServer(""192.0.2.1""); // Add an instance to the load balancer
config.removeServer(""192.0.2.2""); // Remove an instance from the load balancer
```
x??",1045,"Or perhaps you just want to make it easy for developers in your organization to know what APIs are available so they don’t reinvent the wheel. Broadly speaking, all of these use cases fall under the b...",qwen2.5:latest,2025-11-02 10:38:50,8
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,DNS,Different Environments and DNS,"#### Different Environments and DNS
Background context: In a microservices environment, different environments (e.g., development, staging, production) often require distinct configurations for service discovery.

Relevant formulas or data: Domain name templates and separate domain name servers can be used to manage different environments.

:p How can you handle different environments with DNS?
??x
Handling different environments with DNS involves using templates and separate domain name servers. For example, a template like `<servicename>-<environment>.musiccorp.com` can be used for naming conventions. Additionally, having distinct DNS servers for each environment ensures that the same domain names resolve to appropriate instances based on the context.

```java
// Example Java code snippet to set up different environments with DNS
String developmentDomain = ""accounts-dev.musiccorp.com"";
String productionDomain = ""accounts.musiccorp.com"";

// Update DNS records for these domains separately
```
x??",1012,"Or perhaps you just want to make it easy for developers in your organization to know what APIs are available so they don’t reinvent the wheel. Broadly speaking, all of these use cases fall under the b...",qwen2.5:latest,2025-11-02 10:38:50,7
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,DNS,Self-Hosted DNS Management Solutions,"#### Self-Hosted DNS Management Solutions
Background context: While cloud services like Amazon Route53 offer robust solutions, self-hosted DNS management tools can be an alternative.

Relevant formulas or data: The availability and ease of updating entries are key factors in choosing a solution.

:p What is the advantage of using self-hosted DNS management solutions?
??x
The advantage of using self-hosted DNS management solutions is that they provide more control over how updates are handled, which can be crucial in environments where services are frequently created and destroyed. Self-hosted solutions can integrate better with existing infrastructure and processes.

```java
// Example Java code snippet to update a self-hosted DNS entry
import com.example.DNSManager;

DNSManager dnsManager = new DNSManager();
dnsManager.updateRecord(""accounts-dev.musiccorp.com"", ""192.0.2.5""); // Update the IP address for an instance
```
x??

---",942,"Or perhaps you just want to make it easy for developers in your organization to know what APIs are available so they don’t reinvent the wheel. Broadly speaking, all of these use cases fall under the b...",qwen2.5:latest,2025-11-02 10:38:50,3
