# Flashcards: 10A008---Computational-Physics---Rubin-H_-Landau_processed (Part 7)

**Starting Chapter:** 4.1.1 Random Number Generation

---

#### Random Number Generation Overview
Computers cannot generate truly random numbers due to their deterministic nature. Instead, they generate pseudorandom numbers that are designed to mimic randomness but are actually generated by algorithms. These algorithms ensure no correlations among successive numbers.

:p What is the main issue with generating random numbers on a computer?
??x
The main issue is that computers are deterministic devices and cannot inherently produce truly random sequences; instead, they generate pseudorandom numbers which must be designed to appear random but are actually generated by deterministic algorithms. This means that if you know previous numbers in the sequence, it's theoretically possible to predict future ones.
x??

---

#### Linear Congruent Method
The linear congruent method is a common way of generating pseudorandom numbers within a specific range [0, M-1]. The next number in the sequence $r_{i+1}$ is generated using the formula:
$$r_{i+1} = (a \cdot r_i + c) \mod M$$:p How does the linear congruent method generate pseudorandom numbers?
??x
The linear congruent method generates pseudorandom numbers by multiplying the current number $r_i $ by a constant$a $, adding another constant$ c $, and then taking the modulus by$ M$. This ensures that the sequence remains within the range [0, M-1]. The fractional part (remainder) of the result is kept to form the next number in the sequence.
For example:
```python
# Example Python code for linear congruent method
a = 4
c = 1
M = 9
r1 = 3

def linear_congruence(a, c, M, r):
    return (a * r + c) % M

sequence = [r1]
for _ in range(5):
    r_next = linear_congruence(a, c, M, sequence[-1])
    sequence.append(r_next)
print(sequence)  # Output: [3, 4, 8, 6, 7, 2, 0, 1, 5]
```
x??

---

#### Pseudorandom Number Sequence
Using the linear congruent method with specific parameters:
- $a = 4 $-$ c = 1 $-$ M = 9 $- Initial seed$ r_1 = 3$

The sequence generated is: 3, 4, 8, 6, 7, 2, 0, 1, 5.

:p What is the generated pseudorandom number sequence using the given parameters?
??x
Using the given parameters $a = 4 $, $ c = 1 $, and$ M = 9 $ with an initial seed $ r_1 = 3$, the sequence of pseudorandom numbers generated by the linear congruent method is:
$$r_1 = 3,$$
$$r_2 = (4 \times 3 + 1) \mod 9 = 13 \mod 9 = 4,$$
$$r_3 = (4 \times 4 + 1) \mod 9 = 17 \mod 9 = 8,$$
$$r_4 = (4 \times 8 + 1) \mod 9 = 33 \mod 9 = 6,$$
$$r_5 - r_{10} = 7, 2, 0, 1, 5.$$

Thus, the sequence is:
$$3, 4, 8, 6, 7, 2, 0, 1, 5.$$x??

---

#### Scaling Pseudorandom Numbers
To generate pseudorandom numbers in a specific range [A, B], you can scale the generated random numbers $r_i $ by dividing them by$M$ and then multiplying by the desired range:
$$x_i = A + (B - A) \cdot r_i, 0 \leq r_i \leq 1.$$:p How do you generate pseudorandom numbers in a specific range [A, B]?
??x
To generate pseudorandom numbers in a specific range [A, B], you scale the generated random numbers $r_i $ by dividing them by$M$ and then multiplying by the desired range:
$$x_i = A + (B - A) \cdot r_i, 0 \leq r_i \leq 1.$$

For example, if you want to generate pseudorandom numbers in the range [0, 1] from a sequence of integers generated using $M = 9$:
$$x_1 = 3/9 = 0.333,$$
$$x_2 = 4/9 = 0.444,$$
$$x_3 = 8/9 = 0.889,$$and so on.

To generate a number in the range [5, 10]:
```python
A = 5
B = 10
r_sequence = [0.333, 0.444, 0.889, 0.667, 0.778, 0.222, 0.000, 0.111, 0.555, 0.333]
x_sequence = [A + (B - A) * r for r in r_sequence]

print(x_sequence)  # Output: [5.333, 6.444, 9.889, 7.667, 8.778, 5.222, 5.000, 5.111, 7.555, 5.333]
```
x??

---

#### Graphical Display of Random Numbers
Graphing the generated random numbers can help identify patterns or lack thereof. Visual inspection by the human eye is often more effective at detecting non-randomness.

:p How does visualizing pseudorandom numbers on a graph help in identifying randomness?
??x
Visualizing pseudorandom numbers on a graph helps in identifying patterns or lack thereof. The human visual cortex is highly adept at recognizing patterns, and a graph can quickly reveal any correlations or anomalies in the sequence that might indicate non-randomness.

For example, if you plot successive random numbers (x, y) = (r_i, r_{i+1}) using a "bad" generator versus a built-in random number generator:
- A graph from a "bad" generator will likely show clear patterns or correlations.
- A graph from a good generator will appear more random and less structured.

Here’s how you can plot successive pairs of numbers generated by different methods in Python:
```python
import matplotlib.pyplot as plt

# Example data: r1, r2, ..., r9 for both "good" and "bad" generators
r_good = [0.333, 0.444, 0.889, 0.667, 0.778, 0.222, 0.000, 0.111, 0.555]
r_bad = [3, 4, 8, 6, 7, 2, 0, 1, 5]

plt.figure(figsize=(10, 5))

# Plot for good generator
plt.subplot(1, 2, 1)
for i in range(len(r_good) - 1):
    plt.plot([i, i + 1], [r_good[i], r_good[i + 1]], 'b.')
plt.title('Good Generator')
plt.xlabel('Index')
plt.ylabel('Value')

# Plot for bad generator
plt.subplot(1, 2, 2)
for i in range(len(r_bad) - 1):
    plt.plot([i, i + 1], [r_bad[i], r_bad[i + 1]], 'r.')
plt.title('Bad Generator')
plt.xlabel('Index')
plt.ylabel('Value')

plt.show()
```

The left plot will show clear patterns or correlations that indicate non-randomness, while the right plot should look more random and less structured.
x??

---

---

#### Linear Congruent Method for Random Number Generation
Background context: The linear congruential method is a simple algorithm to generate pseudorandom numbers. It is defined by the recurrence relation:
$$r_{i+1} = (a \cdot r_i + c) \mod M$$where $ a $,$ c $, and$ M $are constants, and$ r_0$ is the initial seed.

:p Write a simple program to generate random numbers using the linear congruent method with given constants.
??x
```python
def linear_congruential_method(a, c, M, r1):
    # Constants provided in the problem statement:
    a = 57
    c = 1
    M = 256
    
    # Initial value of r1 (seed)
    r1 = 10
    
    # Generate sequence
    for i in range(10):  # Let's generate first 10 numbers as an example
        r1 = (a * r1 + c) % M
        print(r1)

# Example usage:
linear_congruential_method(a, c, M, r1)
```
x??

---

#### Period of the Sequence
Background context: The period of a sequence generated by the linear congruential method is the number of unique values before the sequence starts repeating.

:p Determine the period for the given unwise choice of parameters.
??x
```python
def find_period(a, c, M, r1):
    # Constants provided in the problem statement:
    a = 57
    c = 1
    M = 256
    
    # Initial value of r1 (seed)
    r1 = 10
    
    sequence = set()
    
    while True:
        if r1 in sequence:
            break
        
        sequence.add(r1)
        r1 = (a * r1 + c) % M
    
    return len(sequence)

# Example usage:
period = find_period(a, c, M, r1)
print("Period:", period)
```
x??

---

#### Correlation and Clustering in Successive Pairs
Background context: To detect correlations in the generated sequence, one can plot successive pairs of numbers $(r_{2i-1}, r_{2i})$. If there are correlations, clustering will be observed.

:p Plot successive pairs for a pedagogical sequence to observe clustering.
??x
```python
import matplotlib.pyplot as plt

def plot_successive_pairs(a, c, M, r1):
    # Constants provided in the problem statement:
    a = 57
    c = 1
    M = 256
    
    # Initial value of r1 (seed)
    r1 = 10
    
    sequence = []
    
    for i in range(100):  # Generate first 100 numbers as an example
        r1 = (a * r1 + c) % M
        if len(sequence) >= 2:
            plt.scatter(r1, sequence[-2], color='blue')
        sequence.append(r1)
    
    plt.xlabel('r_i')
    plt.ylabel('r_{i-1}')
    plt.title('Successive Pairs (Pedagogical Sequence)')
    plt.show()

# Example usage:
plot_successive_pairs(a, c, M, r1)
```
x??

---

#### Scatter Plot of a Built-in Random Number Generator
Background context: For comparison with the linear congruential method, it is useful to plot successive pairs from a built-in random number generator.

:p Generate and plot successive pairs using Python's built-in random number generator.
??x
```python
import matplotlib.pyplot as plt

def plot_builtin_random_pairs():
    sequence = []
    
    for i in range(100):  # Generate first 100 numbers as an example
        r1 = random.random() * 256  # Scale to [0, M-1]
        if len(sequence) >= 2:
            plt.scatter(r1, sequence[-2], color='red')
        sequence.append(r1)
    
    plt.xlabel('r_i')
    plt.ylabel('r_{i-1}')
    plt.title('Successive Pairs (Built-in Random Generator)')
    plt.show()

# Example usage:
import random
plot_builtin_random_pairs()
```
x??

---

#### Testing the Linear Congruent Method with Reasonable Constants
Background context: Using reasonable constants can yield a better pseudorandom sequence, though it may not be perfect for serious work. The goal is to compare this method with built-in generators.

:p Test the linear congruent method again with reasonable constants and plot successive pairs.
??x
```python
def test_linear_congruential_method(a, c, M):
    # Constants provided in the problem statement:
    a = 5DEECE66D  # Convert to decimal: 273673163155 (base8)
    c = B  # Convert to decimal: 11 (base16)
    M = 2^64  # For larger range
    
    sequence = []
    
    for i in range(100):  # Generate first 100 numbers as an example
        r1 = (a * c + B) % M
        if len(sequence) >= 2:
            plt.scatter(r1, sequence[-2], color='green')
        sequence.append(r1)
    
    plt.xlabel('r_i')
    plt.ylabel('r_{i-1}')
    plt.title('Successive Pairs (Reasonable Constants)')
    plt.show()

# Example usage:
test_linear_congruential_method(a, c, M)
```
x??

---

#### Simulating a Random Walk
Background context: A random walk models the movement of particles in a medium. In this example, we simulate a 2D walk where each step is independent and of fixed length.

:p Describe how to simulate a random walk for an artificial walker.
??x
To simulate a random walk, we can use the following steps:
1. Start at the origin $(0, 0)$.
2. Take $N $ steps in the$XY$-plane where each step is of fixed length but direction is independent.

```python
import numpy as np
import matplotlib.pyplot as plt

def simulate_random_walk(N):
    # Step length and direction (uniformly random)
    step_length = 10
    directions = [(1, 0), (-1, 0), (0, 1), (0, -1)]
    
    x, y = [0], [0]
    for _ in range(N):
        dx, dy = np.random.choice(directions)
        x.append(x[-1] + step_length * dx)
        y.append(y[-1] + step_length * dy)
    
    plt.plot(x, y)
    plt.xlabel('X position')
    plt.ylabel('Y position')
    plt.title('Random Walk Simulation')
    plt.show()

# Example usage:
simulate_random_walk(1000)
```
x??

---

#### Random Walk Distance After N Steps
Background context: The provided text discusses how to calculate the radial distance $R $ from the starting point after$N$ steps in a random walk. For a large number of steps, the cross-terms in the equation vanish due to randomness.

Relevant formulas:
$$R^2 = ( \Delta x_1 + \Delta x_2 + \cdots + \Delta x_N )^2 + ( \Delta y_1 + \Delta y_2 + \cdots + \Delta y_N )^2$$

When the walk is random, averaging over a large number of such steps, all cross-terms vanish, and we get:
$$

R_{\text{rms}}^2 = N r_{\text{rms}}^2$$where $ r_{\text{rms}}$ is the root-mean-square (RMS) step size.

:p How does the radial distance $R $ from the starting point after$N$ steps in a random walk behave?
??x
When the number of steps $N $ is large, the average radial distance$R_{\text{rms}}$ from the origin grows as $\sqrt{N}$ times the RMS step size. This means that while the vector displacement averages to zero due to the randomness in direction at each step, the average length of these displacements does not vanish and increases with the square root of the number of steps.

The RMS step size $r_{\text{rms}}$ can be related to the typical step magnitude in a random walk. If each step is normalized to have an RMS value of 1 (unit-length steps), then:
$$R_{\text{rms}} = \sqrt{N}$$

For large $N $, the average distance from the origin will be approximately$\sqrt{N}$ times the typical step size.
x??

---

#### Random Walk Implementation in Python
Background context: The provided text introduces a simple random walk simulation using Python. The key elements involve generating random values for each step's x and y components, normalizing these steps to have unit length.

Relevant code:
```python
import random

def take_step():
    # Generate random values for delta x and y in the range [-1, 1]
    Δx_prime = (random.random() - 0.5) * 2.
    Δy_prime = (random.random() - 0.5) * 2.
    
    # Normalize to have unit length
    L = (Δx_prime**2 + Δy_prime**2)**0.5
    Δx = 1 / L * Δx_prime
    Δy = 1 / L * Δy_prime
    
    return Δx, Δy

# Example usage:
steps = [(take_step() for _ in range(1000))]
```

:p How does the code simulate a unit-length step in a random walk?
??x
The code simulates a unit-length step by first generating random values $\Delta x'$ and $\Delta y'$ in the range [-1, 1]. These values are then normalized to have a length of one (unit vector). This is achieved by calculating the Euclidean norm $L = \sqrt{(\Delta x')^2 + (\Delta y')^2}$, and scaling $\Delta x'$ and $\Delta y'$ by $1/L$.

The resulting $\Delta x $ and$\Delta y$ will be unit vectors in a random direction, ensuring that each step is of unit length.
x??

---

#### Simulation of 2D Random Walk
Background context: The text describes an implementation of a 2D random walk simulation using Python. It includes details on how to increase randomness and conduct multiple trials for more accurate results.

:p What are the steps involved in simulating a 2D random walk?
??x
To simulate a 2D random walk, follow these steps:

1. **Generate Random Steps**: Independently choose random values $\Delta x'$ and $\Delta y'$ in the range [-1, 1].
2. **Normalize to Unit Length**: Convert the generated values into unit vectors.
3. **Repeat for Many Trials**: Perform multiple trials (each with $N$ steps), ensuring each trial starts from a different initial point.

Example pseudocode:
```python
import random

def simulate_random_walk(N):
    # Initialize position at origin
    x, y = 0., 0.
    
    for _ in range(N):
        Δx_prime = (random.random() - 0.5) * 2.
        Δy_prime = (random.random() - 0.5) * 2.
        
        L = (Δx_prime**2 + Δy_prime**2)**0.5
        Δx = 1 / L * Δx_prime
        Δy = 1 / L * Δy_prime
        
        x += Δx
        y += Δy
    
    return x, y

# Example usage:
N = 1000
positions = [simulate_random_walk(N) for _ in range(K)]
```

x??

---

#### Distance vs Steps Plot
Background context: The text mentions plotting the distance covered over steps to observe the behavior of a random walk. This helps in visualizing how the average distance from the origin grows with the number of steps.

:p What does the plot "Distance vs Steps" illustrate?
??x
The "Distance vs Steps" plot illustrates how the root-mean-square (RMS) distance $R_{\text{rms}}$ from the starting point increases as a function of the number of steps $N$. For a random walk, this plot typically shows that the RMS distance scales linearly with $\sqrt{N}$.

The plot can help in understanding the scaling behavior and verifying theoretical predictions. If the simulation results align well with the expected theoretical curve, it indicates that the model is correctly implementing the randomness and step generation process.

Example:
```python
import matplotlib.pyplot as plt

def plot_distance_vs_steps(N_values):
    distances = [np.sqrt(n) for n in N_values]
    
    plt.plot(N_values, distances)
    plt.xlabel('Number of Steps (N)')
    plt.ylabel('RMS Distance')
    plt.title('Distance vs Steps in a 2D Random Walk')
    plt.show()

# Example usage:
N_values = list(range(10, 1000, 10))
plot_distance_vs_steps(N_values)
```

x??

#### Mean Squared Distance Calculation for Random Walks

Background context explaining the concept. The mean squared distance $R^2$ is a statistical measure used to understand the diffusion of particles in a random walk. The formula provided describes how to calculate the average of the mean squared distances over multiple trials.

Theoretical prediction (4.14) states that for a simple 2D random walk, the expected behavior of the mean squared distance $R^2 $ is linear with respect to time$N$. This can be expressed as:

$$R^2(N) = 2 D N$$where $ D$ is the diffusion coefficient.

:p What is the formula for calculating the average mean squared distance over multiple trials?
??x
The formula for calculating the average mean squared distance over multiple trials is given by:
$$\langle R^2(N) \rangle = \frac{1}{K} \sum_{k=1}^{K} R^2(k)(N)$$where $ K $ is the number of trials, and $ R^2(k)(N)$is the mean squared distance for the $ k$-th trial at time step $ N$.

This formula helps to average out fluctuations and provide a more reliable estimate of the diffusion behavior.

x??

---

#### Validity Check for Assumptions

Background context explaining the concept. After calculating the mean squared distance, it's important to validate the assumptions made in the theoretical derivation by checking if certain conditions are met. Specifically, this involves verifying that the correlation between steps is zero, implying independence and isotropy of motion.

The formula provided checks whether:

$$\langle \Delta x_i \Delta x_j \neq i \rangle_{R^2} \approx \langle \Delta x_i \Delta y_j \neq i \rangle_{R^2} \approx 0$$:p How do you check the validity of assumptions in a random walk?
??x
To check the validity of assumptions, calculate the correlation between steps for different directions and ensure that they are approximately zero. This indicates that there is no significant dependence on direction, supporting isotropy.

For instance, if we consider two distinct positions $i $ and$j$, we can compute:

$$\langle \Delta x_i \Delta x_j \neq i \rangle_{R^2}$$and$$\langle \Delta x_i \Delta y_j \neq i \rangle_{R^2}$$where $\Delta x_i, \Delta y_j$ are the displacements in respective directions. If these values are close to zero for both single long runs and averaged over multiple trials, it suggests that the assumptions hold.

x??

---

#### RMS Distance Plotting

Background context explaining the concept. The root mean square (RMS) distance is a measure of the typical displacement from the starting point after $N$ steps in a random walk. It is calculated as:
$$R_{\text{rms}} = \sqrt{\langle R^2(N) \rangle}$$

The goal is to plot this value against the square root of time steps $\sqrt{N}$. This helps to understand how diffusion scales with time.

:p What is the formula for RMS distance?
??x
The formula for calculating the RMS distance is:

$$R_{\text{rms}} = \sqrt{\langle R^2(N) \rangle}$$where $\langle R^2(N) \rangle$ is the average mean squared distance over multiple trials.

This value helps to understand the typical displacement from the starting point after $N$ steps in a random walk, providing insights into diffusion behavior.

x??

---

#### 3D Random Walk Analysis

Background context explaining the concept. The analysis of random walks extends beyond 2D scenarios to include 3D spaces, which are more representative of real-world phenomena such as molecular diffusion within biological tissues like the brain.

:p What is the key difference between 2D and 3D random walk simulations?
??x
The key difference between 2D and 3D random walks lies in the number of dimensions. In a 2D simulation, particles can move along two axes (e.g.,$x $ and$y $), while in a 3D simulation, they can move along three axes (e.g.,$ x $,$ y $, and$ z$).

This increases the complexity of diffusion behavior as more spatial dimensions affect the movement. The theoretical prediction for 3D random walks is:

$$R^2(N) = 6 D N$$where $ D$ is the diffusion coefficient in three dimensions.

x??

---

#### Brain Random Walk Simulations

Background context explaining the concept. Recent research has highlighted the importance of understanding not just the neural networks within a brain but also the fluid-filled extracellular spaces that affect molecular diffusion, such as the movement of radiographers, drugs, metabolites, and signals.

:p What are some key findings in the study of random walks in the brain?
??x
Key findings in the study of random walks in the brain include:

1. **Diffusion Behavior**: Random walk simulations can model how molecules diffuse through neural tissues.
2. **Impediments and Obstructions**: Modeling extracellular spaces with circular obstructions helps understand barriers to diffusion, which is crucial for medical applications like drug delivery.

The left image in Figure 4.5 shows random walks without impediments, while the right image accounts for these obstacles by placing circular obstructions within the simulation volume. This demonstrates how such simulations can provide insights into real-world scenarios.

x??

---

#### Reproducing Random Walk Simulations

Background context explaining the concept. The task involves reproducing and analyzing 2D random walk simulations similar to those presented in Figure 4.5, which are used to study diffusion within brain models.

:p How do you start a simulation of a 2D random walk with no impediments?
??x
To start a 2D random walk without any impediments:

1. **Initialization**: Begin the walk at the origin $(0, 0)$.
2. **Step Size and Direction**: Use equal-sized steps in random directions (e.g., up, down, left, right).
3. **Plotting**: Record each step to plot the trajectory.

Example code snippet for a simple 2D random walk:

```java
public class RandomWalk2D {
    public static void main(String[] args) {
        int N = 1500; // Number of steps
        double[] x = new double[N];
        double[] y = new double[N];
        
        // Initialize at the origin
        x[0] = 0;
        y[0] = 0;

        for (int i = 1; i < N; i++) {
            int direction = (int) Math.floor(Math.random() * 4); // Random direction
            switch (direction) {
                case 0: // Move up
                    y[i] = y[i-1] + 1;
                    break;
                case 1: // Move down
                    y[i] = y[i-1] - 1;
                    break;
                case 2: // Move right
                    x[i] = x[i-1] + 1;
                    break;
                case 3: // Move left
                    x[i] = x[i-1] - 1;
                    break;
            }
        }

        // Plot the walk using a plotting library or simple console output
    }
}
```

This code initializes the random walk at the origin and takes steps in random directions, updating the position arrays accordingly.

x??

---

#### Self-Avoiding Random Walk Simulation

Background context: A self-avoiding random walk is a model used to simulate how proteins fold. In this context, hydrophobic (H) and polar (P) monomers represent different types of amino acids. The goal is to find the configuration with minimal energy, where the number of H–H contacts is maximized.

The effective diffusion coefficient $D$ within a medium can be calculated using Einstein's relation:
$$D = \frac{2d \cdot \langle r^2 \rangle}{\text{dt}}$$where $ d$ is the number of spatial dimensions (2 for 2D, 3 for 3D).

:p How does the diffusion coefficient change with different spatial dimensions in a self-avoiding random walk?
??x
In a 2D space, the effective diffusion coefficient $D$ is given by:
$$D = \frac{4 \cdot \langle r^2 \rangle}{\text{dt}}$$whereas in a 3D space, it would be:
$$

D = \frac{6 \cdot \langle r^2 \rangle}{\text{dt}}$$

This means the diffusion coefficient is higher in 3D compared to 2D due to more available directions for movement.

x??

---

#### Simulation of Protein Folding

Background context: Proteins are large biological molecules formed from chains of amino acids, which consist of hydrophobic (H) and polar (P) monomers. The goal is to simulate the folding process using a Monte Carlo method on a 2D square lattice.

The energy $E$ of a chain is defined as:
$$E = -\epsilon f$$where $\epsilon $ is a positive constant, and$f$ is the number of H–H contacts that are not directly connected (P–P and H–P bonds do not lower the energy).

:p What does the energy function $E = -\epsilon f$ represent in the context of protein folding?
??x
The energy function $E = -\epsilon f $ represents the total energy of a protein sequence, where$\epsilon $ is a positive constant that scales the effect of H–H contacts. The term$f$ counts the number of H–H contacts that are not directly connected (indicating favorable interactions due to steric exclusion).

To minimize energy:
- More H–H contacts lead to lower energy.
- Directly connected H monomers do not affect the energy.

x??

---

#### Spontaneous Decay Simulation

Background context: Spontaneous decay is a natural process where particles have a constant probability of decaying per unit time interval, independent of their age or the presence of other particles. This leads to an exponential decay model when there are large numbers of particles.

The equation for the decay rate in a discrete model is:
$$\lambda = -\frac{\Delta N(t)}{N(t) \cdot \Delta t}$$where $\lambda $ is the decay constant, and$\Delta N(t)$ is the number of decays in time interval $\Delta t$.

:p How does the equation for the decay rate relate to the probability of a particle decaying over time?
??x
The equation:
$$\lambda = -\frac{\Delta N(t)}{N(t) \cdot \Delta t}$$relates the decay rate (probability per unit time) to the change in the number of particles. Here,$\lambda$ is a constant representing the average rate at which particles decay.

In essence:
- The higher the value of $\lambda$, the faster the decay.
- If you observe many identical systems over time, the average decay rate will be consistent with this equation.

x??

---

#### Visualizing the Self-Avoiding Random Walk

Background context: A self-avoiding random walk on a 2D lattice is used to model protein folding. The walk stops at corners or when there are no empty neighboring sites available. Monomers are randomly chosen as H or P with varying probabilities.

:p How does one implement a self-avoiding random walk in code?
??x
To implement a self-avoiding random walk, follow these steps:
1. Initialize an empty lattice.
2. Start at a random position on the lattice.
3. Randomly choose between H and P monomers with weighted probabilities (more H than P).
4. Move to one of the three available neighboring sites (excluding already occupied ones).

```java
public class SelfAvoidingRandomWalk {
    // Initialize lattice size, number of steps, etc.
    
    public void simulateWalk() {
        int[] currentPosition = new int[]{0, 0}; // Start at origin
        boolean[][] visitedSites = new boolean[latticeSize][latticeSize];
        
        for (int step = 0; step < numSteps; step++) {
            char nextMonomer = chooseMonomer(); // H or P with weight
            
            List<int[]> availableNeighbors = getAvailableNeighbors(currentPosition, visitedSites);
            
            if (!availableNeighbors.isEmpty()) {
                int[] newSite = availableNeighbors.get(random.nextInt(availableNeighbors.size()));
                currentPosition[0] = newSite[0];
                currentPosition[1] = newSite[1];
                
                // Mark current site as visited
                visitedSites[currentPosition[0]][currentPosition[1]] = true;
            } else {
                break; // Stop if no valid moves
            }
        }
    }

    private char chooseMonomer() {
        // Randomly select between H and P with weights
        int randomVal = ThreadLocalRandom.current().nextInt(1, 100 + 1);
        return (randomVal <= 70) ? 'H' : 'P';
    }

    private List<int[]> getAvailableNeighbors(int[] currentSite, boolean[][] visitedSites) {
        // Check up to three neighbors and filter by availability
        int[] positions = {{-1, 0}, {1, 0}, {0, -1}, {0, 1}};
        return Arrays.stream(positions)
                     .filter(pos -> isValidMove(currentSite[0] + pos[0], currentSite[1] + pos[1]) && !visitedSites[currentSite[0] + pos[0]][currentSite[1] + pos[1]])
                     .map(pos -> new int[]{pos[0] + currentSite[0], pos[1] + currentSite[1]})
                     .collect(Collectors.toList());
    }

    private boolean isValidMove(int x, int y) {
        // Check if within lattice bounds
        return x >= 0 && x < latticeSize && y >= 0 && y < latticeSize;
    }
}
```

x??

---

#### Extending the Simulation to 3D

Background context: The self-avoiding random walk can be extended from 2D to 3D by considering additional neighboring sites. This would increase the complexity but allow for more realistic simulations of protein folding in three-dimensional space.

:p How does extending the simulation to a 3D lattice change the number of available neighbors?
??x
In a 3D lattice, each site has up to six available neighbors (up, down, left, right, forward, backward). The logic for checking and selecting these neighbors needs to be updated accordingly:

```java
private List<int[]> getAvailableNeighbors(int[] currentSite, boolean[][][] visitedSites) {
    // Check up to six neighbors and filter by availability
    int[] positions = {{-1, 0, 0}, {1, 0, 0}, {0, -1, 0}, {0, 1, 0}, {0, 0, -1}, {0, 0, 1}};
    return Arrays.stream(positions)
                 .filter(pos -> isValidMove(currentSite[0] + pos[0], currentSite[1] + pos[1], currentSite[2] + pos[2]) && !visitedSites[currentSite[0] + pos[0]][currentSite[1] + pos[1]][currentSite[2] + pos[2]])
                 .map(pos -> new int[]{currentSite[0] + pos[0], currentSite[1] + pos[1], currentSite[2] + pos[2]})
                 .collect(Collectors.toList());
}

private boolean isValidMove(int x, int y, int z) {
    // Check if within lattice bounds
    return x >= 0 && x < latticeSizeX && y >= 0 && y < latticeSizeY && z >= 0 && z < latticeSizeZ;
}
```

x??

---

