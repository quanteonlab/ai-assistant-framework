# Flashcards: 2A012---Reinforcement-Learning_processed (Part 67)

**Starting Chapter:** On-policy Control with Approximation. Episodic Semi-gradient Control

---

#### Episodic Semi-gradient Control
Background context explaining how semi-gradient prediction methods are extended to action values. The chapter discusses forming control methods by coupling action-value prediction with policy improvement techniques for continuous or large discrete actions.

:p What is episodic semi-gradient one-step Sarsa?
??x
Episodic semi-gradient one-step Sarsa is an extension of the semi-gradient TD(0) method to action values in on-policy control. It updates the weight vector $\mathbf{w}$ based on the difference between the actual and predicted Q-values, using a Monte Carlo return or n-step return as the update target.

The general gradient-descent update rule is:
$$\mathbf{w}_{t+1} = \mathbf{w}_t + \alpha [R_{t+1} + \hat{q}(S_{t+1}, A_{t+1}, \mathbf{w}_t) - \hat{q}(S_t, A_t, \mathbf{w}_t)] \nabla \hat{q}(S_t, A_t, \mathbf{w}_t).$$

For one-step Sarsa:
$$\mathbf{w}_{t+1} = \mathbf{w}_t + \alpha [R_{t+1} + \hat{q}(S_{t+1}, A_{t+1}, \mathbf{w}_t) - \hat{q}(S_t, A_t, \mathbf{w}_t)] \nabla \hat{q}(S_t, A_t, \mathbf{w}_t).$$

This method updates the weights based on immediate feedback and is useful for controlling policies in episodic tasks.
x??

---
#### Mountain Car Task
Background context explaining the task setup. The problem involves driving a car up a steep mountain with gravity acting against it. The only solution is to move away from the goal initially to build momentum.

:p What are the actions available in the Mountain Car task?
??x
In the Mountain Car task, there are three possible actions:
1. Full throttle forward (+1)
2. Full throttle reverse (-1)
3. Zero throttle (0)

These actions are selected based on the current state and the policy being followed.
x??

---
#### Tile-Coding Feature Vector
Background context explaining how feature vectors are created using tile coding for continuous states and discrete actions.

:p How is the action-value function approximated in this task?
??x
The action-value function $\hat{q}(s, a, \mathbf{w})$ is approximated using a linear combination of features generated by tile coding. For each state-action pair (s, a), a feature vector x(s, a) is created and combined with the parameter vector w to estimate the action-value function:
$$\hat{q}(s, a, \mathbf{w}) = \sum_{i=1}^{d} w_i \cdot x_i(s, a).$$

Here,$d $ is the dimension of the feature space, and each element$x_i(s, a)$ corresponds to whether or not a particular tile in the coding scheme includes the state-action pair.
x??

---
#### Pseudocode for Episodic Semi-gradient Sarsa
Background context explaining how to implement the algorithm step-by-step.

:p What is the pseudocode for the Episodic Semi-gradient Sarsa?
??x
```pseudocode
// Initialize weights and other parameters
Initialize value-function weights w in R^d arbitrarily (e.g., w=0)
Set step size alpha > 0, small epsilon > 0

For each episode:
    Choose initial state s and action a using -greedy policy
    Loop for each step of the episode:
        Take action a, observe reward r and next state s'
        If s' is terminal:
            w = w + alpha * (r - q(s, a, w)) * grad_q(s, a, w)
            Go to next episode
        Choose next action a' as a function of the current weight vector w (e.g., -greedy policy)
        Update weights: 
            w = w + alpha * (r + q(s', a', w) - q(s, a, w)) * grad_q(s, a, w)

// End loop
```
x??

---
#### Mountain Car Task Example
Background context explaining the setup and running of the task with specific parameters.

:p How is the Mountain Car state space discretized for feature vector creation?
??x
The continuous position and velocity states are discretized using grid-tilings. Specifically, 8 tilings are used where each tiling covers $\frac{1}{8}$ th of the bounded distance in each dimension. The tiles have asymmetrical offsets as described in Section 9.5.4.

For a state (x, xdot) and action A, the indices of the active features are obtained using:
```pseudocode
iht = IHT(4096)
indices = tiles(iht, 8, [8*x/(0.5+1.2), 8*xdot/(0.07+0.07)], A)
```

These indices represent the active features in the feature vector.
x??

---

#### Semi-gradient n-step Sarsa Overview
Semi-gradient n-step Sarsa is an extension of semi-gradient Sarsa that uses n-step returns for updating value-function weights. The goal is to leverage information from several steps into one update, potentially leading to faster convergence and better performance.

The formula for the n-step return $G_{t:t+n}$ generalizes from its tabular form (Equation 7.4) to a function approximation form:
$$G_{t:t+n} = R_{t+1} + \gamma R_{t+2} + \cdots + \gamma^{n-1}R_{t+n} + \gamma^n \hat{q}(S_{t+n}, A_{t+n}, w_{t+n-1}), t+n < T,$$where $ T $ is the end of an episode. The usual case when $ t+n = T$ simplifies to:
$$G_t = G_t.$$

The n-step update equation for the weights $w$ is given by:
$$w_{t+n} = w_{t+n-1} + \alpha [G_{t:t+n} - \hat{q}(S_t, A_t, w_{t+n-1})] r(\hat{q}(S_t, A_t, w_{t+n-1})), 0 \leq t < T.$$:p What is the n-step return formula in Semi-gradient n-step Sarsa?
??x
The n-step return formula combines rewards from multiple time steps to form an updated target for a single weight update:
$$

G_{t:t+n} = R_{t+1} + \gamma R_{t+2} + \cdots + \gamma^{n-1}R_{t+n} + \gamma^n \hat{q}(S_{t+n}, A_{t+n}, w_{t+n-1}), t+n < T.$$

For the end of an episode:
$$

G_t = G_t.$$:p What is the update rule for weights in Semi-gradient n-step Sarsa?
??x
The weight update equation uses a step size $\alpha $ and the difference between the expected return$G_{t:t+n}$ and the current estimate $\hat{q}(S_t, A_t, w_{t+n-1})$:
$$w_{t+n} = w_{t+n-1} + \alpha [G_{t:t+n} - \hat{q}(S_t, A_t, w_{t+n-1})] r(\hat{q}(S_t, A_t, w_{t+n-1})), 0 \leq t < T.$$

This equation is used to update the weights based on the n-step return.

:p How does Semi-gradient n-step Sarsa perform on different tasks?
??x
Semi-gradient n-step Sarsa tends to learn faster and obtain better asymptotic performance at an intermediate level of bootstrapping, corresponding to $n > 1 $. For example, on the Mountain Car task, it performs well with $ n=8$ compared to one-step methods.

:p What is the pseudocode for Semi-gradient n-step Sarsa?
??x
```pseudocode
// Pseudocode for Semi-gradient n-step Sarsa

Episodic semi-gradient n-step Sarsa for estimating ˆq ⇡ q⇤ or q⇡
Input: a differentiable action-value function parameterization ˆq : S x A x R^d -> R
Input: a policy ⇡ (if estimating q⇡)
Algorithm parameters: step size α > 0, small ε > 0, a positive integer n

Initialize value-function weights w ∈ R^d arbitrarily (e.g., w=0)

All store and access operations (St, At, and Rt) can take their index mod n+1

Loop for each episode:
    Initialize and store S_0
    Select and store an action A_0 ⇠ ⇡(·|S_0) or ε-greedy wrt ˆq(S_0,·,w)
    T-1 Loop for t=0,1,2,...:

        If t < T,
            Take action At
            Observe and store the next reward as Rt+1 and the next state as St+1
            If St+1 is terminal, then:
                T t+1
            else: 
                Select and store At+1 ⇠ ⇡(·|St+1) or ε-greedy wrt ˆq(St+1,·,w)
                θ t-n+1 (θ is the time whose estimate is being updated)

        If θ ≥ 0:
            G Pmin(θ+n,T) i=θ+1  γ^(i-θ-1)R_i
            If θ+n < T, then
                G += nˆq(S_θ+n, A_θ+n, w)

            w = w + α [G - ˆq(S_θ, A_θ, w)] r(ˆq(S_θ, A_θ, w))

Until θ = T-1

```

:p What are the results of Semi-gradient n-step Sarsa on Mountain Car task?
??x
The performance of semi-gradient n-step Sarsa on the Mountain Car task is better at an intermediate level of bootstrapping (e.g.,$n=8 $) compared to one-step methods ($ n=1 $). The results show faster learning and better asymptotic performance with$ n=8$ when appropriate step sizes are used.

---

#### Average Reward Setting in MDPs

Background context: The average reward setting is a new problem setting for continuous tasks, introduced alongside episodic and discounted settings. Unlike the discounted setting, there's no discounting involved—agents care equally about immediate and delayed rewards.

The quality of a policy $\pi $ is defined as the average rate of reward while following that policy, denoted by$r(\pi)$:
$$r(\pi)=\lim_{t \to \infty} \frac{1}{t}\sum_{i=1}^{t}E[R_i|S_0,A_0:A_{i-1} \sim \pi]$$

Similarly,$$r(\pi)=\lim_{t \to \infty} E[R_t|S_0, A_0:A_{t-1} \sim \pi]$$

The average reward $r(\pi)$ is calculated using the steady-state distribution $\mu_\pi$:
$$\mu_\pi(s)=\lim_{t \to \infty} Pr\{S_t=s|A_0:A_{t-1} \sim \pi \}$$

The assumption of ergodicity, which ensures the existence of these limits, means that initial conditions and early decisions have only temporary effects.

:p What is the concept being described in this flashcard?
??x
This flashcard describes the average reward setting for Markov decision problems (MDPs), where the goal is to find policies that maximize the average rate of reward over time without discounting. The steady-state distribution $\mu_\pi(s)$ and the ergodicity assumption are key components in defining optimal policies.
x??

---

#### Differential Return

Background context: In the average-reward setting, returns are defined as differences between rewards and the average reward:
$$G_t = R_{t+1} - r(\pi) + R_{t+2} - r(\pi) + \cdots$$

This is known as the differential return.

:p What does the differential return represent in this context?
??x
The differential return represents the difference between the actual reward received at each step and the average reward expected over time, according to policy $\pi$.
x??

---

#### Bellman Equations for Differential Value Functions

Background context: The value functions in the average-reward setting are referred to as differential value functions. They have their own set of Bellman equations.

The value function is defined as:
$$v_\pi(s) = E_\pi[G_t | S_t = s]$$

And similarly for action-value functions $q_\pi(s, a)$.

Bellman equations for these functions are slightly different from those in the discounted setting. However, they still follow a similar structure.

:p What is the value function defined as in this context?
??x
The value function $v_\pi(s)$ in the average-reward setting is defined as the expected differential return starting from state $s$:
$$v_\pi(s) = E_\pi[G_t | S_t = s]$$

Where $G_t$ is the differential return.
x??

---

#### Steady-State Distribution

Background context: The steady-state distribution $\mu_\pi(s)$ represents the probability of being in state $ s $ after a long time, given that actions are chosen according to policy $\pi$. It satisfies the equation:
$$\sum_s \mu_\pi(s) \sum_a \pi(a|s) \sum_{s'} p(s'|s,a) = \mu_\pi(s')$$:p What is the steady-state distribution in this context?
??x
The steady-state distribution $\mu_\pi(s)$ is a probability distribution over states that remains unchanged as time progresses, assuming actions are chosen according to policy $\pi$. It satisfies the equation:
$$\sum_s \mu_\pi(s) \sum_a \pi(a|s) \sum_{s'} p(s'|s,a) = \mu_\pi(s')$$

This means that the expected probability of being in any state at time $t+1 $ is the same as the probability of being in that state at time$t$.
x??

---

#### Concept: Average Reward Setting and Definitions
Background context explaining the concept of the average reward setting. The formulas provided define the value function, action-value functions, and how rewards are adjusted for the average reward scenario.

The problem setting is extended to handle continuing tasks where the traditional discounted sum approach might not be applicable due to infinite time horizons. Instead, the average reward per step over all steps is used as a measure of performance.

Relevant formulas:
$$v_\pi(s) = \sum_a \pi(a|s) \sum_{r,s'} p(s', r | s, a) [ r - \bar{r} + v_\pi(s') ]$$
$$q_\pi(s, a) = \sum_{r,s'} p(s', r | s, a) [ r - \bar{r} + \sum_{a'} \pi(a'|s') q_\pi(s', a') ]$$
$$v^*(s) = \max_a \sum_{r,s'} p(s', r | s, a) [ r - \max_\pi \bar{r} + v^*(s') ]$$
$$q^*(s, a) = \sum_{r,s'} p(s', r | s, a) [ r - \max_\pi \bar{r} + \max_a q^*(s', a) ]$$

The differential form of the two TD errors is also provided:
$$\delta_t = R_{t+1} - \bar{R}_t + \hat{v}(S_{t+1}, w_t) - \hat{v}(S_t, w_t)$$
$$\delta'_t = R_{t+1} - \bar{R}_t + \hat{q}(S_{t+1}, A_{t+1}, w_t) - \hat{q}(S_t, A_t, w_t)$$

Where $\bar{R}_t $ is an estimate of the average reward at time step$t$.

:p What are the definitions provided for the value function and action-value functions in the average reward setting?
??x
The definitions provide a way to measure the performance of a policy by considering the expected difference between the actual rewards and the average rewards. This approach is suitable for tasks that continue indefinitely, such as games or simulations where there is no finite termination condition.

For the value function $v_\pi(s)$, it measures how good a state is under policy $\pi$. It accounts for both the immediate reward difference from the average and the expected future rewards discounted by the value of subsequent states.

For the action-value function $q_\pi(s, a)$, it evaluates actions in states considering both the immediate reward difference and the expected future rewards.

The differential forms of these equations are used to update weights more dynamically based on the difference between actual rewards and estimated average rewards.
x??

---

#### Concept: Differential Semi-Gradient Sarsa
Background context explaining how the semi-gradient Sarsa algorithm is adapted for the average reward setting. The differential form of the TD error is crucial in this adaptation.

Relevant formulas:
$$w_{t+1} = w_t + \alpha_t \delta_t q(S_t, A_t, w)$$

Where $\delta_t$ is given by the differential form:
$$\delta_t = R_{t+1} - \bar{R}_t + \hat{q}(S_{t+1}, A_{t+1}, w_t) - \hat{q}(S_t, A_t, w_t)$$:p What is the updated formula for the weight $ w$ in differential semi-gradient Sarsa?
??x
The updated formula for the weight $w$ in differential semi-gradient Sarsa is:
$$w_{t+1} = w_t + \alpha_t (\delta_t q(S_t, A_t, w))$$

Where $\delta_t$ is the difference between the actual reward and the estimated average reward, adjusted for the action-value function predictions at the next state.

The weight update uses the differential TD error to incrementally adjust the weights towards better policy evaluation.
x??

---

#### Concept: Differential Version of Semi-Gradient Q-Learning
Background context explaining how Q-learning is adapted for the average reward setting. The key difference lies in the use of the differential form of the TD error.

:p What pseudocode would you provide for a differential version of semi-gradient Q-learning?
??x
```java
// Pseudocode for Differential Semi-Gradient Q-Learning
Input: A differentiable action-value function parameterization ˆ q:S×A×R^d. R
Algorithm parameters: step sizes α, γ > 0
Initialize value-function weights w ∈ R^d arbitrarily (e.g., w = 0)
Initialize average reward estimate ¯R arbitrarily (e.g., ¯R = 0)
Initialize state S, and action A

Loop for each step:
    Take action A, observe R, S'
    Choose A' as a function of ˆ q(S', ·, w) (e.g., ε-greedy)
    δ = R - ¯R + ˆq(S', A', w) - ˆq(S, A, w)
    ¯R = ¯R + δ / γ
    w = w + α * δ * ˆq(S, A, w)
    S = S'
    A = A'
```
In this pseudocode:
- The weight update uses the differential TD error $\delta$ to adjust the action-value function estimates.
- The average reward estimate is updated incrementally using the differential form of the TD error.
x??

---

#### Concept: Example: Access-Control Queuing Task
Background context explaining a specific example task involving access control and server allocation. The problem setting includes multiple customer priorities, rewards based on priority, and server availability.

:p What are the differential values of the three states in the given Markov reward process?
??x
In the given Markov reward process with a ring of three states A, B, and C:
- State A: On arrival, a reward of +1 is received. The differential value depends on whether other higher-priority customers are being served.
- State B and C: No immediate reward unless transitioning to state A.

Given the differential form of rewards and the policy, the differential values can be calculated based on the expected difference between actual and average rewards considering transitions:
- For state A:$v^*(A) = \max_{\pi} [1 - \bar{r}] + 0.06v^*(B)$- For state B:$ v^*(B) = 0 + 0.5(0.06v^*(C))$- For state C:$ v^*(C) = 0 + 0.5(0.06v^*(A))$ The exact values would depend on solving the equations for each state under different policies, but generally:
$$v^*(A) > v^*(B) = v^*(C)$$x??

---

#### Policy and Value Function for Queue Management
Background context: The text discusses a tabular solution to an access-control queuing task using differential semi-gradient Sarsa. The goal is to decide whether to accept or reject each customer based on their priority and the number of free servers to maximize long-term reward without discounting.

:p What does the policy and value function shown in Figure 10.5 indicate about accepting/rejecting customers?
??x
The policy suggests rejecting low-priority customers when there are many free servers, while accepting high-priority customers regardless of server availability. The value function indicates a higher expected reward for accepting high-priority customers.
??x

---

#### Deterministic Reward Sequence MDP
Background context: An MDP that produces the deterministic sequence of rewards +1, 0, +1, 0, ... is discussed. This violates ergodicity but has a well-defined average reward.

:p What is the average reward for the given MDP?
??x
The average reward is 0.5.
??x

---

#### State Values in Non-Ergodic MDPs
Background context: In non-ergodic MDPs, state values are defined using equation (10.13) as the limit of the discounted sum.

:p What are the values of states A and B according to the given definition?
??x
State A has a value of 1/2 because its reward sequence is +1, 0, +1, 0, .... State B also has a value of 1/2 but starts with 0.
??x

---

#### Error Update in Average Reward Estimation
Background context: The text mentions that the update rule for the average reward estimate $\bar{R}_t $ uses an error term$d_t $ instead of simply$R_{t+1} - \bar{R}_t$.

:p Why is using $d_t $ better than just updating with$R_{t+1} - \bar{R}_t$?
??x
Using $d_t $ provides a more stable update, reducing the impact of noise and leading to faster convergence to the true average reward. This is illustrated by considering the ring MRP example where using$d_t$ helps estimate the value 1/3 accurately.
??x

---

#### Ring MRP Example
Background context: The text uses a ring Markov Reward Process (MRP) with three states to explain why updating $\bar{R}_t $ with$d_t$ is better.

:p Explain the behavior of the average reward estimate using $d_t$ in the ring MRP example.
??x
In the ring MRP, if the estimate was already at 1/3 and updated using $d_t $, it remains stable. However, updating with $ R_{t+1} - \bar{R}_t$ would cause oscillations around 1/3 due to noise in individual rewards.
??x

---

#### Conclusion
This covers the key concepts of policy and value function for queue management, state values in non-ergodic MDPs, and error update methods for average reward estimation. Each flashcard focuses on a specific aspect to aid understanding and recall.

