source_file,chapter_title,flashcard_title,flashcard_content,flashcard_length,input_text_excerpt,model,timestamp,usefulness_rating
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Navigating This Book,Microservices Overview,"#### Microservices Overview
Microservices are an approach to distributed systems that promote the use of finely grained services with their own lifecycles, which collaborate together. These services are primarily modeled around business domains and avoid the problems of traditional tiered architectures. They also integrate new technologies and techniques, helping them avoid pitfalls faced by many service-oriented architecture implementations.
:p What is the primary goal of microservices?
??x
The primary goal of microservices is to enable finely grained services with their own lifecycles that collaborate together in a distributed system. This approach aims to model services around business domains to avoid issues found in traditional tiered architectures and integrate modern technologies to avoid pitfalls seen in service-oriented architecture implementations.
x??",874,"Preface Microservices are an approach to distributed systems that promote the use of finely grained services with their own lifecycles, which collaborate together. Because microservices are primarily ...",qwen2.5:latest,2025-11-02 23:37:34,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Navigating This Book,The Evolutionary Architect,"#### The Evolutionary Architect
This chapter discusses the difficulties architects face when making trade-offs, emphasizing the complexity of microservices. Architects need to consider various aspects like service collaboration techniques, user interfaces, legacy systems integration, deployment strategies, testing approaches, and monitoring requirements.
:p What challenges do architects face in designing microservices?
??x
Architects face numerous challenges when designing microservices. These include making trade-offs between different architectural decisions, considering how services will collaborate, deciding on the best ways to integrate user interfaces and legacy systems, choosing appropriate deployment strategies, developing effective testing plans, and setting up robust monitoring mechanisms.
x??",814,"Preface Microservices are an approach to distributed systems that promote the use of finely grained services with their own lifecycles, which collaborate together. Because microservices are primarily ...",qwen2.5:latest,2025-11-02 23:37:34,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Navigating This Book,Modeling Services with Domain-Driven Design,"#### Modeling Services with Domain-Driven Design
This chapter focuses on defining the boundaries of microservices using techniques from domain-driven design (DDD). DDD helps in focusing the team's thinking around business domains to create more meaningful and cohesive services.
:p How does DDD help in modeling microservices?
??x
Domain-Driven Design (DDD) helps in modeling microservices by guiding teams to think about their system based on business domains. This approach ensures that the service boundaries align with real-world business processes, making the services more coherent and easier to understand.
x??",617,"Preface Microservices are an approach to distributed systems that promote the use of finely grained services with their own lifecycles, which collaborate together. Because microservices are primarily ...",qwen2.5:latest,2025-11-02 23:37:34,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Navigating This Book,Service Integration Techniques,"#### Service Integration Techniques
This chapter explores specific technologies for integrating microservices, including service collaboration techniques such as asynchronous communication, event-driven architectures, and RESTful APIs. It also covers how to integrate microservices with user interfaces and legacy systems.
:p What are some key integration techniques discussed in this chapter?
??x
The chapter discusses several key integration techniques, such as:
- Asynchronous communication using patterns like message queues or event streams
- Event-driven architectures for handling real-time data flows
- RESTful APIs for synchronous service-to-service communication

These techniques help microservices collaborate effectively and integrate with user interfaces and legacy systems.
x??",792,"Preface Microservices are an approach to distributed systems that promote the use of finely grained services with their own lifecycles, which collaborate together. Because microservices are primarily ...",qwen2.5:latest,2025-11-02 23:37:34,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Navigating This Book,Splitting the Monolith,"#### Splitting the Monolith
Many organizations get interested in microservices as a solution to large, hard-to-change monolithic systems. This chapter provides detailed guidance on how to decompose existing monoliths into smaller, manageable microservices.
:p What is the main purpose of splitting the monolith?
??x
The main purpose of splitting the monolith is to address the challenges faced by large, complex, and inflexible monolithic applications. By breaking down these systems into smaller, independently deployable services, organizations can enhance scalability, improve development agility, and increase team autonomy.
x??",632,"Preface Microservices are an approach to distributed systems that promote the use of finely grained services with their own lifecycles, which collaborate together. Because microservices are primarily ...",qwen2.5:latest,2025-11-02 23:37:34,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Navigating This Book,Deployment Strategies for Microservices,"#### Deployment Strategies for Microservices
Deployment strategies play a crucial role in microservice architectures. This chapter discusses how recent changes in technology have impacted deployment practices, focusing on topics like containerization (e.g., Docker), orchestration tools (e.g., Kubernetes), and service meshes.
:p What new technologies impact microservice deployment?
??x
Recent technologies impacting microservice deployment include:
- Containerization tools such as Docker, which allow for lightweight and portable application packaging.
- Orchestration tools like Kubernetes, which automate the deployment, scaling, and management of containerized applications.
- Service meshes that provide advanced traffic management, security, and observability features.

These technologies enable more efficient and scalable deployment practices in microservice architectures.
x??",888,"Preface Microservices are an approach to distributed systems that promote the use of finely grained services with their own lifecycles, which collaborate together. Because microservices are primarily ...",qwen2.5:latest,2025-11-02 23:37:34,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Navigating This Book,Testing Microservices,"#### Testing Microservices
Testing is a critical aspect of microservice architectures due to their distributed nature. This chapter delves into testing strategies, emphasizing the importance of consumer-driven contracts for ensuring service quality across multiple discrete services.
:p Why is testing important in microservices?
??x
Testing is crucial in microservice architectures because these systems are inherently more complex and distributed. To ensure robustness and reliability, comprehensive testing strategies are necessary, especially those that focus on maintaining contract integrity through consumer-driven contracts.
x??",636,"Preface Microservices are an approach to distributed systems that promote the use of finely grained services with their own lifecycles, which collaborate together. Because microservices are primarily ...",qwen2.5:latest,2025-11-02 23:37:34,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Navigating This Book,Monitoring Distributed Systems,"#### Monitoring Distributed Systems
Monitoring is essential for managing the complexity of distributed microservices. This chapter explores techniques for monitoring fine-grained systems, addressing challenges related to emergent complexity in distributed architectures.
:p What challenges does monitoring introduce in microservice environments?
??x
Monitoring introduces several challenges in microservice environments due to their distributed nature:
- Increased failure points: More services mean more potential failure points.
- Complexity: Managing multiple discrete services adds layers of complexity.

To address these, effective monitoring tools and practices are crucial for maintaining system health and performance.
x??",730,"Preface Microservices are an approach to distributed systems that promote the use of finely grained services with their own lifecycles, which collaborate together. Because microservices are primarily ...",qwen2.5:latest,2025-11-02 23:37:34,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Navigating This Book,Security in Microservices,"#### Security in Microservices
Security is a critical topic in microservice architectures. This chapter examines the security aspects, covering topics like user-to-service and service-to-service authentication and authorization.
:p Why is security important in microservices?
??x
Security is vital in microservice architectures because these systems often involve multiple services communicating over networks, making them susceptible to various security threats. Proper authentication and authorization mechanisms are essential to protect sensitive data and ensure system integrity.
x??",587,"Preface Microservices are an approach to distributed systems that promote the use of finely grained services with their own lifecycles, which collaborate together. Because microservices are primarily ...",qwen2.5:latest,2025-11-02 23:37:34,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Navigating This Book,Conway’s Law and System Design,"#### Conway’s Law and System Design
This chapter explores the relationship between organizational structure and architectural design, highlighting that misalignment can lead to architecture issues. It discusses different ways to align system design with team structures to avoid conflicts.
:p How does Conway's Law impact microservice architectures?
??x
Conway's Law states that an organization's communication structure will inevitably influence its software design. In the context of microservices, this means that a company’s organizational structure should be aligned with its microservice architecture to ensure effective collaboration and reduce architectural conflicts.
x??",680,"Preface Microservices are an approach to distributed systems that promote the use of finely grained services with their own lifecycles, which collaborate together. Because microservices are primarily ...",qwen2.5:latest,2025-11-02 23:37:34,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Navigating This Book,Microservices at Scale,"#### Microservices at Scale
As microservices grow in number and traffic volume, managing their complexity becomes increasingly challenging. This chapter addresses these issues by discussing strategies for scaling microservices effectively.
:p What are the key challenges of scaling microservices?
??x
Key challenges of scaling microservices include:
- Increased failure points: More services mean more potential failure points.
- Complexity management: Managing a large number of discrete services adds layers of complexity.

Effective strategies to address these challenges involve robust monitoring, advanced service discovery mechanisms, and resilient system design principles.
x??",684,"Preface Microservices are an approach to distributed systems that promote the use of finely grained services with their own lifecycles, which collaborate together. Because microservices are primarily ...",qwen2.5:latest,2025-11-02 23:37:34,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Navigating This Book,Summary of Microservices Principles,"#### Summary of Microservices Principles
The final chapter outlines seven microservices principles and summarizes the key takeaways from the book. These principles encapsulate the core essence of what makes microservices different and offer a framework for building effective microservice architectures.
:p What are the core principles of microservices?
??x
The core principles of microservices include:
1. Autonomous services with their own lifecycle.
2. Service isolation and boundaries aligned with business domains.
3. Fine-grained, independently deployable services.
4. Decentralized data management.
5. Flexible deployment strategies using modern technologies like containers and service meshes.
6. Robust testing practices to ensure quality.
7. Effective monitoring and observability.

These principles provide a framework for building effective microservice architectures that are scalable, secure, and maintainable.
x??",928,"Preface Microservices are an approach to distributed systems that promote the use of finely grained services with their own lifecycles, which collaborate together. Because microservices are primarily ...",qwen2.5:latest,2025-11-02 23:37:34,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Technology Heterogeneity,Typographical Conventions,"#### Typographical Conventions

Background context explaining how typographical conventions are used to enhance readability and clarity of technical content. This includes differentiating new terms, URLs, email addresses, filenames, and file extensions from other elements.

:p What do we use italic for in this book?
??x
Italic is used to indicate new terms, URLs, email addresses, filenames, and file extensions.
x??",418,"Conventions Used in This Book The following typographical conventions are used in this book: Italic Indicates new terms, URLs, email addresses, filenames, and file extensions. Constant width Used for ...",qwen2.5:latest,2025-11-02 23:37:44,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Technology Heterogeneity,Code Listings,"#### Code Listings

Background context explaining the usage of constant width text for program listings and variable/function names within paragraphs. This helps differentiate them from regular text.

:p What kind of text is used for program listings?
??x
Constant width text is used for program listings.
x??",309,"Conventions Used in This Book The following typographical conventions are used in this book: Italic Indicates new terms, URLs, email addresses, filenames, and file extensions. Constant width Used for ...",qwen2.5:latest,2025-11-02 23:37:44,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Technology Heterogeneity,Literal Commands,"#### Literal Commands

Explanation on how to denote commands or other text that should be typed literally by the user, as opposed to being replaced with user-supplied values or context-dependent values.

:p How do we show text that should be typed exactly as shown?
??x
Text that should be typed exactly as shown is denoted using constant width bold.
x??",354,"Conventions Used in This Book The following typographical conventions are used in this book: Italic Indicates new terms, URLs, email addresses, filenames, and file extensions. Constant width Used for ...",qwen2.5:latest,2025-11-02 23:37:44,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Technology Heterogeneity,User-Provided Values,"#### User-Provided Values

Explanation on how to indicate placeholders for user-supplied values or values determined by the context.

:p How do we denote text that needs to be replaced with a user-supplied value?
??x
Text that should be replaced with user-supplied values is denoted using constant width italic.
x??",315,"Conventions Used in This Book The following typographical conventions are used in this book: Italic Indicates new terms, URLs, email addresses, filenames, and file extensions. Constant width Used for ...",qwen2.5:latest,2025-11-02 23:37:44,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Technology Heterogeneity,Safari Books Online,"#### Safari Books Online

Explanation on the purpose and features of Safari Books Online, including its range of plans and pricing for various audiences.

:p What is Safari Books Online used for by professionals and developers?
??x
Safari Books Online is an on-demand digital library that provides expert content in book and video form from leading authors. It serves as a primary resource for research, problem solving, learning, and certification training for technology professionals, software developers, web designers, business, and creative professionals.

It offers different plans and pricing for enterprises, governments, education institutions, and individuals. Members have access to thousands of books, training videos, and prepublication manuscripts in one fully searchable database from publishers such as O’Reilly Media, Prentice Hall Professional, Addison-Wesley Professional, Microsoft Press, Sams, Que, Peachpit Press, Focal Press, Cisco Press, John Wiley & Sons, Syngress, Morgan Kaufmann, IBM Redbooks, Packt, Adobe Press, FT Press, Apress, Manning, New Riders, McGraw-Hill, Jones & Bartlett, and Course Technology.
x??",1139,"Conventions Used in This Book The following typographical conventions are used in this book: Italic Indicates new terms, URLs, email addresses, filenames, and file extensions. Constant width Used for ...",qwen2.5:latest,2025-11-02 23:37:44,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Technology Heterogeneity,Contact Information,"#### Contact Information

Explanation on how to contact the publisher for comments or questions about the book.

:p How can readers reach out with comments or technical questions?
??x
Readers can send email to bookquestions@oreilly.com to comment or ask technical questions about the book. They can also address their comments and questions to O'Reilly Media at:
O’Reilly Media, Inc.
1005 Gravenstein Highway North
Sebastopol, CA 95472
Phone: 800-998-9938 (in the United States or Canada) | 707-829-0515 (international or local)
Fax: 707-829-0104

Additional information about the book can be found on its web page at http://bit.ly/building-microservices.
x??",659,"Conventions Used in This Book The following typographical conventions are used in this book: Italic Indicates new terms, URLs, email addresses, filenames, and file extensions. Constant width Used for ...",qwen2.5:latest,2025-11-02 23:37:44,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Technology Heterogeneity,Acknowledgments,"#### Acknowledgments

Explanation of the acknowledgments section, dedications, and contributions from specific individuals.

:p What does the acknowledgments section include?
??x
The acknowledgments section includes a dedication to Lindy Stephens for encouraging the author's journey and supporting them through the writing process. It also dedicates the book to the author’s dad, Howard Newman, who has always been there for him. The section thanks Ben Christensen, Vivek Subramaniam, and Martin Fowler for providing detailed feedback during the writing process, helping shape the final product. It also credits James Lewis for discussions on ideas presented in the book.
x??

---",681,"Conventions Used in This Book The following typographical conventions are used in this book: Italic Indicates new terms, URLs, email addresses, filenames, and file extensions. Constant width Used for ...",qwen2.5:latest,2025-11-02 23:37:44,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Technology Heterogeneity,Concept of Microservices Emergence,"#### Concept of Microservices Emergence
Microservices have emerged as a trend or pattern from real-world use, driven by a combination of advancements and practices over many years. These advancements include domain-driven design, continuous delivery, on-demand virtualization, infrastructure automation, small autonomous teams, and systems at scale.
:p How did microservices emerge?
??x
Microservices emerged due to the cumulative impact of various technologies and methodologies that have evolved over time. This includes practices like domain-driven design (DDD), which emphasizes modeling real-world domains in code, and continuous delivery, which promotes frequent releases through automated processes. Other factors include on-demand virtualization, infrastructure automation, small autonomous teams, and the need for systems at scale.
??x",844,"Specifically, I would like to thank (in no particular order) Kane Venables, Anand Krishnaswamy, Kent McNeil, Charles Haynes, Chris Ford, Aidy Lewis, Will Thames, Jon Eaves, Rolf Russell, Badrinath Jan...",qwen2.5:latest,2025-11-02 23:37:55,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Technology Heterogeneity,Continuous Delivery,"#### Continuous Delivery
Continuous delivery is a practice that enables more effective and efficient deployment of software into production by treating every check-in as a release candidate.
:p What does continuous delivery entail?
??x
Continuous delivery involves automating the process of building, testing, and deploying code changes so that they can be released to production quickly and reliably. It ensures that any change made in the development environment is potentially deliverable at any time without human intervention.
```java
public class BuildAndDeployPipeline {
    public void deployCode() throws Exception {
        // Code for automated build and deployment process
        System.out.println(""Building code..."");
        if (isBuildSuccessful()) {
            System.out.println(""Deployment initiated..."");
            if (testPassed()) {
                System.out.println(""Deploying to production environment..."");
                deployToProduction();
            } else {
                throw new RuntimeException(""Tests failed. Deployment aborted."");
            }
        } else {
            throw new RuntimeException(""Build failed. Deployment aborted."");
        }
    }

    private boolean isBuildSuccessful() {
        // Logic for checking if build was successful
        return true;
    }

    private boolean testPassed() {
        // Logic for automated tests
        return true;
    }

    private void deployToProduction() {
        // Code for deploying to production environment
        System.out.println(""Code deployed successfully."");
    }
}
```
x??",1596,"Specifically, I would like to thank (in no particular order) Kane Venables, Anand Krishnaswamy, Kent McNeil, Charles Haynes, Chris Ford, Aidy Lewis, Will Thames, Jon Eaves, Rolf Russell, Badrinath Jan...",qwen2.5:latest,2025-11-02 23:37:55,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Technology Heterogeneity,Hexagonal Architecture,"#### Hexagonal Architecture
Hexagonal architecture, also known as ports and adapters, is a design pattern that guides us away from traditional layered architectures where business logic could be hidden. Instead, it promotes a clear separation between the application core (business logic) and external systems.
:p What is hexagonal architecture?
??x
Hexagonal architecture, or ports and adapters, separates the core domain logic of an application from its external dependencies. It encourages a clean design by exposing well-defined interfaces that allow for easy integration with different types of external systems, such as databases, APIs, user interfaces, etc.
```java
public class CoreApplication {
    private final Port port;

    public CoreApplication(Port port) {
        this.port = port;
    }

    public void processRequest(Request request) {
        // Core logic that interacts only with the port
        if (isValidRequest(request)) {
            Response response = coreLogic.execute(request);
            port.sendResponse(response);
        }
    }

    private boolean isValidRequest(Request request) {
        // Logic to validate the request
        return true;
    }

    private class coreLogic {
        public Response execute(Request request) {
            // Core business logic
            return new Response(""Processed"");
        }
    }

    interface Port {
        void sendResponse(Response response);
    }
}
```
x??",1454,"Specifically, I would like to thank (in no particular order) Kane Venables, Anand Krishnaswamy, Kent McNeil, Charles Haynes, Chris Ford, Aidy Lewis, Will Thames, Jon Eaves, Rolf Russell, Badrinath Jan...",qwen2.5:latest,2025-11-02 23:37:55,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Technology Heterogeneity,Microservices Characteristics,"#### Microservices Characteristics
Microservices are small, autonomous services that work together. They have several characteristics that differentiate them from monolithic applications: fine-grained nature, autonomy, and collaboration.
:p What are the key characteristics of microservices?
??x
The key characteristics of microservices include:
- **Fine-grained:** Each service is responsible for a specific business function or feature.
- **Autonomy:** Services operate independently with their own databases and technology stacks.
- **Collaboration:** Services communicate through well-defined APIs, allowing them to work together in a larger system.
```java
public interface MicroserviceA {
    void performTask1(String input);
}

public class ServiceB implements MicroserviceB {
    private final MicroserviceA serviceA;

    public ServiceB(MicroserviceA serviceA) {
        this.serviceA = serviceA;
    }

    @Override
    public void performTask2(String input) {
        // Logic to call Task1 from ServiceA and process its output
        String outputFromA = serviceA.performTask1(input);
        System.out.println(""Processed by ServiceB: "" + outputFromA);
    }
}
```
x??

---",1189,"Specifically, I would like to thank (in no particular order) Kane Venables, Anand Krishnaswamy, Kent McNeil, Charles Haynes, Chris Ford, Aidy Lewis, Will Thames, Jon Eaves, Rolf Russell, Badrinath Jan...",qwen2.5:latest,2025-11-02 23:37:55,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Technology Heterogeneity,Codebase Growth and Complexity,"#### Codebase Growth and Complexity
Background context: As a software system grows, it becomes increasingly challenging to manage changes and maintain cohesion. The difficulty arises from the sprawling nature of large codebases, which can make identifying where a change is needed nearly impossible.

:p How does the size and complexity of a codebase affect its maintainability?
??x
The larger and more complex a codebase becomes, the harder it is to understand and modify. As features are added over time, related functionality might be scattered throughout different parts of the system, making it difficult to make coherent changes or fix bugs.

For example, consider a monolithic application where similar functionalities are spread across multiple files without clear boundaries:
```java
// Example Monolithic Codebase Structure
public class UserService {
    public void login() {}
    public void register() {}
}

public class ProductService {
    public void addProduct() {}
    public void updateProduct() {}
}
```

If a new requirement is introduced, such as adding user preferences to the system, developers might have trouble deciding where to make changes because similar functionalities are not grouped together.

This disorganized structure can lead to redundant code and increased complexity in maintenance.
x??",1327,"Small, and Focused on Doing One Thing Well Codebases grow as we write code to add new features. Over time, it can be difficult to know where a change needs to be made because the codebase is so large....",qwen2.5:latest,2025-11-02 23:38:08,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Technology Heterogeneity,Single Responsibility Principle (SRP),"#### Single Responsibility Principle (SRP)
Background context: The Single Responsibility Principle (SRP) is a core concept of object-oriented design that states every class should have only one reason to change. This principle promotes the idea that classes should be focused on a single responsibility, which aligns well with microservices architecture where services are designed around business boundaries.

:p What does the Single Responsibility Principle state?
??x
The Single Responsibility Principle (SRP) suggests that a class or module should have only one reason to change. This means grouping code related to similar functions together and separating those that change for different reasons.

For example, consider a `UserService` in a monolithic application:
```java
public class UserService {
    public void login() {}
    public void register() {}
    public void updateUserPreferences() {} // Violates SRP

    // Additional functionalities related to user preferences should be separated.
}
```

This violates the SRP because updating user preferences is not directly related to logging in or registering a user. Separating these functionalities into different classes would make each class more focused and easier to maintain:
```java
public class UserService {
    public void login() {}
    public void register() {}

    private UserPreferencesService preferencesService = new UserPreferencesService();

    public void updateUserPreferences() {
        preferencesService.update();
    }
}

public class UserPreferencesService {
    public void update() {}
}
```

By adhering to the SRP, you can ensure that changes in one area of functionality do not ripple through unrelated parts of your codebase.
x??",1726,"Small, and Focused on Doing One Thing Well Codebases grow as we write code to add new features. Over time, it can be difficult to know where a change needs to be made because the codebase is so large....",qwen2.5:latest,2025-11-02 23:38:08,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Technology Heterogeneity,Microservices and Service Boundaries,"#### Microservices and Service Boundaries
Background context: In microservices architecture, services are designed around business boundaries. This approach helps maintain clear boundaries within the system, making it easier to understand where a particular piece of functionality lives.

:p How does focusing on business boundaries help in managing large codebases?
??x
Focusing on business boundaries in microservices makes it clear where each service's responsibility lies. By aligning services with specific business functions, you can ensure that related functionalities are grouped together, reducing the complexity and improving maintainability of the system.

For example, consider a microservice-based architecture for an e-commerce application:
```java
public class ProductService {
    public void addProduct() {}
    public void updateProduct() {}

    private InventoryService inventoryService = new InventoryService();

    public void handleStockChanges() {
        // Logic to handle stock changes based on product updates.
    }
}

public class InventoryService {
    public void updateInventory() {}
}
```

In this example, `ProductService` handles the logic related to adding and updating products, while `InventoryService` deals with inventory management. This separation helps in managing changes more effectively as each service has a well-defined responsibility.

By focusing on business boundaries, you can avoid the temptation for services to grow too large and become difficult to manage.
x??",1518,"Small, and Focused on Doing One Thing Well Codebases grow as we write code to add new features. Over time, it can be difficult to know where a change needs to be made because the codebase is so large....",qwen2.5:latest,2025-11-02 23:38:08,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Technology Heterogeneity,How Small is Small Enough?,"#### How Small is Small Enough?
Background context: Determining the appropriate size of microservices is challenging due to varying factors such as language expressiveness, dependencies, and domain complexity. The key is finding a balance where each service feels manageable by a small team.

:p What factors should be considered when determining the size of a microservice?
??x
When determining the size of a microservice, several factors should be considered:

1. **Lines of Code**: While lines of code can give an initial indication, they are not precise due to differences in language expressiveness and dependencies.
2. **Team Structure**: The service should be small enough for a small team to manage effectively without feeling overwhelmed.
3. **Business Boundaries**: Services should align with business logic to ensure clear responsibilities and maintainability.
4. **Rewrite Time**: A rule of thumb is that a microservice can be rewritten in two weeks by a small team.

For example, if you have a service managing user authentication, it might look like this:
```java
public class AuthService {
    public boolean authenticate(String username, String password) {}
    public void createUser(String username, String password) {}
}
```

This service is focused on its core responsibility and can be managed by a small team. However, if the service starts to include unrelated functionalities such as managing user preferences or inventory management, it might become too large.

By adhering to these principles, you can ensure that your microservices are appropriately sized for maintainability and scalability.
x??

---",1628,"Small, and Focused on Doing One Thing Well Codebases grow as we write code to add new features. Over time, it can be difficult to know where a change needs to be made because the codebase is so large....",qwen2.5:latest,2025-11-02 23:38:08,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Technology Heterogeneity,Autonomous Microservice Deployment,"#### Autonomous Microservice Deployment
Background context explaining the concept. This topic discusses the idea of deploying services as separate entities, possibly on different machines or even operating systems, to ensure high autonomy and ease of deployment. The separation helps in avoiding tight coupling and promotes easier maintenance and scalability.

:p What is the key benefit of treating a microservice as an autonomous entity?
??x
The key benefits include simplicity in reasoning about the distributed system, independent changeability, and reduced coordination overhead between services. By deploying each service independently, changes can be made without affecting other parts of the application.
x??",716,"Autonomous Our microservice is a separate entity. It might be deployed as an isolated service on a platform as a service (PAAS), or it might be its own operating system process. We try to avoid packin...",qwen2.5:latest,2025-11-02 23:38:17,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Technology Heterogeneity,Service Communication via Network Calls,"#### Service Communication via Network Calls
Background context explaining the concept. The text highlights that microservices communicate with each other through network calls to ensure separation and avoid tight coupling. This approach helps in maintaining loose coupling between services, making them more flexible and easier to manage.

:p How does using network calls for service communication help achieve decoupling?
??x
Using network calls ensures that services are independent of each other's internal workings. This decouples the services such that changes in one service do not require corresponding changes in others, promoting flexibility and ease of maintenance.
x??",680,"Autonomous Our microservice is a separate entity. It might be deployed as an isolated service on a platform as a service (PAAS), or it might be its own operating system process. We try to avoid packin...",qwen2.5:latest,2025-11-02 23:38:17,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Technology Heterogeneity,Importance of Good APIs,"#### Importance of Good APIs
Background context explaining the concept. The passage emphasizes the importance of designing well-architected APIs to enable loose coupling between services. Good APIs allow services to change independently without affecting consumers.

:p Why are good APIs crucial for microservices?
??x
Good APIs are crucial because they ensure that services can evolve independently while maintaining compatibility with their consumers. Poorly designed APIs can lead to tight coupling, making changes difficult and breaking the system.
x??",556,"Autonomous Our microservice is a separate entity. It might be deployed as an isolated service on a platform as a service (PAAS), or it might be its own operating system process. We try to avoid packin...",qwen2.5:latest,2025-11-02 23:38:17,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Technology Heterogeneity,Technology Heterogeneity in Microservices,"#### Technology Heterogeneity in Microservices
Background context explaining the concept. The text discusses how microservices allow for different technologies within each service, enabling the selection of the most appropriate tool for specific tasks.

:p How does technology heterogeneity benefit microservice architecture?
??x
Technology heterogeneity benefits microservices by allowing each component to use the best technology suited to its task. This flexibility can lead to improved performance and more efficient data storage strategies across different parts of the system.
x??",586,"Autonomous Our microservice is a separate entity. It might be deployed as an isolated service on a platform as a service (PAAS), or it might be its own operating system process. We try to avoid packin...",qwen2.5:latest,2025-11-02 23:38:17,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Technology Heterogeneity,Heterogeneous Architecture Example,"#### Heterogeneous Architecture Example
Background context explaining the concept. The passage provides an example of a social network with different types of databases for storing user interactions and posts, illustrating a heterogeneous architecture.

:p Can you provide an example of technology heterogeneity in microservices as described?
??x
Sure, consider a social network where users' interactions are stored in a graph-oriented database (e.g., Neo4j) to capture the interconnected nature of relationships. Meanwhile, user posts might be stored in a document-oriented database (e.g., MongoDB) for efficient text storage and querying.
x??",644,"Autonomous Our microservice is a separate entity. It might be deployed as an isolated service on a platform as a service (PAAS), or it might be its own operating system process. We try to avoid packin...",qwen2.5:latest,2025-11-02 23:38:17,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Technology Heterogeneity,Golden Rule of Microservices,"#### Golden Rule of Microservices
Background context explaining the concept. The passage introduces the idea that services should be designed such that they can be changed independently without affecting other parts of the system.

:p What is the ""golden rule"" mentioned in the text regarding microservices?
??x
The golden rule states that changes to a service should be deployable independently without requiring changes elsewhere. This ensures that each service remains autonomous and flexible.
x??",500,"Autonomous Our microservice is a separate entity. It might be deployed as an isolated service on a platform as a service (PAAS), or it might be its own operating system process. We try to avoid packin...",qwen2.5:latest,2025-11-02 23:38:17,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Technology Heterogeneity,Decoupling through API Design,"#### Decoupling through API Design
Background context explaining the concept. The text stresses the importance of designing APIs that allow services to communicate loosely, thereby reducing coupling.

:p How can API design help in achieving decoupling?
??x
API design can help achieve decoupling by ensuring that services interact via well-defined interfaces. This prevents changes in one service from affecting others and allows for more flexible and independent development.
x??

---",485,"Autonomous Our microservice is a separate entity. It might be deployed as an isolated service on a platform as a service (PAAS), or it might be its own operating system process. We try to avoid packin...",qwen2.5:latest,2025-11-02 23:38:17,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Technology Heterogeneity,Microservices and Technology Adoption,"#### Microservices and Technology Adoption

Microservices offer a flexible approach to technology adoption by breaking down large monolithic applications into smaller, independent services. This allows organizations to try out new technologies with less risk compared to changing an entire system.

:p How do microservices help in adopting new technologies?
??x
Microservices enable the use of different technologies within a single application by isolating changes to individual services. In a monolithic architecture, any change impacts the entire application, whereas in a microservice-based system, you can try out new technologies in a controlled manner, limiting potential risks.

For example, if you want to try a new programming language or framework, you can deploy it in one service without affecting others. This approach minimizes the risk and allows for more frequent technology experimentation.

```java
// Pseudocode Example: Adding a New Technology in a Microservice
public class ServiceA {
    private final JavaService javaService;
    private final PythonService pythonService;

    public ServiceA(JavaService javaService, PythonService pythonService) {
        this.javaService = javaService;
        this.pythonService = pythonService; // Here you can add different technology stacks
    }

    public void performTask() {
        if (shouldUsePython()) {
            pythonService.execute();
        } else {
            javaService.execute();
        }
    }

    private boolean shouldUsePython() {
        // Logic to decide whether to use Python for this task
        return true; // For demonstration purposes, always use Python here
    }
}
```
x??",1677,"Figure 1-1. Microservices can allow you to more easily embrace different technologies With microservices, we are also able to adopt technology more quickly, and understand how new advancements may hel...",qwen2.5:latest,2025-11-02 23:38:31,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Technology Heterogeneity,Risk Management in Microservices,"#### Risk Management in Microservices

When adopting microservices, one of the benefits is mitigating risks associated with new technologies. By isolating changes within individual services, you can test and adopt new technologies without disrupting the entire application.

:p How does using microservices help manage technological risk?
??x
Using microservices helps manage technological risk by allowing developers to introduce and experiment with new technologies in a controlled environment. Each service is independent, so any change or technology upgrade affects only that specific service, not the whole system. This isolation reduces the potential impact of failures or issues.

For instance, if you want to test a new database, you can integrate it into one microservice without impacting other services. If the new technology performs well, you can gradually roll out its usage across more services; otherwise, the failure is localized and does not affect the entire application.

```java
// Pseudocode Example: Introducing a New Database Technology in a Microservice
public class UserManagementService {
    private final MySQLDatabase mySQLDatabase;
    private final NewDBDatabase newDBDatabase;

    public UserManagementService(MySQLDatabase mySQLDatabase, NewDBDatabase newDBDatabase) {
        this.mySQLDatabase = mySQLDatabase;
        this.newDBDatabase = newDBDatabase; // Here you can introduce a new database technology
    }

    public void manageUser(User user) {
        if (shouldUseNewDB()) {
            newDBDatabase.save(user);
        } else {
            mySQLDatabase.save(user);
        }
    }

    private boolean shouldUseNewDB() {
        // Logic to decide whether to use the new database for saving users
        return true; // For demonstration purposes, always use the new database here
    }
}
```
x??",1848,"Figure 1-1. Microservices can allow you to more easily embrace different technologies With microservices, we are also able to adopt technology more quickly, and understand how new advancements may hel...",qwen2.5:latest,2025-11-02 23:38:31,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Technology Heterogeneity,Technology Constraints in Microservices,"#### Technology Constraints in Microservices

Some organizations adopt a constrained approach to technology within microservices by selecting specific platforms or frameworks. This is often done to leverage existing expertise and ensure reliability.

:p How do some organizations manage technology constraints in microservices?
??x
Organizations may choose to limit the variety of technologies used, typically by sticking to a specific platform like the Java Virtual Machine (JVM). For example, Netflix and Twitter predominantly use JVM-based services due to their established knowledge and tools. This approach reduces complexity and ensures high reliability.

However, even with such constraints, these organizations still allow flexibility within the chosen technology stack. They may develop libraries and tooling for the JVM that help in scaling operations more effectively but make it harder for non-JVM technologies to integrate seamlessly.

```java
// Pseudocode Example: Enforcing Technology Constraints
public class ServiceFramework {
    private final JavaService javaService;

    public ServiceFramework(JavaService javaService) {
        this.javaService = javaService; // Ensuring the service uses a specific framework
    }

    public void performTask() {
        javaService.execute();
    }
}
```
x??",1319,"Figure 1-1. Microservices can allow you to more easily embrace different technologies With microservices, we are also able to adopt technology more quickly, and understand how new advancements may hel...",qwen2.5:latest,2025-11-02 23:38:31,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Technology Heterogeneity,Balancing Technology Adoption in Microservices,"#### Balancing Technology Adoption in Microservices

Balancing technology adoption involves finding the right balance between leveraging new technologies and maintaining stability. The key is to introduce changes incrementally while ensuring that each change is thoroughly tested.

:p How does balancing technology adoption work in microservices?
??x
Balancing technology adoption in microservices means introducing new technologies gradually and carefully, with a focus on minimal disruption. Each service can experiment with new technologies independently of others, allowing for iterative improvements without significant risk to the overall system.

For instance, if you want to adopt a new programming language, you could start by implementing it in a small, non-critical microservice first. Once tested and proven, you can then deploy it in more critical services. This approach ensures that any issues are localized, reducing potential harm.

```java
// Pseudocode Example: Gradual Technology Adoption
public class TechnologyAdoption {
    private final OldTechnology oldTech;
    private final NewTechnology newTech;

    public TechnologyAdoption(OldTechnology oldTech, NewTechnology newTech) {
        this.oldTech = oldTech;
        this.newTech = newTech; // Here you can adopt a new technology gradually
    }

    public void performTask() {
        if (shouldUseNewTech()) {
            newTech.execute();
        } else {
            oldTech.execute();
        }
    }

    private boolean shouldUseNewTech() {
        // Logic to decide whether to use the new technology for this task
        return true; // For demonstration purposes, always use the new technology here
    }
}
```
x??

---",1709,"Figure 1-1. Microservices can allow you to more easily embrace different technologies With microservices, we are also able to adopt technology more quickly, and understand how new advancements may hel...",qwen2.5:latest,2025-11-02 23:38:31,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Modules,Bulkheads and Monolithic vs Microservices,"#### Bulkheads and Monolithic vs Microservices

Background context explaining the concept. In resilience engineering, a bulkhead is used to isolate failures within a system so that they do not cascade. In software systems, service boundaries act as bulkheads. In a monolithic application, if one component fails, the entire application can stop working. However, with microservices, individual services can fail without causing the whole system to collapse.

:p What are bulkheads in the context of resilience engineering and how do they apply to software systems?
??x
Bulkheads in resilience engineering refer to barriers designed to contain failures so that their impact does not spread across the entire system. In a monolithic service, if one component fails, the entire application stops working because it is tightly integrated. However, with microservices architecture, each service can fail independently, and only specific functionalities degrade or are isolated.

```java
// Example of isolating services in Java using Spring Cloud
@SpringBootApplication
public class ServiceAApplication {
    public static void main(String[] args) {
        SpringApplication.run(ServiceAApplication.class, args);
    }
}
```
x??",1224,"Resilience A key concept in resilience engineering is the bulkhead. If one component of a system fails, but that failure doesn’t cascade, you can isolate the problem and the rest of the system can car...",qwen2.5:latest,2025-11-02 23:38:43,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Modules,Scaling Monolithic vs Microservices,"#### Scaling Monolithic vs Microservices

Background context explaining the concept. With a monolithic service, scaling is often done all at once for the entire application. However, in microservices architecture, only specific services that need to be scaled are scaled independently.

:p How does scaling work differently in monolithic and microservices architectures?
??x
In monolithic applications, if one part of the system is constrained in performance, the entire application has to scale together. This can lead to unnecessary resource usage. In contrast, with microservices, only specific services that require more resources are scaled independently, allowing other parts of the system to run on smaller, less powerful hardware.

```java
// Example of scaling a service in Java using Spring Cloud Kubernetes
@SpringBootApplication
@EnableDiscoveryClient
public class ServiceAScalingApplication {
    public static void main(String[] args) {
        SpringApplication.run(ServiceAScalingApplication.class, args);
    }
}
```
x??",1037,"Resilience A key concept in resilience engineering is the bulkhead. If one component of a system fails, but that failure doesn’t cascade, you can isolate the problem and the rest of the system can car...",qwen2.5:latest,2025-11-02 23:38:43,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Modules,Resilience and Failure Handling,"#### Resilience and Failure Handling

Background context explaining the concept. Distributed systems have new sources of failure such as network failures and machine failures. Microservices need to handle these failures gracefully by degrading functionality.

:p How do microservices handle resilience in distributed systems?
??x
Microservices architecture enhances resilience by isolating services that can fail independently without bringing down the entire system. They degrade functionality when a service fails, ensuring the rest of the application continues to operate. This requires understanding and handling various failure modes, such as network failures or machine crashes.

```java
// Example of fault tolerance in Java using Circuit Breaker pattern
@CircuitBreaker(name = ""serviceA"", fallbackMethod = ""fallbackServiceAFetch"")
public String fetchFromServiceA() {
    // Service call logic
}

private String fallbackServiceAFetch(Exception e) {
    return ""Fallback service response"";
}
```
x??",1005,"Resilience A key concept in resilience engineering is the bulkhead. If one component of a system fails, but that failure doesn’t cascade, you can isolate the problem and the rest of the system can car...",qwen2.5:latest,2025-11-02 23:38:43,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Modules,Ease of Deployment with Microservices,"#### Ease of Deployment with Microservices

Background context explaining the concept. In monolithic applications, deploying a change requires redeploying the entire application. With microservices, changes can be deployed independently to individual services, reducing deployment risks and enabling faster releases.

:p How does microservice architecture improve ease of deployment compared to monolithic architectures?
??x
In monolithic applications, making even small changes necessitates redeploying the entire application due to its tightly coupled nature. This leads to infrequent deployments with high risk. In contrast, microservices allow deploying individual services independently, reducing deployment risks and enabling faster releases.

```java
// Example of deploying a microservice in Java using Docker and Kubernetes
docker run -d --name serviceB -p 8081:8080 your-service-b-image:latest
```
x??",911,"Resilience A key concept in resilience engineering is the bulkhead. If one component of a system fails, but that failure doesn’t cascade, you can isolate the problem and the rest of the system can car...",qwen2.5:latest,2025-11-02 23:38:43,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Modules,Case Study: Gilt's Adoption of Microservices,"#### Case Study: Gilt's Adoption of Microservices

Background context explaining the concept. Gilt, an online fashion retailer, started with a monolithic Rails application but faced performance issues. By adopting microservices, they were able to handle traffic spikes and scale more effectively.

:p How did Gilt transition from a monolithic architecture to microservices?
??x
Gilt began as a monolithic Rails application that could not handle increasing traffic. By transitioning to microservices, they were able to isolate components, allowing some services to scale independently while others remained on less powerful hardware. Today, Gilt has over 450 microservices, each running on multiple machines, providing better performance and scalability.

```java
// Example of splitting a monolithic application into microservices in Java using Spring Boot
@SpringBootApplication
public class UserServiceApplication {
    public static void main(String[] args) {
        SpringApplication.run(UserServiceApplication.class, args);
    }
}
```
x??

---",1050,"Resilience A key concept in resilience engineering is the bulkhead. If one component of a system fails, but that failure doesn’t cascade, you can isolate the problem and the rest of the system can car...",qwen2.5:latest,2025-11-02 23:38:43,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Modules,Organizational Alignment,"#### Organizational Alignment
Microservices can help align your architecture to your organization by breaking down large teams and codebases into smaller, more manageable units. This approach can lead to increased productivity and better team management, especially when teams are distributed.

:p How do microservices contribute to organizational alignment?
??x
Microservices allow for the decomposition of a large application into smaller services, each owned and managed by a separate team. This breakdown aligns the architecture with the organization's structure, reducing the number of people working on any one codebase to optimize productivity.

For example, consider an e-commerce platform where different teams handle customer support, product catalog management, payment processing, etc., each service being independently deployable and maintainable:

```java
// Customer Support Service
@Service
public class CustomerSupportService {
    public void handleTicket(Ticket ticket) {
        // Logic to process the ticket
    }
}

// Product Catalog Management Service
@Service
public class ProductCatalogService {
    public Product findProductById(String id) {
        // Logic to fetch product details
    }
}
```

x??",1229,"The technology in this space has changed greatly in the last couple of years, and we’ll be looking more deeply into the topic of deployment in a microservice world in  Chapter 6 . Organizational Align...",qwen2.5:latest,2025-11-02 23:38:53,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Modules,Composability in Microservices,"#### Composability in Microservices
Microservices provide a flexible way of building and deploying software by breaking down functionalities into smaller, reusable services. This allows for different components to be combined in various ways depending on the context.

:p How do microservices enhance composability?
??x
Composability is enhanced because each service can be developed independently and reused across different applications or channels. For example, a payment processing service could be used by both web and mobile apps, as well as other services within the same organization.

Example of using a microservice in multiple contexts:

```java
// Payment Service
@Service
public class PaymentService {
    public boolean processPayment(PaymentRequest request) {
        // Logic to process payment
        return true;
    }
}

// Web App Using Payment Service
@RestController
@RequestMapping(""/api/v1"")
public class WebController {
    @Autowired
    private PaymentService paymentService;

    @PostMapping(""/pay"")
    public ResponseEntity<String> pay(@RequestBody PaymentRequest request) {
        if (paymentService.processPayment(request)) {
            return new ResponseEntity<>(""Payment successful"", HttpStatus.OK);
        } else {
            return new ResponseEntity<>(""Payment failed"", HttpStatus.INTERNAL_SERVER_ERROR);
        }
    }
}

// Mobile App Using Payment Service
public class PaymentActivity extends AppCompatActivity {
    private PaymentService paymentService;

    @Override
    protected void onCreate(Bundle savedInstanceState) {
        super.onCreate(savedInstanceState);
        paymentService = new RemotePaymentService(); // Assuming remote service
    }

    public void makePayment(PaymentRequest request) {
        if (paymentService.processPayment(request)) {
            Toast.makeText(this, ""Payment successful"", Toast.LENGTH_SHORT).show();
        } else {
            Toast.makeText(this, ""Payment failed"", Toast.LENGTH_SHORT).show();
        }
    }
}
```

x??",2020,"The technology in this space has changed greatly in the last couple of years, and we’ll be looking more deeply into the topic of deployment in a microservice world in  Chapter 6 . Organizational Align...",qwen2.5:latest,2025-11-02 23:38:53,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Modules,Optimizing for Replaceability,"#### Optimizing for Replaceability
Microservices make it easier to replace or delete services by keeping them small and self-contained. This reduces the risk and cost associated with large monolithic systems.

:p How does microservices optimize replaceability?
??x
By breaking down an application into smaller, independent services, it becomes much easier to update or replace components without affecting the entire system. For example, if a service that handles user authentication is identified as inefficient or outdated, it can be replaced with a newer implementation or even removed entirely.

Example of replacing a legacy service:

```java
// Legacy Authentication Service (Monolithic)
public class LegacyAuthService {
    public User authenticate(String username, String password) throws Exception {
        // Complex and brittle logic to authenticate user
        return null;
    }
}

// New Microservice-based Authentication Service
@Service
public class AuthService {
    @Autowired
    private UserRepository userRepository;

    public User authenticate(String username, String password) throws Exception {
        User user = userRepository.findByUsername(username);
        if (user != null && user.getPassword().equals(password)) {
            return user;
        }
        throw new Exception(""Authentication failed"");
    }
}
```

x??

---",1361,"The technology in this space has changed greatly in the last couple of years, and we’ll be looking more deeply into the topic of deployment in a microservice world in  Chapter 6 . Organizational Align...",qwen2.5:latest,2025-11-02 23:38:53,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Modules,Service-Oriented Architecture (SOA),"#### Service-Oriented Architecture (SOA)
Background context: SOA is a design approach where multiple services collaborate to provide some end set of capabilities. A service typically means a completely separate operating system process, and communication between these services occurs via calls across a network rather than method calls within a process boundary.
This approach emerged as an attempt to address the challenges associated with large monolithic applications by promoting reusability of software components. The goal is to make it easier to maintain or rewrite software, allowing one service to be replaced without affecting others if the semantics remain unchanged.

:p What is SOA and what problem does it aim to solve?
??x
SOA is a design approach where multiple services collaborate to provide some end set of capabilities. It aims to address the challenges associated with large monolithic applications by promoting reusability, making maintenance or rewriting easier. The goal is to enable one service to be replaced without affecting others as long as the semantics remain unchanged.
x??",1107,"When a codebase is just a few hundred lines long, it is difficult for people to become emotionally attached to it, and the cost of replacing it is pretty small. What About Service-Oriented Architectur...",qwen2.5:latest,2025-11-02 23:39:01,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Modules,Challenges in Implementing SOA,"#### Challenges in Implementing SOA
Background context: Despite its potential benefits, there has been a lack of good consensus on how to implement SOA effectively. Many issues attributed to SOA are actually due to communication protocols (e.g., SOAP), vendor middleware, a lack of guidance about service granularity, or incorrect guidance on splitting the system.
Cynics suggest that vendors might have co-opted and driven the SOA movement for commercial gain, which sometimes undermined its goals.

:p What are some common issues with implementing SOA?
??x
Common issues with implementing SOA include communication protocols like SOAP, vendor middleware problems, lack of guidance on service granularity, or incorrect guidance on splitting the system. These factors can lead to challenges in achieving the intended benefits.
x??",830,"When a codebase is just a few hundred lines long, it is difficult for people to become emotionally attached to it, and the cost of replacing it is pretty small. What About Service-Oriented Architectur...",qwen2.5:latest,2025-11-02 23:39:01,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Modules,Microservices Approach,"#### Microservices Approach
Background context: The microservice approach has emerged from real-world use, aiming to address the pitfalls associated with SOA by taking a more granular and practical approach. It is seen as a specific approach for SOA, similar to XP or Scrum being specific approaches for Agile software development.

:p What is the microservices approach and how does it differ from SOA?
??x
The microservices approach is a specific implementation of SOA that focuses on granularity and practicality. Unlike traditional SOA, which has faced challenges due to various issues like communication protocols and guidance, microservices aim to provide clear benefits through granular decomposition. It is akin to XP or Scrum in the Agile methodology space.
x??",770,"When a codebase is just a few hundred lines long, it is difficult for people to become emotionally attached to it, and the cost of replacing it is pretty small. What About Service-Oriented Architectur...",qwen2.5:latest,2025-11-02 23:39:01,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Modules,Decompositional Techniques: Shared Libraries,"#### Decompositional Techniques: Shared Libraries
Background context: A common decompositional technique involves breaking down a codebase into multiple libraries that can be shared between teams and services. This approach helps in reusing functionality and ensuring modularity within the system.

:p What are shared libraries, and how do they contribute to software development?
??x
Shared libraries are modular components of code that can be reused across different parts of an application or by various teams. They help in sharing functionality between services and promote modularity. By encapsulating common functionalities into reusable libraries, developers can maintain cleaner codebases and reduce redundancy.

For example:
```java
public class MathLibrary {
    public static int add(int a, int b) {
        return a + b;
    }

    public static int subtract(int a, int b) {
        return a - b;
    }
}
```
x??

---",929,"When a codebase is just a few hundred lines long, it is difficult for people to become emotionally attached to it, and the cost of replacing it is pretty small. What About Service-Oriented Architectur...",qwen2.5:latest,2025-11-02 23:39:01,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Modules,Language Heterogeneity in Shared Libraries,"#### Language Heterogeneity in Shared Libraries
Background context explaining the importance of language heterogeneity. Discuss how shared libraries typically require the same programming language or platform to function effectively, which can limit technology choices within a project.

:p What are the implications of using shared libraries for code reuse across different services written in different languages?
??x
Using shared libraries for code reuse across different services can lead to limitations in technology heterogeneity. Services may need to be rewritten or refactored into the same language, which can be challenging and time-consuming. This constraint affects the flexibility of adopting new technologies and integrating diverse service architectures.
```java
// Example Java library
public class MathUtils {
    public static int add(int a, int b) {
        return a + b;
    }
}

// Another example in Python (not directly integrable with Java)
def multiply(a, b):
    return a * b
```
x??",1009,"I might create a set of useful collection utilities, for example, or perhaps a statistics library that can be reused. Teams can organize themselves around these libraries, and the libraries themselves...",qwen2.5:latest,2025-11-02 23:39:14,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Modules,Scalability and Deployment Concerns of Shared Libraries,"#### Scalability and Deployment Concerns of Shared Libraries
Background context on the challenges faced when scaling parts of a system independently. Explain how shared libraries can constrain scalability due to redeployment requirements.

:p How does using shared libraries impact the ability to scale different components of a system independently?
??x
Shared libraries often limit the ability to scale parts of your system independently because changes to these libraries usually require redeploying the entire process or service that depends on them. This is problematic as it can lead to downtime and make it harder to achieve true microservices architecture.

```java
// Example: A shared library needs to be updated.
public class ConfigManager {
    public void reloadConfig() {
        // Update configuration settings
    }
}

// Service using the library
public class MyService {
    private final ConfigManager configManager;

    @PostConstruct
    public void init() {
        configManager.reloadConfig();
    }
}
```
x??",1035,"I might create a set of useful collection utilities, for example, or perhaps a statistics library that can be reused. Teams can organize themselves around these libraries, and the libraries themselves...",qwen2.5:latest,2025-11-02 23:39:14,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Modules,Architectural Safety Measures and Coupling Concerns with Shared Libraries,"#### Architectural Safety Measures and Coupling Concerns with Shared Libraries
Discuss architectural safety measures, such as circuit breakers or retries, that can be compromised when using shared libraries.

:p What are the risks associated with lacking architectural safety measures in a system relying heavily on shared libraries?
??x
Relying solely on shared libraries without proper architectural safety measures can lead to increased risk of failure propagation. For instance, if a single library fails, it may cause cascading failures across multiple services due to tight coupling. Implementations like circuit breakers or retries must be carefully designed and managed at the application level.

```java
// Example Circuit Breaker implementation using Guava
import com.google.common.util.concurrent.RateLimiter;

RateLimiter rateLimiter = RateLimiter.create(10.0); // 10 requests per second

public int callService() {
    if (!rateLimiter.tryAcquire()) {
        throw new RateLimitException();
    }
    return serviceCall(); // Assume this is the service call that might fail
}
```
x??",1097,"I might create a set of useful collection utilities, for example, or perhaps a statistics library that can be reused. Teams can organize themselves around these libraries, and the libraries themselves...",qwen2.5:latest,2025-11-02 23:39:14,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Modules,Modular Decomposition in Java with OSGi,"#### Modular Decomposition in Java with OSGi
Background on modular decomposition techniques, specifically focusing on OSGi and its limitations.

:p How does OSGi address some of the issues faced by shared libraries while still having inherent challenges?
??x
OSGi aims to provide a more flexible approach to modular decomposition by allowing modules to be deployed and updated dynamically within a running application. However, it introduces complexity due to its reliance on external frameworks and the need for proper module isolation.

```java
// Example OSGi service registration
public class MyService implements Service {
    public void activate(ComponentContext context) {
        // Initialization code here
    }
}

// Example service use in another component
@Component
public class ConsumerComponent {
    @Reference
    private MyService myService;
}
```
x??",871,"I might create a set of useful collection utilities, for example, or perhaps a statistics library that can be reused. Teams can organize themselves around these libraries, and the libraries themselves...",qwen2.5:latest,2025-11-02 23:39:14,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Modules,Erlang Modules and Their Advantages,"#### Erlang Modules and Their Advantages
Explanation of how Erlang handles modular decomposition, providing detailed benefits.

:p What makes Erlang's module system particularly advantageous for modular decomposition?
??x
Erlang’s modules are designed to be highly independent and self-contained. They can be stopped, restarted, or upgraded without affecting the rest of the application. This allows for more flexible deployment strategies and enhances system resilience.

```erlang
-module(my_module).
-export([start/0, stop/0]).

start() ->
    % Module initialization code here.
    ok.

stop() ->
    % Cleanup code here.
    ok.
```
x??",641,"I might create a set of useful collection utilities, for example, or perhaps a statistics library that can be reused. Teams can organize themselves around these libraries, and the libraries themselves...",qwen2.5:latest,2025-11-02 23:39:14,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Modules,Challenges of Modular Decomposition Within Process Boundaries,"#### Challenges of Modular Decomposition Within Process Boundaries
Explanation on why modular decomposition within a single process can lead to tight coupling and reduced benefits.

:p Why is achieving clean separation in modules difficult even within a single process boundary?
??x
Despite the promise of independent module boundaries, maintaining clean separation within a process can be challenging. Modules may unintentionally become tightly coupled due to shared state or other dependencies, which defeats the purpose of modularity. Process boundaries provide natural isolation but are not always practical.

```java
// Example of potential tight coupling in a monolithic application
public class ServiceA {
    private static final Map<String, String> cache = new HashMap<>();

    public void doSomething() {
        // Accessing shared state
        String value = cache.get(""key"");
        // More logic here
    }
}
```
x??",933,"I might create a set of useful collection utilities, for example, or perhaps a statistics library that can be reused. Teams can organize themselves around these libraries, and the libraries themselves...",qwen2.5:latest,2025-11-02 23:39:14,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Inaccurate Comparisons,No Silver Bullet for Microservices,"#### No Silver Bullet for Microservices
Background context: The passage discusses microservices and their limitations, emphasizing that they are not a one-size-fits-all solution. It highlights that while microservices offer flexibility and choice, they also bring increased complexity due to distributed systems challenges.

:p What is the main point about microservices mentioned in this text?
??x
Microservices are no silver bullet or free lunch; they require significant effort in deployment, testing, monitoring, scaling, and ensuring resilience. Additionally, they introduce complexities such as managing distributed transactions and understanding the CAP theorem.
x??",673,"No Silver Bullet Before we finish, I should call out that microservices are no free lunch or silver bullet, and make for a bad choice as a golden hammer. They have all the associated complexities of d...",qwen2.5:latest,2025-11-02 23:39:24,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Inaccurate Comparisons,Architectural Evolution in Microservices,"#### Architectural Evolution in Microservices
Background context: The passage discusses how microservices change the role of architects from a static, design-oriented position to a more dynamic, evolutionary one that focuses on guiding system evolution through rapid changes.

:p How does the role of an architect evolve with microservices?
??x
With microservices, the traditional static, design-focused architectural role transforms into an evolutionary architecture where architects must navigate and guide rapid changes in technology stacks, programming idioms, and service boundaries.
x??",592,"No Silver Bullet Before we finish, I should call out that microservices are no free lunch or silver bullet, and make for a bad choice as a golden hammer. They have all the associated complexities of d...",qwen2.5:latest,2025-11-02 23:39:24,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Inaccurate Comparisons,Architect Role Redefined,"#### Architect Role Redefined
Background context: The text critiques the term ""architect"" borrowed from other professions like engineering or medicine, pointing out that it often leads to misunderstandings about the architect's role and responsibilities. It highlights issues with certification and the expectations placed on architects.

:p Why is the term ""architect"" problematic in the context of software development?
??x
The term ""architect"" can be problematic because it implies a level of expertise and responsibility that may not accurately reflect the current state of the software industry. Architects are expected to have a deep understanding of complex systems, but the rapid changes and unique challenges of software development mean that these expectations are often unrealistic.
x??",797,"No Silver Bullet Before we finish, I should call out that microservices are no free lunch or silver bullet, and make for a bad choice as a golden hammer. They have all the associated complexities of d...",qwen2.5:latest,2025-11-02 23:39:24,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Inaccurate Comparisons,Architect Responsibilities,"#### Architect Responsibilities
Background context: The passage emphasizes the architect's role in ensuring a cohesive technical vision for the system while managing multiple teams and coordinating efforts across different geographical locations.

:p What is the primary responsibility of an architect according to this text?
??x
The primary responsibility of an architect is to ensure a joined-up technical vision that helps deliver the system required by customers, which involves working with or coordinating multiple teams and managing the overall architecture.
x??",569,"No Silver Bullet Before we finish, I should call out that microservices are no free lunch or silver bullet, and make for a bad choice as a golden hammer. They have all the associated complexities of d...",qwen2.5:latest,2025-11-02 23:39:24,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Inaccurate Comparisons,Challenges in Defining Architect Role,"#### Challenges in Defining Architect Role
Background context: The text discusses the difficulties in precisely defining the architect's role due to its varied responsibilities across different organizational levels. It also mentions societal misunderstandings about the profession.

:p Why is it challenging to define the architect's role accurately?
??x
Defining the architect's role accurately is challenging because it varies widely depending on the level of responsibility—whether working with one team or coordinating multiple teams globally. Additionally, society often borrows terms like ""architect"" and ""engineer"" without fully understanding their application in software development.
x??",697,"No Silver Bullet Before we finish, I should call out that microservices are no free lunch or silver bullet, and make for a bad choice as a golden hammer. They have all the associated complexities of d...",qwen2.5:latest,2025-11-02 23:39:24,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Inaccurate Comparisons,Architect vs. Other Professions,"#### Architect vs. Other Professions
Background context: The passage contrasts the architectural profession with its counterpart in engineering, highlighting differences such as the physical constraints faced by real architects versus the flexibility of software architectures.

:p How do the challenges faced by software architects compare to those faced by traditional architects?
??x
Software architects face fundamentally different challenges compared to traditional architects due to the dynamic and flexible nature of software systems. Unlike traditional architecture, which is constrained by physical rules like materials and gravity, software can adapt and evolve with changing user requirements.
x??",708,"No Silver Bullet Before we finish, I should call out that microservices are no free lunch or silver bullet, and make for a bad choice as a golden hammer. They have all the associated complexities of d...",qwen2.5:latest,2025-11-02 23:39:24,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Inaccurate Comparisons,Conclusion on Microservices and Architect Role,"#### Conclusion on Microservices and Architect Role
Background context: The passage concludes that while microservices offer many benefits, their adoption requires careful consideration based on the specific needs of the organization. It emphasizes the importance of understanding potential pitfalls to navigate a steady path forward.

:p What is the key takeaway regarding microservices and architects according to this text?
??x
The key takeaway is that microservices are not a silver bullet and require significant architectural thought, including considering deployment, testing, monitoring, scaling, and resilience. Architects need to reassess their role in guiding system evolution with a focus on adaptability and rapid change.
x??

---",743,"No Silver Bullet Before we finish, I should call out that microservices are no free lunch or silver bullet, and make for a bad choice as a golden hammer. They have all the associated complexities of d...",qwen2.5:latest,2025-11-02 23:39:24,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,An Evolutionary Vision for the Architect,Evolutionary Thinking for Architects,"#### Evolutionary Thinking for Architects

Background context: The text discusses how architects for software systems need to shift their focus from creating perfect end products to designing frameworks that can evolve over time. This is compared to town planners, who do not specify every building but instead zone areas and plan for the flow of people and utilities.

:p What analogy does the author use to explain how architects should think about evolving software systems?
??x
The author uses the analogy of a town planner in SimCity. Just as a town planner zones different parts of a city (industrial, residential) without specifying every building, software architects should design frameworks that allow for flexibility and growth.
x??",743,An Evolutionary Vision for the Architect Our requirements shift more rapidly than they do for people who design and build buildings — as do the tools and techniques at our disposal. The things we crea...,qwen2.5:latest,2025-11-02 23:39:33,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,An Evolutionary Vision for the Architect,Architect Role as Town Planner,"#### Architect Role as Town Planner

Background context: The role of an IT architect is compared to that of a town planner who optimizes the layout of a city based on current and anticipated future needs. The focus is on creating zones rather than specifying every detail.

:p How does the role of an IT architect differ from traditional architecture?
??x
The role of an IT architect differs significantly because they do not create fixed, perfect end products but instead design frameworks that can evolve over time. They set broad goals and constraints (like zoning in a city) but allow developers to implement specific solutions within these guidelines.
x??",660,An Evolutionary Vision for the Architect Our requirements shift more rapidly than they do for people who design and build buildings — as do the tools and techniques at our disposal. The things we crea...,qwen2.5:latest,2025-11-02 23:39:33,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,An Evolutionary Vision for the Architect,Zoning vs. Detailed Planning,"#### Zoning vs. Detailed Planning

Background context: The text emphasizes the importance of zoning areas for different purposes rather than detailed planning, allowing the system to adapt as needed.

:p Why does the author recommend focusing on zoning rather than detailed planning?
??x
The author recommends focusing on zoning because it allows the system to evolve naturally based on real-world usage. Detailed planning is often impractical given the unpredictability of future needs and changes in technology.
x??",517,An Evolutionary Vision for the Architect Our requirements shift more rapidly than they do for people who design and build buildings — as do the tools and techniques at our disposal. The things we crea...,qwen2.5:latest,2025-11-02 23:39:33,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,An Evolutionary Vision for the Architect,Habitability for Developers,"#### Habitability for Developers

Background context: Architects must ensure that their systems are not only user-friendly but also developer-friendly, providing a good working environment for those who will maintain and extend the system.

:p What does the author mean by ensuring the system is ""habitable"" for developers?
??x
Ensuring the system is habitable for developers means creating an environment where they can work efficiently and effectively. This includes having clear structure, ease of maintenance, and tools that support development processes.
x??",563,An Evolutionary Vision for the Architect Our requirements shift more rapidly than they do for people who design and build buildings — as do the tools and techniques at our disposal. The things we crea...,qwen2.5:latest,2025-11-02 23:39:33,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,An Evolutionary Vision for the Architect,Flexibility in Implementation,"#### Flexibility in Implementation

Background context: Architects should set broad goals but allow detailed implementation to be flexible based on user needs and evolving technology.

:p How should architects balance between setting direction and allowing flexibility?
??x
Architects should set broad strokes for the overall system architecture and constraints (like zoning in a city). However, they should avoid over-specifying details so that developers can adapt solutions as needed. This balance ensures both flexibility and coherence.
x??",544,An Evolutionary Vision for the Architect Our requirements shift more rapidly than they do for people who design and build buildings — as do the tools and techniques at our disposal. The things we crea...,qwen2.5:latest,2025-11-02 23:39:33,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,An Evolutionary Vision for the Architect,Anticipation of Change,"#### Anticipation of Change

Background context: The text highlights the need to anticipate changes but not try to control every aspect of the system's evolution.

:p Why is it important for architects to plan for change?
??x
It is crucial because software systems must adapt to changing user needs, technologies, and business environments. Architects who can anticipate these changes are better positioned to guide the system's evolution without being overly rigid.
x??",470,An Evolutionary Vision for the Architect Our requirements shift more rapidly than they do for people who design and build buildings — as do the tools and techniques at our disposal. The things we crea...,qwen2.5:latest,2025-11-02 23:39:33,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,An Evolutionary Vision for the Architect,User and Developer Happiness,"#### User and Developer Happiness

Background context: The architect’s goal should be to create a system that satisfies both users and developers.

:p How do architects ensure satisfaction for both users and developers?
??x
Architects can achieve this by designing systems that are intuitive for users while providing the necessary flexibility for developers. This balance ensures that everyone involved finds the system valuable and easy to work with.
x??

---",461,An Evolutionary Vision for the Architect Our requirements shift more rapidly than they do for people who design and build buildings — as do the tools and techniques at our disposal. The things we crea...,qwen2.5:latest,2025-11-02 23:39:33,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Zoning,Service Boundaries and Zones,"#### Service Boundaries and Zones
In the context of architecting systems, we often think of our services as zones or coarse-grained groups. These boundaries help us manage complexity by separating concerns into manageable pieces. The focus for architects should be on how these zones communicate with each other rather than on internal details.
:p What are service boundaries in an architectural context?
??x
Service boundaries, or zones, refer to the separation of services into distinct, coarse-grained groups that allow for better management and scaling. Architects need to ensure proper communication protocols between these zones while giving teams autonomy within their zone.
x??",685,"Zoning So, to continue the metaphor of the architect as town planner for a moment, what are our zones? These are our service boundaries, or perhaps coarse-grained groups of services. As architects, we...",qwen2.5:latest,2025-11-02 23:39:42,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Zoning,Microservices and Team Autonomy,"#### Microservices and Team Autonomy
Many organizations have adopted microservices to maximize team autonomy. This approach allows each team to make local decisions about technology stacks and data stores, promoting flexibility and innovation.
:p How do microservices support team autonomy?
??x
Microservices enable teams to independently choose their technology stack and data store based on the specific needs of their service. This autonomy can lead to better alignment with business goals and faster delivery times.
x??",523,"Zoning So, to continue the metaphor of the architect as town planner for a moment, what are our zones? These are our service boundaries, or perhaps coarse-grained groups of services. As architects, we...",qwen2.5:latest,2025-11-02 23:39:42,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Zoning,Standardizing Technology Stacks,"#### Standardizing Technology Stacks
While allowing for diversity in zones, it's important to consider standardization across services. Large organizations like Netflix have standardized on a particular technology (e.g., Cassandra) to reduce operational complexity and training costs.
:p Why might an organization standardize its technology stack?
??x
Standardizing the technology stack can simplify operations by reducing the number of platforms that need to be managed, which in turn lowers operational overhead. It also helps in scaling teams more easily since they do not need to specialize in multiple technologies.
x??",624,"Zoning So, to continue the metaphor of the architect as town planner for a moment, what are our zones? These are our service boundaries, or perhaps coarse-grained groups of services. As architects, we...",qwen2.5:latest,2025-11-02 23:39:42,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Zoning,Integration and Communication Protocols,"#### Integration and Communication Protocols
When different services use different communication protocols (e.g., REST over HTTP vs. Protocol Buffers), integration becomes complex. A consistent approach can help streamline interactions between services.
:p What are the challenges of using diverse communication protocols between services?
??x
Using various communication protocols can lead to increased complexity in integrating services, making it harder for consuming services to handle multiple styles of data interchange. This can result in a tangled and hard-to-maintain system architecture.
x??",601,"Zoning So, to continue the metaphor of the architect as town planner for a moment, what are our zones? These are our service boundaries, or perhaps coarse-grained groups of services. As architects, we...",qwen2.5:latest,2025-11-02 23:39:42,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Zoning,Pair Programming and Architectural Awareness,"#### Pair Programming and Architectural Awareness
Architects should spend time coding with developers to ensure their decisions are practical and align with real-world development challenges. Pair programming is an effective method for architects to gain hands-on experience.
:p How can architects participate in pair programming?
??x
Architects can join teams as a partner during pair programming sessions, working on normal stories alongside team members. This direct interaction helps architects understand the day-to-day realities of development and improve communication with the team.
x??",594,"Zoning So, to continue the metaphor of the architect as town planner for a moment, what are our zones? These are our service boundaries, or perhaps coarse-grained groups of services. As architects, we...",qwen2.5:latest,2025-11-02 23:39:42,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Zoning,Regular Team Engagement,"#### Regular Team Engagement
Regularly spending time with teams is crucial for architects to stay informed about ongoing projects and challenges faced by developers. Frequent interactions can enhance collaboration and ensure that architectural decisions are well-considered.
:p How often should an architect spend time with a team?
??x
The frequency of engagement depends on the size of the team(s). For instance, spending half a day with each team every four weeks can help maintain awareness and improve communication. The key is to make such interactions routine.
x??

---",575,"Zoning So, to continue the metaphor of the architect as town planner for a moment, what are our zones? These are our service boundaries, or perhaps coarse-grained groups of services. As architects, we...",qwen2.5:latest,2025-11-02 23:39:42,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Tailored Service Template,Strategic Goals,"#### Strategic Goals
Strategic goals define where a company is headed and how it intends to make its customers happy. These are high-level, often non-technical objectives that might be defined at a company or division level.

:p What are strategic goals, and why are they important for system design?
??x
Strategic goals are the overarching visions of where your organization wants to go and what kind of customer satisfaction it aims to achieve. For example, ""Expand into Southeast Asia to unlock new markets"" is a clear statement of intent that influences technical choices. Aligning the technology with these strategic goals ensures that the architectural decisions support the company's broader ambitions.

In system design, understanding the strategic goals helps in making informed trade-offs and ensuring that the technical strategy complements the business objectives.
x??",880,"A Principled Approach Rules are for the obedience of fools and the guidance of wise men. Generally attributed to Douglas Bader Making decisions in system design is all about trade-offs, and microservi...",qwen2.5:latest,2025-11-02 23:39:53,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Tailored Service Template,Principles,"#### Principles
Principles are rules established to align specific actions or decisions with larger strategic goals. They can evolve as the organization’s goals change.

:p What are principles in the context of system design?
??x
Principles serve as guiding rules that help make technical decisions consistent with broader business objectives. For instance, if a company aims to accelerate feature delivery, it might establish a principle allowing teams full control over their software lifecycle for quicker releases.

Principles should be few in number—ideally less than 10—to ensure they are memorable and not overly conflicting.
```java
// Example of a principle rule in Java
public class PrincipleExample {
    public static void applyPrinciple(String goal) {
        if (""accelerate feature delivery"".equals(goal)) {
            // Logic to give teams full control over their software lifecycle
        }
    }
}
```
x??",926,"A Principled Approach Rules are for the obedience of fools and the guidance of wise men. Generally attributed to Douglas Bader Making decisions in system design is all about trade-offs, and microservi...",qwen2.5:latest,2025-11-02 23:39:53,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Tailored Service Template,12 Factors,"#### 12 Factors
The Heroku 12 Factors are a set of design principles aimed at creating applications that work well on the Heroku platform. These factors can be applied in other contexts as well.

:p What is the purpose of the 12 Factors, and how might they be applied?
??x
The 12 Factors were created to guide developers in building cloud-native applications suitable for deployment on platforms like Heroku. While originally targeted at Heroku, these principles are applicable beyond that platform:

- **Factor 6: Config** - Externalize configuration so that your application can adapt to different environments without changes.
```java
// Example of externalizing config using properties file or environment variables
public class AppConfig {
    private final String databaseUrl;

    public AppConfig(String dbUrl) {
        this.databaseUrl = dbUrl;
    }

    // Method to get the database URL from a property or env var
}
```

- **Factor 9: Security** - Protect sensitive data and handle security concerns like encryption, authentication, and authorization.
```java
// Example of secure handling using environment variables
public class SecureConfig {
    private static final String SECRET_KEY = System.getenv(""SECRET_KEY"");
    
    public void authenticateUser(String token) throws AuthenticationException {
        if (!isValidToken(token)) {
            throw new AuthenticationException();
        }
    }
}
```
x??",1428,"A Principled Approach Rules are for the obedience of fools and the guidance of wise men. Generally attributed to Douglas Bader Making decisions in system design is all about trade-offs, and microservi...",qwen2.5:latest,2025-11-02 23:39:53,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Tailored Service Template,Trade-offs in Microservice Architectures,"#### Trade-offs in Microservice Architectures
Microservices offer flexibility but come with complex trade-offs, such as the choice between familiarity and performance.

:p What are some key trade-offs involved in choosing a datastore for microservices?
??x
When picking a datastore for microservices, you face several trade-offs. For instance:
- **Familiarity vs. Performance**: You might choose a less familiar but more scalable platform to support growth.
- **Single vs. Multiple Technology Stacks**: Determining whether it's acceptable to have one or multiple technology stacks in your system.

Decision-making on incomplete information requires a framework, such as principles and practices, to guide choices consistently with strategic goals.
```java
// Example of decision making process based on principle
public class MicroserviceDecisions {
    public static String chooseDatastore(String goal) {
        if (""scale"".equals(goal)) {
            return ""NoSQL""; // For better scaling
        } else {
            return ""RDBMS""; // For familiarity and proven reliability
        }
    }
}
```
x??",1104,"A Principled Approach Rules are for the obedience of fools and the guidance of wise men. Generally attributed to Douglas Bader Making decisions in system design is all about trade-offs, and microservi...",qwen2.5:latest,2025-11-02 23:39:53,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Tailored Service Template,Aligning Technology with Business Objectives,"#### Aligning Technology with Business Objectives
Architects must ensure that technical choices support the business’s strategic direction, often requiring interaction with non-technical stakeholders.

:p How do architects align technology decisions with the company's strategic goals?
??x
Aligning technology with strategic goals involves understanding and integrating high-level business objectives into architectural decisions. This might mean working closely with departments like marketing or sales to gather insights on customer needs and preferences that can inform tech choices.

For example, if a key goal is expanding internationally, principles could include making the entire system portable to respect data sovereignty.
```java
// Example of aligning technology with business goals
public class BusinessGoalAlignment {
    public void expandIntoNewMarket() {
        // Ensure all systems are modular and portable for easy deployment in new regions
    }
}
```
x??",977,"A Principled Approach Rules are for the obedience of fools and the guidance of wise men. Generally attributed to Douglas Bader Making decisions in system design is all about trade-offs, and microservi...",qwen2.5:latest,2025-11-02 23:39:53,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Tailored Service Template,Practices Definition and Importance,"#### Practices Definition and Importance
Background context: The provided text introduces practices as a set of detailed, practical guidance for performing tasks. They are technology-specific and should be low level enough that any developer can understand them. Practices often change more frequently than principles due to their technical nature.

:p What is the definition and importance of practices?
??x
Practices are detailed, practical guidelines that ensure principles are carried out in a project or organization. They provide clear steps for developers to follow and can include coding standards, logging requirements, or integration styles. Practices are important because they offer specific instructions on how to implement overarching ideas (principles) into real-world scenarios.

For example:
```java
// Coding Practice: Logging Standard
public void processRequest(HttpServletRequest request, HttpServletResponse response) {
    // Log the start of a new request
    log.info(""Processing Request: {}"", request.getRequestURI());
    
    // Process the request and set appropriate response status.
    try {
        response.setStatus(HttpServletResponse.SC_OK);
    } catch (IOException e) {
        throw new RuntimeException(""Failed to process request."", e);
    }
}
```
x??",1292,"Practices Our practices are how we ensure our principles are being carried out. They are a set of detailed, practical guidance for performing tasks. They will often be technology-specific, and should ...",qwen2.5:latest,2025-11-02 23:40:05,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Tailored Service Template,Combining Principles and Practices,"#### Combining Principles and Practices
Background context: The text mentions that principles are overarching ideas, while practices provide detailed steps for implementing these ideas. In smaller organizations or single teams, combining both might be acceptable, but larger organizations with diverse technologies may need different sets of practices.

:p How do you combine principles and practices in a project?
??x
In projects, principles guide the overall direction and philosophy, whereas practices offer specific implementation details. For example, if a principle states that all services should control their lifecycle fully, this could translate into a practice like deploying services into isolated AWS accounts with self-service management.

Combining both:
```java
// Principle: Full Lifecycle Control for Services
public class ServiceManager {
    private final AWSAccountService awsAccountService;

    public ServiceManager(AWSAccountService awsAccountService) {
        this.awsAccountService = awsAccountService;
    }

    // Practice: Isolated AWS Accounts with Self-Service Management
    public void deployNewService(String serviceName, String serviceDescription) {
        AWSAccount newAccount = awsAccountService.createIsolatedAWSAccount(serviceName, serviceDescription);
        newAccount.setSelfServiceManagement(true); // Enable self-service for the new account.
    }
}
```
x??",1407,"Practices Our practices are how we ensure our principles are being carried out. They are a set of detailed, practical guidance for performing tasks. They will often be technology-specific, and should ...",qwen2.5:latest,2025-11-02 23:40:05,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Tailored Service Template,Real-World Example of Principles and Practices,"#### Real-World Example of Principles and Practices
Background context: The text uses a real-world example to illustrate how principles and practices interact. It mentions that while some practices will change more frequently, principles tend to remain static over time.

:p What is an example provided in the text for illustrating principles and practices?
??x
The text provides Evan Bottcher's diagram as an example of how principles and practices interplay. The diagram shows that while specific practices (on the right) change regularly, overarching principles (in the middle) remain consistent. This helps teams understand which aspects are flexible versus those that need to stay constant.

Example:
```java
// Example Principle: System Manageability
public class SystemManager {
    private final LoggingService loggingService;
    
    public SystemManager(LoggingService loggingService) {
        this.loggingService = loggingService;
    }

    // Practice: Centralized Logging
    public void logEvent(String message) {
        loggingService.log(message);
    }
}
```
x??",1083,"Practices Our practices are how we ensure our principles are being carried out. They are a set of detailed, practical guidance for performing tasks. They will often be technology-specific, and should ...",qwen2.5:latest,2025-11-02 23:40:05,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Tailored Service Template,Required Standard for Variability,"#### Required Standard for Variability
Background context: The text emphasizes the need to define a ""good citizen"" service that ensures the manageability of the system. It suggests identifying what should remain constant across services.

:p What is the required standard for defining variability in systems?
??x
To define a required standard, identify key characteristics of a well-behaved or good service within your system. This includes ensuring services can be managed independently and that no single bad service brings down the entire system. For example, setting up services to handle errors gracefully, log all relevant data, and perform self-service management.

Example:
```java
// Good Citizen Service Principles
public class GoodCitizenService {
    private final ErrorHandlingStrategy errorHandler;
    private final LoggingService logger;

    public GoodCitizenService(ErrorHandlingStrategy errorHandler, LoggingService logger) {
        this.errorHandler = errorHandler;
        this.logger = logger;
    }

    // Ensure services can handle errors gracefully and log relevant data.
    public void processRequest() throws ServiceException {
        try {
            // Process the request
        } catch (Exception e) {
            errorHandler.handle(e);
        }
        logger.logDetails();
    }
}
```
x??

---",1335,"Practices Our practices are how we ensure our principles are being carried out. They are a set of detailed, practical guidance for performing tasks. They will often be technology-specific, and should ...",qwen2.5:latest,2025-11-02 23:40:05,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Tailored Service Template,Monitoring System Health,"#### Monitoring System Health
Background context: Ensuring that a system-wide view of health is crucial for monitoring services. Individual service metrics alone are often insufficient; instead, coherent cross-service views provide better insights into broader issues and trends.

:p How can we ensure a cohesive system-wide view of our microservices' health?
??x
To achieve a cohesive system-wide view, all services should emit standardized health and monitoring-related metrics in the same way. For example, you might use Graphite for metrics and Nagios for health checks. By adopting a push mechanism where each service pushes data to a central location or using polling systems that scrape data from nodes, you can maintain standardization without requiring changes to your monitoring systems.

```java
// Example of pushing metrics to a centralized Graphite server
public class MetricPusher {
    public void pushMetrics(String metricName, int value) {
        // Code to send the metric to the Graphite server
    }
}
```
x??

#### Defining Service Interfaces
Background context: Selecting and standardizing interface technologies helps in integrating new consumers. Too many different integration styles can lead to complexity and potential errors.

:p What is the recommended approach for defining service interfaces?
??x
Picking a small number of defined interface technologies, ideally one or two, is beneficial for integration. For example, if choosing HTTP/REST, decide on specific conventions like using nouns or verbs, handling pagination, and versioning endpoints. This consistency helps in maintaining a unified approach across services.

```java
// Example of defining a RESTful service endpoint with a consistent interface
public class UserService {
    @GetMapping(""/users/{id}"")
    public User getUserById(@PathVariable Long id) {
        // Logic to retrieve user by ID
    }
}
```
x??

#### Architectural Safety Measures
Background context: Ensuring services are resilient against unhealthy downstream calls is crucial for system stability. Using connection pools and circuit breakers can mitigate the impact of failures.

:p How do we ensure architectural safety in microservices?
??x
To ensure architectural safety, each service should have its own connection pool to shield itself from failing downstream calls. Additionally, implementing circuit breakers helps prevent cascading failures. These measures are particularly important at scale and will be covered more thoroughly in Chapter 11.

```java
// Example of using a CircuitBreaker pattern in Java
@CircuitBreaker(name = ""userService"", fallbackMethod = ""handleUserServiceFallback"")
public User getUserById(Long id) {
    // Service call to another microservice
}

private User handleUserServiceFallback(Long id, RuntimeException e) {
    // Fallback logic when the service is unavailable
}
```
x??

#### Response Code Handling
Background context: Proper handling of HTTP response codes ensures that safety measures like circuit breakers work effectively. Consistent use of codes prevents these mechanisms from failing.

:p What are the implications of inconsistent HTTP response code usage in microservices?
??x
Inconsistent use of HTTP response codes, such as sending 2XX for errors or confusing 4XX with 5XX, can undermine the effectiveness of safety measures like circuit breakers. It is essential to differentiate between successful requests, bad requests that prevent processing, and unknown states due to server down.

```java
// Example of handling HTTP responses correctly
public ResponseEntity<User> getUserById(Long id) {
    try {
        User user = userService.getUser(id);
        return ResponseEntity.ok(user); // 200 OK response for success
    } catch (UserServiceException e) {
        return ResponseEntity.status(HttpStatus.BAD_REQUEST).build(); // 400 Bad Request
    }
}
```
x??

---",3888,"And, as with people, what a good citizen is in one context does not reflect what it looks like somewhere else. Nonetheless, there are some common characteristics of well-behaved services that I think ...",qwen2.5:latest,2025-11-02 23:40:14,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Tailored Service Template,Exemplars,"#### Exemplars
Background context explaining how exemplars can be useful to encourage adherence to standards or best practices. Developers prefer code they can run and explore, so having real-world examples can make it easier for them to follow guidelines.

:p What are exemplars used for?
??x
Exemplars are used to provide developers with real-world services that implement good practices, making it easier for them to understand and follow the standards or best practices. By using actual service examples, developers can see how things should be done in practice rather than just reading about them.

```java
// Example of an exemplar class in Java
public class ExemplarService {
    public void run() {
        // Code that follows best practices
    }
}
```
x??",766,"If our services play fast and loose with these rules, we end up with a more vulnerable system. Governance Through Code Getting together and agreeing on how things can be done is a good idea. But spend...",qwen2.5:latest,2025-11-02 23:40:26,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Tailored Service Template,Service Templates,"#### Service Templates
Background context explaining the purpose and benefits of service templates, specifically mentioning Dropwizard and Karyon as microcontainers. These tools help developers get started quickly by providing a basic framework with essential features like health checking, HTTP serving, and metrics.

:p What is a tailored service template?
??x
A tailored service template is a pre-configured set of code that includes most of the necessary components to implement a service according to predefined guidelines. By using such templates, developers can quickly get started while ensuring their services adhere to company standards without much additional effort.

```java
// Example of a basic Dropwizard service setup in Java
public class MyService extends Application<MyApplicationConfiguration> {
    public static void main(String[] args) throws Exception {
        new MyService().run(args);
    }

    @Override
    public void run(MyApplicationConfiguration configuration, Environment environment) {
        // Setup health checks and other services here
    }
}
```
x??",1093,"If our services play fast and loose with these rules, we end up with a more vulnerable system. Governance Through Code Getting together and agreeing on how things can be done is a good idea. But spend...",qwen2.5:latest,2025-11-02 23:40:26,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Tailored Service Template,Using Exemplars Effectively,"#### Using Exemplars Effectively
Background context on the importance of ensuring that exemplars are actually being used in real-world scenarios. This helps validate that the standards or best practices make sense in practice.

:p How can you ensure that your exemplars are effectively used?
??x
To ensure that exemplars are effectively used, it is crucial to use actual services from your system as exemplars rather than isolated perfect examples. Regularly review and update these services to reflect current best practices and continuously monitor their implementation across other services. This ensures that the standards or best practices they represent are practical and useful.

```java
// Example of a real-world service using Hystrix circuit breaker in Java
public class RealWorldService {
    private final Command<String> command;

    public RealWorldService() {
        this.command = new SimpleCommand<>(this::callRemoteService);
    }

    private Future<String> callRemoteService(CommandExecutionCallback callback) {
        // Implementation of the remote service call with Hystrix
    }

    public String execute() throws ExecutionException, InterruptedException {
        return command.execute();
    }
}
```
x??",1234,"If our services play fast and loose with these rules, we end up with a more vulnerable system. Governance Through Code Getting together and agreeing on how things can be done is a good idea. But spend...",qwen2.5:latest,2025-11-02 23:40:26,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Tailored Service Template,Service Templates with Custom Features,"#### Service Templates with Custom Features
Background context on how to tailor a basic microcontainer like Dropwizard or Karyon by adding custom features such as circuit breakers and metrics.

:p How can you integrate Hystrix into a service template?
??x
You can integrate Hystrix into a service template by including the Hystrix library and configuring it within your service setup. Here's how to do it:

```java
// Example of integrating Hystrix with Dropwizard in Java
public class MyService extends Application<MyApplicationConfiguration> {
    private final Command<String> command;

    public static void main(String[] args) throws Exception {
        new MyService().run(args);
    }

    @Override
    public void run(MyApplicationConfiguration configuration, Environment environment) {
        this.command = new SimpleCommand<>(this::callRemoteService);
        // Additional setup for Hystrix if needed
    }

    private Future<String> callRemoteService(CommandExecutionCallback callback) {
        return command.execute();
    }
}
```

This ensures that every service built with the template will have the circuit breaker functionality, reducing the chance of failures due to slow or failing remote services.

x??",1229,"If our services play fast and loose with these rules, we end up with a more vulnerable system. Governance Through Code Getting together and agreeing on how things can be done is a good idea. But spend...",qwen2.5:latest,2025-11-02 23:40:26,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Tailored Service Template,Multiple Technology Stacks and Language Constraints,"#### Multiple Technology Stacks and Language Constraints
Background context on how using a specific service template for one technology stack (e.g., Java) can subtly constrain language choices in teams. This approach might discourage developers from choosing alternative stacks if they have to do more work.

:p How can a tailored service template influence language choices?
??x
A tailored service template, especially when it supports only a single technology stack like Java, can influence team decisions by making other stacks less attractive. Developers may be discouraged from picking alternative languages or frameworks because doing so would require significant additional effort to set up and maintain their services.

```java
// Example of a simplified Dropwizard setup in Java
public class MyService extends Application<MyApplicationConfiguration> {
    public static void main(String[] args) throws Exception {
        new MyService().run(args);
    }

    @Override
    public void run(MyApplicationConfiguration configuration, Environment environment) {
        // Setup services here
    }
}
```

This setup can make it harder for teams to explore or use other languages and technologies, subtly constraining the technology choices available.

x??",1262,"If our services play fast and loose with these rules, we end up with a more vulnerable system. Governance Through Code Getting together and agreeing on how things can be done is a good idea. But spend...",qwen2.5:latest,2025-11-02 23:40:26,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Tailored Service Template,Fault Tolerance Concerns at Netflix,"#### Fault Tolerance Concerns at Netflix
Background context: Netflix is particularly concerned about fault tolerance to ensure that outages in one part of its system do not take everything down. They have invested a lot of effort into client libraries on the JVM to help teams maintain their services well behaved.

:p What are some concerns Netflix has regarding fault tolerance?
??x
Netflix's main concern lies in ensuring that when new technologies or services are introduced, they do not compromise the robust fault-tolerance mechanisms already in place. They focus on minimizing the risk of newly implemented fault tolerance impacting a larger portion of their system negatively.
x??",688,"Netflix, for example, is especially concerned with aspects like fault tolerance, to ensure that the outage of one part of its system cannot take everything down. To handle this, a large amount of work...",qwen2.5:latest,2025-11-02 23:40:40,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Tailored Service Template,Sidecar Services at Netflix,"#### Sidecar Services at Netflix
Background context: To manage fault tolerance, Netflix uses sidecar services that communicate locally with JVMs using appropriate libraries.

:p What is a sidecar service and how does it help in managing fault tolerance?
??x
A sidecar service is a separate process running alongside the main application (often referred to as the primary or business logic). It helps manage aspects such as fault tolerance, security, logging, etc., without affecting the core functionality of the application. This approach ensures that the primary application remains focused on its business logic while sidecar services handle auxiliary tasks like monitoring and resilience.

Example of a sidecar service could be an Envoy proxy that handles network communication and traffic management.
```java
// Pseudocode for a simple sidecar service interaction
public class SidecarService {
    public void start() {
        // Initialize the sidecar service with necessary configurations
        // Example: Setup communication channels, configure logging mechanisms

        // Register as a listener to primary application events
        registerEventListener();
    }

    private void registerEventListener() {
        // Code to listen for primary application events and react accordingly
    }
}
```
x??",1318,"Netflix, for example, is especially concerned with aspects like fault tolerance, to ensure that the outage of one part of its system cannot take everything down. To handle this, a large amount of work...",qwen2.5:latest,2025-11-02 23:40:40,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Tailored Service Template,Central Tools or Architecture Team Mandates,"#### Central Tools or Architecture Team Mandates
Background context: Netflix avoids central tools or architecture teams dictating how services should be implemented, as this can lead to a monolithic framework that stifles innovation.

:p What is the potential risk of having a central tools or architecture team mandate a specific framework?
??x
The primary risk is the creation of an overly complex and rigid framework that can stifle developer productivity and team morale. Developers may feel constrained by imposed practices, leading to resistance or poor adoption. Additionally, such mandates can result in developers spending more time adapting to frameworks rather than focusing on their core tasks.

Example scenario: A central tools team imposes a specific logging framework across all services without considering the unique needs of different teams.
```java
// Example of a mandated framework that could be overbearing
public class MandatoryLoggingFramework {
    public void log(String message) {
        // Enforced logging format and behavior
        System.out.println(""Mandatory: "" + message);
    }
}
```
x??",1125,"Netflix, for example, is especially concerned with aspects like fault tolerance, to ensure that the outage of one part of its system cannot take everything down. To handle this, a large amount of work...",qwen2.5:latest,2025-11-02 23:40:40,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Tailored Service Template,Service Template as an Internal Open Source Approach,"#### Service Template as an Internal Open Source Approach
Background context: Netflix encourages teams to take joint responsibility for updating a service template through an internal open-source approach.

:p How does Netflix encourage team involvement in maintaining the service template?
??x
Netflix promotes an internal open-source culture where multiple teams collaborate on maintaining and updating the service template. This collaborative process ensures that different perspectives are considered, reducing the risk of one team imposing their will on others. It fosters a sense of collective ownership and shared responsibility.

Example implementation: A GitHub repository dedicated to the service template with pull requests for updates.
```java
// Example structure for contribution guidelines in an internal open-source approach
public class ServiceTemplateContributor {
    public void contributeChanges() {
        // Process for contributing changes, such as code reviews and merging PRs
        System.out.println(""Contribution process initiated"");
    }
}
```
x??",1080,"Netflix, for example, is especially concerned with aspects like fault tolerance, to ensure that the outage of one part of its system cannot take everything down. To handle this, a large amount of work...",qwen2.5:latest,2025-11-02 23:40:40,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Tailored Service Template,Perils of Shared Code,"#### Perils of Shared Code
Background context: While sharing code can improve code reuse, it also introduces the risk of coupling between services.

:p What are some risks associated with shared code in a service template?
??x
Shared code increases the risk of introducing unintended dependencies and coupling between services. If one service relies on specific versions or configurations from another, changes to the shared code can impact multiple services, leading to cascading failures. This can make it difficult to maintain and evolve individual services independently.

Example scenario: A shared logging framework that is updated frequently but not all teams are aware of the changes.
```java
// Example of a potential coupling issue with shared code
public class SharedLoggingFramework {
    public static void configureLogger(String version) {
        // Configuring the logger based on version number, potentially causing issues if version changes unexpectedly
        System.out.println(""Configuring logger for version: "" + version);
    }
}
```
x??",1061,"Netflix, for example, is especially concerned with aspects like fault tolerance, to ensure that the outage of one part of its system cannot take everything down. To handle this, a large amount of work...",qwen2.5:latest,2025-11-02 23:40:40,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Tailored Service Template,Tailored Service Template vs. Mandated Framework,"#### Tailored Service Template vs. Mandated Framework
Background context: Netflix considers making a service template optional to balance between ease of use and flexibility.

:p Why might Netflix prefer to make the service template optional rather than mandating its use?
??x
Making the service template optional allows developers more autonomy in choosing how they implement their services, reducing resistance and fostering better adoption. This approach also enables teams to tailor solutions that best fit their specific needs without being constrained by a one-size-fits-all framework.

Example: A service template is available as an optional dependency but can be replaced or extended if necessary.
```java
// Example of an optional service template usage
public class OptionalServiceTemplate {
    private boolean useDefaultTemplate;

    public OptionalServiceTemplate(boolean useDefault) {
        this.useDefaultTemplate = useDefault;
    }

    public void initialize() {
        // Initialize the service using default template or custom implementation based on flag
        if (useDefaultTemplate) {
            System.out.println(""Using default template."");
        } else {
            System.out.println(""Customizing the template."");
        }
    }
}
```
x??

---",1281,"Netflix, for example, is especially concerned with aspects like fault tolerance, to ensure that the outage of one part of its system cannot take everything down. To handle this, a large amount of work...",qwen2.5:latest,2025-11-02 23:40:40,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Governance and Leading from the Center,Technical Debt,"#### Technical Debt
Technical debt refers to shortcuts taken during software development that provide short-term benefits but can have long-term costs. Just like financial debt, technical debt can accumulate and negatively impact future development efforts.

:p What is technical debt, and why is it important for developers to understand this concept?
??x
Technical debt occurs when a team opts for quick fixes or simplifications during software development that compromise the quality of the codebase. This can be due to urgent feature delivery requirements or changes in system vision not being fully reflected in the existing code. Understanding technical debt helps teams recognize the trade-offs and make informed decisions about when and how to pay down this debt.

```java
// Example of accruing technical debt by skipping unit tests
public void logMessage(String message) {
    System.out.println(message); // No unit test coverage here.
}
```
x??",956,"Technical Debt We are often put in situations where we cannot follow through to the letter on our technical vision. Often, we need to make a choice to cut a few corners to get some urgent features out...",qwen2.5:latest,2025-11-02 23:40:53,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Governance and Leading from the Center,Exception Handling,"#### Exception Handling
Exception handling is crucial for ensuring that systems can gracefully handle unexpected situations. This involves documenting exceptions and possibly revising principles or practices based on these documented issues.

:p How do you handle exceptions in software development, and why might it be necessary to change a principle based on exceptions?
??x
Handling exceptions involves logging them for future reference. If enough exceptions occur, they can indicate that the existing principles need to be revised. For example, if initially, a rule was ""always use MySQL for data storage,"" but there are compelling reasons to switch to Cassandra for scalable storage under certain conditions, this principle should evolve.

```java
try {
    // Code block where an exception might occur
} catch (Exception e) {
    System.err.println(""An error occurred: "" + e.getMessage());
    // Log the exception or record it in a log file
}
```
x??",957,"Technical Debt We are often put in situations where we cannot follow through to the letter on our technical vision. Often, we need to make a choice to cut a few corners to get some urgent features out...",qwen2.5:latest,2025-11-02 23:40:53,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Governance and Leading from the Center,Governance and Leading from the Center,"#### Governance and Leading from the Center
Governance ensures that technical decisions align with the overall vision of the organization. Architects play a crucial role in setting these guidelines, ensuring they are understandable to developers, and fostering an environment where teams can make informed decisions.

:p What is governance, and how does it apply to technical architecture?
??x
Governance is about ensuring alignment between the technical architecture and organizational objectives. It involves setting direction through prioritization and decision-making processes, as well as monitoring performance and compliance against these goals. Architects are responsible for establishing principles that guide development, ensuring they align with the organization's strategy while not causing undue stress on developers.

```java
public class GovernanceGroup {
    private List<Technologist> members;

    public void setGovernancePrinciples(List<String> principles) {
        this.members.forEach(member -> member.update(principles));
    }
}
```
x??",1061,"Technical Debt We are often put in situations where we cannot follow through to the letter on our technical vision. Often, we need to make a choice to cut a few corners to get some urgent features out...",qwen2.5:latest,2025-11-02 23:40:53,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Governance and Leading from the Center,Role of Architects in Technical Architecture,"#### Role of Architects in Technical Architecture
Architects are central to maintaining a technical vision and ensuring that all development aligns with this vision. They must balance the trade-offs between short-term gains and long-term sustainability.

:p What is expected from an architect in terms of responsibilities?
??x
An architect should establish guiding principles for development, ensure these principles match organizational strategy, avoid practices that cause developer dissatisfaction, stay updated on new technologies, make informed trade-offs, and communicate decisions effectively. They must also involve team members in the decision-making process to gain buy-in.

```java
public class Architect {
    private String technicalVision;
    private List<Principle> developmentPrinciples;

    public void updateTechnicalVision(String newVision) {
        this.technicalVision = newVision;
        // Notify all teams about the updated vision
    }
}
```
x??",974,"Technical Debt We are often put in situations where we cannot follow through to the letter on our technical vision. Often, we need to make a choice to cut a few corners to get some urgent features out...",qwen2.5:latest,2025-11-02 23:40:53,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Governance and Leading from the Center,Microservices and Team Autonomy,"#### Microservices and Team Autonomy
Microservices offer a way to enhance team autonomy, allowing each team to focus on specific parts of an application without unnecessary constraints.

:p How do microservices contribute to team autonomy?
??x
Microservices enable teams to independently develop, deploy, and scale services that fit their specific needs. This approach reduces the constraints imposed by monolithic architecture, giving developers more freedom to implement solutions tailored to their project requirements.

```java
public class Microservice {
    private String serviceName;
    private List<Developer> developers;

    public void addFeature(String feature) {
        // Developers work on adding a new feature independently
    }
}
```
x??",758,"Technical Debt We are often put in situations where we cannot follow through to the letter on our technical vision. Often, we need to make a choice to cut a few corners to get some urgent features out...",qwen2.5:latest,2025-11-02 23:40:53,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Governance and Leading from the Center,Architectural Decision-Making and Governance,"#### Architectural Decision-Making and Governance
Architects must involve the broader team in governance processes to ensure that decisions are well-informed and widely accepted. This involves creating a collaborative environment where all stakeholders have input.

:p How does an architect share responsibility for decision-making with the wider team?
??x
An architect should chair a governance group composed primarily of technologists from each delivery team. This group discusses and changes principles as needed, ensuring that decisions are informed by diverse perspectives. The goal is to create buy-in through collaboration rather than dictating terms unilaterally.

```java
public class GovernanceGroup {
    private Architect architect;
    private List<Technologist> members;

    public void makeDecision(String decision) {
        // Discuss and agree on the decision with all team leads
        this.architect.notifyTeams(decision);
    }
}
```
x??

---",966,"Technical Debt We are often put in situations where we cannot follow through to the letter on our technical vision. Often, we need to make a choice to cut a few corners to get some urgent features out...",qwen2.5:latest,2025-11-02 23:40:53,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Shared and Hidden Models,Vision,"#### Vision
Background context: The technical leader needs to ensure that there is a clear and communicated technical vision for the system, which will help meet the requirements of customers and the organization. This involves making technology decisions based on a broader perspective than just the technological aspects.

:p What is the primary responsibility of the evolutionary architect in terms of communication?
??x
The primary responsibility is to ensure that there is a clearly communicated technical vision for the system. This vision should help the system meet the requirements of customers and the organization.
x??",629,Building a Team Being the main point person responsible for the technical vision of your system and ensuring that you’re executing on this vision isn’t just about making technology decisions. It’s the...,qwen2.5:latest,2025-11-02 23:41:03,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Shared and Hidden Models,Empathy,"#### Empathy
Background context: Technical decisions made by leaders can have significant impacts on both customers and colleagues. Therefore, it's crucial for the leader to understand these impacts.

:p Why is empathy an essential aspect of a technical leader's role?
??x
Empathy is essential because technical decisions made by leaders can significantly impact their customers and colleagues. By understanding the consequences of these decisions, the leader can make more informed choices that consider the broader context.
x??",529,Building a Team Being the main point person responsible for the technical vision of your system and ensuring that you’re executing on this vision isn’t just about making technology decisions. It’s the...,qwen2.5:latest,2025-11-02 23:41:03,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Shared and Hidden Models,Collaboration,"#### Collaboration
Background context: A strong technical leader engages with peers and colleagues to define, refine, and execute the technical vision. This collaborative approach ensures that everyone is aligned and working towards a common goal.

:p How does collaboration benefit the implementation of a technical vision?
??x
Collaboration benefits the implementation of a technical vision by ensuring alignment among all stakeholders. It helps in defining, refining, and executing the vision effectively, which can lead to better outcomes and smoother project execution.
x??",578,Building a Team Being the main point person responsible for the technical vision of your system and ensuring that you’re executing on this vision isn’t just about making technology decisions. It’s the...,qwen2.5:latest,2025-11-02 23:41:03,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Shared and Hidden Models,Adaptability,"#### Adaptability
Background context: The technical vision should be flexible enough to change as requirements or customer needs evolve. This adaptability is crucial for long-term success.

:p Why is adaptability an important trait for an evolutionary architect?
??x
Adaptability is important because the technical vision must change in response to evolving customer or organizational requirements. Being adaptable allows the architect to respond to changes and maintain relevance.
x??",485,Building a Team Being the main point person responsible for the technical vision of your system and ensuring that you’re executing on this vision isn’t just about making technology decisions. It’s the...,qwen2.5:latest,2025-11-02 23:41:03,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Shared and Hidden Models,Autonomy,"#### Autonomy
Background context: Balancing standardization with enabling autonomy is key for effective team management. This balance helps teams take ownership of their work, leading to higher engagement and better outcomes.

:p What does balancing standardization and enabling autonomy mean in a technical leadership role?
??x
Balancing standardization and enabling autonomy means setting clear guidelines and standards while giving teams the flexibility and freedom to make decisions that best suit their specific needs. This approach helps teams take ownership of their work, leading to higher engagement and better outcomes.
x??",633,Building a Team Being the main point person responsible for the technical vision of your system and ensuring that you’re executing on this vision isn’t just about making technology decisions. It’s the...,qwen2.5:latest,2025-11-02 23:41:03,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Shared and Hidden Models,Governance,"#### Governance
Background context: Ensuring that the system being implemented aligns with the technical vision is crucial for long-term success. Governance involves making sure all aspects of the project are consistent with the overall strategy.

:p What does governance entail in the context of microservices?
??x
Governance entails ensuring that the implementation of the system fits within the broader technical vision. This includes setting policies, standards, and practices to ensure consistency and alignment across multiple autonomous services.
x??",557,Building a Team Being the main point person responsible for the technical vision of your system and ensuring that you’re executing on this vision isn’t just about making technology decisions. It’s the...,qwen2.5:latest,2025-11-02 23:41:03,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Shared and Hidden Models,Constant Balancing Act,"#### Constant Balancing Act
Background context: The role of an evolutionary architect involves a constant balancing act between various forces pushing in different directions. Experience helps in understanding where to push back or go with the flow.

:p Why is it important for an evolutionary architect to be adaptable?
??x
It is important for an evolutionary architect to be adaptable because they must balance various forces that are constantly changing. Adapting allows them to respond effectively to new requirements and challenges without becoming rigid in their thinking.
x??",582,Building a Team Being the main point person responsible for the technical vision of your system and ensuring that you’re executing on this vision isn’t just about making technology decisions. It’s the...,qwen2.5:latest,2025-11-02 23:41:03,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Shared and Hidden Models,Microservices Boundaries,"#### Microservices Boundaries
Background context: The next chapter will delve into finding the right boundaries for microservices, building on the awareness of the architect's role.

:p What is the focus of the next chapter?
??x
The focus of the next chapter is to explore how to define and set appropriate boundaries for microservices, leveraging the understanding gained about the architect's role.
x??

---",409,Building a Team Being the main point person responsible for the technical vision of your system and ensuring that you’re executing on this vision isn’t just about making technology decisions. It’s the...,qwen2.5:latest,2025-11-02 23:41:03,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Shared and Hidden Models,Loose Coupling,"#### Loose Coupling
Background context explaining loose coupling and its importance in microservices. The concept revolves around changes to one service not requiring a change to another, allowing for easier deployment of updates.

:p What is loose coupling?
??x
Loose coupling refers to the design principle where services are designed such that a change in one service does not require any changes in other services. This allows teams to independently deploy and update microservices without affecting others. In practice, this means minimizing dependencies between services so that they can evolve separately.

For example, imagine you have two services: `OrderService` and `PaymentService`. If `OrderService` needs a change and it requires updating `PaymentService`, then these services are tightly coupled. However, if `OrderService` can operate independently of changes in `PaymentService`, the coupling is loose.

Loose coupling helps avoid cascading effects when making changes, making the system more maintainable and scalable.
??x",1040,"You’re probably eager now to go and start making them, right? But where to start? In this chapter, we’ll look at how to think about the boundaries of your microservices that will hopefully maximize th...",qwen2.5:latest,2025-11-02 23:41:14,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Shared and Hidden Models,High Cohesion,"#### High Cohesion
Background context explaining high cohesion within microservices and its importance. High cohesion means that each service should have a single responsibility or a set of closely related responsibilities, which enhances modularity and reusability.

:p What is high cohesion?
??x
High cohesion is the principle that a service should focus on a single task or a set of closely related tasks, ensuring that all functionalities within the service are highly interrelated. This makes services more modular, easier to understand, and maintain.

For example, consider a `UserManagementService` versus a `UserService`. The former would handle user-related tasks such as registration, login, and profile updates, while the latter could be broken down further into sub-services like `Authentication`, `Profile`, and `Account`.

High cohesion ensures that each service does one thing well, making it easier to understand, test, and reuse.
??x",950,"You’re probably eager now to go and start making them, right? But where to start? In this chapter, we’ll look at how to think about the boundaries of your microservices that will hopefully maximize th...",qwen2.5:latest,2025-11-02 23:41:14,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Shared and Hidden Models,Microservices for MusicCorp,"#### Microservices for MusicCorp
Background context about MusicCorp's situation as a brick-and-mortar retailer now focusing on online sales. The company aims to leverage microservices for flexibility in operations.

:p What is the primary goal of MusicCorp in adopting microservices?
??x
The primary goal of MusicCorp in adopting microservices is to enable easier and faster deployment of changes, thereby improving the agility and responsiveness of their online retail platform. This aligns with their ambition to stay competitive despite being behind the curve on new technologies like streaming services.

By breaking down the monolithic architecture into smaller, independent microservices, MusicCorp can focus on delivering high-quality products while maintaining a robust and flexible system.
??x",802,"You’re probably eager now to go and start making them, right? But where to start? In this chapter, we’ll look at how to think about the boundaries of your microservices that will hopefully maximize th...",qwen2.5:latest,2025-11-02 23:41:14,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Shared and Hidden Models,Importance of Loose Coupling in Microservices,"#### Importance of Loose Coupling in Microservices
Background context about why loose coupling is crucial for microservices. Tight coupling can lead to cascading effects during updates, making the system harder to maintain.

:p Why is loose coupling important in microservices?
??x
Loose coupling is crucial in microservices because it enables independent deployment and scaling of services. When changes are made to one service, they should not necessitate changes in other services. This reduces the risk of downtime and maintenance complexity during updates.

For example, if `OrderService` needs an update that requires no change in `PaymentService`, then these services are loosely coupled. However, if updating `OrderService` forces a change in `PaymentService`, it results in tight coupling, leading to potential disruptions.

Loose coupling helps maintain the system's stability and flexibility, making it easier to manage different aspects of the business independently.
??x",983,"You’re probably eager now to go and start making them, right? But where to start? In this chapter, we’ll look at how to think about the boundaries of your microservices that will hopefully maximize th...",qwen2.5:latest,2025-11-02 23:41:14,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Shared and Hidden Models,Avoiding Tight Coupling,"#### Avoiding Tight Coupling
Background context about avoiding common mistakes that lead to tight coupling. Discuss integration styles that can cause such issues.

:p How can you avoid tight coupling in microservices?
??x
To avoid tight coupling in microservices, it's important to design services so that they interact minimally and only when necessary. Common practices include using well-defined APIs with clear boundaries, avoiding shared databases or other stateful dependencies between services, and minimizing the number of inter-service calls.

For example, if `OrderService` needs information from `PaymentService`, instead of sharing a database, use an API to request data. This ensures that changes in one service do not require updates in another.
??x",763,"You’re probably eager now to go and start making them, right? But where to start? In this chapter, we’ll look at how to think about the boundaries of your microservices that will hopefully maximize th...",qwen2.5:latest,2025-11-02 23:41:14,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Shared and Hidden Models,Service Design Example,"#### Service Design Example
Background context about designing services with loose coupling and high cohesion in mind.

:p How should you design a service for MusicCorp?
??x
When designing a service for MusicCorp, it's important to ensure the service has high cohesion by focusing on a single task or a set of related tasks. For example, a `ProductService` could handle all product-related operations like fetching, updating, and deleting products.

Additionally, the design should be loosely coupled to enable independent deployment and scaling. This can be achieved by minimizing direct dependencies between services, using APIs for inter-service communication, and avoiding shared state or mutable data between services.
??x",727,"You’re probably eager now to go and start making them, right? But where to start? In this chapter, we’ll look at how to think about the boundaries of your microservices that will hopefully maximize th...",qwen2.5:latest,2025-11-02 23:41:14,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Shared and Hidden Models,High Cohesion and Bounded Contexts,"#### High Cohesion and Bounded Contexts
Background context explaining the concept of high cohesion, which suggests that related behavior should be grouped together to simplify change management. This helps ensure that changes can be made more efficiently by modifying a single area rather than multiple scattered parts of the system.

This principle is crucial for managing complexity in large software systems, especially when dealing with complex domains like MusicCorp where different departments (e.g., finance and warehouse) operate independently but interact occasionally.
:p What is high cohesion, and why is it important in software design?
??x
High cohesion refers to the concept of grouping related behavior together within a single module or component. This approach facilitates easier maintenance and change management by ensuring that modifications can be made in one place rather than scattered across many parts of the system.

For instance, in MusicCorp's context, we might group all warehouse-related operations like inventory management and order processing into a single ""warehouse"" bounded context.
x??",1122,"High Cohesion We want related behavior to sit together, and unrelated behavior to sit elsewhere. Why? Well, if we want to change behavior, we want to be able to change it in one place, and release tha...",qwen2.5:latest,2025-11-02 23:41:25,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Shared and Hidden Models,Bounded Context,"#### Bounded Context
Background context introducing Eric Evans’s concept of bounded contexts from Domain-Driven Design (DDD). The idea is that a domain can be divided into multiple bounded contexts where each context has its own specific responsibilities and communicates with others through explicit interfaces.

These bounded contexts help in managing complexity by isolating different parts of the system while ensuring necessary interactions.
:p What does ""bounded context"" mean according to Domain-Driven Design?
??x
A bounded context is a specific responsibility enforced by explicit boundaries. It's a way of organizing complex domains into smaller, more manageable pieces that can be developed and maintained independently.

Each bounded context has its own models and rules (domain logic) which are only shared with other contexts through well-defined interfaces.
x??",876,"High Cohesion We want related behavior to sit together, and unrelated behavior to sit elsewhere. Why? Well, if we want to change behavior, we want to be able to change it in one place, and release tha...",qwen2.5:latest,2025-11-02 23:41:25,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Shared and Hidden Models,Explicit Interfaces Between Bounded Contexts,"#### Explicit Interfaces Between Bounded Contexts
Background context explaining how bounded contexts communicate with each other using explicit interfaces. These interfaces define the models and data that can be shared between different contexts while ensuring loose coupling.

This approach helps in managing dependencies between components, making it easier to change one part of a system without affecting others.
:p How do bounded contexts communicate with each other?
??x
Bounded contexts communicate with each other through explicit interfaces. These interfaces define the models and data that can be shared externally. For example, if the finance department needs inventory reports from the warehouse context, it would request this information using pre-defined models.

Here is a simplified example:
```java
public interface WarehouseService {
    InventoryReport getInventoryReport();
}
```
The finance department uses `WarehouseService` to request an inventory report.
x??",982,"High Cohesion We want related behavior to sit together, and unrelated behavior to sit elsewhere. Why? Well, if we want to change behavior, we want to be able to change it in one place, and release tha...",qwen2.5:latest,2025-11-02 23:41:25,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Shared and Hidden Models,The Analogy of Cells and Membranes,"#### The Analogy of Cells and Membranes
Background context using the analogy of cells from biology, where cell membranes define what can enter and leave. This is compared to bounded contexts in software design, which have explicit interfaces that control how information and functionality are shared.

This helps visualize how different parts of a domain operate independently while still interacting through well-defined boundaries.
:p How does the analogy of cells help explain bounded contexts?
??x
The analogy of cells helps explain bounded contexts by comparing them to cell membranes. Just as cell membranes define what is inside and outside the cell, making sure that only certain substances can pass in or out, bounded contexts also have explicit interfaces defining which models and data are shared with other contexts.

For example, a warehouse context might share its inventory levels but not specific forklift truck details with the finance context.
x??",965,"High Cohesion We want related behavior to sit together, and unrelated behavior to sit elsewhere. Why? Well, if we want to change behavior, we want to be able to change it in one place, and release tha...",qwen2.5:latest,2025-11-02 23:41:25,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Shared and Hidden Models,Separation of Shared Models,"#### Separation of Shared Models
Background context explaining how each bounded context has both shared and hidden models. Shared models are those that need to be communicated outside the context, while hidden models contain internal details that should remain private.

This separation helps in managing complexity by ensuring that internal implementation details do not leak into other contexts.
:p What are shared and hidden models in the context of bounded contexts?
??x
Shared models are the parts of a bounded context's domain model that need to be communicated with other contexts. These might include reports, invoices, or general data structures.

Hidden models contain details that are specific to the internal operations of the context and should not be exposed outside. For example, in the warehouse context, forklift truck maintenance schedules would likely remain hidden.
x??

---",894,"High Cohesion We want related behavior to sit together, and unrelated behavior to sit elsewhere. Why? Well, if we want to change behavior, we want to be able to change it in one place, and release tha...",qwen2.5:latest,2025-11-02 23:41:25,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Shared and Hidden Models,Shared Models Between Contexts,"#### Shared Models Between Contexts
Background context explaining the concept. The finance department and warehouse have different needs, but they share some information to maintain accurate records. For example, stock levels are critical for both departments.

The shared model includes concepts like stock items, which are vital for valuation purposes by the finance department.
:p What is a key reason why the finance department and warehouse need to share models?
??x
Both departments require up-to-date stock levels to ensure accurate financial records and efficient operations. The finance team needs this information to maintain the company's accounts correctly, while the warehouse uses it for inventory management.
x??",727,"Now the finance department doesn’t need to know about the detailed inner workings of the warehouse. It does need to know some things, though — for example it needs to know about stock levels to keep t...",qwen2.5:latest,2025-11-02 23:41:34,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Shared and Hidden Models,Internal vs External Representations,"#### Internal vs External Representations
Background context explaining the concept. The internal representation of a stock item in the warehouse may include details not necessary or relevant for external sharing with other departments.

For example, internally, the warehouse might track where each stock item should be stored, but this information is not shared externally.
:p What does the internal-only representation of a stock item typically contain?
??x
The internal-only representation includes detailed tracking such as storage location, which helps in managing inventory within the warehouse. This information is necessary for internal processes and decision-making but may not be useful or appropriate to share with other departments like finance.
x??",762,"Now the finance department doesn’t need to know about the detailed inner workings of the warehouse. It does need to know some things, though — for example it needs to know about stock levels to keep t...",qwen2.5:latest,2025-11-02 23:41:34,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Shared and Hidden Models,Context-Specific Models,"#### Context-Specific Models
Background context explaining the concept. A single model (e.g., ""return"") can have different meanings depending on the context in which it is used.

For instance, a return could mean something completely different for a customer versus a warehouse, indicating different processes and tasks involved.
:p How does the meaning of a ""return"" differ between customer and warehouse contexts?
??x
In the customer context, a return involves printing a shipping label, dispatching a package, and awaiting a refund. In contrast, in the warehouse context, it might involve receiving a returned package, restocking items, and possibly generating a restock request.

```java
// Example of a simplified return process within the warehouse context
public class Warehouse {
    public void handleReturn(Package returnPackage) {
        // Logic to receive and inspect the package
        if (isValidReturn(returnPackage)) {
            generateRestockRequest(returnPackage.getItem());
        }
    }

    private boolean isValidReturn(Package package) {
        // Validation logic
        return true;  // Simplified for example
    }

    private void generateRestockRequest(Item item) {
        // Logic to create a restock request
        System.out.println(""Generating restock request for: "" + item.getName());
    }
}
```
x??",1346,"Now the finance department doesn’t need to know about the detailed inner workings of the warehouse. It does need to know some things, though — for example it needs to know about stock levels to keep t...",qwen2.5:latest,2025-11-02 23:41:34,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Shared and Hidden Models,Bounded Contexts and Shared Models,"#### Bounded Contexts and Shared Models
Background context explaining the concept. Different contexts within an organization (like finance and warehouse) have distinct responsibilities but may need to share models.

For example, while stock items are shared between these two departments, not all internal details need to be exposed.
:p How do bounded contexts like finance and warehouse interact in terms of shared models?
??x
Bounded contexts ensure that each department has a clear understanding of its specific tasks. Finance needs basic stock levels for accounting purposes but does not require detailed warehouse management information. These shared models help maintain consistency between departments without exposing unnecessary internal details.

For instance, while the finance context might need to know about stock levels and total value, it doesn't need to know where each item is stored in the warehouse.
x??

---",928,"Now the finance department doesn’t need to know about the detailed inner workings of the warehouse. It does need to know some things, though — for example it needs to know about stock levels to keep t...",qwen2.5:latest,2025-11-02 23:41:34,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Turtles All the Way Down,Bounded Contexts and Microservices Alignment,"#### Bounded Contexts and Microservices Alignment
Background context explaining that bounded contexts are crucial for creating well-defined, self-contained microservices. Properly aligning microservices with these bounded contexts ensures loose coupling and high cohesion within the system.

:p What is a bounded context, and why is it important in microservices architecture?
??x
A bounded context is a region of the domain where all like-minded business capabilities live together, providing clear boundaries for the scope of responsibilities. It helps in avoiding tight coupling by clearly defining what models should be shared and not shared between services.

This concept ensures that each service focuses on specific functionalities, making it easier to manage and scale individual components independently. Bounded contexts are essential because they allow teams to work on different parts of a system without interfering with others.
x??",946,"Modules and Services By thinking clearly about what models should be shared, and not sharing our internal representations, we avoid one of the potential pitfalls that can result in tight coupling (the...",qwen2.5:latest,2025-11-02 23:41:47,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Turtles All the Way Down,Module and Service Boundaries,"#### Module and Service Boundaries
Background context explaining how modules can be used within a process boundary to keep related code together while reducing coupling between services.

:p How do you start organizing your new codebase into modules and services?
??x
Start by identifying the bounded contexts in your domain. These contexts should be modeled as modules, where shared models are exposed and hidden models remain private. This organization helps in creating a structure that aligns well with microservices.

When starting out, it's advisable to keep the system more monolithic; this allows you to get familiar with the domain before deciding on service boundaries. Overly aggressive decomposition can lead to high costs due to changing requirements or misaligned services.
x??",791,"Modules and Services By thinking clearly about what models should be shared, and not sharing our internal representations, we avoid one of the potential pitfalls that can result in tight coupling (the...",qwen2.5:latest,2025-11-02 23:41:47,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Turtles All the Way Down,Microservices and Bounded Contexts,"#### Microservices and Bounded Contexts
Background context explaining that microservices should cleanly align with bounded contexts, ensuring loose coupling and strong cohesion.

:p Why should microservices be aligned with bounded contexts?
??x
Microservices should be aligned with bounded contexts because it ensures that each service focuses on a specific set of functionalities. This alignment helps in achieving high cohesion within the services and loose coupling between them, making the system more maintainable and scalable.

Aligning microservices with bounded contexts also simplifies testing and deployment processes since each context can operate independently.
x??",677,"Modules and Services By thinking clearly about what models should be shared, and not sharing our internal representations, we avoid one of the potential pitfalls that can result in tight coupling (the...",qwen2.5:latest,2025-11-02 23:41:47,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Turtles All the Way Down,Premature Decomposition,"#### Premature Decomposition
Background context explaining how ThoughtWorks experienced challenges when decomposing a system into microservices too quickly. It highlights that premature decomposition can lead to high costs due to changing requirements or misaligned services.

:p What is the risk of prematurely splitting a system into microservices?
??x
Prematurely decomposing a system into microservices can be costly, especially if you are new to the domain. When initial boundaries are not well thought out, they may need frequent changes leading to high costs and complex maintenance. It's often better to have an existing monolithic codebase that can be gradually refactored into microservices once the domain is more familiar.
x??",738,"Modules and Services By thinking clearly about what models should be shared, and not sharing our internal representations, we avoid one of the potential pitfalls that can result in tight coupling (the...",qwen2.5:latest,2025-11-02 23:41:47,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Turtles All the Way Down,Business Capabilities,"#### Business Capabilities
Background context explaining the focus should be on business capabilities rather than shared data when identifying bounded contexts.

:p What should you consider first when defining a bounded context?
??x
When defining a bounded context, start by considering what business capabilities that context provides to other parts of the domain. For example, the warehouse may provide the capability to get a current stock list or handle order fulfillment. This approach ensures that services are designed around their primary functions rather than shared data.

Focusing on business capabilities helps in creating more meaningful and cohesive microservices.
x??",682,"Modules and Services By thinking clearly about what models should be shared, and not sharing our internal representations, we avoid one of the potential pitfalls that can result in tight coupling (the...",qwen2.5:latest,2025-11-02 23:41:47,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Turtles All the Way Down,Nested Bounded Contexts,"#### Nested Bounded Contexts
Background context explaining that bounded contexts can contain further nested contexts, which may be better represented as a single service or split into separate services based on organizational structure.

:p How do you handle nested bounded contexts when designing microservices?
??x
Nested bounded contexts can exist within larger, coarser-grained contexts. It's important to first consider the larger context and then subdivide along these nested contexts for potential benefits. Whether to model the higher-level bounded context as a top-level service or split it into separate services depends on the organizational structure.

For example, you could decompose the warehouse into order fulfillment, inventory management, or goods receiving capabilities. If different teams manage these functionalities, they should be modeled as separate microservices. Otherwise, the nested approach might make sense for simplicity and easier testing.
x??",976,"Modules and Services By thinking clearly about what models should be shared, and not sharing our internal representations, we avoid one of the potential pitfalls that can result in tight coupling (the...",qwen2.5:latest,2025-11-02 23:41:47,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Turtles All the Way Down,Turtles All the Way Down,"#### Turtles All the Way Down
Background context explaining that bounded contexts can have sub-contexts which may remain hidden to collaborating services.

:p How do you design microservices when there are nested bounded contexts?
??x
When designing microservices with nested bounded contexts, start by considering the larger coarser-grained contexts. You can either model these as top-level boundaries or split them into separate services based on organizational structure and testing needs. For example, a warehouse context might have sub-contexts like inventory management, order fulfillment, and goods receiving.

These nested contexts can be hidden from collaborating microservices, making the overall system appear to operate using business capabilities without revealing the underlying service architecture.
x??",818,"Modules and Services By thinking clearly about what models should be shared, and not sharing our internal representations, we avoid one of the potential pitfalls that can result in tight coupling (the...",qwen2.5:latest,2025-11-02 23:41:47,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Turtles All the Way Down,Testing and Isolation,"#### Testing and Isolation
Background context explaining how testing can benefit from understanding the boundaries between services, especially when dealing with complex architectures like microservices.

:p How does understanding service boundaries help in testing?
??x
Understanding service boundaries helps in designing effective tests. You might decide to have end-to-end tests where all services within a bounded context are launched together, but for other collaborators, you can stub out these internal services. This approach provides a unit of isolation when considering larger-scoped tests.

For example, if testing a service that consumes the warehouse, you don't need to stub each service inside the warehouse context; instead, you might just stub the more coarse-grained API.
x??

---",797,"Modules and Services By thinking clearly about what models should be shared, and not sharing our internal representations, we avoid one of the potential pitfalls that can result in tight coupling (the...",qwen2.5:latest,2025-11-02 23:41:47,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Technical Boundary,Changes to System Functionality,"#### Changes to System Functionality
Background context: The changes made to a system are often driven by business needs and involve modifying the functionality or capabilities exposed to customers. When the system is decomposed into bounded contexts, these changes can be isolated more effectively within one microservice boundary.

:p What does changing system functionality typically involve?
??x
Changing system functionality involves altering the behavior of the software in a way that better aligns with business requirements. This might include adding new features, modifying existing ones, or removing outdated functionalities to enhance user experience and meet evolving business goals.
x??",699,Communication in Terms of Business Concepts The changes we implement to our system are often about changes the business wants to make to how the system behaves. We are changing functionality — capabil...,qwen2.5:latest,2025-11-02 23:41:57,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Technical Boundary,Bounded Contexts and Microservices,"#### Bounded Contexts and Microservices
Background context: Bounded contexts represent different parts of an organization's domain and are used in Domain-Driven Design (DDD) to ensure that terms have a clear meaning within specific areas. When microservices align with bounded contexts, it helps maintain consistency in business terminology across the system.

:p How do bounded contexts affect communication between microservices?
??x
Bounded contexts ensure that each service uses consistent and well-defined terms and concepts from their respective domain areas. This alignment is crucial for clear and effective communication between microservices because it reduces ambiguity and ensures that services understand and interpret requests correctly based on shared business language.

For example, if a bounded context in the ""Order Management"" area defines a ""Product"", another service within the same context should use this definition without confusion.
x??",962,Communication in Terms of Business Concepts The changes we implement to our system are often about changes the business wants to make to how the system behaves. We are changing functionality — capabil...,qwen2.5:latest,2025-11-02 23:41:57,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Technical Boundary,Technical Boundary Example,"#### Technical Boundary Example
Background context: The example provided discusses issues arising from incorrectly modeling services. Initially, a monolithic system was split into two parts based on geographical and organizational lines rather than domain boundaries.

:p What were the main issues with the service decomposition in this case?
??x
The main issues included:
1. Frequent changes required in both front-end and back-end services.
2. Excessive use of RPC-style method calls, leading to brittleness.
3. Chatty interfaces resulting in performance issues necessitating elaborate batching mechanisms.

This decomposition did not align with the business domain but instead followed a technical seam (splitting repository layers).
x??",740,Communication in Terms of Business Concepts The changes we implement to our system are often about changes the business wants to make to how the system behaves. We are changing functionality — capabil...,qwen2.5:latest,2025-11-02 23:41:57,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Technical Boundary,Onion Architecture,"#### Onion Architecture
Background context: The term ""onion architecture"" refers to systems with many layers, which can make maintenance and changes difficult. The given example describes such an architecture that had issues due to its complexity.

:p What is the issue described as ""onion architecture"" in this text?
??x
The ""onion architecture"" described here refers to a system with multiple layers (like an onion), making it challenging to navigate and modify. In the case provided, the service was split into two parts, each with its own interfaces and logic, leading to high complexity and difficulty in managing changes.

Code example:
```java
public class OrderService {
    private final ProductRepository productRepo;
    
    public OrderService(ProductRepository productRepo) {
        this.productRepo = productRepo;
    }
    
    public void placeOrder(Order order) {
        Product product = productRepo.findById(order.getProductId());
        // ... more complex logic ...
    }
}
```

Explanation: This example shows a simple service that depends on another repository, which can add layers of complexity in larger systems.
x??",1146,Communication in Terms of Business Concepts The changes we implement to our system are often about changes the business wants to make to how the system behaves. We are changing functionality — capabil...,qwen2.5:latest,2025-11-02 23:41:57,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Technical Boundary,Vertical vs. Horizontal Service Boundaries,"#### Vertical vs. Horizontal Service Boundaries
Background context: The text contrasts vertical decomposition (aligned with the business domain) against horizontal decomposition (splitting along technical seams).

:p What is the difference between vertical and horizontal service boundaries?
??x
Vertical service boundaries align services with specific parts of the business domain, ensuring that each microservice encapsulates a cohesive set of functionalities. Horizontal boundaries split services based on operational concerns like data storage or geographical distribution.

For example:
- Vertical: An ""Order Management"" service handles all aspects related to orders.
- Horizontal: A database service might be horizontally scaled across multiple machines but still falls under the same domain (e.g., order management).

Horizontal boundaries can be useful for performance optimizations, while vertical boundaries support clearer business logic and easier maintenance.
x??

---",981,Communication in Terms of Business Concepts The changes we implement to our system are often about changes the business wants to make to how the system behaves. We are changing functionality — capabil...,qwen2.5:latest,2025-11-02 23:41:57,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Shared Database,Bounded Contexts and Microservices,"---
#### Bounded Contexts and Microservices
Bounded contexts are a fundamental concept in Domain-Driven Design (DDD) that help identify domains within a problem space. They allow for clear boundaries between different parts of a system, ensuring loose coupling and high cohesion among microservices.

Background context:
In DDD, the term ""bounded context"" refers to the area where a specific domain model is valid and applicable. Each bounded context has its own vocabulary, rules, and constraints that must be respected by all parties within it.

:p What are bounded contexts in Domain-Driven Design?
??x
Bounded contexts are areas of focus within a larger problem space where a particular domain model is valid and follows specific rules and constraints. They help in identifying independent domains with distinct boundaries, ensuring microservices maintain their autonomy.
x??",879,"Summary In this chapter, you’ve learned a bit about what makes a good service, and how to find seams in our problem space that give us the dual benefits of both loose coupling and high cohesion. Bound...",qwen2.5:latest,2025-11-02 23:42:07,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Shared Database,Microservice Subdivision,"#### Microservice Subdivision
The chapter hints at the potential need to further subdivide microservices into smaller components.

Background context:
Even after defining bounded contexts, there might be complex interactions within those contexts that require even finer-grained decomposition. This can help in reducing complexity and improving manageability of individual services.

:p Why is it important to consider subdividing microservices?
??x
Subdividing microservices can help address complex internal logic by breaking down the system into more manageable parts, enhancing modularity, and making it easier to scale and maintain.
x??",641,"Summary In this chapter, you’ve learned a bit about what makes a good service, and how to find seams in our problem space that give us the dual benefits of both loose coupling and high cohesion. Bound...",qwen2.5:latest,2025-11-02 23:42:07,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Shared Database,MusicCorp Example Domain,"#### MusicCorp Example Domain
MusicCorp serves as an example domain throughout the book.

Background context:
Using a specific domain like MusicCorp helps illustrate various concepts related to microservices and DDD. It provides a concrete context for discussing principles such as bounded contexts and service boundaries.

:p What is the role of MusicCorp in the book?
??x
MusicCorp acts as an example domain used throughout the book to demonstrate key concepts related to microservices, bounded contexts, and Domain-Driven Design.
x??",536,"Summary In this chapter, you’ve learned a bit about what makes a good service, and how to find seams in our problem space that give us the dual benefits of both loose coupling and high cohesion. Bound...",qwen2.5:latest,2025-11-02 23:42:07,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Shared Database,Interface Technology for Microservices,"#### Interface Technology for Microservices
Choosing the right technology for integrating microservices is crucial.

Background context:
There are several technologies available for microservice integration, such as SOAP, XML-RPC, REST, Protocol Buffers, etc. Each has its strengths and weaknesses, but some considerations include avoiding breaking changes and ensuring API agnosticism to accommodate future changes in technology stacks.

:p What factors should be considered when choosing an interface technology for microservices?
??x
When selecting an interface technology for microservices, consider factors such as minimizing breaking changes, ensuring backward compatibility, and keeping APIs technology-agnostic to support potential shifts in the tech stack.
x??",769,"Summary In this chapter, you’ve learned a bit about what makes a good service, and how to find seams in our problem space that give us the dual benefits of both loose coupling and high cohesion. Bound...",qwen2.5:latest,2025-11-02 23:42:07,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Shared Database,Avoiding Breaking Changes,"#### Avoiding Breaking Changes
Handling breaking changes is important to maintain service autonomy.

Background context:
Even small changes can impact consumers of a microservice if not managed correctly. Techniques like semantic versioning can help manage such changes without causing disruptions.

:p How can we handle breaking changes when making updates to a microservice?
??x
To handle breaking changes, use techniques like semantic versioning and ensure that any new features or changes are backward-compatible unless absolutely necessary. This helps maintain the autonomy of services and their consumers.
x??",615,"Summary In this chapter, you’ve learned a bit about what makes a good service, and how to find seams in our problem space that give us the dual benefits of both loose coupling and high cohesion. Bound...",qwen2.5:latest,2025-11-02 23:42:07,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Shared Database,Technology-Agnostic APIs,"#### Technology-Agnostic APIs
Ensuring API agnosticism is crucial for future-proofing.

Background context:
As technology evolves rapidly, APIs should be designed in a way that minimizes dependencies on specific technologies or frameworks. This allows services to adapt more easily when adopting new tools or changing stacks.

:p Why is it important to keep APIs technology-agnostic?
??x
Keeping APIs technology-agnostic is essential for future-proofing systems and allowing them to adapt to changes in the tech stack without significant disruptions. It promotes flexibility and maintainability.
x??

---",604,"Summary In this chapter, you’ve learned a bit about what makes a good service, and how to find seams in our problem space that give us the dual benefits of both loose coupling and high cohesion. Bound...",qwen2.5:latest,2025-11-02 23:42:07,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Shared Database,Microservices and API Technology-Agnosticism,"---
#### Microservices and API Technology-Agnosticism
In this context, microservices are favored for their flexibility. One of the core principles is to ensure that communication between services (microservices) uses APIs that do not dictate specific technology stacks, thus maintaining technological freedom.
:p Why is it important to keep APIs used in microservices technology-agnostic?
??x
It's essential because using a technology-specific API could limit our choice of technologies for implementing microservices. This means we should avoid integration methods or tools that force us into a particular tech stack. For instance, choosing an HTTP-based API is more flexible than one tied to a specific framework or library.
```java
// Example of a simple REST API method in Java
public class CustomerService {
    @GetMapping(""/customers/{id}"")
    public ResponseEntity<Customer> getCustomer(@PathVariable Long id) {
        // Logic to fetch customer from database
        return new ResponseEntity<>(customer, HttpStatus.OK);
    }
}
```
x??",1047,"I am a big fan of keeping my options open, which is why I am such a fan of microservices. It is also why I think it is very important to ensure that you keep the APIs used for communication between mi...",qwen2.5:latest,2025-11-02 23:42:16,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Shared Database,Simplifying Service Consumption for Consumers,"#### Simplifying Service Consumption for Consumers
The objective here is to make services easy to use by consumers. This involves considering the cost of using a service and providing tools that can ease adoption without tying consumers too closely to our internal implementation.
:p How can we ensure that our microservices are simple for their consumers to use?
??x
We should aim to give consumers full freedom in their technology choices while making it easy for them. One approach is to provide client libraries, but this must be balanced against increased coupling between the service and its consumers. We want to avoid exposing internal implementation details that could cause disruptions if changes are made.
```java
// Example of a simple client library method in Java
public class CustomerClient {
    public void registerCustomer(Customer customer) {
        // Logic to send request to CustomerService
    }
}
```
x??",929,"I am a big fan of keeping my options open, which is why I am such a fan of microservices. It is also why I think it is very important to ensure that you keep the APIs used for communication between mi...",qwen2.5:latest,2025-11-02 23:42:16,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Shared Database,Hiding Internal Implementation Details,"#### Hiding Internal Implementation Details
To prevent consumers from being bound to our internal implementation, we should avoid exposing details that could cause issues if changes are made. This helps reduce coupling and the cost of change.
:p Why is it important to hide internal implementation details in microservices?
??x
Hiding internal implementation details ensures that when we need to make changes inside a microservice, those changes do not impact consumers of the service. By doing so, we reduce technical debt within the service and maintain flexibility in our development processes.
```java
// Example of abstracting database access in Java
public class CustomerRepository {
    public void save(Customer customer) {
        // Abstracted logic to save customer without revealing internal details
    }
}
```
x??",827,"I am a big fan of keeping my options open, which is why I am such a fan of microservices. It is also why I think it is very important to ensure that you keep the APIs used for communication between mi...",qwen2.5:latest,2025-11-02 23:42:16,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Shared Database,Common Integration Options: The Shared Database Approach,"#### Common Integration Options: The Shared Database Approach
The shared database is a common integration approach where services read and write data directly from the same database. While simple, it can lead to tight coupling between services.
:p How does the shared database method work in microservices architecture?
??x
In the shared database method, multiple services interact with the database to retrieve or modify data. This approach is straightforward but can result in high coupling because changes made to the schema or data access patterns by one service may affect others.
```java
// Example of a simple SQL operation in Java using JPA
public class CustomerService {
    @PersistenceContext
    private EntityManager entityManager;

    public void createCustomer(Customer customer) {
        entityManager.persist(customer);
    }
}
```
x??
---",858,"I am a big fan of keeping my options open, which is why I am such a fan of microservices. It is also why I think it is very important to ensure that you keep the APIs used for communication between mi...",qwen2.5:latest,2025-11-02 23:42:16,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Shared Database,Exposing Internal Implementation Details Through DB Integration,"#### Exposing Internal Implementation Details Through DB Integration
Background context explaining the concept. When services use direct database integration to access and modify data, they are essentially allowing external parties to view and bind to internal implementation details. This can lead to breaking changes if the schema or logic is altered. The data stored in the database becomes a large shared API that is brittle and prone to failures during updates.

If a change is required in how helpdesk manages customers, it must be carefully managed to avoid breaking other services tied to the same database structure.
:p What are the risks of allowing external parties to view and bind to internal implementation details through DB integration?
??x
The main risks include:
1. Breaking changes: Any schema modifications or logic updates can disrupt consumers that rely on the current structure.
2. Brittle API: The database becomes a large, shared API that is sensitive to any changes.
3. Regression testing: Significant effort is required to ensure no part of the schema breaks during updates.

For example, if you decide to add a new field or change data types in a customer table used by multiple services:
```java
// Old schema
public class Customer {
    private String name;
    private int age;
}

// New schema with additional fields
public class Customer {
    private String name;
    private int age;
    private boolean isActive;
}
```
x??",1458,"This is a common enough pattern, but it’s one fraught with difficulties. Figure 4-1. Using DB integration to access and change customer information First, we are allowing external parties to view and ...",qwen2.5:latest,2025-11-02 23:42:27,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Shared Database,Tying Consumers to Specific Technology Choices,"#### Tying Consumers to Specific Technology Choices
Background context explaining the concept. When services directly interact with a database, they are tied to specific technology choices such as relational databases or nonrelational ones. This tight coupling can cause problems if there is a need for a different storage solution in the future.

If your system starts using a relational database but later requires a NoSQL database due to scalability issues, consumers will be tightly coupled to their current implementation.
:p How does direct DB integration tie consumers to specific technology choices?
??x
Direct DB integration ties consumers to specific technology choices because:
1. Consumers are dependent on the database schema and structure.
2. Changing the storage solution (e.g., from a relational to a NoSQL database) can require significant changes in consumer code.

For example, if you initially use JPA for a relational database but later need to switch to MongoDB:
```java
// JPA Entity
@Entity
public class Customer {
    @Id
    private Long id;
    private String name;
}

// Potential change when moving to MongoDB (example)
MongoCollection<Document> customerCollection = db.getCollection(""customers"");
Document document = new Document(""name"", ""John Doe"").append(""_id"", 1L);
customerCollection.insertOne(document);
```
x??",1346,"This is a common enough pattern, but it’s one fraught with difficulties. Figure 4-1. Using DB integration to access and change customer information First, we are allowing external parties to view and ...",qwen2.5:latest,2025-11-02 23:42:27,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Shared Database,Impact on Logic and Cohesion,"#### Impact on Logic and Cohesion
Background context explaining the concept. Direct manipulation of database records by consumers leads to scattered logic, making it difficult to maintain a cohesive service.

If multiple services (e.g., warehouse, registration UI, call center) all need to edit customer information, changes must be propagated across these different points.
:p How does direct DB integration affect the behavior and cohesion of services?
??x
Direct DB integration affects behavior and cohesion because:
1. Logic for manipulating data is spread across multiple consumers (services).
2. Any bug or change in logic needs to be fixed in multiple places, leading to poor code organization.

For example, if you need to update a customer's address through different entry points:
```java
// Warehouse service
public void updateCustomerAddress(Long id, String newAddress) {
    // Database call to update address
}

// Registration UI service
public void updateCustomerAddress(Long id, String newAddress) {
    // Database call to update address
}
```
x??",1065,"This is a common enough pattern, but it’s one fraught with difficulties. Figure 4-1. Using DB integration to access and change customer information First, we are allowing external parties to view and ...",qwen2.5:latest,2025-11-02 23:42:27,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Shared Database,Breaking Strong Cohesion and Loose Coupling,"#### Breaking Strong Cohesion and Loose Coupling
Background context explaining the concept. The core principles of microservices include strong cohesion (where a service has a single responsibility) and loose coupling (services interact minimally). Direct database integration undermines these principles by exposing internal representations.

Consumers are directly manipulating DB records, leading to tight coupling and poor cohesion.
:p How does direct DB integration break the core principles of good microservices?
??x
Direct DB integration breaks the core principles of good microservices by:
1. Breaking strong cohesion: Services have multiple responsibilities (data manipulation and business logic).
2. Reducing loose coupling: Consumers are tightly coupled to specific implementation details, making changes difficult.

For example, if a customer service needs to add new functionality but is tightly coupled to the database:
```java
// Original Customer Service
public void updateCustomer(Customer customer) {
    // Logic for updating customer in DB
}

// New feature requiring changes
public void changeCustomerStatus(Long id, boolean status) {
    // This requires changing multiple places where customer information is updated
}
```
x??

---",1255,"This is a common enough pattern, but it’s one fraught with difficulties. Figure 4-1. Using DB integration to access and change customer information First, we are allowing external parties to view and ...",qwen2.5:latest,2025-11-02 23:42:27,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Orchestration Versus Choreography,Synchronous Communication,"#### Synchronous Communication
Synchronous communication involves making a call to a remote server which blocks until the operation completes. This means that the caller waits for the response before proceeding further.
:p What is synchronous communication?
??x
In synchronous communication, a client sends a request and waits for a response before moving on. The process is blocking, meaning the client cannot do anything else while waiting for the result. 
```java
public String makeSynchronousCall() {
    // Simulate making a call to a remote server
    Thread.sleep(1000);  // Blocking operation
    return ""Response from Server"";
}
```
x??",645,"Synchronous Versus Asynchronous Before we start diving into the specifics of different technology choices, we should discuss one of the most important decisions we can make in terms of how services co...",qwen2.5:latest,2025-11-02 23:42:38,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Orchestration Versus Choreography,Asynchronous Communication,"#### Asynchronous Communication
Asynchronous communication involves calling a service where the caller doesn’t wait for the operation to complete before returning, and may not even care whether or not the operation completes at all. It’s useful in scenarios where keeping a connection open is impractical due to network latency.
:p What is asynchronous communication?
??x
In asynchronous communication, the client sends a request without waiting for a response. The process is non-blocking; the client continues execution and may be notified later via a callback or event handler if necessary. 
```java
public void makeAsynchronousCall() {
    // Simulate making an asynchronous call to a remote server
    new Thread(() -> {
        try {
            Thread.sleep(1000);  // Simulating network delay
        } catch (InterruptedException e) {
            e.printStackTrace();
        }
        System.out.println(""Operation completed"");
    }).start();  // Non-blocking operation
}
```
x??",990,"Synchronous Versus Asynchronous Before we start diving into the specifics of different technology choices, we should discuss one of the most important decisions we can make in terms of how services co...",qwen2.5:latest,2025-11-02 23:42:38,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Orchestration Versus Choreography,Request/Response Model,"#### Request/Response Model
The request/response model involves a client initiating a request and waiting for the response. It can work with both synchronous and asynchronous communication.
:p What is the request/response model?
??x
In the request/response model, a client sends a request to a service and waits for a response before proceeding. This model aligns well with synchronous communication but also supports asynchronous calls where clients register callbacks or event listeners. 
```java
public String processRequest(String input) {
    // Simulate processing and returning a response
    Thread.sleep(1000);  // Blocking operation
    return ""Processed: "" + input;
}
```
x??",686,"Synchronous Versus Asynchronous Before we start diving into the specifics of different technology choices, we should discuss one of the most important decisions we can make in terms of how services co...",qwen2.5:latest,2025-11-02 23:42:38,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Orchestration Versus Choreography,Event-Based Collaboration,"#### Event-Based Collaboration
Event-based collaboration involves the client emitting events, and other parties react to these events. The model is inherently asynchronous.
:p What is event-based collaboration?
??x
In event-based collaboration, a client emits an event (like ""Customer created"") which other services can subscribe to. These subscribers then handle the event based on their specific logic. This approach is highly decoupled and leverages asynchronous communication.
```java
public void emitEvent(String eventType) {
    // Simulate emitting an event
    System.out.println(eventType + "" event emitted"");
}
```
x??",628,"Synchronous Versus Asynchronous Before we start diving into the specifics of different technology choices, we should discuss one of the most important decisions we can make in terms of how services co...",qwen2.5:latest,2025-11-02 23:42:38,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Orchestration Versus Choreography,Orchestration vs Choreography,"#### Orchestration vs Choreography
Orchestration involves a central brain (like a service) guiding the process through request/response calls. Choreography involves services independently reacting to events they subscribe to.
:p What is orchestration?
??x
In orchestration, a central brain (often a specific service like Customer Service in our example) manages and coordinates multiple steps of a process using request/response calls. This approach can be simpler but can centralize too much logic into one place.
```java
public void handleCustomerCreation() {
    // Simulate calling other services via request/response
    loyaltyPointsBank.createRecord();
    emailService.sendWelcomeEmail();
    postalService.sendWelcomePack();
}
```
x??",743,"Synchronous Versus Asynchronous Before we start diving into the specifics of different technology choices, we should discuss one of the most important decisions we can make in terms of how services co...",qwen2.5:latest,2025-11-02 23:42:38,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Orchestration Versus Choreography,Orchestration vs Choreography (continued),"#### Orchestration vs Choreography (continued)
Orchestration involves a central service guiding the process, while choreography has each component independently reacting to events. Choreography is more decoupled but requires additional monitoring for tracking processes.
:p What is choreography?
??x
In choreography, services independently react to events they subscribe to without relying on a centralized orchestrator. Each service understands its role and reacts accordingly. This approach leads to highly decoupled systems but requires extra effort for process monitoring.
```java
public void handleCustomerCreation() {
    // Simulate emitting an event
    emitEvent(""Customer created"");
}
```
x??

---",707,"Synchronous Versus Asynchronous Before we start diving into the specifics of different technology choices, we should discuss one of the most important decisions we can make in terms of how services co...",qwen2.5:latest,2025-11-02 23:42:38,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Brittleness,Remote Procedure Calls (RPC) Overview,"---
#### Remote Procedure Calls (RPC) Overview
Background context: RPC is a technique for making local calls that execute on remote services. It aims to make remote method calls appear as if they were local, simplifying development by abstracting network communications.

:p What are some key characteristics of RPC technologies?
??x
Some key characteristics include:
- The goal is to hide the complexity of remote calls behind a local call interface.
- They can be binary (like Java RMI, Thrift) or use XML for message formats (like SOAP).
- Some rely on an interface definition like WSDL, while others require tighter coupling.

These technologies often provide client and server stub generation tools which help in rapid development but come with downsides such as tight platform coupling.
x??",796,Remote Procedure Calls Remote procedure call  refers to the technique of making a local call and having it execute on a remote service somewhere. There are a number of different types of RPC technolog...,qwen2.5:latest,2025-11-02 23:42:49,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Brittleness,Platform Coupling in RPC,"#### Platform Coupling in RPC
Background context: Many RPC mechanisms are tightly coupled to specific platforms or languages, limiting interoperability. For example, Java RMI ties both the client and server to the JVM.

:p How does platform coupling affect RPC implementations?
??x
Platform coupling can restrict which technologies can be used on the client and server sides. Using an interface like Java RMI means that both parties must use Java, which can limit flexibility but also provide tighter integration.

This tight coupling can expose internal technical implementation details, reducing interoperability between different platforms.
x??",647,Remote Procedure Calls Remote procedure call  refers to the technique of making a local call and having it execute on a remote service somewhere. There are a number of different types of RPC technolog...,qwen2.5:latest,2025-11-02 23:42:49,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Brittleness,Performance Considerations in RPC,"#### Performance Considerations in RPC
Background context: Local calls are much faster than remote ones due to the overhead of marshalling and un-marshalling payloads and network latency. RPC implementations need to handle these differences carefully when designing APIs.

:p How do local and remote method calls differ, and why is this important for API design?
??x
Local method calls are fast because they occur in-process without any network communication. Remote procedure calls, however, involve significant overhead due to serialization/deserialization and network latency. This means that the performance characteristics of local methods cannot be assumed when designing remote interfaces.

For instance:
```java
// Local call example (fast)
public Customer findCustomer(String id) {
    // In-process call
}

// Remote call example (potentially slow)
public Customer findCustomer(String id) throws RemoteException {
    // Network communication and serialization/deserialization overhead
}
```
Developers must consider these differences when designing APIs to ensure efficient performance.
x??",1101,Remote Procedure Calls Remote procedure call  refers to the technique of making a local call and having it execute on a remote service somewhere. There are a number of different types of RPC technolog...,qwen2.5:latest,2025-11-02 23:42:49,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Brittleness,Brittleness in RPC Implementations,"#### Brittleness in RPC Implementations
Background context: Some popular RPC implementations, such as Java RMI, can introduce brittleness into the system due to their tight coupling and reliance on fixed method signatures.

:p What are the consequences of changing a remote API's methods in an RPC implementation?
??x
Changes to remote APIs can lead to significant deployment issues. For example, adding or removing fields in a class that is serialized over the network requires updating both client and server components simultaneously.

Consider:
```java
// Original method signature
public Customer findCustomer(String id) throws RemoteException;
```
Adding a new method:
```java
// New method signature
public Customer findCustomer(String emailAddress) throws RemoteException;
```
This change necessitates regenerating stubs, which can cause compatibility issues with existing clients.

To mitigate this:
- Design APIs carefully to avoid frequent changes.
- Use versioning strategies to manage backward and forward compatibility.
x??",1037,Remote Procedure Calls Remote procedure call  refers to the technique of making a local call and having it execute on a remote service somewhere. There are a number of different types of RPC technolog...,qwen2.5:latest,2025-11-02 23:42:49,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Brittleness,Network Reliability in RPC,"#### Network Reliability in RPC
Background context: Networks are inherently unreliable and can fail in various ways. This unreliability must be considered when designing resilient systems using RPC.

:p How should developers handle network failures in their RPC-based applications?
??x
Developers should assume that networks are prone to failure, whether fast or slow, and can even corrupt data packets. Key considerations include:
- Implementing retry mechanisms.
- Handling timeouts gracefully.
- Designing stateless services where possible to reduce dependency on network reliability.

For example:
```java
public class NetworkResilientService {
    public Customer findCustomer(String id) throws Exception {
        try {
            return remoteFindCustomer(id);
        } catch (RemoteException e) {
            log.error(""Network error, retrying..."", e);
            return retryFindCustomer(id); // Retry logic
        }
    }

    private Customer remoteFindCustomer(String id) throws RemoteException {
        // Actual RPC call
    }

    private Customer retryFindCustomer(String id) throws Exception {
        // Retry implementation
    }
}
```
This ensures that the application can handle transient network issues.
x??

---",1239,Remote Procedure Calls Remote procedure call  refers to the technique of making a local call and having it execute on a remote service somewhere. There are a number of different types of RPC technolog...,qwen2.5:latest,2025-11-02 23:42:49,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Hypermedia As the Engine of Application State,RPC Overview,"#### RPC Overview
Background context: Remote Procedure Call (RPC) is a method of communication between different processes or programs. It allows a program to call functions on another computer (or even the same computer, if they are separate processes) without knowing anything about where that function is implemented.

:p What does RPC stand for and what does it allow in programming?
??x
RPC stands for Remote Procedure Call, which allows a program to invoke a procedure on a different system over a network. It essentially abstracts away the complexity of remote communication.
x??",586,"Is RPC Terrible? Despite its shortcomings, I wouldn’t go so far as to call RPC terrible. Some of the common implementations that I have encountered can lead to the sorts of problems I have outlined he...",qwen2.5:latest,2025-11-02 23:43:03,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Hypermedia As the Engine of Application State,Shortcomings of RPC,"#### Shortcomings of RPC
Background context: While RPC can be beneficial for certain operations, some implementations and common practices lead to several issues such as tight coupling between client and server code, difficulty in upgrading the server interface without affecting clients, and potential pitfalls related to network latency and failure.

:p What are some common shortcomings or challenges with using RPC?
??x
Some common challenges include:
- Tight coupling of client and server interfaces.
- Difficulty in evolving the server without forcing synchronized upgrades from all clients.
- Potential for significant overhead due to network latency and reliability issues.
- Need for consistent release cycles between client and server versions.
x??",758,"Is RPC Terrible? Despite its shortcomings, I wouldn’t go so far as to call RPC terrible. Some of the common implementations that I have encountered can lead to the sorts of problems I have outlined he...",qwen2.5:latest,2025-11-02 23:43:03,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Hypermedia As the Engine of Application State,Protocol Buffers or Thrift,"#### Protocol Buffers or Thrift
Background context: To mitigate some of the shortcomings of traditional RPC, modern mechanisms like Protocol Buffers (protobuf) and Apache Thrift are used. These tools allow defining interface descriptions in a language-neutral manner that can be used by different programming languages to generate corresponding client and server code.

:p What are Protocol Buffers and Thrift, and why are they useful?
??x
Protocol Buffers and Thrift are tools for serializing structured data between applications or systems written in different languages. They help avoid the need for lock-step releases of client and server code by generating language-specific implementations from a common interface definition.

For example:
- Protocol Buffers: Define message formats using `.proto` files, which can be used to generate corresponding code in various languages.
```java
// Example .proto file snippet
message Customer {
  int32 id = 1;
  string name = 2;
}
```

- Thrift: Similar functionality but with a different syntax and tooling. Both tools help in reducing the boilerplate code for network communication.

These tools allow more flexible interface evolution, making it easier to add or modify methods without breaking existing client implementations.
x??",1280,"Is RPC Terrible? Despite its shortcomings, I wouldn’t go so far as to call RPC terrible. Some of the common implementations that I have encountered can lead to the sorts of problems I have outlined he...",qwen2.5:latest,2025-11-02 23:43:03,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Hypermedia As the Engine of Application State,REST Overview,"#### REST Overview
Background context: Representational State Transfer (REST) is an architectural style inspired by the web, where resources are represented and manipulated using standard HTTP methods like GET, POST, PUT, DELETE. The focus on resource-based interactions makes it a good fit for microservices architectures.

:p What is REST, and how does it differ from RPC?
??x
REST is an architectural style focused on stateless communication based on the representation of resources. It differs from RPC in that:
- Resources are central: Clients interact with specific resources rather than calling procedures.
- Standard protocols: Uses HTTP for requests and responses (GET, POST, PUT, DELETE).
- Decoupling: The internal storage format is independent of the external representation.

Example REST interaction using HTTP methods:
```java
// Example of a simple GET request to fetch customer data
public Customer getCustomer(int id) {
    String url = ""http://api.example.com/customers/"" + id;
    // Use HTTP client library to send GET request and parse response
    return customerData; // Parsed JSON or XML into Customer object
}
```
x??",1144,"Is RPC Terrible? Despite its shortcomings, I wouldn’t go so far as to call RPC terrible. Some of the common implementations that I have encountered can lead to the sorts of problems I have outlined he...",qwen2.5:latest,2025-11-02 23:43:03,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Hypermedia As the Engine of Application State,Resource Concept in REST,"#### Resource Concept in REST
Background context: In REST, a resource is something that the service exposes for manipulation. Resources can be anything such as a Customer, Product, Order, etc. The key idea is to treat these entities uniformly and represent them with unique URIs (Uniform Resource Identifiers).

:p What is a resource in REST architecture?
??x
A resource in REST refers to any object or entity that the server exposes for manipulation through standard HTTP methods. For example, a customer record can be treated as a resource. Resources are uniquely identified by URIs and can have different representations (e.g., JSON, XML).

Example URI: `http://api.example.com/customers/12345`

Interactions with this resource might include:
- GET `/customers/12345` to fetch the customer details.
- PUT `/customers/12345` to update the customer information.
- DELETE `/customers/12345` to remove the customer record.

The representation of the resource (e.g., in JSON) is independent of its internal storage format.
x??",1024,"Is RPC Terrible? Despite its shortcomings, I wouldn’t go so far as to call RPC terrible. Some of the common implementations that I have encountered can lead to the sorts of problems I have outlined he...",qwen2.5:latest,2025-11-02 23:43:03,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Hypermedia As the Engine of Application State,HTTP and REST,"#### HTTP and REST
Background context: While REST can use various underlying protocols, it is most commonly implemented over HTTP due to its built-in support for CRUD operations through standard verbs like GET, POST, PUT, DELETE. These methods are well-defined by the HTTP specification.

:p How does HTTP fit into the REST architecture?
??x
HTTP fits into the REST architecture as a means of transmitting resource representations between client and server. The HTTP verbs (GET, POST, PUT, DELETE) map directly to basic operations on resources in REST:

- GET: Retrieve a representation of a resource.
- POST: Create or update a resource.
- PUT: Update an existing resource.
- DELETE: Remove a resource.

Example HTTP interaction for creating a new customer:
```java
// Example using HttpClient library
HttpClient httpClient = HttpClient.newHttpClient();
HttpRequest request = HttpRequest.newBuilder()
    .uri(URI.create(""http://api.example.com/customers""))
    .header(""Content-Type"", ""application/json"")
    .POST(HttpRequest.BodyPublishers.ofString(jsonPayload))
    .build();

HttpResponse<String> response = httpClient.send(request, HttpResponse.BodyHandlers.ofString());
// Process the HTTP response
```
x??

---",1219,"Is RPC Terrible? Despite its shortcomings, I wouldn’t go so far as to call RPC terrible. Some of the common implementations that I have encountered can lead to the sorts of problems I have outlined he...",qwen2.5:latest,2025-11-02 23:43:03,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Hypermedia As the Engine of Application State,GET vs POST Methods,"#### GET vs POST Methods
Background context explaining the use of GET and POST methods. The GET method is idempotent, meaning it can be called any number of times without different results each time (e.g., retrieving a customer's details). The POST method is used to create new resources on the server.
:p What are the key differences between GET and POST methods?
??x
The GET method retrieves existing resources from the server in an idempotent way, meaning multiple identical requests will have no additional effect beyond a single request. On the other hand, the POST method creates new resources on the server and is not idempotent because sending the same request more than once can result in different outcomes.
???x
The answer with detailed explanations.
In essence, GET is used for fetching data without changing state (like customer details), while POST is used for creating or updating data (like a new customer record). Here’s a simple pseudocode to illustrate:

```pseudocode
// Example of using GET and POST methods in pseudo-code
function getCustomerDetails(id) {
    // Makes an idempotent request to retrieve customer details
    sendGETRequest(""https://api.example.com/customers/"" + id)
}

function createNewCustomer(customerData) {
    // Creates a new resource on the server by sending POST data
    sendPOSTRequest(""https://api.example.com/customers"", customerData)
}
```

This illustrates how GET and POST methods are used in different scenarios to achieve idempotent and non-idempotent operations respectively.
????",1537,"GET retrieves a resource in an idempotent way, and POST creates a new resource. This means we can avoid lots of different  createCustomer  or  editCustomer  methods. Instead, we can simply POST a cust...",qwen2.5:latest,2025-11-02 23:43:15,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Hypermedia As the Engine of Application State,HTTP Ecosystem Benefits,"#### HTTP Ecosystem Benefits
Background context explaining the benefits of using HTTP, including caching proxies like Varnish, load balancers like mod_proxy, and various monitoring tools that support HTTP natively. The ecosystem provides numerous security controls such as basic auth and client certificates.
:p What are some key benefits of using HTTP for web services?
??x
The primary benefits include the ability to use existing infrastructure like caching proxies (Varnish) and load balancers (mod_proxy), which can help handle large volumes of traffic efficiently. Additionally, the ecosystem supports various security measures such as basic authentication and client certificates out of the box.
???x
The answer with detailed explanations.
Using HTTP for web services leverages a robust ecosystem that includes tools like Varnish for caching responses to reduce load on servers and mod_proxy for distributing requests across multiple servers, enhancing scalability. Security-wise, built-in features such as basic authentication (using headers) or client certificates provide a secure connection layer.

Here’s an example of using HTTP Basic Auth in Java:
```java
import java.net.HttpURLConnection;
import java.net.URL;

public class HttpClient {
    public static void makeRequest(String username, String password) throws Exception {
        URL url = new URL(""https://api.example.com/resource"");
        HttpURLConnection conn = (HttpURLConnection) url.openConnection();
        conn.setRequestMethod(""GET""); // Or ""POST"" for creating resources
        conn.setRequestProperty(""Authorization"", ""Basic "" + 
            new String(Base64.getEncoder().encode((username + "":"" + password).getBytes())));
        int responseCode = conn.getResponseCode();
        if (responseCode == HttpURLConnection.HTTP_OK) {
            // Handle success
        } else {
            // Handle failure
        }
    }
}
```
This code demonstrates how to use HTTP Basic Auth by encoding the credentials and sending them in the request header.
????",2035,"GET retrieves a resource in an idempotent way, and POST creates a new resource. This means we can avoid lots of different  createCustomer  or  editCustomer  methods. Instead, we can simply POST a cust...",qwen2.5:latest,2025-11-02 23:43:15,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Hypermedia As the Engine of Application State,Hypermedia as the Engine of Application State (HATEOAS),"#### Hypermedia as the Engine of Application State (HATEOAS)
Background context explaining HATEOAS, which is a principle that encourages clients to navigate resources through hyperlinks. The concept involves rich content with links to other pieces of content, similar to how web pages work today.
:p What does ""Hypermedia as the Engine of Application State"" (HATEOAS) mean?
??x
HATEOAS means that client applications should interact with servers by following links embedded within hypermedia representations. Instead of hardcoding specific URIs or endpoints in client code, HATEOAS allows clients to discover and navigate resources dynamically via hyperlinks.
???x
The answer with detailed explanations.
In essence, HATEOAS is a design principle that promotes state transitions and interactions through links present in the responses from the server. This approach minimizes coupling between the client and server by allowing the server to dictate which actions are available.

For example, consider an API response containing hyperlinks:
```json
{
    ""id"": 123,
    ""name"": ""John Doe"",
    ""actions"": [
        {
            ""href"": ""/customers/123/orders"",
            ""rel"": ""orders""
        },
        {
            ""href"": ""/customers/123/edit"",
            ""rel"": ""edit""
        }
    ]
}
```
The client can navigate to `/customers/123/orders` or `/customers/123/edit` based on the `actions` links provided in the JSON response, without needing to know these URIs beforehand.

This dynamic linking is a key feature of modern web applications and APIs like RESTful services.
????

---",1590,"GET retrieves a resource in an idempotent way, and POST creates a new resource. This means we can avoid lots of different  createCustomer  or  editCustomer  methods. Instead, we can simply POST a cust...",qwen2.5:latest,2025-11-02 23:43:15,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Hypermedia As the Engine of Application State,Hypermedia Controls and Shopping Carts Analogy,"#### Hypermedia Controls and Shopping Carts Analogy

Background context: The text discusses how hypermedia controls, similar to the familiar shopping cart on websites, enable users (both human and electronic) to interact with web services in a more intuitive way. These controls provide links or references that guide clients (like browsers or apps) to perform actions such as navigating to another page or executing an operation.
:p What is the analogy used to explain hypermedia controls?
??x
The analogy compares hypermedia controls to shopping carts on websites, where users can still recognize and interact with them even if their appearance changes over time. This comparison illustrates how clients need to understand implicit contracts, such as knowing that a cart contains items for purchase.
x??",805,"The location of the shopping cart has changed over time. The graphic has changed. The link has changed. But as humans we are smart enough to still see a shopping cart, know what it is, and interact wi...",qwen2.5:latest,2025-11-02 23:43:26,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Hypermedia As the Engine of Application State,Hypermedia Controls in MusicCorp Example,"#### Hypermedia Controls in MusicCorp Example

Background context: The text provides an example of hypermedia controls used on a MusicCorp album listing page. These controls help the client (e.g., a browser or app) perform actions like navigating to an artist's page or purchasing an album by understanding and following specific links with defined relations.
:p What are the two hypermedia controls mentioned in the MusicCorp example?
??x
The two hypermedia controls mentioned are:
1. A link with `rel=""/artist""` that navigates to the artist’s page.
2. A link with `rel=""/instantpurchase""` that allows instant purchase of the album.
x??",637,"The location of the shopping cart has changed over time. The graphic has changed. The link has changed. But as humans we are smart enough to still see a shopping cart, know what it is, and interact wi...",qwen2.5:latest,2025-11-02 23:43:26,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Hypermedia As the Engine of Application State,Decoupling Between Client and Server,"#### Decoupling Between Client and Server

Background context: The text explains how hypermedia controls provide a high level of decoupling between clients and servers by allowing changes in the underlying implementation without breaking client functionality. This is achieved through understanding and following specific links or references, similar to how humans recognize and interact with shopping carts.
:p How does hypermedia control help achieve decoupling?
??x
Hypermedia controls help achieve decoupling by abstracting the client from the detailed implementation of the server. Clients can navigate using defined relations (e.g., artist or instantpurchase) without needing to know specific URIs, making it easier for both parties to change their implementations over time.
```java
// Example pseudo-code for following a hypermedia control
public void followHyperlink(String rel, String href) {
    if (""artist"".equals(rel)) {
        navigateToArtistPage(href);
    } else if (""instantpurchase"".equals(rel)) {
        purchaseAlbum(href);
    }
}
```
x??",1063,"The location of the shopping cart has changed over time. The graphic has changed. The link has changed. But as humans we are smart enough to still see a shopping cart, know what it is, and interact wi...",qwen2.5:latest,2025-11-02 23:43:26,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Hypermedia As the Engine of Application State,Progressive Discovery of the API,"#### Progressive Discovery of the API

Background context: The text emphasizes that following links through hypermedia controls allows clients to progressively discover the available operations and endpoints, which is beneficial for implementing new clients. However, this approach can be ""chatty,"" meaning it may involve more requests than a direct method.
:p What does progressive discovery mean in the context of API usage?
??x
Progressive discovery means that clients use hypermedia controls (links) to explore and interact with different parts of an API over multiple steps or requests. This allows for gradual learning of the available operations without needing all information upfront, which is useful for implementing new clients.
x??",743,"The location of the shopping cart has changed over time. The graphic has changed. The link has changed. But as humans we are smart enough to still see a shopping cart, know what it is, and interact wi...",qwen2.5:latest,2025-11-02 23:43:26,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Hypermedia As the Engine of Application State,Trade-offs in Using Hypermedia Controls,"#### Trade-offs in Using Hypermedia Controls

Background context: The text discusses both benefits and potential drawbacks of using hypermedia controls, such as increased flexibility but also ""chattiness."" It suggests starting with this approach and optimizing only if necessary due to performance concerns.
:p What is a trade-off mentioned for using hypermedia controls?
??x
A trade-off mentioned is the potential increase in ""chattiness,"" meaning more requests or interactions required by clients to discover all available operations. While this can be less efficient, it provides greater flexibility and decoupling between client and server implementations.
```java
// Example pseudo-code for handling multiple API calls
public void exploreAPI() {
    followHyperlink(""artist"", ""/artist/theBrakes"");
    followHyperlink(""instantpurchase"", ""/instantPurchase/1234"");
}
```
x??",877,"The location of the shopping cart has changed over time. The graphic has changed. The link has changed. But as humans we are smart enough to still see a shopping cart, know what it is, and interact wi...",qwen2.5:latest,2025-11-02 23:43:26,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Hypermedia As the Engine of Application State,Initial Work Required,"#### Initial Work Required

Background context: The text acknowledges that using hypermedia controls requires initial setup and understanding, but the benefits often come later. This is a common challenge in adopting new technologies or approaches.
:p Why might not everyone be sold on using hypermedia controls?
??x
Not everyone may be enthusiastic about using hypermedia controls because there is an initial upfront work required to understand and implement this approach. While the long-term benefits include better decoupling and flexibility, these advantages often take time to materialize.
x??

---",604,"The location of the shopping cart has changed over time. The graphic has changed. The link has changed. But as humans we are smart enough to still see a shopping cart, know what it is, and interact wi...",qwen2.5:latest,2025-11-02 23:43:26,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Downsides to REST Over HTTP,JSON vs. XML,"#### JSON vs. XML
Background context explaining the differences between JSON and XML, their popularity, and use cases. Highlight the advantages and disadvantages of both formats.

:p What are the main differences between JSON and XML in terms of simplicity and ease of consumption?

??x
JSON is a simpler format compared to XML, making it easier for clients to consume resources over HTTP. It is more popular due to its lightweight nature and ease of parsing with tools like JSONPATH. However, JSON lacks the hypermedia control definitions that XML provides, such as link elements, which are crucial for RESTful services.

XML, on the other hand, offers better tool support, especially in terms of data extraction using XPATH or CSS selectors. This makes it easier to extract specific parts of the payload when versioning APIs. However, JSON is generally considered more compact and easier to handle due to its simpler syntax.

Example:
```json
{
  ""name"": ""John Doe"",
  ""age"": 30,
  ""email"": ""johndoe@example.com""
}
```

vs.

Example:
```xml
<user>
    <name>John Doe</name>
    <age>30</age>
    <email>johndoe@example.com</email>
</user>
```
x??",1148,"JSON, XML, or Something Else? The use of standard textual formats gives clients a lot of flexibility as to how they consume resources, and REST over HTTP lets us use a variety of formats. The examples...",qwen2.5:latest,2025-11-02 23:43:41,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Downsides to REST Over HTTP,HAL and Hypermedia Controls,"#### HAL and Hypermedia Controls
Background context explaining the importance of hypermedia controls in RESTful services, especially with JSON. Introduce HAL as a standard for adding such controls to JSON payloads.

:p What is HAL and how does it address the lack of hypermedia control in JSON?

??x
HAL (Hypertext Application Language) is a standard that attempts to add hypermedia controls to JSON payloads, similar to how XML defines link elements. This makes it easier to navigate resources within RESTful services by providing links directly within the JSON payload.

For example:
```json
{
  ""_links"": {
    ""self"": { ""href"": ""/users/123"" },
    ""avatar"": { ""href"": ""/users/123/avatar.jpg"" }
  }
}
```

HAL provides a structured way to include links and other metadata within JSON responses, enhancing the discoverability of resources. Tools like the web-based HAL browser can help in exploring these hypermedia controls.

x??",932,"JSON, XML, or Something Else? The use of standard textual formats gives clients a lot of flexibility as to how they consume resources, and REST over HTTP lets us use a variety of formats. The examples...",qwen2.5:latest,2025-11-02 23:43:41,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Downsides to REST Over HTTP,Using HTML as an API Format,"#### Using HTML as an API Format
Background context on using HTML for both UI and API purposes, highlighting its benefits and drawbacks compared to XML or JSON.

:p Can you explain why some interfaces use HTML instead of XML or JSON?

??x
Using HTML as a format for APIs can be attractive because it can serve both as a user interface (UI) and an application programming interface (API). This dual functionality allows developers to leverage existing HTML parsers and tools. However, this approach has pitfalls since the interactions between humans and computers differ significantly.

For instance:
```html
<!DOCTYPE html>
<html>
  <body>
    <div class=""user-profile"">
      <h1>John Doe</h1>
      <p><img src=""/users/123/avatar.jpg"" /></p>
      <a href=""/users"">Back to Users List</a>
    </div>
  </body>
</html>
```

While this can be convenient, it may lead to issues such as misinterpretation of HTML semantics in the context of an API. It is important to ensure that interactions are clearly defined and separated to avoid confusion.

x??",1048,"JSON, XML, or Something Else? The use of standard textual formats gives clients a lot of flexibility as to how they consume resources, and REST over HTTP lets us use a variety of formats. The examples...",qwen2.5:latest,2025-11-02 23:43:41,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Downsides to REST Over HTTP,Frameworks and Short-Term Gain,"#### Frameworks and Short-Term Gain
Background on how popular frameworks for RESTful services can sometimes promote bad practices by prioritizing ease of implementation over long-term maintainability.

:p Why do some frameworks encourage the direct exposure of database objects in API responses?

??x
Some frameworks, such as Spring Boot, make it very easy to directly expose database objects via APIs. While this can speed up initial development, it often leads to tight coupling between the data storage and the service's interface. This approach can cause significant issues in the long term due to the difficulty of changing the underlying data representation without affecting the API.

For example:
```java
@RestController
@RequestMapping(""/users"")
public class UserController {
    @GetMapping(""/{id}"")
    public User getUserById(@PathVariable Long id) {
        return userRepository.findById(id).orElseThrow(() -> new ResourceNotFoundException(""User not found""));
    }
}
```

This practice can lead to maintenance problems and make it hard to evolve the API independently of changes in data storage. A better approach is to delay implementing proper persistence until the interface has stabilized, allowing the design to be driven by consumer needs rather than technical implementation details.

x??",1310,"JSON, XML, or Something Else? The use of standard textual formats gives clients a lot of flexibility as to how they consume resources, and REST over HTTP lets us use a variety of formats. The examples...",qwen2.5:latest,2025-11-02 23:43:41,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Downsides to REST Over HTTP,REST Over HTTP Drawbacks,"#### REST Over HTTP Drawbacks
Background on the challenges associated with creating client stubs for RESTful services compared to RPC-based systems. Discuss the overhead of HTTP and potential performance issues.

:p What are some downsides of using REST over HTTP in terms of client consumption?

??x
One major downside of REST over HTTP is the difficulty in generating client stubs, unlike with RPC-based systems where tools can easily generate clients from service definitions. While HTTP client libraries provide excellent support for making requests, hypermedia controls require custom implementation and cannot leverage well-established client frameworks.

For instance:
- Lack of built-in support for hypermedia controls makes it challenging to automatically generate a client.
- Potential need to manually implement client logic that can lead to RPC-like behavior or shared client-server code, which introduces coupling issues.

Example:
```java
// Manual Hypermedia Handling in REST Client
public class MyRestClient {
    private RestTemplate restTemplate;

    public User getUserById(Long id) {
        String url = ""/users/"" + id;
        ResponseEntity<User> response = restTemplate.getForEntity(url, User.class);
        if (response.hasBody()) {
            return response.getBody();
        }
        throw new ResourceNotFoundException(""User not found"");
    }
}
```

The overhead of HTTP also includes the initial request and response headers, which can add latency. For low-latency applications, more lightweight protocols like WebSockets or direct TCP-based communication might be preferred.

x??

---",1621,"JSON, XML, or Something Else? The use of standard textual formats gives clients a lot of flexibility as to how they consume resources, and REST over HTTP lets us use a variety of formats. The examples...",qwen2.5:latest,2025-11-02 23:43:41,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Technology Choices,Message Brokers for Event-Based Communication,"#### Message Brokers for Event-Based Communication
Message brokers like RabbitMQ offer a scalable and resilient way to handle event-based, asynchronous communication between microservices. They allow producers to publish events and consumers to subscribe to these events via APIs.

:p What are message brokers, and how do they facilitate asynchronous communication in microservices?
??x
Message brokers act as intermediaries that enable producers (publishers) of events to communicate with subscribers without needing direct interaction. This is particularly useful for implementing event-driven architectures where services need to react to external or internal events asynchronously.

Code example:
```java
// Producer (Publisher)
RabbitTemplate rabbitTemplate = new RabbitTemplate(connectionFactory);
rabbitTemplate.convertAndSend(""queueName"", ""eventPayload"");

// Consumer (Subscriber)
@RabbitListener(queues = ""queueName"")
public void handleEvent(String event) {
    // Process the event
}
```
x??",1002,"Implementing Asynchronous Event-Based Collaboration We’ve talked for a bit about some technologies that can help us implement request/response patterns. What about event-based, asynchronous communicat...",qwen2.5:latest,2025-11-02 23:43:52,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Technology Choices,ATOM as an Event Propagation Method,"#### ATOM as an Event Propagation Method
ATOM is a REST-compliant specification that can be used to propagate events. It defines semantics for publishing feeds of resources, allowing clients to poll these feeds for changes.

:p How does ATOM differ from message brokers in propagating events?
??x
ATOM allows services to publish events by creating and consuming feeds. While it reuses the existing HTTP infrastructure and associated libraries, it lacks some of the built-in features provided by message brokers such as handling duplicate messages, managing shared state among consumers, and ensuring reliable delivery.

Code example:
```java
// Publishing an event using ATOM
Feed feed = new Feed();
Entry entry = new Entry();
entry.setTitle(""New Customer Created"");
feed.add(entry);
webClient.post()
    .uri(""/events"")
    .header(HttpHeaders.CONTENT_TYPE, AtomMediaType.ATOM_XML_VALUE)
    .body(BodyInserters.fromValue(feed))
    .retrieve()
    .toBodilessEntity();

// Consuming the feed
ReactiveFeedsClient client = FeedsClient.create();
client.subscribe(""/events"", event -> {
    System.out.println(""Received event: "" + event.getTitle());
});
```
x??",1158,"Implementing Asynchronous Event-Based Collaboration We’ve talked for a bit about some technologies that can help us implement request/response patterns. What about event-based, asynchronous communicat...",qwen2.5:latest,2025-11-02 23:43:52,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Technology Choices,Competing Consumer Pattern with ATOM,"#### Competing Consumer Pattern with ATOM
The Competing Consumer pattern is used to scale up the number of workers handling independent jobs by having multiple consumers compete for messages. However, managing shared state among these consumers can be complex.

:p What challenges does using the Competing Consumer pattern face when implementing ATOM-based event propagation?
??x
Using the Competing Consumer pattern with ATOM requires careful management of shared state to avoid duplicate processing. Unlike message brokers, where a standard queue automatically handles this issue, ATOM consumers must implement their own logic to track seen messages and coordinate among themselves.

Code example:
```java
// Pseudocode for Competing Consumers in ATOM
List<Consumer> consumers = new ArrayList<>();
for (int i = 0; i < numberOfConsumers; i++) {
    Consumer consumer = new Consumer();
    consumers.add(consumer);
}

while (!allMessagesProcessed()) {
    Entry entry = pollFeedEntries();
    List<Consumer> winners = selectWinningConsumers(entry);
    
    for (Consumer winner : winners) {
        processEntry(winner, entry);
    }
}
```
x??",1144,"Implementing Asynchronous Event-Based Collaboration We’ve talked for a bit about some technologies that can help us implement request/response patterns. What about event-based, asynchronous communicat...",qwen2.5:latest,2025-11-02 23:43:52,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Technology Choices,Synchronous vs. Asynchronous Communication,"#### Synchronous vs. Asynchronous Communication
The same considerations apply to the encoding of events as with requests and responses in synchronous communication. JSON is often a suitable choice for encoding asynchronous event data.

:p How does synchronous request/response differ from asynchronous event-based communication when it comes to encoding messages?
??x
In both synchronous and asynchronous communication, JSON can be used effectively for encoding messages. The primary difference lies in the handling of these messages: synchronous methods involve immediate acknowledgment and response, while asynchronous methods rely on event subscriptions and callbacks.

Code example:
```java
// Synchronous Request/Response Example
public String sendRequest(String request) {
    // Send request to server
    String response = sendAndReceive(request);
    return response;
}

// Asynchronous Event Handling Example
@RabbitListener(queues = ""eventQueue"")
public void handleEvent(String event) {
    System.out.println(""Received event: "" + event);
}
```
x??

---",1064,"Implementing Asynchronous Event-Based Collaboration We’ve talked for a bit about some technologies that can help us implement request/response patterns. What about event-based, asynchronous communicat...",qwen2.5:latest,2025-11-02 23:43:52,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Complexities of Asynchronous Architectures,Event-Driven Architecture Complexity,"---
#### Event-Driven Architecture Complexity
Event-driven architectures offer more decoupled and scalable systems, but they also introduce complexities related to managing messages. This includes challenges with long-running async requests and short-lived async tasks.

:p What are some of the complexities associated with event-driven architectures?
??x
Some key complexities include handling response management, ensuring proper message delivery in distributed systems, dealing with transient failures like node down scenarios, and implementing robust error recovery mechanisms. Additionally, monitoring and tracing across process boundaries become essential.

Code Example: Implementing a basic correlation ID for tracing requests.
```java
public class Request {
    private String correlationId;
    
    public Request(String correlationId) {
        this.correlationId = correlationId;
    }
    
    // Method to log the request with correlation ID
    public void logRequest() {
        System.out.println(""Request logged with correlation ID: "" + correlationId);
    }
}
```
x??",1087,"Complexities of Asynchronous Architectures Some of this asynchronous stuff seems fun, right? Event-driven architectures seem to lead to significantly more decoupled, scalable systems. And they can. Bu...",qwen2.5:latest,2025-11-02 23:44:03,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Complexities of Asynchronous Architectures,Competing Consumer Pattern and Catastrophic Failover,"#### Competing Consumer Pattern and Catastrophic Failover
In an event-driven architecture, using a competing consumer pattern can lead to issues like catastrophic failover. This happens when messages are retried indefinitely without proper limits.

:p What is a catastrophic failover in the context of asynchronous architectures?
??x
A catastrophic failover occurs when a message processing system continuously retries messages that cause failures without any retry limit or error handling mechanism, leading to infinite loops and resource exhaustion. This can be exacerbated by bugs that cause workers to crash repeatedly on specific tasks.

Code Example: Implementing a simple retry mechanism with a maximum limit.
```java
public class Worker {
    private int maxRetries = 3;
    
    public void processMessage(String message) {
        for (int i = 0; i < maxRetries; i++) {
            try {
                // Process the message
                System.out.println(""Processing message: "" + message);
                break; // Successfully processed, exit loop
            } catch (Exception e) {
                if (i == maxRetries - 1) { // Last retry attempt
                    throw new RuntimeException(""Failed to process message after "" + maxRetries + "" retries"", e);
                }
                System.out.println(""Message processing failed. Retrying..."");
            }
        }
    }
}
```
x??",1416,"Complexities of Asynchronous Architectures Some of this asynchronous stuff seems fun, right? Event-driven architectures seem to lead to significantly more decoupled, scalable systems. And they can. Bu...",qwen2.5:latest,2025-11-02 23:44:03,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Complexities of Asynchronous Architectures,Dead Letter Queue (DLQ),"#### Dead Letter Queue (DLQ)
A dead letter queue is essential in event-driven systems for handling messages that fail multiple times due to unexpected issues.

:p What is a dead letter queue and why is it necessary?
??x
A dead letter queue (DLQ) is a separate queue where failed or problematic messages are moved after they have exceeded their retry limits. It helps in diagnosing and managing messages that cannot be processed correctly, providing a safety net for error-prone tasks.

Code Example: Redirecting messages to a DLQ.
```java
public class MessageQueueManager {
    private Map<String, Queue> queues = new HashMap<>();
    
    public void sendMessageToDLQ(String message) {
        String queueName = ""deadLetterQueue"";
        
        if (!queues.containsKey(queueName)) {
            queues.put(queueName, createQueue());
        }
        
        // Send the message to the DLQ
        send(message, queueName);
    }
    
    private Queue createQueue() {
        return new Queue(); // Assume this creates a new queue
    }
    
    private void send(String message, String queueName) {
        queues.get(queueName).send(message); // Simulate sending a message to the DLQ
    }
}
```
x??",1208,"Complexities of Asynchronous Architectures Some of this asynchronous stuff seems fun, right? Event-driven architectures seem to lead to significantly more decoupled, scalable systems. And they can. Bu...",qwen2.5:latest,2025-11-02 23:44:03,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Complexities of Asynchronous Architectures,Monitoring and Correlation IDs,"#### Monitoring and Correlation IDs
Monitoring and using correlation IDs are crucial for tracing request paths in event-driven architectures. This ensures better visibility into the system's behavior.

:p Why is monitoring and correlation ID implementation important?
??x
Monitoring helps in tracking the performance, health, and flow of data through a distributed system. Correlation IDs allow developers to trace requests across different services or processes, making it easier to diagnose issues and understand the request lifecycle.

Code Example: Logging with correlation IDs for tracing.
```java
public class RequestLogger {
    private String correlationId;
    
    public RequestLogger(String correlationId) {
        this.correlationId = correlationId;
    }
    
    public void logRequest() {
        System.out.println(""Request logged with correlation ID: "" + correlationId);
    }
}
```
x??

---",910,"Complexities of Asynchronous Architectures Some of this asynchronous stuff seems fun, right? Event-driven architectures seem to lead to significantly more decoupled, scalable systems. And they can. Bu...",qwen2.5:latest,2025-11-02 23:44:03,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Access by Reference,Services as State Machines,"#### Services as State Machines
Background context explaining that services are modeled around bounded contexts, where each service owns its logic and controls lifecycle events. This approach avoids anemic services and maintains cohesion by centralizing decision-making within the service itself.

:p What is a microservice's role in managing lifecycle events?
??x
A microservice should control all lifecycle events related to its domain concepts. For instance, when dealing with a customer, the customer service decides which changes are allowed based on its internal logic and rules.
x??",589,"Services as State Machines Whether you choose to become a REST ninja, or stick with an RPC-based mechanism like SOAP, the core concept of the service as a state machine is powerful. We’ve spoken befor...",qwen2.5:latest,2025-11-02 23:44:09,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Access by Reference,Reactive Extensions (Rx),"#### Reactive Extensions (Rx)
Background context explaining that reactive extensions allow for more flexible handling of asynchronous operations by observing outcomes instead of blocking calls. This is particularly useful in distributed systems where concurrent calls need to be managed efficiently.

:p How does Rx help manage multiple service calls in a microservice architecture?
??x
Reactive extensions enable you to handle multiple service calls without blocking, making it easier to manage concurrent requests and compose their results. For example, you can use `Observable` from RxJava to observe the response of a downstream service call and react accordingly.
x??",672,"Services as State Machines Whether you choose to become a REST ninja, or stick with an RPC-based mechanism like SOAP, the core concept of the service as a state machine is powerful. We’ve spoken befor...",qwen2.5:latest,2025-11-02 23:44:09,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Access by Reference,DRY Principle in Microservices,"#### DRY Principle in Microservices
Background context explaining the Don't Repeat Yourself (DRY) principle and how it applies to microservice architectures by emphasizing avoiding redundant code that duplicates system behavior.

:p How does the DRY principle apply differently in a microservice world compared to traditional monolithic applications?
??x
In a microservice world, DRY means avoiding duplication of behavior and knowledge across services. This is important because each service should be self-contained and responsible for its domain logic. Redundant code can lead to inconsistencies and increased maintenance complexity.
x??

---",645,"Services as State Machines Whether you choose to become a REST ninja, or stick with an RPC-based mechanism like SOAP, the core concept of the service as a state machine is powerful. We’ve spoken befor...",qwen2.5:latest,2025-11-02 23:44:09,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Access by Reference,DRY Principle and Microservices,"#### DRY Principle and Microservices
Background context explaining the importance of DRY (Don't Repeat Yourself) in software development. In microservice architectures, while applying DRY can lead to reusable code, it also poses risks such as introducing coupling between services. Code duplication is easier to manage compared to the complexity introduced by shared libraries.
:p What are the main concerns when applying DRY in a microservice architecture?
??x
Applying DRY in a microservice architecture can introduce unnecessary coupling if changes propagate across multiple services, especially when common code leaks outside service boundaries. This can lead to maintenance issues and potential bugs due to misaligned updates between services.
```java
// Example of a shared domain object
public class SharedDomainObject {
    private String id;
    private String name;

    public void updateName(String newName) {
        // Update logic here
    }
}
```
x??",966,"When you want to change behavior, and that behavior is duplicated in many parts of your system, it is easy to forget everywhere you need to make a change, which can lead to bugs. So using DRY as a man...",qwen2.5:latest,2025-11-02 23:44:19,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Access by Reference,Coupling in Microservices via Common Code,"#### Coupling in Microservices via Common Code
Context around the dangers of using common code across microservices. This can lead to tight coupling, especially if changes in shared libraries require updates in multiple services and message queues.
:p What are some potential issues with shared domain objects or common libraries in a microservice architecture?
??x
Shared domain objects or common libraries can cause tight coupling between services, as changes in one service necessitate updates across many others. This can lead to maintenance overhead, increased risk of bugs due to synchronization issues, and the need for draining message queues.
```java
// Example of shared domain object causing coupling issues
public class SharedDomainObject {
    private String id;
    private String name;

    public void updateName(String newName) {
        // Update logic here
    }
}
```
x??",891,"When you want to change behavior, and that behavior is duplicated in many parts of your system, it is easy to forget everywhere you need to make a change, which can lead to bugs. So using DRY as a man...",qwen2.5:latest,2025-11-02 23:44:19,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Access by Reference,Tailored Service Templates and Code Duplication,"#### Tailored Service Templates and Code Duplication
Explanation on the benefits and limitations of using a tailored service template. While it can help in avoiding code duplication, it can also limit technology choices if strictly enforced.
:p How does RealEstate.com.au ensure they avoid coupling when creating new services?
??x
RealEstate.com.au uses a tailored service template to bootstrap new service creation but avoids sharing this code across different services by copying the template for each new service. This approach helps in maintaining loose coupling and reducing the risk of breaking changes propagating through multiple services.
```java
// Example of using a tailored service template
public class ServiceTemplate {
    // Common configurations and setup logic here
}
```
x??",794,"When you want to change behavior, and that behavior is duplicated in many parts of your system, it is easy to forget everywhere you need to make a change, which can lead to bugs. So using DRY as a man...",qwen2.5:latest,2025-11-02 23:44:19,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Access by Reference,Client Libraries and API Design,"#### Client Libraries and API Design
Discussion on the importance of client libraries for ease of use but also the risks of tightly coupling server and client logic. AWS' approach with SDKs is highlighted as a good model.
:p Why can creating client libraries pose challenges in microservice architectures?
??x
Creating client libraries can introduce tight coupling between the server API and the client code, potentially leading to changes in one needing updates in the other. This can limit technology choices and make it harder to implement fixes or use alternative technologies.
```java
// Example of a client library with tightly coupled logic
public class ServiceClient {
    public void doSomething() {
        // Server-specific logic here
    }
}
```
x??",762,"When you want to change behavior, and that behavior is duplicated in many parts of your system, it is easy to forget everywhere you need to make a change, which can lead to bugs. So using DRY as a man...",qwen2.5:latest,2025-11-02 23:44:19,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Access by Reference,Amazon Web Services Model for Client Libraries,"#### Amazon Web Services Model for Client Libraries
Explanation on AWS' approach where SDKs provide abstractions over underlying APIs, written by different teams to avoid coupling.
:p What is the key benefit of using community or third-party SDKs in microservices?
??x
Using community or third-party SDKs can help decouple client code from server logic, allowing for greater flexibility in technology choices and reducing the risk of tightly coupled updates. This approach leverages abstractions provided by trusted sources to ensure loose coupling.
```java
// Example of using an AWS SDK
public class AwsServiceClient {
    public void doSomething() {
        // Use AWS SDK methods here
    }
}
```
x??",704,"When you want to change behavior, and that behavior is duplicated in many parts of your system, it is easy to forget everywhere you need to make a change, which can lead to bugs. So using DRY as a man...",qwen2.5:latest,2025-11-02 23:44:19,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Access by Reference,Client Library Approach and Its Implications,"#### Client Library Approach and Its Implications
Background context: The client library approach is often discussed as a way to manage service interactions, particularly within large-scale systems like Netflix. This method ensures reliability and scalability by handling aspects such as service discovery, failure modes, logging, etc. However, it also comes with the risk of increased coupling between client and server.
:p How does the client library approach benefit and potentially harm system design?
??x
The benefits include centralized management of common issues like service discovery and failure handling, which can significantly improve reliability and scalability. However, over time, this can lead to tight coupling between clients and servers, making it harder to make changes independently.

```java
// Example of a simplified client library method for service discovery
public class ClientLibrary {
    public String discoverService(String serviceName) {
        // Logic to discover the service endpoint
        return ""http://service-endpoint"";
    }
}
```
x??",1078,"This degree of separation seems to work, and avoids some of the pitfalls of client libraries. Part of the reason this works so well is that the client is in charge of when the upgrade happens. If you ...",qwen2.5:latest,2025-11-02 23:44:31,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Access by Reference,Service Discovery and Failure Handling,"#### Service Discovery and Failure Handling
Background context: When using microservices, services need to be able to find each other at runtime. Netflix's client libraries manage this dynamically through mechanisms like DNS or configuration management systems.
:p What is service discovery in the context of microservices?
??x
Service discovery refers to the process by which a microservice identifies and locates another microservice during runtime. This is crucial for dynamic, distributed systems where services can fail or scale up/down independently.

```java
// Pseudocode example of service discovery using DNS
public class ServiceDiscovery {
    public String getEndpoint(String serviceName) {
        // Query DNS for the endpoint of the service
        return ""http://service-endpoint-123"";
    }
}
```
x??",817,"This degree of separation seems to work, and avoids some of the pitfalls of client libraries. Part of the reason this works so well is that the client is in charge of when the upgrade happens. If you ...",qwen2.5:latest,2025-11-02 23:44:31,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Access by Reference,Client/Server Coupling and Upgrades,"#### Client/Server Coupling and Upgrades
Background context: Ensuring that client upgrades can happen independently of server updates is critical. Netflix emphasizes this by allowing clients to decide when they upgrade, maintaining system reliability even if one component fails or changes.
:p How does Netflix manage the coupling between client and server?
??x
Netflix manages coupling through its client libraries, which handle service discovery, failure modes, logging, etc., allowing clients to independently upgrade without disrupting the entire system. This is achieved by separating concerns: handling underlying transport protocols from business logic.

```java
// Pseudocode example of client managing upgrades
public class ClientUpgradeManager {
    public void checkForUpdate() {
        // Logic to check for and apply updates
        if (shouldUpdate()) {
            updateClientLibrary();
        }
    }

    private boolean shouldUpdate() {
        // Check conditions to decide whether an upgrade is necessary
        return true;
    }
}
```
x??",1064,"This degree of separation seems to work, and avoids some of the pitfalls of client libraries. Part of the reason this works so well is that the client is in charge of when the upgrade happens. If you ...",qwen2.5:latest,2025-11-02 23:44:31,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Access by Reference,Passing Domain Entity References,"#### Passing Domain Entity References
Background context: In a microservice architecture, it's important to ensure that the lifecycle of domain entities is managed within their respective services. This means when you request data, it should be from the source service where the entity's state is considered authoritative.
:p How do we handle references to domain entities in a microservice architecture?
??x
When retrieving a domain entity like `Customer`, treat the service hosting that entity as the single source of truth. Pass around a reference to the original resource instead of a cached memory of its previous state, ensuring you can always fetch the latest data.

```java
// Example method for fetching customer details from the Customer Service
public class CustomerServiceClient {
    public Customer getCustomer(String customerId) {
        // Fetch the current state of the customer from the service
        return client.request(""GET"", ""/customer/"" + customerId);
    }
}
```
x??",994,"This degree of separation seems to work, and avoids some of the pitfalls of client libraries. Part of the reason this works so well is that the client is in charge of when the upgrade happens. If you ...",qwen2.5:latest,2025-11-02 23:44:31,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Access by Reference,Avoiding Data Duplication and Maintaining Efficiency,"#### Avoiding Data Duplication and Maintaining Efficiency
Background context: In microservices, frequently accessing data can lead to inefficiencies. Balancing between having a cached memory of an entity and ensuring you fetch the latest state from the source service is crucial for maintaining system performance.
:p How do we balance between using a cached memory and fetching the latest state?
??x
You should use a cached memory of an entity but also maintain a reference to its original resource. This way, if the data needs to be refreshed or updated, you can fetch the new state from the source service.

```java
// Example method for handling customer references
public class CustomerHandler {
    private final String customerId;
    private final Customer cachedCustomer;

    public CustomerHandler(String customerId) {
        this.customerId = customerId;
        this.cachedCustomer = getCustomerFromCache();
    }

    public void updateCustomer() {
        // Check if the cached customer is stale and fetch from the service
        if (isStale(cachedCustomer)) {
            cachedCustomer = getCustomerFromService();
        }
    }

    private Customer getCustomerFromCache() {
        // Return a cached memory of the customer
        return new Customer(""John Doe"", ""john.doe@example.com"");
    }

    private boolean isStale(Customer cachedCustomer) {
        // Logic to determine if the cache needs to be refreshed
        return true;
    }

    private Customer getCustomerFromService() {
        // Fetch the latest state from the service
        return customerServiceClient.getCustomer(customerId);
    }
}
```
x??",1643,"This degree of separation seems to work, and avoids some of the pitfalls of client libraries. Part of the reason this works so well is that the client is in charge of when the upgrade happens. If you ...",qwen2.5:latest,2025-11-02 23:44:31,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Access by Reference,Asynchronous Request Handling vs. Resource References,"#### Asynchronous Request Handling vs. Resource References
Background context explaining how asynchronous request handling and resource references can be used to manage data retrieval and processing efficiently. When an order is shipped, sending a request with customer details can work, but using URIs for Customer and Order resources allows the email service to fetch the latest information when needed.
:p How does using URIs for Customer and Order resources benefit asynchronous request handling?
??x
Using URIs for Customer and Order resources benefits asynchronous request handling by allowing the email service to look up the latest state of these entities when it is time to send the email. This approach ensures that the most current data is used, reducing the risk of sending outdated information.
```java
// Example of using URI in a request
public void sendShipmentNotification(String customerUri) {
    // The email service fetches the customer details from the URI
    Customer customer = customerService.getCustomer(customerUri);
    String emailBody = generateEmailBody(customer.getName(), customer.getOrderDetails());
    sendEmail(emailBody);
}
```
x??",1170,"Let’s consider the example where we ask the email service to send an email when an order has been shipped. Now we could send in the request to the email service with the customer’s email address, name...",qwen2.5:latest,2025-11-02 23:44:43,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Access by Reference,Event-Based Collaboration and Resource State Management,"#### Event-Based Collaboration and Resource State Management
Background context explaining event-based collaboration, where a ""this happened"" message is sent without the need to include all details. The importance of knowing what specifically happened when an event occurs is highlighted.
:p Why is it important to know ""what happened"" in event-based collaborations?
??x
Knowing ""what happened"" in event-based collaborations is crucial because it provides context and details about the state or change that occurred, which can be essential for making informed decisions. For instance, if a customer's resource changes, knowing what the customer looked like at the time of the event helps in understanding the specific circumstances leading to the change.
```java
// Example of an event handler receiving a change in a Customer resource
public void handleCustomerChangeEvent(CustomerChangeEvent event) {
    // Retrieve the state of the Customer at the time of the event
    Customer previousState = customerService.getCustomer(event.getCustomerId(), event.getTimeStamp());
    processCustomerChange(previousState, event);
}
```
x??",1131,"Let’s consider the example where we ask the email service to send an email when an order has been shipped. Now we could send in the request to the email service with the customer’s email address, name...",qwen2.5:latest,2025-11-02 23:44:43,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Access by Reference,Load Management and Caching Strategies,"#### Load Management and Caching Strategies
Background context explaining how accessing resources by reference can lead to increased load on the resource service. The use of caching strategies can mitigate this issue, but it requires careful management of data freshness.
:p How can caching help in managing load when using references for resource access?
??x
Caching can help manage load when using references for resource access by storing frequently accessed data temporarily. By setting appropriate cache controls and invalidating cached data as needed, the system can reduce the number of direct requests to the resource service, thereby减轻负载。例如，HTTP 提供了多种缓存控制选项，如 `Cache-Control` 和 `ETag`，可以用来管理数据的时效性。
```java
// Example of using cache controls in HTTP headers
public void sendShipmentNotification(String customerUri) {
    String url = ""http://example.com/api/customers/"" + customerUri;
    // Using Cache-Control header to manage caching
    HttpHeaders headers = new HttpHeaders();
    headers.setCacheControl(""max-age=3600"");
    
    ResponseEntity<Customer> response = restTemplate.exchange(
        url, HttpMethod.GET, new HttpEntity<>(headers), Customer.class);
    
    if (response.getStatusCode() == HttpStatus.NOT_MODIFIED) {
        // Handle cached data
    } else {
        // Process the fetched customer details
        Customer customer = response.getBody();
        String emailBody = generateEmailBody(customer.getName(), customer.getOrderDetails());
        sendEmail(emailBody);
    }
}
```
x??",1523,"Let’s consider the example where we ask the email service to send an email when an order has been shipped. Now we could send in the request to the email service with the customer’s email address, name...",qwen2.5:latest,2025-11-02 23:44:43,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Access by Reference,Data Freshness and Request Efficiency,"#### Data Freshness and Request Efficiency
Background context explaining the importance of knowing data freshness when passing around information in requests. Overloading requests with too much data can increase coupling, but not including enough might lead to sending outdated information.
:p How does ensuring data freshness impact request efficiency?
??x
Ensuring data freshness impacts request efficiency by reducing the amount of redundant data passed in each request and minimizing the risk of using outdated information. By providing clear indicators of when a resource was in a given state, services can make more informed decisions about whether to fetch new data or use cached information.
```java
// Example of including freshness information with a request
public void sendShipmentNotification(String customerUri) {
    String url = ""http://example.com/api/customers/"" + customerUri;
    // Adding ETag header for freshness check
    HttpHeaders headers = new HttpHeaders();
    headers.set(""If-None-Match"", ""etag-value"");
    
    ResponseEntity<Customer> response = restTemplate.exchange(
        url, HttpMethod.GET, new HttpEntity<>(headers), Customer.class);
    
    if (response.getStatusCode() == HttpStatus.NOT_MODIFIED) {
        // Handle cached data
    } else {
        // Process the fetched customer details
        Customer customer = response.getBody();
        String emailBody = generateEmailBody(customer.getName(), customer.getOrderDetails());
        sendEmail(emailBody);
    }
}
```
x??
---",1526,"Let’s consider the example where we ask the email service to send an email when an order has been shipped. Now we could send in the request to the email service with the customer’s email address, name...",qwen2.5:latest,2025-11-02 23:44:43,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Defer It for as Long as Possible,Defer Breaking Changes for as Long as Possible,"---
#### Defer Breaking Changes for as Long as Possible
Background context: The objective is to reduce the impact of making breaking changes by deferring them. This can be achieved through careful choice of integration technologies and encouraging good behavior in clients to avoid tight coupling.

:p How does picking REST over database integration help in avoiding breaking changes?
??x
Picking REST helps because it reduces the likelihood of changes in internal implementation details affecting the service interface. For example, if you're using a database for integration, a change in schema might require immediate updates in all dependent services due to direct access to database fields.

For contrast:
```java
// Example of tight coupling with database (not recommended)
public class OrderService {
    private final CustomerRepository customerRepo;

    public void sendOrderShippedEmail(int customerId) {
        var customer = customerRepo.findById(customerId).orElseThrow();
        // Direct access and manipulation
        String firstName = customer.getFirstName();
        String lastName = customer.getLastName();
        
        // Email logic here...
    }
}
```

In REST, you can use a more flexible approach:
```java
// Example of using a more flexible approach with REST
public class OrderService {
    private final HttpClient httpClient;
    
    public void sendOrderShippedEmail(int customerId) {
        var customerResponse = httpClient.sendRequestToCustomerService(customerId);
        
        // Use XML parsing or JSON to extract necessary fields
        String firstName = customerResponse.getFirstname();
        String lastName = customerResponse.getLastname();
        
        // Email logic here...
    }
}
```
x??",1754,"Versioning In every single talk I have ever done about microservices, I get asked  how do you do versioning?  People have the legitimate concern that eventually they will have to make a change to the ...",qwen2.5:latest,2025-11-02 23:44:55,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Defer It for as Long as Possible,Tolerant Reader Pattern,"#### Tolerant Reader Pattern
Background context: A tolerant reader is a pattern where the service implementation can evolve without breaking clients by ignoring changes that are not relevant to the client. This allows for more flexible and future-proof services.

:p How does the tolerant reader pattern help in avoiding breaking changes?
??x
The tolerant reader pattern helps because it allows the service to change its internal structure or fields without affecting the external interface, provided that irrelevant details can be ignored by clients.

For example:
```xml
<!-- Original response -->
<customer>
    <firstname> Sam </firstname>
    <lastname> Newman </lastname>
    <email> sam@magpiebrain.com </email>
    <telephoneNumber> 555-1234-5678 </telephoneNumber>
</customer>

<!-- Restructured response after removing telephoneNumber -->
<customer>
    <naming>
        <firstname> Sam </firstname>
        <lastname> Newman </lastname>
        <nickname> Magpiebrain </nickname>
        <fullname> Sam ""Magpiebrain"" Newman </fullname>
    </naming>
    <email> sam@magpiebrain.com </email>
</customer>

// Implementation in Java
public String getEmailFromCustomerResponse(String customerId) {
    // Example using XPath to extract email without needing to know the inner structure
    String xml = getCustomerXml(customerId);
    String email = XPathExpression.evaluate(""//email"", xml, null);
    return email;
}
```
x??",1432,"Versioning In every single talk I have ever done about microservices, I get asked  how do you do versioning?  People have the legitimate concern that eventually they will have to make a change to the ...",qwen2.5:latest,2025-11-02 23:44:55,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Defer It for as Long as Possible,Postel’s Law (Robustness Principle),"#### Postel’s Law (Robustness Principle)
Background context: Postel's Law states that ""Be conservative in what you do, be liberal in what you accept from others."" This principle encourages systems to be forgiving of errors and flexible with input, which is useful in scenarios where the service might change or unexpected variations occur.

:p How does Postel’s Law apply to handling changes in microservices?
??x
Postel's Law applies by encouraging services to be tolerant of unexpected or changed inputs while maintaining strictness about their own outputs. This means that a service should validate and handle inputs gracefully, allowing it to evolve without breaking other services that depend on it.

For example:
```java
public class EmailService {
    public void sendOrderShippedEmail(String customerId) {
        // Robust parsing of customer response
        String xml = getCustomerDetails(customerId);
        
        try {
            // Extract necessary fields using a tolerant approach
            String firstName = XPathExpression.evaluate(""//firstname"", xml, null);
            String lastName = XPathExpression.evaluate(""//lastname"", xml, null);
            
            // Proceed with email logic...
        } catch (XPathExpressionException e) {
            log.error(""Failed to parse customer details."", e);
            throw new RuntimeException(""Customer data missing or malformed."");
        }
    }
}
```
x??

---",1442,"Versioning In every single talk I have ever done about microservices, I get asked  how do you do versioning?  People have the legitimate concern that eventually they will have to make a change to the ...",qwen2.5:latest,2025-11-02 23:44:55,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Coexist Different Endpoints,Catch Breaking Changes Early,"#### Catch Breaking Changes Early
Background context: The importance of identifying breaking changes early is crucial to maintain service compatibility and user satisfaction. Consumer-driven contracts and running tests with supported client libraries are techniques to detect such issues before deployment.

:p What technique can help identify breaking changes early?
??x
Using consumer-driven contracts or running tests using each supported library against the latest service can help spot these problems early.
x??",516,"Catch Breaking Changes Early It’s crucial to make sure we pick up changes that will break consumers as soon as possible, because even if we choose the best possible technology, breaks can still happen...",qwen2.5:latest,2025-11-02 23:45:03,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Coexist Different Endpoints,Use Semantic Versioning,"#### Use Semantic Versioning
Background context: Semantic versioning is a specification that uses MAJOR.MINOR.PATCH to indicate the type of changes in software releases. This allows clients to predict whether their application will work with new versions of a service.

:p How does semantic versioning help in managing service compatibility?
??x
Semantic versioning helps by clearly defining what changes are backward incompatible, compatible, or bug fixes through MAJOR.MINOR.PATCH increments. Clients can easily determine if they need to update their code based on the version number.
x??",590,"Catch Breaking Changes Early It’s crucial to make sure we pick up changes that will break consumers as soon as possible, because even if we choose the best possible technology, breaks can still happen...",qwen2.5:latest,2025-11-02 23:45:03,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Coexist Different Endpoints,Coexist Different Endpoints,"#### Coexist Different Endpoints
Background context: Coexisting different versions of endpoints allows for gradual migration without forcing all consumers to upgrade simultaneously. This approach is useful when breaking changes are necessary.

:p How can coexistence of different endpoint versions be managed?
??x
By deploying a new service that supports both the old and new interfaces, allowing time for consumers to migrate gradually. Once all consumers have migrated, the old endpoint can be removed.
x??",508,"Catch Breaking Changes Early It’s crucial to make sure we pick up changes that will break consumers as soon as possible, because even if we choose the best possible technology, breaks can still happen...",qwen2.5:latest,2025-11-02 23:45:03,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Coexist Different Endpoints,Expand and Contract Pattern,"#### Expand and Contract Pattern
Background context: The expand and contract pattern is used when coexisting multiple versions of an API. It involves initially supporting both the old and new versions, then eventually retiring the old version.

:p How does the expand and contract pattern work in practice?
??x
Initially, support both old (e.g., V1) and new (e.g., V2 or V3) interfaces. Gradually phase out the old interface as consumers start using the new one until it can be completely removed.
x??",501,"Catch Breaking Changes Early It’s crucial to make sure we pick up changes that will break consumers as soon as possible, because even if we choose the best possible technology, breaks can still happen...",qwen2.5:latest,2025-11-02 23:45:03,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Coexist Different Endpoints,URI Versioning vs Request Header Versioning,"#### URI Versioning vs Request Header Versioning
Background context: Different methods for routing requests to different API versions include using version numbers in URIs and headers. Both approaches have their pros and cons.

:p How can versioning be implemented via URI?
??x
Using version numbers in the URI, such as `/v1/customer/` or `/v2/customer/`, makes it clear which version is being requested but may allow clients to hardcode templates.
x??",452,"Catch Breaking Changes Early It’s crucial to make sure we pick up changes that will break consumers as soon as possible, because even if we choose the best possible technology, breaks can still happen...",qwen2.5:latest,2025-11-02 23:45:03,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Coexist Different Endpoints,Protocol Buffer Versioning,"#### Protocol Buffer Versioning
Background context: In RPC systems, different versions of methods can be managed by placing them in different namespaces. This approach becomes complex when dealing with the same types across versions.

:p How can versioning be handled using protocol buffers?
??x
Using namespace prefixes like `v1.createCustomer` and `v2.createCustomer` to differentiate between method versions. However, this complexity increases when managing different types sent over the network.
x??

---",508,"Catch Breaking Changes Early It’s crucial to make sure we pick up changes that will break consumers as soon as possible, because even if we choose the best possible technology, breaks can still happen...",qwen2.5:latest,2025-11-02 23:45:03,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Use Multiple Concurrent Service Versions,Multiple Concurrent Service Versions,"#### Multiple Concurrent Service Versions

Background context: This versioning approach involves running different versions of a microservice simultaneously. It is particularly useful when there are legacy consumers that cannot be immediately updated to the latest service version, due to high costs or constraints with older devices and systems.

:p What are the main reasons for using multiple concurrent service versions?
??x
The main reasons include:
- Supporting legacy consumers that can't be quickly upgraded.
- Handling rare cases where it's too costly to change old consumer systems.
- Ensuring backward compatibility while allowing newer features in updated services. 

This approach is used sparingly by Netflix, especially when dealing with older or embedded devices that are tied to specific API versions.",818,"Use Multiple Concurrent Service Versions Another versioning solution often cited is to have different versions of the service live at once, and for older consumers to route their traffic to the older ...",qwen2.5:latest,2025-11-02 23:45:13,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Use Multiple Concurrent Service Versions,Codebase Branching for Multiple Versions,"#### Codebase Branching for Multiple Versions

Background context: When implementing multiple concurrent service versions, it often requires branching the codebase to maintain two different sets of services. This can lead to complexity and issues related to merging changes between branches.

:p What is a significant challenge when using codebase branching for maintaining multiple service versions?
??x
A significant challenge is that branching the codebase for each service version means having to manage two distinct sets of code, which complicates the development process and increases the risk of integration issues. It also requires careful management of feature branches and merges.

```java
public class ExampleBranching {
    private String branchVersion;

    public ExampleBranching(String branch) {
        this.branchVersion = branch;
    }

    // Code to handle logic differently based on branch version
}
```
x??",929,"Use Multiple Concurrent Service Versions Another versioning solution often cited is to have different versions of the service live at once, and for older consumers to route their traffic to the older ...",qwen2.5:latest,2025-11-02 23:45:13,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Use Multiple Concurrent Service Versions,Middleware for Routing,"#### Middleware for Routing

Background context: In the multiple concurrent service versions approach, smarts are required to direct consumers to the correct microservice version. This often involves implementing routing logic in middleware or a bunch of `nginx` scripts.

:p What is a potential downside of implementing routing logic using middleware?
??x
A potential downside is that routing logic can become complex and hard to reason about. The behavior for directing traffic to specific service versions might end up scattered across multiple files or systems, leading to increased difficulty in understanding how the system handles different consumer versions.

```java
public class ServiceRouter {
    public void routeRequest(String consumerVersion) {
        if (consumerVersion.equals(""oldVersion"")) {
            // Route to old version of the service
        } else {
            // Route to new version of the service
        }
    }
}
```
x??",956,"Use Multiple Concurrent Service Versions Another versioning solution often cited is to have different versions of the service live at once, and for older consumers to route their traffic to the older ...",qwen2.5:latest,2025-11-02 23:45:13,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Use Multiple Concurrent Service Versions,Persistent State Management,"#### Persistent State Management

Background context: When managing multiple concurrent service versions, it is crucial to ensure that data created by one version of a service can be accessed and used by all other versions. This involves handling persistent state in a way that is consistent across different service versions.

:p What are the challenges related to managing persistent state with coexisting service versions?
??x
Challenges include:
- Ensuring that customers created by either version of the service can be stored and made visible to all services, regardless of which version was used to create them.
- Managing data consistency when different versions of a microservice need access to the same persistent state.

These challenges can introduce additional complexity in designing and maintaining the system.

x??",829,"Use Multiple Concurrent Service Versions Another versioning solution often cited is to have different versions of the service live at once, and for older consumers to route their traffic to the older ...",qwen2.5:latest,2025-11-02 23:45:13,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Use Multiple Concurrent Service Versions,Blue/Green Deployments,"#### Blue/Green Deployments

Background context: Coexisting multiple service versions is particularly useful during blue/green deployments or canary releases. These deployment strategies allow for gradual rollouts, minimizing risks by keeping old and new services running concurrently for a short period.

:p How does a blue/green deployment work?
??x
In a blue/green deployment:
- Two identical environments (blue and green) run in parallel.
- Initially, all traffic is directed to the blue environment.
- When ready, the blue environment is shut down, and traffic is redirected to the green one.

This process allows for seamless switching between versions without downtime.

```java
public class BlueGreenDeployer {
    public void deployNewVersion(String newEnv) {
        if (newEnv.equals(""green"")) {
            // Switch traffic from blue to green
        } else {
            // Maintain current setup
        }
    }
}
```
x??

---",941,"Use Multiple Concurrent Service Versions Another versioning solution often cited is to have different versions of the service live at once, and for older consumers to route their traffic to the older ...",qwen2.5:latest,2025-11-02 23:45:13,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,UI Fragment Composition,User Interface Evolution,"#### User Interface Evolution
Background context: The text discusses how user interfaces have evolved over time, starting from desktop applications using Swing and Motif to web-based interfaces with JavaScript for dynamic behavior. It then moves towards a more holistic digital approach where various constraints need to be considered.

:p What are the key stages of evolution in user interface design mentioned in the text?
??x
The key stages of evolution in user interface design include:
1. Early desktop applications using technologies like Swing and Motif.
2. Web-based interfaces where most logic is on the server side, rendering full pages to clients.
3. Introduction of JavaScript for more dynamic client-side interactions.
4. Current focus on digital holistically, considering various devices and constraints.

x??",823,"User Interfaces So far, we haven’t really touched on the world of the user interface. A few of you out there might just be providing a cold, hard, clinical API to your customers, but many of us find o...",qwen2.5:latest,2025-11-02 23:45:24,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,UI Fragment Composition,Thin vs Fat UIs,"#### Thin vs Fat UIs
Background context: The text contrasts thin and fat user interfaces, explaining that in early web applications, the server rendered the entire page with minimal interaction from the browser. With time, more dynamic behavior has been added to client-side interfaces using JavaScript.

:p How did the nature of user interfaces change from their initial stage to modern web applications?
??x
Initially, user interfaces were ""thin"" where most logic resided on the server side and rendered full pages to clients. Modern web applications have become ""fat,"" integrating more dynamic behavior with JavaScript running in the browser.

Example code showing a simple fat UI using JavaScript:
```javascript
// Pseudocode for a simple interactive form that updates dynamically
function updateForm() {
    let input = document.getElementById('inputField').value;
    document.getElementById('outputField').innerText = 'You entered: ' + input;
}
```

x??",960,"User Interfaces So far, we haven’t really touched on the world of the user interface. A few of you out there might just be providing a cold, hard, clinical API to your customers, but many of us find o...",qwen2.5:latest,2025-11-02 23:45:24,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,UI Fragment Composition,Desktop vs Web Applications,"#### Desktop vs Web Applications
Background context: The text contrasts desktop applications, which were initially more ""fat"" with local file manipulation and server-side components, to web applications that started as ""thin,"" where the server rendered full pages.

:p How did the nature of user interfaces change from traditional desktop applications to modern web applications?
??x
Desktop applications were traditionally ""fat,"" characterized by rich client-side experiences involving local file manipulation and extensive use of technologies like Swing. Web applications initially emerged as ""thin,"" with most logic on the server rendering full pages to clients. Over time, these evolved into more dynamic and interactive experiences using JavaScript.

Example code showing a simple form submission in an early web application:
```html
<!-- Early web application HTML -->
<form action=""/submit"" method=""post"">
    <input type=""text"" name=""message"">
    <button type=""submit"">Submit</button>
</form>
```

x??",1010,"User Interfaces So far, we haven’t really touched on the world of the user interface. A few of you out there might just be providing a cold, hard, clinical API to your customers, but many of us find o...",qwen2.5:latest,2025-11-02 23:45:24,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,UI Fragment Composition,Holistic Digital Approach,"#### Holistic Digital Approach
Background context: The text discusses the shift towards a more holistic digital approach where different interfaces (desktop, mobile, wearable, physical) are considered to deliver services.

:p What is the current trend in designing user interfaces according to the text?
??x
The current trend in designing user interfaces involves adopting a more holistic and integrated approach. Instead of treating web or mobile applications separately, organizations aim to provide seamless experiences across various devices like desktops, mobile devices, wearables, and physical stores.

Example code showing how microservices can be used to curate different experiences:
```java
// Pseudocode for a service that adapts based on the user's device type
public class UserInterfaceService {
    public String getUIExperience(String deviceType) {
        if (""mobile"".equals(deviceType)) {
            return ""Mobile UI"";
        } else if (""web"".equals(deviceType)) {
            return ""Web UI"";
        } else {
            return ""Desktop UI"";
        }
    }
}
```

x??",1092,"User Interfaces So far, we haven’t really touched on the world of the user interface. A few of you out there might just be providing a cold, hard, clinical API to your customers, but many of us find o...",qwen2.5:latest,2025-11-02 23:45:24,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,UI Fragment Composition,Constraints in User Interface Design,"#### Constraints in User Interface Design
Background context: The text highlights the various constraints that need to be considered when designing user interfaces, including browser compatibility, screen resolution, and mobile-specific limitations such as network bandwidth and battery life.

:p What are some of the key constraints mentioned for modern web applications?
??x
Key constraints for modern web applications include:
- Browser compatibility and user agent variations.
- Screen resolution and aspect ratios.
- Network conditions and bandwidth limitations, especially on mobile devices.
- Battery management concerns that can affect user experience due to interactions draining battery life.

Example code showing a simple check for screen size in JavaScript:
```javascript
// Pseudocode for checking the screen size and adapting UI
function adaptUI() {
    if (window.innerWidth > 768) {
        // Large screen - apply desktop styles
    } else {
        // Small screen - apply mobile or tablet styles
    }
}
```

x??

---",1037,"User Interfaces So far, we haven’t really touched on the world of the user interface. A few of you out there might just be providing a cold, hard, clinical API to your customers, but many of us find o...",qwen2.5:latest,2025-11-02 23:45:24,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,UI Fragment Composition,Tablet Interaction Constraints,"#### Tablet Interaction Constraints
Background context: The nature of interactions changes significantly when using different devices like tablets. Traditional mouse-based interactions such as right-clicking are not straightforward on touchscreens, requiring adaptation for better user experience.

:p How do tablet interaction constraints differ from traditional desktop interactions?
??x
Tablet interactions are primarily based on touch and gestures rather than a mouse or keyboard. This means that common actions like right-clicks need to be emulated through long-presses or other multi-finger gestures. Designing for tablets involves considering how users will interact with the screen, including navigation, selection, and input methods.

```java
// Example pseudocode for handling tablet interactions
public void handleTabletInteraction(LongPressGesture gesture) {
    if (gesture.isRightClickEmulation()) {
        // Handle right-click emulation
    } else if (gesture.isSwipe()) {
        // Handle swipe gestures
    }
}
```
x??",1038,"The nature of interactions changes, too. I can’t easily right-click on a tablet. On a mobile phone, I may want to design my interface to be used mostly one-handed, with most operations being controlle...",qwen2.5:latest,2025-11-02 23:45:36,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,UI Fragment Composition,Mobile Phone Interaction Design,"#### Mobile Phone Interaction Design
Background context: On mobile phones, the design needs to cater to one-handed use and primarily thumb control. This involves optimizing the layout for single-hand operation, focusing on navigation and interaction with a limited set of controls.

:p How does designing for mobile phones differ from other devices?
??x
Designing for mobile phones requires considering the ergonomics of single-handed use. Key interactions should be accessible using only one hand, particularly with the thumb. This includes optimizing button placements, minimizing the number of steps required to complete tasks, and ensuring that critical actions can be performed without requiring extensive navigation.

```java
// Example pseudocode for mobile phone interface layout
public void designForMobileThumbControl() {
    // Place frequently used buttons within easy reach of the thumb
    Button拇指Button = new Button();
    ThumbPositionLayout.add(m指姆Button);
    
    // Ensure all critical actions are easily accessible with one thumb swipe or tap
}
```
x??",1074,"The nature of interactions changes, too. I can’t easily right-click on a tablet. On a mobile phone, I may want to design my interface to be used mostly one-handed, with most operations being controlle...",qwen2.5:latest,2025-11-02 23:45:36,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,UI Fragment Composition,SMS Interface for Low-Bandwidth Areas,"#### SMS Interface for Low-Bandwidth Areas
Background context: In regions where bandwidth is limited, using SMS as an interface can be a viable alternative. This method ensures that services remain accessible even when internet connectivity is poor.

:p How does SMS-based interaction benefit in low-bandwidth areas?
??x
SMS-based interactions are beneficial in low-bandwidth areas because they do not rely on high-speed data connections. Services can provide information and functionality via text messages, making them more reliable and cost-effective for users with limited internet access. This approach ensures that critical services remain accessible even during periods of poor network conditions.

```java
// Example pseudocode for SMS interaction
public void sendSMSForService(String message) {
    // Code to send an SMS with the provided message
    send(""1234567890"", message);
    
    // Handle incoming SMS responses
    handleIncomingSMS();
}
```
x??",966,"The nature of interactions changes, too. I can’t easily right-click on a tablet. On a mobile phone, I may want to design my interface to be used mostly one-handed, with most operations being controlle...",qwen2.5:latest,2025-11-02 23:45:36,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,UI Fragment Composition,API Composition for Web and Mobile Interfaces,"#### API Composition for Web and Mobile Interfaces
Background context: When building user interfaces, especially for web and mobile applications, directly interacting with APIs can provide flexibility. This approach allows the UI to query or modify data through HTTP calls, leveraging existing service-to-service communication protocols.

:p What is the main advantage of using API composition in web and mobile interfaces?
??x
The main advantage of using API composition is that it leverages existing service-to-service communication protocols (HTTP/JSON/XML) to build user interfaces dynamically. This approach provides flexibility in how data is presented, allowing for tailored responses based on the device capabilities. It enables easy integration with back-end services and supports a wide range of devices.

```java
// Example pseudocode for API composition
public void retrieveCustomerRecord(String customerId) {
    // Make an HTTP GET request to retrieve customer record
    String url = ""https://api.example.com/customers/"" + customerId;
    HttpClient client = new HttpClient();
    String response = client.executeGetRequest(url);
    
    // Process the response and update UI components accordingly
    processResponse(response);
}
```
x??",1255,"The nature of interactions changes, too. I can’t easily right-click on a tablet. On a mobile phone, I may want to design my interface to be used mostly one-handed, with most operations being controlle...",qwen2.5:latest,2025-11-02 23:45:36,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,UI Fragment Composition,UI Fragment Composition for Services,"#### UI Fragment Composition for Services
Background context: Instead of having the UI make API calls to retrieve all data, services can provide pre-composed UI fragments. These fragments are then combined with other UI elements to create a complete user interface.

:p What is the advantage of using UI fragment composition?
??x
The advantage of using UI fragment composition is that it allows for modular and reusable components. Services can provide specific parts of the UI, which can be easily integrated into larger interfaces. This approach reduces redundancy in code and ensures that each service focuses on its core functionality while still contributing to the overall user experience.

```java
// Example pseudocode for UI fragment composition
public void createUserInterface() {
    // Retrieve a recommendation widget from the recommendation service
    Widget recommendationWidget = getRecommendationService().getRecommendationWidget();
    
    // Combine this with other UI elements to form the complete interface
    InterfaceBuilder.add(recommendationWidget);
    InterfaceBuilder.add(otherUIElements);
}
```
x??
---",1134,"The nature of interactions changes, too. I can’t easily right-click on a tablet. On a mobile phone, I may want to design my interface to be used mostly one-handed, with most operations being controlle...",qwen2.5:latest,2025-11-02 23:45:36,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,UI Fragment Composition,Coarse-Grained UI Fragments,"#### Coarse-Grained UI Fragments
Background context: This approach involves serving up coarser-grained parts of a user interface (UI) from server-side applications. These fragments are assembled to form a complete application or website, often managed by specific teams responsible for certain functionalities. The aim is to allow faster development cycles and easier management of different parts of the UI.
:p What is the main advantage of serving up coarse-grained UI fragments?
??x
The primary advantage is that the same team that makes changes to the services can also manage those changes in the corresponding UI components, facilitating quicker deployment and updates. This aligns well with team ownership models, where each team handles specific functionalities.
```java
// Example of a simple server-side template for assembling fragments
public class UIAssemblyService {
    public String assembleUI(String fragment) {
        // Logic to fetch and assemble UI fragments
        return ""<html><body>"" + fragment + ""</body></html>"";
    }
}
```
x??",1057,It might get rendered as a box on a web page along with other content. A variation of this approach that can work well is to assemble a series of coarser-grained parts of a UI. So rather than creating...,qwen2.5:latest,2025-11-02 23:45:47,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,UI Fragment Composition,Ensuring Consistent User Experience,"#### Ensuring Consistent User Experience
Background context: When serving up coarser-grained UI components, maintaining a consistent user experience is crucial. Users expect seamless interactions across different parts of the interface. Techniques like living style guides can be employed to share assets and enforce consistency.
:p How do living style guides help in ensuring consistent user experiences?
??x
Living style guides provide a shared repository for HTML components, CSS, and images, which ensures that developers adhere to a unified design language. By centralizing these resources, teams can maintain a cohesive look and feel across different parts of the UI, reducing inconsistencies.
```java
// Example of a living style guide concept in code form
public class LivingStyleGuide {
    public void shareAssets(String componentName) {
        // Logic to fetch and provide assets for a specific component
        System.out.println(""Fetching assets for "" + componentName);
    }
}
```
x??",1001,It might get rendered as a box on a web page along with other content. A variation of this approach that can work well is to assemble a series of coarser-grained parts of a UI. So rather than creating...,qwen2.5:latest,2025-11-02 23:45:47,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,UI Fragment Composition,Hybrid Approach for Native Applications,"#### Hybrid Approach for Native Applications
Background context: While the coarse-grained UI approach works well in web environments, native applications may require a different strategy. A hybrid approach can involve using native apps to serve up HTML components, but this has shown downsides and is not ideal for all scenarios.
:p What are some disadvantages of using a hybrid approach with native applications?
??x
Some disadvantages include performance overhead due to the need to render HTML within native applications, potential security risks, and challenges in maintaining consistent user experiences across different platforms. Additionally, the complexity of handling UI components can increase development time and maintenance efforts.
```java
// Example of a hybrid approach code snippet
public class HybridAppRenderer {
    public void renderHTML(String htmlContent) {
        // Code to render HTML content within a native application context
        System.out.println(""Rendering HTML: "" + htmlContent);
    }
}
```
x??",1034,It might get rendered as a box on a web page along with other content. A variation of this approach that can work well is to assemble a series of coarser-grained parts of a UI. So rather than creating...,qwen2.5:latest,2025-11-02 23:45:47,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,UI Fragment Composition,Responsive Components for Different Devices,"#### Responsive Components for Different Devices
Background context: Even with web-only UIs, different devices may require varying treatments. Using responsive design can help adapt components to fit various screen sizes and orientations, ensuring a good user experience across devices.
:p How does responsive design benefit the development of web applications?
??x
Responsive design ensures that web applications adapt gracefully to different screen sizes and resolutions, providing an optimal user experience on desktops, tablets, and mobile phones. By using CSS media queries and flexible layouts, developers can ensure components scale appropriately, maintaining usability and aesthetics.
```css
/* Example of responsive design CSS */
@media (max-width: 600px) {
    .component {
        font-size: 14px;
    }
}
```
x??",824,It might get rendered as a box on a web page along with other content. A variation of this approach that can work well is to assemble a series of coarser-grained parts of a UI. So rather than creating...,qwen2.5:latest,2025-11-02 23:45:47,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,UI Fragment Composition,Challenges with Cross-Cutting Interactions,"#### Challenges with Cross-Cutting Interactions
Background context: In some cases, the capabilities offered by a service may not fit neatly into widgets or pages. For example, dynamic recommendations that need to be triggered on search queries might require more complex interactions than what coarse-grained fragments can provide.
:p What are scenarios where coarse-grained UI fragments fall short?
??x
Coarse-grained UI fragments may struggle with cross-cutting interactions, such as real-time recommendations or dynamic content updates. For instance, when a user performs a search, immediate feedback like type-ahead suggestions requiring fresh data might not be easily accommodated within the fragment-based approach.
```java
// Example of handling complex interaction
public class SearchService {
    public void handleSearch(String query) {
        // Logic to fetch and display dynamic recommendations
        System.out.println(""Handling search: "" + query);
    }
}
```
x??",981,It might get rendered as a box on a web page along with other content. A variation of this approach that can work well is to assemble a series of coarser-grained parts of a UI. So rather than creating...,qwen2.5:latest,2025-11-02 23:45:47,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Backends for Frontends,Chatty Interfaces and Backend Services,"#### Chatty Interfaces and Backend Services
Chatty interfaces refer to user interfaces that make many requests to backend services, which can lead to inefficiencies and performance issues. To address this problem, a common solution is to use an API gateway or server-side aggregation endpoint that handles multiple backend calls and aggregates content as needed for different devices.
:p What is the purpose of using an API gateway or server-side aggregation endpoint?
??x
The purpose is to centralize requests from frontend applications to backend services. This reduces the number of direct connections, improves performance by caching results, and allows for easier management of content variations across different devices.
```java
// Example of a simplified API Gateway handling two backend calls
public class ApiGateway {
    private final BackendService service1;
    private final BackendService service2;

    public ApiResponse handleRequest(Request request) {
        // Logic to aggregate responses from multiple services
        ApiResponse response1 = service1.handleRequest(request);
        ApiResponse response2 = service2.handleRequest(request);
        
        // Combine responses as needed for different devices
        return combineResponses(response1, response2);
    }
}
```
x??",1304,"Backends for Frontends A common solution to the problem of chatty interfaces with backend services, or the need to vary content for different types of devices, is to have a server-side aggregation end...",qwen2.5:latest,2025-11-02 23:45:56,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Backends for Frontends,Monolithic Gateway vs. Dedicated Backends for Frontend (BFFs),"#### Monolithic Gateway vs. Dedicated Backends for Frontend (BFFs)
A monolithic gateway handles calls to/from multiple user interfaces and services in one giant layer, which can lead to a loss of isolation between different UI components and limit the ability to release them independently.
:p What are the disadvantages of using a single monolithic gateway?
??x
The main disadvantages include:
- Loss of isolation: Different UIs may need different logic or data, but a shared gateway might not support this effectively.
- Complexity: As more services and UIs get added, the monolithic gateway can become complex and harder to maintain.
- Dependency issues: Changes in one part of the system can impact others if they share a single gateway.

A better approach is to use dedicated backends for frontend (BFFs), where each backend handles requests specific to a particular user interface. This allows teams focusing on different UIs to manage their own server-side components independently.
```java
// Example of BFF pattern with one backend per UI
public class HomeUIBackend {
    private final UserService userService;
    
    public ApiResponse handleHomeRequest(Request request) {
        // Specific logic for handling home UI requests
        return userService.handleHomeRequest(request);
    }
}
```
x??",1311,"Backends for Frontends A common solution to the problem of chatty interfaces with backend services, or the need to vary content for different types of devices, is to have a server-side aggregation end...",qwen2.5:latest,2025-11-02 23:45:56,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Backends for Frontends,Business Logic and API Authentication,"#### Business Logic and API Authentication
When using BFFs, it's important to keep business logic specific to the services themselves and avoid moving this logic into the BFF layer. Additionally, an API authentication and authorization layer can sit between the BFF and UI to manage security concerns.
:p Why should business logic stay in the services themselves instead of being moved to BFFs?
??x
Business logic should remain in the services because:
- Services are responsible for their own data and operations; moving this logic into BFFs can lead to redundancy and inconsistencies.
- It promotes better separation of concerns, making it easier to maintain and scale individual components.
- Security and business rules should be defined where they make the most sense within the application architecture.

For example, if a service handles user authentication, this logic should reside in that service rather than being replicated in multiple BFFs.
```java
// Example of keeping business logic in services
public class UserService {
    public ApiResponse authenticateUser(Request request) {
        // Authentication logic
        return new ApiResponse(true); // Simplified example
    }
}
```
x??

---",1209,"Backends for Frontends A common solution to the problem of chatty interfaces with backend services, or the need to vary content for different types of devices, is to have a server-side aggregation end...",qwen2.5:latest,2025-11-02 23:45:56,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,On Your Own Terms,Fragment-Based Assembly for Websites vs. Backend-for-Frontends for Mobile Applications,"#### Fragment-Based Assembly for Websites vs. Backend-for-Frontends for Mobile Applications
Organizations can adopt different approaches to building their applications based on specific needs and use cases, rather than using a one-size-fits-all solution. For instance, they might use fragment-based assembly for websites while opting for backend-for-frontends (BFF) for mobile applications.

:p How does the choice between fragment-based assembly and BFF differ in web development compared to mobile app development?
??x
The decision is driven by considerations such as performance, maintenance, and integration complexity. Fragment-based assembly allows for modular components that can be assembled differently depending on the context (e.g., desktop vs. mobile). BFFs, on the other hand, are specialized backends tailored specifically for front-end consumption, which can enhance performance in mobile apps.

In detail, fragment-based assembly works by breaking down the web application into reusable, self-contained pieces of UI and logic that can be dynamically assembled based on user interactions or device capabilities. This approach promotes reusability and flexibility across different platforms.

BFFs, however, are designed to serve a specific client (like a mobile app) and can optimize data retrieval, transformations, and presentation tailored for the mobile environment, reducing network latency and improving performance.
??x
```java
// Example of BFF setup in Java
public class MobileService {
    public Map<String, Object> getMobileData() {
        // Logic to fetch and transform data specifically for mobile app
        return transformedData;
    }
}
```
x??",1680,"A Hybrid Approach Many of the aforementioned options don’t need to be one-size-fits-all. I could see an organization adopting the approach of fragment-based assembly to create a website, but using a b...",qwen2.5:latest,2025-11-02 23:46:12,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,On Your Own Terms,Integrating with Third-Party Software (COTS or SaaS),"#### Integrating with Third-Party Software (COTS or SaaS)
Organizations often need to integrate with third-party commercial off-the-shelf (COTS) software or software as a service (SaaS). The decision on whether to build custom solutions or purchase existing products depends on the uniqueness and strategic importance of those tools.

:p Why do organizations typically prefer buying COTS or SaaS over building custom solutions?
??x
Organizations often opt for commercial off-the-shelf software because it allows them to meet their needs more efficiently without reinventing the wheel. Key reasons include:

1. **Cost-effectiveness**: Building a custom solution can be prohibitively expensive compared to acquiring and integrating existing products.
2. **Time-to-market**: Custom development takes time, whereas buying an existing product often reduces this period significantly.
3. **Risk management**: Leveraging tried-and-tested solutions reduces the risk associated with developing in-house systems.

However, organizations should carefully evaluate whether a product is truly strategic or can be customized to fit their unique requirements before making a decision.
??x
```java
// Example of integrating with a third-party SaaS API in Java
public class ExternalServiceClient {
    private String apiKey;
    
    public ExternalServiceClient(String apiKey) {
        this.apiKey = apiKey;
    }
    
    public ResponseData fetchData() {
        // Logic to fetch data from the external service using the API key
        return response;
    }
}
```
x??",1557,"A Hybrid Approach Many of the aforementioned options don’t need to be one-size-fits-all. I could see an organization adopting the approach of fragment-based assembly to create a website, but using a b...",qwen2.5:latest,2025-11-02 23:46:12,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,On Your Own Terms,Lack of Control When Integrating with COTS Products,"#### Lack of Control When Integrating with COTS Products
When integrating with commercial off-the-shelf products, organizations often face limitations in terms of control over technical decisions and customization options. This can lead to challenges such as dependency on proprietary protocols or environments.

:p What are the main challenges when integrating with third-party software from a technical standpoint?
??x
The primary challenges include:

1. **Vendor Lock-In**: Being tied to a vendor’s technology stack, which may not align with internal development practices.
2. **Limited Customization Options**: Proprietary systems often restrict how and where customizations can be made.
3. **Integration Complexity**: Dealing with various communication protocols (e.g., SOAP, REST) or direct database access, leading to tightly coupled systems.
4. **Maintenance Overhead**: Managing updates, compatibility issues, and support for third-party products.

For example, integrating a CMS that does not support continuous integration can lead to significant maintenance overhead as any upgrade might break custom features.
??x
```java
// Example of handling different protocols in Java
public class IntegrationManager {
    public void integrateServices(List<Service> services) throws IOException {
        for (Service service : services) {
            if (""SOAP"".equals(service.getProtocol())) {
                // Handle SOAP integration logic
            } else if (""REST"".equals(service.getProtocol())) {
                // Handle REST integration logic
            }
        }
    }
}
```
x??",1598,"A Hybrid Approach Many of the aforementioned options don’t need to be one-size-fits-all. I could see an organization adopting the approach of fragment-based assembly to create a website, but using a b...",qwen2.5:latest,2025-11-02 23:46:12,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,On Your Own Terms,Customization Challenges with COTS Products,"#### Customization Challenges with COTS Products
Even when a third-party tool is customizable, the cost and effort required for customization can be high. Organizations must consider whether the benefits of customizing outweigh the costs.

:p When should organizations opt against complex customization of third-party tools?
??x
Organizations should avoid complex customization when:

1. **The Tool’s Core Capabilities Aren’t Unique**: If the tool provides capabilities that are broadly applicable and not tailored to specific business needs.
2. **Cost vs. Benefit Analysis**: Customization might be more expensive than developing a custom solution from scratch, especially if it doesn’t add significant value.
3. **Maintenance Overhead**: Continuous maintenance of customizations can become a burden.

For instance, using an off-the-shelf CMS that lacks modern features like continuous integration and robust APIs might make the effort to customize it not worth the cost, especially for non-strategic applications.
??x
```java
// Example of assessing customization feasibility in Java
public class CustomizationAssessment {
    public boolean isCustomizationFeasible(Product product) {
        // Logic to evaluate whether customizing the product makes sense based on criteria such as uniqueness and strategic importance
        return feasible;
    }
}
```
x??",1362,"A Hybrid Approach Many of the aforementioned options don’t need to be one-size-fits-all. I could see an organization adopting the approach of fragment-based assembly to create a website, but using a b...",qwen2.5:latest,2025-11-02 23:46:12,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,On Your Own Terms,Integration Spaghetti: The Challenge of Multiple Communication Protocols,"#### Integration Spaghetti: The Challenge of Multiple Communication Protocols
The integration complexity can increase significantly when multiple systems use different communication protocols, leading to what is known as ""integration spaghetti.""

:p How does the use of multiple communication protocols (e.g., SOAP, REST) complicate integration?
??x
Using various communication protocols complicates integration due to:

1. **Protocol Differences**: Each protocol has its own set of rules and formats, making it difficult to standardize on a single approach.
2. **Tight Coupling**: Direct access to underlying data stores or proprietary APIs can lead to tightly coupled systems that are hard to maintain and scale.
3. **Maintenance Overhead**: Managing different protocols requires additional development effort and increases the overall complexity of the system.

For example, integrating a SOAP-based service with an XML-RPC service introduces challenges in handling different message formats and ensuring compatibility.
??x
```java
// Example of handling multiple communication protocols in Java
public class ProtocolAdapter {
    public void handleRequest(String protocol, String request) throws IOException {
        if (""SOAP"".equals(protocol)) {
            // Handle SOAP request logic
        } else if (""XML-RPC"".equals(protocol)) {
            // Handle XML-RPC request logic
        }
    }
}
```
x??

---",1417,"A Hybrid Approach Many of the aforementioned options don’t need to be one-size-fits-all. I could see an organization adopting the approach of fragment-based assembly to create a website, but using a b...",qwen2.5:latest,2025-11-02 23:46:12,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Example The multirole CRM system,CMS as a Service,"#### CMS as a Service
Background context: Customizing or integrating Content Management Systems (CMS) is often necessary for enterprise organizations to display dynamic content. However, many commercial CMSes are not ideal platforms for custom coding due to their limitations in page layout and template systems.

:p How can you customize the functionality of a CMS while maintaining control over your own technology stack?
??x
By fronting the CMS with your own service that handles the website's presentation and integration with other services. This approach allows you to use the CMS for content creation and retrieval while writing custom code in your own environment.

To illustrate, let’s consider an example where a static site needs dynamic content such as customer records or product offerings:

```java
public class WebsiteFacade {
    private final CMS cms;

    public WebsiteFacade(CMS cms) {
        this.cms = cms;
    }

    public String getDynamicContent(String contentId) {
        // Retrieve and format the dynamic content from the CMS
        return ""Dynamic Content: "" + cms.getContent(contentId);
    }
}
```

Here, `WebsiteFacade` acts as a service that interacts with the underlying CMS to provide dynamic content to the external world. This separation allows you to manage your custom logic independently of the CMS.

x??",1348,"On Your Own Terms COTS and SAAS products absolutely have their place, and it isn’t feasible (or sensible) for most of us to build everything from scratch. So how do we resolve these challenges? The ke...",qwen2.5:latest,2025-11-02 23:46:20,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Example The multirole CRM system,Multirole CRM System,"#### Multirole CRM System
Background context: Customer Relationship Management (CRM) tools can become single points of failure and tangled dependencies, especially when they are used extensively in an organization. These tools often lack flexibility and control over their API interfaces, making them challenging to integrate with other systems.

:p How can you decouple the critical functionality of a CRM system while maintaining domain integrity?
??x
By creating façade services that model your business domain concepts, thereby abstracting away the complexities of the CRM tool. This approach helps in separating concerns and providing cleaner integration points for both internal and external systems.

For instance, if a CRM tool is used to manage projects but multiple other systems require project data:

```java
public class ProjectService {
    private final CRM crm;

    public ProjectService(CRM crm) {
        this.crm = crm;
    }

    public Project getProjectById(String projectId) {
        // Retrieve and format the project details from the CRM
        return new Project(crm.getProjectDetails(projectId));
    }
}
```

Here, `ProjectService` acts as a facade that interacts with the CRM to retrieve and present project information in a structured manner. This separation ensures that changes or issues in the CRM do not affect other parts of your system.

x??

---",1385,"On Your Own Terms COTS and SAAS products absolutely have their place, and it isn’t feasible (or sensible) for most of us to build everything from scratch. So how do we resolve these challenges? The ke...",qwen2.5:latest,2025-11-02 23:46:20,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Its All About Seams,Strangler Application Pattern,"#### Strangler Application Pattern
The Strangler Application Pattern is a useful technique when dealing with legacy systems or third-party commercial off-the-shelf (COTS) platforms that are not fully under your control. The goal is to gradually replace the existing system without causing significant disruptions. This pattern involves capturing and intercepting calls to the old system, allowing you to decide whether these calls should be directed to existing legacy code or new code you have written.
:p What is the Strangler Application Pattern used for?
??x
The Strangler Application Pattern is used to replace legacy or third-party systems by gradually replacing them with new functionality while maintaining compatibility. It allows you to introduce new services and phase out old ones incrementally, ensuring that critical operations can continue uninterrupted during the transition.
x??",895,"The Strangler Pattern When it comes to legacy or even COTS platforms that aren’t totally under our control, we also have to deal with what happens when we want to remove them or at least move away fro...",qwen2.5:latest,2025-11-02 23:46:29,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Its All About Seams,Microservices Interception,"#### Microservices Interception
In microservice architectures, when dealing with existing legacy systems, it is often more practical to use a series of microservices instead of a single monolithic application to intercept calls. This approach allows for finer control over which parts of the legacy system are being replaced and can be managed more efficiently.
:p How do you handle interception in a microservice architecture?
??x
In a microservice architecture, interception is handled by using multiple microservices to capture and redirect original calls from the legacy system. These microservices act as intermediaries between the external requests and the old system or new code. This can be achieved through the use of proxies that forward requests to either the legacy system or the new microservices based on predefined rules.
x??",840,"The Strangler Pattern When it comes to legacy or even COTS platforms that aren’t totally under our control, we also have to deal with what happens when we want to remove them or at least move away fro...",qwen2.5:latest,2025-11-02 23:46:29,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Its All About Seams,Proxy Usage in Interception,"#### Proxy Usage in Interception
When dealing with complex interception scenarios, especially when using microservices, a proxy may be necessary to capture and redirect original calls. Proxies act as intermediaries, enabling the gradual replacement of old functionality without disrupting existing operations.
:p What role does a proxy play in interception?
??x
A proxy plays a crucial role in interception by acting as an intermediary between external requests and either the legacy system or new microservices. It captures incoming calls and decides whether to route them to the legacy system or the newer services based on predefined conditions. This allows for a smooth transition without requiring all clients to be immediately updated.
x??",745,"The Strangler Pattern When it comes to legacy or even COTS platforms that aren’t totally under our control, we also have to deal with what happens when we want to remove them or at least move away fro...",qwen2.5:latest,2025-11-02 23:46:29,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Its All About Seams,Monolithic Application Decomposition,"#### Monolithic Application Decomposition
Monolithic applications often grow large and complex over time, making it difficult to manage and evolve their design. The Strangler Application Pattern can also be applied to decompose these monoliths into smaller, more manageable microservices. This approach helps in gradually refactoring the application without causing significant disruption.
:p How do you decompose a monolithic application?
??x
Decomposing a monolithic application involves using the Strangler Application Pattern to capture and redirect calls from the old system to new microservices or legacy code as needed. By carefully planning which parts of the monolith are replaced first, you can gradually break down the application into smaller, more manageable services that are easier to maintain and evolve.
x??",824,"The Strangler Pattern When it comes to legacy or even COTS platforms that aren’t totally under our control, we also have to deal with what happens when we want to remove them or at least move away fro...",qwen2.5:latest,2025-11-02 23:46:29,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Its All About Seams,Legacy System Decomposition,"#### Legacy System Decomposition
When working with older systems, the Strangler Application Pattern can also be applied to decompose these legacy applications. This involves capturing and redirecting calls from the old system to new microservices or legacy code as appropriate, allowing for a gradual transition without disrupting existing operations.
:p How does the Strangler Application Pattern apply to legacy systems?
??x
The Strangler Application Pattern applies to legacy systems by allowing you to capture and redirect calls to the old system. This enables you to replace functionality gradually while ensuring that critical operations continue uninterrupted. It involves using microservices or proxies to manage these redirections, making it easier to phase out older components over time.
x??

---",807,"The Strangler Pattern When it comes to legacy or even COTS platforms that aren’t totally under our control, we also have to deal with what happens when we want to remove them or at least move away fro...",qwen2.5:latest,2025-11-02 23:46:29,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Breaking Apart MusicCorp,Importance of Cohesion and Coupling in Services,"---
#### Importance of Cohesion and Coupling in Services
Background context explaining that monolithic services often lack cohesion and tight coupling, making them hard to maintain and evolve. Michael Feathers’ concept of a seam is introduced as a way to address this issue by isolating parts of code for easier modification.
:p What is the problem with monoliths in terms of cohesion and coupling?
??x
The main issues with monolithic services are that they tend to be poorly cohesive (mixed with unrelated code) and tightly coupled, where changes in one part can impact many other parts. This makes it difficult to make changes without significant ripple effects.
```java
// Example of a monolithic service function that mixes different functionalities
public class MonolithicService {
    public void processOrderAndRecommend() {
        // Code for processing order
        // Code for generating recommendations
    }
}
```
x??",931,It’s All About Seams We discussed in  Chapter 3  that we want our services to be highly cohesive and loosely coupled. The problem with the monolith is that all too often it is the opposite of both. Ra...,qwen2.5:latest,2025-11-02 23:46:39,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Breaking Apart MusicCorp,Concept of Seams in Software Development,"#### Concept of Seams in Software Development
Background context explaining Michael Feathers’ definition of a seam, which is a portion of code that can be isolated and modified without impacting the rest of the system.
:p What is a seam in software development?
??x
A seam is a part of the codebase that can be treated independently. It allows developers to modify or replace functionality with minimal impact on other parts of the system, enabling easier maintenance and evolution.
```java
// Example of refactoring a method into a seam
public class Service {
    private void processCatalogData() { ... }
    
    public void processAllData() {
        // Logic that now calls processCatalogData as a separate entity
    }
}
```
x??",734,It’s All About Seams We discussed in  Chapter 3  that we want our services to be highly cohesive and loosely coupled. The problem with the monolith is that all too often it is the opposite of both. Ra...,qwen2.5:latest,2025-11-02 23:46:39,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Breaking Apart MusicCorp,Identifying Bounded Contexts for Seams,"#### Identifying Bounded Contexts for Seams
Background context explaining how bounded contexts are crucial in identifying seams, representing cohesive and loosely coupled boundaries within an organization.
:p How do bounded contexts help identify seams?
??x
Bounded contexts represent specific areas of functionality that can be isolated. They help us identify seams because they define clear boundaries where changes in one area won't necessarily affect another. This makes it easier to refactor the codebase into smaller, more manageable services.
```java
// Example of identifying a bounded context (Catalog)
public class CatalogService {
    private void manageMetadata() { ... }
    
    public void serveItems() {
        // Logic for managing metadata and serving items
    }
}
```
x??",792,It’s All About Seams We discussed in  Chapter 3  that we want our services to be highly cohesive and loosely coupled. The problem with the monolith is that all too often it is the opposite of both. Ra...,qwen2.5:latest,2025-11-02 23:46:39,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Breaking Apart MusicCorp,Breaking Apart the Monolithic Backend of MusicCorp,"#### Breaking Apart the Monolithic Backend of MusicCorp
Background context explaining how to identify and separate bounded contexts in a large monolithic backend service. The example uses MusicCorp’s services as a case study.
:p How should we approach breaking apart a large monolithic backend service?
??x
Start by identifying high-level bounded contexts, such as Catalog, Finance Reporting, Warehouse Dispatching, and Recommendation. Then create packages for each context and move the relevant code into these new packages to isolate them from one another.
```java
// Example of creating package structures in Java
package com.musiccorp.catalog;

public class CatalogService {
    private void manageMetadata() { ... }
}

package com.musiccorp.finance;

public class FinanceService {
    private void handlePayments() { ... }
}
```
x??",837,It’s All About Seams We discussed in  Chapter 3  that we want our services to be highly cohesive and loosely coupled. The problem with the monolith is that all too often it is the opposite of both. Ra...,qwen2.5:latest,2025-11-02 23:46:39,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Breaking Apart MusicCorp,Analyzing Dependencies and Refactoring Codebases,"#### Analyzing Dependencies and Refactoring Codebases
Background context explaining the importance of analyzing dependencies between packages to ensure accurate refactoring. Tools like Structure 101 are mentioned as aids in this process.
:p How can we use tools to analyze dependencies between packages?
??x
Tools like Structure 101 allow us to visualize the dependencies graphically, making it easier to understand and refactor our codebase. By analyzing these visualizations, we can identify misplaced or unnecessary dependencies, ensuring that each package operates independently as intended.
```java
// Example of using a tool to visualize dependencies
Structure101 tool = new Structure101();
tool.analyzeProjects(""com.musiccorp.*"");
tool.showDependencies();
```
x??

---",775,It’s All About Seams We discussed in  Chapter 3  that we want our services to be highly cohesive and loosely coupled. The problem with the monolith is that all too often it is the opposite of both. Ra...,qwen2.5:latest,2025-11-02 23:46:39,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Getting to Grips with the Problem,Pace of Change,"#### Pace of Change
Background context explaining the concept. If a part of the monolith is changing more frequently, separating it into its own service can help manage those changes more efficiently and independently.
:p Which driver could suggest that we start by splitting out the code related to inventory management?
??x
It suggests starting with the ""Pace of Change"" driver because if there are frequent updates or significant modifications planned for a specific part of the system, like managing inventory, separating this area can allow for more agile and independent development cycles. This separation helps in making changes faster without impacting other parts of the monolith.
x??",694,The Reasons to Split the Monolith Deciding that you’d like a monolithic service or application to be smaller is a good start. But I would strongly advise you to chip away at these systems. An incremen...,qwen2.5:latest,2025-11-02 23:46:49,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Getting to Grips with the Problem,Team Structure,"#### Team Structure
Background context explaining the concept. When teams are geographically dispersed or have specialized roles, it might make sense to separate their respective code bases into distinct services so that each team can take ownership and manage their specific areas more effectively.
:p How does separating the code for a particular region's team (e.g., Hawaii) support better management of change?
??x
Separating the code for the Hawaii team allows them to have full ownership over their specific area, enabling faster and more focused development cycles. This approach can reduce dependency on other teams and streamline communication and coordination within the team.
x??",690,The Reasons to Split the Monolith Deciding that you’d like a monolithic service or application to be smaller is a good start. But I would strongly advise you to chip away at these systems. An incremen...,qwen2.5:latest,2025-11-02 23:46:49,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Getting to Grips with the Problem,Security,"#### Security
Background context explaining the concept. If there is a need to enforce stricter security measures in certain parts of an application, separating those areas into distinct services can enhance protection by isolating sensitive information or operations.
:p Why might it be advantageous to split out finance-related code for improved security?
??x
Splitting out the finance-related code as a separate service allows for enhanced security measures such as more robust monitoring, stronger data encryption during transit and at rest. This isolation helps in reducing the attack surface and ensuring that sensitive financial information is protected more effectively.
x??",682,The Reasons to Split the Monolith Deciding that you’d like a monolithic service or application to be smaller is a good start. But I would strongly advise you to chip away at these systems. An incremen...,qwen2.5:latest,2025-11-02 23:46:49,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Getting to Grips with the Problem,Technology,"#### Technology
Background context explaining the concept. Introducing new technologies or algorithms into specific parts of an application can be facilitated by separating those areas into independent services, which allows for testing and deployment without affecting other components.
:p How could splitting out recommendation code benefit the system?
??x
Splitting out the recommendation code into a separate service enables the team to experiment with new algorithms using tools like Clojure. This separation facilitates the introduction of alternative implementations for testing and evaluation, potentially leading to improved user experiences.
x??",655,The Reasons to Split the Monolith Deciding that you’d like a monolithic service or application to be smaller is a good start. But I would strongly advise you to chip away at these systems. An incremen...,qwen2.5:latest,2025-11-02 23:46:49,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Getting to Grips with the Problem,Tangled Dependencies,"#### Tangled Dependencies
Background context explaining the concept. Understanding and managing dependencies between different parts of an application is crucial when deciding which parts to separate into microservices. The goal is to minimize interdependencies to ensure smoother refactoring and development processes.
:p What approach can help in identifying the least entangled part of a monolith for separation?
??x
Using tools like package modeling or dependency graph visualization (e.g., SchemaSpy) can help identify the parts of the code that are least dependent on others. By focusing on these less entangled areas, it becomes easier to separate them into distinct services with minimal disruption.
x??",711,The Reasons to Split the Monolith Deciding that you’d like a monolithic service or application to be smaller is a good start. But I would strongly advise you to chip away at these systems. An incremen...,qwen2.5:latest,2025-11-02 23:46:49,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Getting to Grips with the Problem,The Database,"#### The Database
Background context explaining the concept. Databases often act as a central hub for data access and storage in monolithic applications. Separating database access code can provide better isolation and management of different parts of an application’s data model.
:p How does splitting out repository layers help in understanding database dependencies?
??x
Splitting out repository layers allows for clearer separation between the business logic and the database access code. By organizing these layers based on bounded contexts, it becomes easier to identify which parts of the database are used by specific sections of the application. Tools like SchemaSpy can then be used to visualize relationships between tables, helping to understand and manage dependencies.
x??",786,The Reasons to Split the Monolith Deciding that you’d like a monolithic service or application to be smaller is a good start. But I would strongly advise you to chip away at these systems. An incremen...,qwen2.5:latest,2025-11-02 23:46:49,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Example Breaking Foreign Key Relationships,Breaking Foreign Key Relationships,"#### Breaking Foreign Key Relationships
Background context: The example discusses a scenario where the finance code uses a foreign key relationship to reference data from the catalog code. This leads to concerns about data integrity and performance when the catalog and finance services are separated into their own rights.

:p What is the problem introduced by using a foreign key relationship between the ledger table (finance) and the line item table (catalog)?
??x
The problem arises because the finance code directly accesses the line item table, which should be part of the catalog service. This coupling introduces tight integration that needs to be managed when services are separated.

Example:
```java
// Pseudocode showing direct access in finance package
public class FinanceService {
    public void generateReport(int ledgerId) {
        LineItem lineItem = lineItemRepository.findById(ledger.getLineItemId());
        // Use lineItem data for report generation
    }
}
```
x??",991,"Example: Breaking Foreign Key Relationships In this example, our catalog code uses a generic line item table to store information about an album. Our finance code uses a ledger table to track financia...",qwen2.5:latest,2025-11-02 23:47:00,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Example Breaking Foreign Key Relationships,Exposing Data via API Call,"#### Exposing Data via API Call
Background context: To decouple the finance code from the catalog code, an API call is introduced instead of direct database access. This change allows the finance service to request necessary information through a well-defined interface.

:p How can we refactor the finance code to avoid direct database access and instead use an API call?
??x
We should create an API in the catalog package that exposes the required data. The finance code will then make an API call to retrieve this information.

Example:
```java
// Pseudocode for API call in FinanceService
public class FinanceService {
    public void generateReport(int ledgerId) {
        String catalogData = catalogClient.getCatalogData(ledger.getLineItemId());
        // Use catalogData for report generation
    }
}
```
x??",817,"Example: Breaking Foreign Key Relationships In this example, our catalog code uses a generic line item table to store information about an album. Our finance code uses a ledger table to track financia...",qwen2.5:latest,2025-11-02 23:47:00,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Example Breaking Foreign Key Relationships,Performance Considerations,"#### Performance Considerations
Background context: The introduction of an API call might increase the number of database calls needed to generate reports. This is a trade-off between data integrity and performance.

:p Why do we need to consider performance when making changes in database relationships?
??x
We need to consider performance because multiple database calls can impact system speed, especially under load. However, sometimes sacrificing some performance for better separation of concerns or other benefits (like easier maintenance) is acceptable if the change does not significantly degrade overall performance.

Example:
```java
// Pseudocode comparing current and proposed systems
public class CurrentSystem {
    // Single database call to generate report
}

public class NewSystem {
    // Two API calls to generate report
}
```
x??",852,"Example: Breaking Foreign Key Relationships In this example, our catalog code uses a generic line item table to store information about an album. Our finance code uses a ledger table to track financia...",qwen2.5:latest,2025-11-02 23:47:00,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Example Breaking Foreign Key Relationships,Removal of Foreign Key Constraint,"#### Removal of Foreign Key Constraint
Background context: With the introduction of services, foreign key constraints are no longer appropriate. Instead, each service must manage its own data consistency.

:p What happens when a foreign key constraint is removed from the database?
??x
When a foreign key constraint is removed, data integrity becomes managed by the application logic rather than enforced at the database level. This means that the services need to implement their own mechanisms for ensuring data consistency.

Example:
```java
// Pseudocode showing service-level validation
public class OrderService {
    public void processOrder(Order order) {
        if (catalogService.checkCatalogItemExists(order.getCatalogItemId())) {
            // Proceed with processing
        } else {
            throw new InvalidOrderException(""Invalid catalog item ID"");
        }
    }
}
```
x??",896,"Example: Breaking Foreign Key Relationships In this example, our catalog code uses a generic line item table to store information about an album. Our finance code uses a ledger table to track financia...",qwen2.5:latest,2025-11-02 23:47:00,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Example Breaking Foreign Key Relationships,Managing Data Consistency Across Services,"#### Managing Data Consistency Across Services
Background context: When foreign key relationships are removed, ensuring data consistency across services becomes a manual process. This may require implementing custom checks or triggers.

:p How can we ensure data consistency when removing foreign keys between services?
??x
We need to implement custom checks and possibly trigger actions to clean up related data in the event of inconsistencies. For example, if an order refers to a non-existent catalog item, the system should either prevent such orders or handle them gracefully by updating or deleting invalid references.

Example:
```java
// Pseudocode for custom consistency check
public class OrderService {
    public void processOrder(Order order) {
        try {
            if (catalogService.checkCatalogItemExists(order.getCatalogItemId())) {
                // Proceed with processing
            } else {
                throw new InvalidOrderException(""Invalid catalog item ID"");
            }
        } catch (InvalidOrderException e) {
            orderRepository.delete(order);  // Clean up invalid order
        }
    }
}
```
x??

---",1153,"Example: Breaking Foreign Key Relationships In this example, our catalog code uses a generic line item table to store information about an album. Our finance code uses a ledger table to track financia...",qwen2.5:latest,2025-11-02 23:47:00,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Example Shared Static Data,Managing Shared Static Data in Databases,"#### Managing Shared Static Data in Databases

Background context: The passage discusses various approaches to handling shared static data, such as country codes, within a system that involves multiple services. The primary challenge is ensuring consistency across these services when updating this static data.

:p How can we manage shared static data like country codes between different services?

??x
One approach is to store the shared static data in a database table and have each service read from its own copy of this table. However, this method can introduce challenges in maintaining consistency if updates are not synchronized across all services.

```java
// Pseudocode for fetching country codes from a database
public List<String> fetchCountryCodes() {
    Connection conn = null;
    Statement stmt = null;
    ResultSet rs = null;

    try {
        conn = DatabaseConnection.getConnection();
        stmt = conn.createStatement();
        rs = stmt.executeQuery(""SELECT code FROM countries"");
        while (rs.next()) {
            countryCodes.add(rs.getString(""code""));
        }
    } catch (SQLException e) {
        // Handle exception
    } finally {
        closeResources(conn, stmt, rs);
    }

    return countryCodes;
}

private void closeResources(Connection conn, Statement stmt, ResultSet rs) {
    try {
        if (rs != null) rs.close();
        if (stmt != null) stmt.close();
        if (conn != null) conn.close();
    } catch (SQLException e) {
        // Handle exception
    }
}
```

x??",1528,Example: Shared Static Data I have seen perhaps as many country codes stored in databases (shown in  Figure 5-4 ) as I have written  StringUtils  classes for in-house Java projects. This seems to impl...,qwen2.5:latest,2025-11-02 23:47:12,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Example Shared Static Data,Duplicating Shared Static Data Tables,"#### Duplicating Shared Static Data Tables

Background context: One option mentioned in the passage is duplicating the shared static data tables for each package or service. This approach can simplify updates but introduces a consistency challenge.

:p How does duplicating the table for each package or service help manage shared static data?

??x
Duplicating the table ensures that each service has its own copy of the shared static data, which can be easier to maintain and update independently. However, this can lead to inconsistencies if not all services are updated simultaneously.

```java
// Pseudocode for duplicating a table in SQL
public void duplicateTable(String originalTableName, String newTableName) {
    Connection conn = null;
    Statement stmt = null;

    try {
        conn = DatabaseConnection.getConnection();
        stmt = conn.createStatement();

        // Create a new table with the same schema as the original
        String createStatement = ""CREATE TABLE "" + newTableName + "" AS SELECT * FROM "" + originalTableName;
        stmt.executeUpdate(createStatement);
        
        System.out.println(""Table duplicated successfully."");
    } catch (SQLException e) {
        e.printStackTrace();
    } finally {
        closeResources(conn, stmt);
    }
}

private void closeResources(Connection conn, Statement stmt) {
    try {
        if (stmt != null) stmt.close();
        if (conn != null) conn.close();
    } catch (SQLException e) {
        // Handle exception
    }
}
```

x??",1516,Example: Shared Static Data I have seen perhaps as many country codes stored in databases (shown in  Figure 5-4 ) as I have written  StringUtils  classes for in-house Java projects. This seems to impl...,qwen2.5:latest,2025-11-02 23:47:12,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Example Shared Static Data,Treating Shared Static Data as Code,"#### Treating Shared Static Data as Code

Background context: Another option is to treat the shared static data like code, stored in configuration files or directly in the service's codebase. This method can simplify updates and maintain consistency more easily.

:p How does treating shared static data as code help manage it?

??x
Treating shared static data as code (e.g., using properties files or enums) allows for easier updates since configuration files are typically simpler to change than database tables. This approach also helps in maintaining a single source of truth within the application's codebase.

```java
// Pseudocode for reading country codes from a properties file
public List<String> fetchCountryCodes() {
    Properties prop = new Properties();
    InputStream input = null;

    try {
        // load a properties file
        input = new FileInputStream(""config/country_codes.properties"");
        prop.load(input);

        Enumeration<Object> keys = prop.propertyNames();
        while (keys.hasMoreElements()) {
            String key = (String) keys.nextElement();
            countryCodes.add(key);
        }
    } catch (IOException ex) {
        ex.printStackTrace();
    } finally {
        if (input != null) {
            try {
                input.close();
            } catch (IOException e) {
                e.printStackTrace();
            }
        }
    }

    return countryCodes;
}
```

x??",1436,Example: Shared Static Data I have seen perhaps as many country codes stored in databases (shown in  Figure 5-4 ) as I have written  StringUtils  classes for in-house Java projects. This seems to impl...,qwen2.5:latest,2025-11-02 23:47:12,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Example Shared Static Data,Pushing Static Data into a Separate Service,"#### Pushing Static Data into a Separate Service

Background context: In some cases, the volume and complexity of static reference data may warrant creating a dedicated service to manage this data. This approach ensures consistency but can be overkill for simpler scenarios.

:p When should we consider pushing static data into a separate service?

??x
This approach is appropriate when the static reference data is highly complex or frequently updated, requiring specialized management and validation logic. However, it may be overkill if the data is simple (like country codes) and infrequently changes.

```java
// Pseudocode for an API to get country codes from a dedicated service
public class StaticDataServiceClient {
    private static final String SERVICE_URL = ""http://static-data-service/country-codes"";

    public List<String> fetchCountryCodes() throws IOException, InterruptedException {
        HttpClient client = HttpClient.newHttpClient();
        HttpRequest request = HttpRequest.newBuilder()
                .uri(URI.create(SERVICE_URL))
                .build();

        HttpResponse<String> response = client.send(request, BodyHandlers.ofString());
        
        return Arrays.asList(response.body().split("",""));
    }
}
```

x??

---",1262,Example: Shared Static Data I have seen perhaps as many country codes stored in databases (shown in  Figure 5-4 ) as I have written  StringUtils  classes for in-house Java projects. This seems to impl...,qwen2.5:latest,2025-11-02 23:47:12,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Example Shared Data,Shared Mutable Data Concept,"#### Shared Mutable Data Concept
Background context: In systems design, shared mutable data can lead to complex interdependencies and make it difficult to maintain and scale individual components. This is often seen when different parts of an application modify a common database table or object.

:p What are the challenges associated with shared mutable data in a multi-component system?
??x
The challenges include increased complexity due to concurrent modifications, race conditions, and inconsistent states across different modules. It can also lead to tightly coupled code, making it harder to make changes without affecting other parts of the system.
```java
public void updateCustomerPayment(int customerId, double amount) {
    // Code to update customer payment in a shared table
}
```
x??",799,"Example: Shared Data Now let’s dive into a more complex example, but one that can be a common problem when you’re trying to tease apart systems: shared mutable data. Our finance code tracks payments m...",qwen2.5:latest,2025-11-02 23:47:20,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Example Shared Data,Domain Concept Absence,"#### Domain Concept Absence
Background context: The text highlights that sometimes domain concepts are not explicitly modeled in code but are represented implicitly in databases. This can lead to issues when trying to understand and separate the responsibilities of different components.

:p Why is recognizing domain concepts important in systems design?
??x
Recognizing domain concepts helps in defining clear boundaries between different parts of a system, making it easier to implement changes without affecting other components. It also aids in maintaining a clean architecture by ensuring that each component has well-defined responsibilities.
```java
public class CustomerService {
    // Methods to interact with customer data
}
```
x??",744,"Example: Shared Data Now let’s dive into a more complex example, but one that can be a common problem when you’re trying to tease apart systems: shared mutable data. Our finance code tracks payments m...",qwen2.5:latest,2025-11-02 23:47:20,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Example Shared Data,Bounded Context Introduction,"#### Bounded Context Introduction
Background context: A bounded context is a way of defining the scope and responsibility of a particular part of an application. It helps in creating clear, isolated units that can be developed independently while still maintaining coherence within their specific domain.

:p What does recognizing the bounded context of a customer mean?
??x
Recognizing the bounded context of a customer means identifying the specific domain or area where the customer data is relevant and defining the rules, responsibilities, and constraints associated with it. This helps in isolating concerns and ensuring that changes to one part do not inadvertently affect another.
```java
public class FinanceService {
    // Methods for managing payments
}
```
x??",773,"Example: Shared Data Now let’s dive into a more complex example, but one that can be a common problem when you’re trying to tease apart systems: shared mutable data. Our finance code tracks payments m...",qwen2.5:latest,2025-11-02 23:47:20,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Example Shared Data,Package Creation for Customer Service,"#### Package Creation for Customer Service
Background context: To address issues related to shared mutable data, a new package is created to encapsulate the customer-related logic. This separation helps in isolating changes and improving modularity.

:p What is the benefit of creating a dedicated `Customer` package?
??x
The benefit is that it provides a clear boundary for managing customer data, reducing the risk of unintended side effects when making changes. It also improves code organization and makes it easier to understand the flow of information within the system.
```java
package com.example.customer;
public class Customer {
    // Attributes and methods related to customers
}
```
x??

---",704,"Example: Shared Data Now let’s dive into a more complex example, but one that can be a common problem when you’re trying to tease apart systems: shared mutable data. Our finance code tracks payments m...",qwen2.5:latest,2025-11-02 23:47:20,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Transactional Boundaries,Identifying Database Concerns Through Bounded Contexts,"#### Identifying Database Concerns Through Bounded Contexts
Background context explaining how database concerns can be identified through bounded contexts. This involves recognizing distinct areas of responsibility within an application that should correspond to separate database schemas.
:p How can you identify different database concerns in a system?
??x
By identifying and grouping the application code around bounded contexts, you can recognize different areas of responsibility that need to be managed separately. For instance, if your application handles both customer orders and warehouse inventory management, these two functionalities might have distinct data requirements and should be stored separately.
x??",720,"Example: Shared Tables Figure 5-7  shows our last example. Our catalog needs to store the name and price of the records we sell, and the warehouse needs to keep an electronic record of inventory. We d...",qwen2.5:latest,2025-11-02 23:47:29,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Transactional Boundaries,Splitting Shared Tables Across Contexts,"#### Splitting Shared Tables Across Contexts
Background context explaining the necessity of splitting shared tables into separate contexts when dealing with different bounded contexts that require unique database schemas. This is a step towards separating concerns within a monolithic schema to ensure each context has its dedicated data storage.
:p What happens if you keep shared tables across different contexts?
??x
Keeping shared tables in a single schema can lead to conflating concerns, where multiple functionalities are mixed together, making it harder to manage and evolve the database structure. For example, maintaining both customer orders and warehouse inventory in one table might make sense initially but becomes problematic as the application scales and more complex business rules are introduced.
x??",818,"Example: Shared Tables Figure 5-7  shows our last example. Our catalog needs to store the name and price of the records we sell, and the warehouse needs to keep an electronic record of inventory. We d...",qwen2.5:latest,2025-11-02 23:47:29,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Transactional Boundaries,Staging Service Separation,"#### Staging Service Separation
Background context explaining how separation of services should be staged before fully implementing it. This involves temporarily separating schemas while keeping the application code together to test changes without disrupting other parts of the system.
:p What is the advantage of staging a service separation?
??x
Staging a service separation allows you to test and validate schema changes in isolation from application logic, reducing risk and allowing for easier rollback if issues arise. By separating schemas but keeping application code together, you can ensure that any issues related to database changes do not impact existing functionality.
x??",687,"Example: Shared Tables Figure 5-7  shows our last example. Our catalog needs to store the name and price of the records we sell, and the warehouse needs to keep an electronic record of inventory. We d...",qwen2.5:latest,2025-11-02 23:47:29,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Transactional Boundaries,Transactional Boundaries and Schema Splitting,"#### Transactional Boundaries and Schema Splitting
Background context explaining the importance of maintaining transactional boundaries when splitting schemas. This involves ensuring that actions affecting multiple tables within a single operation are performed atomically to maintain data consistency.
:p What happens when you split schemas but operations span across them?
??x
When splitting schemas, if transactions need to span across different databases, it can lead to issues with transactional integrity and increased complexity in database interactions. For example, creating an order and updating the warehouse picking table might work together within a single transaction in a monolithic schema, but this atomicity is lost when each table resides in its own schema.
x??",779,"Example: Shared Tables Figure 5-7  shows our last example. Our catalog needs to store the name and price of the records we sell, and the warehouse needs to keep an electronic record of inventory. We d...",qwen2.5:latest,2025-11-02 23:47:29,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Transactional Boundaries,Example of Transaction Spanning Boundaries,"#### Example of Transaction Spanning Boundaries
Background context explaining how transactions can span across boundaries after splitting schemas. This involves demonstrating a scenario where an operation that updates multiple tables needs to be performed as part of a single transaction.
:p How does the MusicCorp example illustrate transaction spanning?
??x
In the MusicCorp example, creating an order and updating the warehouse picking table should ideally happen atomically. However, when these are split into separate schemas, they need to be updated independently, potentially leading to inconsistent states if one update fails while the other succeeds.
```java
// Pseudocode for a potential solution
try {
    // Insert order record in customer schema
    Order order = new Order();
    orderRepository.save(order);

    // Insert picking record in warehouse schema
    PickingRecord record = new PickingRecord();
    pickingRepository.save(record);
} catch (Exception e) {
    // Rollback both operations if any fail
    orderRepository.rollbackSave(order.getId());
    pickingRepository.rollbackSave(record.getId());
}
```
x??",1135,"Example: Shared Tables Figure 5-7  shows our last example. Our catalog needs to store the name and price of the records we sell, and the warehouse needs to keep an electronic record of inventory. We d...",qwen2.5:latest,2025-11-02 23:47:29,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Reporting Database,Handling Order Placement Failures,"#### Handling Order Placement Failures
Background context: When an order is placed but fails to be captured, it can be handled by either retrying the operation later or aborting and compensating for the failure. This approach aligns with eventual consistency principles, where a system may not always be in a consistent state immediately after operations, but will eventually reach one.
:p How do we handle order placement failures?
??x
We can queue up the failed order and attempt to insert it into the warehouse’s picking table later. Alternatively, if the operation cannot be completed due to constraints, we might need to abort the entire operation and revert any committed transactions. In both cases, ensuring that the system eventually reaches a consistent state is key.
??x",781,"Try Again Later The fact that the order was captured and placed might be enough for us, and we may decide to retry the insertion into the warehouse’s picking table at a later date. We could queue up t...",qwen2.5:latest,2025-11-02 23:47:39,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Reporting Database,Retrying Failed Operations,"#### Retrying Failed Operations
Background context: For some operations, retrying them at a later time can resolve issues if they are transient or due to temporary network or resource constraints. This approach uses eventual consistency principles where the system corrects itself over time.
:p What is an example of handling failed order placement through retry?
??x
We could queue up the order in a log file or another queue and attempt to insert it into the warehouse’s picking table again at a later date. If the network error was temporary, retrying might successfully complete the operation.
??x",601,"Try Again Later The fact that the order was captured and placed might be enough for us, and we may decide to retry the insertion into the warehouse’s picking table at a later date. We could queue up t...",qwen2.5:latest,2025-11-02 23:47:39,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Reporting Database,Compensating Transactions for Failed Operations,"#### Compensating Transactions for Failed Operations
Background context: When an operation fails but has already committed part of its transaction, a compensating transaction is required to undo the partial changes and revert the system back to its initial state. This ensures that no data is lost or left in an inconsistent state.
:p How do we handle a failed order insertion using a compensating transaction?
??x
We would issue a DELETE statement to remove the order from the database, effectively rolling it back. Additionally, we need to report via the UI that the operation failed and inform the user. The logic for handling this could be part of the same service or potentially distributed across different services.
```java
// Pseudocode for compensating transaction
public void handleFailedOrderInsertion() {
    try {
        // Remove order from database
        deleteFromDatabase(""DELETE FROM orders WHERE id = ?"", orderId);
        
        // Report failure to UI
        reportFailureToUI(orderId, ""Order insertion failed."");
    } catch (Exception e) {
        log.error(""Compensating transaction failed: "", e);
        // Handle retry or cleanup if necessary
    }
}
```
x??",1191,"Try Again Later The fact that the order was captured and placed might be enough for us, and we may decide to retry the insertion into the warehouse’s picking table at a later date. We could queue up t...",qwen2.5:latest,2025-11-02 23:47:39,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Reporting Database,Eventual Consistency in Long-Lived Operations,"#### Eventual Consistency in Long-Lived Operations
Background context: For long-lived operations where immediate consistency is not critical, eventual consistency can be employed. This approach allows the system to be inconsistent for a short period and resolve any inconsistencies over time.
:p How does eventual consistency apply to order placement?
??x
Eventual consistency means that even if an operation like order placement fails initially, it will eventually succeed once network or resource issues are resolved. The system can queue failed operations and retry them later until they complete successfully.
??x",617,"Try Again Later The fact that the order was captured and placed might be enough for us, and we may decide to retry the insertion into the warehouse’s picking table at a later date. We could queue up t...",qwen2.5:latest,2025-11-02 23:47:39,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Reporting Database,Distributed Transactions for Immediate Consistency,"#### Distributed Transactions for Immediate Consistency
Background context: For critical operations where immediate consistency is required across multiple systems, distributed transactions using a transaction manager can be used to ensure that all involved transactions either commit or roll back together.
:p What is the two-phase commit algorithm?
??x
The two-phase commit (2PC) algorithm involves two phases:
1. Voting phase: All participating nodes in the transaction are asked if they are ready to commit.
2. Commit phase: If all nodes agree, the transaction commits; otherwise, it rolls back.

Here’s an example of how 2PC might be implemented:
```java
// Pseudocode for Two-Phase Commit
public void twoPhaseCommit(TransactionManager manager) {
    // Voting Phase
    if (manager.prepare()) {
        // Commit Phase
        manager.commit();
    } else {
        manager.rollback();
    }
}
```
x??

---",912,"Try Again Later The fact that the order was captured and placed might be enough for us, and we may decide to retry the insertion into the warehouse’s picking table at a later date. We could queue up t...",qwen2.5:latest,2025-11-02 23:47:39,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Reporting Database,Distributed Transaction Voting Mechanism,"#### Distributed Transaction Voting Mechanism
Background context explaining how participants (cohorts) in a distributed transaction communicate with a central transaction manager to decide on committing or rolling back transactions. The process involves each cohort signaling whether it can proceed, and based on consensus, the transaction manager decides the fate of all participating parties.
:p What does each participant do during the voting phase of a distributed transaction?
??x
During the voting phase, each participant (cohort) in the distributed transaction communicates with the central transaction manager to indicate if its local transaction can go ahead. This is essentially a yes or no vote on whether it's ready for the transaction to be committed.
```java
public class Cohort {
    public String vote() {
        // Logic to check if the local transaction can proceed
        boolean canProceed = checkLocalTransactionConditions();
        
        return canProceed ? ""yes"" : ""no"";
    }
}
```
x??",1015,This is where each participant (also called a  cohort  in this context) in the distributed transaction tells the transaction manager whether it thinks its local transaction can go ahead. If the transa...,qwen2.5:latest,2025-11-02 23:47:52,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Reporting Database,Outcome of Voting and Transaction Manager's Decision,"#### Outcome of Voting and Transaction Manager's Decision
Background context explaining that based on the votes received from all participants, the transaction manager decides whether to commit or roll back the entire transaction. If any participant gives a ""no"" vote, the transaction is rolled back.
:p What happens if one cohort responds with a ""no"" during voting?
??x
If any of the cohorts respond with a ""no,"" the transaction manager will decide to rollback all participants and not proceed with the commit. This ensures that no partial transactions occur and maintains consistency among all involved parties.
```java
public class TransactionManager {
    public void processVotingResults(Map<String, String> votes) {
        boolean hasNoVote = votes.values().contains(""no"");
        
        if (hasNoVote) {
            // Rollback all participants
            rollbackAllParticipants(votes);
        } else {
            // Commit all participants
            commitAllParticipants(votes);
        }
    }
}
```
x??",1023,This is where each participant (also called a  cohort  in this context) in the distributed transaction tells the transaction manager whether it thinks its local transaction can go ahead. If the transa...,qwen2.5:latest,2025-11-02 23:47:52,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Reporting Database,Vulnerability to Outages and Resource Locking,"#### Vulnerability to Outages and Resource Locking
Background context discussing the risk of outages where a transaction manager or a participant failing to respond can cause pending transactions to never complete. Also, the coordination process involves locks which can lead to resource contention, making scaling systems more challenging.
:p What are the risks associated with the central transaction manager going down?
??x
If the transaction manager goes down, any ongoing transactions will remain in limbo and never complete. This is because all participants halt until they receive instructions from the transaction manager. Consequently, this can lead to incomplete or lost transactions, which is a significant risk.
```java
public class TransactionManager {
    public void handleTransactionManagerDown() {
        // Logic to handle the scenario where the transaction manager goes down
        System.out.println(""Transaction manager has gone down. Pending transactions will not complete."");
        
        // Optionally, retry logic or fallback mechanisms can be implemented here
    }
}
```
x??",1107,This is where each participant (also called a  cohort  in this context) in the distributed transaction tells the transaction manager whether it thinks its local transaction can go ahead. If the transa...,qwen2.5:latest,2025-11-02 23:47:52,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Reporting Database,Compensation and Consistency in Distributed Systems,"#### Compensation and Consistency in Distributed Systems
Background context discussing how distributed transactions add complexity but might inhibit scaling. The text also mentions the importance of considering eventual consistency when possible to simplify system design.
:p When is it preferable to use compensating retry logic over distributed transactions?
??x
It is preferable to use compensating retry logic, especially for operations that do not require strict transactional consistency across all components. This approach allows systems to be more scalable and easier to manage by leveraging eventual consistency, where state eventually converges despite individual failures.
```java
public class OrderProcessingSystem {
    public void processOrder() {
        // Attempt to process the order with retry logic if necessary
        try {
            performOrderOperations();
        } catch (Exception e) {
            handleCompensatingActions(e);
        }
    }

    private void performOrderOperations() {
        // Perform operations like placing an order in the database and sending a confirmation email
    }

    private void handleCompensatingActions(Exception e) {
        // Logic to undo partial changes if necessary, ensuring eventual consistency
    }
}
```
x??",1286,This is where each participant (also called a  cohort  in this context) in the distributed transaction tells the transaction manager whether it thinks its local transaction can go ahead. If the transa...,qwen2.5:latest,2025-11-02 23:47:52,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Reporting Database,Concrete Concepts for Complex Transactions,"#### Concrete Concepts for Complex Transactions
Background context on creating specific concepts (like ""in-process-order"") to manage complex transactions in distributed systems. This approach helps with monitoring and managing these more intricate operations.
:p How can creating a concrete concept like an ""in-process-order"" aid in managing complex transactions?
??x
Creating a concrete concept such as an ""in-process-order"" allows you to encapsulate all logic related to processing the order, making it easier to manage exceptions and ensure consistency. This abstraction provides a clear structure and a single point of reference for handling operations like compensating transactions.
```java
public class InProcessOrder {
    private boolean isCommitted = false;
    
    public void commit() {
        if (!isCommitted) {
            // Logic to fully commit the order
            System.out.println(""Order has been fully committed."");
            isCommitted = true;
        } else {
            System.out.println(""Order is already committed."");
        }
    }

    public void rollback() {
        if (isCommitted) {
            // Logic to undo partial changes
            System.out.println(""Rolling back order due to an error."");
            isCommitted = false;
        }
    }
}
```
x??",1301,This is where each participant (also called a  cohort  in this context) in the distributed transaction tells the transaction manager whether it thinks its local transaction can go ahead. If the transa...,qwen2.5:latest,2025-11-02 23:47:52,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Reporting Database,Reporting Database Overview,"#### Reporting Database Overview
Background context: In a monolithic service architecture, reporting typically involves combining data from various parts of an organization to generate useful insights. The standard approach is to use a single database that serves both the main application and the reporting system.

:p What are the challenges in maintaining a single database for both primary and reporting purposes?
??x
The main challenges include schema management and optimization conflicts:

- **Schema Management**: A change in the schema affects both the live services and reporting systems, making changes difficult to coordinate.
- **Optimization Conflicts**: Optimizing the database for one use case (e.g., read-heavy operations) may negatively impact the other use case (e.g., transactional write operations).

These challenges often result in a suboptimal schema that doesn't perform well for either purpose.

x??",925,"Reporting As we’ve already seen, in splitting a service into smaller parts, we need to also potentially split up how and where data is stored. This creates a problem, however, when it comes to one vit...",qwen2.5:latest,2025-11-02 23:48:02,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Reporting Database,Read Replication,"#### Read Replication
Background context: In standard architectures, read replicas are used to offload reporting queries from the main database. This setup helps prevent high load on the primary database during query execution.

:p How does using a read replica benefit the reporting system?
??x
Using a read replica benefits the reporting system by:

- Reducing the load on the primary database.
- Allowing more efficient and faster querying for reports without impacting the main service's performance.

However, this setup still faces challenges such as schema management conflicts and limited optimization options. The schema must support both live operations and reporting queries, which can be challenging to balance.

x??",728,"Reporting As we’ve already seen, in splitting a service into smaller parts, we need to also potentially split up how and where data is stored. This creates a problem, however, when it comes to one vit...",qwen2.5:latest,2025-11-02 23:48:02,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Reporting Database,Schema Management Challenges,"#### Schema Management Challenges
Background context: In a monolithic architecture, the database schema serves as an API for both the application services and the reporting system. This shared schema can lead to complications when making changes, as any change in the schema affects multiple systems.

:p Why is managing the schema challenging in a monolithic setup?
??x
Managing the schema is challenging because:

- **Interconnected Systems**: Changes to the schema must be carefully coordinated between all services and the reporting system.
- **Impact on Performance**: Schema changes can have significant performance implications for both live operations and reporting queries.

This interdependence often results in compromises that may not optimize either use case effectively.

x??",789,"Reporting As we’ve already seen, in splitting a service into smaller parts, we need to also potentially split up how and where data is stored. This creates a problem, however, when it comes to one vit...",qwen2.5:latest,2025-11-02 23:48:02,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Reporting Database,Data Optimization Conflicts,"#### Data Optimization Conflicts
Background context: The limitations of a single database schema mean that optimizations for one part of the system (e.g., read-heavy operations) can negatively impact another part (e.g., write-heavy operations). This creates a trade-off where optimizing for reporting might degrade the performance of the main application.

:p How does the single database model limit data optimization?
??x
The single database model limits data optimization because:

- **Shared Schema**: The schema must cater to both read and write operations, often leading to compromises.
- **Performance Trade-offs**: Optimizations that improve one aspect (e.g., faster reads) may degrade another (e.g., slower writes).

This can result in a suboptimal database design where the schema is not ideal for either primary application use or reporting.

x??",857,"Reporting As we’ve already seen, in splitting a service into smaller parts, we need to also potentially split up how and where data is stored. This creates a problem, however, when it comes to one vit...",qwen2.5:latest,2025-11-02 23:48:02,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Reporting Database,Exploring New Database Technologies,"#### Exploring New Database Technologies
Background context: The monolithic database model might not be optimal when dealing with diverse data models and storage needs. Different applications may benefit from different types of databases (e.g., graph, document, column-oriented).

:p What are the benefits of exploring new database technologies for reporting?
??x
Exploring new database technologies offers several benefits:

- **Improved Data Modeling**: Using a graph database like Neo4j or a document store like MongoDB can better fit certain data models.
- **Scalability and Performance**: Column-oriented databases like Cassandra can provide better performance for large volumes of data.

These technologies allow for more specialized optimizations that might not be possible with traditional relational databases, leading to improved efficiency and flexibility in handling diverse data needs.

x??

---",908,"Reporting As we’ve already seen, in splitting a service into smaller parts, we need to also potentially split up how and where data is stored. This creates a problem, however, when it comes to one vit...",qwen2.5:latest,2025-11-02 23:48:02,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Data Retrieval via Service Calls,Data Retrieval via Service Calls,"#### Data Retrieval via Service Calls
Data retrieval for reporting systems often involves fetching data from various sources using API calls. For simple reports, this can be straightforward, but complex scenarios require pulling large volumes of data across multiple systems, leading to performance issues and challenges with maintaining accuracy.

:p What is the main challenge when using API calls for data retrieval in a reporting system?
??x
The main challenge is that the APIs exposed by various microservices may not be designed for efficient reporting use cases. This can lead to inefficient data retrieval methods, such as making multiple API calls to gather all necessary data, which can be slow and resource-intensive.

For example, if you need customer data, a service might only allow fetching customers by ID or searching by fields, but not in bulk. You would have to make a separate call for each customer, which is inefficient.
x??",946,"Data Retrieval via Service Calls There are many variants of this model, but they all rely on pulling the required data from the source systems via API calls. For a very simple reporting system, like a...",qwen2.5:latest,2025-11-02 23:48:14,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Data Retrieval via Service Calls,Batch Data Retrieval,"#### Batch Data Retrieval
To handle large volumes of data more efficiently, batch APIs can be used. These APIs are designed to retrieve multiple records at once rather than individual ones.

:p How does a batch API improve the efficiency of data retrieval?
??x
A batch API improves efficiency by allowing you to pass a list of IDs or other identifiers in one request, reducing the number of calls needed and thus improving performance. For example, instead of making separate requests for each customer ID, you can send a single request with all the required IDs.

Example pseudocode:
```java
// Pseudocode for using batch API
List<Long> customerIDs = getCustomerIDs();
BatchRequest batchRequest = new BatchRequest(customerIDs);
response = postToAPI(batchRequest);

if (response.status == 202) {
    // Wait until the request is completed
    while (!isRequestCompleted(response.location)) {}
    
    response = getFromAPI(response.location); // Check for status 201 to ensure data is ready
    if (response.status == 201) {
        exportData(response.data);
    }
}
```
x??",1076,"Data Retrieval via Service Calls There are many variants of this model, but they all rely on pulling the required data from the source systems via API calls. For a very simple reporting system, like a...",qwen2.5:latest,2025-11-02 23:48:14,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Data Retrieval via Service Calls,Reporting System Challenges with APIs,"#### Reporting System Challenges with APIs
Reporting systems often face challenges when interacting with APIs due to the design of these APIs, which may not be optimized for reporting tasks.

:p What are some challenges in using existing APIs for a reporting system?
??x
Some challenges include:
1. **Inefficient API Design**: APIs might only support fetching data by ID or specific criteria rather than in bulk.
2. **Multiple Calls Required**: To gather comprehensive data, multiple calls may be necessary, leading to inefficiency and increased load on the service being queried.
3. **Cache Misses**: Reporting often involves accessing less frequently used data (the long tail), which can lead to cache misses despite having cache headers.

Example:
If you need customer data for a 24-month report, making individual API calls for each customer could result in many cache misses, slowing down the process.
x??",910,"Data Retrieval via Service Calls There are many variants of this model, but they all rely on pulling the required data from the source systems via API calls. For a very simple reporting system, like a...",qwen2.5:latest,2025-11-02 23:48:14,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Data Retrieval via Service Calls,Batch Export Resource Endpoint,"#### Batch Export Resource Endpoint
A batch export resource endpoint allows users to request large datasets by sending a POST request with a list of identifiers.

:p What is the purpose of a batch export resource endpoint?
??x
The purpose of a batch export resource endpoint is to allow efficient data retrieval for reporting purposes. Instead of making multiple API calls, this endpoint accepts a list of IDs or other criteria and processes them in one go, returning an HTTP 202 response indicating that the request has been accepted but not yet processed.

Once processing is complete, it returns an HTTP 201 Created status along with the data. This approach avoids frequent cache misses and reduces API load by batching requests into fewer calls.

Example:
```java
// Pseudocode for using batch export endpoint
BatchCustomerExportRequest request = new BatchCustomerExportRequest(customerIDs);
response = postToAPI(request);

if (response.status == 202) {
    // Poll the resource until it is ready
    while (!isRequestCompleted(response.location)) {}
    
    response = getFromAPI(response.location); // Check for status 201 to ensure data is ready
    
    if (response.status == 201) {
        fileLocation = response.data;
        // Fetch and process the CSV file from the shared location
    }
}
```
x??",1313,"Data Retrieval via Service Calls There are many variants of this model, but they all rely on pulling the required data from the source systems via API calls. For a very simple reporting system, like a...",qwen2.5:latest,2025-11-02 23:48:14,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Data Retrieval via Service Calls,Traditional Reporting Needs vs. Batch Export,"#### Traditional Reporting Needs vs. Batch Export
While batch export can be useful, it might not always be the best solution for traditional reporting needs due to complexity and potential inefficiencies.

:p Why is a batch export resource endpoint less favored for traditional reporting systems?
??x
A batch export resource endpoint may introduce unnecessary complexity for simple reporting tasks and does not scale as well for traditional reporting. It is more suited for specific scenarios, such as data exports or bulk insertions, rather than routine reporting needs where simplicity and performance are crucial.

For example, a simpler solution might involve periodically fetching data into an SQL database using a scheduled job, which can be easier to integrate with third-party tools and maintain.
x??

---",813,"Data Retrieval via Service Calls There are many variants of this model, but they all rely on pulling the required data from the source systems via API calls. For a very simple reporting system, like a...",qwen2.5:latest,2025-11-02 23:48:14,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Data Pumps,Data Pump Mechanism,"---
#### Data Pump Mechanism
Background context: The text discusses an alternative approach to data retrieval for reporting, where a standalone program (data pump) pushes data from the source database directly into a central reporting database. This method reduces overhead associated with HTTP calls and simplifies integration management.

:p What is the main benefit of using a data pump in the context described?
??x
The primary benefit is reduced overhead compared to making numerous HTTP requests, as well as simplified management by having the same team handle both the service's internal schema and the reporting database. This approach minimizes coupling issues that typically arise when multiple systems integrate with a shared database.
x??",750,"Data Pumps Rather than have the reporting system pull the data, we could instead have the data pushed to the reporting system. One of the downsides of retrieving the data by standard HTTP calls is the...",qwen2.5:latest,2025-11-02 23:48:23,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Data Pumps,Implementation of Data Pump,"#### Implementation of Data Pump
Background context: The data pump is implemented using command-line tools or scripts triggered via Cron jobs, ensuring it runs periodically to update the central reporting database. It requires intimate knowledge of both source and target schemas to map one schema to another.

:p How can a simple command-line program be set up to trigger a data pump?
??x
A simple command-line program could use shell commands like `cron` or other scheduling tools to run at specified intervals. For example, using Bash:
```bash
# Example cron entry in crontab file
0 5 * * * /path/to/data-pump.sh >> /path/to/logfile.log 2>&1
```
This cron job runs the script `data-pump.sh` every day at 5 AM and logs output to a specified log file.
x??",756,"Data Pumps Rather than have the reporting system pull the data, we could instead have the data pushed to the reporting system. One of the downsides of retrieving the data by standard HTTP calls is the...",qwen2.5:latest,2025-11-02 23:48:23,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Data Pumps,Version Control for Data Pump,"#### Version Control for Data Pump
Background context: To ensure consistency and reduce risks, the data pump is version-controlled along with the service code. Builds of the data pump are created as an additional artifact in the service build process.

:p Why should the data pump be version-controlled alongside the service?
??x
Version-controlling the data pump ensures that any changes or updates to how data is extracted and transformed from the source database to the reporting schema are tracked and managed systematically. This practice helps maintain a history of changes, facilitating easier rollback if issues arise post-deployment.
x??",646,"Data Pumps Rather than have the reporting system pull the data, we could instead have the data pushed to the reporting system. One of the downsides of retrieving the data by standard HTTP calls is the...",qwen2.5:latest,2025-11-02 23:48:23,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Data Pumps,Materialized Views for Schema Segmentation,"#### Materialized Views for Schema Segmentation
Background context: For relational databases, materialized views can be used to create separate schemas for each service in the reporting database. This approach allows exposing only necessary parts of the schema while maintaining performance.

:p How do materialized views help in managing a segmented schema?
??x
Materialized views allow creating aggregated or transformed views of data from various services within the reporting database, minimizing direct access to underlying schemas. This can be particularly useful for large-scale applications with multiple data sources needing distinct reporting mechanisms.
x??",668,"Data Pumps Rather than have the reporting system pull the data, we could instead have the data pushed to the reporting system. One of the downsides of retrieving the data by standard HTTP calls is the...",qwen2.5:latest,2025-11-02 23:48:23,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Data Pumps,Challenges and Mitigations,"#### Challenges and Mitigations
Background context: While data pumps offer benefits like reduced overhead and easier management, challenges such as schema coupling persist. Implementing techniques like materialized views can mitigate some of these issues but may introduce additional complexities.

:p What are the primary downsides of implementing a segmented schema approach?
??x
The main downsides include increased complexity in managing different schemas, potential performance degradation due to frequent updates, and the challenge of maintaining backward compatibility when altering underlying data structures.
x??

---",626,"Data Pumps Rather than have the reporting system pull the data, we could instead have the data pushed to the reporting system. One of the downsides of retrieving the data by standard HTTP calls is the...",qwen2.5:latest,2025-11-02 23:48:23,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Mapping Continuous Integration to Microservices,Alternative Destinations On Data Pumps and Event-Based Reporting,"#### Alternative Destinations On Data Pumps and Event-Based Reporting

Background context: In a previous project, JSON files were populated into AWS S3 to serve as a data mart. However, when scaling became necessary, the solution was changed to use a cube that can integrate with standard reporting tools like Excel and Tableau.

:p What is the main change made in this project regarding data population?
??x
The main change involves shifting from using data pumps to populate JSON files stored in S3 to integrating directly with a cube for better integration with standard reporting tools.
x??",594,"Alternative Destinations On one project I was involved with, we used a series of data pumps to populate JSON files in AWS S3, effectively using S3 to masquerade as a giant data mart. This approach wor...",qwen2.5:latest,2025-11-02 23:48:31,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Mapping Continuous Integration to Microservices,Event Data Pump,"#### Event Data Pump

Background context: Microservices emit events based on state changes, and these events can be used by external systems (like our own event subscribers) to update the reporting database. This approach avoids direct coupling with the underlying database of the source microservice.

:p How does an event data pump differ from a traditional data pump in terms of data handling?
??x
An event data pump differs from a traditional data pump because it uses events generated by state changes in microservices rather than scheduled updates. This means that data can be sent to the reporting system as soon as an event occurs, making the data more up-to-date and efficient.
x??",690,"Alternative Destinations On one project I was involved with, we used a series of data pumps to populate JSON files in AWS S3, effectively using S3 to masquerade as a giant data mart. This approach wor...",qwen2.5:latest,2025-11-02 23:48:31,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Mapping Continuous Integration to Microservices,Event Data Pump: Temporal Nature,"#### Event Data Pump: Temporal Nature

Background context: The temporal nature of events makes them suitable for handling real-time or near-real-time updates in a reporting database. Events are timestamped, which helps manage processing and avoid sending duplicate data.

:p Why is the temporal nature of events beneficial when used with event-based reporting?
??x
The temporal nature of events is beneficial because it allows the system to process only new events as they arrive, ensuring that old events do not need to be reprocessed. This reduces redundancy and improves efficiency by sending deltas (only changes) rather than full data sets.
x??",649,"Alternative Destinations On one project I was involved with, we used a series of data pumps to populate JSON files in AWS S3, effectively using S3 to masquerade as a giant data mart. This approach wor...",qwen2.5:latest,2025-11-02 23:48:31,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Mapping Continuous Integration to Microservices,Event Data Pump: Looser Coupling,"#### Event Data Pump: Looser Coupling

Background context: By using events for reporting, the system can decouple itself from the internal state of microservices, making it easier to manage and scale independently.

:p How does event-based processing offer looser coupling in comparison to traditional data pumps?
??x
Event-based processing offers looser coupling because it subscribes to external events rather than relying on direct database access. This separation means that changes in the service's internal state do not directly impact the reporting system, allowing for more independent evolution of both systems.
x??",624,"Alternative Destinations On one project I was involved with, we used a series of data pumps to populate JSON files in AWS S3, effectively using S3 to masquerade as a giant data mart. This approach wor...",qwen2.5:latest,2025-11-02 23:48:31,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Mapping Continuous Integration to Microservices,Backup Data Pump,"#### Backup Data Pump

Background context: Netflix uses a backup solution as an alternative method to handle large volumes of data. This approach can be considered a special case of a traditional data pump but leverages existing tools and infrastructure.

:p What is the key benefit of using backups as a data pump for reporting?
??x
The key benefit is that it utilizes existing backup solutions, which are often designed to handle large volumes of data efficiently. This method addresses scaling issues by leveraging robust backup tools that can process and store data without additional custom development.
x??

---",617,"Alternative Destinations On one project I was involved with, we used a series of data pumps to populate JSON files in AWS S3, effectively using S3 to masquerade as a giant data mart. This approach wor...",qwen2.5:latest,2025-11-02 23:48:31,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Mapping Continuous Integration to Microservices,Cassandra Data Backup Strategy,"#### Cassandra Data Backup Strategy
Background context: Netflix uses Cassandra as a backing store for its services and has implemented a data backup strategy to ensure data durability. The standard approach is to make copies of SSTable files, which are stored in Amazon S3.

:p How does Netflix back up its Cassandra data?
??x
Netflix backs up its Cassandra data by making a copy of the SSTables (data files) and storing them in Amazon S3, which provides significant data durability guarantees. This approach ensures that backup copies are safe and can be used for reporting or recovery purposes.
```java
// Pseudocode to illustrate the process
public class BackupManager {
    public void backupSSTables(String bucketName) {
        List<String> sstables = getCassandraNode().listSSTables();
        for (String table : sstables) {
            copyFileToS3(table, bucketName);
        }
    }

    private void copyFileToS3(String filePath, String bucketName) {
        // Code to copy the SSTable file from Cassandra node to S3 bucket
    }
}
```
x??",1052,"Netflix has decided to standardize on Cassandra as the backing store for its services, of which there are many. Netflix has invested significant time in building tools to make Cassandra easy to work w...",qwen2.5:latest,2025-11-02 23:48:42,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Mapping Continuous Integration to Microservices,Aegisthus Project,"#### Aegisthus Project
Background context: Netflix has open sourced a project called Aegisthus that processes large amounts of data using backup SSTables as the source for jobs.

:p What is Aegisthus and how does it work?
??x
Aegisthus is an open-source project from Netflix designed to process large amounts of data by utilizing Cassandra SSTable backups. It works by reading SSTables directly, which are then used as the input for Hadoop jobs or other processing tasks.

```java
// Pseudocode to illustrate Aegisthus's operation
public class Aegisthus {
    public void processSSTables(String sstablePath) {
        File sstableFile = new File(sstablePath);
        // Read and process the SSTable file
        processDataFromSSTable(sstableFile);
    }

    private void processDataFromSSTable(File sstableFile) {
        // Code to read, parse, and process data from SSTable
    }
}
```
x??",894,"Netflix has decided to standardize on Cassandra as the backing store for its services, of which there are many. Netflix has invested significant time in building tools to make Cassandra easy to work w...",qwen2.5:latest,2025-11-02 23:48:42,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Mapping Continuous Integration to Microservices,Eventing Systems for Real-Time Reporting,"#### Eventing Systems for Real-Time Reporting
Background context: As mentioned in the text, the need to consolidate all reporting into one location is being reconsidered due to different use cases requiring varying levels of accuracy and timeliness.

:p Why are eventing systems becoming more important?
??x
Eventing systems are becoming increasingly important because they allow data to be routed to multiple destinations depending on the specific needs of each use case. This approach supports a variety of reporting requirements, such as dashboards, alerting, financial reports, and user analytics, which have different tolerances for accuracy and timeliness.

```java
// Pseudocode to illustrate event routing with an eventing system
public class EventRouter {
    public void routeEvent(Event event) {
        String routingKey = getRoutingKey(event);
        switch (routingKey) {
            case ""dashboard"":
                sendToDashboard(event);
                break;
            case ""alerting"":
                triggerAlerts(event);
                break;
            case ""financialReport"":
                generateFinancialReport(event);
                break;
            default:
                logUnexpectedEvent(event);
                break;
        }
    }

    private String getRoutingKey(Event event) {
        // Determine the appropriate routing key based on event properties
    }
}
```
x??",1419,"Netflix has decided to standardize on Cassandra as the backing store for its services, of which there are many. Netflix has invested significant time in building tools to make Cassandra easy to work w...",qwen2.5:latest,2025-11-02 23:48:42,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Mapping Continuous Integration to Microservices,Cost of Change in Data Management,"#### Cost of Change in Data Management
Background context: The text emphasizes the importance of making small, incremental changes to manage the cost and risk associated with altering code or databases.

:p How can we mitigate the costs associated with changing data management strategies?
??x
Mitigating the costs associated with changing data management strategies involves several key practices:

1. **Whiteboard Thinking**: Use whiteboards for initial brainstorming and prototyping, where the cost of change is minimal.
2. **Incremental Changes**: Make small, incremental changes to understand their impact before fully implementing them.
3. **Tools and Automation**: Utilize tools and automation to support quick fixes if mistakes occur.

```java
// Example of using a whiteboard for initial planning
public class DataManagementStrategy {
    public void planChange() {
        // Initial planning on the whiteboard
        System.out.println(""Planning data management changes on the whiteboard"");
        
        // Mock-up code for small incremental change
        makeSmallIncrementalChange();
    }

    private void makeSmallIncrementalChange() {
        // Code to implement a small, incremental change
        // This could involve adding or modifying a single function in an existing system
    }
}
```
x??",1320,"Netflix has decided to standardize on Cassandra as the backing store for its services, of which there are many. Netflix has invested significant time in building tools to make Cassandra easy to work w...",qwen2.5:latest,2025-11-02 23:48:42,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Mapping Continuous Integration to Microservices,Service Decomposition Concepts,"#### Service Decomposition Concepts
Background context: The passage discusses strategies for decomposing large services into smaller, more manageable ones. This is important to maintain system scalability and ease of maintenance.

:p What are the steps involved in identifying seams for service boundaries?
??x
The process involves finding areas where a service can be split while ensuring that each part has clear responsibilities and collaborators. Key aspects include understanding use cases and how they interact, avoiding circular references, and ensuring services aren't overly chatty (i.e., don’t communicate too frequently).

For example, in a music shop application:
- A customer search might involve the `RecordService` collaborating with the `UserService`.
- Registration involves interactions between `UserService` and `CustomerDatabase`.

This helps ensure that each service has clear boundaries and responsibilities.
x??",934,"Sketch out your proposed design. See what happens when you run use cases across what you think your service boundaries will be. For our music shop, for example, imagine what happens when a customer se...",qwen2.5:latest,2025-11-02 23:48:53,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Mapping Continuous Integration to Microservices,Class-Responsibility-Collaboration (CRC) Cards,"#### Class-Responsibility-Collaboration (CRC) Cards
Background context: The passage suggests using CRC cards to visualize service responsibilities and collaborations. This technique aids in designing smaller, more focused services.

:p How do you use CRC cards for service design?
??x
You create index cards for each class or service, writing down its responsibilities and the collaborators it works with. For example:

- **Card 1 (RecordService)**
  - Responsibilities: Search records by title or artist.
  - Collaborates with: UserService, RecordDatabase.

This approach helps in identifying seams where services can be split while ensuring that each service has a clear purpose.

For instance:
```
+-------------------+      +-----------------+
| RecordService     | <--> | UserService     |
+-------------------+      +-----------------+
        ^
        |
+-------------------+
| RecordDatabase    |
+-------------------+
```

x??",936,"Sketch out your proposed design. See what happens when you run use cases across what you think your service boundaries will be. For our music shop, for example, imagine what happens when a customer se...",qwen2.5:latest,2025-11-02 23:48:53,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Mapping Continuous Integration to Microservices,Root Causes of Large Services,"#### Root Causes of Large Services
Background context: The passage explains that large services grow over time, and this growth is sometimes necessary but should be managed to avoid becoming too complex.

:p Why do services often grow beyond a manageable size?
??x
Services tend to grow because they need to support additional features or handle more use cases. This can happen incrementally without any clear stopping point. For example, in a music shop, starting with just record management might lead to adding user registration and purchase functionalities.

However, if services become too large, it becomes difficult to maintain them. The passage suggests identifying the seams where splitting the service is necessary before it becomes too costly.

For instance, a monolithic `MusicShopService` might start as:

```java
public class MusicShopService {
    public void searchRecords(String keyword) {}
    public void registerCustomer(Customer customer) {}
    public void purchaseAlbum(int albumId, Customer customer) {}
}
```

As the service grows, it might be better split into smaller services like `RecordService`, `UserService`, and `PurchaseService`.

x??",1168,"Sketch out your proposed design. See what happens when you run use cases across what you think your service boundaries will be. For our music shop, for example, imagine what happens when a customer se...",qwen2.5:latest,2025-11-02 23:48:53,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Mapping Continuous Integration to Microservices,Incremental Service Splitting,"#### Incremental Service Splitting
Background context: The passage emphasizes that splitting services should be done incrementally to avoid making the initial decomposition too expensive.

:p How can we manage the cost of splitting a large service?
??x
To manage costs, you can use libraries and lightweight service frameworks. Additionally, providing self-service virtual machines or PaaS can help in quickly setting up and testing new services without significant overhead.

For example:
- Use a lightweight framework like Spring Boot for creating new microservices.
- Utilize cloud platforms that offer pre-configured environments (e.g., AWS EC2 instances, Docker containers) to reduce setup time.

This helps ensure that the initial split is manageable and reduces the overall cost of decomposition.

x??",808,"Sketch out your proposed design. See what happens when you run use cases across what you think your service boundaries will be. For our music shop, for example, imagine what happens when a customer se...",qwen2.5:latest,2025-11-02 23:48:53,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Mapping Continuous Integration to Microservices,Decomposition as an Incremental Process,"#### Decomposition as an Incremental Process
Background context: The passage highlights that decomposing a system should be done incrementally to facilitate ongoing growth and evolution.

:p Why is incremental decomposition important?
??x
Incremental decomposition allows for continuous improvement without disrupting the entire system. It enables you to address new requirements or refactor old components step-by-step, making it easier to manage complexity over time.

For example:
- Start with identifying clear boundaries between services.
- Gradually decompose large monolithic applications into smaller, more focused services as needed.
- Test and refine each service independently before fully integrating them.

This approach ensures that the system remains manageable and adaptable.

x??

---",801,"Sketch out your proposed design. See what happens when you run use cases across what you think your service boundaries will be. For our music shop, for example, imagine what happens when a customer se...",qwen2.5:latest,2025-11-02 23:48:53,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Mapping Continuous Integration to Microservices,Continuous Integration (CI),"#### Continuous Integration (CI)
Background context explaining the concept. CI has been around for several years and is crucial for keeping team members synchronized by ensuring that newly checked-in code properly integrates with existing code. The core goal of CI is to automate the process of verifying code, typically through compiling it and running tests.
:p What is continuous integration?
??x
Continuous Integration (CI) is a practice where developers frequently merge their working copies into a shared mainline. This helps in detecting integration issues early and keeping everyone's work synchronized. The CI server automatically detects changes, checks them out, compiles the code, runs automated tests, and ensures that the new code integrates with existing code.
??x",779,We have many more moving parts to get into production now. So next up we’ll dive into the world of deployment. Chapter 6. Deployment Deploying a monolithic application is a fairly straightforward proc...,qwen2.5:latest,2025-11-02 23:49:00,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Mapping Continuous Integration to Microservices,Benefits of Continuous Integration,"#### Benefits of Continuous Integration
Explanation: CI provides several benefits such as fast feedback on code quality, automation of binary artifact creation, version control of build artifacts, and traceability back to specific code versions. These benefits make CI a valuable practice in software development.
:p What are the key benefits of continuous integration?
??x
The key benefits of CI include:
- Fast feedback on the quality of code
- Automation of creating binary artifacts (like deploying services)
- Version control of all code required for building an artifact
- Traceability from deployed artifacts back to specific code versions
- Visibility into which tests were run on the code and artifacts

Code example: This is more about understanding the benefits, but if we consider a simple CI server script, it might look like this:
```bash
#!/bin/bash
# Script for performing basic CI tasks
git pull origin main  # Pull latest changes from the repository
mvn clean install    # Run Maven to compile and run tests
if [ $? -eq 0 ]; then
  echo ""Build successful""
else
  echo ""Build failed, check logs""
fi
```
??x",1123,We have many more moving parts to get into production now. So next up we’ll dive into the world of deployment. Chapter 6. Deployment Deploying a monolithic application is a fairly straightforward proc...,qwen2.5:latest,2025-11-02 23:49:00,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Mapping Continuous Integration to Microservices,Artifacts in Continuous Integration for Microservices,"#### Artifacts in Continuous Integration for Microservices
Explanation: In the context of microservices, artifacts can be various outputs such as Docker images or JAR files. These are created during the CI process and used for deployment.
:p What are artifacts in continuous integration for microservices?
??x
Artifacts in continuous integration for microservices refer to the output products that are generated from the build process. Common types of artifacts include:
- Docker images: For containerized applications
- JAR/WAR files: For Java-based applications

These artifacts are version-controlled and stored in repositories (either provided by CI tools or external systems) to ensure they can be reused across deployments.
??x",733,We have many more moving parts to get into production now. So next up we’ll dive into the world of deployment. Chapter 6. Deployment Deploying a monolithic application is a fairly straightforward proc...,qwen2.5:latest,2025-11-02 23:49:00,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Mapping Continuous Integration to Microservices,Jez Humble’s Three Questions on CI,"#### Jez Humble’s Three Questions on CI

Background context: Jez Humble, a prominent figure in DevOps and continuous integration (CI), suggests three essential questions to test one's understanding of CI principles. These questions are:
1. Do you check in to mainline once per day?
2. Do you have a suite of tests to validate your changes?
3. When the build is broken, is it the #1 priority of the team to fix it?

Explanation: These questions ensure that integration and testing practices are followed rigorously.

:p Jez Humble’s first question about daily check-ins aims to assess which practice ensures frequent integration with others' code.
??x
The frequency of check-ins and their impact on integration. Frequent check-ins (e.g., once per day) reduce the complexity of integrating multiple changes, making future merges less error-prone.

Code example: A simple git workflow:
```shell
git commit -m ""Commit message""
git push origin mainline
```
x??

#### Suite of Tests in CI

Background context: The second question about a suite of tests ensures that code changes are verified for correct behavior, beyond just syntactical correctness. This is crucial to maintain the integrity of the system.

Explanation: A comprehensive test suite helps catch bugs early and ensures that new features or bug fixes do not inadvertently break existing functionality.

:p Jez Humble’s second question focuses on ensuring what aspect of CI?
??x
The focus here is on having a robust suite of tests (unit, integration, end-to-end) to verify that the code changes behave as expected. Tests are essential for catching defects early in the development cycle.

Code example: A simple unit test using JUnit:
```java
import static org.junit.Assert.assertEquals;
import org.junit.Test;

public class ExampleTest {
    @Test
    public void testAddition() {
        int result = 1 + 2;
        assertEquals(3, result);
    }
}
```
x??

#### Build Breakage Prioritization

Background context: The third question about build prioritization ensures that the team treats a broken build as the highest priority task. This practice helps in maintaining a reliable and consistent build environment.

Explanation: A failing build indicates potential issues that need immediate attention to prevent cascading failures or delays in deployment.

:p Jez Humble’s third question addresses what is the top priority when the CI build fails?
??x
The answer is to prioritize fixing a broken build as the topmost task. This ensures that the integration environment remains reliable and that any issues are addressed promptly to avoid further complications.

Code example: Handling a build failure in an automated system:
```java
public class BuildFailureHandler {
    public void handleFailure() {
        // Logic to notify team and fix the issue
        System.out.println(""Handling build failure. Prioritizing fix."");
    }
}
```
x??",2899,I really like Jez Humble’s three questions he asks people to test if they really understand what CI is about: Do you check in to mainline once per day? You need to make sure your code integrates. If y...,qwen2.5:latest,2025-11-02 23:49:13,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Mapping Continuous Integration to Microservices,Single Repository for Microservices CI,"#### Single Repository for Microservices CI

Background context: The text discusses a scenario where all microservices are managed in one single repository, leading to a unified CI pipeline that integrates and tests changes across all services.

Explanation: While this approach simplifies the setup by reducing the number of repositories and maintaining a unified build process, it can be less efficient for incremental changes as it involves testing multiple services even if only one has been modified.

:p How does Jez Humble suggest managing microservices in CI?
??x
Jez Humble suggests that all microservices can be managed under a single repository with a single CI build. This approach simplifies the setup but can lead to inefficiencies when changes are made to individual services, as the entire suite of tests and builds might still run.

Code example: A simple CI configuration in Jenkins for a monorepo:
```groovy
pipeline {
    agent any
    stages {
        stage('Build') {
            steps {
                // Run all necessary build and test commands
                sh 'mvn clean install'
            }
        }
        stage('Test') {
            steps {
                // Execute comprehensive tests across multiple services
                sh 'mvn test'
            }
        }
    }
}
```
x??",1319,I really like Jez Humble’s three questions he asks people to test if they really understand what CI is about: Do you check in to mainline once per day? You need to make sure your code integrates. If y...,qwen2.5:latest,2025-11-02 23:49:13,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Mapping Continuous Integration to Microservices,Microservices CI with Independent Builds,"#### Microservices CI with Independent Builds

Background context: An alternative approach to the single repository model is to map each microservice to its own CI build, ensuring that changes are tested and deployed independently.

Explanation: This method reduces unnecessary tests for unrelated services, making the build process more efficient. It aligns well with the goals of DevOps, where rapid iteration and deployment of individual components are prioritized.

:p How should microservices be mapped in a CI environment according to the text?
??x
Microservices should each have their own CI builds to ensure that changes are tested and deployed independently. This approach minimizes unnecessary tests for unrelated services, making the build process more efficient and aligned with DevOps practices.

Code example: A simple CI configuration for individual microservices:
```groovy
pipeline {
    agent any
    stages {
        stage('Build User Service') {
            steps {
                // Build user service specific commands
                sh 'mvn clean install -Puser-service'
            }
        }
        stage('Test User Service') {
            steps {
                // Test user service specific tests
                sh 'mvn test -Puser-service'
            }
        }
    }
}
```
x??",1313,I really like Jez Humble’s three questions he asks people to test if they really understand what CI is about: Do you check in to mainline once per day? You need to make sure your code integrates. If y...,qwen2.5:latest,2025-11-02 23:49:13,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Mapping Continuous Integration to Microservices,Cycle Time and Deployment Artifacts,"#### Cycle Time and Deployment Artifacts
In this context, organizations are concerned about how quickly changes can be moved from development to production (live). The challenge lies in determining which specific artifacts should be deployed without affecting others. A common issue is whether deploying a small change requires pushing all build services together, making it hard to pinpoint the exact dependencies.

:p What is the concern regarding cycle time and deployment?
??x
The concern is about the speed of moving changes from development to production (live) and accurately identifying which specific artifacts should be deployed without causing issues in other systems. Deploying everything together often becomes necessary due to difficulties in pinpointing exactly which services have changed based on commit messages alone.
```java
public class DeploymentChecker {
    public boolean checkChanges(String[] commits) {
        // Logic to identify changes in specific services
        return false;
    }
}
```
x??",1025,"This impacts our cycle time, the speed at which we can move a single change from development to live. More troubling, though, is knowing what artifacts should or shouldn’t be deployed. Do I now need t...",qwen2.5:latest,2025-11-02 23:49:23,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Mapping Continuous Integration to Microservices,Monolithic Build Approach,"#### Monolithic Build Approach
The monolithic approach involves a single source tree and multiple CI builds mapping to parts of this source tree. While it simplifies the check-in/check-out process by managing everything in one repository, it can lead to practices like checking in code for multiple services at once, which might couple different services.

:p What is a potential downside of the monolithic build approach?
??x
A potential downside is that it encourages bad practices such as checking in code for multiple services simultaneously, leading to coupled services. While this simplifies the repository management, it increases the risk of introducing dependencies between services unintentionally.
```java
public class MonolithicBuild {
    public void checkInMultipleServices(String[] serviceNames) {
        // Code that checks in changes for multiple services together
    }
}
```
x??",898,"This impacts our cycle time, the speed at which we can move a single change from development to live. More troubling, though, is knowing what artifacts should or shouldn’t be deployed. Do I now need t...",qwen2.5:latest,2025-11-02 23:49:23,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Mapping Continuous Integration to Microservices,Microservice Build Approach,"#### Microservice Build Approach
The preferred approach is having a single CI build per microservice, allowing quick validation and deployment of changes. Each microservice has its own source code repository and CI build process, making the change and test processes more focused.

:p Why is a microservice build approach favored?
??x
A microservice build approach is favored because it allows for quicker validation and deployment of changes by focusing on individual services. This avoids issues where a small change in one service can break others due to coupled codebases. Each microservice has its own repository, making ownership clearer and reducing the risk of unintended side effects.
```java
public class MicroserviceBuild {
    public void buildMicroservice(String serviceName) {
        // Build process for a single microservice
    }
}
```
x??",857,"This impacts our cycle time, the speed at which we can move a single change from development to live. More troubling, though, is knowing what artifacts should or shouldn’t be deployed. Do I now need t...",qwen2.5:latest,2025-11-02 23:49:23,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Mapping Continuous Integration to Microservices,Source Code Repository and CI Builds,"#### Source Code Repository and CI Builds
Each microservice has its own source code repository, leading to its own CI build. This setup ensures that changes can be validated independently before being deployed. Tests should also live in the same source control as the service's code.

:p What is the benefit of having a separate source code repository for each microservice?
??x
The benefit is that it allows independent validation and deployment of changes, reducing the risk of breaking other services due to coupled codebases. Each microservice can be tested and built in isolation, ensuring clarity on ownership and reducing the chances of unintended side effects.
```java
public class MicroserviceRepository {
    public void createBuild(String serviceName) {
        // Create a CI build for the specified microservice
    }
}
```
x??",840,"This impacts our cycle time, the speed at which we can move a single change from development to live. More troubling, though, is knowing what artifacts should or shouldn’t be deployed. Do I now need t...",qwen2.5:latest,2025-11-02 23:49:23,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Mapping Continuous Integration to Microservices,Continuous Delivery Integration,"#### Continuous Delivery Integration
Beyond CI, continuous delivery ensures that validated changes can be deployed into production. This involves fully automated processes to create deployable artifacts and aligns well with the microservices approach.

:p How does continuous delivery fit into the overall process?
??x
Continuous delivery fits by ensuring that validated changes from the CI process are automatically deployed into production environments. It integrates seamlessly with the microservice architecture, allowing for rapid and reliable deployment of changes without manual intervention.
```java
public class ContinuousDelivery {
    public void deployToProduction(String serviceName) {
        // Automated deployment logic for a microservice
    }
}
```
x??

---",776,"This impacts our cycle time, the speed at which we can move a single change from development to live. More troubling, though, is knowing what artifacts should or shouldn’t be deployed. Do I now need t...",qwen2.5:latest,2025-11-02 23:49:23,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Build Pipelines and Continuous Delivery,Build Pipelines and Stages,"#### Build Pipelines and Stages

Build pipelines break down a build into multiple stages, each serving a specific purpose. This approach helps manage different types of tests more efficiently, especially when dealing with fast and slow tests.

:p What is the benefit of using build pipelines?
??x
Using build pipelines allows for faster feedback loops by segregating fast and slow tests. It ensures that only necessary tests are run based on the results of previous stages, reducing unnecessary wait times and improving overall efficiency.
For example, if quick unit tests fail, there's no need to proceed with integration or system tests.

```java
public class ExamplePipeline {
    public void runTests() {
        // Run fast unit tests
        if (fastUnitTestsPass()) {
            // Proceed to slow integration tests if needed
            if (slowIntegrationTestsPass()) {
                System.out.println(""All tests passed."");
            } else {
                System.out.println(""Slow integration tests failed."");
            }
        } else {
            System.out.println(""Fast unit tests failed."");
        }
    }

    private boolean fastUnitTestsPass() {
        // Logic to run and verify fast unit tests
        return true;
    }

    private boolean slowIntegrationTestsPass() {
        // Logic to run and verify slow integration tests
        return false;
    }
}
```
x??",1400,"Build Pipelines and Continuous Delivery Very early on in using continuous integration, we realized the value in sometimes having multiple stages inside a build. Tests are a very common case where this...",qwen2.5:latest,2025-11-02 23:49:35,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Build Pipelines and Continuous Delivery,Continuous Delivery (CD),"#### Continuous Delivery (CD)

Continuous delivery extends the concept of build pipelines by treating each check-in as a potential release candidate. It emphasizes constant feedback on production readiness.

:p What is continuous delivery?
??x
Continuous delivery involves automatically deploying code changes to a staging environment and making them available for deployment to production with minimal manual intervention. This approach ensures that every code change can be released to production at any time.
Tools that support CD allow you to model the entire path from check-in to production, providing visibility into each stage of the release process.

```java
public class ContinuousDeliveryTool {
    public void deployToProduction() {
        // Model and visualize the deployment pipeline
        if (passedAllTests()) {
            System.out.println(""Deploying code to production."");
        } else {
            System.out.println(""Code failed tests and cannot be deployed."");
        }
    }

    private boolean passedAllTests() {
        // Check if all necessary tests have been passed
        return true;
    }
}
```
x??",1140,"Build Pipelines and Continuous Delivery Very early on in using continuous integration, we realized the value in sometimes having multiple stages inside a build. Tests are a very common case where this...",qwen2.5:latest,2025-11-02 23:49:35,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Build Pipelines and Continuous Delivery,Build Artifacts in Pipelines,"#### Build Artifacts in Pipelines

Build artifacts are essential components that move through the various stages of a pipeline. They represent intermediate or final versions of software being developed.

:p What role do build artifacts play in pipelines?
??x
Build artifacts serve as the central component that moves through different stages of the pipeline, from initial development to production deployment. Each artifact represents a state of the project and is used for various checks and tests.
For example, an artifact could be a JAR file or a Docker image.

```java
public class ArtifactHandler {
    public void handleArtifact() {
        // Generate and pass artifacts through different stages
        String artifact = generateArtifact();
        if (stage1(artifact)) {
            if (stage2(artifact)) {
                System.out.println(""Artifact passed all stages."");
            } else {
                System.out.println(""Failed at stage 2."");
            }
        } else {
            System.out.println(""Failed at stage 1."");
        }
    }

    private String generateArtifact() {
        // Logic to create and return an artifact
        return ""artifact-1.0.jar"";
    }

    private boolean stage1(String artifact) {
        // Stage-specific logic for testing the artifact
        return true;
    }

    private boolean stage2(String artifact) {
        // Further tests on the artifact
        return false;
    }
}
```
x??",1452,"Build Pipelines and Continuous Delivery Very early on in using continuous integration, we realized the value in sometimes having multiple stages inside a build. Tests are a very common case where this...",qwen2.5:latest,2025-11-02 23:49:35,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Build Pipelines and Continuous Delivery,Multi-stage Pipelines in Microservices,"#### Multi-stage Pipelines in Microservices

In a microservices architecture, each service has its own pipeline to ensure independent deployment and release.

:p Why is it important to have one pipeline per service?
??x
Having one pipeline per service ensures that changes can be deployed independently without affecting other services. This approach allows for faster and more frequent deployments while maintaining the stability of the system.
For example, Service A might update its database schema, while Service B updates its API endpoints.

```java
public class MicroservicePipeline {
    public void deployService() {
        // Define a pipeline for each microservice
        if (deployServiceA()) {
            System.out.println(""Service A deployed successfully."");
        } else {
            System.out.println(""Failed to deploy Service A."");
        }

        if (deployServiceB()) {
            System.out.println(""Service B deployed successfully."");
        } else {
            System.out.println(""Failed to deploy Service B."");
        }
    }

    private boolean deployServiceA() {
        // Logic for deploying and testing Service A
        return true;
    }

    private boolean deployServiceB() {
        // Logic for deploying and testing Service B
        return false;
    }
}
```
x??

---",1318,"Build Pipelines and Continuous Delivery Very early on in using continuous integration, we realized the value in sometimes having multiple stages inside a build. Tests are a very common case where this...",qwen2.5:latest,2025-11-02 23:49:35,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Custom Images,Initial Services on Larger Sides,"---
#### Initial Services on Larger Sides
When starting a new project, especially greenfield ones, there is often significant churn in determining service boundaries. This period requires flexibility and adaptability to ensure stable services eventually.
:p During the initial phase of a new project, why might it be beneficial to keep your services larger?
??x
During the initial phase of a new project, keeping services larger can facilitate experimentation and exploration of the domain without frequent reorganization. Changes across service boundaries are more likely during this phase, making it easier to manage them when everything is in one build.
x??",660,"And the Inevitable Exceptions As with all good rules, there are exceptions we need to consider too. The “one microservice per build” approach is absolutely something you should aim for, but are there ...",qwen2.5:latest,2025-11-02 23:49:42,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Custom Images,Transitionary Step in Microservices,"#### Transitionary Step in Microservices
As service APIs stabilize, transitioning from monolithic builds to separate builds for each microservice can improve maintainability and scalability.
:p When should you consider separating services into individual builds?
??x
You should consider separating services into individual builds once the service APIs have stabilized. This transition helps in improving maintainability and scalability by allowing independent deployments of each service.
x??",492,"And the Inevitable Exceptions As with all good rules, there are exceptions we need to consider too. The “one microservice per build” approach is absolutely something you should aim for, but are there ...",qwen2.5:latest,2025-11-02 23:49:42,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Custom Images,Platform-Specific Artifacts,"#### Platform-Specific Artifacts
Different technology stacks come with their own artifacts (e.g., JAR files for Java, gems for Ruby). These artifacts are often sufficient but may require additional software installation and configuration using tools like Puppet or Chef.
:p Why might you need to use tools like Puppet or Chef when deploying microservices?
??x
You might need to use tools like Puppet or Chef because platform-specific artifacts alone might not be enough. For example, a Ruby gem or Python egg might require running inside an Apache or Nginx process managed by another tool, which can be handled using configuration management tools.
x??",652,"And the Inevitable Exceptions As with all good rules, there are exceptions we need to consider too. The “one microservice per build” approach is absolutely something you should aim for, but are there ...",qwen2.5:latest,2025-11-02 23:49:42,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Custom Images,Operating System Artifacts,"#### Operating System Artifacts
To avoid the complexities of technology-specific artifacts, you can create native operating system artifacts that are more portable and easier to manage across different environments.
:p What is the advantage of creating operating system artifacts in microservices?
??x
The advantage of creating operating system artifacts is that they are more portable and easier to manage across different environments. This approach reduces the complexity associated with deploying services built on different technology stacks, making the deployment process smoother and more uniform.
x??

---",613,"And the Inevitable Exceptions As with all good rules, there are exceptions we need to consider too. The “one microservice per build” approach is absolutely something you should aim for, but are there ...",qwen2.5:latest,2025-11-02 23:49:42,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Custom Images,OS-Specific Artifacts,"#### OS-Specific Artifacts
Background context explaining the concept. When deploying software, it's common to use artifacts native to the operating system (OS) for packaging and installation. This approach leverages the tools provided by the OS, which can handle tasks like dependency resolution, uninstallation, and package management.

The advantages include:

- **Ease of Deployment**: Using native tools simplifies deployment since you don't need to reinvent the wheel.
- **Automatic Dependency Management**: The OS package manager handles dependencies, reducing manual configuration work.
- **Package Repositories**: Tools like FPM for Linux and Chocolatey NuGet for Windows can push packages to repositories.

However, there are downsides:

- **Complexity in Creation**: Creating artifacts for different OSes can be challenging. For example, using tools like FPM or Chocolatey can simplify this process.
- **Multiple Operating Systems**: Managing artifacts for multiple OSes can increase complexity and overhead.

:p What is the main advantage of using OS-specific artifacts?
??x
The main advantages are ease of deployment and automatic dependency management provided by the native OS package managers. These tools handle tasks like installing dependencies, uninstalling packages, and managing repositories.
x??",1317,"For example, for a RedHat– or CentOS-based system, I might build RPMs; for Ubuntu, I might build a deb package; or for Windows, an MSI. The advantage of using OS-specific artifacts is that from a depl...",qwen2.5:latest,2025-11-02 23:49:50,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Custom Images,Custom Images,"#### Custom Images
Background context explaining the concept. When deploying software, particularly in cloud environments like AWS, creating custom images can reduce the time needed to provision servers and configure them for specific applications.

If applicable, add code examples with explanations:
```java
public class Example {
    // code here
}
```
:p What is a challenge of using automated configuration management systems?
??x
A significant challenge is the time taken to run scripts on a machine during provisioning. For example, in a Java application deployment scenario, configuring a server to allow for deployment might take a considerable amount of time when using Puppet or Chef.
x??",699,"For example, for a RedHat– or CentOS-based system, I might build RPMs; for Ubuntu, I might build a deb package; or for Windows, an MSI. The advantage of using OS-specific artifacts is that from a depl...",qwen2.5:latest,2025-11-02 23:49:50,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Custom Images,Deployment of Microservices Using OS-Based Package Management,"#### Deployment of Microservices Using OS-Based Package Management
Background context explaining the concept. Moving to OS-based package management can simplify the deployment approach, especially for teams working with microservices and disparate technology stacks.

:p What is an advantage of moving to OS-based package management?
??x
An advantage is simplifying deployment by reducing the complexity of big, complex deployment scripts. This makes it easier to manage microservices deployed across different technology stacks.
x??",533,"For example, for a RedHat– or CentOS-based system, I might build RPMs; for Ubuntu, I might build a deb package; or for Windows, an MSI. The advantage of using OS-specific artifacts is that from a depl...",qwen2.5:latest,2025-11-02 23:49:50,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Custom Images,Chocolatey NuGet for Windows,"#### Chocolatey NuGet for Windows
Background context explaining the concept. Chocolatey NuGet extends the ideas of package managers in Linux, providing a more unified approach for managing tools and services on Windows.

:p What is one downside of deploying onto multiple operating systems?
??x
One downside is the increased overhead of managing artifacts for different OSes. This can lead to complex deployment processes if you're not using unified or standardized images across your infrastructure.
x??",504,"For example, for a RedHat– or CentOS-based system, I might build RPMs; for Ubuntu, I might build a deb package; or for Windows, an MSI. The advantage of using OS-specific artifacts is that from a depl...",qwen2.5:latest,2025-11-02 23:49:50,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Custom Images,Unifying Operating Systems,"#### Unifying Operating Systems
Background context explaining the concept. Using a single operating system across all machines can reduce variations in behavior and simplify deployment and maintenance tasks.

:p Why should teams consider unifying their operating systems?
??x
Teams should consider unifying their operating systems to reduce variations in behavior from one machine to another, which simplifies deployment and maintenance tasks.
x??

---",452,"For example, for a RedHat– or CentOS-based system, I might build RPMs; for Ubuntu, I might build a deb package; or for Windows, an MSI. The advantage of using OS-specific artifacts is that from a depl...",qwen2.5:latest,2025-11-02 23:49:50,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Custom Images,Virtual Machine Image Creation,"#### Virtual Machine Image Creation
Virtual machine images can be created to include common dependencies, reducing setup time. Building these images takes time but can significantly save on deployment times. The process involves creating a custom image that includes tools like collectd, logstash, and nagios for monitoring and statistics gathering.
:p What is the purpose of creating a virtual machine image with common dependencies?
??x
The primary purpose is to reduce setup and deployment times by pre-installing commonly used software on a VM image. This allows developers or operators to quickly spin up new instances without needing to install these tools repeatedly, streamlining the development and production processes.
x??",733,"The first thing I need to do is install the Oracle JVM to run my Java application. I’ve seen this simple process take around five minutes, with a couple of minutes taken up by the machine being provis...",qwen2.5:latest,2025-11-02 23:49:58,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Custom Images,Baking Dependencies into Images,"#### Baking Dependencies into Images
Baking dependencies like collectd for OS stats, logstash for logging, and nagios monitoring components into a custom virtual machine image can significantly speed up deployment times. This approach is particularly useful in environments where services need to be deployed frequently or in multiple instances.
:p What are the common tools that can be baked into a VM image?
??x
Common tools that can be baked into a VM image include collectd for OS stats, logstash for log aggregation, and appropriate components of nagios for monitoring. These tools help streamline the deployment process by pre-installing necessary software on the machine.
x??",682,"The first thing I need to do is install the Oracle JVM to run my Java application. I’ve seen this simple process take around five minutes, with a couple of minutes taken up by the machine being provis...",qwen2.5:latest,2025-11-02 23:49:58,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Custom Images,Drawbacks of Baking Dependencies,"#### Drawbacks of Baking Dependencies
While baking dependencies into images can speed up deployment times, it also comes with drawbacks such as lengthy build times and potentially large image sizes. These factors need to be considered when deciding whether to use this approach in development or production environments.
:p What are some drawbacks of building custom VM images that include common dependencies?
??x
Some drawbacks include long build times for the initial image creation, which can delay deployment processes if not managed properly. Additionally, large image sizes can complicate network transfers and storage requirements. These factors need careful consideration when deciding on an image-building strategy.
x??",729,"The first thing I need to do is install the Oracle JVM to run my Java application. I’ve seen this simple process take around five minutes, with a couple of minutes taken up by the machine being provis...",qwen2.5:latest,2025-11-02 23:49:58,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Custom Images,Packer Tool Introduction,"#### Packer Tool Introduction
Packer is a tool that simplifies the process of creating machine images for different platforms from a single configuration script. It supports multiple virtualization providers such as AWS, VMWare, Rackspace Cloud, and more, making it easier to manage cross-platform deployments.
:p What does Packer simplify in the context of image creation?
??x
Packer simplifies the process by allowing the creation of machine images for different platforms from a single configuration script. It supports various virtualization providers like AWS, VMWare, Rackspace Cloud, and Vagrant, facilitating consistent deployment across multiple environments.
x??",672,"The first thing I need to do is install the Oracle JVM to run my Java application. I’ve seen this simple process take around five minutes, with a couple of minutes taken up by the machine being provis...",qwen2.5:latest,2025-11-02 23:49:58,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Custom Images,Packer's Support for Multiple Platforms,"#### Packer's Support for Multiple Platforms
Packer can generate images for different cloud platforms and development tools using the same configuration. For instance, it can create an image suitable for production on AWS while simultaneously creating a matching Vagrant image for local development and testing.
:p How does Packer help manage multiple deployment environments?
??x
Packer helps by enabling the creation of machine images for various deployment environments from a single configuration script. This means you can generate an image for your production AWS environment and a corresponding Vagrant image for local development, all with the same setup.
x??

---",672,"The first thing I need to do is install the Oracle JVM to run my Java application. I’ve seen this simple process take around five minutes, with a couple of minutes taken up by the machine being provis...",qwen2.5:latest,2025-11-02 23:49:58,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Environments,Virtual Machine Images as Artifacts,"#### Virtual Machine Images as Artifacts
Background context: The text discusses using virtual machine (VM) images to encapsulate dependencies and services, which speeds up deployment. Netflix has adopted this model for AWS AMIs. By baking service dependencies into the image, it abstracts away differences in technology stacks.
:p What is the advantage of creating VM images that contain all necessary dependencies?
??x
Creating VM images with all dependencies baked in ensures faster spin-up times and simplifies the process by abstracting away the underlying technology stack differences. This allows developers to focus on automating image creation and deployment, making the environment more consistent.
x??",711,"Images as Artifacts So we can create virtual machine images that bake in dependencies to speed up feedback, but why stop there? We could go further, bake our service into the image itself, and adopt t...",qwen2.5:latest,2025-11-02 23:50:07,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Environments,Immutable Servers,"#### Immutable Servers
Background context: The concept of immutable servers involves storing all configuration in source control and ensuring that no changes can be made directly to a running server. Changes must go through a build pipeline to create new machines, preventing configuration drift.
:p How does an immutable server approach help prevent configuration drift?
??x
Immutable servers help prevent configuration drift by enforcing that any change to the system goes through a controlled process (e.g., code changes in source control and a deployment pipeline). This means no manual changes can be made directly on running servers, ensuring that the production environment always reflects the state defined in version-controlled configurations.
x??",756,"Images as Artifacts So we can create virtual machine images that bake in dependencies to speed up feedback, but why stop there? We could go further, bake our service into the image itself, and adopt t...",qwen2.5:latest,2025-11-02 23:50:07,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Environments,Environments and Deployment Pipelines,"#### Environments and Deployment Pipelines
Background context: As software moves through different stages of the Continuous Delivery (CD) pipeline, it is deployed into various environments such as test, UAT, performance, and production. Each environment serves a specific purpose and can have distinct configurations and host setups.
:p Why do we need to consider multiple environments when deploying microservices?
??x
Multiple environments are necessary because each one serves a different purpose in the deployment lifecycle. For example, a test environment might be simpler with fewer hosts, while a production environment could require more complex setups like load balancing across data centers for durability. This variety helps catch issues early and ensures that the service behaves correctly under various conditions.
x??",831,"Images as Artifacts So we can create virtual machine images that bake in dependencies to speed up feedback, but why stop there? We could go further, bake our service into the image itself, and adopt t...",qwen2.5:latest,2025-11-02 23:50:07,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Environments,Session State Replication Issue,"#### Session State Replication Issue
Background context: The text describes an issue encountered when deploying Java web services into a clustered WebLogic application container, where session state replication failed due to changes in configuration between environments (test vs. production).
:p What lesson can be learned from the described issue with WebLogic session state?
??x
The key lesson is that environment differences can introduce subtle but critical issues, such as session state replication failure. To avoid these, it's essential to closely mirror the target environment during testing and ensure that all relevant configurations are consistent across different stages of deployment.
x??",702,"Images as Artifacts So we can create virtual machine images that bake in dependencies to speed up feedback, but why stop there? We could go further, bake our service into the image itself, and adopt t...",qwen2.5:latest,2025-11-02 23:50:07,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Environments,Managing Environments for Microservices,"#### Managing Environments for Microservices
Background context: The text highlights the complexity of managing multiple environments (e.g., test, UAT, performance, production) for microservices. Each environment can have distinct characteristics, requiring careful configuration management and balancing between fast feedback and production-like environments.
:p What is a key challenge in managing environments for microservices?
??x
A key challenge is balancing the need for environments that closely mimic production to catch issues early with the requirement for faster deployment cycles and shorter feedback loops. This balance can be complex, especially as the number of environments increases, requiring careful planning and automation.
x??

---",753,"Images as Artifacts So we can create virtual machine images that bake in dependencies to speed up feedback, but why stop there? We could go further, bake our service into the image itself, and adopt t...",qwen2.5:latest,2025-11-02 23:50:07,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Multiple Services Per Host,Service Configuration Management,"#### Service Configuration Management

Background context explaining the concept of managing service configurations for different environments. The key issue is to ensure that the configuration does not change fundamental service behavior and should be kept minimal. There are challenges with building artifacts per environment, as it can lead to deployment issues where the correct artifact may not always be used.

:p How should we handle configuration differences between environments to avoid problems?
??x
To manage configuration effectively, create a single artifact for all environments and keep configurations separate from the codebase. This could involve using properties files or parameterized install processes. Avoid building artifacts with configurations baked in because it can lead to deployment discrepancies where the wrong version of the software might be deployed.

```bash
# Example properties file (config.properties)
database.username=testuser
database.password=secret
```
x??",999,"Service Configuration Our services need some configuration. Ideally, this should be a small amount, and limited to those features that change from one environment to another, such as  what username an...",qwen2.5:latest,2025-11-02 23:50:16,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Multiple Services Per Host,Service-to-Host Mapping,"#### Service-to-Host Mapping

Background context explaining the concept of deploying services on hosts. The term ""host"" is used to refer to a unit of isolation, which could be a physical machine or a virtualized environment. The goal is to determine how many microservices should reside on a single host.

:p How do you decide how many microservices to deploy per host?
??x
The decision depends on various factors such as ease of management, cost efficiency, and the ability to scale independently. Deploying multiple services per host can simplify infrastructure management but may complicate monitoring, resource allocation, and deployment processes. Each service should ideally be deployed independently to maintain autonomy and avoid side effects.

```java
public class ServiceDeployer {
    public void deployServicesOnHost(String hostName, List<Service> services) {
        for (Service service : services) {
            service.deploy(hostName);
        }
    }
}
```
x??",978,"Service Configuration Our services need some configuration. Ideally, this should be a small amount, and limited to those features that change from one environment to another, such as  what username an...",qwen2.5:latest,2025-11-02 23:50:16,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Multiple Services Per Host,Multiple Services Per Host,"#### Multiple Services Per Host

Background context explaining the benefits and challenges of deploying multiple microservices on a single host. This model is attractive due to simplicity in management and cost savings but can introduce complexities related to monitoring, resource allocation, and deployment.

:p What are the benefits of having multiple services per host?
??x
Benefits include:
1. **Simplified Host Management**: Reduces the number of hosts that need to be managed.
2. **Cost Efficiency**: Minimizes the overhead associated with virtualization.
3. **Familiarity for Developers**: Simplifies local development and deployment practices.

However, challenges include:
- **Monitoring Complexity**: Difficulty in isolating resource usage per service.
- **Resource Contention**: Load on one service can affect others running on the same host.
- **Impact Analysis**: Host failures can have broader ripple effects.
- **Deployment Complexity**: Ensuring one deployment does not affect another.

```java
public class HostManager {
    public void manageHostResources(String hostName) {
        // Logic to monitor and allocate resources for multiple services on a single host
    }
}
```
x??

---",1204,"Service Configuration Our services need some configuration. Ideally, this should be a small amount, and limited to those features that change from one environment to another, such as  what username an...",qwen2.5:latest,2025-11-02 23:50:16,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Application Containers,Application Containers Overview,"#### Application Containers Overview
Background context explaining the concept of application containers. Multiple distinct services or applications can sit inside a single container, providing benefits such as clustering support and monitoring tools. However, this setup also has downsides including technology stack constraints and potential for slower spin-up times.
:p What are the key advantages of using application containers for deploying services?
??x
The key advantages include improved manageability through features like clustering and monitoring tools, and reduced overhead by sharing a single runtime environment across multiple applications. For example, running five Java services in one container means only one JVM is needed instead of five.
```java
// Example of setting up a simple embedded servlet container
public class EmbeddedServletContainer {
    public void start() {
        JettyServer jetty = new JettyServer();
        jetty.start();
        // More setup code here
    }
}
```
x??",1012,"Application Containers If you’re familiar with deploying .NET applications behind IIS or Java applications into a servlet container, you will be well acquainted with the model where multiple distinct ...",qwen2.5:latest,2025-11-02 23:50:24,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Application Containers,Downsides of Application Containers,"#### Downsides of Application Containers
The text mentions several downsides to using application containers, such as technology stack constraints, limited automation and management options, slow spin-up times for applications, and added resource overhead.
:p What are some of the main drawbacks associated with application containers?
??x
Some key drawbacks include constrained technology choices, reduced flexibility in automation and management solutions, slower startup times for applications, and additional resource overhead due to their commercial nature or inherent complexity. For instance, embedded Jetty has faster spin-up times compared to full JVMs but still adds some overhead.
```java
// Example of a simple Jetty server start method
public class SimpleJettyServer {
    public void start() {
        // Initialize and start the Jetty server
        Server jetty = new Server();
        jetty.start();
    }
}
```
x??",932,"Application Containers If you’re familiar with deploying .NET applications behind IIS or Java applications into a servlet container, you will be well acquainted with the model where multiple distinct ...",qwen2.5:latest,2025-11-02 23:50:24,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Application Containers,Self-Contained Deployable Microservices,"#### Self-Contained Deployable Microservices
The text suggests considering self-contained deployable microservices as artifacts. Examples include using Nancy for .NET and embedded Jetty in Java, which can provide lightweight solutions with better scalability and faster startup times.
:p Why might self-contained microservices be preferred over application containers?
??x
Self-contained microservices offer more flexibility in technology choices and automation options, potentially faster spin-up times, and reduced resource overhead. They allow services to run independently without the constraints of a shared process, making them easier to manage and scale. For example, using embedded Jetty for serving static content demonstrates how lightweight solutions can operate at scale.
```java
// Example of embedding Jetty in a Java application
public class EmbeddedJettyApplication {
    public void start() {
        // Initialize and configure Jetty server
        Server jetty = new Server();
        HttpConfiguration httpConfig = new HttpConfiguration();
        // Start the server
        jetty.start();
    }
}
```
x??

---",1131,"Application Containers If you’re familiar with deploying .NET applications behind IIS or Java applications into a servlet container, you will be well acquainted with the model where multiple distinct ...",qwen2.5:latest,2025-11-02 23:50:24,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Single Service Per Host,Single Service Per Host Model,"#### Single Service Per Host Model
Background context: The single service per host model aims to simplify monitoring, remediation, and scaling while reducing single points of failure. This model places one microservice on each host, ensuring that an outage in one host impacts only one service.
:p What is the primary benefit of using a single-service-per-host model?
??x
The primary benefits include simpler monitoring and remediation, reduced complexity in dealing with failures, easier scaling of individual services, and enhanced security management. Each host runs only one microservice, which simplifies troubleshooting and resource allocation.
x??",654,"Single Service Per Host With a single-service-per-host model shown in  Figure 6-8 , we avoid side effects of multiple hosts living on a single host, making monitoring and remediation much simpler. We ...",qwen2.5:latest,2025-11-02 23:50:33,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Single Service Per Host,Reduced Complexity Through Single Service Per Host,"#### Reduced Complexity Through Single Service Per Host
Background context: Implementing a single-service-per-host model helps reduce the overall system's complexity by isolating each service on its own host. This isolation makes it easier to manage failures, scale services independently, and handle security concerns.
:p How does implementing a single-service-per-host help in reducing system complexity?
??x
Implementing a single-service-per-host reduces complexity because each microservice runs independently on its own host. This setup simplifies fault management as outages are isolated to individual services rather than affecting multiple services hosted on the same machine. It also eases scaling, as you can scale each service independently without impacting others.
x??",781,"Single Service Per Host With a single-service-per-host model shown in  Figure 6-8 , we avoid side effects of multiple hosts living on a single host, making monitoring and remediation much simpler. We ...",qwen2.5:latest,2025-11-02 23:50:33,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Single Service Per Host,Deployment Techniques with Single Service Per Host,"#### Deployment Techniques with Single Service Per Host
Background context: The single-service-per-host model opens up opportunities for using alternative deployment techniques such as image-based deployments or the immutable server pattern. These methods can further enhance flexibility and reliability in microservice architectures.
:p What are some advantages of using image-based deployments or the immutable server pattern with a single service per host?
??x
Using image-based deployments or the immutable server pattern with a single service per host allows for consistent, repeatable deployments that minimize configuration drift. This approach ensures that each host runs exactly the same version of the service, reducing the risk of bugs and inconsistencies across different environments.
```java
// Example of an Immutable Server Pattern in Pseudo-code
public class DeploymentManager {
    public void updateService(String serviceName) throws Exception {
        // Create a new image with updated code
        Image newImage = createNewImage(serviceName);
        
        // Stop the old service instance on the host
        stopServiceInstance(newImage.serviceName);
        
        // Start the new service instance using the updated image
        startServiceInstance(newImage.serviceName, newImage.imageId);
    }
}
```
x??",1340,"Single Service Per Host With a single-service-per-host model shown in  Figure 6-8 , we avoid side effects of multiple hosts living on a single host, making monitoring and remediation much simpler. We ...",qwen2.5:latest,2025-11-02 23:50:33,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Single Service Per Host,Increased Number of Hosts and Management Challenges,"#### Increased Number of Hosts and Management Challenges
Background context: While the single-service-per-host model offers numerous benefits, it also introduces challenges such as increased management overhead and potential cost implications. More hosts mean more servers to manage, which can increase operational complexity.
:p What are some downsides of having a high number of hosts in a microservice architecture?
??x
The downsides include higher server management costs, more complex monitoring and maintenance, and increased risk associated with managing multiple systems. Each additional host adds to the overall operational overhead, requiring more resources for setup, maintenance, and security.
x??",709,"Single Service Per Host With a single-service-per-host model shown in  Figure 6-8 , we avoid side effects of multiple hosts living on a single host, making monitoring and remediation much simpler. We ...",qwen2.5:latest,2025-11-02 23:50:33,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Single Service Per Host,Preferred Model for Microservices,"#### Preferred Model for Microservices
Background context: The single-service-per-host model is favored because it simplifies the architecture by isolating services, making troubleshooting easier and reducing the risk of inter-service conflicts. However, it's acknowledged that this approach may not be feasible in all cases due to resource constraints or existing infrastructure.
:p Why is the single service per host model preferred for microservices?
??x
The single service per host model is preferred because it isolates each service, making it easier to manage and scale independently. This setup also simplifies fault isolation and enhances security by focusing on a single service at a time. However, its implementation may depend on the availability of suitable infrastructure and resources.
x??

---",808,"Single Service Per Host With a single-service-per-host model shown in  Figure 6-8 , we avoid side effects of multiple hosts living on a single host, making monitoring and remediation much simpler. We ...",qwen2.5:latest,2025-11-02 23:50:33,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Traditional Virtualization,Platform as a Service (PaaS) Overview,"#### Platform as a Service (PaaS) Overview
PaaS allows users to work at a higher-level abstraction, focusing on deploying applications rather than managing infrastructure. Most PaaS solutions rely on taking technology-specific artifacts and automatically provisioning them for you. Some platforms handle scaling transparently, but more often they provide control over the number of nodes while handling other aspects.
:p What is Platform as a Service (PaaS)?
??x
PaaS provides a platform to deploy applications without managing infrastructure. It abstracts away many details, allowing developers to focus on their application logic rather than deployment and scaling issues. Examples include Heroku for Java WAR files or Ruby gems.
```java
// Example of deploying an artifact using PaaS (pseudo-code)
public class DeployApplication {
    public void deployArtifact(String artifactPath) {
        // Code to upload and run the artifact
        System.out.println(""Deploying application: "" + artifactPath);
    }
}
```
x??",1020,"Platform as a Service When using a platform as a service (PaaS), you are working at a higher-level abstraction than at a single host. Most of these platforms rely on taking a technology-specific artif...",qwen2.5:latest,2025-11-02 23:50:44,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Traditional Virtualization,Hosted vs. Self-Hosted PaaS Solutions,"#### Hosted vs. Self-Hosted PaaS Solutions
Hosted PaaS solutions are fully managed by a third party, offering services like automatic scaling and database management. However, they can be less flexible when your application requirements deviate from the standard use cases. Self-hosted solutions exist but are generally less mature.
:p What are the main differences between hosted and self-hosted PaaS?
??x
Hosted PaaS is fully managed by a third party, providing scalable services out of the box. Self-hosted PaaS requires more manual configuration and maintenance. Hosted options like Heroku offer easy setup but may not fit nonstandard applications as well.
```java
// Example of choosing between hosted and self-hosted (pseudo-code)
public class PaaSChoice {
    public String choosePaaS(String applicationType) {
        if (""nonStandard"".equals(applicationType)) {
            return ""Self-Hosted"";
        } else {
            return ""Heroku"";
        }
    }
}
```
x??",976,"Platform as a Service When using a platform as a service (PaaS), you are working at a higher-level abstraction than at a single host. Most of these platforms rely on taking a technology-specific artif...",qwen2.5:latest,2025-11-02 23:50:44,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Traditional Virtualization,Automation in DevOps,"#### Automation in DevOps
Automation is crucial for managing and deploying applications as the number of services increases. Manual management becomes impractical with multiple servers and services, but automation can keep overhead constant regardless of the number of hosts.
:p Why is automation important in modern software development?
??x Automation is essential because it reduces manual intervention, which minimizes human error and allows scaling without increasing work proportionally. With more moving parts (services), automation ensures that deployment, monitoring, and management tasks are handled efficiently.
```java
// Example of an automated deployment script (pseudo-code)
public class AutomatedDeployment {
    public void deployServices(List<Service> services) {
        for (Service service : services) {
            System.out.println(""Deploying "" + service.getName());
            // Code to handle actual deployment
        }
    }
}
```
x??",964,"Platform as a Service When using a platform as a service (PaaS), you are working at a higher-level abstraction than at a single host. Most of these platforms rely on taking a technology-specific artif...",qwen2.5:latest,2025-11-02 23:50:44,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Traditional Virtualization,Managing Services with PaaS,"#### Managing Services with PaaS
PaaS solutions like Heroku can simplify the process of deploying applications, including database management. However, they may not always fit every application's needs due to their one-size-fits-all approach.
:p How do PaaS solutions typically handle services and databases?
??x PaaS platforms often provide pre-configured environments for running your application, such as Heroku which supports Java WAR files or Ruby gems directly. They also offer managed database services. However, these solutions may not be ideal if your application has unique requirements that deviate from the standard use cases.
```java
// Example of deploying a service on Heroku (pseudo-code)
public class DeployOnHeroku {
    public void deployApplication(String appName) {
        System.out.println(""Deploying "" + appName + "" to Heroku"");
        // Code for actual deployment
    }
}
```
x??",907,"Platform as a Service When using a platform as a service (PaaS), you are working at a higher-level abstraction than at a single host. Most of these platforms rely on taking a technology-specific artif...",qwen2.5:latest,2025-11-02 23:50:44,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Traditional Virtualization,Overhead in Multi-Service Environments,"#### Overhead in Multi-Service Environments
In environments with many services, managing each service individually can become cumbersome. Automation tools help manage the overhead, making it possible to scale and maintain a large number of services without increasing manual effort.
:p What is the main challenge in managing multiple services?
??x The primary challenge in managing multiple services is the increased administrative burden as more services are added. Manual management can lead to inefficiencies and errors, but automation tools help keep this overhead constant by handling tasks like deployment, scaling, and monitoring.
```java
// Example of managing a list of services with automation (pseudo-code)
public class ServiceManager {
    public void manageServices(List<Service> services) {
        for (Service service : services) {
            System.out.println(""Managing "" + service.getName());
            // Code to handle management tasks like deployment and scaling
        }
    }
}
```
x??",1013,"Platform as a Service When using a platform as a service (PaaS), you are working at a higher-level abstraction than at a single host. Most of these platforms rely on taking a technology-specific artif...",qwen2.5:latest,2025-11-02 23:50:44,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Traditional Virtualization,Multiple Deployments and Services Management,"#### Multiple Deployments and Services Management
Background context explaining how managing multiple deployments, services, and logs can become complex without automation. Automation helps ensure developers remain productive by allowing self-service provisioning of individual or groups of services.

:p How does automation contribute to keeping the complexities of microservice architectures in check?
??x
Automation contributes by enabling tasks such as launching virtual machines, deploying code, and managing database changes automatically. This reduces manual intervention, freeing up developer time for other tasks. For example, using tools like Docker and Kubernetes can help automate container orchestration and service deployment.

```bash
# Example of starting a VM with Bash script
#!/bin/bash
echo ""Starting VM...""
# Logic to start the virtual machine here
```
x??",877,"That means multiple deployments to handle, services to monitor, logs to collect. Automation is essential. Automation is also how we can make sure that our developers still remain productive. Giving th...",qwen2.5:latest,2025-11-02 23:50:55,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Traditional Virtualization,Developer Self-Service Provisioning,"#### Developer Self-Service Provisioning
Context explaining the importance of developers having access to the same tool chain used in production, ensuring early detection of issues. This involves giving developers tools to provision and manage their services independently.

:p What is the key benefit of allowing developers to self-provision services using the same toolchain as production?
??x
The key benefit is that it ensures consistency between development and production environments, reducing discrepancies and potential bugs. Developers can quickly set up and tear down environments as needed, accelerating development cycles and improving productivity.

```bash
# Example Bash script for provisioning a service
#!/bin/bash
echo ""Provisioning Service...""
# Logic to provision the necessary services here, e.g., deploying code, setting up databases, etc.
```
x??",870,"That means multiple deployments to handle, services to monitor, logs to collect. Automation is essential. Automation is also how we can make sure that our developers still remain productive. Giving th...",qwen2.5:latest,2025-11-02 23:50:55,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Traditional Virtualization,Case Study: RealEstate.com.au (REA),"#### Case Study: RealEstate.com.au (REA)
Context describing how RealEstate.com.au moved towards microservices and experienced significant productivity gains through automation. It took them 18 months to move from two services to over 60-70 services.

:p How did RealEstate.com.au benefit from implementing an automated toolchain?
??x
RealEstate.com.au benefited by reducing the initial setup time for new microservices, allowing a rapid increase in service count. By automating deployment, monitoring, and support processes, they were able to scale their development efforts more efficiently.

In the first 18 months, RealEstate moved from 2 services to over 60-70 services, with developers responsible for the entire lifecycle of these services. Automation tools helped in provisioning machines, deploying code, and monitoring services seamlessly.

```bash
# Example Bash script for service deployment at REA
#!/bin/bash
echo ""Deploying Service...""
# Logic to deploy new microservices here
```
x??",998,"That means multiple deployments to handle, services to monitor, logs to collect. Automation is essential. Automation is also how we can make sure that our developers still remain productive. Giving th...",qwen2.5:latest,2025-11-02 23:50:55,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Traditional Virtualization,Case Study: Gilt Online Fashion Retailer,"#### Case Study: Gilt Online Fashion Retailer
Context explaining how Gilt transitioned from a monolithic architecture to a microservice-based system, leveraging automation extensively. By 2014, Gilt had over 450 services.

:p What role did automation play in the success of Gilt's move to microservices?
??x
Automation played a crucial role by enabling frequent and reliable deployments without manual intervention. This allowed Gilt to scale their service count significantly while maintaining quality and reliability. As the number of microservices grew, automation tools helped manage complexity, ensuring that developers could focus on code rather than infrastructure.

By 2014, Gilt had over 450 services, with a ratio of about three services per developer. Automation tools such as CI/CD pipelines and orchestration platforms were key in supporting this growth.

```bash
# Example CI/CD script for Gilt's microservices deployment
#!/bin/bash
echo ""Starting CI/CD Pipeline...""
# Logic to trigger continuous integration and deployment here
```
x??",1051,"That means multiple deployments to handle, services to monitor, logs to collect. Automation is essential. Automation is also how we can make sure that our developers still remain productive. Giving th...",qwen2.5:latest,2025-11-02 23:50:55,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Traditional Virtualization,Traditional Virtualization and Host Management,"#### Traditional Virtualization and Host Management
Context discussing the benefits of traditional virtualization tools like VMware or AWS in reducing host management overhead. This helps in efficiently managing a large number of hosts.

:p How do virtualization tools help manage multiple physical machines?
??x
Virtualization tools, such as those provided by VMware or AWS, enable chunking up existing physical machines into smaller parts, thereby reducing the overhead associated with managing each machine individually. This approach enhances resource utilization and simplifies host management tasks like provisioning, scaling, and monitoring.

For example:
```bash
# Example Bash script to create a virtual machine using AWS CLI
#!/bin/bash
echo ""Creating VM on AWS...""
aws ec2 run-instances --image-id ami-0abcdef1234567890 --count 1 --instance-type t2.micro
```
x??

---",878,"That means multiple deployments to handle, services to monitor, logs to collect. Automation is essential. Automation is also how we can make sure that our developers still remain productive. Giving th...",qwen2.5:latest,2025-11-02 23:50:55,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Traditional Virtualization,Physical Host Limitations,"#### Physical Host Limitations
Background context explaining why having lots of physical hosts can be expensive. The overhead comes from needing a separate host for each service, making it challenging to scale efficiently.
:p Why is using multiple physical hosts per service expensive?
??x
Using multiple physical hosts per service increases costs because each service requires its own dedicated hardware. This leads to underutilized resources and higher expenses due to the need for more physical infrastructure.
x??",517,"However, there have been some new advances in this space that are well worth exploring, as they can open up even more interesting possibilities for dealing with our microservice architecture. Traditio...",qwen2.5:latest,2025-11-02 23:51:04,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Traditional Virtualization,Virtualization Overview,"#### Virtualization Overview
Background context explaining how virtualization allows slicing a single physical server into multiple virtual machines (VMs), but at the cost of overhead.
:p What is the main idea behind using virtualization in microservice architectures?
??x
Virtualization enables running multiple services on a single physical host by creating separate VMs. However, this comes with additional overhead due to the hypervisor managing these VMs, which can impact performance and resource utilization.
x??",519,"However, there have been some new advances in this space that are well worth exploring, as they can open up even more interesting possibilities for dealing with our microservice architecture. Traditio...",qwen2.5:latest,2025-11-02 23:51:04,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Traditional Virtualization,Type 2 Virtualization vs Lightweight Containers,"#### Type 2 Virtualization vs Lightweight Containers
Background context comparing traditional type 2 virtualization (e.g., AWS, VMware) with lightweight container technologies like Docker in terms of overhead and efficiency.
:p What are the key differences between type 2 virtualization and lightweight containers?
??x
Type 2 virtualization involves running a hypervisor on top of an existing host OS, which introduces significant overhead as the hypervisor manages resources. In contrast, lightweight containers like Docker run directly on the host OS without an additional layer of abstraction, reducing overhead.
x??",619,"However, there have been some new advances in this space that are well worth exploring, as they can open up even more interesting possibilities for dealing with our microservice architecture. Traditio...",qwen2.5:latest,2025-11-02 23:51:04,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Traditional Virtualization,Hypervisor Overhead in Type 2 Virtualization,"#### Hypervisor Overhead in Type 2 Virtualization
Background context explaining that the hypervisor in type 2 virtualization sets aside resources for its own operations, thereby reducing available resources for VMs. This overhead becomes a constraint when trying to slice physical infrastructure into smaller parts.
:p How does the hypervisor's role in managing VMs affect resource allocation?
??x
The hypervisor's job of mapping resources from VMs to the host and controlling those VMs takes up valuable CPU, I/O, and memory resources. As more VMs are created, this overhead increases, limiting the total number of VMs that can be run efficiently.
x??",652,"However, there have been some new advances in this space that are well worth exploring, as they can open up even more interesting possibilities for dealing with our microservice architecture. Traditio...",qwen2.5:latest,2025-11-02 23:51:04,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Traditional Virtualization,Physical Drawer Analogy,"#### Physical Drawer Analogy
Background context using a physical drawer analogy to explain why adding dividers (virtualization layers) reduces overall storage capacity (resource efficiency).
:p Why is it not possible to store more socks by adding more dividers in your drawer?
??x
Adding more dividers in the drawer increases organizational complexity but decreases the total storage space available for socks. Similarly, adding more VMs or virtualization layers on a physical host consumes resources that could otherwise be used for running services.
x??",555,"However, there have been some new advances in this space that are well worth exploring, as they can open up even more interesting possibilities for dealing with our microservice architecture. Traditio...",qwen2.5:latest,2025-11-02 23:51:04,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Traditional Virtualization,Resource Allocation in Virtualization,"#### Resource Allocation in Virtualization
Background context illustrating how resource allocation is managed between the hypervisor and VMs, leading to diminishing returns when slicing hosts into smaller parts.
:p How does resource allocation work in type 2 virtualization?
??x
In type 2 virtualization, the hypervisor runs on top of a host OS and manages resources like CPU and memory for each VM. As more VMs are created, the hypervisor takes up more resources, reducing the available capacity for running services efficiently.
x??

---",539,"However, there have been some new advances in this space that are well worth exploring, as they can open up even more interesting possibilities for dealing with our microservice architecture. Traditio...",qwen2.5:latest,2025-11-02 23:51:04,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Linux Containers,Vagrant Overview,"#### Vagrant Overview
Vagrant provides a platform for creating and managing virtual machines (VMs) on your local machine, using tools like VirtualBox. The configuration of VMs is defined in a text file that can be checked into version control systems and shared among team members.
:p What does Vagrant provide?
??x
Vagrant provides an environment to create and manage VMs locally, leveraging virtualization technologies such as VirtualBox. Configurations are stored in a text file that can be shared across the development team, ensuring everyone has an identical setup.
x??",575,"Vagrant Vagrant is a very useful deployment platform, which is normally used for dev and test rather than production. Vagrant provides you with a virtual cloud on your laptop. Underneath, it uses a st...",qwen2.5:latest,2025-11-02 23:51:12,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Linux Containers,Running Multiple VMs with Vagrant,"#### Running Multiple VMs with Vagrant
With Vagrant, you can define multiple VMs in a single configuration file and manage them as a group. This allows for easier setup of complex systems on your local machine and enables testing of failure modes by shutting down individual VMs.
:p How does managing multiple VMs with Vagrant simplify development?
??x
Managing multiple VMs with Vagrant simplifies development by allowing you to define all required services in a single configuration file. This ensures consistency across the team and makes it easy to spin up or shut down specific services for testing different failure modes.
x??",632,"Vagrant Vagrant is a very useful deployment platform, which is normally used for dev and test rather than production. Vagrant provides you with a virtual cloud on your laptop. Underneath, it uses a st...",qwen2.5:latest,2025-11-02 23:51:12,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Linux Containers,Linux Containers Introduction,"#### Linux Containers Introduction
Linux containers are an alternative to full virtualization, offering faster startup times due to not needing a hypervisor. They share the same kernel but can run different operating system distributions as long as they use the same kernel version.
:p What is the key difference between virtual machines and Linux containers?
??x
The key difference lies in their underlying architecture: VMs require a hypervisor to segment hardware resources, while containers leverage process space within the host OS to isolate processes. This means containers share the same kernel but can run different distributions of operating systems.
x??",664,"Vagrant Vagrant is a very useful deployment platform, which is normally used for dev and test rather than production. Vagrant provides you with a virtual cloud on your laptop. Underneath, it uses a st...",qwen2.5:latest,2025-11-02 23:51:12,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Linux Containers,Advantages of Linux Containers,"#### Advantages of Linux Containers
Linux containers offer faster startup times and better resource utilization compared to full virtual machines. They are more lightweight, allowing for many more instances running on the same hardware, and provide finer-grained control over resources.
:p What advantages do Linux containers have over full VMs?
??x
Linux containers are advantageous because they start much faster (seconds vs. minutes), utilize fewer system resources, and allow for more instances to run on the same hardware due to their lightweight nature. They also offer better resource allocation control compared to traditional virtualization.
x??",654,"Vagrant Vagrant is a very useful deployment platform, which is normally used for dev and test rather than production. Vagrant provides you with a virtual cloud on your laptop. Underneath, it uses a st...",qwen2.5:latest,2025-11-02 23:51:12,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Linux Containers,Isolation in Linux Containers,"#### Isolation in Linux Containers
While Linux containers provide process isolation, there are known vulnerabilities that can allow processes from one container to interact with others or the host system. This requires careful consideration of security and code reliability when using containers.
:p What potential issue should be considered when using Linux containers?
??x
A potential issue is that despite providing some level of isolation, Linux containers can still have vulnerabilities allowing processes in one container to interact with others or the underlying host. This necessitates thorough testing and reliable coding practices for secure use.
x??

---",665,"Vagrant Vagrant is a very useful deployment platform, which is normally used for dev and test rather than production. Vagrant provides you with a virtual cloud on your laptop. Underneath, it uses a st...",qwen2.5:latest,2025-11-02 23:51:12,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Docker,Docker Overview,"#### Docker Overview
Docker is a platform built on top of lightweight containers that automates the deployment, scaling, and management of applications. It simplifies the creation and deployment of apps, which are referred to as images in this context. Docker manages container provisioning, networking, and provides its own registry for storing and versioning Docker applications.
:p What does Docker automate in application development?
??x
Docker automates the deployment, scaling, and management of applications by using lightweight containers. This allows developers to focus on building their services rather than managing underlying infrastructure.
x??",659,"Docker Docker is a platform built on top of lightweight containers. It handles much of the work around handling containers for you. In Docker, you create and deploy  apps , which are synonymous with i...",qwen2.5:latest,2025-11-02 23:51:26,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Docker,Docker App Abstraction,"#### Docker App Abstraction
Just as with VM images, the underlying technology used to implement a service is hidden from us when using Docker apps. Builds for services are created into Docker applications, and these are stored in the Docker registry for easy deployment and versioning.
:p How do Docker apps differ from traditional VM images?
??x
Docker apps provide an abstraction layer where the specific implementation details of the underlying operating system are not visible to the developer. They can be seen as containers that encapsulate an application's code, libraries, and dependencies, making them portable across different environments.
x??",654,"Docker Docker is a platform built on top of lightweight containers. It handles much of the work around handling containers for you. In Docker, you create and deploy  apps , which are synonymous with i...",qwen2.5:latest,2025-11-02 23:51:26,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Docker,Docker for Development and Testing,"#### Docker for Development and Testing
Docker can reduce the complexity of running multiple services locally by using a single VM with Vagrant to host a Docker instance. This allows for faster provisioning of individual services compared to managing multiple independent VMs.
:p How does Docker simplify local development and testing?
??x
Using Docker, developers can set up and tear down a Docker platform on a single VM in Vagrant quickly. Each service can be run as an individual container, which speeds up the development and testing process significantly compared to running multiple independent virtual machines.
```java
// Example of setting up a Docker container for a Java application
public class DockerSetup {
    public void setupDocker() {
        // Pseudocode to create a Dockerfile with necessary configurations
        String dockerFile = ""FROM openjdk:8\n"" +
                            ""COPY target/myapp.jar /app/\n"" +
                            ""EXPOSE 8080\n"" +
                            ""CMD java -jar /app/myapp.jar"";
        
        // Code to build and run the Docker container
        String command = ""docker build -t myapp . && docker run -p 8080:8080 myapp"";
    }
}
```
x??",1209,"Docker Docker is a platform built on top of lightweight containers. It handles much of the work around handling containers for you. In Docker, you create and deploy  apps , which are synonymous with i...",qwen2.5:latest,2025-11-02 23:51:26,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Docker,CoreOS and Lightweight OS for Containers,"#### CoreOS and Lightweight OS for Containers
CoreOS is a Linux-based operating system designed with Docker in mind, providing only essential services to allow containers to run efficiently. This makes it resource-efficient, suitable for running multiple containers.
:p What makes CoreOS unique compared to other operating systems?
??x
CoreOS is unique because it is a stripped-down Linux OS that focuses on minimalism and efficiency. It includes only the necessary components required to run Docker containers, making it lighter and more efficient than traditional OSes like Ubuntu or CentOS.
```bash
# Example of installing software using CoreOS's package manager
sudo yum install -y docker-container-tool
```
x??",715,"Docker Docker is a platform built on top of lightweight containers. It handles much of the work around handling containers for you. In Docker, you create and deploy  apps , which are synonymous with i...",qwen2.5:latest,2025-11-02 23:51:26,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Docker,Kubernetes and Container Scheduling,"#### Kubernetes and Container Scheduling
Kubernetes is an open-source platform that helps manage services across multiple Docker instances on different machines. It acts as a scheduling layer to find available Docker containers for running new requests.
:p What does Kubernetes provide in terms of container management?
??x
Kubernetes provides a mechanism for managing Docker containers across multiple nodes or machines. Its primary function is to schedule, deploy, and manage the lifecycle of containers based on defined rules and policies, ensuring that applications run smoothly even when containers are restarted or fail.
```java
// Pseudocode for creating a Kubernetes deployment
public class KubernetesDeployment {
    public void createDeployment() {
        // Define the desired state of the application
        String deploymentYaml = ""apiVersion: apps/v1\n"" +
                                ""kind: Deployment\n"" +
                                ""metadata:\n"" +
                                ""  name: myapp-deployment\n"" +
                                ""spec:\n"" +
                                ""  replicas: 3\n"" +
                                ""  selector:\n"" +
                                ""    matchLabels:\n"" +
                                ""      app: myapp\n"" +
                                ""  template:\n"" +
                                ""    metadata:\n"" +
                                ""      labels:\n"" +
                                ""        app: myapp\n"" +
                                ""    spec:\n"" +
                                ""      containers:\n"" +
                                ""      - name: myapp-container\n"" +
                                ""        image: myapp:latest"";
        
        // Code to apply the deployment configuration
        String command = ""kubectl apply -f deploymentYaml.yaml"";
    }
}
```
x??",1883,"Docker Docker is a platform built on top of lightweight containers. It handles much of the work around handling containers for you. In Docker, you create and deploy  apps , which are synonymous with i...",qwen2.5:latest,2025-11-02 23:51:26,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Docker,Deis and PaaS on Docker,"#### Deis and PaaS on Docker
Deis is a tool that provides a Heroku-like Platform as a Service (PaaS) on top of Docker. It aims to simplify container-based deployments, making it easier for developers to manage applications.
:p How does Deis compare to traditional PaaS solutions?
??x
Deis offers a more lightweight and flexible alternative to traditional PaaS providers like Heroku by leveraging Docker containers. It simplifies the deployment process and allows for greater control over the underlying infrastructure while still providing many of the benefits of managed services.
```java
// Example of deploying an application on Deis
public class DeisDeployment {
    public void deployApplication() {
        // Pseudocode to push a new app to Deis
        String command = ""deis create myapp && deis buildpack:add heroku/java && git push deis master"";
    }
}
```
x??

---",877,"Docker Docker is a platform built on top of lightweight containers. It handles much of the work around handling containers for you. In Docker, you create and deploy  apps , which are synonymous with i...",qwen2.5:latest,2025-11-02 23:51:26,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,A Deployment Interface,Single Command-Line Deployment Interface,"#### Single Command-Line Deployment Interface

Background context explaining the importance of having a uniform interface for deploying microservices. Highlight that this approach simplifies deployment across different environments and reduces errors.

:p What is the single command-line call used to trigger deployments, and what parameters does it take?
??x
The `deploy` command-line script is used to trigger any deployment. It takes three parameters: 
- `artifact`: The name of the microservice.
- `environment`: The target environment for the deployment (e.g., local, CI, integrated_qa).
- `version`: The version of the artifact to deploy.

For example:
```sh
$ deploy artifact=catalog environment=local version=local
```
This command can be used by developers locally or by CI tools in a pipeline.
x??",807,"A Deployment Interface Whatever underlying platform or artifacts you use, having a uniform interface to deploy a given service is vital. We’ll want to trigger deployment of a microservice on demand in...",qwen2.5:latest,2025-11-02 23:51:37,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,A Deployment Interface,Versioning Strategy,"#### Versioning Strategy

Background context on how different environments might require different versions of microservices, with specific examples provided for local development, CI testing, and QA testing.

:p How does the deployment script handle versioning during different stages (local, CI, QA)?
??x
The deployment script handles versioning based on the environment:
- **Local Development**: The current local version is used.
- **CI Testing**: The latest ""green"" build, which could be the most recent blessed artifact in the repository, is used. This is typically identified by a specific build number.
- **QA Testing**: The latest version available for testing and diagnosing issues.

For example:
```sh
$ deploy artifact=catalog environment=ci version=b456
```
Here, `b456` could be the CI build number from the recent pipeline run. 
x??",847,"A Deployment Interface Whatever underlying platform or artifacts you use, having a uniform interface to deploy a given service is vital. We’ll want to trigger deployment of a microservice on demand in...",qwen2.5:latest,2025-11-02 23:51:37,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,A Deployment Interface,Deployment Environments,"#### Deployment Environments

Background context on how microservice topologies might differ between environments but the deployment script abstracts away these differences.

:p What is the role of the `environment` parameter in the deployment command?
??x
The `environment` parameter specifies the target environment for deploying the microservice. This abstraction ensures that the same deployment logic can be used across different environments, hiding the underlying topology and infrastructure details from developers or operators.

For example:
```sh
$ deploy artifact=catalog environment=local version=local
```
This command deploys the `catalog` service into a local environment using the current local version.
x??",723,"A Deployment Interface Whatever underlying platform or artifacts you use, having a uniform interface to deploy a given service is vital. We’ll want to trigger deployment of a microservice on demand in...",qwen2.5:latest,2025-11-02 23:51:37,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,A Deployment Interface,Sample Deployment Command,"#### Sample Deployment Command

Background context on common deployment scenarios, including examples of commands for different environments and purposes.

:p Provide an example of how to use the `deploy` script in a CI pipeline.
??x
In a CI pipeline, the `deploy` command is used after a build artifact has been created. For instance:
```sh
$ deploy artifact=catalog environment=ci version=b456
```
Here, `b456` would be the latest build number generated by the CI system.

This ensures that the correct artifact (in this case, build `b456`) is deployed to the CI environment for further testing.
x??",601,"A Deployment Interface Whatever underlying platform or artifacts you use, having a uniform interface to deploy a given service is vital. We’ll want to trigger deployment of a microservice on demand in...",qwen2.5:latest,2025-11-02 23:51:37,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,A Deployment Interface,Script Implementation,"#### Script Implementation

Background context on the tools and libraries used to implement such a deployment script. Mention Fabric and Boto as examples for Python, and Capistrano or PowerShell for other environments.

:p What tools are commonly used to implement the `deploy` script?
??x
The `deploy` script can be implemented using various tools and libraries depending on the environment:
- **Python**: Use Fabric with a library like Boto for AWS interactions.
  ```python
  from fabric.api import run, env

  def deploy():
      # Example function to SSH into an instance
      env.hosts = ['user@remote.example.com']
      run('echo Deploying artifact...')
  ```

- **Ruby**: Use Capistrano for deployment tasks.
  ```ruby
  namespace :deploy do
    desc 'Deploy the application'
    task :default do
      # Example Capistrano task to deploy the application
    end
  end
  ```

- **Windows**: Utilize PowerShell scripts.
  ```powershell
  function Deploy {
      param (
          [string]$Artifact,
          [string]$Environment,
          [string]$Version
      )
      Write-Output ""Deploying $Artifact to $Environment with version $Version""
  }
  ```

These tools provide a flexible and powerful way to manage deployments, abstracting away the complexity of different environments.
x??",1298,"A Deployment Interface Whatever underlying platform or artifacts you use, having a uniform interface to deploy a given service is vital. We’ll want to trigger deployment of a microservice on demand in...",qwen2.5:latest,2025-11-02 23:51:37,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Environment Definition,Environment Definition,"#### Environment Definition
Background context explaining how environment definitions are used to specify resources and services for different environments. YAML files were used as an example, storing details such as nodes, services, credentials, and regions.

:p What is an environment definition?
??x
An environment definition is a mapping from microservices to compute, network, and storage resources in specific environments like development or production. It includes details such as the AMI ID, instance size, number of instances, credentials, and services.
??x
For example:
```yaml
development:
  nodes:
    - ami_id: ami-e1e1234
      size: t1.micro
      credentials_name: eu-west-ssh
      services: [catalog-service]
      region: eu-west-1

production:
  nodes:
    - ami_id: ami-e1e1234
      size: m3.xlarge
      credentials_name: prod-credentials
      services: [catalog-service]
      number: 5
```
x??",920,"Environment Definition Clearly, for this to work, we need to have some way of defining what our environments look like, and what our service looks like in a given environment. You can think of an envi...",qwen2.5:latest,2025-11-02 23:51:49,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Environment Definition,Service Definition,"#### Service Definition
Background context explaining how service definitions store information that remains constant across different environments. Puppet manifests are mentioned as an example of such information.

:p What is a service definition?
??x
A service definition stores information about microservices, which remains the same regardless of the environment. For instance, it might include details like the Puppet manifest file to run.
??x
For example:
```yaml
catalog-service:
  puppet_manifest: catalog.pp
```
x??",524,"Environment Definition Clearly, for this to work, we need to have some way of defining what our environments look like, and what our service looks like in a given environment. You can think of an envi...",qwen2.5:latest,2025-11-02 23:51:49,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Environment Definition,Environment-Specific Resources,"#### Environment-Specific Resources
Background context on how resources and configurations differ between environments, such as varying node sizes for cost-effectiveness.

:p How do environment definitions handle resource variation?
??x
Environment definitions allow specifying different resources based on the environment. For example, smaller instances can be used in development environments to save costs while larger instances are used in production environments.
??x
For instance:
```yaml
development:
  nodes:
    - ami_id: ami-e1e1234
      size: t1.micro

production:
  nodes:
    - ami_id: ami-e1e1234
      size: m3.xlarge
```
x??",641,"Environment Definition Clearly, for this to work, we need to have some way of defining what our environments look like, and what our service looks like in a given environment. You can think of an envi...",qwen2.5:latest,2025-11-02 23:51:49,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Environment Definition,Credentials Management,"#### Credentials Management
Background context on managing different credentials for sensitive environments, stored separately and accessed by specific personnel.

:p How are credentials managed in environment definitions?
??x
Credentials are managed differently based on the environment. Sensitive environments use separate credential stores accessible only to selected individuals, while non-sensitive ones might have more open access.
??x
For example:
```yaml
development:
  nodes:
    - ami_id: ami-e1e1234
      size: t1.micro
      credentials_name: eu-west-ssh

production:
  nodes:
    - ami_id: ami-e1e1234
      size: m3.xlarge
      credentials_name: prod-credentials
```
x??",686,"Environment Definition Clearly, for this to work, we need to have some way of defining what our environments look like, and what our service looks like in a given environment. You can think of an envi...",qwen2.5:latest,2025-11-02 23:51:49,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Environment Definition,Load Balancing,"#### Load Balancing
Background context on automatically creating load balancers for services with multiple instances.

:p How are load balancers managed in environment definitions?
??x
Load balancers are automatically created if a service has more than one instance. This is done to distribute traffic efficiently across the nodes.
??x
For example:
```yaml
development:
  nodes:
    - ami_id: ami-e1e1234
      size: t1.micro
      services: [catalog-service]

production:
  nodes:
    - ami_id: ami-e1e1234
      size: m3.xlarge
      number: 5
```
x??",553,"Environment Definition Clearly, for this to work, we need to have some way of defining what our environments look like, and what our service looks like in a given environment. You can think of an envi...",qwen2.5:latest,2025-11-02 23:51:49,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Environment Definition,Port and Connectivity Configuration,"#### Port and Connectivity Configuration
Background context on normalizing port usage for services.

:p How are ports configured in environment definitions?
??x
Ports are normalized across environments to ensure consistent service connectivity. Load balancers are automatically set up if more than one instance is present.
??x
For example:
```yaml
catalog-service:
  puppet_manifest: catalog.pp
  connectivity:
    - protocol: tcp
      ports: [8080, 8081]
      allowed: [WORLD]
```
x??",487,"Environment Definition Clearly, for this to work, we need to have some way of defining what our environments look like, and what our service looks like in a given environment. You can think of an envi...",qwen2.5:latest,2025-11-02 23:51:49,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Environment Definition,Tooling and Future Directions,"#### Tooling and Future Directions
Background context on tools like Terraform that can help manage environment definitions.

:p What is Terraform?
??x
Terraform is a new tool from HashiCorp designed to handle environment definitions, resource provisioning, and configuration management. It aims to create an open-source solution in this space.
??x
For example:
```yaml
provider ""aws"" {
  region = ""eu-west-1""
}

resource ""aws_instance"" ""example"" {
  ami           = ""ami-e1e1234""
  instance_type = ""t1.micro""

  tags = {
    Name = ""development-node""
  }
}
```
x??

---",569,"Environment Definition Clearly, for this to work, we need to have some way of defining what our environments look like, and what our service looks like in a given environment. You can think of an envi...",qwen2.5:latest,2025-11-02 23:51:49,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Types of Tests,Microservice Deployment Practices Recap,"#### Microservice Deployment Practices Recap
Background context: The provided summary covers best practices for deploying microservices, emphasizing independence, automation, and using tools like AWS. It highlights the importance of a single repository per service and CI builds.

:p What are the main points covered in the recap section?
??x
The key points include:
1. Maintaining the ability to release services independently.
2. Ensuring technology supports independent release.
3. Preferring one repository per microservice.
4. Having one CI build per microservice for separate deployment.
5. Moving towards single-service per host/container using technologies like LXC or Docker.
6. Emphasizing automation and using platforms like AWS for ease of management.
7. Understanding the impact on developers and providing tools for self-service deployments.

x??",860,"Summary We’ve covered a lot of ground here, so a recap is in order. First, focus on maintaining the ability to release one service independently from another, and make sure that whatever technology yo...",qwen2.5:latest,2025-11-02 23:52:01,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Types of Tests,Testing Microservices,"#### Testing Microservices
Background context: The summary discusses the challenges in testing microservices due to their distributed nature and introduces a quadrant system for categorizing tests by Brian Marick.

:p What does the testing section cover?
??x
The section covers:
1. Challenges in effectively and efficiently testing microservices.
2. Types of tests using the Marick quadrant (technology-facing vs. user-facing).
3. The trend towards automating as much testing as possible.
4. Importance of automated testing for validating software quickly.

x??",561,"Summary We’ve covered a lot of ground here, so a recap is in order. First, focus on maintaining the ability to release one service independently from another, and make sure that whatever technology yo...",qwen2.5:latest,2025-11-02 23:52:01,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Types of Tests,Marick’s Testing Quadrant,"#### Marick’s Testing Quadrant
Background context: Brian Marick's quadrant categorizes tests into technology-facing and user-facing categories, with subcategories for performance tests, unit tests, acceptance tests, and exploratory testing.

:p What is the Marick testing quadrant?
??x
The Marick testing quadrant divides tests into:
1. Technology-Facing (Performance Tests & Unit Tests)
2. User-Facing (Acceptance Tests & Exploratory Testing)

Example code for a simple performance test in Java:
```java
import org.junit.Test;
import static org.hamcrest.CoreMatchers.is;
import static org.junit.Assert.assertThat;

public class PerformanceTest {
    @Test
    public void testPerformance() {
        long start = System.currentTimeMillis();
        // Code to execute and measure time
        long end = System.currentTimeMillis();
        assertThat(""The method should not take longer than 100ms"", (end - start) <= 100, is(true));
    }
}
```

x??",949,"Summary We’ve covered a lot of ground here, so a recap is in order. First, focus on maintaining the ability to release one service independently from another, and make sure that whatever technology yo...",qwen2.5:latest,2025-11-02 23:52:01,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Types of Tests,Technology-Facing Tests,"#### Technology-Facing Tests
Background context: Technology-facing tests are automated and aid developers in creating the system. They include performance tests and small-scoped unit tests.

:p What are technology-facing tests?
??x
Technology-facing tests are those that help developers create the system, such as:
- Performance tests (measure speed and resource usage)
- Small-scoped unit tests (test individual components)

Example code for a simple unit test in Java:
```java
import org.junit.Test;
import static org.junit.Assert.assertEquals;

public class MyServiceTest {
    @Test
    public void testFunctionality() {
        MyService service = new MyService();
        assertEquals(5, service.add(2, 3));
    }
}
```

x??",730,"Summary We’ve covered a lot of ground here, so a recap is in order. First, focus on maintaining the ability to release one service independently from another, and make sure that whatever technology yo...",qwen2.5:latest,2025-11-02 23:52:01,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Types of Tests,User-Facing Tests,"#### User-Facing Tests
Background context: User-facing tests are automated and help non-technical stakeholders understand how the system works. They include large-scoped end-to-end tests and manual testing like user testing.

:p What are user-facing tests?
??x
User-facing tests provide insights to non-technical stakeholders and include:
- Large-scoped, end-to-end tests (acceptance tests)
- Manual testing (like exploratory or user testing)

Example code for a simple acceptance test in Java using Selenium WebDriver (pseudo-code):
```java
import org.openqa.selenium.By;
import org.openqa.selenium.WebDriver;
import org.openqa.selenium.WebElement;
import org.testng.Assert;

public class MyWebPageTest {
    @Test
    public void testWebPage() {
        WebDriver driver = initializeWebDriver();
        try {
            WebElement element = driver.findElement(By.id(""someId""));
            Assert.assertEquals(element.getText(), ""Expected Text"");
        } finally {
            driver.quit();
        }
    }
}
```

x??",1024,"Summary We’ve covered a lot of ground here, so a recap is in order. First, focus on maintaining the ability to release one service independently from another, and make sure that whatever technology yo...",qwen2.5:latest,2025-11-02 23:52:01,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Types of Tests,Trade-Offs in Automated Testing,"#### Trade-Offs in Automated Testing
Background context: The summary mentions the need to balance speed of release with software quality through automated testing, highlighting that different types of tests have their roles and trade-offs.

:p What are the key considerations when deciding on types of automated tests?
??x
Key considerations include:
1. Balancing speed of release vs. software quality.
2. Deciding how much manual vs. automated testing is needed.
3. Choosing between performance, unit, acceptance, or exploratory tests based on system requirements.

Example of a trade-off scenario in Java:
```java
// Scenario: Opt for fewer but more robust end-to-end tests over many small unit tests
public class MySystem {
    @Test
    public void testEntireWorkflow() {
        // Simulate workflow steps
        boolean result = performComplexOperation();
        Assert.assertTrue(result);
    }
}
```

x??

---",919,"Summary We’ve covered a lot of ground here, so a recap is in order. First, focus on maintaining the ability to release one service independently from another, and make sure that whatever technology yo...",qwen2.5:latest,2025-11-02 23:52:01,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Test Scope,Test Pyramid Overview,"#### Test Pyramid Overview
Background context explaining the concept. The Test Pyramid, as outlined by Mike Cohn in his book ""Succeeding with Agile,"" helps us understand the types and proportions of automated tests needed for a software project. Traditionally, it splits automated tests into three layers: Unit, Service, and UI.
:p What is the Test Pyramid?
??x
The Test Pyramid is a model that illustrates the proportions of different types of automated tests in a software development process, emphasizing the importance of various test levels to ensure comprehensive coverage.
x??",583,"Test Scope In his book  Succeeding with Agile  (Addison-Wesley), Mike Cohn outlines a model called the Test Pyramid to help explain what types of automated tests you need. The pyramid helps us think a...",qwen2.5:latest,2025-11-02 23:52:10,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Test Scope,Ambiguity in Definitions,"#### Ambiguity in Definitions
Background context explaining the concept. The terms used in the Test Pyramid can be ambiguous and have varying interpretations among developers. ""Service"" is particularly open to interpretation, and definitions of unit tests vary widely.
:p What are some issues with using generic terms like ""Service"" in test categorization?
??x
Using generic terms like ""Service"" in test categorization can lead to confusion because these terms may mean different things to different people. For example, a ""service"" could refer to an internal service within the application or an external API, which complicates the categorization of tests.
x??",661,"Test Scope In his book  Succeeding with Agile  (Addison-Wesley), Mike Cohn outlines a model called the Test Pyramid to help explain what types of automated tests you need. The pyramid helps us think a...",qwen2.5:latest,2025-11-02 23:52:10,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Test Scope,Preference for UI Tests,"#### Preference for UI Tests
Background context explaining the concept. The author prefers using ""end-to-end"" tests for UI interactions rather than sticking with the term ""UI"" test to avoid ambiguity.
:p Why does the author prefer ""end-to-end"" over ""UI"" for tests?
??x
The author prefers ""end-to-end"" over ""UI"" because it better captures the essence of testing entire flows and user journeys, reducing confusion. ""UI"" can be misleading as it might imply only graphical interface interactions without considering other layers.
x??",529,"Test Scope In his book  Succeeding with Agile  (Addison-Wesley), Mike Cohn outlines a model called the Test Pyramid to help explain what types of automated tests you need. The pyramid helps us think a...",qwen2.5:latest,2025-11-02 23:52:10,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Test Scope,Layer Descriptions,"#### Layer Descriptions
Background context explaining the concept. The Test Pyramid traditionally categorizes tests into three levels: Unit, Service, and UI. However, these terms are ambiguous and may need clarification.
:p What are the traditional layers of the Test Pyramid?
??x
The traditional layers of the Test Pyramid are:
- **Unit Tests**: Tests that focus on individual components or functions.
- **Service Tests**: Tests for integration between components (can vary in interpretation).
- **UI Tests**: Tests focusing on end-to-end user interactions with the application.

These layers aim to cover different aspects of the software, from low-level to high-level functionality.
x??",689,"Test Scope In his book  Succeeding with Agile  (Addison-Wesley), Mike Cohn outlines a model called the Test Pyramid to help explain what types of automated tests you need. The pyramid helps us think a...",qwen2.5:latest,2025-11-02 23:52:10,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Test Scope,Example Scenario,"#### Example Scenario
Background context explaining the concept. An example scenario is provided involving a helpdesk application and a main website interacting with customer service, which in turn interacts with a loyalty points bank.
:p Describe the example scenario presented in the text?
??x
The example scenario involves a music shop system with:
- A helpdesk application
- A main website 
- Customer service for retrieving, reviewing, and editing customer details
- Interaction with a loyalty points bank where customers accrue points from purchases.

This setup is used to illustrate different testing scenarios across various layers of the Test Pyramid.
x??",665,"Test Scope In his book  Succeeding with Agile  (Addison-Wesley), Mike Cohn outlines a model called the Test Pyramid to help explain what types of automated tests you need. The pyramid helps us think a...",qwen2.5:latest,2025-11-02 23:52:10,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Test Scope,Diving into Scenarios,"#### Diving into Scenarios
Background context explaining the concept. The example scenario provides specific interactions that can be tested at different levels of the Test Pyramid.
:p What does the author suggest we should look at in this example?
??x
The author suggests looking at specific interactions within the example to understand how they would be tested at different layers of the Test Pyramid, such as unit tests for individual functions, service tests for interactions between components, and end-to-end tests for full user journeys.
x??

---",554,"Test Scope In his book  Succeeding with Agile  (Addison-Wesley), Mike Cohn outlines a model called the Test Pyramid to help explain what types of automated tests you need. The pyramid helps us think a...",qwen2.5:latest,2025-11-02 23:52:10,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Those Tricky End-to-End Tests,Unit Tests,"#### Unit Tests
Background context explaining the concept. Unit tests typically test a single function or method call, generated by techniques like TDD and property-based testing. These tests are technology-facing and help catch most of the bugs. They are very fast on modern hardware, allowing for running thousands in less than a minute.
If applicable, add code examples with explanations.
:p What is the purpose of unit tests?
??x
Unit tests provide very fast feedback about whether our functionality is good. They support refactoring by catching mistakes during restructuring of code without external dependencies or network connections.

```java
public class CustomerServiceTest {
    @Test
    public void testGetCustomerInfo() {
        // Arrange
        String customerId = ""12345"";
        Customer expectedCustomer = new Customer(""John Doe"", 30);
        
        // Act & Assert
        assertEquals(expectedCustomer, customerService.getCustomer(customerId));
    }
}
```
x??",987,"Unit Tests These are tests that typically test a single function or method call. The tests generated as a side effect of  test-driven design  (TDD) will fall into this category, as do the sorts of tes...",qwen2.5:latest,2025-11-02 23:52:22,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Those Tricky End-to-End Tests,Service Tests,"#### Service Tests
Background context explaining the concept. Service tests bypass the user interface to test services directly in a monolithic application or individual services in a microservices system. They help improve isolation by stubbing out external collaborators.
If applicable, add code examples with explanations.
:p How do service tests differ from unit tests?
??x
Service tests are more comprehensive as they test individual services' capabilities independently, whereas unit tests focus on smaller functions. Service tests can be faster or slower depending on the complexity of interactions (e.g., real databases, network calls). They provide better isolation and fewer moving parts compared to end-to-end tests.

```java
public class UserServiceTest {
    @Test
    public void testUserService() {
        // Arrange
        String username = ""testUser"";
        User expectedUser = new User(username);
        
        // Stub external collaborators if needed
        
        // Act & Assert
        assertEquals(expectedUser, userService.getUserByUsername(username));
    }
}
```
x??",1102,"Unit Tests These are tests that typically test a single function or method call. The tests generated as a side effect of  test-driven design  (TDD) will fall into this category, as do the sorts of tes...",qwen2.5:latest,2025-11-02 23:52:22,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Those Tricky End-to-End Tests,End-to-End Tests,"#### End-to-End Tests
Background context explaining the concept. End-to-end tests cover an entire system, often driving a GUI through a browser or mimicking user interactions. They provide high confidence that production code works but can be tricky to implement in microservices due to increased complexity.
If applicable, add code examples with explanations.
:p What are end-to-end tests useful for?
??x
End-to-end tests cover the entire system scope and simulate real-world usage scenarios, giving developers a sense of confidence that the application will work correctly in production. However, they can be slower and harder to debug when something goes wrong.

```java
public class CustomerServiceTest {
    @Test
    public void testCustomerUploadFile() {
        // Arrange
        WebDriver driver = new ChromeDriver();
        
        // Act & Assert
        driver.get(""http://localhost:8080/upload"");
        WebElement uploadButton = driver.findElement(By.id(""upload-button""));
        File file = new File(""path/to/file"");
        uploadButton.sendKeys(file.getAbsolutePath());
        assertTrue(driver.getPageSource().contains(""File uploaded successfully""));
    }
}
```
x??",1190,"Unit Tests These are tests that typically test a single function or method call. The tests generated as a side effect of  test-driven design  (TDD) will fall into this category, as do the sorts of tes...",qwen2.5:latest,2025-11-02 23:52:22,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Those Tricky End-to-End Tests,Trade-Offs,"#### Trade-Offs
Background context explaining the concept. As you move up the test pyramid, the scope of tests increases, along with confidence in functionality working correctly but at the cost of longer feedback cycles and more difficult debugging when a test fails.
If applicable, add code examples with explanations.
:p What are the trade-offs between different levels of tests?
??x
The trade-offs include:
- **Speed vs. Scope**: Unit tests are fast, while end-to-end tests take longer to run.
- **Debugging Complexity**: Smaller unit tests are easier to debug compared to complex end-to-end scenarios.
- **Confidence Levels**: Larger scope like end-to-end provides higher confidence but at the cost of slower feedback cycles.

```java
// Example of a test suite covering different levels
@RunWith(Parameterized.class)
public class TestSuite {
    @Parameterized.Parameters
    public static Collection<Object[]> data() {
        return Arrays.asList(new Object[][]{
            { ""Unit Test"", 10, 1_000_000 },
            { ""Service Test"", 50, 300_000 },
            { ""End-to-End Test"", 100, 200_000 }
        });
    }

    private String testType;
    private int speed; // in milliseconds
    private int confidence;

    public TestSuite(String testType, int speed, int confidence) {
        this.testType = testType;
        this.speed = speed;
        this.confidence = confidence;
    }

    @Test
    public void test() {
        System.out.println(""Running "" + testType);
        assertEquals(this.speed > 0 ? ""Fast"" : ""Slow"", ""Fast"");
        assertTrue(confidence >= 90 ? true : false); // This is just an example condition for clarity.
    }
}
```
x??

---",1674,"Unit Tests These are tests that typically test a single function or method call. The tests generated as a side effect of  test-driven design  (TDD) will fall into this category, as do the sorts of tes...",qwen2.5:latest,2025-11-02 23:52:22,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Those Tricky End-to-End Tests,Importance of Different Scope Tests,"---
#### Importance of Different Scope Tests
Background context explaining that different types of automated tests are used for various purposes, each with trade-offs. The pyramid model (unit tests at the bottom, service and end-to-end tests at the top) is often used to illustrate the need for these different levels of tests.
:p What is the main idea behind using tests of different scope?
??x
The main idea is that unit tests provide fast feedback on small changes, while broader-scoped tests like service and end-to-end tests ensure that the overall system works. However, each type of test has its trade-offs—unit tests are quick but less comprehensive, whereas broader tests cover more ground but take longer to run.
x??",726,"We find broken functionality faster, our continuous integration builds are faster, and we are less likely to move on to a new task before finding out we have broken something. When those smaller-scope...",qwen2.5:latest,2025-11-02 23:52:34,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Those Tricky End-to-End Tests,Feedback Cycles in Testing,"#### Feedback Cycles in Testing
Explanation on how different types of tests affect feedback cycles. Smaller-scoped tests like unit tests provide quicker feedback and pinpoint the exact issue, while larger-scoped tests help ensure overall system functionality.
:p How do smaller-scoped tests improve our development process?
??x
Smaller-scoped tests, such as unit tests, allow developers to identify issues quickly by focusing on small code changes. This leads to faster feedback cycles and easier debugging because the failure is often localized to a single line of code or a small component.
x??",596,"We find broken functionality faster, our continuous integration builds are faster, and we are less likely to move on to a new task before finding out we have broken something. When those smaller-scope...",qwen2.5:latest,2025-11-02 23:52:34,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Those Tricky End-to-End Tests,Trade-offs in Test Pyramid,"#### Trade-offs in Test Pyramid
Description that different types of automated tests have trade-offs, with an order of magnitude more tests recommended as you descend the pyramid (unit > service > end-to-end).
:p What rule of thumb is suggested for balancing test coverage across levels?
??x
A good rule of thumb is to have approximately an order of magnitude more unit tests than service or end-to-end tests. For example, if there are 4,000 unit tests, there might be 1,000 service tests and 60 end-to-end tests.
x??",516,"We find broken functionality faster, our continuous integration builds are faster, and we are less likely to move on to a new task before finding out we have broken something. When those smaller-scope...",qwen2.5:latest,2025-11-02 23:52:34,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Those Tricky End-to-End Tests,Monolithic System Example,"#### Monolithic System Example
Example provided with specific numbers to illustrate the balance of tests in a system: 4,000 unit tests, 1,000 service tests, and 60 end-to-end tests. Mentioned that there was an imbalance where too many service and end-to-end tests were present.
:p What does the example demonstrate about test coverage in a monolithic system?
??x
The example demonstrates that while having a large number of unit tests is beneficial for quick feedback, the presence of 1,000 service tests and only 60 end-to-end tests suggests an imbalance. The service and end-to-end tests were identified as problematic, leading to efforts to replace them with more unit tests.
x??",682,"We find broken functionality faster, our continuous integration builds are faster, and we are less likely to move on to a new task before finding out we have broken something. When those smaller-scope...",qwen2.5:latest,2025-11-02 23:52:34,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Those Tricky End-to-End Tests,Test Snow Cone,"#### Test Snow Cone
Description of a test snow cone or inverted pyramid where there are few small-scoped tests but many larger-scoped ones, resulting in slow feedback cycles and long build times.
:p What is the issue with an inverted pyramid (test snow cone) approach to testing?
??x
An inverted pyramid approach leads to very long feedback cycles because large, broad tests take longer to run. This can cause significant delays in identifying issues, especially when integrated into continuous integration builds, where slow test runs reduce build frequency and increase the time a broken system remains undetected.
x??",620,"We find broken functionality faster, our continuous integration builds are faster, and we are less likely to move on to a new task before finding out we have broken something. When those smaller-scope...",qwen2.5:latest,2025-11-02 23:52:34,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Those Tricky End-to-End Tests,Implementing Service Tests,"#### Implementing Service Tests
Explanation on how service tests aim to test functionality across multiple services while isolating dependencies using stubs. Need to deploy a binary artifact for the service under test and configure stub services for any downstream collaborators.
:p How do you implement service tests?
??x
Service tests require deploying a stubbed version of collaborating services to isolate the service under test. The steps include:
1. Deploying the binary artifact for the service.
2. Launching or ensuring that stub services are running.
3. Configuring the service under test to connect to these stub services.
4. Setting up the stubs to return expected responses.

Example code might look like this:
```java
// Pseudocode example
public class ServiceUnderTest {
    private final CustomerService customerService;
    
    public ServiceUnderTest(CustomerService customerService) {
        this.customerService = customerService;
    }
    
    public void testFunctionality() {
        // Test logic here, using the stubbed services to mock external dependencies.
    }
}

// Stub service configuration
public class StubCustomerService implements CustomerService {
    @Override
    public Points getPoints(Customer customer) {
        if (customer.getId().equals(""expectedCustomerId"")) {
            return 100;
        }
        return 0;
    }
}
```
x??",1379,"We find broken functionality faster, our continuous integration builds are faster, and we are less likely to move on to a new task before finding out we have broken something. When those smaller-scope...",qwen2.5:latest,2025-11-02 23:52:34,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Those Tricky End-to-End Tests,Mocking and Stubs,"#### Mocking and Stubs
Explanation on the difference between mocking and stubbing. Stubbing involves creating a mock service that responds with canned responses to known requests.
:p What is the purpose of stubbing in testing?
??x
Stubbing is used to simulate or replace real dependencies during tests, allowing for isolated unit tests. By setting up predefined responses (canned responses), stubs enable developers to test specific behaviors without relying on external services, which might be slow or unreliable.
x??

---",524,"We find broken functionality faster, our continuous integration builds are faster, and we are less likely to move on to a new task before finding out we have broken something. When those smaller-scope...",qwen2.5:latest,2025-11-02 23:52:34,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Those Tricky End-to-End Tests,Stub vs Mock Approach,"#### Stub vs Mock Approach
Background context: The passage discusses the differences between using stubs and mocks in testing, explaining that while stubs are used to simulate a dependency without worrying about the number of calls, mocks ensure that expected methods are called. This is particularly relevant for service tests where interactions with external services need to be controlled.
:p What is the main difference between using a stub and a mock when testing?
??x
The key difference lies in their behavior regarding method calls:
- A **stub** returns predefined responses but doesn't care if it's called multiple times or not. It’s used for simulating dependencies without worrying about interaction details.
- A **mock**, on the other hand, ensures that specific methods are called and can fail tests if these expectations aren’t met.

For example, in a service test:
```java
// Stub Example (simulates a dependency)
public class CustomerServiceTest {
    @Test
    public void testBalance() {
        PointsBankStub pointsBank = new PointsBankStub();
        pointsBank.setReturnValueForCustomer123(15000);
        
        // Test logic using the stubbed points bank.
    }
}

// Mock Example (ensures a method is called)
public class CustomerServiceTest {
    @Test
    public void testBalance() throws Exception {
        PointsBankMock pointsBank = new PointsBankMock();
        pointsBank.expects().method(""getCustomerBalance"").withArgs(123);
        
        // Test logic using the mocked points bank.
    }
}
```
x??",1536,"For example, I might tell my stub points bank that when asked for the balance of customer 123, it should return 15,000. The test doesn’t care if the stub is called 0, 1, or 100 times. A variation on t...",qwen2.5:latest,2025-11-02 23:52:47,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Those Tricky End-to-End Tests,Scaffolding Tests with Stubs and Mocks,"#### Scaffolding Tests with Stubs and Mocks
Background context: The passage suggests that while stubs are used to simplify tests by providing predefined responses, mocks are more detailed and can ensure specific interactions happen. This distinction is crucial for maintaining test isolation and ensuring the correct behavior of dependent services.
:p Why might one choose a mock over a stub in service testing?
??x
One chooses a mock over a stub when they need to verify that certain methods on a dependency were called during the execution of a test. Stubs are used primarily to provide consistent responses, whereas mocks can enforce expected interactions and validate method calls.

Example scenario:
```java
// Mock Example (ensures specific interaction)
public class CustomerServiceTest {
    @Test
    public void testCreateCustomer() throws Exception {
        PointsBankMock pointsBank = new PointsBankMock();
        pointsBank.expects().method(""initializeBalance"").withArgs(123);
        
        // Logic to create a customer and expect the balance initialization.
    }
}
```
x??",1092,"For example, I might tell my stub points bank that when asked for the balance of customer 123, it should return 15,000. The test doesn’t care if the stub is called 0, 1, or 100 times. A variation on t...",qwen2.5:latest,2025-11-02 23:52:47,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Those Tricky End-to-End Tests,Implementing Stubs Manually vs Using Tools,"#### Implementing Stubs Manually vs Using Tools
Background context: The text mentions various methods of creating stubs, from using web servers like Apache or Nginx to embedded Jetty containers. However, it also highlights the utility of tools like Mountebank for programmatically setting up stub endpoints.
:p What is Mountebank and how does it simplify test setup?
??x
Mountebank is a software appliance that can be programmed via HTTP requests to simulate various types of services (HTTP, TCP) as stubs or mocks. It simplifies the process by allowing users to define responses, set expectations, and dynamically manage these configurations without manually setting up servers.

Example usage:
```java
// Starting Mountebank with a specific port and protocol
public class TestSetup {
    public void startMountebank() throws Exception {
        ProcessBuilder pb = new ProcessBuilder(""mountebank"", ""-p"", ""8080"");
        pb.inheritIO();
        Process process = pb.start();
        
        // Configure Mountebank to stub a service.
        URL url = new URL(""http://localhost:8080/stubs/points-bank"");
        HttpURLConnection conn = (HttpURLConnection) url.openConnection();
        conn.setRequestMethod(""PUT"");
        String response = ""{ \""responses\"": [{ \""delay\"": 1, \""body\"": {\""balance\"": 15000} }] }"";
        conn.setDoOutput(true);
        OutputStream os = conn.getOutputStream();
        os.write(response.getBytes());
        os.flush();
    }
}
```
x??",1475,"For example, I might tell my stub points bank that when asked for the balance of customer 123, it should return 15,000. The test doesn’t care if the stub is called 0, 1, or 100 times. A variation on t...",qwen2.5:latest,2025-11-02 23:52:47,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Those Tricky End-to-End Tests,Deploying Service Tests with Dependency Simulations,"#### Deploying Service Tests with Dependency Simulations
Background context: The passage concludes by discussing the implications of using stubs and mocks in deployment scenarios. It emphasizes that while tests may pass when only the primary service is tested, changes could potentially break dependent services.
:p What potential risk does Mountebank testing face during deployment?
??x
Mountebank testing can introduce a risk where passing tests for a single service do not guarantee that all dependent services will work correctly in production. This means that while the customer service passes tests using the stubbed points bank, changes could potentially break other services like the helpdesk and web shop if they rely on the same behavior.

Example scenario:
```java
public class DeploymentTest {
    @Test
    public void testDeployment() throws Exception {
        // Start Mountebank to simulate the points bank.
        startMountebank();
        
        // Test customer service with stubbed points bank.
        CustomerService customerService = new CustomerService();
        assertTrue(customerService.verifyCustomerBalance(123, 15000));
        
        // Deploy the customer service and verify that helpdesk and web shop also work correctly.
    }
}
```
x??

---",1283,"For example, I might tell my stub points bank that when asked for the balance of customer 123, it should return 15,000. The test doesn’t care if the stub is called 0, 1, or 100 times. A variation on t...",qwen2.5:latest,2025-11-02 23:52:47,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Those Tricky End-to-End Tests,End-to-End Tests Overview,"#### End-to-End Tests Overview
Background context: In a microservice architecture, end-to-end tests are crucial to ensure that all services work together as expected. These tests drive functionality through user interfaces and cover a wide range of the system, providing high confidence but at the cost of speed and diagnostic complexity.
:p What is the purpose of end-to-end tests in a microservice system?
??x
End-to-end tests aim to verify the interaction between multiple services in a microservice architecture. They simulate real-world user interactions and provide an overview of how different components work together, ensuring that changes do not introduce bugs or break existing functionality.
```java
// Example pseudocode for running end-to-end tests
public void runEndToEndTests() {
    // Deploy all services together
    deployServices();
    
    // Run tests against the deployed services
    runUserInterfaceTests();
}
```
x??",944,"Of course, we have forgotten the important tests at the top of the pyramid: the end-to-end tests. Those Tricky End-to-End Tests In a microservice system, the capabilities we expose via our user interf...",qwen2.5:latest,2025-11-02 23:53:00,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Those Tricky End-to-End Tests,Deployment Strategy for End-to-End Tests,"#### Deployment Strategy for End-to-End Tests
Background context: To effectively manage end-to-end tests, a deployment strategy that integrates them with other pipelines is necessary. This ensures that changes in one service can trigger tests on dependent services without manual intervention.
:p How should end-to-end tests be integrated into the CI/CD pipeline?
??x
End-to-end tests should be part of a multi-pipeline model where each service's build triggers an integration test stage, which then runs end-to-end tests. This approach ensures that any change in one service can trigger tests on dependent services automatically.
```java
// Pseudocode for integrating end-to-end tests into CI/CD pipeline
public void integrateEndToEndTests() {
    // Define a listener for service builds
    buildListener = new BuildListener();
    
    // Register the listener to trigger integration tests
    buildListener.registerTrigger(new IntegrationTestTrigger());
    
    // Whenever a build is triggered, run end-to-end tests
    buildListener.onBuildTriggered(() -> runEndToEndTests());
}
```
x??",1093,"Of course, we have forgotten the important tests at the top of the pyramid: the end-to-end tests. Those Tricky End-to-End Tests In a microservice system, the capabilities we expose via our user interf...",qwen2.5:latest,2025-11-02 23:53:00,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Those Tricky End-to-End Tests,Fan-in Model for End-to-End Tests,"#### Fan-in Model for End-to-End Tests
Background context: A fan-in model allows multiple services to trigger end-to-end tests in a single stage. This approach ensures that changes are validated across all dependent components without duplicating effort or deploying unnecessary services.
:p What is the benefit of using a fan-in model for end-to-end tests?
??x
The fan-in model benefits from avoiding redundant deployments and tests by having multiple pipelines trigger the same end-to-end test stage. It ensures that every time a service changes, its related tests are run, but it also validates interactions with other services without additional manual steps.
```java
// Pseudocode for implementing fan-in model
public void implementFanInModel() {
    // Define an integration test pipeline
    IntegrationTestPipeline pipeline = new IntegrationTestPipeline();
    
    // Add multiple service build stages to the pipeline
    ServiceAStage stageA = new ServiceAStage();
    ServiceBStage stageB = new ServiceBStage();
    
    // Fan-in by adding both stages to the end-to-end tests stage
    EndToEndTestsStage endToEndStage = new EndToEndTestsStage();
    endToEndStage.addStage(stageA);
    endToEndStage.addStage(stageB);
}
```
x??",1240,"Of course, we have forgotten the important tests at the top of the pyramid: the end-to-end tests. Those Tricky End-to-End Tests In a microservice system, the capabilities we expose via our user interf...",qwen2.5:latest,2025-11-02 23:53:00,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Those Tricky End-to-End Tests,Handling Different Versions in End-to-End Tests,"#### Handling Different Versions in End-to-End Tests
Background context: When deploying services, it's critical to determine which versions of dependent services should be used for end-to-end tests. This decision can impact the accuracy and reliability of the test results.
:p How do you handle different versions of services during end-to-end testing?
??x
To handle different versions of services, you can either use production versions or the latest staging versions, depending on the CI/CD pipeline's maturity. A common approach is to fan in multiple pipelines that run end-to-end tests against both stable and staging environments.
```java
// Pseudocode for handling different service versions
public void handleServiceVersions() {
    // Define a method to get the correct version based on deployment stage
    ServiceVersion getCorrectVersion(String stage) {
        if (stage.equals(""production"")) {
            return productionVersion;
        } else {
            return stagingVersion;
        }
    }
    
    // Use this method in end-to-end tests
    EndToEndTestsStage endToEndStage = new EndToEndTestsStage(getCorrectVersion);
}
```
x??",1152,"Of course, we have forgotten the important tests at the top of the pyramid: the end-to-end tests. Those Tricky End-to-End Tests In a microservice system, the capabilities we expose via our user interf...",qwen2.5:latest,2025-11-02 23:53:00,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Those Tricky End-to-End Tests,Avoiding Redundant Tests and Effort,"#### Avoiding Redundant Tests and Effort
Background context: Running the same tests across multiple services can lead to redundant effort. It's essential to design a pipeline that minimizes this redundancy while ensuring comprehensive coverage.
:p How can you avoid redundant tests in end-to-end testing pipelines?
??x
To avoid redundant tests, ensure that each service has its own local and integration tests before running end-to-end tests. This approach ensures that the end-to-end tests focus only on interactions between services rather than duplicating functionality.
```java
// Pseudocode for avoiding redundant tests
public void avoidRedundantTests() {
    // Define a pipeline stage for each service's local and integration tests
    ServiceATestStage stageA = new ServiceATestStage();
    ServiceBTestStage stageB = new ServiceBTestStage();
    
    // Fan in these stages to the end-to-end test stage
    EndToEndTestsStage endToEndStage = new EndToEndTestsStage();
    endToEndStage.addStage(stageA);
    endToEndStage.addStage(stageB);
}
```
x??",1058,"Of course, we have forgotten the important tests at the top of the pyramid: the end-to-end tests. Those Tricky End-to-End Tests In a microservice system, the capabilities we expose via our user interf...",qwen2.5:latest,2025-11-02 23:53:00,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Consumer-Driven Tests to the Rescue,Flaky and Brittle Tests,"---
#### Flaky and Brittle Tests
Background context explaining the concept. As test scope increases, so too do the number of moving parts, which can introduce issues that do not relate to the functionality under test but rather to external factors like service availability or network conditions.

:p What are flaky tests in end-to-end testing?
??x
Flaky tests are those that sometimes fail and pass when re-run. They do not provide reliable information about the state of the system being tested because their failure is often due to external factors such as service outages or temporary network glitches rather than actual bugs in the code.

For example, consider a test verifying an order placement process involving multiple services:
```java
public class OrderPlacementTest {
    @Test
    public void shouldPlaceOrder() {
        // Code to place an order through multiple services
        // Test might fail due to one of the services being down or network issues.
    }
}
```
x??",987,"Downsides to End-to-End Testing There are, unfortunately, many disadvantages to end-to-end testing. Flaky and Brittle Tests As test scope increases, so too do the number of moving parts. These moving ...",qwen2.5:latest,2025-11-02 23:53:07,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Consumer-Driven Tests to the Rescue,Normalization of Deviance,"#### Normalization of Deviance
Explanation of Diane Vaughan's concept and its relevance in software testing. It refers to a situation where, over time, deviant behavior (in this case, flaky tests) becomes accepted as normal within an organization.

:p What does normalization of deviance mean in the context of end-to-end testing?
??x
Normalization of deviance means that over time, organizations can become accustomed to accepting substandard or problematic behaviors (like consistently failing but unreliable tests) as being acceptable. This acceptance can lead to a loss of faith in the test suite and result in real issues going unnoticed.

For instance, if a flaky test fails frequently and everyone just re-runs it hoping it will pass next time, this could indicate normalization of deviance.
x??",802,"Downsides to End-to-End Testing There are, unfortunately, many disadvantages to end-to-end testing. Flaky and Brittle Tests As test scope increases, so too do the number of moving parts. These moving ...",qwen2.5:latest,2025-11-02 23:53:07,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Consumer-Driven Tests to the Rescue,Test Ownership for End-to-End Tests,"#### Test Ownership for End-to-End Tests
Explanation that end-to-end tests covering specific services should ideally be written by the team responsible for those services.

:p Who is typically responsible for writing end-to-end tests?
??x
The team that owns a particular service should write and maintain its end-to-end tests. This ensures accountability and understanding of the system's behavior from both the developers and testers.

For example, if there are multiple teams involved in a project, and one team is responsible for the order placement service, then this team would be expected to write all related end-to-end tests.
x??

---",642,"Downsides to End-to-End Testing There are, unfortunately, many disadvantages to end-to-end testing. Flaky and Brittle Tests As test scope increases, so too do the number of moving parts. These moving ...",qwen2.5:latest,2025-11-02 23:53:07,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Consumer-Driven Tests to the Rescue,Test Ownership and Shared Codebase Approach,"#### Test Ownership and Shared Codebase Approach

Background context: The passage discusses a common issue where end-to-end tests become a free-for-all, leading to an explosion of test cases without clear ownership. This can result in broken tests being ignored or becoming outdated due to lack of engagement from the original code developers.

:p How can teams ensure that the health and maintenance of their end-to-end test suite are managed effectively?
??x
To ensure effective management of the end-to-end test suite, it is essential to treat it as a shared codebase with joint ownership. Teams should be free to check in tests but must share responsibility for maintaining and improving the overall health of the suite. This approach helps align developers with their own testing responsibilities, reducing the likelihood of ignoring broken or obsolete tests.

```java
// Example of how teams can collaborate on test maintenance
public class TestCollaboration {
    public static void main(String[] args) {
        // Pseudocode for a shared responsibility mechanism
        List<String> teamMembers = Arrays.asList(""TeamA"", ""TeamB"", ""TeamC"");
        
        while (true) {
            int randomIndex = new Random().nextInt(teamMembers.size());
            String currentOwner = teamMembers.get(randomIndex);
            
            System.out.println(""Current owner: "" + currentOwner);
            // Code to run maintenance tasks and ensure tests are up-to-date
            // This could involve writing, reviewing, or fixing tests
            
            try {
                Thread.sleep(30 * 60 * 1000); // Sleep for half an hour before next round
            } catch (InterruptedException e) {
                System.out.println(""Thread was interrupted, failed to complete operation"");
            }
        }
    }
}
```
x??",1842,"I have seen a number of anti-patterns caused here. These tests become a free-for-all, with all teams granted access to add tests without any understanding of the health of the whole suite. This can of...",qwen2.5:latest,2025-11-02 23:53:21,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Consumer-Driven Tests to the Rescue,Test Suite Performance and Flakiness,"#### Test Suite Performance and Flakiness

Background context: The text highlights the challenges of managing long-running test suites that can be flaky. This includes issues like slow execution times and frequent false positives or negatives.

:p How does the length and reliability of end-to-end tests affect team productivity?
??x
The length and reliability of end-to-end tests significantly impact team productivity. Long-running tests can consume large amounts of time, making them less effective for rapid feedback cycles. Flaky tests add to this problem by generating false positives or negatives, leading to unnecessary debugging sessions and decreased trust in the test suite.

```java
// Example of a simple timer class to measure test execution time
public class TestExecutionTimer {
    public static void main(String[] args) {
        long startTime = System.currentTimeMillis();
        
        // Simulate running a test that takes some time
        try {
            Thread.sleep(10 * 60 * 1000); // Sleep for 10 minutes
        } catch (InterruptedException e) {
            Thread.currentThread().interrupt();
            return;
        }
        
        long endTime = System.currentTimeMillis();
        long executionTime = endTime - startTime;
        System.out.println(""Test took: "" + executionTime / 60000 + "" minutes"");
    }
}
```
x??",1364,"I have seen a number of anti-patterns caused here. These tests become a free-for-all, with all teams granted access to add tests without any understanding of the health of the whole suite. This can of...",qwen2.5:latest,2025-11-02 23:53:21,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Consumer-Driven Tests to the Rescue,Parallel Execution of Tests,"#### Parallel Execution of Tests

Background context: The text suggests using parallel test execution tools, such as Selenium Grid, to mitigate the issues with slow and flaky tests. This approach can help distribute load across multiple machines or browsers.

:p How can parallel execution improve test suite performance?
??x
Parallel execution improves test suite performance by distributing the workload across multiple threads or machines. This reduces the overall time taken for the entire test suite to run, making it more efficient and faster.

```java
// Example of how to use a Selenium Grid to run tests in parallel
public class ParallelTestRunner {
    public static void main(String[] args) throws MalformedURLException {
        // Register with the remote hub
        DesiredCapabilities desiredCapabilities = new DesiredCapabilities();
        URL gridHubUrl = new URL(""http://localhost:4444/wd/hub"");
        
        WebDriver driver = new RemoteWebDriver(gridHubUrl, desiredCapabilities);
        
        try {
            // Simulate running a test in parallel on the remote node
            driver.get(""https://example.com"");
            System.out.println(""Test passed on "" + gridHubUrl);
            
        } finally {
            driver.quit();
        }
    }
}
```
x??",1295,"I have seen a number of anti-patterns caused here. These tests become a free-for-all, with all teams granted access to add tests without any understanding of the health of the whole suite. This can of...",qwen2.5:latest,2025-11-02 23:53:21,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Consumer-Driven Tests to the Rescue,Avoiding Test Duplication and Redundancy,"#### Avoiding Test Duplication and Redundancy

Background context: The text mentions the importance of curating end-to-end test suites to reduce overlap in test coverage. This is crucial for maintaining a manageable and efficient testing process.

:p How can teams ensure they are not duplicating tests or leaving gaps in their test coverage?
??x
To avoid duplicating tests or leaving gaps, teams should regularly review and refactor their test suite to ensure comprehensive but non-redundant coverage. This involves identifying overlapping tests that can be merged or refactored into more generalized versions.

```java
// Example of how to refactor redundant tests
public class TestRefactoring {
    public static void main(String[] args) {
        // Original tests for two similar functionalities
        System.out.println(""Test 1: "" + testFunctionalityA());
        System.out.println(""Test 2: "" + testFunctionalityB());
        
        // Refactored version to avoid redundancy
        System.out.println(""Refactored Test: "" + refactorTests());
    }
    
    private static boolean testFunctionalityA() {
        return true; // Simulate a passing test for functionality A
    }
    
    private static boolean testFunctionalityB() {
        return true; // Simulate a passing test for functionality B
    }
    
    private static boolean refactorTests() {
        return testFunctionalityA() && testFunctionalityB(); // Refactor into one generalized test
    }
}
```
x??

---",1486,"I have seen a number of anti-patterns caused here. These tests become a free-for-all, with all teams granted access to add tests without any understanding of the health of the whole suite. This can of...",qwen2.5:latest,2025-11-02 23:53:21,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Consumer-Driven Tests to the Rescue,Risk and Test Curation,"#### Risk and Test Curation
Background context: The text discusses the challenges of balancing test value versus burden, especially with large-scale test suites. It mentions that while removing tests can reduce running time, it increases risk if a bug is introduced.

:p How does the balance between test value and burden affect test management?
??x
The balance between test value and burden affects test management by requiring a careful assessment of risk. If a feature is covered in multiple tests, eliminating redundant ones can save significant time but may increase the risk of undetected bugs. Effective test curation requires a deep understanding of risks associated with each test.

```java
public class TestCurationExample {
    public boolean shouldRemoveTest(Test oldTest, Test newTest) {
        // Check if both tests cover the same feature
        boolean coversSameFeature = oldTest.coveredFeature().equals(newTest.coveredFeature());
        
        // If they do, determine if one is redundant based on factors like coverage and time taken
        return coversSameFeature && (oldTest.getTimeTaken() > newTest.getTimeTaken() || !newTest.isCritical());
    }
}
```
x??",1185,It is hard to have a balanced conversation about the value something adds versus the burden it entails. It can also be a difficult risk/reward trade-off. Do you get thanked if you remove a test? Maybe...,qwen2.5:latest,2025-11-02 23:53:31,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Consumer-Driven Tests to the Rescue,Feedback Cycles in End-to-End Tests,"#### Feedback Cycles in End-to-End Tests
Background context: The text highlights the issue of long feedback cycles with end-to-end tests, which can lead to a pile-up when integration breaks. This affects developer productivity and deployment timelines.

:p How do long feedback cycles impact deployments?
??x
Long feedback cycles impact deployments by increasing the time required to fix issues. When an end-to-end test suite is lengthy, fixing a break takes longer, reducing the frequency of successful tests. This can result in fewer services being deployable into production, leading to pile-ups where new changes accumulate while waiting for builds.

```java
public class FeedbackCycleExample {
    public void updateDeploymentStatus(int buildTime) {
        if (buildTime > 60 * 5) { // Build time of more than 5 minutes
            System.out.println(""Pile-up warning: Long feedback cycle detected."");
        }
    }
}
```
x??",933,It is hard to have a balanced conversation about the value something adds versus the burden it entails. It can also be a difficult risk/reward trade-off. Do you get thanked if you remove a test? Maybe...,qwen2.5:latest,2025-11-02 23:53:31,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Consumer-Driven Tests to the Rescue,The Metaversion Concept,"#### The Metaversion Concept
Background context: The text introduces the idea of metaversion, where multiple services are versioned together, leading to coupling and a return to monolithic practices. This concept challenges the core advantage of microservices.

:p What is the metaversion and why is it problematic?
??x
The metaversion refers to the practice of treating an entire system as one unit for deployment purposes by versioning all its services together. This approach can lead to coupling, where once-separate services become tightly integrated due to frequent simultaneous deployments. Over time, this can revert the benefits of microservices and result in a tangled mess that is harder to manage.

```java
public class MetaversionExample {
    public void deploySystemVersion(String systemVersion) {
        // Deploy all services with the same version
        for (Service service : services) {
            service.deploy(systemVersion);
        }
    }
}
```
x??",977,It is hard to have a balanced conversation about the value something adds versus the burden it entails. It can also be a difficult risk/reward trade-off. Do you get thanked if you remove a test? Maybe...,qwen2.5:latest,2025-11-02 23:53:31,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Consumer-Driven Tests to the Rescue,Coupling and Microservices,"#### Coupling and Microservices
Background context: The text discusses how coupling can arise when services are deployed together due to frequent simultaneous changes, leading to a less flexible microservice architecture.

:p How does coupling affect the deployment of microservices?
??x
Coupling affects the deployment of microservices by causing services to become interdependent. When multiple services are deployed simultaneously and their versions are tied (metaversion), it reduces the independence that microservices aim to provide. This can make individual service deployments harder, as changes in one service may require redeployments in others, leading to a less modular architecture.

```java
public class MicroserviceCouplingExample {
    public void deployService(Service service) {
        // Check for coupling with other services before deploying
        if (isCoupled(service)) {
            System.out.println(""Deployment blocked: Service is coupled."");
        } else {
            service.deploy();
        }
    }

    private boolean isCoupled(Service service) {
        // Logic to check if the service is coupled with others
        return true; // Simplified for example
    }
}
```
x??

---",1217,It is hard to have a balanced conversation about the value something adds versus the burden it entails. It can also be a difficult risk/reward trade-off. Do you get thanked if you remove a test? Maybe...,qwen2.5:latest,2025-11-02 23:53:31,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Consumer-Driven Tests to the Rescue,Test Journeys Instead of Stories,"---
#### Test Journeys Instead of Stories
In managing a system composed of many services, end-to-end tests can become unmanageably large and complex. This is particularly true when dealing with more than two or three services, leading to a ""Cartesian-like explosion"" of test scenarios. Over time, every new piece of functionality may result in a new end-to-end test, creating a bloated suite that provides poor feedback cycles due to overlaps.
:p Why are end-to-end tests problematic for systems with multiple services?
??x
End-to-end tests become overly complex and unmanageable as the number of services increases. They can lead to a large number of test scenarios (Cartesian explosion) and result in a bloated suite that offers poor feedback cycles due to overlaps in test coverage.
```java
// Example of a simple end-to-end test in Java using JUnit
@Test
public void testOrderCD() {
    // Setup
    OrderService orderService = new OrderService();
    
    // Action
    OrderResult result = orderService.placeOrder(""Customer1"", ""CD123"");
    
    // Assert
    assertEquals(OrderStatus.COMPLETED, result.getStatus());
}
```
x??",1132,"This is bad. Test Journeys, Not Stories Despite the disadvantages just outlined, for many users end-to-end tests can still be manageable with one or two services, and in these situations still make a ...",qwen2.5:latest,2025-11-02 23:53:38,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Consumer-Driven Tests to the Rescue,Consumer-Driven Tests (CDCs),"#### Consumer-Driven Tests (CDCs)
Consumer-driven tests are used to ensure that when a new service is deployed, it won't break its consumers. This method avoids the need for end-to-end testing by defining and enforcing consumer expectations on a service in code form as tests. These tests run as part of the CI build of the producer, ensuring no deployment if any contract is broken.
:p What are consumer-driven tests (CDCs) used for?
??x
Consumer-driven tests (CDCs) ensure that deploying new services does not break their consumers by defining and enforcing expectations in code form. They run during the CI process to prevent deployments that violate these contracts.
```java
// Example of a CDC test in Java using JUnit
@Test
public void testCustomerServiceForHelpdesk() {
    // Setup
    CustomerService customerService = mock(CustomerService.class);
    
    // Action & Assert
    when(customerService.getCustomerDetails(""12345"")).thenReturn(new Customer(""John Doe"", ""Sales Rep""));
    assertEquals(""John Doe"", customerService.getCustomerDetails(""12345"").getName());
}
```
x??

---",1089,"This is bad. Test Journeys, Not Stories Despite the disadvantages just outlined, for many users end-to-end tests can still be manageable with one or two services, and in these situations still make a ...",qwen2.5:latest,2025-11-02 23:53:38,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Pact,Pact Overview,"#### Pact Overview
Pact is a consumer-driven testing tool that was originally developed for Ruby and has since expanded to include JVM and .NET ports. It allows consumers to define expectations about how services will behave, which are then verified by producers.

The process involves creating expectations on the consumer side using a domain-specific language (DSL), launching a local mock server, and running these expectations against it to generate a Pact specification file in JSON format.
:p What is Pact and how does it work?
??x
Pact works by allowing consumers to define what they expect from producers through contracts. These contracts are then verified by the producer's service. The key steps include:
1. Defining expectations using a Ruby DSL on the consumer side.
2. Running these expectations against a local mock server to create a Pact specification file (JSON).
3. Verifying these expectations on the producer side.

Example in pseudocode:
```pseudocode
// Consumer-side code to define expectations and run them against a mock service
define_expectations() {
    // Define interactions between consumer and producer using DSL
}

run_pact_specifications() {
    start_mock_server()
    run_expectations()
    generate_json_file()
}
```
x??",1258,"Pact Pact  is a consumer-driven testing tool that was originally developed in-house at RealEstate.com.au, but is now open source, with Beth Skurrie driving most of the development. Originally just for...",qwen2.5:latest,2025-11-02 23:53:48,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Pact,Pact Specification File,"#### Pact Specification File
The Pact specification file is a formal JSON document that contains the expectations defined by the consumer. It serves as a contract between the producer and consumer, ensuring both services behave as expected.
:p What is the Pact specification file?
??x
The Pact specification file is a JSON-formatted document that encapsulates the interactions and responses expected from one service to another. This file acts as a formal agreement or contract, allowing isolated testing of the consumer against the producer's API.

Example in JSON format:
```json
{
    ""consumer"": {
        ""name"": ""Payment Service""
    },
    ""provider"": {
        ""name"": ""Account Service""
    },
    ""interactions"": [
        {
            ""description"": ""Verify account balance"",
            ""request"": {
                ""method"": ""GET"",
                ""path"": ""/accounts/12345""
            },
            ""response"": {
                ""status"": 200,
                ""body"": {""balance"": 500},
                ""headers"": {""Content-Type"": ""application/json""}
            }
        }
    ]
}
```
x??",1104,"Pact Pact  is a consumer-driven testing tool that was originally developed in-house at RealEstate.com.au, but is now open source, with Beth Skurrie driving most of the development. Originally just for...",qwen2.5:latest,2025-11-02 23:53:48,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Pact,Pact Broker,"#### Pact Broker
The Pact Broker is a tool that helps manage multiple versions of Pact specification files. It allows for storing and versioning these contracts, enabling tests to be run against different versions of the consumer or producer.
:p What is a Pact Broker?
??x
A Pact Broker is a service that stores multiple versions of Pact specification files. This enables teams to run consumer-driven contract tests against various versions of consumers or producers, facilitating better collaboration and version control in development environments.

Example setup with a Pact Broker:
```sh
# Adding the Pact CLI tool for interaction with the broker
pip install pact-jvm-provider-cucumber

# Running the provider app with Pact Broker
./target/pact-jvm-provider-cucumber-1.0.0.jar --broker-url http://localhost:8080
```
x??",823,"Pact Pact  is a consumer-driven testing tool that was originally developed in-house at RealEstate.com.au, but is now open source, with Beth Skurrie driving most of the development. Originally just for...",qwen2.5:latest,2025-11-02 23:53:48,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Pact,Pacto vs Pact,"#### Pacto vs Pact
Pacto is another Ruby tool for consumer-driven testing, which records interactions between client and server to generate expectations dynamically. In contrast, Pact regenerates expectations with every build.

Key differences include:
- **Dynamic vs Static Expectations**: Pacto generates static expectations based on existing interactions, whereas Pact regenerates expectations in the consumer codebase.
- **Usage Context**: Pact is more suitable for services still being developed or as part of a continuous integration pipeline.

Example comparison:
```pseudocode
// Using Pacto to record and generate expectations
generate_expectations() {
    start_server()
    record_interactions()
    generate_static_specifications()
}

// Using Pact to define and regenerate expectations dynamically
regenerate_expectations() {
    define_new_expectations()
    run_tests()
    update_pact_file()
}
```
x??",917,"Pact Pact  is a consumer-driven testing tool that was originally developed in-house at RealEstate.com.au, but is now open source, with Beth Skurrie driving most of the development. Originally just for...",qwen2.5:latest,2025-11-02 23:53:48,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Separating Deployment from Release,Agile Stories as Conversations,"#### Agile Stories as Conversations
Agile stories are placeholders for conversations about what a service API should look like. CDCs (Continuous Dialogues and Contracts) act similarly, serving as codifications of discussions on evolving APIs. They trigger conversations when issues arise.
:p What is the purpose of using Continuous Dialogue and Contracts (CDCs) in agile development?
??x
CDCs are used to codify conversations about service APIs and their evolution. When these contracts break, they prompt new dialogues to refine or update the API design. This ensures that both consumer and producing services maintain good communication and trust.
x??",653,"It’s About Conversations In agile, stories are often referred to as a placeholder for a conversation. CDCs are just like that. They become the codification of a set of discussions about what a service...",qwen2.5:latest,2025-11-02 23:54:01,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Separating Deployment from Release,End-to-End Tests vs CDCs,"#### End-to-End Tests vs CDCs
End-to-end tests have significant disadvantages as systems grow more complex. Many organizations prefer CDCs (Continuous Dialogues and Contracts) over extensive end-to-end testing, especially in microservices environments. However, some still use limited end-to-end tests for semantic monitoring.
:p Should you rely on end-to-end tests or CDCs?
??x
End-to-end tests are useful during the learning phase of implementing microservices but can be phased out as you improve CDCs and production monitoring techniques. In some organizations with a low appetite for learning in production, they may continue to use extensive end-to-end testing, though this approach is generally seen as less efficient.
x??",729,"It’s About Conversations In agile, stories are often referred to as a placeholder for a conversation. CDCs are just like that. They become the codification of a set of discussions about what a service...",qwen2.5:latest,2025-11-02 23:54:01,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Separating Deployment from Release,Semantic Monitoring,"#### Semantic Monitoring
Semantic monitoring uses existing journey tests to monitor the production system after deployment, instead of running new tests every time. This allows teams to focus on improving other aspects like CDCs and monitoring without compromising safety.
:p How does semantic monitoring work?
??x
Semantic monitoring leverages pre-existing end-to-end tests run in a production-like environment to detect issues before they impact users. These tests help ensure the system behaves as expected once deployed, acting more like continuous integration for the live environment.
x??",594,"It’s About Conversations In agile, stories are often referred to as a placeholder for a conversation. CDCs are just like that. They become the codification of a set of discussions about what a service...",qwen2.5:latest,2025-11-02 23:54:01,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Separating Deployment from Release,Smoke Tests and Blue/Green Deployments,"#### Smoke Tests and Blue/Green Deployments
Smoke tests are small sets of tests run against newly deployed software to confirm that basic functionality is working correctly in a new environment. Blue/green deployments involve running two versions of a service simultaneously, with traffic directed only to one version until it passes all tests.
:p What is the purpose of smoke tests?
??x
The purpose of smoke tests is to quickly verify that a newly deployed microservice functions correctly without full-scale production traffic. This helps catch environment-specific issues early in the deployment process.
```java
public class SmokeTestRunner {
    public void runSmokeTests() {
        // Test basic functionality like database connections, API calls, etc.
    }
}
```
x??",775,"It’s About Conversations In agile, stories are often referred to as a placeholder for a conversation. CDCs are just like that. They become the codification of a set of discussions about what a service...",qwen2.5:latest,2025-11-02 23:54:01,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Separating Deployment from Release,Blue/Green Deployments,"#### Blue/Green Deployments
Blue/green deployments involve running two versions of a service simultaneously. One version receives real production traffic while the other is tested in situ for issues before going live. This approach minimizes downtime and allows for easy rollbacks if needed.
:p How does blue/green deployment work?
??x
In blue/green deployments, you deploy both old (blue) and new (green) versions of a microservice side-by-side. Initially, only the old version receives traffic. Once tested, the traffic is switched to the new version, which then becomes live while the previous version remains available as a fallback.
```java
public class DeploymentManager {
    public void deployNewVersion(String serviceName) {
        // Deploy both blue and green versions
        // Test the green version in situ
        if (testsPass()) {
            redirectTraffic(""green"");
        } else {
            rollback();
        }
    }

    private boolean testsPass() {
        // Run in situ tests on the new version
        return true; // Placeholder for actual test logic
    }

    private void redirectTraffic(String version) {
        // Redirect traffic to the specified version
    }

    private void rollback() {
        // Roll back to the old version if issues are detected
    }
}
```
x??",1312,"It’s About Conversations In agile, stories are often referred to as a placeholder for a conversation. CDCs are just like that. They become the codification of a set of discussions about what a service...",qwen2.5:latest,2025-11-02 23:54:01,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Separating Deployment from Release,Canary Releasing,"#### Canary Releasing
Canary releasing is a technique where a small subset of users or traffic is directed to a new microservice version. If it passes, more users are gradually introduced until full-scale deployment. This allows for gradual and controlled rollouts.
:p What is canary releasing?
??x
Canary releasing involves deploying a new version of a microservice to a small group of users first. If the new version performs well, it is rolled out incrementally to more users before being fully deployed, allowing issues to be identified and addressed in a controlled manner.
```java
public class CanaryReleaseManager {
    public void canaryRelease(String serviceName) {
        // Deploy new version with a small percentage of traffic
        if (testsPassForCanary()) {
            graduallyIncreaseTraffic();
        } else {
            rollback();
        }
    }

    private boolean testsPassForCanary() {
        // Run initial tests on the canary group
        return true; // Placeholder for actual test logic
    }

    private void graduallyIncreaseTraffic() {
        // Gradually increase traffic to the new version
    }

    private void rollback() {
        // Roll back to the old version if issues are detected
    }
}
```
x??
---",1253,"It’s About Conversations In agile, stories are often referred to as a placeholder for a conversation. CDCs are just like that. They become the codification of a set of discussions about what a service...",qwen2.5:latest,2025-11-02 23:54:01,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Performance Tests,Canary Releasing,"#### Canary Releasing
Canary releasing involves verifying newly deployed software by directing a portion of production traffic to a new version while comparing it against an existing baseline. This approach helps ensure that the new release performs as expected without impacting all users at once, allowing for gradual rollouts and easier rollback mechanisms.
:p What is canary releasing?
??x
Canary releasing is a method where a small subset of production traffic is directed to a newly deployed version (the ""canary"") while the majority remains on the current baseline version. This allows developers to observe how the new software performs in real-world conditions, compare metrics such as response times and error rates, before gradually increasing the amount of traffic directed to it.
```java
// Example Pseudocode for Canary Releasing
public class CanaryReleaser {
    private final int totalTraffic;
    private final int canaryTraffic;

    public CanaryReleaser(int totalTraffic) {
        this.totalTraffic = totalTraffic;
        // Assume 10% of traffic is canary
        this.canaryTraffic = (int)(totalTraffic * 0.1);
    }

    public boolean shouldServeCanary(int requestNumber) {
        return requestNumber <= canaryTraffic;
    }
}
```
x??",1262,"Canary Releasing With canary releasing, we are verifying our newly deployed software by directing amounts of production traffic against the system to see if it performs as expected. “Performing as exp...",qwen2.5:latest,2025-11-02 23:54:13,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Performance Tests,Differences Between Canary Releasing and Blue/Green Deployment,"#### Differences Between Canary Releasing and Blue/Green Deployment
While both methods involve deploying new versions, the key difference lies in their duration of coexistence and the approach to traffic distribution. In blue/green deployment, a new version is deployed alongside an existing one, but once the new version is ready, all traffic is quickly switched over, making it a more abrupt transition compared to canary releasing.
:p How does canary releasing differ from blue/green deployment?
??x
In blue/green deployment, both the old and new versions coexist briefly before switching all traffic to the new version. Canary releasing, on the other hand, allows for longer periods where multiple versions are active simultaneously, with a gradual ramp-up of traffic directed towards the new version.
```java
// Example Pseudocode for Blue/Green Deployment
public class BlueGreenReleaser {
    private final int totalTraffic;
    private final int blueTraffic;

    public BlueGreenReleaser(int totalTraffic) {
        this.totalTraffic = totalTraffic;
        // Assume 10% of traffic is blue (baseline)
        this.blueTraffic = (int)(totalTraffic * 0.9);
    }

    public boolean shouldServeBlue(int requestNumber) {
        return requestNumber <= blueTraffic;
    }
}
```
x??",1287,"Canary Releasing With canary releasing, we are verifying our newly deployed software by directing amounts of production traffic against the system to see if it performs as expected. “Performing as exp...",qwen2.5:latest,2025-11-02 23:54:13,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Performance Tests,Shadowing Production Traffic,"#### Shadowing Production Traffic
Shadowing production traffic involves directing a copy of the production load to the canary version without affecting external users. This allows for a more controlled comparison between the baseline and new versions, reducing the risk of exposing any issues that might arise during testing.
:p What is shadowing production traffic?
??x
Shadowing production traffic means routing a subset of live requests from production to the canary version while maintaining the original responses externally. This approach helps in comparing the performance and behavior of both versions without customer disruption, making it easier to detect any issues before full-scale deployment.
```java
// Example Pseudocode for Shadowing Production Traffic
public class TrafficShifter {
    private final int totalTraffic;
    private final int shadowTraffic;

    public TrafficShifter(int totalTraffic) {
        this.totalTraffic = totalTraffic;
        // Assume 5% of traffic is being shadowed
        this.shadowTraffic = (int)(totalTraffic * 0.05);
    }

    public boolean shouldShadow(int requestNumber) {
        return requestNumber <= shadowTraffic;
    }
}
```
x??",1191,"Canary Releasing With canary releasing, we are verifying our newly deployed software by directing amounts of production traffic against the system to see if it performs as expected. “Performing as exp...",qwen2.5:latest,2025-11-02 23:54:13,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Performance Tests,Mean Time to Repair and Mean Time Between Failures,"#### Mean Time to Repair and Mean Time Between Failures
These metrics are crucial for understanding the reliability of a system in production. MTTR (Mean Time to Repair) measures how quickly an issue can be resolved, while MTBF (Mean Time Between Failures) assesses the expected time between failures.
:p What do MTTR and MTBF stand for?
??x
MTTR stands for Mean Time to Repair, which is a measure of the average time taken to identify and fix issues in a system. MTBF stands for Mean Time Between Failures, indicating the average duration a system operates without failure before needing repair or replacement.
```java
// Example Pseudocode for Calculating MTTR and MTBF
public class ReliabilityMetrics {
    public double calculateMTTR(double downtime) {
        // Assume downtime is in hours
        return downtime;
    }

    public double calculateMTBF(double uptime) {
        // Assume uptime is in hours
        return uptime;
    }
}
```
x??

---",957,"Canary Releasing With canary releasing, we are verifying our newly deployed software by directing amounts of production traffic against the system to see if it performs as expected. “Performing as exp...",qwen2.5:latest,2025-11-02 23:54:13,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Performance Tests,MTBF and MTTR Trade-off,"---
#### MTBF and MTTR Trade-off
Background context explaining the concept of Mean Time Between Failures (MTBF) and Mean Time To Repair (MTTR). These terms are crucial for understanding how to balance system reliability with maintenance efficiency. Organizations often focus on reducing both MTBF and MTTR, but there is a trade-off between optimizing these two metrics.

In web operations, techniques like blue/green deployments can be used to quickly revert to the previous version if issues arise in production. This reduces the impact on users by minimizing downtime.

:p What does MTBF stand for, and what does it measure?
??x
MTBF stands for Mean Time Between Failures. It measures the average time a system operates without failing.
x??",742,"In the web operations world, this is often referred to as the trade-off between optimizing for  mean time between failures (MTBF)  and  mean time to repair (MTTR) . Techniques to reduce the time to re...",qwen2.5:latest,2025-11-02 23:54:21,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Performance Tests,Blue/Green Deployment,"#### Blue/Green Deployment
Explanation of blue/green deployments as a technique to quickly rollback changes in production if issues are detected.

:p How does a blue/green deployment work?
??x
In a blue/green deployment, you deploy a new version of your application to a set of machines (green) while keeping the current version running on another set of machines (blue). Users continue to use the old version until you are confident that the new version is stable. Once it's confirmed, traffic is switched over to the new version.

Example code for switching traffic might look like this in a simplified scenario:
```java
public void switchTraffic(String deploymentType) {
    if (""green"".equals(deploymentType)) {
        // Serve requests from the green environment
        serveGreenRequests();
    } else {
        // Serve requests from the blue environment
        serveBlueRequests();
    }
}
```
x??",908,"In the web operations world, this is often referred to as the trade-off between optimizing for  mean time between failures (MTBF)  and  mean time to repair (MTTR) . Techniques to reduce the time to re...",qwen2.5:latest,2025-11-02 23:54:21,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Performance Tests,Nonfunctional Requirements (NFRs),"#### Nonfunctional Requirements (NFRs)
Explanation of nonfunctional requirements and how they differ from functional requirements. The term ""nonfunctional"" can be misleading, as some NFRs are quite important to system performance.

:p What is an example of a nonfunctional requirement?
??x
An example of a nonfunctional requirement is the acceptable latency of a web page. This measures how long users have to wait for content to load and can significantly impact user satisfaction and retention.
x??",500,"In the web operations world, this is often referred to as the trade-off between optimizing for  mean time between failures (MTBF)  and  mean time to repair (MTTR) . Techniques to reduce the time to re...",qwen2.5:latest,2025-11-02 23:54:21,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Performance Tests,Cross-Functional Testing (CFT),"#### Cross-Functional Testing (CFT)
Explanation that cross-functional testing refers to testing system behaviors that result from cross-cutting work, such as performance or security.

:p What is the purpose of cross-functional testing?
??x
The purpose of cross-functional testing is to verify nonfunctional requirements (NFRs) that are critical for the overall functionality and user experience of a system. Examples include checking how responsive the application should be under load or ensuring data privacy regulations are met.
x??",535,"In the web operations world, this is often referred to as the trade-off between optimizing for  mean time between failures (MTBF)  and  mean time to repair (MTTR) . Techniques to reduce the time to re...",qwen2.5:latest,2025-11-02 23:54:21,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Performance Tests,Property Testing,"#### Property Testing
Explanation that property testing falls into the quadrant where tests are designed to see if the system is moving towards meeting nonfunctional requirements.

:p What does property testing involve?
??x
Property testing involves defining properties of a system and checking whether these properties hold true during different stages of development. For example, performance testing checks if the application can handle the expected number of users without significant degradation in response time.
x??

---",527,"In the web operations world, this is often referred to as the trade-off between optimizing for  mean time between failures (MTBF)  and  mean time to repair (MTTR) . Techniques to reduce the time to re...",qwen2.5:latest,2025-11-02 23:54:21,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Performance Tests,Trade-offs Based on Service Durability Requirements,"#### Trade-offs Based on Service Durability Requirements
Background context: The text discusses how different services within a microservice-based system can have varying requirements for durability, downtime tolerance, and other cross-functional requirements (CFRs). This flexibility allows teams to make trade-offs based on their business needs. For example, payment service might require higher reliability, while music recommendation could tolerate some downtime.
:p What are the key points about making trade-offs in a microservice-based system?
??x
The text emphasizes that different services can have varying requirements for durability and downtime tolerance. Teams should identify which services need high availability (like payment services) versus those where brief outages are acceptable (like music recommendation). This approach allows fine-grained control over service reliability, impacting the overall design of the microservices architecture.
x??",964,"For some CFRs, you may want to track them at an individual service level. For example, you may decide that the durability of service you require from your payment service is significantly higher, but ...",qwen2.5:latest,2025-11-02 23:54:36,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Performance Tests,Fine-Grained Nature of Microservice-Based Systems,"#### Fine-Grained Nature of Microservice-Based Systems
Background context: The text highlights that in a microservice-based system, services can be designed to meet specific durability and downtime requirements. This granularity enables teams to make targeted trade-offs between different services within the same application.
:p How does the fine-grained nature of microservices affect trade-offs?
??x
The fine-grained nature of microservices allows for more detailed control over the reliability and availability of individual services. By treating each service as a separate unit, teams can tailor their design to meet specific requirements without affecting other parts of the system.
For example:
```java
@Service
public class PaymentService {
    // High durability and low downtime required here
}

@Service
public class MusicRecommendationService {
    // Lower durability with acceptable short downtimes
}
```
x??",922,"For some CFRs, you may want to track them at an individual service level. For example, you may decide that the durability of service you require from your payment service is significantly higher, but ...",qwen2.5:latest,2025-11-02 23:54:36,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Performance Tests,Performance Tests in Microservice-Based Systems,"#### Performance Tests in Microservice-Based Systems
Background context: The text explains that microservices can introduce additional network calls, which can impact performance. Therefore, it's crucial to perform performance tests at various levels of granularity.
:p Why are performance tests important in microservices?
??x
Performance tests are essential because they help identify and mitigate potential bottlenecks introduced by the increased number of network calls between microservices. These tests ensure that critical operations meet performance expectations, which can significantly impact user experience and system reliability.

```java
public class PerformanceTest {
    @Test
    public void testEndToEndLoad() throws Exception {
        // Simulate a high volume of requests to the entire system
        for (int i = 0; i < 1000; i++) {
            performRequest();
        }
    }

    private void performRequest() {
        // Logic to simulate an HTTP request
        // Measure response time and error rates
    }
}
```
x??",1047,"For some CFRs, you may want to track them at an individual service level. For example, you may decide that the durability of service you require from your payment service is significantly higher, but ...",qwen2.5:latest,2025-11-02 23:54:36,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Performance Tests,Early Involvement of Cross-Functional Requirements (CFRs),"#### Early Involvement of Cross-Functional Requirements (CFRs)
Background context: The text advises on the importance of considering CFRs early in the development process. This ensures that non-functional requirements are integrated into the design from the beginning, rather than as an afterthought.
:p Why should cross-functional requirements be considered early?
??x
Considering cross-functional requirements (CFRs) early helps integrate non-functional aspects like performance, security, and accessibility into the system design at the outset. This proactive approach ensures that these requirements are not overlooked or addressed haphazardly during later stages of development.

For example:
```java
public class AccessibilityTest {
    @Test
    public void testMarkupAccessibility() throws Exception {
        // Generate HTML markup and validate against accessibility standards
        Document doc = generateHTML();
        boolean isValid = isAccessible(doc);
        assertTrue(isValid, ""HTML does not meet accessibility requirements"");
    }

    private Document generateHTML() {
        // Logic to generate HTML content
        return null;
    }

    private boolean isAccessible(Document doc) {
        // Validation logic for accessibility
        return true;
    }
}
```
x??",1295,"For some CFRs, you may want to track them at an individual service level. For example, you may decide that the durability of service you require from your payment service is significantly higher, but ...",qwen2.5:latest,2025-11-02 23:54:36,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Performance Tests,End-to-End Performance Testing,"#### End-to-End Performance Testing
Background context: The text mentions the importance of end-to-end performance testing, especially when decomposing systems into microservices. This type of testing helps ensure that core journeys within the system meet performance expectations.
:p What is an example of an end-to-End performance test?
??x
An end-to-end performance test involves simulating a complete user journey through the system to measure overall performance and identify potential bottlenecks. For instance, if a system includes multiple microservices, you might simulate a process where a user logs in, searches for products, adds items to a cart, checks out, and receives an order confirmation.

```java
public class EndToEndPerformanceTest {
    @Test
    public void testLoginAndCheckout() throws Exception {
        // Simulate the entire checkout process
        performLogin();
        performSearch();
        addItemsToCart();
        proceedToCheckout();
        confirmOrder();

        // Measure response time and error rates
    }

    private void performLogin() {
        // Logic to simulate a login request
    }

    private void performSearch() {
        // Logic to search for products
    }

    private void addItemsToCart() {
        // Logic to add items to the cart
    }

    private void proceedToCheckout() {
        // Logic to proceed to checkout and place an order
    }

    private void confirmOrder() {
        // Logic to confirm the order
    }
}
```
x??",1501,"For some CFRs, you may want to track them at an individual service level. For example, you may decide that the durability of service you require from your payment service is significantly higher, but ...",qwen2.5:latest,2025-11-02 23:54:36,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Performance Tests,Performance Testing Individual Services,"#### Performance Testing Individual Services
Background context: The text suggests starting with performance tests that check core journeys in the system before moving on to isolated service-level tests. This approach ensures that critical paths are validated first, and then individual components can be tested for performance.
:p How can you perform performance testing at the individual service level?
??x
Performance testing individual services involves isolating specific microservices and measuring their performance under various conditions. For instance, if a payment service is crucial, you might test its response times during high load to ensure it remains reliable.

```java
public class PaymentServicePerformanceTest {
    @Test
    public void testPaymentProcessing() throws Exception {
        // Simulate multiple concurrent payments
        for (int i = 0; i < 100; i++) {
            processPayment();
        }

        // Measure response time and error rates
    }

    private void processPayment() {
        // Logic to simulate a payment transaction
    }
}
```
x??

---",1094,"For some CFRs, you may want to track them at an individual service level. For example, you may decide that the durability of service you require from your payment service is significantly higher, but ...",qwen2.5:latest,2025-11-02 23:54:36,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Performance Tests,Latency Variability and Load Testing,"#### Latency Variability and Load Testing
Background context: Understanding how latency changes as load increases is crucial for performance testing. This helps in identifying bottlenecks that might occur under high traffic conditions.

:p How can you test the impact of increasing load on system latency?
??x
To test the impact of increasing load on system latency, you need to set up a performance test environment that simulates varying levels of user activity. You would typically start with low load and gradually increase it while measuring the response time (latency) at each level.

For example:
```java
public class LoadTesting {
    public void simulateUserActivity(int users) {
        for (int i = 0; i < users; i++) {
            // Simulate user activity like making a call to an API
            // Measure the latency here and log it
        }
    }
}
```
x??",874,"This allows you to see how latency of calls varies with increasing load. This means that performance tests can take a while to run. In addition, you’ll want the system to match production as closely a...",qwen2.5:latest,2025-11-02 23:54:46,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Performance Tests,Production-Like Environment Setup,"#### Production-Like Environment Setup
Background context: Achieving as close a match as possible between your test environment and production is essential for accurate performance predictions. This includes acquiring data volumes similar to those in production and using infrastructure that mimics the production setup.

:p Why is it important to have a production-like environment for testing?
??x
It is crucial because any discrepancies between the test environment and the production environment can lead to false positives or negatives during performance tests. Ensuring that your tests are as close to production conditions as possible helps in getting reliable performance metrics, which can better predict real-world system behavior.

For instance:
```java
public class EnvironmentSetup {
    public void setupProductionLikeEnvironment() {
        // Acquire data volume similar to production
        // Configure database and other systems similarly to production
    }
}
```
x??",988,"This allows you to see how latency of calls varies with increasing load. This means that performance tests can take a while to run. In addition, you’ll want the system to match production as closely a...",qwen2.5:latest,2025-11-02 23:54:46,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Performance Tests,Frequency of Performance Tests,"#### Frequency of Performance Tests
Background context: Regular performance testing is necessary to identify and address performance issues early in the development cycle. However, due to the time required for these tests, it’s not practical to run them after every code commit.

:p How often should performance tests be run?
??x
Performance tests should be run regularly but not necessarily after every code commit. A common practice is to run a subset of tests daily and a larger set weekly. This approach balances the need for continuous monitoring against the time required to conduct thorough testing.

For example:
```java
public class TestFrequency {
    public void defineTestSchedule() {
        // Define a schedule where some tests are run every day, and others on a weekly basis.
    }
}
```
x??",807,"This allows you to see how latency of calls varies with increasing load. This means that performance tests can take a while to run. In addition, you’ll want the system to match production as closely a...",qwen2.5:latest,2025-11-02 23:54:46,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Performance Tests,Importance of Tracking Performance Results,"#### Importance of Tracking Performance Results
Background context: To effectively track down performance issues, it is essential to regularly review the results of your performance tests. Without this review, teams may not identify problems introduced by new code changes.

:p Why is reviewing test results important?
??x
Reviewing test results helps in identifying performance regressions or improvements early in the development process. By keeping a record of these results, developers can quickly pinpoint which recent changes have caused performance issues, thereby saving time and effort during debugging.

For example:
```java
public class ResultReview {
    public void reviewPerformanceResults() {
        // Log test results for every run
        // Compare with historical data to detect trends or anomalies
    }
}
```
x??",835,"This allows you to see how latency of calls varies with increasing load. This means that performance tests can take a while to run. In addition, you’ll want the system to match production as closely a...",qwen2.5:latest,2025-11-02 23:54:46,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Performance Tests,Visualizing System Behavior,"#### Visualizing System Behavior
Background context: Performance tests should use the same visualization tools as those in production. This ensures that any discrepancies between expected and actual system behavior can be easily identified.

:p How does using the same tools for performance testing help?
??x
Using the same tools for performance testing as in production helps in maintaining consistency across environments, making it easier to compare real-time system behavior with test results. This approach allows teams to spot issues more effectively by leveraging familiar visualization methods and patterns.

For example:
```java
public class VisualizationTools {
    public void useProductionToolsForTesting() {
        // Use the same monitoring tools for both testing and production environments.
        // Ensure that any visualizations match those used in production.
    }
}
```
x??

---",902,"This allows you to see how latency of calls varies with increasing load. This means that performance tests can take a while to run. In addition, you’ll want the system to match production as closely a...",qwen2.5:latest,2025-11-02 23:54:46,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Correlation IDs,Holistic Approach to Testing,"#### Holistic Approach to Testing
Background context: The text outlines a testing approach focused on optimizing for fast feedback and separating tests based on their types. It also mentions using consumer-driven contracts and understanding the trade-offs between MTBF (Mean Time Between Failures) and MTTR (Mean Time To Repair).

:p What is the main objective of the holistic approach to testing described in this text?
??x
The primary goal is to create a system that provides fast feedback on code quality before it reaches production. This involves optimizing test execution times, separating different types of tests, and using consumer-driven contracts to streamline communication between development teams.

To optimize for fast feedback, consider implementing continuous integration (CI) pipelines where builds and tests run automatically after every commit. This ensures issues are caught early rather than at the end of a development cycle.
```java
public class ExampleCiPipeline {
    @Test
    public void checkBuild() throws Exception {
        // Code to build and test code changes
    }
}
```
x??",1111,"Summary Bringing this all together, what I have outlined here is a holistic approach to testing that hopefully gives you some general guidance on how to proceed when testing your own systems. To reite...",qwen2.5:latest,2025-11-02 23:54:58,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Correlation IDs,Consumer-Driven Contracts,"#### Consumer-Driven Contracts
Background context: The text suggests using consumer-driven contracts as an alternative to end-to-end tests. This approach focuses on defining the expected behavior of services from their consumers' perspective.

:p How can consumer-driven contracts help in testing microservices?
??x
Consumer-driven contracts (CDCs) help by specifying how a service should behave based on what its dependent services expect it to do. By generating test cases that match the contract, developers ensure that changes made to one service don't break the expectations of other services.

For example, if Service A expects a response from Service B with specific data fields, you can create a mock client for Service B that sends this expected data. Any change in Service B's behavior will trigger a failure in Service A’s tests.
```java
public class ConsumerDrivenContractTest {
    @Test
    public void testServiceBResponse() throws Exception {
        // Mock client setup to send the expected response from Service B
        mockClient.sendExpectedResponse();
        
        // Call Service A and verify its behavior matches expectations
        ServiceA serviceA = new ServiceA();
        serviceA.getResponseFromB(); // This should pass if Service B behaves as expected
    }
}
```
x??",1305,"Summary Bringing this all together, what I have outlined here is a holistic approach to testing that hopefully gives you some general guidance on how to proceed when testing your own systems. To reite...",qwen2.5:latest,2025-11-02 23:54:58,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Correlation IDs,Trade-offs Between MTBF and MTTR,"#### Trade-offs Between MTBF and MTTR
Background context: The text mentions understanding the trade-offs between optimizing for Mean Time Between Failures (MTBF) and Mean Time To Repair (MTTR). This is crucial in determining how much effort to put into testing versus handling failures.

:p Explain the concept of trade-off between MTBF and MTTR.
??x
The trade-off between MTBF and MTTR involves balancing how often issues occur with the speed at which they can be resolved. Optimizing for high MTBF means minimizing the frequency of issues, while optimizing for low MTTR focuses on reducing the time it takes to fix problems once they arise.

For example, you might choose to spend more time writing comprehensive unit tests and integration tests (MTBF) but accept that when an issue does occur, it will take longer to resolve (high MTTR). Alternatively, you could focus on quick fixes and monitoring tools (low MTTR) even if it means issues happen more frequently (lower MTBF).

To illustrate this in practice:
```java
public class TestOptimization {
    @Test
    public void testHighMTBF() throws Exception {
        // Comprehensive tests to catch issues early
        runAllTests();
    }
    
    @Test(timeout = 1000)
    public void testLowMTTR() throws Exception {
        // Quick smoke tests to detect major issues fast
        checkCriticalPaths();
    }
}
```
x??",1377,"Summary Bringing this all together, what I have outlined here is a holistic approach to testing that hopefully gives you some general guidance on how to proceed when testing your own systems. To reite...",qwen2.5:latest,2025-11-02 23:54:58,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Correlation IDs,Monitoring in Microservices,"#### Monitoring in Microservices
Background context: The text discusses the challenges of monitoring microservice-based systems, where traditional monolithic system monitoring is simpler due to a single point of failure.

:p What are some key differences between monitoring monolithic applications and microservices?
??x
Key differences include:
- **Complexity**: Monolithic applications have a single entry point for issues, whereas microservices introduce multiple services that need individual monitoring.
- **Latency Issues**: Microservices can experience network latency issues, making it harder to pinpoint where problems lie compared to monolithic applications.

To monitor effectively in such systems, you might use distributed tracing tools like Jaeger or Zipkin to understand the flow of requests across different microservices. Additionally, setting up centralized logging and alerting mechanisms can help identify failures quickly.
```java
public class MonitoringMicroservices {
    public void setupTracing() throws Exception {
        // Setup distributed tracing with Jaeger
        Tracer tracer = new Tracer();
        tracer.traceRequest(""service-a"", ""service-b"");
        
        // Log important metrics and logs centrally
        Logger.logMetrics();
    }
}
```
x??

---",1293,"Summary Bringing this all together, what I have outlined here is a holistic approach to testing that hopefully gives you some general guidance on how to proceed when testing your own systems. To reite...",qwen2.5:latest,2025-11-02 23:54:58,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Correlation IDs,Monitoring a Single Node Service,"#### Monitoring a Single Node Service
Background context: In a basic setup, one host runs one service. The objective is to monitor this system to ensure it operates correctly and can be fixed if issues arise.

:p What should you monitor on a single node?
??x
You should monitor the host's CPU usage, memory usage, disk space, and logs. These metrics help identify when something goes wrong so that it can be addressed promptly.
??x

#### Example of Monitoring Metrics
Background context: Monitoring critical system metrics such as CPU, memory, and disk usage is essential to maintaining a healthy service.

:p What tools can you use to monitor these metrics?
??x
You can use tools like Nagios or New Relic. Nagios provides a way to set up alerts based on thresholds for various metrics, while New Relic offers a comprehensive suite of monitoring services.
??x

#### Logging and Error Handling
Background context: Logs are crucial for diagnosing issues when they occur. Ensuring logs are properly managed is important.

:p How can you manage logs effectively?
??x
You can use `logrotate` to manage old log files, preventing them from taking up too much disk space. This ensures that the system remains efficient and responsive.
??x

#### Response Time Monitoring
Background context: Monitoring response time helps ensure that your service performs well under various loads.

:p How do you monitor the response time of a single service?
??x
You can monitor response times by checking logs from a web server in front of your service or directly from the service itself. This helps in identifying performance bottlenecks.
??x

#### Scaling to Multiple Nodes
Background context: As loads increase, scaling to multiple nodes becomes necessary. Monitoring across multiple hosts introduces complexity.

:p How does monitoring change when you have multiple nodes?
??x
With multiple nodes, you need to monitor both individual hosts and aggregated metrics across all nodes. This helps in isolating issues that may be host-specific or related to the service itself.
??x

#### Log Management for Multiple Nodes
Background context: With a load balancer distributing requests across multiple hosts, managing logs becomes more complex.

:p How can you manage logs when running services on multiple nodes?
??x
Use tools like `ssh-multiplexers` to run commands on multiple hosts simultaneously. This helps in efficiently searching through logs across all nodes.
??x",2447,"The answer here is pretty straightforward: monitor the small things, and use aggregation to see the bigger picture. To see how, we’ll start with the simplest system we can: a single node. Single Servi...",qwen2.5:latest,2025-11-02 23:55:05,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Correlation IDs,Load Balancer Monitoring and Service Health,"#### Load Balancer Monitoring and Service Health
Background context explaining how load balancers are crucial for distributing traffic among servers, but they can also become a point of failure. Proper monitoring ensures that the load balancer itself is functioning correctly and that unhealthy nodes are removed from service to maintain overall application health.
:p How do we monitor the load balancer's performance and ensure it doesn't misbehave?
??x
We track the load balancer's own metrics, such as response times and error rates, to ensure it functions properly. When a node becomes unhealthy, it is typically configured to be removed from the pool of available nodes by the load balancer itself.
```java
// Pseudocode for removing an unhealthy node
public void removeUnhealthyNode(String nodeId) {
    // Code to update load balancer configuration to exclude this node
}
```
x??",887,"For tasks like response time tracking, we can get some of the aggregation for free by tracking at the load balancer itself. But we need to track the load balancer as well, of course; if that misbehave...",qwen2.5:latest,2025-11-02 23:55:15,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Correlation IDs,Log Collection and Central Aggregation,"#### Log Collection and Central Aggregation
Background context explaining that with multiple services running on various hosts, managing logs becomes challenging. Tools like logstash are used for centralized collection of logs from different sources, making it easier to analyze issues across all nodes.
:p How do we handle log collection in a distributed environment?
??x
Logstash is used to collect logs from multiple sources and centralize them. This allows us to aggregate logs and perform comprehensive analysis without the need to access each host individually.
```java
// Pseudocode for using Logstash to collect logs
public class LogCollector {
    public void startCollection() {
        // Code to configure Logstash with input plugins for different log sources
    }
}
```
x??",787,"For tasks like response time tracking, we can get some of the aggregation for free by tracking at the load balancer itself. But we need to track the load balancer as well, of course; if that misbehave...",qwen2.5:latest,2025-11-02 23:55:15,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Correlation IDs,Metric Tracking Across Multiple Services,"#### Metric Tracking Across Multiple Services
Background context explaining the need for monitoring metrics in a complex system, where individual service metrics must be aggregated and analyzed over time to identify trends and anomalies.
:p How do we track metrics across multiple services?
??x
Metrics are collected from each instance of a service and then aggregated at higher levels (system-wide, per service, or even per instance) using tools that support metadata association for better insight. This helps in identifying systemic issues versus isolated incidents.
```java
// Pseudocode for aggregating metrics
public class MetricsAggregator {
    public void aggregateMetrics(String serviceName) {
        // Code to collect and aggregate metrics from different hosts running the same service
    }
}
```
x??",814,"For tasks like response time tracking, we can get some of the aggregation for free by tracking at the load balancer itself. But we need to track the load balancer as well, of course; if that misbehave...",qwen2.5:latest,2025-11-02 23:55:15,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Correlation IDs,Kibana for Log Analysis,"#### Kibana for Log Analysis
Background context explaining that Kibana, backed by ElasticSearch, provides a user-friendly interface for searching, analyzing, and visualizing logs. It supports complex queries and can generate graphs to help identify patterns over time.
:p How does Kibana aid in log analysis?
??x
Kibana allows users to query logs effectively using advanced search capabilities, including date ranges and regular expressions. Additionally, it generates visualizations such as error counts over time, making it easier to diagnose issues.
```java
// Pseudocode for querying logs with Kibana
public class LogAnalyzer {
    public void analyzeLogs(String query) {
        // Code to send a complex search query to ElasticSearch via Kibana API and retrieve results
    }
}
```
x??",791,"For tasks like response time tracking, we can get some of the aggregation for free by tracking at the load balancer itself. But we need to track the load balancer as well, of course; if that misbehave...",qwen2.5:latest,2025-11-02 23:55:15,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Correlation IDs,Understanding System Behavior Through Metrics,"#### Understanding System Behavior Through Metrics
Background context explaining the importance of long-term metric collection to identify patterns that indicate normal versus abnormal system behavior. Frequent provisioning of new instances necessitates easy setup for metric collection.
:p How do we ensure metrics are collected from new hosts?
??x
Metrics systems should be designed to automatically collect data from newly provisioned instances with minimal configuration changes. This could involve setting up agents on each host that periodically send metric data to a central aggregator.
```java
// Pseudocode for configuring metric collection
public class MetricCollector {
    public void configureNewHost(String hostname) {
        // Code to set up the necessary metric collection setup on new hosts
    }
}
```
x??

---",830,"For tasks like response time tracking, we can get some of the aggregation for free by tracking at the load balancer itself. But we need to track the load balancer as well, of course; if that misbehave...",qwen2.5:latest,2025-11-02 23:55:15,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Correlation IDs,Graphite Overview,"#### Graphite Overview
Graphite simplifies the process of sending and querying metrics, allowing real-time data collection. It handles large volumes by reducing the resolution of older metrics over time to ensure storage efficiency.
:p What is the primary function of Graphite?
??x
Graphite primarily functions as a system for collecting and storing metrics in real-time while efficiently managing storage through automated data aggregation and downsampling.
x??",462,Graphite is one such system that makes this very easy. It exposes a very simple API and allows you to send metrics in real time. It then allows you to query those metrics to produce charts and other d...,qwen2.5:latest,2025-11-02 23:55:23,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Correlation IDs,Volume Handling Mechanism,"#### Volume Handling Mechanism
To manage large volumes of data, Graphite configures different sampling intervals based on the age of the data. For instance, it records more frequent samples recently and less frequent ones over time.
:p How does Graphite handle data volume?
??x
Graphite handles data volume by configuring varying sampling rates. It records metrics frequently (e.g., every 10 seconds) for recent data and aggregates them into coarser intervals (e.g., once per minute, then hourly, daily, etc.) as the data ages.
x??",531,Graphite is one such system that makes this very easy. It exposes a very simple API and allows you to send metrics in real time. It then allows you to query those metrics to produce charts and other d...,qwen2.5:latest,2025-11-02 23:55:23,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Correlation IDs,Aggregation in Graphite,"#### Aggregation in Graphite
Graphite allows aggregation across different samples to provide a broader view of system behavior. This can be used to see overall trends or drill down into specific details.
:p How does Graphite enable aggregation?
??x
Graphite enables aggregation through its powerful query language and functions that allow you to combine data from multiple time series. For example, `aggregatedData = sumSeries(data) / countSeries(data)` would calculate the average of a metric over time.
x??",508,Graphite is one such system that makes this very easy. It exposes a very simple API and allows you to send metrics in real time. It then allows you to query those metrics to produce charts and other d...,qwen2.5:latest,2025-11-02 23:55:23,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Correlation IDs,Capacity Planning with Metrics,"#### Capacity Planning with Metrics
Understanding metrics helps in making informed decisions for capacity planning. By tracking usage patterns, one can predict when additional resources might be needed to avoid bottlenecks.
:p How does understanding trends aid in capacity planning?
??x
Understanding trends aids in capacity planning by providing insights into resource utilization over time. This allows for proactive scaling of infrastructure based on actual need rather than fixed schedules, making the system more cost-effective and responsive.
x??",552,Graphite is one such system that makes this very easy. It exposes a very simple API and allows you to send metrics in real time. It then allows you to query those metrics to produce charts and other d...,qwen2.5:latest,2025-11-02 23:55:23,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Correlation IDs,Service Metrics Collection,"#### Service Metrics Collection
Service metrics such as response times, error rates, and custom business-critical metrics should be collected to monitor application performance effectively.
:p What types of service metrics are recommended to collect?
??x
Recommended service metrics include response times, error rates, and custom business-critical metrics like the number of customer views or revenue generated in a specific period. These metrics help in assessing both technical and business performance.
x??",510,Graphite is one such system that makes this very easy. It exposes a very simple API and allows you to send metrics in real time. It then allows you to query those metrics to produce charts and other d...,qwen2.5:latest,2025-11-02 23:55:23,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Correlation IDs,Example Metrics for Web Services,"#### Example Metrics for Web Services
For web services, essential metrics such as response times and error rates should be exposed to monitor server health and user experience. Custom metrics can provide deeper insights into specific business operations.
:p What are some basic metrics for a web service?
??x
Basic metrics for a web service include response times and error rates. These help in monitoring the server's performance and ensuring good user experience. Additional custom metrics, such as customer views or revenue generated, offer deeper insights into business operations.
x??

---",594,Graphite is one such system that makes this very easy. It exposes a very simple API and allows you to send metrics in real time. It then allows you to query those metrics to produce charts and other d...,qwen2.5:latest,2025-11-02 23:55:23,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Correlation IDs,Reacting to User Behavior,"#### Reacting to User Behavior
Background context: The passage discusses how metrics can help improve a system based on user behavior. It mentions an example where a significant increase in searches by genre on the catalog service was observed after a new version of the website was pushed out.

:p Is an increase in searches by genre following a website update expected or problematic?
??x
An increase in searches by genre could be seen as expected if the update made it easier to browse by genre, possibly through better navigation or categorization. However, without context on user feedback and metrics like engagement time, it's hard to determine if this is normal behavior or indicative of a problem.

```java
public class MetricsAnalyzer {
    public void analyzeUserBehaviorMetrics() {
        // Logic to check for significant changes in genre searches
        if (searchesByGenreIncreasedSignificantly()) {
            System.out.println(""Increase in searches by genre detected."");
            // Further investigation needed
        }
    }

    private boolean searchesByGenreIncreasedSignificantly() {
        // Pseudo-code to compare current and previous metrics
        return getCurrentMetricValue(""genre_searches"") - getLastMonthMetricValue(""genre_searches"") > THRESHOLD;
    }
}
```
x??",1305,"Second, we are getting better than ever at reacting to how our users are using our system to work out how to improve it. Metrics that inform us of how our systems behave can only help us here. We push...",qwen2.5:latest,2025-11-02 23:55:36,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Correlation IDs,Metrics Library for JVM,"#### Metrics Library for JVM
Background context: The passage introduces Codahale's Metrics library, which is designed for the Java Virtual Machine (JVM). This library allows services to send metrics data, such as counters, timers, and gauges, and supports sending this data to aggregation systems like Graphite.

:p How does Codahale’s Metrics library support the collection and reporting of system behavior?
??x
Codahale's Metrics library supports collecting various types of metrics (counters, timers, and gauges) and allows time-boxing metrics. It can send this data to aggregating systems such as Graphite. Here is an example setup:

```java
import com.codahale.metrics.MetricRegistry;
import com.codahale.metrics.Timer;
import com.yammer.metrics.graphite.Graphite;

public class MetricsSetup {
    public void configureMetrics() throws Exception {
        // Create a metric registry
        MetricRegistry registry = new MetricRegistry();
        
        // Create a timer for tracking response times
        Timer timer = registry.timer(""responseTime"");
        
        // Send metrics to Graphite
        Graphite graphite = new Graphite(new InetSocketAddress(""localhost"", 2003));
        registry.register(""metricsName"", new Counter()); // Example counter
        
        // Pseudo-code to report data periodically
        Timer.Context context = timer.time();
        try {
            // Some operation that takes time
        } finally {
            context.stop();
            graphite.send(registry, System.currentTimeMillis());
        }
    }
}
```
x??",1571,"Second, we are getting better than ever at reacting to how our users are using our system to work out how to improve it. Metrics that inform us of how our systems behave can only help us here. We push...",qwen2.5:latest,2025-11-02 23:55:36,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Correlation IDs,Synthetic Monitoring,"#### Synthetic Monitoring
Background context: The passage explains synthetic monitoring as a method to check if services are healthy by simulating user interactions. This approach helps detect anomalies in real-time and triggers alerts when values deviate from expected levels.

:p How does synthetic monitoring work, and what tools can be used for this purpose?
??x
Synthetic monitoring works by programmatically generating fake events or requests that simulate typical user behavior to test the system's health. Tools like Nagios can be used to trigger these checks and send alerts if something goes wrong. For example:

```java
import org.nachos.util.Nagios;

public class SyntheticMonitoring {
    public void checkSystemHealth() {
        // Pseudo-code for a Nagios check
        Nagios nagios = new Nagios();
        
        // Define the command to run every minute or so
        String command = ""insert-fake-event"";
        String[] args = {command};
        
        int exitCode = nagios.run(args);
        if (exitCode != 0) {
            System.out.println(""System health check failed."");
            // Trigger alerts
        }
    }
}
```
x??",1159,"Second, we are getting better than ever at reacting to how our users are using our system to work out how to improve it. Metrics that inform us of how our systems behave can only help us here. We push...",qwen2.5:latest,2025-11-02 23:55:36,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Correlation IDs,Generating Fake Events for Testing,"#### Generating Fake Events for Testing
Background context: The passage describes an approach to test system performance by generating fake events. This was done in a scenario where the team needed to ensure calculations were completed within 10 seconds of receiving market event data.

:p How did the team generate and use fake events during development?
??x
The team generated fake events using Nagios, which would periodically insert them into the queue to simulate user interactions. Here’s an example setup:

```java
public class FakeEventGenerator {
    public void runFakeEvents() {
        // Pseudo-code for generating a fake event every minute
        while (true) {
            try {
                Thread.sleep(60000); // Wait 1 minute
                // Insert a fake event into the queue
                insertFakeMarketEvent();
            } catch (InterruptedException e) {
                Thread.currentThread().interrupt();
                System.out.println(""Interrupted while waiting for next fake event."");
            }
        }
    }

    private void insertFakeMarketEvent() {
        // Logic to create and send a synthetic market event
        // This could involve updating the database or queue with dummy data
        System.out.println(""Inserted fake market event into the system."");
    }
}
```
x??

---",1336,"Second, we are getting better than ever at reacting to how our users are using our system to work out how to improve it. Metrics that inform us of how our systems behave can only help us here. We push...",qwen2.5:latest,2025-11-02 23:55:36,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Correlation IDs,Synthetic Transactions and Semantic Monitoring,"#### Synthetic Transactions and Semantic Monitoring
Background context explaining how synthetic transactions are used to simulate real user interactions for testing purposes. This technique ensures that the system behaves as expected under various conditions, providing a higher-level view of its performance compared to monitoring lower-level metrics.

:p What is a synthetic transaction?
??x
A synthetic transaction is a simulated event or action created within a test environment to mimic real-world user interactions. It helps in ensuring that the system functions correctly and behaves semantically as expected, by running end-to-end tests without relying solely on lower-level metrics.
x??",695,"Our system picked it up and ran all the various calculations just like any other job, except the results appeared in the  junk book, which was used only for testing. If a re-pricing wasn’t seen within...",qwen2.5:latest,2025-11-02 23:55:44,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Correlation IDs,Implementing Semantic Monitoring,"#### Implementing Semantic Monitoring
Background context explaining how semantic monitoring can be implemented using existing tests designed for end-to-end service or system testing. The idea is to run a subset of these tests continuously to monitor the system's behavior.

:p How does implementing semantic monitoring work?
??x
Semantic monitoring involves running a subset of end-to-end tests continuously to ensure that the system behaves as expected. These tests are already present in the test suite and can be used directly or adapted for ongoing monitoring. The goal is to catch issues at a higher level, which provides better context than lower-level metrics.

For example:
```java
public class EndToEndTest {
    @Test
    public void customerRegistrationTest() {
        // Simulate user registration with valid and invalid data points.
        // Check if the system responds correctly in both scenarios.
        // Log any issues that arise during the test.
    }
}
```
x??",985,"Our system picked it up and ran all the various calculations just like any other job, except the results appeared in the  junk book, which was used only for testing. If a re-pricing wasn’t seen within...",qwen2.5:latest,2025-11-02 23:55:44,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Correlation IDs,Data Requirements for Tests,"#### Data Requirements for Tests
Background context explaining the importance of adapting tests to different live datasets over time. This ensures that the synthetic transactions remain relevant and useful.

:p How should tests adapt to changing data?
??x
Tests need to adapt to changes in the live dataset over time to ensure they remain relevant. One approach is to use a set of fake users with known data for testing, which can be updated as needed. This helps maintain the consistency and reliability of the synthetic transactions.

For example:
```java
public class DataAdapter {
    private Map<String, String> testData;

    public DataAdapter() {
        // Initialize testData with predefined sets of user data.
    }

    public Object[] provideUserDetails(String userId) {
        return testData.get(userId).split("","");
    }
}
```
x??",847,"Our system picked it up and ran all the various calculations just like any other job, except the results appeared in the  junk book, which was used only for testing. If a re-pricing wasn’t seen within...",qwen2.5:latest,2025-11-02 23:55:44,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Correlation IDs,Correlation IDs for Tracing,"#### Correlation IDs for Tracing
Background context explaining the challenge of diagnosing issues in complex systems with multiple service calls. The need to trace error contexts and reconstruct call chains is highlighted.

:p How do correlation IDs help in tracing?
??x
Correlation IDs are used to trace error contexts by enabling the reconstruction of call chains, even when an error occurs in a downstream service. By assigning unique IDs to each initiating request and propagating them through subsequent calls, it becomes easier to track the flow of requests and identify the root cause of issues.

For example:
```java
public class Request {
    private String correlationId;

    public Request(String correlationId) {
        this.correlationId = correlationId;
    }

    // Method to log or propagate the correlation ID
    public void logRequest() {
        System.out.println(""Correlation ID: "" + correlationId);
    }
}
```
x??

---",945,"Our system picked it up and ran all the various calculations just like any other job, except the results appeared in the  junk book, which was used only for testing. If a re-pricing wasn’t seen within...",qwen2.5:latest,2025-11-02 23:55:44,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Correlation IDs,Correlation IDs Overview,"#### Correlation IDs Overview
Correlation IDs are a method to track event chains across multiple services. They are particularly useful in complex, distributed systems where events can trigger cascades of calls through various components.

:p What is a correlation ID and why is it important?
??x
A correlation ID is a unique identifier generated for each call made within a system. It helps in tracing the flow of requests and responses across different services, which is crucial for debugging and understanding event storms or anomalies. The importance lies in its ability to link related events together, making it easier to diagnose issues that might span multiple service boundaries.

For example:
```java
public class CorrelationIDGenerator {
    private static final ThreadLocal<String> threadLocal = new ThreadLocal<>();
    
    public static String generate() {
        return UUID.randomUUID().toString();
    }
    
    public static void set(String id) {
        threadLocal.set(id);
    }
    
    public static String get() {
        return threadLocal.get();
    }
}
```
In this code, `generate` creates a unique ID for each call. The `set` and `get` methods manage the storage and retrieval of these IDs within threads.

x??",1242,"One approach that can be useful here is to use correlation IDs. When the first call is made, you generate a GUID for the call. This is then passed along to all subsequent calls, as seen in  Figure 8-5...",qwen2.5:latest,2025-11-02 23:55:57,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Correlation IDs,Enforcing Correlation IDs Across Services,"#### Enforcing Correlation IDs Across Services
Ensuring that each service in your system knows how to pass on the correlation ID is crucial for effective traceability. This involves standardizing the handling and propagation of the ID across all services.

:p How can you enforce the use of correlation IDs consistently across multiple services?
??x
To enforce the consistent use of correlation IDs, you need a standardized approach where each service knows how to pass on the ID to downstream services. This can be achieved by:

1. **Adding Headers**: When making API calls between services, include the correlation ID in headers.
2. **Environment Variables or Configuration Files**: Use environment variables or configuration files to set default values for IDs and ensure all services read these settings.

For example:
```java
public class ServiceClient {
    private String correlationId;

    public void makeRequest() {
        // Assuming you have a way of getting the current correlation ID, e.g., from headers.
        this.correlationId = getCorrelationIDFromHeaders();
        
        // Make API call and pass along the correlation ID in the header.
        HttpClient client = HttpClientBuilder.create().build();
        HttpPost request = new HttpPost(""http://example.com/api"");
        request.setHeader(""X-Correlation-ID"", this.correlationId);
        // ...
    }
}
```
This code snippet shows how to include the correlation ID in HTTP headers when making API calls.

x??",1490,"One approach that can be useful here is to use correlation IDs. When the first call is made, you generate a GUID for the call. This is then passed along to all subsequent calls, as seen in  Figure 8-5...",qwen2.5:latest,2025-11-02 23:55:57,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Correlation IDs,Zipkin for Tracing Service Calls,"#### Zipkin for Tracing Service Calls
Zipkin is a tool designed to trace service-to-service calls, providing detailed insights into interservice interactions. It can be useful but might require significant setup and custom client implementations.

:p What is Zipkin and how does it help in tracing interservice calls?
??x
Zipkin is an open-source distributed tracing system that helps visualize the request flow through a distributed system. By collecting traces from various services, Zipkin provides detailed insights into service interactions, making it easier to debug issues and optimize performance.

To use Zipkin:
1. **Collect Traces**: Each service sends its trace data to a central collector.
2. **Visualize Data**: Zipkin provides an interface to visualize these traces, showing the flow of requests across services.

Example setup in Java:
```java
// Pseudocode for configuring Zipkin client in a service
ZipkinClient zipkinClient = new HttpJsonSender(new URL(""http://localhost:9411/api/v2/spans""));
Tracer tracer = new Tracing.Builder()
    .zipkin(zipkinClient)
    .build();
```
This setup configures the `zipkinClient` to send trace data and uses it in a tracing builder.

x??",1192,"One approach that can be useful here is to use correlation IDs. When the first call is made, you generate a GUID for the call. This is then passed along to all subsequent calls, as seen in  Figure 8-5...",qwen2.5:latest,2025-11-02 23:55:57,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Correlation IDs,Thin Shared Client Wrapper Libraries,"#### Thin Shared Client Wrapper Libraries
To handle tasks like consistently passing through correlation IDs, using thin shared client wrapper libraries can be beneficial. These libraries help ensure that each service is calling downstream services correctly without adding unnecessary complexity.

:p Why might you consider creating an in-house client library for handling correlation IDs?
??x
Creating an in-house client library to manage common tasks such as passing correlation IDs can simplify the integration process across multiple services. This approach ensures consistency and reduces errors related to forgetting or incorrectly handling these IDs.

Key benefits include:
1. **Consistency**: Ensures that all services use a standardized method for handling correlation IDs.
2. **Reduced Duplication of Code**: Minimizes code duplication, making maintenance easier.
3. **Ease of Debugging**: Makes it simpler to trace issues by ensuring consistent data flow and logging.

Example implementation in Java:
```java
public class ServiceClient {
    private final Tracing tracing;
    
    public ServiceClient(Tracing tracing) {
        this.tracing = tracing;
    }
    
    public void makeRequest() {
        String correlationId = tracing.getCorrelationID();
        
        // Make API call and pass along the correlation ID in the header.
        HttpClient client = HttpClientBuilder.create().build();
        HttpPost request = new HttpPost(""http://example.com/api"");
        request.setHeader(""X-Correlation-ID"", correlationId);
        // ...
    }
}
```
This code demonstrates how a wrapper library can abstract away common tasks, ensuring that each service handles correlation IDs consistently.

x??

---",1721,"One approach that can be useful here is to use correlation IDs. When the first call is made, you generate a GUID for the call. This is then passed along to all subsequent calls, as seen in  Figure 8-5...",qwen2.5:latest,2025-11-02 23:55:57,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Summary,Importance of Monitoring Integration Points,"#### Importance of Monitoring Integration Points

Background context: In a distributed system, it's crucial to monitor not just individual services but also their interactions. Network failures between services can lead to cascading issues that aren't immediately apparent by looking at service health alone.

:p Why is monitoring integration points between systems important?
??x
Monitoring integration points is vital because network failures or degradation may prevent services from communicating with each other, even if the services themselves appear healthy. Traditional service-level health checks might not catch these issues. Synthetic monitoring can help detect such problems by simulating real user interactions.

Example: If a music shop website cannot fetch song catalogs due to an issue between the website and catalog service, standard health checks on both services would likely pass but the functionality would be broken.
??x",942,The Cascade Cascading failures can be especially perilous. Imagine a situation where the network connection between our music shop website and the catalog service goes down. The services themselves ap...,qwen2.5:latest,2025-11-02 23:56:09,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Summary,Tracking Downstream Service Health,"#### Tracking Downstream Service Health

Background context: Each service instance should monitor its downstream dependencies, such as databases or other collaborating services. This includes tracking response times and error rates of these dependencies.

:p How can you ensure that a service tracks its downstream dependencies effectively?
??x
Services should log and expose the health status of their downstream dependencies. This involves measuring response time and detecting errors in downstream calls. Libraries like Hystrix can help implement circuit breakers to handle cascading failures more gracefully.

Example: A music shop's website might use Hystrix to wrap network calls to a catalog service, which helps manage failures and degrade the system gracefully if an issue arises.
```java
import com.netflix.hystrix.HystrixCommand;

public class CatalogServiceCommand extends HystrixCommand<String> {
    private final String catalogServiceUrl;
    
    public CatalogServiceCommand(String catalogServiceUrl) {
        super(Setter.withGroupKey(HystrixCommandGroupKey.Factory.asKey(""CatalogGroup""))
                    .andCommandKey(HystrixCommandKey.Factory.asKey(""CatalogServiceCall"")));
        this.catalogServiceUrl = catalogServiceUrl;
    }
    
    @Override
    protected String run() throws Exception {
        // Make the network call to fetch song catalogs
        return new URL(catalogServiceUrl).openStream().readAllBytes();
    }
}
```
x??

#### Standardizing Metrics and Logging

Background context: To ensure consistency across services, it's important to standardize metrics and logging formats. This helps in aggregating data effectively without losing valuable information.

:p Why is standardization of metrics and logs crucial?
??x
Standardization ensures that metrics are consistently named and formatted, making it easier to aggregate and compare data across different services. It also aids in providing a holistic view of the system's performance and health.

Example: Instead of using `ResponseTime` for one service and `RspTimeSecs` for another when they mean the same thing (e.g., response time in seconds), standardizing on `response_time_seconds` would make it easier to aggregate metrics.
??x",2235,The Cascade Cascading failures can be especially perilous. Imagine a situation where the network connection between our music shop website and the catalog service goes down. The services themselves ap...,qwen2.5:latest,2025-11-02 23:56:09,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Summary,Tailoring Data for Different Audiences,"#### Tailoring Data for Different Audiences

Background context: The data collected from monitoring should be tailored to different audiences based on their needs. Immediate alerts are necessary for support teams, while detailed analysis might only be needed by higher-level management.

:p How should you structure alerting and dashboards for different users?
??x
Alerts and dashboards should be customized for the specific needs of each user group. For example, immediate alerts can be sent to the support team if a critical test fails, whereas longer-term trends like CPU usage might only be monitored by the operations team.

Example: A support dashboard could show real-time error rates and response times in large, visible displays, while an operational dashboard might track CPU usage over time.
??x",806,The Cascade Cascading failures can be especially perilous. Imagine a situation where the network connection between our music shop website and the catalog service goes down. The services themselves ap...,qwen2.5:latest,2025-11-02 23:56:09,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Summary,Unifying Metrics and Event Processing,"#### Unifying Metrics and Event Processing

Background context: Traditionally, different tools are used for business-level metrics (e.g., revenue) and system-level metrics (e.g., response times). However, modern systems need unified event processing to handle all types of data efficiently.

:p Why is unifying metrics and event processing important?
??x
Unifying metrics and event processing allows for a more holistic view of the system. This approach can simplify architecture by using generic event routing systems that handle both business and operational metrics effectively.

Example: Tools like Riemann or Suro can be used to unify metric collection, aggregation, and storage, making it easier to analyze and respond to events across the entire system.
??x
---",768,The Cascade Cascading failures can be especially perilous. Imagine a situation where the network connection between our music shop website and the catalog service goes down. The services themselves ap...,qwen2.5:latest,2025-11-02 23:56:09,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Single Sign-On Gateway,Authentication and Authorization Overview,"#### Authentication and Authorization Overview
Authentication is the process of verifying a user's identity, while authorization defines what actions that user can perform. In security terms, these are core concepts for ensuring data integrity and access control.

:p What are authentication and authorization?
??x
Authentication is about confirming who a person is, typically through credentials like usernames and passwords or more advanced methods such as fingerprints. Authorization then determines the permissions granted to that identified individual based on their role within the system.
???x",600,"Chapter 9.  Security We’ve become familiar with stories about security breaches of large-scale systems resulting in our data being exposed to all sorts of dodgy characters. But more recently, events l...",qwen2.5:latest,2025-11-02 23:56:16,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Single Sign-On Gateway,Single Sign-On (SSO) Implementations,"#### Single Sign-On (SSO) Implementations
Common SSO solutions include SAML and OpenID Connect, which are widely used in enterprise environments.

:p What are some common single sign-on implementations?
??x
SAML is a popular choice for enterprise SSO, where an identity provider handles authentication and provides attributes to the service provider. OpenID Connect is another implementation based on OAuth 2.0, providing simpler REST-based interactions.
???x",459,"Chapter 9.  Security We’ve become familiar with stories about security breaches of large-scale systems resulting in our data being exposed to all sorts of dodgy characters. But more recently, events l...",qwen2.5:latest,2025-11-02 23:56:16,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Single Sign-On Gateway,Single Sign-On Gateway in Microservices,"#### Single Sign-On Gateway in Microservices
A gateway can handle SSO by acting as a proxy between services and the external world, centralizing the authentication process.

:p How does a single sign-on gateway work within microservices?
??x
A gateway can manage redirection to identity providers and perform the handshake for all services. It populates headers with information about principals (e.g., usernames or roles) to be used by downstream services.
???x",462,"Chapter 9.  Security We’ve become familiar with stories about security breaches of large-scale systems resulting in our data being exposed to all sorts of dodgy characters. But more recently, events l...",qwen2.5:latest,2025-11-02 23:56:16,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Single Sign-On Gateway,Shibboleth as an Example of SSO Gateway,"#### Shibboleth as an Example of SSO Gateway
Shibboleth is a tool that can be integrated into Apache to handle SAML-based identity providers, populating HTTP headers with principal information.

:p What is Shibboleth and how does it work?
??x
Shibboleth is a tool used to integrate with SAML-based identity providers. It can be deployed with Apache to populate HTTP headers with information about the authenticated user, such as their username or roles.
???x",458,"Chapter 9.  Security We’ve become familiar with stories about security breaches of large-scale systems resulting in our data being exposed to all sorts of dodgy characters. But more recently, events l...",qwen2.5:latest,2025-11-02 23:56:16,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Single Sign-On Gateway,Defense in Depth Concept,"#### Defense in Depth Concept
Implementing security measures at multiple layers (network perimeter, subnets, firewalls, operating systems) is a strategy known as defense in depth.

:p What does the concept of defense in depth entail?
??x
Defense in depth involves layering security mechanisms to protect data and services. This approach includes network controls, subnet segmentation, firewall rules, OS hardening, and hardware security measures.
???x",451,"Chapter 9.  Security We’ve become familiar with stories about security breaches of large-scale systems resulting in our data being exposed to all sorts of dodgy characters. But more recently, events l...",qwen2.5:latest,2025-11-02 23:56:16,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,API Keys,Fine-Grained vs Coarse-Grained Authorization,"---
#### Fine-Grained vs Coarse-Grained Authorization
Background context: The text discusses the difference between coarse-grained and fine-grained authorization mechanisms, emphasizing that while coarse-grained authentication might be sufficient for basic access control (e.g., preventing non-logged-in users from accessing a helpdesk application), more nuanced decisions are needed to manage permissions based on roles or groups. This finer control allows differentiating levels of access within the same authenticated user.

:p What is the difference between coarse-grained and fine-grained authorization?
??x
Coarse-grained authorization involves making broad, general decisions about who can access a resource or perform an action (e.g., allowing anyone with a role 'STAFF' to access the helpdesk application). Fine-grained authorization provides more detailed control by considering specific attributes of the user or their roles (e.g., a CALL_CENTER member may view customer details but not payment information, while a CALL_CENTER_TEAM_LEADER can issue larger refunds).

In contrast, fine-grained roles should be modeled around organizational functions rather than specific behaviors within services. This approach allows for more flexibility in managing access independently of the services' internal logic.

```java
// Example code to check coarse-grained and fine-grained authorization
public boolean hasAccess(String role) {
    // Coarse-grained: Check if user is part of 'STAFF' or higher role
    return roles.contains(""STAFF"") || roles.contains(""CALL_CENTER_LEADER"");
}

public boolean canViewPaymentDetails() {
    // Fine-grained: Check if user is in 'CALL_CENTER' and not 'TEAM_LEADER'
    return roles.contains(""CALL_CENTER"") && !roles.contains(""CALL_CENTER_TEAM_LEADER"");
}
```
x??",1802,"Fine-Grained Authorization A gateway may be able to provide fairly effective coarse-grained authentication. For example, it could prevent access to any non-logged-in user to the helpdesk application. ...",qwen2.5:latest,2025-11-02 23:56:26,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,API Keys,Service-to-Service Authentication and Authorization,"#### Service-to-Service Authentication and Authorization
Background context: The text introduces the concept of service-to-service authentication, which refers to secure communication between services or programs rather than humans. It emphasizes the importance of distinguishing internal calls from external ones to maintain security within a network perimeter.

:p What is service-to-service authentication, and why is it important?
??x
Service-to-service authentication involves securing communication between different software services or applications that interact with each other. This is distinct from human user authentication because it deals with automated interactions rather than individual logins. The importance lies in maintaining security within a network perimeter by ensuring that only trusted internal calls can bypass certain security checks.

The key objective is to avoid exposing sensitive data or functionality through unauthorized service-to-service communications, even if the perimeter of your network is secured against external threats.

```java
// Example code for internal authentication check
public boolean authenticateInternalRequest(String serviceName) {
    // Check if the request comes from an allowed service within our network
    return authorizedServices.contains(serviceName);
}
```
x??",1330,"Fine-Grained Authorization A gateway may be able to provide fairly effective coarse-grained authentication. For example, it could prevent access to any non-logged-in user to the helpdesk application. ...",qwen2.5:latest,2025-11-02 23:56:26,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,API Keys,Man-in-the-Middle (MitM) Attacks and Internal Security,"#### Man-in-the-Middle (MitM) Attacks and Internal Security
Background context: The text warns about the risk of man-in-the-middle attacks, where attackers can intercept and manipulate data transmitted between services. Even if external access is secured by perimeter defenses, internal communications remain vulnerable to such threats.

:p How do man-in-the-middle (MitM) attacks pose a threat within a network's perimeter?
??x
Man-in-the-Middle (MitM) attacks are a significant security risk because they allow an attacker to intercept and potentially manipulate data being transmitted between services. Even if the external perimeter of your network is secure, internal communications can still be vulnerable to MitM attacks.

For example, an attacker who has managed to penetrate the network could use packet sniffing tools or other means to eavesdrop on service-to-service communications, alter data in transit, or impersonate one of the services. This threat exists because services often assume that any traffic coming from within the same perimeter is inherently trusted and do not apply additional security checks.

To mitigate this risk, it's crucial to implement robust encryption for internal communications and use secure authentication mechanisms even for internal calls.
x??

---",1294,"Fine-Grained Authorization A gateway may be able to provide fairly effective coarse-grained authentication. For example, it could prevent access to any non-logged-in user to the helpdesk application. ...",qwen2.5:latest,2025-11-02 23:56:26,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,API Keys,HTTP Basic Authentication,"#### HTTP Basic Authentication
HTTP Basic Authentication allows for a client to send a username and password in a standard HTTP header. The server can then check these details and confirm that the client is allowed to access the service. This method is widely used due to its simplicity, but it comes with significant security risks.
:p What are the main issues associated with using HTTP Basic Authentication without HTTPS?
??x
Using HTTP Basic Authentication over plain HTTP poses a serious risk because the username and password are sent in clear text. Any intermediate party can intercept this information, leading to potential unauthorized access. For example:
```java
// Example of sending credentials via HTTP (insecure)
String credentials = ""username:password"";
String encodedCredentials = Base64.getEncoder().encodeToString(credentials.getBytes());
HttpURLConnection conn = (HttpURLConnection) new URL(""http://example.com/api"").openConnection();
conn.setRequestProperty(""Authorization"", ""Basic "" + encodedCredentials);
```
x??",1035,"This is by far the most common form of inside-perimeter trust I see in organizations. They may decide to run this traffic over HTTPS, but they don’t do much else. I’m not saying that is a good thing. ...",qwen2.5:latest,2025-11-02 23:56:40,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,API Keys,Use of HTTPS,"#### Use of HTTPS
When using HTTP Basic Authentication, it is crucial to use HTTPS to encrypt the data transmission. HTTPS ensures that the client gains strong guarantees that they are talking to the correct server and provides additional protection against eavesdropping or tampering.
:p Why should HTTP Basic Authentication be used over HTTPS?
??x
Using HTTPS with HTTP Basic Authentication encrypts the username and password, making it much harder for an attacker to intercept them. The encryption ensures that even if data is intercepted, it cannot be read by unauthorized parties. For example:
```java
// Example of sending credentials via HTTPS (secure)
String credentials = ""username:password"";
String encodedCredentials = Base64.getEncoder().encodeToString(credentials.getBytes());
HttpURLConnection conn = (HttpURLConnection) new URL(""https://example.com/api"").openConnection();
conn.setRequestProperty(""Authorization"", ""Basic "" + encodedCredentials);
```
x??",968,"This is by far the most common form of inside-perimeter trust I see in organizations. They may decide to run this traffic over HTTPS, but they don’t do much else. I’m not saying that is a good thing. ...",qwen2.5:latest,2025-11-02 23:56:40,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,API Keys,Certificate Management,"#### Certificate Management
Managing SSL certificates can be problematic, especially when managing multiple machines. Organizations may need to handle their own certificate issuing process, which adds administrative and operational burdens.
:p What are the challenges of managing SSL certificates?
??x
The main challenges include:
- Issuing and renewing certificates for multiple servers.
- Handling self-signed certificates that require more thought in disaster scenarios.
- Ensuring proper revocation procedures when a certificate is compromised.
For example, a simplified process might look like this:
```java
// Example of certificate management (simplified)
KeyStore keyStore = KeyStore.getInstance(""JKS"");
FileInputStream fis = new FileInputStream(""/path/to/keystore.jks"");
keyStore.load(fis, ""password"".toCharArray());
KeyManagerFactory kmf = KeyManagerFactory.getInstance(KeyManagerFactory.getDefaultAlgorithm());
kmf.init(keyStore, ""password"".toCharArray());

SSLContext sslContext = SSLContext.getInstance(""TLS"");
sslContext.init(kmf.getKeyManagers(), null, new SecureRandom());
```
x??",1096,"This is by far the most common form of inside-perimeter trust I see in organizations. They may decide to run this traffic over HTTPS, but they don’t do much else. I’m not saying that is a good thing. ...",qwen2.5:latest,2025-11-02 23:56:40,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,API Keys,Caching and SSL,"#### Caching and SSL
When using SSL, traffic cannot be cached by reverse proxies like Varnish or Squid. This means that caching must either occur within the server or client.
:p How can we enable caching when using SSL?
??x
To enable caching with SSL, one approach is to have a load balancer terminate the SSL connection and then cache the content. The cached content would be sent unencrypted over the internal network. For example:
```java
// Example of load balancing and caching (simplified)
Nginx config snippet for terminating SSL and caching:
```
stream {
    upstream backend {
        server backend1.example.com;
        server backend2.example.com;
    }

    server {
        listen 443 ssl;
        ssl_certificate /path/to/certificate.pem;
        ssl_certificate_key /path/to/private.key;

        proxy_pass backend;
        proxy_set_header Host $host;
    }
}

http {
    ...
    proxy_cache_path /var/cache/nginx levels=1:2 keys_zone=my_cache:10m inactive=60m use_temp_path=off;

    server {
        listen 80;
        location / {
            proxy_pass http://backend;
            proxy_set_header Host $host;
            proxy_cache my_cache;
            proxy_cache_valid 200 304 30m;
        }
    }
}
```
x??",1234,"This is by far the most common form of inside-perimeter trust I see in organizations. They may decide to run this traffic over HTTPS, but they don’t do much else. I’m not saying that is a good thing. ...",qwen2.5:latest,2025-11-02 23:56:40,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,API Keys,Integration with SSO Solutions,"#### Integration with SSO Solutions
If an organization is already using SAML or OpenID Connect for authentication, it can leverage these solutions for service-to-service interactions. This approach reduces the need to manage multiple sets of credentials and leverages existing security infrastructure.
:p How can we integrate service-to-service interactions with existing SSO solutions?
??x
Integrating service-to-service interactions with SAML or OpenID Connect involves configuring each service to communicate through the gateway that handles authentication. For example:
```java
// Example of integrating with an SAML provider (simplified)
public class SsoService {
    private final String samlMetadataUrl;
    private final String identityProviderUrl;

    public void authenticateAndAuthorize(String username, String password) throws AuthenticationException {
        // Code to interact with the identity provider and validate credentials
    }
}
```
x??",961,"This is by far the most common form of inside-perimeter trust I see in organizations. They may decide to run this traffic over HTTPS, but they don’t do much else. I’m not saying that is a good thing. ...",qwen2.5:latest,2025-11-02 23:56:40,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,API Keys,Avoiding Self-Signed Certificates,"#### Avoiding Self-Signed Certificates
Self-signed certificates are not easily revokable and require careful thought in disaster scenarios. Organizations should avoid self-signing whenever possible to reduce administrative burdens.
:p What are the downsides of using self-signed certificates?
??x
Using self-signed certificates can lead to several issues:
- They cannot be revoked, making it harder to address security breaches.
- Users may receive certificate warnings and errors.
- It complicates trust management within the organization.
For example, a warning might look like this in a browser:
```
This website's certificate is not trusted!
```
x??",653,"This is by far the most common form of inside-perimeter trust I see in organizations. They may decide to run this traffic over HTTPS, but they don’t do much else. I’m not saying that is a good thing. ...",qwen2.5:latest,2025-11-02 23:56:40,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,API Keys,Existing Infrastructure and Centralized Directory Server,"#### Existing Infrastructure and Centralized Directory Server
Background context explaining the use of existing infrastructure for service access control. This method involves centralizing all service access controls within a directory server, which can provide fine-grained authentication mechanisms.

In this approach, clients authenticate themselves with an identity provider using credentials, and services retrieve necessary information to make authentication decisions.
:p What are the advantages of using an existing infrastructure with centralized directory servers?
??x
The key advantages include leveraging pre-existing infrastructure and simplifying management through a central directory server. This setup also allows for easier revocation of access if credentials are compromised by assigning them to specific service accounts.

For example, each microservice can have its own set of credentials, making it simpler to manage access control.
```java
public class AuthService {
    public void authenticateClient(String clientId, String secret) {
        // Authenticate client using the identity provider and retrieve necessary information
    }
}
```
x??",1168,"The advantage here is that you’re making use of existing infrastructure, and get to centralize all your service access controls in a central directory server. We’d still need to route this over HTTPS ...",qwen2.5:latest,2025-11-02 23:56:53,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,API Keys,Secure Storage of Credentials,"#### Secure Storage of Credentials
Background context explaining the importance of securely storing credentials when using authentication mechanisms like Basic Auth. Ensuring that sensitive data such as usernames and passwords are stored in a secure manner is crucial to prevent unauthorized access.

This also involves implementing secure storage practices for client credentials.
:p How do we securely store credentials for clients?
??x
To securely store credentials, it's essential to use strong encryption methods and secure key management practices. This can be achieved by using environment variables, encrypted files, or secrets managers like HashiCorp Vault. Additionally, avoid hardcoding passwords directly in the application code.

For example:
```java
public class SecureStorage {
    private String username;
    private String password;

    public SecureStorage(String encryptedUsername, String encryptedPassword) {
        // Decrypt and set the credentials here
        this.username = decrypt(encryptedUsername);
        this.password = decrypt(encryptedPassword);
    }

    private String decrypt(String encryptedValue) {
        // Implement decryption logic using a secure method like AES
        return decryptedValue;
    }
}
```
x??",1257,"The advantage here is that you’re making use of existing infrastructure, and get to centralize all your service access controls in a central directory server. We’d still need to route this over HTTPS ...",qwen2.5:latest,2025-11-02 23:56:53,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,API Keys,SAML vs. OpenID Connect,"#### SAML vs. OpenID Connect
Background context explaining the differences between SAML and OpenID Connect in terms of implementation complexity and support levels. While both provide robust authentication mechanisms, their ease of use varies.

SAML typically requires more coding effort due to its complex protocols, whereas OpenID Connect offers a simpler workflow but has limited widespread adoption.
:p What are the advantages and disadvantages of using SAML for authentication?
??x
Advantages of SAML include strong security guarantees and support for a wide range of applications. However, implementing SAML involves significant coding complexity because it requires handling SSO (Single Sign-On) flows and token exchanges.

Disadvantages include the need for detailed implementation knowledge and higher maintenance overhead due to the intricate protocol.
```java
public class SamlAuthentication {
    public boolean authenticateUser(String assertion) {
        // Validate the SAML assertion using a trusted identity provider
        return isValidAssertion(assertion);
    }

    private boolean isValidAssertion(String assertion) {
        // Implement logic to validate the assertion against known criteria
        return true;
    }
}
```
x??",1254,"The advantage here is that you’re making use of existing infrastructure, and get to centralize all your service access controls in a central directory server. We’d still need to route this over HTTPS ...",qwen2.5:latest,2025-11-02 23:56:53,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,API Keys,Client Certificates and TLS,"#### Client Certificates and TLS
Background context explaining the use of client certificates for authentication, leveraging Transport Layer Security (TLS). This approach provides strong guarantees about the identity of clients but comes with significant operational challenges due to certificate management.

Issues include creating, managing, and diagnosing certificate problems, as well as handling certificate revocation.
:p What are the benefits and drawbacks of using client certificates for authentication?
??x
Benefits:
- Provides strong guarantees about the authenticity of the client.
- Can be used in environments where data sensitivity is high or network control is limited.

Drawbacks:
- Complex certificate management processes.
- Difficulty diagnosing issues related to certificates.
- Challenges with revoking and reissuing certificates when necessary.

Example:
```java
public class CertificateAuthentication {
    public boolean authenticateClient(X509Certificate cert) {
        // Verify the client's certificate using a trusted CA
        return verifyCertificate(cert);
    }

    private boolean verifyCertificate(X509Certificate cert) {
        // Implement logic to validate the certificate against known criteria
        return true;
    }
}
```
x??",1275,"The advantage here is that you’re making use of existing infrastructure, and get to centralize all your service access controls in a central directory server. We’d still need to route this over HTTPS ...",qwen2.5:latest,2025-11-02 23:56:53,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,API Keys,HMAC Over HTTP,"#### HMAC Over HTTP
Background context explaining why Basic Auth over plain HTTP is not secure and the use of HMAC (Hash-based Message Authentication Code) as an alternative. HMAC provides a way to ensure message integrity and authenticity without the need for HTTPS.

While still not as secure as full TLS, HMAC can be used when some level of security is needed but HTTPS cannot be implemented.
:p What are the benefits and drawbacks of using HMAC over HTTP?
??x
Benefits:
- Provides a degree of security by ensuring message integrity and authenticity.
- Can be more straightforward to implement than setting up HTTPS.

Drawbacks:
- Does not provide confidentiality, so sensitive information should never be sent in plain text.
- Still requires careful handling of keys and potential exposure risks.

Example:
```java
public class HMACAuthentication {
    public boolean verifySignature(String message, String key) {
        // Generate the HMAC signature for the message using the provided key
        return isValidHMAC(message, key);
    }

    private boolean isValidHMAC(String message, String key) {
        // Implement logic to validate the HMAC signature against expected value
        return true;
    }
}
```
x??",1224,"The advantage here is that you’re making use of existing infrastructure, and get to centralize all your service access controls in a central directory server. We’d still need to route this over HTTPS ...",qwen2.5:latest,2025-11-02 23:56:53,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,API Keys,HMAC-Based Signing for Requests,"#### HMAC-Based Signing for Requests
Background context: Amazon’s S3 APIs and parts of OAuth use HMAC (Hash-based Message Authentication Code) to sign requests. This approach ensures that request bodies are securely transmitted by hashing them with a private key, which is never sent over the network. The server then verifies the hash using its own copy of the private key.
:p What is HMAC and how does it work?
??x
HMAC works by combining a cryptographic hash function (like SHA-256) with a secret key to produce a message authentication code. This code is appended or embedded in the request, ensuring that any tampering can be detected.

The process involves:
1. Hashing the request body and private key.
2. Sending the hashed value along with the request.
3. The server recomputes the hash using its copy of the private key to verify the request integrity.

Example pseudocode for HMAC generation:

```pseudocode
function generateHMAC(requestBody, privateKey) {
    let hmac = hash(requestBody + privateKey); // Concatenate and hash
    return hmac;
}

// Usage in a request signing function
request.hmacSignature = generateHMAC(request.body, privateKey);
```
x??",1168,"An alternative approach, as used extensively by Amazon’s S3 APIs for AWS and in parts of the OAuth specification, is to use a  hash-based messaging code (HMAC)  to sign the request. With HMAC the body...",qwen2.5:latest,2025-11-02 23:57:05,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,API Keys,Shared Secrets for HMAC Security,"#### Shared Secrets for HMAC Security
Background context: For HMAC to work effectively, both the client and server need a shared secret (private key). This secret is used to generate the hash that is sent with each request. The main challenge is securely sharing this secret between the client and server without compromising its security.
:p How do you ensure the security of a shared secret in an HMAC-based system?
??x
Securing a shared secret involves several steps:
1. Avoid hardcoding the key at both ends; instead, use secure methods to distribute it.
2. If distributed over another protocol, ensure that this protocol is highly secure and trusted.
3. Implement mechanisms for revoking access if the secret becomes compromised.

Example of secure key distribution (hypothetical):

```pseudocode
function securelyDistributeKey(client, server) {
    // Use a secure channel to exchange keys, such as TLS with certificate-based authentication
    client.key = server.receiveKey();
    server.key = client.sendKey();
}
```
x??",1029,"An alternative approach, as used extensively by Amazon’s S3 APIs for AWS and in parts of the OAuth specification, is to use a  hash-based messaging code (HMAC)  to sign the request. With HMAC the body...",qwen2.5:latest,2025-11-02 23:57:05,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,API Keys,JSON Web Tokens (JWT),"#### JSON Web Tokens (JWT)
Background context: JWTs are another form of signing requests that work similarly to HMAC. They encode the payload and a signature into a compact, URL-safe string. JWTs are used in OAuth 2.0 for access tokens.
:p What is a JSON Web Token (JWT)?
??x
A JSON Web Token (JWT) is an open standard (RFC 7519) that allows users to securely transmit information between parties as a JSON object. It is signed with a secret key or a public-private key pair.

Example of JWT structure:

```json
{
    ""alg"": ""HS256"",
    ""typ"": ""JWT""
}
.
{
    ""sub"": ""1234567890"",
    ""name"": ""John Doe"",
    ""iat"": 1516239022
}
.
""signature""
```

Here, the token consists of three parts: header, payload, and signature. The header specifies the algorithm (e.g., HS256 for HMAC SHA-256) and the type (JWT).

```java
import io.jsonwebtoken.Jwts;
import io.jsonwebtoken.SignatureAlgorithm;

public class JwtExample {
    public String createJwt(String subject, long ttlMillis) {
        SignatureAlgorithm signatureAlgorithm = SignatureAlgorithm.HS256;
        byte[] apiKeySecretBytes = DatatypeConverter.parseBase64Binary(""secretkey"");
        Key signingKey = new SecretKeySpec(apiKeySecretBytes, signatureAlgorithm.getJcaName());
        
        Date now = new Date();
        Date expiryDate = new Date(now.getTime() + ttlMillis);

        return Jwts.builder()
                .setSubject(subject)
                .setIssuedAt(now)
                .setExpiration(expiryDate)
                .signWith(signatureAlgorithm, signingKey)
                .compact();
    }
}
```
x??",1582,"An alternative approach, as used extensively by Amazon’s S3 APIs for AWS and in parts of the OAuth specification, is to use a  hash-based messaging code (HMAC)  to sign the request. With HMAC the body...",qwen2.5:latest,2025-11-02 23:57:05,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,API Keys,API Keys for Authentication,"#### API Keys for Authentication
Background context: Many public APIs (like Twitter and AWS) use API keys to identify users and apply rate limiting or other access controls. API keys are often implemented using a single key shared between client and server, or through a public-private key pair system.
:p What is an API Key?
??x
An API Key is a unique identifier that services like Twitter, Google, Flickr, and AWS use to authenticate calls to their APIs. It allows the service to identify who is making the request and apply access control policies.

Example usage:

```java
public class ApiKeyAuth {
    public boolean validateApiKey(String apiKey) {
        // Assume we have a list of valid API keys stored in memory or database.
        List<String> validKeys = Arrays.asList(""1234567890"", ""abcdef123456"");
        
        return validKeys.contains(apiKey);
    }
}
```

x??

---",886,"An alternative approach, as used extensively by Amazon’s S3 APIs for AWS and in parts of the OAuth specification, is to use a  hash-based messaging code (HMAC)  to sign the request. With HMAC the body...",qwen2.5:latest,2025-11-02 23:57:05,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Deputy Problem,Confused Deputy Problem,"#### Confused Deputy Problem
This problem arises when a service (the deputy) makes calls on behalf of another entity, potentially leading to unauthorized access. In the context of microservices, it can occur if an authenticated service attempts to make requests to downstream services that should not be accessible by its current principal.
:p What is the confused deputy problem in the context of microservices?
??x
The confused deputy problem refers to a situation where a malicious party can trick a deputy service into making calls to a downstream service on behalf of another entity, potentially leading to unauthorized access. For example, if an online shopping system authenticates a user and makes requests to services that should only be accessible by the authenticated user, it could result in sensitive data being exposed or manipulated.
```java
// Example code where a deputy might misuse its permissions
public class DeputyService {
    public void makeRequest(String principalId) {
        // Assume this service has access to multiple downstream services
        OrderService orderService = new OrderService();
        ShippingService shippingService = new ShippingService();

        // Malicious user could manipulate the 'principalId' parameter
        orderService.getOrder(principalId);
        shippingService.getShippingInfo(principalId);
    }
}
```
x??",1376,The Deputy Problem Having a principal authenticate with a given microserservice is simple enough. But what happens if that service then needs to make additional calls to complete an operation? Take a ...,qwen2.5:latest,2025-11-02 23:57:16,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Deputy Problem,Authentication in Microservices Communication,"#### Authentication in Microservices Communication
In microservices architecture, especially when services communicate with each other, it is crucial to ensure that only authorized calls are made. This can be achieved through various authentication methods such as SAML or OpenID Connect.
:p How do you authenticate service-to-service communication in a microservices environment?
??x
Service-to-service communication in a microservices environment can be authenticated using protocols like SAML (Security Assertion Markup Language) or OpenID Connect, which are designed to handle the secure exchange of authentication and authorization information between services. These protocols help ensure that only authorized calls are made by verifying the identity and permissions of the caller.
```java
// Example of using a token for service-to-service communication
public class ServiceAuthenticator {
    public String authenticateService(String serviceName) {
        // Logic to generate or validate an authentication token
        return ""service_token"";
    }
}
```
x??",1069,The Deputy Problem Having a principal authenticate with a given microserservice is simple enough. But what happens if that service then needs to make additional calls to complete an operation? Take a ...,qwen2.5:latest,2025-11-02 23:57:16,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Deputy Problem,Implicit Trust in Microservices,"#### Implicit Trust in Microservices
Implicit trust involves assuming that services within the same perimeter are trustworthy and allowing them to make calls to each other without additional checks. However, this approach can lead to security vulnerabilities if a service is compromised.
:p What does implicit trust mean in microservices?
??x
Implicit trust means that services within the same microservices environment or network assume that all services are trustworthy and do not perform additional authentication or authorization checks on calls between them. While it simplifies communication, this approach can lead to security risks if a service is compromised, as an attacker could exploit the implicit trust to make unauthorized requests.
```java
// Example of implicit trust in microservices
public class ServiceA {
    public void callServiceB() {
        // Implicitly trusts that calls from within the same network are valid
        ServiceB serviceB = new ServiceB();
        serviceB.performAction();
    }
}
```
x??",1031,The Deputy Problem Having a principal authenticate with a given microserservice is simple enough. But what happens if that service then needs to make additional calls to complete an operation? Take a ...,qwen2.5:latest,2025-11-02 23:57:16,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Deputy Problem,Protecting Against Confused Deputy Problem,"#### Protecting Against Confused Deputy Problem
To mitigate the confused deputy problem, one approach is to have the original principal's credentials passed downstream so that each subsequent service can verify the identity of the caller. However, this can be complex and often impractical with some authentication schemes.
:p How can you protect against the confused deputy problem?
??x
Protecting against the confused deputy problem involves ensuring that each service validates the identity of the original principal before making calls to downstream services. This can be done by passing the original principal's credentials along with each request, although this approach is complex and often impractical due to limitations in certain authentication schemes like SAML.

For example, you could modify your service to include the original user's credentials:
```java
public class OrderService {
    public void getOrder(String orderId, String userId) {
        // Validate that the call is made on behalf of the correct user
        if (userId.equals(getAuthenticatedUserId())) {
            // Proceed with fetching order information
        } else {
            throw new UnauthorizedAccessException();
        }
    }

    private String getAuthenticatedUserId() {
        // Logic to retrieve the authenticated user ID, e.g., from a token
        return ""user_123"";
    }
}
```
x??

---",1393,The Deputy Problem Having a principal authenticate with a given microserservice is simple enough. But what happens if that service then needs to make additional calls to complete an operation? Take a ...,qwen2.5:latest,2025-11-02 23:57:16,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,A Worked Example,Importance of Using Well-Known Encryption Algorithms,"---
#### Importance of Using Well-Known Encryption Algorithms
Background context explaining why using well-known encryption algorithms is crucial. Highlight the risks associated with implementing custom encryption solutions.

:p Why should one avoid implementing their own encryption algorithms?
??x
Implementing your own encryption algorithm can be highly risky because it introduces significant vulnerabilities that may not have been considered or tested thoroughly. Instead, using well-reviewed and regularly patched implementations of established encryption standards like AES-128 or AES-256 is much safer.

For example, consider the following pseudocode for encrypting data with a well-known algorithm:
```python
from Crypto.Cipher import AES

def encrypt_data(key, plaintext):
    # Ensure the key length matches the block size of AES-128 (16 bytes)
    if len(key) != 16 and len(key) != 32:
        raise ValueError(""Key must be either 16 or 32 bytes long"")

    cipher = AES.new(key, AES.MODE_EAX)
    ciphertext, tag = cipher.encrypt_and_digest(plaintext.encode('utf-8'))
    return (ciphertext, tag)

# Example usage
key = b'supersecretkey'  # Must be 16 or 32 bytes
plaintext = ""Sensitive data""
encrypted_data, _ = encrypt_data(key, plaintext)
print(f""Encrypted Data: {encrypted_data}"")
```
x??",1305,"Securing Data at Rest Data lying about is a liability, especially if it is sensitive. Hopefully we’ve done everything we can to ensure attackers cannot breach our network, and also that they cannot br...",qwen2.5:latest,2025-11-02 23:57:26,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,A Worked Example,Salted Password Hashing,"#### Salted Password Hashing
Background context explaining the importance of protecting passwords and the risks associated with poorly implemented password hashing.

:p Why is salted password hashing important?
??x
Salted password hashing adds an additional layer of security by appending a unique, random value (salt) to the password before hashing. This makes it more difficult for attackers to use precomputed hash tables or rainbow tables to crack passwords. The salt ensures that even if two users have the same password, their hashes will be different.

Example in pseudocode:
```java
import java.security.MessageDigest;
import javax.crypto.SecretKeyFactory;
import javax.crypto.spec.PBEKeySpec;

public class PasswordHashing {
    public static String hashPassword(String password, byte[] salt) throws Exception {
        SecretKeyFactory skf = SecretKeyFactory.getInstance(""PBKDF2WithHmacSHA1"");
        PBEKeySpec spec = new PBEKeySpec(password.toCharArray(), salt, 65536, 128);
        return new String(skf.generateSecret(spec).getEncoded());
    }

    public static void main(String[] args) {
        byte[] salt = ""random_salt_value"".getBytes(); // Must be unique per user
        String password = ""user_password"";
        try {
            String hashedPassword = hashPassword(password, salt);
            System.out.println(""Hashed Password: "" + hashedPassword);
        } catch (Exception e) {
            e.printStackTrace();
        }
    }
}
```
x??",1470,"Securing Data at Rest Data lying about is a liability, especially if it is sensitive. Hopefully we’ve done everything we can to ensure attackers cannot breach our network, and also that they cannot br...",qwen2.5:latest,2025-11-02 23:57:26,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,A Worked Example,Secure Key Management and Storage,"#### Secure Key Management and Storage
Background context explaining the importance of secure key management for encryption. Highlight that storing keys securely is critical to protect encrypted data.

:p Why is it crucial to store keys separately from the encrypted data?
??x
Storing keys separately ensures that even if an attacker gains access to the encrypted data, they cannot decrypt it without the correct keys. This separation helps prevent unauthorized decryption and ensures that the encryption provides actual security benefits.

Example in pseudocode:
```java
public class KeyManager {
    private static final String KEY_FILE_PATH = ""path/to/keyfile.key"";

    public byte[] getKey() throws Exception {
        // Load the key from a secure location, e.g., a hardware security module (HSM)
        try (FileInputStream fis = new FileInputStream(KEY_FILE_PATH)) {
            return IOUtils.toByteArray(fis);
        }
    }

    public static void main(String[] args) {
        try {
            KeyManager manager = new KeyManager();
            byte[] key = manager.getKey();
            System.out.println(""Key loaded successfully: "" + Arrays.toString(key));
        } catch (Exception e) {
            e.printStackTrace();
        }
    }
}
```
x??

---",1270,"Securing Data at Rest Data lying about is a liability, especially if it is sensitive. Hopefully we’ve done everything we can to ensure attackers cannot breach our network, and also that they cannot br...",qwen2.5:latest,2025-11-02 23:57:26,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,A Worked Example,Avoid Implementing Your Own Encryption Solutions,"#### Avoid Implementing Your Own Encryption Solutions
Background context: The advice provided emphasizes using well-researched and established encryption solutions rather than implementing custom ones. Custom implementations can introduce security vulnerabilities due to lack of scrutiny and testing.

:p What is the primary recommendation regarding encryption methods?
??x
The primary recommendation is to avoid implementing your own encryption solutions. Instead, use well-established, vetted encryption algorithms provided by libraries or frameworks.
```java
// Example usage of a secure encryption library in Java
import javax.crypto.Cipher;
import java.security.KeyGenerator;

public class EncryptionExample {
    private static final String ALGORITHM = ""AES"";
    
    public static void main(String[] args) throws Exception {
        KeyGenerator keyGen = KeyGenerator.getInstance(ALGORITHM);
        SecretKey secretKey = keyGen.generateKey();
        
        Cipher cipher = Cipher.getInstance(ALGORITHM);
        cipher.init(Cipher.ENCRYPT_MODE, secretKey);
        
        byte[] encryptedData = cipher.doFinal(""Sensitive data"".getBytes());
        System.out.println(""Encrypted: "" + java.util.Arrays.toString(encryptedData));
    }
}
```
x??",1255,"Avoid implementing your own, and do some good research. Pick Your Targets Assuming everything should be encrypted can simplify things somewhat. There is no guesswork about what should or should not be...",qwen2.5:latest,2025-11-02 23:57:40,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,A Worked Example,Pick Your Targets,"#### Pick Your Targets
Background context: The text suggests that encrypting everything can be overly complex and resource-intensive. Instead, it recommends focusing on critical data stores and limiting encryption to specific tables or fields.

:p How should you approach identifying what needs to be encrypted?
??x
You should identify the most sensitive data stores and apply encryption selectively. Subdivide your system into services to find large chunks of data that can be encrypted together but recognize that encrypting everything might not be practical.
```java
// Example pseudo-code for identifying critical tables in a database
public class DatabaseEncryption {
    private Map<String, String> criticalTables;
    
    public void identifyCriticalTables() {
        // Logic to analyze which tables contain sensitive information
        this.criticalTables.put(""orders"", ""AES"");
        this.criticalTables.put(""customer_details"", ""RSA"");
        // More logic...
    }
}
```
x??",990,"Avoid implementing your own, and do some good research. Pick Your Targets Assuming everything should be encrypted can simplify things somewhat. There is no guesswork about what should or should not be...",qwen2.5:latest,2025-11-02 23:57:40,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,A Worked Example,Decrypt on Demand,"#### Decrypt on Demand
Background context: The principle of decrypting data only when necessary helps minimize the risk of exposing sensitive information. This approach ensures that data remains encrypted until it is actively used.

:p Why should you decrypt data only on demand?
??x
You should decrypt data only when needed to reduce the exposure window and minimize the risk of unauthorized access to sensitive information. Decrypting only on-demand also simplifies storage requirements since decrypted data can be stored in unencrypted form for the duration of its use.
```java
// Example pseudo-code for decrypting data on demand
public class DataDecryptor {
    private Cipher cipher;
    
    public void initialize() throws Exception {
        // Initialize encryption/decryption setup
        this.cipher = Cipher.getInstance(""AES"");
    }
    
    public String decryptData(String encryptedData, byte[] key) throws Exception {
        cipher.init(Cipher.DECRYPT_MODE, new SecretKeySpec(key, ""AES""));
        return new String(cipher.doFinal(Base64.getDecoder().decode(encryptedData)));
    }
}
```
x??",1110,"Avoid implementing your own, and do some good research. Pick Your Targets Assuming everything should be encrypted can simplify things somewhat. There is no guesswork about what should or should not be...",qwen2.5:latest,2025-11-02 23:57:40,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,A Worked Example,Encrypt Backups,"#### Encrypt Backups
Background context: Backing up encrypted data ensures that the backup itself is secure. However, managing key versions and ensuring the integrity of backups requires careful planning.

:p Why should you encrypt your backups?
??x
You should encrypt your backups to protect them from unauthorized access. Since backups often contain critical and sensitive information, encrypting them ensures they remain protected even if they are stolen or accessed without proper authorization.
```java
// Example pseudo-code for encrypting a backup file
public class BackupEncryptor {
    private Cipher cipher;
    
    public void initialize() throws Exception {
        this.cipher = Cipher.getInstance(""AES"");
    }
    
    public byte[] encryptBackup(byte[] data, byte[] key) throws Exception {
        cipher.init(Cipher.ENCRYPT_MODE, new SecretKeySpec(key, ""AES""));
        return cipher.doFinal(data);
    }
}
```
x??",932,"Avoid implementing your own, and do some good research. Pick Your Targets Assuming everything should be encrypted can simplify things somewhat. There is no guesswork about what should or should not be...",qwen2.5:latest,2025-11-02 23:57:40,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,A Worked Example,Defense in Depth,"#### Defense in Depth
Background context: Implementing multiple layers of security measures helps mitigate the risk of a single point of failure. This approach includes various security controls like firewalls and logging.

:p What does defense in depth entail?
??x
Defense in depth involves layering multiple security mechanisms to protect your system from various types of threats. It includes securing data in transit, at rest, and implementing additional protections such as firewalls and logging.
```java
// Example pseudo-code for setting up a firewall rule
public class FirewallRule {
    private String ipAddress;
    private int port;
    
    public void applyFirewallRule(String ipAddress, int port) {
        // Logic to apply the firewall rule
        System.out.println(""Allowing access to "" + ipAddress + "":"" + port);
    }
}
```
x??",848,"Avoid implementing your own, and do some good research. Pick Your Targets Assuming everything should be encrypted can simplify things somewhat. There is no guesswork about what should or should not be...",qwen2.5:latest,2025-11-02 23:57:40,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,A Worked Example,Logging,"#### Logging
Background context: Good logging practices help in detecting and responding to security incidents. However, it's crucial to handle sensitive data carefully to avoid potential leaks.

:p How can good logging aid in security?
??x
Good logging can assist in detecting and recovering from security incidents by providing a trail of events that occurred within the system. It helps in post-incident analysis to understand what happened and enables quicker response times.
```java
// Example pseudo-code for secure logging
public class SecureLogger {
    private String logFilePath;
    
    public void logSecureEvent(String message) {
        // Logic to securely write logs without exposing sensitive data
        System.out.println(""Writing to "" + logFilePath);
    }
}
```
x??",788,"Avoid implementing your own, and do some good research. Pick Your Targets Assuming everything should be encrypted can simplify things somewhat. There is no guesswork about what should or should not be...",qwen2.5:latest,2025-11-02 23:57:40,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,A Worked Example,Intrusion Detection Systems (IDS) and Intrusion Prevention Systems (IPS),"#### Intrusion Detection Systems (IDS) and Intrusion Prevention Systems (IPS)
Background context: IDS and IPS are critical components of network security, focusing on monitoring for suspicious activity within a network perimeter. Unlike firewalls that primarily focus on external threats, IDS and IPS actively monitor internal traffic to detect potential attacks. IDS is more passive, generating alerts when suspicious behavior is detected, whereas IPS can take action to prevent such behavior.

:p What are the primary differences between an Intrusion Detection System (IDS) and an Intrusion Prevention System (IPS)?
??x
An IDS primarily monitors for suspicious activity within a network perimeter and generates alerts when it detects potential threats. An IPS not only monitors but also takes proactive measures to stop suspicious activities before they can cause harm.

For example, if an IPS detects a known malicious pattern in incoming traffic, it will block that traffic immediately without waiting for further confirmation.
??x
The key difference lies in their roles:
- **IDS**: Alerts on potential threats (passive).
- **IPS**: Prevents or mitigates threats (active).

```java
public class IPSExample {
    public void handleTraffic(TrafficPacket packet) {
        if (isMalicious(packet)) {
            blockTraffic(packet);
        } else {
            processSafeTraffic(packet);
        }
    }

    private boolean isMalicious(TrafficPacket packet) {
        // Logic to detect malicious patterns
    }

    private void blockTraffic(TrafficPacket packet) {
        // Code to block the traffic
    }

    private void processSafeTraffic(TrafficPacket packet) {
        // Safe processing logic
    }
}
```
x??",1724,"Intrusion Detection (and Prevention) System Intrusion detection systems (IDS)  can monitor networks or hosts for suspicious behavior, reporting problems when it sees them. Intrusion prevention systems...",qwen2.5:latest,2025-11-02 23:57:53,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,A Worked Example,Network Segregation Using Microservices and VPCs,"#### Network Segregation Using Microservices and VPCs
Background context: In a monolithic architecture, network segmentation is limited. However, with microservices, you can implement fine-grained network segregation to enhance security. This involves using Virtual Private Clouds (VPCs) provided by cloud providers like AWS, which allow you to create separate subnets for different services.

:p How can microservices be used to improve network security through segmentation?
??x
Microservices architecture allows for the creation of smaller, independent components that can communicate with each other. By placing these microservices into different network segments, we can control how they interact and limit potential attack surfaces. Using VPCs in AWS, you can create separate subnets for different services and define peering rules to manage inter-subnet communication securely.

For example, you might segment your application based on team ownership or risk level.
??x
Network segregation using microservices and VPCs enhances security by isolating components that could be targeted. Here’s a simplified example:

```java
public class NetworkSegmentationExample {
    public void createSubnets(String teamName) {
        // Code to provision subnets for the given team
        System.out.println(""Provisioning subnet for "" + teamName);
    }

    public void definePeeringRules(Subnet subnet1, Subnet subnet2) {
        // Logic to define peering rules between subnets
        System.out.println(""Defining peering rule between subnets"");
    }
}
```
x??",1561,"Intrusion Detection (and Prevention) System Intrusion detection systems (IDS)  can monitor networks or hosts for suspicious behavior, reporting problems when it sees them. Intrusion prevention systems...",qwen2.5:latest,2025-11-02 23:57:53,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,A Worked Example,Operating System Security,"#### Operating System Security
Background context: The security of your application is only as strong as the operating system and other supporting tools it runs on. Common vulnerabilities in OSes can expose applications to risk, especially if they are running with high privileges like root.

:p How should you manage user permissions on an operating system to enhance security?
??x
To enhance security, run services under users that have minimal permissions necessary for their operation. This way, even if a service is compromised, the damage will be limited. Regularly patch your software to fix known vulnerabilities. Use tools like Microsoft’s SCCM or RedHat’s Spacewalk to automate patch management and ensure all machines are up-to-date.

For example, always apply security patches and updates promptly.
??x
Running services with minimal permissions:
```java
public class UserManagementExample {
    public void runServiceWithMinimalPermissions(String serviceName) {
        // Logic to configure service to use a user with minimal permissions
        System.out.println(""Running "" + serviceName + "" with minimal permissions"");
    }
}
```
Automating patch management:
```java
public class PatchManagementExample {
    public void applyPatches() {
        // Logic to check and apply patches using SCCM or Spacewalk
        System.out.println(""Applying latest patches..."");
    }
}
```
x??",1396,"Intrusion Detection (and Prevention) System Intrusion detection systems (IDS)  can monitor networks or hosts for suspicious behavior, reporting problems when it sees them. Intrusion prevention systems...",qwen2.5:latest,2025-11-02 23:57:53,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,A Worked Example,Security Modules for Operating Systems,"#### Security Modules for Operating Systems
Background context: Security modules can be installed in operating systems to add an extra layer of security. These modules can provide enhanced functionality such as mandatory access control (MAC) policies, which restrict how data can flow between different processes.

:p What are security modules and why should they be considered for use?
??x
Security modules enhance the base operating system by adding features like MAC policies that enforce strict rules on process interactions. This can prevent unauthorized data flows and improve overall security posture. For example, SELinux in Linux systems provides fine-grained control over file permissions.

For instance, SELinux policies can be configured to restrict what processes can do with files.
??x
Security modules like SELinux:
```java
public class SecurityModuleExample {
    public void configureSELinuxPolicy(String policy) {
        // Logic to configure SELinux policies
        System.out.println(""Configuring SELinux policy: "" + policy);
    }
}
```
x??

---",1068,"Intrusion Detection (and Prevention) System Intrusion detection systems (IDS)  can monitor networks or hosts for suspicious behavior, reporting problems when it sees them. Intrusion prevention systems...",qwen2.5:latest,2025-11-02 23:57:53,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,A Worked Example,AppArmor Overview,"#### AppArmor Overview
AppArmor allows you to define how your application is expected to behave, with the kernel keeping an eye on it. If it starts doing something it shouldn't, the kernel steps in. This security module has been around for a while and is used by default in Ubuntu and SUSE.
:p What is AppArmor?
??x
AppArmor allows you to define how your application is expected to behave, with the kernel monitoring its actions. If an application tries to perform unauthorized operations, the kernel can intervene to prevent them. It's commonly used on systems like Ubuntu and SUSE.
x??",587,"AppArmour, for example, allows you to define how your application is expected to behave, with the kernel keeping an eye on it. If it starts doing something it shouldn’t, the kernel steps in. AppArmour...",qwen2.5:latest,2025-11-02 23:58:01,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,A Worked Example,SELinux Overview,"#### SELinux Overview
SELinux (Security-Enhanced Linux) has traditionally been well supported by RedHat. It provides a more granular approach to security compared to AppArmor and aims to enhance system security with flexible policies.
:p What is SELinux?
??x
SELinux is an enhanced version of the Linux kernel that provides a more granular and flexible approach to security than AppArmor. It includes advanced security policies that can be customized to fit specific use cases, making it particularly useful for systems like RedHat where fine-grained control over resource access is required.
x??",596,"AppArmour, for example, allows you to define how your application is expected to behave, with the kernel keeping an eye on it. If it starts doing something it shouldn’t, the kernel steps in. AppArmour...",qwen2.5:latest,2025-11-02 23:58:01,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,A Worked Example,GrSecurity Overview,"#### GrSecurity Overview
GrSecurity aims to provide a simpler yet enhanced version of security compared to both AppArmor and SELinux. However, it requires a custom kernel to work properly.
:p What is GrSecurity?
??x
GrSecurity provides an enhanced security layer by simplifying the process compared to AppArmor and SELinux. It includes features that can be enabled in a custom kernel, offering more robust protection for applications but requiring a specific kernel version or build.
x??",487,"AppArmour, for example, allows you to define how your application is expected to behave, with the kernel keeping an eye on it. If it starts doing something it shouldn’t, the kernel steps in. AppArmour...",qwen2.5:latest,2025-11-02 23:58:01,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,A Worked Example,Security Architecture of MusicCorp,"#### Security Architecture of MusicCorp
MusicCorp needs to secure its data both in transit (HTTP) and at rest (database). The architecture must differentiate between public access and internal services. External traffic should be secured with HTTPS, while internal services can use more relaxed protocols.
:p How does MusicCorp's architecture handle security for data in transit?
??x
For data in transit, MusicCorp uses HTTPS to secure communications between the customer's browser and its servers. This ensures that sensitive information is encrypted during transmission, protecting it from eavesdropping or interception.
x??",626,"AppArmour, for example, allows you to define how your application is expected to behave, with the kernel keeping an eye on it. If it starts doing something it shouldn’t, the kernel steps in. AppArmour...",qwen2.5:latest,2025-11-02 23:58:01,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,A Worked Example,Client Certificates for Royalty Gateway,"#### Client Certificates for Royalty Gateway
MusicCorp needs to ensure that requests to the third-party royalty payment system are legitimate by using client certificates. These certificates confirm the identity of the requesting party and add an extra layer of security.
:p How does MusicCorp secure communications with its third-party royalty gateway?
??x
To secure communications with the third-party royalty gateway, MusicCorp requires the use of client certificates. This ensures that only authorized entities can request data from the system by verifying their identities through cryptographic means.
x??",610,"AppArmour, for example, allows you to define how your application is expected to behave, with the kernel keeping an eye on it. If it starts doing something it shouldn’t, the kernel steps in. AppArmour...",qwen2.5:latest,2025-11-02 23:58:01,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,A Worked Example,Internal Services and External Access,"#### Internal Services and External Access
Internal services within the network perimeter are used for collaboration and can be less strict about security since they operate within a trusted environment. However, public-facing services like web browsers need to be secured with HTTPS to protect user data.
:p How does MusicCorp handle internal services compared to external access?
??x
For internal services, MusicCorp uses more relaxed protocols because these services are only accessed by trusted parties within the network perimeter. For external access, particularly from customers' web browsers, MusicCorp employs HTTPS to ensure that all sensitive data is encrypted during transmission.
x??

---",701,"AppArmour, for example, allows you to define how your application is expected to behave, with the kernel keeping an eye on it. If it starts doing something it shouldn’t, the kernel steps in. AppArmour...",qwen2.5:latest,2025-11-02 23:58:01,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,A Worked Example,Sharing Catalog Data Widely,"#### Sharing Catalog Data Widely
Background context explaining the importance of sharing catalog data to allow easy music purchases while preventing abuse. API keys are suggested as a solution to track usage and prevent misuse.

:p How can we ensure our catalog data is shared widely but not misused?
??x
We can use API keys to share our catalog data widely, allowing people to easily buy music from us. This helps in tracking who is using the data without compromising it.
x??",477,"For the feeds of catalog data, we want this information shared as widely as possible to allow people to easily buy music from us. However, we don’t want this abused, and we’d like some idea of who is ...",qwen2.5:latest,2025-11-02 23:58:08,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,A Worked Example,Network Perimeter Security,"#### Network Perimeter Security
Explanation on securing the network perimeter by implementing a properly configured firewall and choosing an appropriate security appliance for detecting malicious traffic like port scanning or denial-of-service attacks.

:p What steps can we take to secure our network perimeter?
??x
To secure the network perimeter, we should configure a proper firewall and select a suitable hardware or software security appliance to monitor and block malicious traffic such as port scanning and denial-of-service (DoS) attacks.
x??",551,"For the feeds of catalog data, we want this information shared as widely as possible to allow people to easily buy music from us. However, we don’t want this abused, and we’d like some idea of who is ...",qwen2.5:latest,2025-11-02 23:58:08,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,A Worked Example,Data Encryption for Customer Service,"#### Data Encryption for Customer Service
Explanation on encrypting customer data held by the customer service and decrypting it on read. This approach ensures that even if attackers breach the network, they cannot easily retrieve bulk customer data.

:p How do we protect our customers' data in the customer service?
??x
We can protect our customers' data by encrypting it at rest and decrypting it on read. If attackers penetrate our network, they will still be able to make API requests but won't be able to obtain bulk customer data without decryption keys.
x??",565,"For the feeds of catalog data, we want this information shared as widely as possible to allow people to easily buy music from us. However, we don’t want this abused, and we’d like some idea of who is ...",qwen2.5:latest,2025-11-02 23:58:08,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,A Worked Example,Implementation Details for Secure System,"#### Implementation Details for Secure System
Explanation of the final architecture design where catalog data is shared via an API with proper security measures, while customer data is protected through encryption and access controls.

:p What does the final secure system look like in terms of architecture?
??x
The final secure system involves sharing catalog data widely using APIs with API keys for tracking. For customer data, we encrypt it at rest and decrypt it on read to prevent unauthorized bulk data retrieval even if attackers penetrate our network.
x??

---",570,"For the feeds of catalog data, we want this information shared as widely as possible to allow people to easily buy music from us. However, we don’t want this abused, and we’d like some idea of who is ...",qwen2.5:latest,2025-11-02 23:58:08,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Adapting to Communication Pathways,Data Minimization (Datensparsamkeit),"#### Data Minimization (Datensparsamkeit)
Background context explaining the concept. In privacy legislation, particularly in Germany, ""Datensparsamkeit"" emphasizes storing only necessary data to fulfill business operations or satisfy local laws. This approach helps reduce risks of data breaches and unauthorized access.

The key idea is to scrub personally identifiable information as much as possible and store it for as short a period as required by law or business needs.
:p How can businesses implement the principle of ""Datensparsamkeit""?
??x
By implementing data minimization, businesses should only collect and retain personal data that is strictly necessary for their operations. For example:
- When logging user requests, store only the last few digits of an IP address instead of the full IP.
- Use age ranges and postcodes rather than exact date of birth or other identifying details when targeting product offers.

This reduces storage needs and potential risks.
x??",979,"Be Frugal As disk space becomes cheaper and the capabilities of the databases improve, the ease with which bulk amounts of information can be captured and stored is improving rapidly. This data is val...",qwen2.5:latest,2025-11-02 23:58:16,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Adapting to Communication Pathways,Human Element in Security,"#### Human Element in Security
Background context explaining the importance of human factors. Despite robust technological safeguards, security vulnerabilities often stem from human errors such as credential mismanagement, social engineering attacks, or insider threats.
:p What are some best practices to manage the human element in an organization’s security strategy?
??x
Best practices include:
- Revoking access credentials promptly when employees leave.
- Implementing strict policies against social engineering tactics.
- Conducting risk assessments from the perspective of a disgruntled ex-employee.

For example, create processes like automated revocation scripts and regular training programs to educate staff on security risks.
x??",742,"Be Frugal As disk space becomes cheaper and the capabilities of the databases improve, the ease with which bulk amounts of information can be captured and stored is improving rapidly. This data is val...",qwen2.5:latest,2025-11-02 23:58:16,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Adapting to Communication Pathways,Security Best Practices: Cryptography,"#### Security Best Practices: Cryptography
Background context explaining the importance of using well-established cryptographic tools. Writing custom encryption algorithms or protocols is risky due to potential vulnerabilities and lack of peer review.
:p Why should developers avoid writing their own crypto?
??x
Developers should avoid creating custom encryption solutions because:
- They are likely to introduce security flaws that professional cryptographers have already addressed in established algorithms like AES.
- Even experienced experts can make mistakes, making it safer to use battle-tested tools.

For instance, using AES for encrypting data is a reliable choice compared to rolling your own solution. Here’s an example of how to use AES in Java:
```java
import javax.crypto.Cipher;
import javax.crypto.spec.SecretKeySpec;

public class Example {
    private static final String ALGORITHM = ""AES"";
    
    public static void main(String[] args) throws Exception {
        String key = ""ThisIsASecretKey123""; // 16 bytes (128 bits)
        byte[] data = ""Sensitive Data"".getBytes(""UTF-8"");
        
        SecretKeySpec secretKey = new SecretKeySpec(key.getBytes(), ALGORITHM);
        Cipher cipher = Cipher.getInstance(ALGORITHM);
        cipher.init(Cipher.ENCRYPT_MODE, secretKey);
        
        byte[] encryptedData = cipher.doFinal(data);
        System.out.println(new String(encryptedData));
    }
}
```
x??

---",1438,"Be Frugal As disk space becomes cheaper and the capabilities of the databases improve, the ease with which bulk amounts of information can be captured and stored is improving rapidly. This data is val...",qwen2.5:latest,2025-11-02 23:58:16,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Adapting to Communication Pathways,Educating Developers on Security,"#### Educating Developers on Security
Background context explaining the importance of educating developers about security concerns. Raising awareness can help reduce potential vulnerabilities.
:p How does educating developers contribute to security?
??x
Educating developers helps raise their general awareness of security issues, reducing the likelihood of introducing vulnerabilities in code. Familiarizing them with resources like OWASP Top Ten list and Security Testing Framework is beneficial.

```java
// Example: Adding a comment for a developer to review security implications
public void processInput(String userInput) {
    // Ensure input validation and sanitization
    String safeInput = sanitizeInput(userInput);
    // Further processing of the sanitized input
}
```
x??",785,"Baking Security In Just as with automated functional testing, we don’t want security to be left to a different set of people, nor do we want to leave everything to the last minute. Helping educate dev...",qwen2.5:latest,2025-11-02 23:58:28,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Adapting to Communication Pathways,Using Automated Security Tools,"#### Using Automated Security Tools
Background context on using automated tools like ZAP for probing systems for vulnerabilities. Mentioning Brakeman as an example tool for Ruby projects.
:p What are some examples of automated security tools and how do they help?
??x
Tools like Zed Attack Proxy (ZAP) can probe your system for vulnerabilities, simulating malicious attacks to identify potential weaknesses. For Ruby projects, tools like Brakeman can be used for static analysis to find common coding mistakes that could lead to security holes.

```java
// Example of integrating a tool into CI using a script
public void setupSecurityChecks() {
    // Command-line example in Java pseudocode
    String command = ""zap-quickstart -config api.key=YOUR_API_KEY"";
    Process process = Runtime.getRuntime().exec(command);
    BufferedReader reader = new BufferedReader(new InputStreamReader(process.getInputStream()));
    String line;
    while ((line = reader.readLine()) != null) {
        System.out.println(line);
    }
}
```
x??",1031,"Baking Security In Just as with automated functional testing, we don’t want security to be left to a different set of people, nor do we want to leave everything to the last minute. Helping educate dev...",qwen2.5:latest,2025-11-02 23:58:28,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Adapting to Communication Pathways,Microsoft Security Development Lifecycle,"#### Microsoft Security Development Lifecycle
Background context on the lifecycle approach for integrating security into software development. Mentioning aspects of it that may feel waterfall-like but can be adapted.
:p What is the purpose of Microsoft's Security Development Lifecycle (SDL)?
??x
Microsoft’s SDL provides a framework to integrate security throughout the entire software development process, from planning and design through implementation, testing, and deployment. While some parts may feel overly waterfall in nature, it offers valuable models that can fit into various workflow stages.

```java
// Example of integrating SDL principles in Java code
public class SecureDevelopment {
    public void secureCodeReview() {
        // Code review process to identify security vulnerabilities
        String[] codeFiles = getProjectSourceFiles();
        for (String file : codeFiles) {
            File inputFile = new File(file);
            if (!reviewSecurityOf(inputFile)) {
                System.err.println(""Security issues found in "" + file);
            }
        }
    }

    private boolean reviewSecurityOf(File file) {
        // Logic to check for security issues
        return true; // Placeholder, actual logic would be more complex
    }
}
```
x??",1279,"Baking Security In Just as with automated functional testing, we don’t want security to be left to a different set of people, nor do we want to leave everything to the last minute. Helping educate dev...",qwen2.5:latest,2025-11-02 23:58:28,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Adapting to Communication Pathways,External Verification Through Penetration Testing,"#### External Verification Through Penetration Testing
Background context on the value of external assessments like penetration testing in validating system security. Mentioning the need for dedicated infosec teams or external parties.
:p Why is external verification important in security?
??x
External verification, such as penetration testing by an outside party, provides a realistic assessment of potential vulnerabilities that internal teams might overlook due to proximity bias. External experts can provide unbiased evaluations and help fill gaps in internal testing.

```java
// Example of scheduling a penetration test with an external team
public void schedulePenetrationTest() {
    // Pseudocode for communicating with an external security team
    String message = ""We would like to schedule a penetration test on [DATE]. Please let us know if this works for you."";
    sendEmail(""infosec@example.com"", message);
}
```
x??",936,"Baking Security In Just as with automated functional testing, we don’t want security to be left to a different set of people, nor do we want to leave everything to the last minute. Helping educate dev...",qwen2.5:latest,2025-11-02 23:58:28,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Adapting to Communication Pathways,Decomposing Systems into Microservices,"#### Decomposing Systems into Microservices
Background context on how decomposing systems into microservices can reduce the impact of security breaches and allow for more granular security controls. Mentioning considerations for different threat levels.
:p How does decomposition into microservices help with security?
??x
Decomposing a system into microservices allows for finer-grained control over data and security, potentially reducing the impact of a breach in any single service. It enables organizations to apply more complex and secure approaches where data is sensitive while using lighter-weight methods when risks are lower.

```java
// Example of securing different services based on threat levels
public class ServiceSecurity {
    public void configureSecurityLevel(String serviceName) {
        if (""HighRiskService"".equals(serviceName)) {
            // Apply strict security measures, e.g., encryption at rest and in transit
            enableEncryptionAtRest();
            enableTLS12ForCommunication();
        } else {
            // Lighter-weight security for less critical services
            enableBasicAuthentication();
        }
    }

    private void enableEncryptionAtRest() {
        // Implementation details for enabling encryption
    }

    private void enableTLS12ForCommunication() {
        // Implementation to ensure TLS 1.2 is used for communication
    }

    private void enableBasicAuthentication() {
        // Basic auth setup for a less critical service
    }
}
```
x??

---",1523,"Baking Security In Just as with automated functional testing, we don’t want security to be left to a different set of people, nor do we want to leave everything to the last minute. Helping educate dev...",qwen2.5:latest,2025-11-02 23:58:28,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Adapting to Communication Pathways,Defense in Depth,"#### Defense in Depth
Background context explaining the importance of defense in depth. This concept involves layering security controls to protect against cyber threats effectively. Patching operating systems is a fundamental part of this approach, as it addresses vulnerabilities that could be exploited by attackers.

:p What are the key aspects of defense in depth mentioned in the text?
??x
The key aspects include patching your operating systems and not implementing your own cryptography unless you have deep expertise in security. Additionally, resources like OWASP Top 10 Security Risk document should be consulted for general application security.
x??",661,"Finally, understand the importance of defense in depth, make sure you patch your operating systems, and even if you consider yourself a rock star, don’t try to implement your own cryptography. If you ...",qwen2.5:latest,2025-11-02 23:58:36,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Adapting to Communication Pathways,Open Web Application Security Project (OWASP),"#### Open Web Application Security Project (OWASP)
Background context explaining the importance of OWASP for web developers, highlighting its role in providing security guidelines and documentation.

:p What is OWASP and why is it important for developers?
??x
OWASP is a nonprofit organization that focuses on improving the security of software. It provides essential resources such as the Top 10 Security Risks document, which is crucial reading for any developer working with web applications.
x??",500,"Finally, understand the importance of defense in depth, make sure you patch your operating systems, and even if you consider yourself a rock star, don’t try to implement your own cryptography. If you ...",qwen2.5:latest,2025-11-02 23:58:36,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Adapting to Communication Pathways,"Cryptography Engineering by Niels Ferguson, Bruce Schneier, and Tadayoshi Kohno","#### Cryptography Engineering by Niels Ferguson, Bruce Schneier, and Tadayoshi Kohno
Background context explaining the book's focus on cryptography, its importance in security, and how it can help developers understand encryption better.

:p What resource does the text suggest for learning about cryptography?
??x
The text suggests reading ""Cryptography Engineering"" by Niels Ferguson, Bruce Schneier, and Tadayoshi Kohno. This book is recommended as a comprehensive guide to understanding cryptographic principles and their implementation.
x??",545,"Finally, understand the importance of defense in depth, make sure you patch your operating systems, and even if you consider yourself a rock star, don’t try to implement your own cryptography. If you ...",qwen2.5:latest,2025-11-02 23:58:36,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Adapting to Communication Pathways,Security's Human Element,"#### Security's Human Element
Background context explaining the importance of considering people in security design, emphasizing that ignoring human factors can lead to significant vulnerabilities.

:p Why is understanding people crucial for security?
??x
Understanding how people interact with and use systems is critical because human error or misconfiguration can introduce vulnerabilities. This includes awareness of organizational structures and communication patterns, as highlighted by Conway's Law.
x??",510,"Finally, understand the importance of defense in depth, make sure you patch your operating systems, and even if you consider yourself a rock star, don’t try to implement your own cryptography. If you ...",qwen2.5:latest,2025-11-02 23:58:36,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Adapting to Communication Pathways,Conway’s Law and System Design,"#### Conway’s Law and System Design
Background context explaining Conway’s Law, its implications for system design based on the organizational structure, and an example illustrating its application.

:p What is Conway’s Law and how does it impact system design?
??x
Conway’s Law states that a system's architecture will mirror the communication structures of the organization that created it. This can lead to designs where multiple teams working on different parts of a system result in a poorly integrated final product, such as a 4-pass compiler when four groups work independently.
x??",589,"Finally, understand the importance of defense in depth, make sure you patch your operating systems, and even if you consider yourself a rock star, don’t try to implement your own cryptography. If you ...",qwen2.5:latest,2025-11-02 23:58:36,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Adapting to Communication Pathways,Key Length and AES-256,"#### Key Length and AES-256
Background context explaining the relationship between key length and security strength, mentioning Bruce Schneier's concerns about AES-256.

:p How does key length affect security in encryption?
??x
Key length directly impacts the security of encrypted data. Longer keys make brute-force attacks more difficult because they require exponentially more computational effort. However, some experts like Bruce Schneier have raised concerns about specific implementations of AES-256, so it's important to stay updated with current advice.
x??

---",571,"Finally, understand the importance of defense in depth, make sure you patch your operating systems, and even if you consider yourself a rock star, don’t try to implement your own cryptography. If you ...",qwen2.5:latest,2025-11-02 23:58:36,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Adapting to Communication Pathways,Loose and Tightly Coupled Organizations,"#### Loose and Tightly Coupled Organizations
Background context: The authors Alan MacCormack, John Rusnak, and Carliss Baldwin explored how organizational structure impacts software systems. They categorized organizations as either loosely or tightly coupled based on their characteristics and goals.

:p What are the key differences between loosely and tightly coupled organizations according to this study?
??x
Loosely coupled organizations are typically distributed open source communities that create more modular, less coupled systems. Tightly coupled organizations, such as commercial product firms, are colocated with strongly aligned visions and goals, often resulting in less modularized software.
x??",710,"But you don’t have to take my word for it: since Conway’s original submission, a lot of work has been done in this area. A number of studies have been carried out to explore the interrelation of organ...",qwen2.5:latest,2025-11-02 23:58:43,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Adapting to Communication Pathways,Windows Vista Case Study,"#### Windows Vista Case Study
Background context: Microsoft conducted an empirical study on how its organizational structure affected the quality of a specific product, Windows Vista. The study examined multiple factors to determine error-proneness metrics.

:p What did Microsoft find regarding the impact of organizational structure on software quality?
??x
Microsoft found that measures associated with organizational structures were the most statistically relevant in determining the error-proneness of components in their systems.
x??",539,"But you don’t have to take my word for it: since Conway’s original submission, a lot of work has been done in this area. A number of studies have been carried out to explore the interrelation of organ...",qwen2.5:latest,2025-11-02 23:58:43,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Adapting to Communication Pathways,Two-Pizza Teams at Amazon,"#### Two-Pizza Teams at Amazon
Background context: Amazon implemented a policy called ""two-pizza teams"" to ensure small team sizes, allowing for faster work. This approach influenced the development of Amazon Web Services (AWS) and other internal tools.

:p What is the rationale behind Amazon's two-pizza team size?
??x
The rationale is that no team should be so large that it cannot be fed with two pizzas, promoting smaller, more manageable, and efficient teams.
x??",469,"But you don’t have to take my word for it: since Conway’s original submission, a lot of work has been done in this area. A number of studies have been carried out to explore the interrelation of organ...",qwen2.5:latest,2025-11-02 23:58:43,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Adapting to Communication Pathways,Netflix and Independent Teams,"#### Netflix and Independent Teams
Background context: Netflix adopted an organizational structure centered around small, independent teams to ensure the resulting system architecture was optimized for rapid change.

:p How did Netflix ensure its team structure aligned with its desired system architecture?
??x
Netflix structured itself around small, independent teams from the start, which led to services being designed and managed independently, optimizing for speed of change.
x??",485,"But you don’t have to take my word for it: since Conway’s original submission, a lot of work has been done in this area. A number of studies have been carried out to explore the interrelation of organ...",qwen2.5:latest,2025-11-02 23:58:43,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Adapting to Communication Pathways,Organizational Structure Influences System Quality,"#### Organizational Structure Influences System Quality
Background context: Various studies and real-world examples demonstrate that an organization's structure significantly affects the nature and quality of the systems it creates.

:p How does organizational structure impact the quality of a system according to the evidence provided?
??x
Organizational structure strongly influences the modularity, coupling, and error-proneness of the systems created. Loosely coupled organizations tend to produce more modular and less coupled systems compared to tightly coupled ones.
x??

---",583,"But you don’t have to take my word for it: since Conway’s original submission, a lot of work has been done in this area. A number of studies have been carried out to explore the interrelation of organ...",qwen2.5:latest,2025-11-02 23:58:43,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Adapting to Communication Pathways,Impact of Organizational Structure on System Design,"#### Impact of Organizational Structure on System Design

Organizations can significantly influence how services are designed and maintained, particularly regarding communication pathways. Understanding these impacts helps in designing systems that align with team dynamics.

:p How does a single, geolocated team affect service design compared to a geographically distributed one?
??x
A single, geolocated team allows for frequent, fine-grained communication, making it easier to implement changes and refactorings. They have a high pace of internal change due to close collaboration.

```java
// Example code showing tight integration in a colocated team
public class CatalogService {
    private void updateProduct(int productId) {
        // Update logic here
    }
}
```
x??",779,So how does this understanding help us? Let’s look at a few different organizational situations and understand what impact each might have on our system design. Adapting to Communication Pathways Let’...,qwen2.5:latest,2025-11-02 23:58:55,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Adapting to Communication Pathways,Coarse-Grained Communication in Distributed Teams,"#### Coarse-Grained Communication in Distributed Teams

In geographically distributed teams, fine-grained communication becomes more difficult due to geographical and time zone boundaries. This leads to a higher cost of coordination, often resulting in reduced changes or large, hard-to-maintain codebases.

:p How does the high cost of communication affect service ownership in distributed teams?
??x
High communication costs can lead to decreased frequency of changes, either through finding ways to reduce coordination overhead or by ceasing changes altogether. This results in less frequent updates and potentially larger, harder-to-maintain codebases. For example:

```java
// Example showing reduced integration due to high communication costs
public class CatalogServiceUK {
    private void updateProduct(int productId) {
        // Update logic here
    }
}

public class CatalogServiceIndia {
    private void updateProduct(int productId) {
        // Update logic here
    }
}
```
x??",995,So how does this understanding help us? Let’s look at a few different organizational situations and understand what impact each might have on our system design. Adapting to Communication Pathways Let’...,qwen2.5:latest,2025-11-02 23:58:55,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Adapting to Communication Pathways,Ownership and Service Decomposition,"#### Ownership and Service Decomposition

Ownership of services by colocated teams can facilitate easier maintenance and change due to frequent communication. Conversely, distributed ownership requires more coarse-grained communication and potentially specialized ownership within the service.

:p How does shared ownership across multiple locations affect a single service?
??x
Shared ownership in geographically distributed teams often leads to specialization within the team for different parts of the service. This reduces the overall coordination cost by allowing each team to focus on specific areas, making changes more manageable and reducing the need for frequent communication.

```java
// Example showing specialized ownership in a distributed environment
public class CatalogService {
    private static final UKTeam ukTeam = new UKTeam();
    private static final IndiaTeam indiaTeam = new IndiaTeam();

    public void updateProduct(int productId) {
        if (ukTeam.shouldHandle(productId)) {
            ukTeam.updateProduct(productId);
        } else if (indiaTeam.shouldHandle(productId)) {
            indiaTeam.updateProduct(productId);
        }
    }
}

class UKTeam {
    boolean shouldHandle(int productId) {
        // Logic to determine if this team handles the product
    }

    void updateProduct(int productId) {
        // Update logic here
    }
}
```
x??",1389,So how does this understanding help us? Let’s look at a few different organizational situations and understand what impact each might have on our system design. Adapting to Communication Pathways Let’...,qwen2.5:latest,2025-11-02 23:58:55,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Adapting to Communication Pathways,Service Design Based on Organizational Structure,"#### Service Design Based on Organizational Structure

The structure of an organization can drive service decomposition and ownership. Geographically distributed teams may lead to a more modular system design due to the necessity for coarse-grained communication.

:p How does a more loosely coupled organizational architecture influence service design?
??x
A more loosely coupled organizational architecture tends to result in more modular systems because it forces services to be designed with fewer tight integrations, reducing the need for frequent and fine-grained communications between different geographical locations.

```java
// Example of a modular system driven by distributed team structure
public interface CatalogService {
    void updateProduct(int productId);
}

class UKCatalogService implements CatalogService {
    // Implementation details specific to UK
}

class IndiaCatalogService implements CatalogService {
    // Implementation details specific to India
}
```
x??",990,So how does this understanding help us? Let’s look at a few different organizational situations and understand what impact each might have on our system design. Adapting to Communication Pathways Let’...,qwen2.5:latest,2025-11-02 23:58:55,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Adapting to Communication Pathways,Deciding on Service Decomposition,"#### Deciding on Service Decomposition

When an organization expands geographically, the decision to open a new office can drive the need for further service decomposition and ownership. This process helps in managing change more effectively by reducing coordination costs.

:p How should you approach expanding services when opening a new office?
??x
When opening a new office, consider which parts of your system can be moved over first. This decision can drive the next steps in decomposing services into more manageable components that each team can own and maintain with lower communication overhead.

```java
// Example of considering expansion impact on service design
public class ServiceDecompositionPlan {
    public void planExpansion() {
        if (canMoveServiceToNewOffice(""Catalog"")) {
            // Move Catalog service to new office
            // Define UK and India teams for Catalog
        }
        // Continue planning other services
    }

    private boolean canMoveServiceToNewOffice(String serviceName) {
        // Logic to determine if a service can be moved
    }
}
```
x??

---",1110,So how does this understanding help us? Let’s look at a few different organizational situations and understand what impact each might have on our system design. Adapting to Communication Pathways Let’...,qwen2.5:latest,2025-11-02 23:58:55,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Case Study RealEstate.com.au,Service Ownership,"---
#### Service Ownership
Service ownership means that the team responsible for a service is fully accountable and has full control over it. This includes making changes, restructuring the code, building, deploying, and maintaining the application. Microservices are particularly suited to this model due to their small size and ease of management by a dedicated team.
:p What does service ownership imply in the context of microservices?
??x
Service ownership implies that the team responsible for a microservice can make changes to its codebase without worrying about affecting other teams, as long as those changes do not break any consuming services. This model promotes autonomy and rapid delivery by allowing the team to focus on improving their specific service.
```java
public class ServiceOwnerTeam {
    public void updateServiceCode() {
        // Code to make necessary modifications to the microservice's codebase
        System.out.println(""Updating microservice with new features or optimizations."");
    }
}
```
x??",1032,"Service Ownership What do I mean by  service ownership ? In general, it means that the team owning a service is responsible for making changes to that service. The team should feel free to restructure...",qwen2.5:latest,2025-11-02 23:59:04,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Case Study RealEstate.com.au,Shared Services Ownership,"#### Shared Services Ownership
Shared services ownership involves multiple teams working together on a single service. This approach is less favored due to potential coordination issues and lack of accountability, but it may be necessary in certain scenarios.
:p Why is shared service ownership considered suboptimal?
??x
Shared service ownership can lead to inefficiencies because multiple teams are involved in maintaining and developing the same service. It often results in unclear responsibilities, making it difficult to determine which team is accountable for specific issues or changes. Additionally, coordination between teams can be challenging, leading to delays and conflicts.
```java
public class SharedServiceTeamA {
    public void modifySharedService() {
        // Code that might conflict with modifications from Team B
        System.out.println(""Modifying shared service code; potential conflicts possible."");
    }
}
```
x??",945,"Service Ownership What do I mean by  service ownership ? In general, it means that the team owning a service is responsible for making changes to that service. The team should feel free to restructure...",qwen2.5:latest,2025-11-02 23:59:04,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Case Study RealEstate.com.au,Too Hard to Split,"#### Too Hard to Split
The challenge of splitting a large monolithic system into smaller services can be too costly. This is particularly common in organizations where the existing system is so intertwined that breaking it apart would require significant effort and risk.
:p What are the challenges associated with splitting a monolithic system?
??x
Splitting a monolithic system into microservices requires substantial work, including refactoring code, redesigning architecture, and potentially rewriting parts of the application. The cost can be high due to the complexity involved in ensuring that the new services integrate seamlessly with existing systems. Additionally, there is always the risk that breaking up the system might introduce new issues or reduce performance.
```java
public class MonolithRefactoring {
    public void refactorToMicroservices() {
        // Code representing the significant effort required to split a monolithic application
        System.out.println(""Refactoring monolithic app into microservices; complex and costly."");
    }
}
```
x??",1074,"Service Ownership What do I mean by  service ownership ? In general, it means that the team owning a service is responsible for making changes to that service. The team should feel free to restructure...",qwen2.5:latest,2025-11-02 23:59:04,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Case Study RealEstate.com.au,Feature Teams,"#### Feature Teams
Feature teams are small groups that work on implementing specific features, often crossing traditional component or service boundaries. This approach aims to keep the team focused on delivering end-to-end functionality.
:p What is the purpose of feature teams?
??x
The purpose of feature teams is to ensure that development remains aligned with business goals by focusing on complete features rather than individual components. It helps in avoiding siloed work and encourages cross-functional collaboration, making it easier to deliver cohesive functionality.
```java
public class FeatureTeam {
    public void implementFeature() {
        // Code representing a team implementing an end-to-end feature
        System.out.println(""Implementing feature by collaborating across UI, logic, and database layers."");
    }
}
```
x??

---",850,"Service Ownership What do I mean by  service ownership ? In general, it means that the team owning a service is responsible for making changes to that service. The team should feel free to restructure...",qwen2.5:latest,2025-11-02 23:59:04,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Case Study RealEstate.com.au,Microservices and Service Custodianship,"#### Microservices and Service Custodianship
Background context: The text discusses microservices, specifically focusing on how business-aligned teams can better retain a customer focus. It contrasts this with traditional service custodianship roles that may become complex if every team can change any piece of code.
:p How does aligning the ownership of services along business domains help in retaining a customer focus?
??x
Aligning the ownership of services along business domains helps teams have a holistic understanding and ownership of all technology associated with a service, making it more likely that they will retain a customer focus. This alignment ensures that each team is responsible for a specific domain, leading to better feature development through by seeing their changes in context.
??x
The answer explains the importance of business-aligned teams for maintaining a customer focus.",905,"Everyone can change every service, every piece of code. The role of the service custodians here becomes much more complex, if the role exists at all. Unfortunately, I rarely see functioning custodians...",qwen2.5:latest,2025-11-02 23:59:12,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Case Study RealEstate.com.au,Delivery Bottlenecks and Service Ownership,"#### Delivery Bottlenecks and Service Ownership
Background context: The text discusses potential delivery bottlenecks when multiple services need changes. It provides scenarios where specific services are backlogged, leading to delays or additional resource allocation.
:p What is a scenario described in the text that highlights delivery bottlenecks?
??x
A scenario described is rolling out features like displaying genre information on the website and adding virtual musical ringtones for mobile phones. Both changes require modifications to the catalog service, but with half of the team unavailable due to illness or other issues, there's a risk of delays.
??x
The answer outlines the situation where multiple services need changes simultaneously, leading to potential bottlenecks.",785,"Everyone can change every service, every piece of code. The role of the service custodians here becomes much more complex, if the role exists at all. Unfortunately, I rarely see functioning custodians...",qwen2.5:latest,2025-11-02 23:59:12,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Case Study RealEstate.com.au,Service Splitting Strategies,"#### Service Splitting Strategies
Background context: The text suggests splitting large services into smaller ones when facing significant backlog. It provides examples of how this can be done based on feature requirements and future development likelihood.
:p How might a service like catalog be split according to the text?
??x
The catalog service could be split into a general music catalog and a ringtone catalog if there is a large backlog related specifically to ringtones, and the team owning these services (the mobile app team) can take ownership of the new ringtones.
??x
The answer describes how splitting a large service based on specific feature requirements can help manage bottlenecks.",700,"Everyone can change every service, every piece of code. The role of the service custodians here becomes much more complex, if the role exists at all. Unfortunately, I rarely see functioning custodians...",qwen2.5:latest,2025-11-02 23:59:12,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Case Study RealEstate.com.au,Internal Open Source Model,"#### Internal Open Source Model
Background context: The text suggests that if shared services cannot be avoided, adopting an internal open source model might make sense. This allows for broader participation and collaboration without the overhead of formal committer roles.
:p What is the suggestion when it's challenging to avoid a few shared services?
??x
The suggestion is to properly embrace the internal open source model, which can involve multiple teams contributing to the same service without the need for strict committer roles, promoting broader participation and collaboration.
??x
The answer explains how internal open source can help manage challenges with shared services by encouraging more flexible contributions.

---",735,"Everyone can change every service, every piece of code. The role of the service custodians here becomes much more complex, if the role exists at all. Unfortunately, I rarely see functioning custodians...",qwen2.5:latest,2025-11-02 23:59:12,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Case Study RealEstate.com.au,Open Source Project Management within Organizations,"#### Open Source Project Management within Organizations

Background context: This section discusses how managing open source projects can be adapted for internal use within an organization. It highlights the role of core committers and untrusted contributors, emphasizing the importance of code quality, consistency, and future compatibility.

:p What are the roles of core committers in an internal open source project?
??x
Core committers play a crucial role as custodians of the codebase. They are responsible for reviewing, approving, and integrating changes submitted by untrusted contributors. Core committers ensure that the code adheres to coding guidelines and maintain consistency across the entire codebase. They also need to assess whether changes will make future development easier or harder.
x??",811,"They are the custodians of the code. If you want a change to an open source project, you either ask one of the committers to make the change for you, or else you make the change yourself and send them...",qwen2.5:latest,2025-11-02 23:59:24,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Case Study RealEstate.com.au,Vetting Changes in Internal Open Source Projects,"#### Vetting Changes in Internal Open Source Projects

Background context: The text explains how core committers must vet and approve changes submitted by untrusted contributors to ensure high-quality code and consistency with existing standards.

:p How do core committers typically vet changes from untrusted contributors?
??x
Core committers review the proposed changes for quality, ensuring they align with the coding guidelines of the project. They may work closely with submitters to improve the changes if necessary. The process involves clear communication and sometimes inline comments on pull requests or similar tools.

```java
// Example: A core committer might provide feedback on a commit message.
public class ReviewExample {
    public void reviewCommitMessage(String message) {
        // Check for adherence to guidelines, such as proper formatting and clarity.
        if (message.startsWith(""Fixes #"")) {
            System.out.println(""Commit message is well-formatted."");
        } else {
            System.out.println(""Please use the 'Fixes #' format for commit messages."");
        }
    }
}
```
x??",1124,"They are the custodians of the code. If you want a change to an open source project, you either ask one of the committers to make the change for you, or else you make the change yourself and send them...",qwen2.5:latest,2025-11-02 23:59:24,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Case Study RealEstate.com.au,Core Team vs. Untrusted Contributors,"#### Core Team vs. Untrusted Contributors

Background context: This part differentiates between core team members (trusted committers) and untrusted contributors, emphasizing the need for clear roles and responsibilities to maintain code quality.

:p Who are core team members in an internal open source project?
??x
Core team members are trusted individuals who have commit rights and can directly contribute to and manage changes within the project. They are responsible for ensuring that the codebase remains of high quality, follows coding guidelines, and maintains consistency with existing standards.
x??",610,"They are the custodians of the code. If you want a change to an open source project, you either ask one of the committers to make the change for you, or else you make the change yourself and send them...",qwen2.5:latest,2025-11-02 23:59:24,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Case Study RealEstate.com.au,Evaluating the Value of Gatekeeping,"#### Evaluating the Value of Gatekeeping

Background context: The text discusses whether it is worthwhile for core team members to act as gatekeepers by reviewing pull requests from untrusted contributors.

:p What factors should be considered when deciding if allowing untrusted committers to submit patches is worth the effort?
??x
Several factors should be evaluated, including:
- Whether the core team could spend that time more effectively on other tasks.
- The potential benefits of having a broader community contribute and the possible drawbacks of increased complexity in managing contributions.

```java
// Example: A decision-making process to evaluate if gatekeeping is worth it.
public class GatekeepingDecision {
    public boolean shouldAllowPatches(int coreTeamTime, int estimatedPatchVettingTime) {
        // Assume 20% more time spent on vetting is acceptable for community contribution.
        return (estimatedPatchVettingTime * 1.2) <= coreTeamTime;
    }
}
```
x??",988,"They are the custodians of the code. If you want a change to an open source project, you either ask one of the committers to make the change for you, or else you make the change yourself and send them...",qwen2.5:latest,2025-11-02 23:59:24,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Case Study RealEstate.com.au,Maturity of a Service,"#### Maturity of a Service

Background context: The text highlights the importance of considering the maturity level of a service when deciding to allow untrusted contributors.

:p How does the maturity of a service influence decisions about allowing external contributions?
??x
The maturity of a service is critical. Before key components are stable, it may be challenging to determine what constitutes good quality work. Allowing external contributions too early can lead to inconsistent code or poor integration with existing functionality. It's generally recommended to restrict contributions until the core features are well-established and the project has defined clear coding standards.
x??",697,"They are the custodians of the code. If you want a change to an open source project, you either ask one of the committers to make the change for you, or else you make the change yourself and send them...",qwen2.5:latest,2025-11-02 23:59:24,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Case Study RealEstate.com.au,Tooling for Internal Open Source Projects,"#### Tooling for Internal Open Source Projects

Background context: The text discusses the importance of having appropriate tooling in place, such as distributed version control systems and pull request management tools.

:p What kind of tooling is essential for supporting internal open source projects?
??x
Essential tooling includes:
- Distributed version control systems (e.g., Git) that support branching and merging.
- Pull request management systems that allow untrusted contributors to submit changes, with the core team reviewing them.
- Code review tools or platforms where inline comments can be added to patches.

```java
// Example: A basic structure for a pull request handling system.
public class PullRequestHandler {
    public void handlePullRequest(String repositoryUrl, String PRId) {
        // Retrieve and process the pull request from the given repository URL and PR ID.
        System.out.println(""Handling pull request "" + PRId + "" from "" + repositoryUrl);
    }
}
```
x??

---",1003,"They are the custodians of the code. If you want a change to an open source project, you either ask one of the committers to make the change for you, or else you make the change yourself and send them...",qwen2.5:latest,2025-11-02 23:59:24,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Case Study RealEstate.com.au,Bounded Contexts and Team Structures,"#### Bounded Contexts and Team Structures

Background context explaining the concept. This involves organizing teams based on bounded contexts, which are areas of a business domain with distinct boundaries. This approach simplifies understanding and interaction within a specific area, making it easier to manage services that interact frequently.

This organizational structure helps in aligning team roles and responsibilities more closely with business domains, improving communication and the likelihood of successful project outcomes.

:p How do bounded contexts influence team structures?
??x
Bounded contexts are used to define distinct areas of responsibility within a business domain. Teams are then aligned along these contexts, which facilitates easier understanding and interaction among services that belong to the same context. This approach enhances the team's ability to manage changes and interact with domain experts effectively.

For example, if an organization has a bounded context for ""Consumer Web Sales,"" it might include teams responsible for website, cart, and recommendation services. Even if one of these services (like the cart) is not frequently changed, the team aligned with this context would still be its de facto owner.

```java
public class BoundedContextExample {
    public void alignTeamsToBoundedContexts() {
        Team consumerWebSalesTeam = new Team();
        consumerWebSalesTeam.addService(new WebsiteService());
        consumerWebSalesTeam.addService(new CartService());
        consumerWebSalesTeam.addService(new RecommendationService());
        
        // The team would be responsible for maintaining and evolving these services
    }
}
```
x??",1699,"Finally, you’ll need to make it very easy for a committer to build and deploy your software, and make it available for others. Typically this involves having well- defined build and deployment pipelin...",qwen2.5:latest,2025-11-02 23:59:35,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Case Study RealEstate.com.au,Orphaned Services,"#### Orphaned Services

Background context explaining the concept. Smaller, simpler microservices are easier to maintain and less likely to change frequently. However, even when a service is not actively maintained, it still needs an owner who can manage changes or updates as needed.

In an organization with well-defined bounded contexts, teams aligned with those contexts naturally become responsible for any services within their scope, including those that may be static for extended periods.

:p How do orphaned services fit into the context of microservices and team structures?
??x
Orphaned services are those microservices that have not been actively maintained or modified for a significant period. Even though these services might not require frequent changes, they still need an owner who can make necessary updates when required. In a well-structured organization with bounded contexts, teams aligned with specific business domains naturally become responsible for managing any associated services, including those that are static.

For instance, if the ""Consumer Web Sales"" context includes a cart service that hasn't been changed in months, it would still fall to the team aligned with this context to handle any necessary updates or changes.

```java
public class OrphanServiceHandling {
    public void assignTeamToOrphanedService() {
        Team consumerWebSalesTeam = getConsumerWebSalesTeam();
        
        if (isCartServiceOrphaned()) {
            consumerWebSalesTeam.takeOwnershipOf(cartService);
        }
    }
}
```
x??",1551,"Finally, you’ll need to make it very easy for a committer to build and deploy your software, and make it available for others. Typically this involves having well- defined build and deployment pipelin...",qwen2.5:latest,2025-11-02 23:59:35,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Case Study RealEstate.com.au,Case Study: RealEstate.com.au,"#### Case Study: RealEstate.com.au

Background context explaining the concept. The case study provides an example of how a company like REA organizes its IT delivery teams around different lines of business (LOBs) to manage various facets of its core business, such as residential property in Australia and commercial properties.

Each LOB has its own team or squad responsible for specific services related to that area. This structure ensures focused development and maintenance efforts tailored to the needs of each business line.

:p How does RealEstate.com.au organize its IT delivery teams?
??x
RealEstate.com.au organizes its IT delivery teams around different lines of business (LOBs) corresponding to various facets of its core business, such as residential property in Australia and commercial properties. Each LOB has an associated team or squad that focuses on creating and maintaining the necessary services for that particular area.

For example, one team might be responsible for developing and maintaining websites, listing services, and recommendations related to residential property sales in Australia. This alignment ensures that teams are well-equipped to manage their specific areas of responsibility.

```java
public class RealEstateTeamOrganization {
    public void organizeTeamsByLOBs() {
        Team residentialPropertyTeam = new Team();
        Team commercialPropertyTeam = new Team();
        
        // Add relevant services for each team
        residentialPropertyTeam.addService(new ResidentialWebsiteService());
        residentialPropertyTeam.addService(new ResidentialListingService());
        
        commercialPropertyTeam.addService(new CommercialWebsiteService());
        commercialPropertyTeam.addService(new CommercialListingService());
    }
}
```
x??",1800,"Finally, you’ll need to make it very easy for a committer to build and deploy your software, and make it available for others. Typically this involves having well- defined build and deployment pipelin...",qwen2.5:latest,2025-11-02 23:59:35,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Case Study RealEstate.com.au,Team Rotation and Domain Awareness,"#### Team Rotation and Domain Awareness

People rotate between teams but tend to stay within their line of business (LOB) for extended periods. This helps build domain-specific expertise among team members, enhancing communication with stakeholders.

:p How does rotating people while staying within a line of business affect team dynamics?

??x
Rotating people ensures fresh perspectives on projects while maintaining deep knowledge in specific domains. It helps maintain strong awareness and builds trust between teams and their respective stakeholders. Team members can share insights and best practices, but the extended stay within one LOB fosters domain expertise which is crucial for effective communication.

```java
public class TeamRotation {
    private String lineOfBusiness;
    private int rotationPeriod; // in months

    public TeamRotation(String lineOfBusiness, int rotationPeriod) {
        this.lineOfBusiness = lineOfBusiness;
        this.rotationPeriod = rotationPeriod;
    }

    public void rotateTeamMembers() {
        // Logic to rotate team members within the same LOB
        System.out.println(""Rotating team members in "" + lineOfBusiness);
    }
}
```
x??",1189,"People rotate between these teams every now and then, but  tend to stay within that line of business for extended periods, ensuring that the team members can build up a strong awareness of that part o...",qwen2.5:latest,2025-11-02 23:59:52,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Case Study RealEstate.com.au,Full Lifecycle Ownership,"#### Full Lifecycle Ownership

Each squad inside a line of business is responsible for the entire lifecycle of services, including building, testing, releasing, supporting, and decommissioning.

:p What are the responsibilities of each squad within a line of business?

??x
Each squad owns the full lifecycle of their services. This includes:
- Building: Developing new features or services.
- Testing: Ensuring that developed services meet quality standards.
- Releasing: Deploying services to production environments.
- Supporting: Providing ongoing maintenance and troubleshooting support.
- Decommissioning: Planning and executing the removal of outdated services.

```java
public class SquadResponsibilities {
    private String serviceName;
    private boolean building;
    private boolean testing;
    private boolean releasing;
    private boolean supporting;
    private boolean decommissioning;

    public SquadResponsibilities(String serviceName) {
        this.serviceName = serviceName;
        this.building = true; // Default state
        this.testing = false;
        this.releasing = false;
        this.supporting = false;
        this.decommissioning = false;
    }

    public void startTesting() {
        testing = true;
    }

    public void stopBuilding() {
        building = false;
    }
}
```
x??",1327,"People rotate between these teams every now and then, but  tend to stay within that line of business for extended periods, ensuring that the team members can build up a strong awareness of that part o...",qwen2.5:latest,2025-11-02 23:59:52,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Case Study RealEstate.com.au,Advice and Guidance from Core Delivery Services Team,"#### Advice and Guidance from Core Delivery Services Team

A core delivery services team provides advice, guidance, and tooling to squads.

:p What does the core delivery services team provide to squads?

??x
The core delivery services team offers:
- Advice on best practices.
- Guidance on architecture and technology decisions.
- Tooling to support development processes (e.g., CI/CD pipelines).

```java
public class CoreDeliveryServices {
    private String advice;
    private String guidance;
    private String tooling;

    public void provideAdvice(String advice) {
        this.advice = advice;
    }

    public void provideGuidance(String guidance) {
        this.guidance = guidance;
    }

    public void provideTooling(String tooling) {
        this.tooling = tooling;
    }
}
```
x??",800,"People rotate between these teams every now and then, but  tend to stay within that line of business for extended periods, ensuring that the team members can build up a strong awareness of that part o...",qwen2.5:latest,2025-11-02 23:59:52,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Case Study RealEstate.com.au,Autonomous Teams with Automation,"#### Autonomous Teams with Automation

REA emphasizes automation and uses AWS to enable autonomous teams.

:p How does REA achieve autonomy for its development teams?

??x
REA achieves autonomy by:
- Encouraging a strong culture of automation.
- Utilizing AWS services to provide tools and infrastructure that support self-service and rapid deployment.
- Allowing teams to manage their services independently, with minimal intervention from central IT.

```java
public class AutonomousTeam {
    private String serviceName;
    private boolean isAutomated;
    private String awsServiceUsed;

    public AutonomousTeam(String serviceName) {
        this.serviceName = serviceName;
        this.isAutomated = true; // Default state
        this.awsServiceUsed = ""S3""; // Example service
    }

    public void useAWSForDeployment(String service) {
        awsServiceUsed = service;
    }
}
```
x??",896,"People rotate between these teams every now and then, but  tend to stay within that line of business for extended periods, ensuring that the team members can build up a strong awareness of that part o...",qwen2.5:latest,2025-11-02 23:59:52,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Case Study RealEstate.com.au,Integrated Communication Methods,"#### Integrated Communication Methods

REA enforces asynchronous batch communication between LOBs, while services within the same LOB can communicate freely.

:p How does REA manage integration methods between and within LOBs?

??x
Between LOBs, REA mandates:
- Asynchronous batch communication.
- This coarse-grained communication aligns with how different parts of the business operate.

Within a single LOB, services are free to communicate in any way they decide.

```java
public class CommunicationMethods {
    private String intraLOBCommunication;
    private String interLOBCommunication;

    public CommunicationMethods(String intraLOBCommunication, String interLOBCommunication) {
        this.intraLOBCommunication = intraLOBCommunication; // Example: REST API
        this.interLOBCommunication = interLOBCommunication; // Example: SNS/SQS
    }

    public void setInterLOBCommunication(String method) {
        interLOBCommunication = method;
    }
}
```
x??",973,"People rotate between these teams every now and then, but  tend to stay within that line of business for extended periods, ensuring that the team members can build up a strong awareness of that part o...",qwen2.5:latest,2025-11-02 23:59:52,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Case Study RealEstate.com.au,Autonomous Business Units,"#### Autonomous Business Units

The structure of REA allows for significant autonomy in both teams and business units.

:p How does the autonomous structure of REA benefit its operations?

??x
Autonomy at REA:
- Enables rapid decision-making.
- Allows services to be taken down or maintained without impacting others.
- Fosters innovation and adaptability.
- Helps in achieving faster time-to-market for new features and functionality.

```java
public class AutonomousBusinessUnit {
    private String service;
    private boolean canTakeDownService;

    public AutonomousBusinessUnit(String service) {
        this.service = service;
        this.canTakeDownService = true; // Default state
    }

    public void takeDownService() {
        if (canTakeDownService) {
            System.out.println(service + "" has been taken down."");
        } else {
            System.out.println(""Cannot take down "" + service);
        }
    }
}
```
x??",942,"People rotate between these teams every now and then, but  tend to stay within that line of business for extended periods, ensuring that the team members can build up a strong awareness of that part o...",qwen2.5:latest,2025-11-02 23:59:52,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Case Study RealEstate.com.au,Scalable Service Growth,"#### Scalable Service Growth

REA's growth from a few services to hundreds of services indicates the effectiveness of their structure.

:p What does REA's growth indicate about their organizational and architectural strategies?

??x
REA’s growth:
- From a handful of services to hundreds, with more services than people.
- Rapid growth driven by efficient development and deployment processes.
- Scalability enabled by well-defined architecture and autonomous teams.
- This growth supports the company’s expansion into new markets.

```java
public class ServiceGrowth {
    private int initialServices;
    private int currentServices;

    public ServiceGrowth(int initialServices, int currentServices) {
        this.initialServices = initialServices;
        this.currentServices = currentServices;
    }

    public void showGrowth() {
        System.out.println(""From "" + initialServices + "" to "" + currentServices + "" services."");
    }
}
```
x??",952,"People rotate between these teams every now and then, but  tend to stay within that line of business for extended periods, ensuring that the team members can build up a strong awareness of that part o...",qwen2.5:latest,2025-11-02 23:59:52,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Case Study RealEstate.com.au,Iterative Organizational Change,"#### Iterative Organizational Change

REA views its architecture and organizational structure as evolving, with continuous improvement.

:p How does REA’s view on change impact their development process?

??x
REA's approach:
- Architecture and organizational structure are not static but continually evolving.
- They embrace adaptability to stay competitive.
- Continuous iteration ensures they can respond to market changes and maintain efficiency.

```java
public class IterativeChange {
    private boolean isArchitectureChanging;
    private boolean isOrganizationChanging;

    public IterativeChange() {
        this.isArchitectureChanging = true;
        this.isOrganizationChanging = true;
    }

    public void showStatus() {
        System.out.println(""Architecture: "" + (isArchitectureChanging ? ""Changing"" : ""Stable""));
        System.out.println(""Organization: "" + (isOrganizationChanging ? ""Changing"" : ""Stable""));
    }
}
```
x??

---",950,"People rotate between these teams every now and then, but  tend to stay within that line of business for extended periods, ensuring that the team members can build up a strong awareness of that part o...",qwen2.5:latest,2025-11-02 23:59:52,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,How Much Is Too Much,Conway’s Law in Reverse,"#### Conway’s Law in Reverse
Conway’s law states that an organization's communication structure will influence its software architecture. The provided text suggests that a system design can also shape organizational structures, especially when the business context changes.

Background: The company was initially focused on print operations with a small online presence. Over time, the digital side of the business grew significantly while the physical print business declined. Despite this shift in focus, the original three-part system (input, core, output) persisted, leading to an organizational structure that aligned with these parts.

:p Can you explain how Conway’s Law in reverse can impact a company's organization?
??x
Conway’s Law in reverse implies that as a system evolves and changes over time, so too will the organizational structure that supports it. In the example provided, the original design of the three-part system (input, core, output) influenced the creation of an IT organizational structure with corresponding channels or divisions for each part. This structure persisted even as the company’s primary business operations shifted from print to digital. As a result, changing this organizational structure to better align with current business needs required rethinking both the system and its governance.

x??",1337,"Conway’s Law in Reverse So far, we’ve spoken about how the organization impacts the system design. But what about the reverse? Namely, can a system design change the organization? While I haven’t been...",qwen2.5:latest,2025-11-03 00:00:02,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,How Much Is Too Much,Microservices and Developer Awareness,"#### Microservices and Developer Awareness
In a microservice environment, developers must consider more operational concerns such as network calls and service failures compared to monolithic systems where these issues are often abstracted away. This shift can be challenging for developers accustomed to simpler coding environments.

Background: Moving from a monolithic system to microservices requires developers to think about the broader implications of their code, particularly in terms of network boundaries and potential failure points. This is due to the distributed nature of microservices where components are loosely coupled and communicate via well-defined APIs or protocols.

:p How does moving from a monolithic system to microservices impact developer responsibilities?
??x
Moving from a monolithic system to microservices significantly expands the scope of developers' responsibilities. In a monolithic architecture, developers primarily focus on coding within their modules without needing deep understanding of other parts of the application and its deployment environments. However, in a microservice environment, developers must consider network calls across boundaries, handle failures robustly, and manage data consistency issues.

This transition necessitates that developers are more aware of operational concerns such as fault tolerance, security, and performance. They need to design services that can communicate effectively over the network while maintaining high availability and resilience.

x??",1525,"Conway’s Law in Reverse So far, we’ve spoken about how the organization impacts the system design. But what about the reverse? Namely, can a system design change the organization? While I haven’t been...",qwen2.5:latest,2025-11-03 00:00:02,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,How Much Is Too Much,Organizational Structure in a Microservice Environment,"#### Organizational Structure in a Microservice Environment
Organizations using microservices might face challenges if their development teams have not been trained or accustomed to working within this environment. This shift can be particularly difficult for developers who were previously used to simpler coding paradigms in monolithic systems.

Background: The transition from monolithic architecture to microservices requires developers to adapt to new ways of thinking about software design, deployment, and maintenance. Developers must understand the implications of network calls and service failures more deeply than they would in a monolithic setup where such concerns are often abstracted away by the underlying infrastructure.

:p What challenges might arise when transitioning from monolithic systems to microservices?
??x
Transitioning from monolithic systems to microservices can present several challenges, primarily due to the shift in how developers think about their work. Developers accustomed to monolithic architectures may struggle with the increased complexity and operational concerns associated with microservices.

Key challenges include:
- **Understanding Network Boundaries:** Developers must write code that interacts effectively across network boundaries.
- **Handling Failures:** Robust error handling is critical, as individual services might fail independently of others.
- **Data Management:** Managing data consistency in a distributed system can be complex and requires careful design.

These changes require developers to develop new skills and potentially undergo training or reorientation to adapt to the microservices environment.

x??

---",1680,"Conway’s Law in Reverse So far, we’ve spoken about how the organization impacts the system design. But what about the reverse? Namely, can a system design change the organization? While I haven’t been...",qwen2.5:latest,2025-11-03 00:00:02,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,How Much Is Too Much,Pushing Power into Development Teams,"#### Pushing Power into Development Teams
Background context: The passage discusses the challenges of granting more autonomy to development teams, especially when transitioning from a traditional to a microservices architecture. It highlights that developers accustomed to blaming others may struggle with full accountability.

:p What are some challenges associated with pushing power and increased autonomy into development teams?
??x
The key challenges include:
1. Developers who have traditionally thrown work ""over the wall"" might feel uncomfortable being fully accountable for their work.
2. There can be contractual barriers preventing developers from carrying support pagers for systems they support.
3. It's essential to consider the current staff's appetite for change and avoid overwhelming them too quickly.

Understanding these challenges is crucial before making significant changes, as it helps in crafting a more effective transition plan.

x??",960,"Likewise, pushing power into development teams to increase autonomy can be fraught. People who have in the past thrown work over the wall to someone else are accustomed to having someone else to blame...",qwen2.5:latest,2025-11-03 00:00:11,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,How Much Is Too Much,Aligning Service Ownership with Colocated Teams,"#### Aligning Service Ownership with Colocated Teams
Background context: The text emphasizes aligning service ownership with colocated teams to ensure that the system design matches organizational structure. This alignment helps in reducing tension points and ensures smoother operations.

:p How does aligning service ownership with colocated teams help reduce tensions?
??x
Aligning service ownership with colocated teams can significantly reduce tensions by:
1. Ensuring that team members are responsible for services related to their bounded contexts.
2. Facilitating better communication and coordination among team members.
3. Aligning individual responsibilities with the overall organizational goals.

This approach helps in creating a more cohesive development environment where each team owns its specific set of services, leading to fewer conflicts and improved efficiency.

x??",889,"Likewise, pushing power into development teams to increase autonomy can be fraught. People who have in the past thrown work over the wall to someone else are accustomed to having someone else to blame...",qwen2.5:latest,2025-11-03 00:00:11,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,How Much Is Too Much,Challenges at Scale,"#### Challenges at Scale
Background context: The passage highlights that microservices become increasingly complex as they grow from small examples to larger, more intricate systems. It discusses handling multiple service failures and managing hundreds of services, which introduces new challenges.

:p What are some challenges faced when microservice architectures grow beyond a few discrete services?
??x
Some key challenges include:
1. Managing the failure of multiple separate services.
2. Handling complexity as the number of services increases.
3. Ensuring effective communication and coordination across numerous services.
4. Dealing with the increased load on support teams.

These challenges require careful planning and robust strategies to maintain system reliability and performance.

x??",800,"Likewise, pushing power into development teams to increase autonomy can be fraught. People who have in the past thrown work over the wall to someone else are accustomed to having someone else to blame...",qwen2.5:latest,2025-11-03 00:00:11,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,How Much Is Too Much,Failure Management in Microservices,"#### Failure Management in Microservices
Background context: The text mentions that understanding failure is crucial, as hardware failures (like hard disks) and software crashes can occur frequently in microservice architectures.

:p What are some common types of failures mentioned in the text?
??x
The common types of failures mentioned include:
1. Hardware failures such as hard disk failures.
2. Software crashes or bugs within services.

These failures highlight the need for robust failure management strategies, including redundancy, monitoring, and recovery mechanisms.

x??",582,"Likewise, pushing power into development teams to increase autonomy can be fraught. People who have in the past thrown work over the wall to someone else are accustomed to having someone else to blame...",qwen2.5:latest,2025-11-03 00:00:11,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,How Much Is Too Much,Importance of People in Transition,"#### Importance of People in Transition
Background context: The passage stresses that people are central to any organizational change, especially when transitioning to a microservices architecture. It emphasizes understanding staff's readiness for change and ensuring clear communication about responsibilities.

:p Why is it important to understand the current staff’s appetite for change before making significant changes?
??x
It is crucial to understand the staff’s appetite for change because:
1. Rapid or forced changes can lead to resistance and lower morale.
2. Tailoring the transition pace according to team readiness ensures smoother implementation.
3. Clear communication about responsibilities helps in building trust and commitment among team members.

By understanding these aspects, organizations can better manage transitions and enhance overall success rates.

x??

---",886,"Likewise, pushing power into development teams to increase autonomy can be fraught. People who have in the past thrown work over the wall to someone else are accustomed to having someone else to blame...",qwen2.5:latest,2025-11-03 00:00:11,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,How Much Is Too Much,Embracing Failure in Distributed Systems,"#### Embracing Failure in Distributed Systems
Background context: In distributed systems, network unreliability and hardware failures are common. Understanding that failure is inevitable at scale allows for better design practices. For example, using cheaper components can be more efficient if you plan for frequent replacement due to high likelihood of failure.
:p What does the text suggest about handling service failures?
??x
The text suggests embracing the possibility of service failures and planning for them. By expecting that services might fail, it becomes easier to handle planned outages and make different trade-offs in system design. For instance, instead of spending a lot on making a single node highly resilient, one can use cheaper components like bare motherboards with velcro-hard drives.
??x
This approach allows organizations to focus more on graceful recovery mechanisms rather than futile attempts to prevent failure altogether.

```java
public class ServiceFailureHandler {
    public void handleServiceFailure() {
        // Code for handling failures gracefully
        System.out.println(""Handling service failure."");
        // Example: Gracefully stopping the service and restarting it from a backup.
        startBackupService();
    }

    private void startBackupService() {
        // Pseudocode for starting a backup service
        System.out.println(""Starting backup service..."");
    }
}
```
x??",1434,"And as anyone who has read the  fallacies of distributed computing  can tell you, we know that the network is unreliable. We can do our best to try to limit the causes of failure, but at a certain sca...",qwen2.5:latest,2025-11-03 00:00:19,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,How Much Is Too Much,Cross-Functional Requirements in Distributed Systems,"#### Cross-Functional Requirements in Distributed Systems
Background context: Cross-functional requirements involve understanding durability, availability, throughput, and latency in distributed systems. The goal is to determine the appropriate level of resilience needed based on specific use cases.
:p What does the text imply about autoscaling systems for a reporting system that runs only twice a month?
??x
The text implies that an autoscaling system might be overkill for a reporting system that only needs to run twice a month, as being down for a day or two isn't a significant issue. Therefore, resources can be allocated more efficiently by understanding the specific requirements and not overengineering solutions.
??x
This approach allows organizations to allocate resources effectively based on real needs rather than hypothetical worst-case scenarios.

```java
public class ReportingSystem {
    public void scheduleReportingRun() {
        // Code for scheduling a reporting run that only occurs twice a month
        System.out.println(""Scheduling monthly reports."");
        // Example: Running the report every 30 days without autoscaling.
        runReportOnScheduledDay();
    }

    private void runReportOnScheduledDay() {
        // Pseudocode for running the report on scheduled day
        System.out.println(""Running scheduled reports..."");
    }
}
```
x??
---",1386,"And as anyone who has read the  fallacies of distributed computing  can tell you, we know that the network is unreliable. We can do our best to try to limit the causes of failure, but at a certain sca...",qwen2.5:latest,2025-11-03 00:00:19,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,How Much Is Too Much,Blue/Green Deployments,"---
#### Blue/Green Deployments
Blue/green deployments are a strategy to minimize downtime during software updates. In this approach, two identical environments (blue and green) run concurrently, with one version handling live traffic while the other is prepared for deployment. After thorough testing, the traffic is switched from the blue environment to the green environment.

This method allows for rolling updates without interrupting service to end-users, as both versions of the application coexist during the transition period.
:p What are blue/green deployments used for?
??x
Blue/green deployments are a strategy to minimize downtime during software updates by maintaining two identical environments (blue and green) where one version handles live traffic while the other is prepared for deployment. This allows for testing new versions in parallel with the current ones, ensuring that any issues can be identified before fully switching over.
x??",957,"Likewise, figuring out how to do blue/green deployments to eliminate downtime of a service might make sense for your online ecommerce system, but for your corporate intranet knowledge base it’s probab...",qwen2.5:latest,2025-11-03 00:00:29,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,How Much Is Too Much,Understanding User Needs,"#### Understanding User Needs
Understanding user needs involves recognizing how much failure a system can tolerate or how fast it should perform based on the users' requirements. These requirements are driven by the nature and criticality of the service being provided.

For example, an online ecommerce system might require frequent updates with minimal downtime to ensure smooth customer experiences, whereas a corporate intranet knowledge base may not need such rigorous measures.
:p How do user needs influence system design?
??x
User needs significantly impact system design by determining factors like tolerance for failure and required response times. For instance, an online ecommerce system might require high availability and quick response times to ensure smooth transactions, while a corporate intranet knowledge base may tolerate some downtime or slower performance since it serves internal users with less critical access requirements.
x??",953,"Likewise, figuring out how to do blue/green deployments to eliminate downtime of a service might make sense for your online ecommerce system, but for your corporate intranet knowledge base it’s probab...",qwen2.5:latest,2025-11-03 00:00:29,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,How Much Is Too Much,Response Time/Latency,"#### Response Time/Latency
Response time and latency are crucial in measuring the speed at which operations within a system complete. It's important to establish targets for various percentiles of response times under different load conditions.

For example, you might set a target where 90% of responses should be completed within 2 seconds when handling 200 concurrent connections per second.
:p How do you measure and define acceptable response time?
??x
Acceptable response time is measured by setting targets for various percentiles of response times under different load conditions. For instance, a target might be defined as: ""We expect the website to have a 90th-percentile response time of 2 seconds when handling 200 concurrent connections per second."" This helps ensure that most users experience fast service while allowing some outliers.
x??",854,"Likewise, figuring out how to do blue/green deployments to eliminate downtime of a service might make sense for your online ecommerce system, but for your corporate intranet knowledge base it’s probab...",qwen2.5:latest,2025-11-03 00:00:29,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,How Much Is Too Much,Availability,"#### Availability
Availability refers to the percentage of time a system is operational and available for use. It's important to consider whether services are expected to be available 24/7 or if there can be acceptable periods of downtime.

Measuring availability often involves calculating uptime percentages, but it’s crucial to understand that these metrics might not directly reflect user experience.
:p How do you measure system availability?
??x
System availability is typically measured by the percentage of time a service is operational and available for use. This can be calculated as: `Availability = (Uptime / Total Time) * 100`. However, it's important to note that this metric might not directly reflect user experience since users expect consistent reliability rather than just uptime percentages.
x??",815,"Likewise, figuring out how to do blue/green deployments to eliminate downtime of a service might make sense for your online ecommerce system, but for your corporate intranet knowledge base it’s probab...",qwen2.5:latest,2025-11-03 00:00:29,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,How Much Is Too Much,Durability of Data,"#### Durability of Data
Durability of data refers to how long and under what conditions data can be retained. This varies based on the specific requirements and nature of the data.

For instance, financial transaction records may need to be kept for many years, whereas user session logs might only need to be stored for a year or less.
:p How do you determine data durability?
??x
Data durability is determined by considering how long and under what conditions data should be retained. This varies based on specific requirements, such as keeping financial transaction records for multiple years while user session logs might only need to be stored for a year or less.
x??

---",677,"Likewise, figuring out how to do blue/green deployments to eliminate downtime of a service might make sense for your online ecommerce system, but for your corporate intranet knowledge base it’s probab...",qwen2.5:latest,2025-11-03 00:00:29,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Architectural Safety Measures,Degrading Functionality in Microservices,"#### Degrading Functionality in Microservices
Background context: In a microservice architecture, ensuring that functionality can degrade gracefully when individual services are down is crucial for building a resilient system. This involves understanding the impact of each service's outage and preparing alternative actions.

:p What does degrading functionality entail in a microservice architecture?
??x
Degrading functionality means designing your application to continue functioning with reduced but essential features even if one or more microservices fail. For example, if the shopping cart service is down, you might still show the product details page but hide the cart section.

```java
// Example pseudocode for degrading function in Java
public void displayProductDetails(Product product) {
    // Display product details normally
    displayDetails(product);
    
    // Check if the shopping cart service is available
    boolean isCartServiceAvailable = checkIfServiceIsUp(""shopping-cart-service"");
    
    if (!isCartServiceAvailable) {
        // If not, degrade by showing a placeholder instead of full cart functionality
        showPlaceholderForShoppingCart();
    }
}
```
x??",1198,"Degrading Functionality An essential part of building a resilient system, especially when your functionality is spread over a number of different microservices that may be up or down, is the ability t...",qwen2.5:latest,2025-11-03 00:00:43,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Architectural Safety Measures,Architectural Safety Measures in Microservices,"#### Architectural Safety Measures in Microservices
Background context: To prevent cascading failures and ensure that one microservice outage doesn't bring down the entire system, it's important to implement architectural safety measures. These include circuit breakers, timeouts, and bulkheads.

:p What are some common architectural safety measures used in a microservice architecture?
??x
Some common architectural safety measures include:

1. **Circuit Breaker**: A mechanism that allows you to stop making requests to a problematic service until it has recovered.
2. **Timeouts**: Setting appropriate timeouts for HTTP calls to prevent blocking the application when a downstream service is slow or unresponsive.
3. **Bulkheads (or Service Isolation)**: Using separate connection pools and limits to prevent one service from overloading others.

```java
// Example pseudocode for implementing circuit breaker in Java using Resilience4j
import io.github.resilience4j.circuitbreaker.CircuitBreaker;
import io.github.resilience4j.circuitbreaker.CircuitBreakerRegistry;

public class ServiceClient {
    private final CircuitBreakerRegistry registry = CircuitBreakerRegistry.ofDefaults();
    private final CircuitBreaker circuitBreaker = registry.getCircuitBreaker(""shopping-cart-service"");

    public void processRequest() {
        try {
            if (circuitBreaker.isOpen()) {
                throw new RuntimeException(""Circuit breaker is open, retry later"");
            }
            
            // Perform the request
            performRequest();
        } catch (RuntimeException e) {
            circuitBreaker.executeWithTry(() -> performRequest());
        }
    }

    private void performRequest() {
        // Logic to make a downstream service call
    }
}
```
x??",1786,"Degrading Functionality An essential part of building a resilient system, especially when your functionality is spread over a number of different microservices that may be up or down, is the ability t...",qwen2.5:latest,2025-11-03 00:00:43,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Architectural Safety Measures,Strangler Pattern in Microservices,"#### Strangler Pattern in Microservices
Background context: The strangler pattern is used when you need to gradually replace an old system with a new one. It involves wrapping the old application's functionality while incrementally moving away from it.

:p What is the strangler pattern, and how does it work?
??x
The strangler pattern is a technique where a new application wraps around the legacy system, providing a layer of abstraction that gradually replaces the old functionalities over time. This allows for a smooth transition without requiring an immediate full-scale replacement.

```java
// Example pseudocode for wrapping functionality in Java
public class ClassifiedAdsSystem {
    private LegacyAdSystem legacyAdSystem;
    private NewAdSystem newAdSystem;

    public void displayAd(String adId) {
        try {
            // First, attempt to get the ad from the new system
            AdDetails adDetails = newAdSystem.getAd(adId);
            
            if (adDetails != null) {
                return adDetails;
            }
            
            // If the new system fails, fallback to the legacy system
            adDetails = legacyAdSystem.getAd(adId);
        } catch (Exception e) {
            log.error(""Failed to get ad from both systems"", e);
        }

        return adDetails; // Return whatever was found
    }
}
```
x??",1360,"Degrading Functionality An essential part of building a resilient system, especially when your functionality is spread over a number of different microservices that may be up or down, is the ability t...",qwen2.5:latest,2025-11-03 00:00:43,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Architectural Safety Measures,Example of a Cascading Failure in Microservices,"#### Example of a Cascading Failure in Microservices
Background context: The text provides an example where a downstream service's slow response led to a cascading failure, causing the entire system to go down.

:p What caused the cascading failure described in the text?
??x
The cascading failure was caused by a slow-down in one of the older legacy ad systems that was still serving a significant portion of the traffic. The slow service exhausted the HTTP connection pool, leading to an accumulation of blocked threads and ultimately bringing down the system.

```java
// Pseudocode illustrating the issue with the HTTP connection pool
public class HttpConnectionPool {
    private BlockingQueue<Worker> workers;

    public void makeRequest(HttpRequest request) throws InterruptedException {
        Worker worker = workers.take(); // Wait until a worker is available

        try {
            // Make an HTTP request using the worker
            HttpResponse response = makeHttpRequest(request);
            
            if (response.isTimeout()) {
                // If the request timed out, put the worker back and wait for another one
                putWorkerBack(worker);
            }
        } catch (IOException e) {
            // Handle exceptions by marking the worker as failed and re-adding to the queue
            markAsFailedAndReaddToQueue(worker);
        }
    }

    private void makeHttpRequest(HttpRequest request) throws IOException, InterruptedException {
        // Simulate a slow request
        Thread.sleep(1000); // Sleep for 1 second to simulate slowness
    }
}
```
x??

---",1613,"Degrading Functionality An essential part of building a resilient system, especially when your functionality is spread over a number of different microservices that may be up or down, is the ability t...",qwen2.5:latest,2025-11-03 00:00:43,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Antifragile Organization,Antifragile Organization Concept,"#### Antifragile Organization Concept
Background context: Nassim Taleb introduced the concept of antifragility, which refers to systems that benefit from failure and disorder. Ariel Tseitlin applied this idea to organizations like Netflix, highlighting how they embrace and even incite failure through various mechanisms.
:p What does the concept of an antifragile organization involve?
??x
The concept involves creating organizational structures that not only tolerate but also thrive on failure and uncertainty. Organizations should be designed in such a way that they can benefit from disorder, learning from failures to become more robust over time.
??x",657,"The Antifragile Organization In his book  Antifragile  (Random House), Nassim Taleb talks about things that actually benefit from failure and disorder. Ariel Tseitlin used this concept to coin the con...",qwen2.5:latest,2025-11-03 00:00:54,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Antifragile Organization,Game Days for Simulating Failures,"#### Game Days for Simulating Failures
Background context: Google regularly performs 'game days,' where it simulates server failures and has teams respond to these events. This helps ensure preparedness and resilience in their systems.
:p What is a game day, and why does Netflix adopt similar practices?
??x
A game day is an exercise where organizations simulate system failures and have various teams react to these scenarios. By doing so, they can test the resilience of their systems and improve response times. Netflix also uses similar methods, such as running the Chaos Monkey in production daily.
??x",608,"The Antifragile Organization In his book  Antifragile  (Random House), Nassim Taleb talks about things that actually benefit from failure and disorder. Ariel Tseitlin used this concept to coin the con...",qwen2.5:latest,2025-11-03 00:00:54,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Antifragile Organization,The Chaos Monkey Tool,"#### The Chaos Monkey Tool
Background context: Netflix has developed a tool called the Chaos Monkey that runs in production environments daily to simulate machine failures. This ensures developers are prepared for unexpected issues.
:p What is the Chaos Monkey, and how does it work?
??x
The Chaos Monkey is a program developed by Netflix that randomly turns off machines during certain hours of the day in production environments. Its purpose is to test system resilience against unexpected failures.
```java
public class ChaosMonkey {
    public void run() {
        while (true) {
            Random random = new Random();
            int machineToTurnOff = random.nextInt(totalNumberOfMachines);
            turnOffMachine(machineToTurnOff);
        }
    }

    private void turnOffMachine(int machineNumber) {
        // Code to safely shut down the specified machine
    }
}
```
??x",889,"The Antifragile Organization In his book  Antifragile  (Random House), Nassim Taleb talks about things that actually benefit from failure and disorder. Ariel Tseitlin used this concept to coin the con...",qwen2.5:latest,2025-11-03 00:00:54,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Antifragile Organization,Simian Army of Failure Bots,"#### Simian Army of Failure Bots
Background context: Netflix’s Simian Army includes various failure bots, such as Chaos Gorilla and Latency Monkey, designed to simulate different types of system failures.
:p What is the Simian Army in the context of Netflix?
??x
The Simian Army refers to a suite of tools developed by Netflix that simulate different types of failures. These include Chaos Gorilla (which takes out an entire availability center) and Latency Monkey (which simulates slow network connectivity between machines).
```java
public class SimianArmy {
    public void runChaosGorilla() {
        // Code to simulate failure in an entire AWS availability center
    }

    public void runLatencyMonkey() {
        // Code to simulate slow network connectivity
    }
}
```
??x",783,"The Antifragile Organization In his book  Antifragile  (Random House), Nassim Taleb talks about things that actually benefit from failure and disorder. Ariel Tseitlin used this concept to coin the con...",qwen2.5:latest,2025-11-03 00:00:54,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Antifragile Organization,Embracing and Inciting Failure Through Software,"#### Embracing and Inciting Failure Through Software
Background context: By actively causing failures, Netflix ensures that their systems are robust enough to handle real-world issues. This approach is crucial for organizations dealing with highly distributed systems.
:p Why does Netflix embrace failure through software?
??x
Netflix embraces failure through software because it prepares the system for real-world issues by ensuring developers are constantly ready and able to handle unexpected failures. This proactive approach improves overall system resilience and reliability.
??x",585,"The Antifragile Organization In his book  Antifragile  (Random House), Nassim Taleb talks about things that actually benefit from failure and disorder. Ariel Tseitlin used this concept to coin the con...",qwen2.5:latest,2025-11-03 00:00:54,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Antifragile Organization,Blameless Culture in Organizations,"#### Blameless Culture in Organizations
Background context: Netflix promotes a blameless culture where mistakes are seen as learning opportunities rather than punishment grounds. This fosters an environment where employees feel safe to report issues and improve the system.
:p How does Netflix promote a blameless culture?
??x
Netflix promotes a blameless culture by treating failures as learning opportunities and not assigning blame when things go wrong. This approach encourages team members to report issues openly, which in turn helps the organization continuously evolve and improve.
??x",593,"The Antifragile Organization In his book  Antifragile  (Random House), Nassim Taleb talks about things that actually benefit from failure and disorder. Ariel Tseitlin used this concept to coin the con...",qwen2.5:latest,2025-11-03 00:00:54,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,The Antifragile Organization,Importance of Handling Failures in Distributed Systems,"#### Importance of Handling Failures in Distributed Systems
Background context: As systems become more distributed, they inherently face greater risks of failure. However, embracing these failures through robust testing can lead to more resilient systems that better support customer needs.
:p Why is it important for organizations to prepare for system failures?
??x
It is crucial for organizations to prepare for system failures because modern systems are increasingly distributed and prone to unpredictable issues. By simulating and handling these failures proactively, organizations can build more robust and reliable systems that can withstand real-world challenges.
??x",675,"The Antifragile Organization In his book  Antifragile  (Random House), Nassim Taleb talks about things that actually benefit from failure and disorder. Ariel Tseitlin used this concept to coin the con...",qwen2.5:latest,2025-11-03 00:00:54,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Circuit Breakers,Timeouts,"#### Timeouts
Timeouts are crucial for managing downstream system failures. They determine how long a system should wait before considering a call to be failed and acting on it. If set too low, you risk losing calls that might have succeeded; if too high, your system can become slow or unresponsive.

:p What is the importance of setting timeouts correctly in a distributed system?
??x
Timeouts are essential because they balance between ensuring timely responses from downstream systems and avoiding unnecessary delays. Incorrect timeout settings can lead to either failing too early (resulting in lost requests) or failing too late (leading to prolonged processing times).

For example, consider a scenario where you have an HTTP request that may take 5-10 seconds under normal conditions but could hang indefinitely if the backend service is down.

```java
public class TimeoutExample {
    public static void makeRequest(String url, int timeout) throws Exception {
        try (CloseableHttpClient httpClient = HttpClients.createDefault()) {
            HttpGet httpGet = new HttpGet(url);
            CloseableHttpResponse response = httpClient.execute(httpGet, new BasicHttpContext(), timeout);
            // Process the response
        } catch (IOException e) {
            throw new Exception(""Request timed out or failed"", e);
        }
    }
}
```
x??",1364,"Timeouts Timeouts are something it is easy to overlook, but in a downstream system they are important to get right. How long can I wait before I can consider a downstream system to actually be down? W...",qwen2.5:latest,2025-11-03 00:01:07,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Circuit Breakers,Circuit Breakers,"#### Circuit Breakers
Circuit breakers act as a safeguard for downstream services that might be experiencing issues. They monitor service health and, upon detecting a fault threshold, stop sending requests to the problematic service, thereby preventing the main system from being overwhelmed by failed calls.

:p What is a circuit breaker in software architecture?
??x
A circuit breaker acts like an electrical one but for your application's services. When a downstream service fails repeatedly, instead of continuously retrying (which can overwhelm both the upstream and downstream systems), the circuit breaker trips, failing fast and sending errors back to the caller.

Here’s a simplified example in Java:

```java
public class CircuitBreakerExample {
    private final int maxFailures;
    private volatile boolean isBroken = false;
    private AtomicInteger failureCount = new AtomicInteger(0);

    public CircuitBreakerExample(int maxFailures) {
        this.maxFailures = maxFailures;
    }

    public void recordFailure() {
        if (isBroken()) {
            incrementFailureCount();
        }
    }

    public boolean isBroken() {
        return isBroken && failureCount.get() >= maxFailures;
    }

    private void incrementFailureCount() {
        if (!isBroken) {
            failureCount.incrementAndGet();
        } else if (failureCount.incrementAndGet() > 2 * maxFailures) {
            // Reset after a period of stability
            resetCircuitBreaker();
        }
    }

    private void resetCircuitBreaker() {
        isBroken = false;
        failureCount.set(0);
    }
}
```
x??",1611,"Timeouts Timeouts are something it is easy to overlook, but in a downstream system they are important to get right. How long can I wait before I can consider a downstream system to actually be down? W...",qwen2.5:latest,2025-11-03 00:01:07,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Circuit Breakers,Handling Failures During Maintenance,"#### Handling Failures During Maintenance
During maintenance, circuit breakers can be manually blown to safely isolate a microservice or component. This allows for the safe shutdown and restart of services without affecting other parts of the system.

:p How can you use circuit breakers during routine maintenance?
??x
Circuit breakers are useful tools that help manage service health. During maintenance, they can be manually triggered to fail fast, ensuring that downstream services do not receive requests from the maintenance target, thus preventing unexpected failures in other components.

For example:

```java
public class MaintenanceMode {
    private final CircuitBreaker circuitBreaker;

    public MaintenanceMode(CircuitBreaker breaker) {
        this.circuitBreaker = breaker;
    }

    public void triggerMaintenance() {
        circuitBreaker.recordFailure();
    }

    public boolean isMaintenanceModeActive() {
        return circuitBreaker.isBroken();
    }
}
```
x??",989,"Timeouts Timeouts are something it is easy to overlook, but in a downstream system they are important to get right. How long can I wait before I can consider a downstream system to actually be down? W...",qwen2.5:latest,2025-11-03 00:01:07,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Circuit Breakers,Queuing and Retrying Requests,"#### Queuing and Retrying Requests
During the blown state of a circuit breaker, you have options such as queuing requests. This can be beneficial if retries are part of the business logic or if it’s an asynchronous job.

:p What is one way to handle requests during the blown state of a circuit breaker?
??x
One approach is to queue up failed requests and retry them later when the circuit breaker returns to the closed state. This is particularly useful for non-critical operations that can be retried without impacting the primary workflow.

For example, if you have an asynchronous job processing emails:

```java
public class RequestQueue {
    private final BlockingQueue<Runnable> queue = new LinkedBlockingQueue<>();

    public void submitRequest(Runnable task) throws InterruptedException {
        queue.put(task);
    }

    public boolean isQueueEmpty() {
        return queue.isEmpty();
    }
}

public class RetryService {
    private final RequestQueue queue;
    private final CircuitBreaker breaker;

    public RetryService(RequestQueue queue, CircuitBreaker breaker) {
        this.queue = queue;
        this.breaker = breaker;
    }

    public void processRequest(Runnable task) throws InterruptedException {
        if (breaker.isBroken()) {
            // Queue the request
            queue.submitRequest(task);
        } else {
            // Process normally
            task.run();
        }
    }
}
```
x??",1435,"Timeouts Timeouts are something it is easy to overlook, but in a downstream system they are important to get right. How long can I wait before I can consider a downstream system to actually be down? W...",qwen2.5:latest,2025-11-03 00:01:07,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Bulkheads,Bulkheads in Software Architecture,"#### Bulkheads in Software Architecture
Background context explaining the concept. The bulkhead pattern is introduced by Nygard as a way to isolate parts of a system from failure, similar to how bulkheads on ships protect sections from damage if one part is compromised.
:p What are bulkheads and how do they work in software architecture?
??x
Bulkheads in software architecture help prevent the failure of one part of the system from causing widespread issues. They can be implemented through various means such as using separate connection pools, microservices, or circuit breakers. For instance, using different connection pools for each downstream service ensures that if one pool gets exhausted, others remain unaffected.
```java
// Example of separating connection pools in Java
DataSource pool1 = new DataSource(); // Connection pool 1
DataSource pool2 = new DataSource(); // Connection pool 2

// Code to use separate pools for different services
if (serviceANeedsConnection()) {
    Connection conn = pool1.getConnection();
} else if (serviceBNeedsConnection()) {
    Connection conn = pool2.getConnection();
}
```
x??",1127,"Bulkheads In another pattern from  Release It. , Nygard introduces the concept of a  bulkhead  as a way to isolate yourself from failure. In shipping, a bulkhead is a part of the ship that can be seal...",qwen2.5:latest,2025-11-03 00:01:16,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Bulkheads,Microservices and Separation of Concerns,"#### Microservices and Separation of Concerns
Background context explaining the concept. By separating functionality into microservices, we can reduce the risk of one part of the system failing to impact others.
:p How does separation of concerns through microservices help in implementing bulkheads?
??x
Separating functionalities into separate microservices helps isolate failures within specific services, preventing them from cascading and affecting other parts of the system. This approach ensures that if a particular service fails or behaves slowly, it won't disrupt the entire application.
```java
// Example of separating services into different processes
public class UserService {
    public User getUserById(int id) { ... }
}

public class OrderService {
    public Order getOrderById(int id) { ... }
}
```
x??",822,"Bulkheads In another pattern from  Release It. , Nygard introduces the concept of a  bulkhead  as a way to isolate yourself from failure. In shipping, a bulkhead is a part of the ship that can be seal...",qwen2.5:latest,2025-11-03 00:01:16,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Bulkheads,Circuit Breakers for Bulkheads,"#### Circuit Breakers for Bulkheads
Background context explaining the concept. Circuit breakers act as automatic mechanisms to isolate failing services, protecting both the consumer and the downstream service from further damage.
:p What is a circuit breaker and how does it function in implementing bulkheads?
??x
A circuit breaker acts as an automated mechanism that isolates a failing part of the system, preventing further calls to that service. It can also protect the downstream service by limiting the number of requests to avoid overwhelming it. When a service fails or behaves slowly, the circuit breaker trips and stops sending requests to it until it recovers.
```java
// Example of using Hystrix for implementing a circuit breaker in Java
HystrixCommand.Setter withGroupKey(""serviceA"")
    .andCommandKey(""getUserById"")
    .build()
.execute();
```
x??",864,"Bulkheads In another pattern from  Release It. , Nygard introduces the concept of a  bulkhead  as a way to isolate yourself from failure. In shipping, a bulkhead is a part of the ship that can be seal...",qwen2.5:latest,2025-11-03 00:01:16,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Bulkheads,Load Shedding and Bulkheads,"#### Load Shedding and Bulkheads
Background context explaining the concept. Load shedding is a technique where bulkheads reject requests under certain conditions to prevent resource saturation.
:p What is load shedding, and how does it relate to bulkheads?
??x
Load shedding in bulkheads involves rejecting requests when resources are becoming saturated. This helps ensure that critical systems don't become overwhelmed and act as bottlenecks for other services. When a service is nearing its capacity, it can reject additional requests to prevent resource exhaustion.
```java
// Example of load shedding using Hystrix in Java
HystrixCommand.Setter withGroupKey(""serviceA"")
    .andCommandPropertiesDefaults(HystrixCommandProperties.Setter()
        .withCircuitBreakerRequestVolumeThreshold(10)
        .withCircuitBreakerErrorThresholdPercentage(50))
    .build();
```
x??

---",879,"Bulkheads In another pattern from  Release It. , Nygard introduces the concept of a  bulkhead  as a way to isolate yourself from failure. In shipping, a bulkhead is a part of the ship that can be seal...",qwen2.5:latest,2025-11-03 00:01:16,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Idempotency,Service Isolation,"#### Service Isolation
Background context: The more one service depends on another being up, the more the health of one impacts the ability of the other to do its job. If we can use integration techniques that allow a downstream server to be offline, upstream services are less likely to be affected by outages, planned or unplanned.
This is particularly important for maintaining system stability and ensuring that failures in one part of the service do not cascade into others.

:p What are the benefits of increasing isolation between services?
??x
Increasing isolation between services reduces the need for coordination between teams, allowing them more autonomy to operate and evolve their services. It also minimizes the impact of outages in downstream services on upstream services.",789,"Isolation The more one service depends on another being up, the more the health of one impacts the ability of the other to do its job. If we can use integration techniques that allow a downstream serv...",qwen2.5:latest,2025-11-03 00:01:29,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Idempotency,Idempotency,"#### Idempotency
Background context: In idempotent operations, the outcome doesn’t change after the first application, even if the operation is subsequently applied multiple times. This characteristic is very useful when recovering from errors by replaying messages that haven't been processed.

:p What does it mean for an operation to be idempotent?
??x
An idempotent operation ensures that applying the operation multiple times has the same effect as applying it once. For example, in Example 11-2, adding points to a customer's account is idempotent because adding the same amount of points again does not change the total.

:p How can we make a non-idempotent operation like adding points more idempotent?
??x
By providing additional information that uniquely identifies the operation. In Example 11-2, the credit call becomes idempotent by including a `reason` element with an order ID.",892,"Isolation The more one service depends on another being up, the more the health of one impacts the ability of the other to do its job. If we can use integration techniques that allow a downstream serv...",qwen2.5:latest,2025-11-03 00:01:29,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Idempotency,HTTP Idempotency and Service Design,"#### HTTP Idempotency and Service Design
Background context: Some HTTP verbs like GET and PUT are defined to be idempotent in the HTTP specification. However, for these operations to remain idempotent, the service must handle them in such a way that subsequent identical requests have no additional effect.

:p Why is it important for a service handling HTTP GET or PUT requests to ensure they are idempotent?
??x
Ensuring that services handle GET and PUT requests idempotently prevents unexpected side effects when these requests are called multiple times. For example, if a PUT request updates a resource, repeated calls should not result in duplicate updates.

:p How can a service implement an idempotent HTTP PUT method for updating customer information?
??x
To make the `PUT /customers/{id}` endpoint idempotent, you could hash the request data and use it as a unique identifier. Only update the database if the hashed data matches the existing record.
```java
public void updateCustomer(@PathVariable(""id"") String id, @RequestBody CustomerRequest customer) {
    String existingDataHash = customerRepository.getDataHashForId(id);
    String currentHash = hashRequest(customer);
    if (!existingDataHash.equals(currentHash)) {
        // Update the database with new data
        customerRepository.save(id, customer);
    }
}
```",1337,"Isolation The more one service depends on another being up, the more the health of one impacts the ability of the other to do its job. If we can use integration techniques that allow a downstream serv...",qwen2.5:latest,2025-11-03 00:01:29,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Idempotency,Event Processing and Idempotency,"#### Event Processing and Idempotency
Background context: In event-driven architectures, processing events in an idempotent manner ensures that even if messages are delivered multiple times due to delivery failures or retries, the system's state remains consistent.

:p How can you ensure that your service processes events idempotently?
??x
Ensure that each event is processed only once by maintaining a record of which events have been processed. If an event is received more than once, it should be ignored or handled appropriately to avoid duplicate processing.
```java
public void handleEvent(Event event) {
    if (eventProcessor.hasProcessed(event)) {
        // Ignore the event as it has already been processed
        return;
    }
    // Process the event and mark it as handled
    process(event);
    eventProcessor.markAsProcessed(event);
}
```",858,"Isolation The more one service depends on another being up, the more the health of one impacts the ability of the other to do its job. If we can use integration techniques that allow a downstream serv...",qwen2.5:latest,2025-11-03 00:01:29,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Idempotency,HTTP Verb Idempotency in Practice,"#### HTTP Verb Idempotency in Practice
Background context: Not all operations that are defined to be idempotent in HTTP (like GET) should always be treated as such. For example, a service might record the fact that a request was received even if it doesn't perform any state-changing operation.

:p Can you use an HTTP GET method for logging purposes without causing side effects?
??x
Yes, you can use an HTTP GET method to log the receipt of a request and collect metrics like response time. The key is to ensure that such calls are idempotent in terms of the underlying business logic, not the entire system state.
```java
@GetMapping(""/log"")
public ResponseEntity<?> logRequest() {
    long startTime = System.currentTimeMillis();
    // Log the request details
    logger.info(""Received GET request at: {}"", new Date(startTime));
    
    // Record response time for monitoring purposes
    long endTime = System.currentTimeMillis();
    double elapsedTime = (endTime - startTime) / 1000.0;
    metricsService.logResponseTime(elapsedTime);
    
    return ResponseEntity.ok().build(); // Non-idempotent in terms of system state, but idempotent in business logic
}
```",1171,"Isolation The more one service depends on another being up, the more the health of one impacts the ability of the other to do its job. If we can use integration techniques that allow a downstream serv...",qwen2.5:latest,2025-11-03 00:01:29,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Idempotency,Service Autonomy and Isolation,"#### Service Autonomy and Isolation
Background context: Increased isolation between services allows teams to operate more autonomously. This is beneficial because it reduces the coordination needed between service owners, enabling them to evolve their services more freely.

:p How does increasing isolation between services impact team autonomy?
??x
Increasing isolation between services allows teams to operate with greater independence. With less need for coordinated changes and fewer dependencies on other services, teams can make decisions and implement changes without waiting for consensus or synchronization with other teams.
---",638,"Isolation The more one service depends on another being up, the more the health of one impacts the ability of the other to do its job. If we can use integration techniques that allow a downstream serv...",qwen2.5:latest,2025-11-03 00:01:29,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Load Balancing,Scaling for Performance,"---
#### Scaling for Performance
In microservice architectures, scaling is primarily done to handle more load and reduce latency. Vertical scaling involves getting a bigger box with faster CPU and better I/O capabilities, which can improve processing speed and throughput but may be expensive and not always resource-efficient.

:p What does vertical scaling refer to in the context of scaling microservices?
??x
Vertical scaling refers to increasing the performance of a single server by upgrading its hardware resources such as CPU, memory, or storage. This approach aims to handle more load and reduce latency through enhanced computational power.
```java
// Example of applying vertical scaling using cloud services API
public class CloudInstanceResize {
    public void resizeInstance(int newCores, int newMemoryGB) throws Exception {
        // Code to resize the instance in a cloud environment with updated resources
        // Assume this is part of a larger orchestration framework
        // Example pseudo-code:
        // cloudAPI.resize(instanceId, newCores, newMemoryGB);
    }
}
```
x??",1102,"Scaling We scale our systems in general for one of two reasons. First, to help deal with failure: if we’re worried that something will fail, then having more of it will help, right? Second, we scale f...",qwen2.5:latest,2025-11-03 00:01:39,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Load Balancing,Splitting Workloads,"#### Splitting Workloads
To improve throughput and scaling, microservices can be split across multiple hosts. Initially, it might be tempting to run multiple services on a single host for cost efficiency or simplicity of management. However, as the application grows, this approach may lead to resource bottlenecks.

:p How does splitting workloads help in microservice architectures?
??x
Splitting workloads helps by distributing the load among multiple hosts, thereby improving throughput and scaling. By separating critical functionalities from non-critical ones, we can optimize each service for its specific tasks. For example, if an accounts service handles both customer financial record management and query generation, splitting these into two separate services can reduce the load on the core functionality while ensuring that reporting queries don't impact critical operations.
```java
// Example of splitting a microservice based on workload
public class AccountService {
    public void manageAccounts() {
        // Code for managing customer financial records
    }

    public void generateReports() {
        // Code for generating reports, potentially using more efficient query methods
    }
}

// Suggested refactoring
public class AccountsService {
    public void manageCustomerRecords() {
        // Refactored code focusing on critical operations
    }
}

public class ReportingService {
    public void generateFinancialReports() {
        // Specialized code for generating reports, possibly using distributed computing or caching
    }
}
```
x??",1572,"Scaling We scale our systems in general for one of two reasons. First, to help deal with failure: if we’re worried that something will fail, then having more of it will help, right? Second, we scale f...",qwen2.5:latest,2025-11-03 00:01:39,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Load Balancing,Spreading Your Risk,"#### Spreading Your Risk
To ensure resilience in the system, it's important to avoid concentrating all services on a single host. This reduces the impact of any single point failure.

:p How can spreading risk improve the resilience of microservices?
??x
Spreading your risk means distributing the services across multiple hosts to minimize the impact of any single point failure. If one service fails, others remain operational, ensuring that the overall system remains functional. For example, if you have a financial accounts service and an independent reporting service, each can fail independently without causing significant disruptions.

```java
// Example of hosting microservices on different instances for resilience
public class ServiceDeployment {
    public void deployServices() {
        // Code to deploy accounts service on one instance
        deploy(""AccountsService"", ""instance1"");

        // Code to deploy reporting service on a separate instance
        deploy(""ReportingService"", ""instance2"");
    }

    private void deploy(String serviceName, String instanceId) {
        // Pseudo-code for deploying services
        System.out.println(""Deploying "" + serviceName + "" on "" + instanceId);
    }
}
```
x??
---",1234,"Scaling We scale our systems in general for one of two reasons. First, to help deal with failure: if we’re worried that something will fail, then having more of it will help, right? Second, we scale f...",qwen2.5:latest,2025-11-03 00:01:39,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Load Balancing,Virtualization and Host Distribution,"#### Virtualization and Host Distribution
Background context explaining how virtual hosts can impact service availability. Discuss the importance of distributing services across different physical boxes to reduce downtime risks.

:p How can virtualization impact the availability of services?
??x
Virtualization platforms can present a single point of failure if all services are running on virtual hosts hosted on the same physical box. If that physical box fails, multiple services could go down simultaneously. To mitigate this risk, it is crucial to distribute the virtual hosts across different physical boxes or data centers.

```java
// Example of distributing VMs across physical hosts
public class HostDistributor {
    public void distributeVMs(List<VirtualMachine> vms) {
        List<PhysicalHost> physicalHosts = getPhysicalHosts();
        for (VirtualMachine vm : vms) {
            int randomHostIndex = getRandomInt(physicalHosts.size());
            PhysicalHost host = physicalHosts.get(randomHostIndex);
            host.addVM(vm);
        }
    }
}
```
x??",1077,"A simplistic example of this is making sure that you don’t have multiple services on one host, where an outage would impact multiple services. But let’s consider what  host  means. In most situations ...",qwen2.5:latest,2025-11-03 00:01:53,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Load Balancing,SAN Failure Impact,"#### SAN Failure Impact
Background context on the risks associated with Storage Area Networks (SANs). Explain how a failure of a large, expensive SAN can affect multiple virtual machines connected to it.

:p How does a SAN failure impact VMs?
??x
A SAN failure can be catastrophic as it may take down all connected VMs since they are typically mapped to a single SAN. This risk is significant because SANs are critical infrastructure designed not to fail but do occasionally go down, causing severe outages and data loss.

```java
// Example of handling SAN failures in a distributed system
public class SanFailHandler {
    public void handleSanFailure(List<VirtualMachine> vms) {
        // Notify all VMs connected to the failed SAN about their failure state.
        for (VirtualMachine vm : vms) {
            if (vm.isAttachedToFailedSan()) {
                vm.shutdownGracefully();
                System.out.println(""Notifying "" + vm.getName() + "" of San Failure"");
            }
        }
    }
}
```
x??",1014,"A simplistic example of this is making sure that you don’t have multiple services on one host, where an outage would impact multiple services. But let’s consider what  host  means. In most situations ...",qwen2.5:latest,2025-11-03 00:01:53,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Load Balancing,Data Center and Rack Placement,"#### Data Center and Rack Placement
Background context on the importance of placing services in different racks or across multiple data centers to avoid a single point of failure.

:p Why is it important to distribute services across multiple racks?
??x
Distributing services across multiple racks within a data center can prevent a localized issue, such as a network outage or physical damage, from impacting all services. Similarly, spreading services across different data centers provides redundancy and protects against large-scale disasters like natural calamities.

```java
// Example of distributing services across racks
public class RackDistributor {
    public void distributeServices(List<Service> services) {
        List<Rack> racks = getRacks();
        for (Service service : services) {
            int randomRackIndex = getRandomInt(racks.size());
            Rack rack = racks.get(randomRackIndex);
            rack.addService(service);
        }
    }
}
```
x??",981,"A simplistic example of this is making sure that you don’t have multiple services on one host, where an outage would impact multiple services. But let’s consider what  host  means. In most situations ...",qwen2.5:latest,2025-11-03 00:01:53,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Load Balancing,Service Level Agreements (SLAs),"#### Service Level Agreements (SLAs)
Background context on the significance of SLAs in ensuring uptime and recovery time objectives for services. Explain how to plan accordingly if the service provider does not meet your required SLA.

:p What is an SLA, and why is it important?
??x
An SLA is a formal agreement that defines the level of service expected by a customer from a service provider. It typically includes uptime guarantees, recovery time objectives (RTOs), and other performance metrics. Understanding the SLA terms is crucial to ensure your services meet the required availability. If the SLA does not match your needs, you might need alternative solutions or plan for redundancy.

```java
// Example of checking against an SLA
public class SlADownTimeChecker {
    public boolean checkSLADowntime(double actualDowntime) {
        double agreedUptime = 99.95; // Example agreed uptime in percentage
        double maxAllowedDowntime = (100 - agreedUptime) / 100;
        return actualDowntime <= maxAllowedDowntime;
    }
}
```
x??",1044,"A simplistic example of this is making sure that you don’t have multiple services on one host, where an outage would impact multiple services. But let’s consider what  host  means. In most situations ...",qwen2.5:latest,2025-11-03 00:01:53,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Load Balancing,AWS Regions and Availability Zones,"#### AWS Regions and Availability Zones
Background context on how AWS is structured with regions and availability zones to provide high availability. Explain the importance of distributing workloads across multiple availability zones.

:p How do AWS regions and availability zones contribute to service availability?
??x
AWS is divided into regions, each containing one or more availability zones (AZs). AZs are designed to be isolated from failures in other AZs within the same region, ensuring higher availability. Distributing services across multiple AZs minimizes the risk of a single point of failure.

```java
// Example of distributing workloads across AWS availability zones
public class AwsWorkloadDistributor {
    public void distributeWorkloads(List<Service> services) {
        List<AvailabilityZone> azs = getAzs();
        for (Service service : services) {
            int randomAzIndex = getRandomInt(azs.size());
            AvailabilityZone az = azs.get(randomAzIndex);
            az.addService(service);
        }
    }
}
```
x??",1051,"A simplistic example of this is making sure that you don’t have multiple services on one host, where an outage would impact multiple services. But let’s consider what  host  means. In most situations ...",qwen2.5:latest,2025-11-03 00:01:53,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Load Balancing,Disaster Recovery Planning,"#### Disaster Recovery Planning
Background context on the importance of having a backup hosting platform to ensure resilience. Explain how multiple suppliers can provide additional layers of protection.

:p Why is disaster recovery planning important?
??x
Disaster recovery planning is crucial for minimizing downtime and ensuring service availability in case your primary provider fails to meet its SLA guarantees. Having an alternative hosting platform with a different supplier provides redundancy and helps mitigate the risk of catastrophic failures.

```java
// Example of setting up a disaster recovery plan
public class DisasterRecoveryPlanner {
    public void setupDisasterRecovery(List<Service> services, Supplier backupSupplier) {
        for (Service service : services) {
            BackupService backup = backupSupplier.getBackupFor(service);
            service.setBackup(backup);
        }
    }
}
```
x??

---",927,"A simplistic example of this is making sure that you don’t have multiple services on one host, where an outage would impact multiple services. But let’s consider what  host  means. In most situations ...",qwen2.5:latest,2025-11-03 00:01:53,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Load Balancing,Load Balancers and Scaling,"#### Load Balancers and Scaling

Background context explaining the concept. Load balancers are essential for distributing network traffic to multiple instances of a service, ensuring that no single instance is overwhelmed. They also manage the health check of these instances and can add or remove them as needed.

:p What is a load balancer?
??x
A load balancer distributes incoming network requests across multiple backend servers based on predefined algorithms. It ensures that no single server bears too much traffic, thus improving overall system performance and reliability.
x??",584,"Figure 11-4. An example of a load balancing approach to scale the number of customer service instances Load balancers come in all shapes and sizes, from big and expensive hardware appliances to softwa...",qwen2.5:latest,2025-11-03 00:02:02,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Load Balancing,SSL Termination,"#### SSL Termination

Background context explaining the concept. SSL termination involves decrypting HTTPS traffic at the load balancer before it reaches the target instances. Historically, this was done to offload encryption/decryption tasks from the backend servers. However, modern systems often use this feature to simplify host configuration.

:p What is SSL termination?
??x
SSL termination means that a load balancer decrypts incoming HTTPS traffic and forwards it as HTTP to the backend server. This reduces the overhead on backend instances but can introduce security risks if not properly managed.
x??",611,"Figure 11-4. An example of a load balancing approach to scale the number of customer service instances Load balancers come in all shapes and sizes, from big and expensive hardware appliances to softwa...",qwen2.5:latest,2025-11-03 00:02:02,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Load Balancing,VLAN for Improved Security,"#### VLAN for Improved Security

Background context explaining the concept. A Virtual Local Area Network (VLAN) is used to isolate network segments, ensuring that requests only enter specific parts of a network via routers. In this case, it helps in securing microservices by limiting external access.

:p How can VLANs improve security?
??x
Using VLANs improves security by isolating the internal communication of microservices from external access. All traffic between instances within the same VLAN is handled over HTTP, while only HTTPS is used for external requests. This reduces the risk of man-in-the-middle attacks.
x??",627,"Figure 11-4. An example of a load balancing approach to scale the number of customer service instances Load balancers come in all shapes and sizes, from big and expensive hardware appliances to softwa...",qwen2.5:latest,2025-11-03 00:02:02,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Load Balancing,AWS Elastic Load Balancers (ELBs),"#### AWS Elastic Load Balancers (ELBs)

Background context explaining the concept. AWS provides a variety of load balancers, including Elastic Load Balancers (ELBs), which can handle SSL termination and be configured with security groups or VPCs to manage network traffic.

:p What are AWS ELBs?
??x
AWS Elastic Load Balancers (ELBs) distribute incoming traffic across multiple targets in an Auto Scaling group. They support features like SSL termination, where HTTPS traffic is decrypted at the load balancer before being forwarded as HTTP to the backend instances.
x??",570,"Figure 11-4. An example of a load balancing approach to scale the number of customer service instances Load balancers come in all shapes and sizes, from big and expensive hardware appliances to softwa...",qwen2.5:latest,2025-11-03 00:02:02,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Load Balancing,Software vs. Hardware Load Balancers,"#### Software vs. Hardware Load Balancers

Background context explaining the concept. While hardware load balancers offer robust performance and reliability, they can be difficult to automate. In contrast, software load balancers like mod_proxy provide more flexibility in configuration.

:p What is a key difference between software and hardware load balancers?
??x
The primary difference lies in automation and flexibility. Hardware load balancers are designed for high performance and reliability but may lack the ease of reconfiguration and automation compared to software load balancers, which can be easier to modify as needed.
x??",637,"Figure 11-4. An example of a load balancing approach to scale the number of customer service instances Load balancers come in all shapes and sizes, from big and expensive hardware appliances to softwa...",qwen2.5:latest,2025-11-03 00:02:02,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Load Balancing,Single Points of Failure,"#### Single Points of Failure

Background context explaining the concept. A single point of failure occurs when a critical component in a system can bring down the entire system if it fails. In distributed systems, ensuring that no single component is a bottleneck or critical path is essential.

:p What is a single point of failure?
??x
A single point of failure refers to any element within a system whose failure would cause the entire system to fail. In microservices architectures, this could be a database running on a single host. Ensuring there are no such points is crucial for maintaining system resilience.
x??",622,"Figure 11-4. An example of a load balancing approach to scale the number of customer service instances Load balancers come in all shapes and sizes, from big and expensive hardware appliances to softwa...",qwen2.5:latest,2025-11-03 00:02:02,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Load Balancing,Managing Load Balancer Configuration,"#### Managing Load Balancer Configuration

Background context explaining the concept. Proper management of load balancer configuration ensures that changes can be automated and stored in version control, reducing the risk of human error.

:p Why should load balancer configurations be managed like service configurations?
??x
Load balancer configurations should be treated similarly to other service configurations—stored in version control systems and able to be automatically applied. This helps maintain consistency across environments and reduces manual configuration errors.
x??

---",588,"Figure 11-4. An example of a load balancing approach to scale the number of customer service instances Load balancers come in all shapes and sizes, from big and expensive hardware appliances to softwa...",qwen2.5:latest,2025-11-03 00:02:02,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Caching in HTTP,Worker-Based Systems Load Balancing,"#### Worker-Based Systems Load Balancing

Worker-based systems are an alternative to traditional load balancing mechanisms where a pool of worker instances process tasks from a shared backlog. These workers can be employed for batch work, asynchronous jobs, and handling peak loads through dynamic scaling.

This model is particularly useful for tasks such as image thumbnail processing, sending emails, or generating reports. The key benefit is improved resiliency since failing workers do not result in lost data; only delays in task completion are experienced.

If the work queue itself is resilient (e.g., using a message broker like Kafka), worker-based systems can scale both for increased throughput and better fault tolerance.
:p How does a worker-based system handle tasks compared to traditional load balancing?
??x
A worker-based system processes tasks from a shared backlog where multiple instances of workers collaborate. Unlike traditional load balancers that route requests to available servers, workers pull tasks from a queue and process them independently.

This approach is especially useful for batch work and asynchronous jobs, ensuring that the impact of a worker failing is minimal since tasks can be retried later.
x??",1242,"Worker-Based Systems Load balancing isn’t the only way to have multiple instances of your service share load and reduce fragility. Depending on the nature of the operations, a worker-based system coul...",qwen2.5:latest,2025-11-03 00:02:10,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Caching in HTTP,Scaling Architectures,"#### Scaling Architectures

As systems grow, initial architectures may become inadequate. According to Jeff Dean's principle, ""design for ~10× growth, but plan to rewrite before ~100×,"" scaling requires rethinking and rewriting parts of the architecture.

This often involves refactoring monolithic applications into microservices, selecting more scalable data stores, or adopting new technologies like event-driven architectures. The key is preparing for potential radical changes as the system evolves.
:p Why might a simple monolithic application need to be redesigned when handling increased load?
??x
A simple monolithic application may struggle with increased load due to its single-threaded nature and shared resources, leading to bottlenecks under higher concurrency.

To handle more substantial growth, you might split the monolith into microservices, each responsible for a specific task. This modular approach allows better scaling and resilience compared to a single large application.
x??",1001,"Worker-Based Systems Load balancing isn’t the only way to have multiple instances of your service share load and reduce fragility. Depending on the nature of the operations, a worker-based system coul...",qwen2.5:latest,2025-11-03 00:02:10,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Caching in HTTP,Gilt Case Study,"#### Gilt Case Study

Gilt started with a simple Rails application that managed well for two years but faced significant load as its business grew rapidly.

At a certain point, the existing architecture became insufficient, necessitating a redesign. The company eventually split their monolithic application into microservices to better manage different aspects of their e-commerce system.
:p What happened at Gilt when they reached a specific scaling threshold?
??x
Gilt's initial Rails application performed well for two years but struggled with increased load as the business expanded.

To handle this, they had to redesign the application by breaking it into microservices. This allowed them to manage different parts of their e-commerce system more efficiently and scale each component independently.
x??

---",814,"Worker-Based Systems Load balancing isn’t the only way to have multiple instances of your service share load and reduce fragility. Depending on the nature of the operations, a worker-based system coul...",qwen2.5:latest,2025-11-03 00:02:10,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Caching in HTTP,Rapid Experimentation and Scaling Considerations,"#### Rapid Experimentation and Scaling Considerations
Background context: When starting a new project, it is crucial to experiment rapidly without over-preparing for scale. Eric Ries' story illustrates that focusing on understanding demand can save time and resources compared to building for scale upfront.

:p What does the text suggest about how projects should be initiated in terms of scaling?
??x
The text suggests initiating projects by rapidly experimenting rather than preparing for massive scale at the outset. This approach allows teams to understand if there is any demand before investing heavily in scaling infrastructure. For instance, Eric Ries could have tested market interest by creating a dummy link and spent six months on a beach instead of building a product that no one used.

```java
public class RapidExperimentation {
    public static void main(String[] args) {
        // Simulate a quick test to check if there is any demand
        boolean hasDemand = true;  // Hypothetical check for market interest
        if (hasDemand) {
            System.out.println(""Proceed with development."");
        } else {
            System.out.println(""Explore other ideas or pivot."");
        }
    }
}
```
x??",1225,"This can be disastrous. At the start of a new project, we often don’t know exactly what we want to build, nor do we know if it will be successful. We need to be able to rapidly experiment, and underst...",qwen2.5:latest,2025-11-03 00:02:21,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Caching in HTTP,Scaling Stateless Microservices,"#### Scaling Stateless Microservices
Background context: Stateless microservices are easier to scale compared to those that store data. The focus is on handling increased read and write operations without maintaining state across requests.

:p How does scaling stateless microservices differ from stateful services?
??x
Scaling stateless microservices involves managing the number of instances or replicas based on load, whereas stateful services require additional considerations for state management and data consistency. Stateless services can be more straightforward to scale horizontally by adding more instances, while stateful services might need specialized technologies like distributed databases.

```java
public class StatelessMicroserviceScaling {
    public static void main(String[] args) {
        // Example of scaling stateless microservices
        int currentLoad = 50;  // Current number of requests per second
        if (currentLoad > 100) {  // Threshold for scaling
            System.out.println(""Spinning up additional instances."");
        } else {
            System.out.println(""Current load is manageable with existing resources."");
        }
    }
}
```
x??",1188,"This can be disastrous. At the start of a new project, we often don’t know exactly what we want to build, nor do we know if it will be successful. We need to be able to rapidly experiment, and underst...",qwen2.5:latest,2025-11-03 00:02:21,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Caching in HTTP,Availability vs. Durability of Data,"#### Availability vs. Durability of Data
Background context: It’s crucial to distinguish between the availability of a service and the durability of data stored within it. These are two independent aspects that require different strategies.

:p What does the text highlight about the separation of service availability and data durability?
??x
The text emphasizes that service availability refers to whether users can access the application, while data durability concerns the persistence of user data. For example, storing data in a resilient filesystem ensures data safety but may not guarantee service availability if the database fails. A standby replica ensures data safety but requires mechanisms to bring it back online.

```java
public class AvailabilityVersusDurability {
    public static void main(String[] args) {
        boolean isDatabaseDown = true;  // Hypothetical scenario
        String dataSafetyStrategy = ""Resilient filesystem"";  // Ensures data isn't lost
        if (isDatabaseDown) {
            System.out.println(""Data safety strategy: "" + dataSafetyStrategy);
            System.out.println(""Service availability may be compromised."");
        }
    }
}
```
x??",1189,"This can be disastrous. At the start of a new project, we often don’t know exactly what we want to build, nor do we know if it will be successful. We need to be able to rapidly experiment, and underst...",qwen2.5:latest,2025-11-03 00:02:21,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Caching in HTTP,Scaling for Reads in Databases,"#### Scaling for Reads in Databases
Background context: Many services are read-heavy, making efficient read scaling essential. Caching and read replicas are common strategies to manage high read loads.

:p How does the text suggest scaling reads in databases?
??x
The text suggests using caching and read replicas as effective strategies for managing high read loads. Caching stores frequently accessed data in memory to reduce database load, while read replicas distribute read operations across multiple copies of the database.

```java
public class ReadReplicaExample {
    public static void main(String[] args) {
        // Simulate reading from a primary and replica databases
        String primaryRead = ""SELECT * FROM products"";  // Query for read data
        String replicaRead = ""SELECT * FROM products"";  // Same query to another replica

        System.out.println(""Primary database: "" + primaryRead);
        System.out.println(""Replica database: "" + replicaRead);
    }
}
```
x??

---",1000,"This can be disastrous. At the start of a new project, we often don’t know exactly what we want to build, nor do we know if it will be successful. We need to be able to rapidly experiment, and underst...",qwen2.5:latest,2025-11-03 00:02:21,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Caching in HTTP,Eventually Consistent Systems,"#### Eventually Consistent Systems
Background context explaining the concept. Eventually consistent systems allow temporary inconsistencies to achieve scalability, but require handling these inconsistencies appropriately.
:p What is an eventually consistent system?
??x
An eventually consistent system allows for temporary data inconsistency across nodes or replicas to provide high availability and scalability. Developers must handle these inconsistencies when reading data from the system.
x??",496,"Such a setup is called  eventually consistent , and if you can handle the temporary inconsistency it is a fairly easy and common way to help scale systems. We’ll look into this in more depth shortly w...",qwen2.5:latest,2025-11-03 00:02:29,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Caching in HTTP,Using Read Replicas for Scaling Reads,"#### Using Read Replicas for Scaling Reads
Background context explaining how read replicas can be used to scale reads, providing a mechanism for improving performance without significant changes.
:p What is a read replica in database scaling?
??x
A read replica is an additional database node that mirrors the primary database. It helps distribute read operations and thus improves read scalability without significantly impacting write operations.
x??",452,"Such a setup is called  eventually consistent , and if you can handle the temporary inconsistency it is a fairly easy and common way to help scale systems. We’ll look into this in more depth shortly w...",qwen2.5:latest,2025-11-03 00:02:29,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Caching in HTTP,Sharding for Writes: Overview,"#### Sharding for Writes: Overview
Background context explaining how sharding can be used to scale writes by distributing data across multiple nodes, but with challenges in handling queries.
:p What is sharding?
??x
Sharding involves splitting the dataset into smaller parts and storing each part on a separate database node. This allows horizontal scaling of write operations by distributing the load across multiple nodes.
x??",428,"Such a setup is called  eventually consistent , and if you can handle the temporary inconsistency it is a fairly easy and common way to help scale systems. We’ll look into this in more depth shortly w...",qwen2.5:latest,2025-11-03 00:02:29,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Caching in HTTP,Handling Queries Across Shards,"#### Handling Queries Across Shards
Background context explaining the challenges of querying data across shards, including possible solutions like map/reduce jobs or caching.
:p How can you handle queries that span multiple shards?
??x
Queries spanning multiple shards can be handled using asynchronous mechanisms like map/reduce jobs in databases such as MongoDB. These jobs execute complex queries on each shard and then combine the results to provide a unified view.
x??",473,"Such a setup is called  eventually consistent , and if you can handle the temporary inconsistency it is a fairly easy and common way to help scale systems. We’ll look into this in more depth shortly w...",qwen2.5:latest,2025-11-03 00:02:29,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Caching in HTTP,Adding Shards to an Existing Cluster,"#### Adding Shards to an Existing Cluster
Background context explaining how adding more database nodes (shards) affects data distribution, with potential downtime issues addressed by some systems.
:p How can you add shards to an existing cluster?
??x
Adding shards to an existing cluster involves rebalancing the data across new and existing nodes. Some databases handle this asynchronously in the background, such as Cassandra, which can add shards without significant downtime.
x??",483,"Such a setup is called  eventually consistent , and if you can handle the temporary inconsistency it is a fairly easy and common way to help scale systems. We’ll look into this in more depth shortly w...",qwen2.5:latest,2025-11-03 00:02:29,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Caching in HTTP,Resiliency and Data Replication with Sharding,"#### Resiliency and Data Replication with Sharding
Background context explaining how sharding can affect resiliency and the importance of replicating data to multiple nodes for reliability.
:p What is the impact of sharding on resiliency?
??x
Sharding can reduce resiliency if not managed properly. For example, if customer records A–M are always written to a single instance, that instance being unavailable would mean losing access to those records. Cassandra addresses this by replicating data across multiple nodes in a ring.
x??",533,"Such a setup is called  eventually consistent , and if you can handle the temporary inconsistency it is a fairly easy and common way to help scale systems. We’ll look into this in more depth shortly w...",qwen2.5:latest,2025-11-03 00:02:29,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Caching in HTTP,Evaluating Database Technologies for Scaling Writes,"#### Evaluating Database Technologies for Scaling Writes
Background context explaining the challenges of scaling write volume and the considerations when choosing database technology.
:p When might you need to change your database technology?
??x
You might need to change your database technology when you hit limits on scaling write volume. Buying a bigger box is often quick, but evaluating systems like Cassandra, MongoDB, or Riak can offer better long-term solutions for scalable writes.
x??

---",500,"Such a setup is called  eventually consistent , and if you can handle the temporary inconsistency it is a fairly easy and common way to help scale systems. We’ll look into this in more depth shortly w...",qwen2.5:latest,2025-11-03 00:02:29,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Caching in HTTP,Shared Database Infrastructure,"---
#### Shared Database Infrastructure
Background context explaining how traditional RDBMS separate databases and schemas, allowing multiple microservices to run on a single database. This setup reduces the number of machines needed but introduces significant risks due to potential single points of failure.

:p What is shared database infrastructure used for?
??x
Shared database infrastructure separates the concept of the database itself from the schema, enabling one running database to host multiple independent schemas, each serving a different microservice. This can reduce the need for multiple databases and servers but also increases the risk of a single point of failure if the database goes down.

x??",715,"Shared Database Infrastructure Some types of databases, such as the traditional RDBMS, separate the concept of the database itself and the schema. This means one running database could host multiple, ...",qwen2.5:latest,2025-11-03 00:02:37,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Caching in HTTP,Command-Query Responsibility Segregation (CQRS),"#### Command-Query Responsibility Segregation (CQRS)
Background context on CQRS, which separates commands for modifying state from queries used to retrieve data. It allows different models to handle scaling independently, potentially offering more flexibility in terms of how and where data is stored and accessed.

:p What does the CQRS pattern separate?
??x
The CQRS pattern separates commands (for modifying state) and queries (for retrieving data). These are processed by separate systems, allowing for different scaling strategies and storage models. For instance, commands can be event-sourced, while queries can use projections or a different type of store.

x??",669,"Shared Database Infrastructure Some types of databases, such as the traditional RDBMS, separate the concept of the database itself and the schema. This means one running database could host multiple, ...",qwen2.5:latest,2025-11-03 00:02:37,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Caching in HTTP,Event Sourcing,"#### Event Sourcing
Background context on how CQRS can implement event sourcing, where commands are stored as events, which can then be used to reconstruct the state of the domain objects. This approach allows for more flexible and scalable query models.

:p How does event sourcing work in CQRS?
??x
In CQRS with event sourcing, commands are recorded as events in a data store. These events can later be processed to update the model or to generate projections that represent the current state of the domain objects. This process allows for maintaining a history of changes and reconstructing the current state.

Example code:
```java
public class EventSourcingService {
    private final List<Event> eventList;

    public void applyCommand(Command command) {
        // Apply the command by creating an event
        Event event = new Event(command.getDetails());
        eventList.add(event);
        
        // Optionally, update state or projections based on the event
    }
}
```

x??",992,"Shared Database Infrastructure Some types of databases, such as the traditional RDBMS, separate the concept of the database itself and the schema. This means one running database could host multiple, ...",qwen2.5:latest,2025-11-03 00:02:37,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Caching in HTTP,Caching,"#### Caching
Background context explaining how caching can be used to store previous results of operations and serve them faster in subsequent requests. It is commonly used to reduce database round-trips and improve performance.

:p What is caching used for?
??x
Caching stores the result of expensive or frequent operations, so that future calls with the same parameters can retrieve the result from cache instead of recalculating it. This reduces the load on underlying systems like databases and improves response times.

Example code:
```java
public class CacheManager {
    private Map<String, String> cache = new HashMap<>();

    public String getCachedValue(String key) {
        return cache.getOrDefault(key, fetchFromDatabase(key));
    }

    private String fetchFromDatabase(String key) {
        // Simulate fetching from a database or other service
        return ""value"";
    }
}
```

x??
---",908,"Shared Database Infrastructure Some types of databases, such as the traditional RDBMS, separate the concept of the database itself and the schema. This means one running database could host multiple, ...",qwen2.5:latest,2025-11-03 00:02:37,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Caching in HTTP,Client-Side Caching,"#### Client-Side Caching
Client-side caching involves storing cached results directly on the client's device, allowing it to decide when and if to fetch a fresh copy. This can significantly reduce network calls, leading to performance benefits. However, managing stale data invalidation and rolling out changes can be challenging.
:p What is client-side caching?
??x
Client-side caching refers to storing cached results directly on the client's device. The client determines whether to retrieve a fresh copy, which can drastically reduce network calls and alleviate load on downstream services. However, invalidating stale data and making changes to caching behavior across multiple consumers might be difficult.
x??",716,"Used well, it can yield huge performance benefits. The reason that HTTP scales so well in handling large numbers of requests is that the concept of caching is built in. Even with a simple monolithic w...",qwen2.5:latest,2025-11-03 00:02:47,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Caching in HTTP,Proxy Caching,"#### Proxy Caching
Proxy caching involves placing a proxy between the client and server. This setup can cache traffic from various services, providing a transparent way to add caching without modifying clients or servers directly. Reverse proxies like Squid or Varnish are common examples.
:p What is proxy caching?
??x
Proxy caching places a proxy between the client and server, handling caching for multiple services in an opaque manner. This can be simpler than changing existing systems, as it caches generic traffic that passes through it. However, adding additional network hops should be considered.
x??",610,"Used well, it can yield huge performance benefits. The reason that HTTP scales so well in handling large numbers of requests is that the concept of caching is built in. Even with a simple monolithic w...",qwen2.5:latest,2025-11-03 00:02:47,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Caching in HTTP,Server-Side Caching,"#### Server-Side Caching
Server-side caching involves storing cached data on servers using tools like Redis or Memcache. It offloads the client from managing cache invalidation and reduces load on downstream services by keeping frequently accessed data closer to the source.
:p What is server-side caching?
??x
Server-side caching stores cached results on servers, often utilizing systems like Redis or Memcache. This approach keeps frequently accessed data close to the service, reducing load on downstream services and simplifying invalidation of stale data within a single service boundary.
x??",597,"Used well, it can yield huge performance benefits. The reason that HTTP scales so well in handling large numbers of requests is that the concept of caching is built in. Even with a simple monolithic w...",qwen2.5:latest,2025-11-03 00:02:47,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Caching in HTTP,Mixed Caching Approaches,"#### Mixed Caching Approaches
For public-facing websites, combining client-side, proxy caching, and server-side caching is common. Each method addresses different needs, such as reducing network calls, adding transparency through proxies, or handling cache invalidation locally.
:p How do you typically approach caching in a public-facing website?
??x
Typically, public-facing websites use a combination of client-side, proxy, and server-side caching. Client-side caching reduces network load, while proxy caching adds transparency by caching generic traffic. Server-side caching handles local invalidation and keeps frequently accessed data close to the service.
x??",667,"Used well, it can yield huge performance benefits. The reason that HTTP scales so well in handling large numbers of requests is that the concept of caching is built in. Even with a simple monolithic w...",qwen2.5:latest,2025-11-03 00:02:47,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Caching in HTTP,No Caching Strategy,"#### No Caching Strategy
In some distributed systems, no caching strategy might be sufficient if the system can handle the current load without performance bottlenecks. The decision on whether or not to cache depends on factors such as required data freshness and current system capabilities.
:p In what scenarios might you avoid using a caching strategy?
??x
Avoiding a caching strategy in distributed systems is appropriate when the system can handle its current load without performance issues. This might be the case if the application doesn't experience high traffic or strict requirements for data freshness, making caching unnecessary and potentially complicating the architecture.
x??",692,"Used well, it can yield huge performance benefits. The reason that HTTP scales so well in handling large numbers of requests is that the concept of caching is built in. Even with a simple monolithic w...",qwen2.5:latest,2025-11-03 00:02:47,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Caching in HTTP,Key Considerations for Caching,"#### Key Considerations for Caching
The choice of caching strategy depends on factors like expected load, data freshness needs, and current system capabilities. Each approach (client-side, proxy, server-side) has its pros and cons in terms of performance, management complexity, and invalidation mechanisms.
:p What key factors should you consider when choosing a caching strategy?
??x
Key considerations for choosing a caching strategy include expected load, required data freshness, and current system capabilities. Each approach—client-side, proxy, server-side—has distinct advantages (performance benefits, ease of implementation) and challenges (complexity in invalidation, additional network hops). Understanding these factors helps in selecting the most appropriate caching mechanism.
x??

---",800,"Used well, it can yield huge performance benefits. The reason that HTTP scales so well in handling large numbers of requests is that the concept of caching is built in. Even with a simple monolithic w...",qwen2.5:latest,2025-11-03 00:02:47,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Caching in HTTP,Cache-Control Directives,"---
#### Cache-Control Directives
Background context explaining how `cache-control` directives work. These headers are used by servers to instruct clients on whether and for how long a resource should be cached.

:p What are `cache-control` directives, and what do they control?
??x
`cache-control` directives allow server-side configuration of caching behavior. They inform the client about whether to cache a resource and for how long. This can significantly reduce the number of requests made to the server by serving stale content from the cache.

Example usage in an HTTP response:
```http
Cache-Control: max-age=3600, public
```
This tells clients that they should cache this resource for 1 hour (`max-age=3600`) and allows them to share (publish) the cached copy with other users.

x??",792,"Caching in HTTP HTTP provides some really useful controls to help us cache either on the client side or server side, which are worth understanding even if you aren’t using HTTP itself. First, with HTT...",qwen2.5:latest,2025-11-03 00:02:54,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Caching in HTTP,Expires Header,"#### Expires Header
Background context explaining the `Expires` header. Unlike `cache-control`, which specifies a time-to-live, the `Expires` header sets an absolute date and time after which a resource should be considered stale.

:p How does the `Expires` header differ from `Cache-Control` directives in managing cache freshness?
??x
The `Expires` header provides a specific timestamp when a cached resource becomes outdated. Unlike `cache-control`, it gives clients a fixed date and time to fetch the latest version of the content.

Example usage:
```http
Expires: Thu, 01 Dec 2022 16:00:00 GMT
```
This sets an absolute expiration date for the resource. After this timestamp, clients should revalidate or fetch a fresh copy from the server.

x??",750,"Caching in HTTP HTTP provides some really useful controls to help us cache either on the client side or server side, which are worth understanding even if you aren’t using HTTP itself. First, with HTT...",qwen2.5:latest,2025-11-03 00:02:54,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Caching in HTTP,ETags and Conditional GETs,"#### ETags and Conditional GETs
Background context explaining Entity Tags (ETags) and how they are used in conditional requests. ETags allow servers to identify unique versions of resources, enabling efficient updates without changing the URI.

:p What is an ETag, and when would you use it?
??x
An ETag is a unique identifier for a version of a resource. It helps determine if the cached content has been updated since the last request. Using ETags in conditional GET requests (via `If-None-Match` header) ensures that clients only receive new or updated resources.

Example usage:
```http
GET /customer-record HTTP/1.1
Host: example.com
If-None-Match: o5t6fkd2sa
```
This request checks if the ETag of the customer record matches `o5t6fkd2sa`. If it does, a `304 Not Modified` is returned; otherwise, a new resource and updated ETag are sent.

x??
---",853,"Caching in HTTP HTTP provides some really useful controls to help us cache either on the client side or server side, which are worth understanding even if you aren’t using HTTP itself. First, with HTT...",qwen2.5:latest,2025-11-03 00:02:54,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Autoscaling,Caching for Writes,"#### Caching for Writes
Caching can be beneficial for writes, particularly through a technique called write-behind caching. This approach involves writing data to a local cache and flushing it to a downstream source later. It's useful when dealing with bursts of writes or situations where the same data might be written multiple times.
Write-behind caches can also buffer and batch writes, providing performance optimizations. If the downstream service is unavailable, buffered writes can be queued up and sent through when the service becomes available again.

:p What are the benefits of using write-behind caching?
??x
Using write-behind caching offers several benefits:
1. **Performance Optimization**: By buffering writes in a local cache before sending them to the downstream source, you reduce the load on the network and can batch multiple writes.
2. **Resilience Against Downtime**: If the downstream service is unavailable, previously buffered writes can still be sent through when it becomes available again.
3. **Handling Write Bursts**: It helps manage sudden spikes in write traffic by smoothing out these bursts.

This approach ensures that data updates are not lost even if the downstream service is temporarily down.

x??",1239,"Caching for Writes Although you’ll find yourself using caching for reads more often, there are some use cases where caching for writes make sense. For example, if you make use of a write-behind cache,...",qwen2.5:latest,2025-11-03 00:03:03,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Autoscaling,Caching for Resilience,"#### Caching for Resilience
Client-side caching can enhance system resilience, especially when dealing with potential failures. When a downstream service is unavailable, cached but potentially stale data can be used to serve requests.
Additionally, techniques like periodic crawling of live sites and generating static versions can ensure that even if the main site goes down, a version of the website remains available.

:p How does client-side caching improve system resilience?
??x
Client-side caching improves system resilience by providing fallback options when the downstream service is unavailable:
1. **Stale Data Serves**: When the cache contains stale but cached data, it can be served to clients, ensuring that no requests are left unanswered.
2. **Static Site Generation**: Techniques such as periodic crawling of a live site and generating static versions can ensure that even if the main system fails, a version of the site remains available.

This approach ensures high availability with potentially stale but usable data, improving overall system reliability.

x??",1080,"Caching for Writes Although you’ll find yourself using caching for reads more often, there are some use cases where caching for writes make sense. For example, if you make use of a write-behind cache,...",qwen2.5:latest,2025-11-03 00:03:03,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Autoscaling,Hiding the Origin,"#### Hiding the Origin
In scenarios where a cache failure leads to a large number of requests hitting the origin, it's important to protect the origin from overwhelming traffic. One effective strategy is to have the origin itself populate the cache asynchronously when needed.
This way, if there's a cache miss, the origin can be alerted and start repopulating the cache in the background.

:p How does hiding the origin behind caching help?
??x
Hiding the origin behind caching helps by protecting it from overwhelming traffic during cache failures:
1. **Asynchronous Repopulation**: When a cache miss occurs, the origin is notified to populate the cache asynchronously.
2. **Background Processing**: This allows for background processing of cache requests, preventing a sudden surge in load on the origin.

By ensuring that the origin only handles necessary requests and deferring others to background processes, the system can remain stable and responsive.

x??

---",969,"Caching for Writes Although you’ll find yourself using caching for reads more often, there are some use cases where caching for writes make sense. For example, if you make use of a write-behind cache,...",qwen2.5:latest,2025-11-03 00:03:03,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Autoscaling,Hiding the Origin from the Client and Populating Cache Asynchronously,"#### Hiding the Origin from the Client and Populating Cache Asynchronously
Background context explaining how hiding the origin from the client can be a strategy to ensure system resilience. By failing requests fast, avoiding resource usage or latency increases, and ensuring cache failures do not cascade downstream, it gives the system a chance to recover.

:p How does hiding the origin from the client contribute to system resilience?
??x
Hiding the origin from the client helps in maintaining system availability by quickly failing requests that encounter issues. This prevents the failure from consuming resources or increasing latency, which could otherwise propagate and affect other parts of the system. If a cache fails, the request is served directly from the source, ensuring that downstream components are not burdened with failed cache requests.
x??",862,"Figure 11-7. Hiding the origin from the client and populating the cache asynchronously This sort of approach may not make sense for some situations, but it can be a way to ensure the system remains up...",qwen2.5:latest,2025-11-03 00:03:13,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Autoscaling,Failing Requests Fast,"#### Failing Requests Fast
Background context on how failing requests quickly can help prevent resource usage and reduce latency, thereby maintaining overall system performance even when parts of the system fail.

:p Why should failures be handled by immediately failing requests?
??x
Failing requests fast helps in not consuming unnecessary resources or increasing latency. By ensuring that a failed request does not wait for additional processing, the system can quickly recover and handle subsequent requests more efficiently. This approach minimizes the impact of transient errors and ensures that other parts of the system remain responsive.
x??",650,"Figure 11-7. Hiding the origin from the client and populating the cache asynchronously This sort of approach may not make sense for some situations, but it can be a way to ensure the system remains up...",qwen2.5:latest,2025-11-03 00:03:13,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Autoscaling,Caching Considerations,"#### Caching Considerations
Background context on the importance of keeping caching simple to avoid data staleness issues in complex architectures like microservices.

:p Why is it important to keep caching simple?
??x
Keeping caching simple reduces the complexity and potential for errors, making it easier to manage and maintain. In a microservice architecture where multiple services are involved, having too many caches can make it difficult to track and ensure data freshness. A simpler cache design allows for better control over how and when data is cached, reducing the risk of serving stale or incorrect data.
x??",622,"Figure 11-7. Hiding the origin from the client and populating the cache asynchronously This sort of approach may not make sense for some situations, but it can be a way to ensure the system remains up...",qwen2.5:latest,2025-11-03 00:03:13,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Autoscaling,Cache Poisoning,"#### Cache Poisoning
Background context on the risks associated with cache poisoning, particularly in scenarios where multiple layers of caching exist between a user and the source of fresh data.

:p What is cache poisoning, and why is it problematic?
??x
Cache poisoning occurs when cached data is altered to serve stale or incorrect data. This can be particularly problematic in architectures with multiple caches, such as CDNs, ISPs, and user browsers, where controlling these layers can be challenging. If a cache is not properly invalidated, stale data can persist indefinitely, leading to issues for users.
x??",616,"Figure 11-7. Hiding the origin from the client and populating the cache asynchronously This sort of approach may not make sense for some situations, but it can be a way to ensure the system remains up...",qwen2.5:latest,2025-11-03 00:03:13,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Autoscaling,Example of Cache Poisoning Scenario,"#### Example of Cache Poisoning Scenario
Background context on an actual scenario where cache poisoning led to serving stale data due to misconfigured HTTP headers.

:p Describe the scenario where cache poisoning caused serving stale data?
??x
In a project using a strangler application to intercept calls to multiple legacy systems, a small subset of pages started serving stale data indefinitely. This happened because a bug introduced in the application logic resulted in not setting proper HTTP cache headers on some responses. The downstream application had an `Expires: Never` header that was not overridden by our application's headers. As a result, these pages remained cached indefinitely in Squid and user browsers until manually cleared or replaced.
x??",764,"Figure 11-7. Hiding the origin from the client and populating the cache asynchronously This sort of approach may not make sense for some situations, but it can be a way to ensure the system remains up...",qwen2.5:latest,2025-11-03 00:03:13,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Autoscaling,Clearing Cache to Resolve Cache Poisoning,"#### Clearing Cache to Resolve Cache Poisoning
Background context on the steps taken to resolve cache poisoning issues, including code fixes and manual cache clearing.

:p How was the cache poisoning issue resolved?
??x
To resolve the cache poisoning issue, the following steps were taken:
1. A fix was applied to the application's cache header insertion code.
2. A release was pushed to deploy the fix.
3. The relevant region of Squid cache was manually cleared to remove old data.

These actions helped mitigate the impact but did not fully resolve the problem since pages with `Expires: Never` headers were still cached in user browsers. The final solution involved changing the URLs of affected pages so that they would be refetched by users.
x??

---",755,"Figure 11-7. Hiding the origin from the client and populating the cache asynchronously This sort of approach may not make sense for some situations, but it can be a way to ensure the system remains up...",qwen2.5:latest,2025-11-03 00:03:13,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Autoscaling,Caching and Its Complexities,"#### Caching and Its Complexities
Caching is a powerful mechanism that can significantly improve performance by reducing access times to data. However, it requires careful management of the entire data path from source to destination to avoid potential issues such as stale or inconsistent data.

:p What are the key considerations for managing caching in distributed systems?
??x
The key considerations include ensuring data consistency across cache and database layers, handling cache invalidation policies effectively, and understanding how changes propagate through the system. You need to design your caching strategy based on the specific application requirements and data access patterns.
```java
public class CacheManager {
    public void invalidateCache(String key) {
        // Code to invalidate cache entry for a given key
    }
}
```
x??",851,"Caching can be very powerful indeed, but you need to understand the full path of data that is cached from source to destination to really appreciate its complexities and what can go wrong. Autoscaling...",qwen2.5:latest,2025-11-03 00:03:22,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Autoscaling,Autoscaling Fundamentals,"#### Autoscaling Fundamentals
Autoscaling allows you to automatically adjust the number of active instances based on demand. This is particularly useful in managing cost and ensuring optimal performance.

:p What are some scenarios where autoscaling can be effectively used?
??x
Autoscaling can be effective for managing load variations, such as daily or seasonal trends, and responding quickly to sudden spikes in traffic. For example, scaling up instances during peak hours and downgrading them when demand decreases helps in maintaining optimal performance while controlling costs.
```java
public class Autoscaler {
    public void scaleUp(int desiredInstances) {
        // Code to launch additional instances
    }

    public void scaleDown() {
        // Code to terminate excess instances
    }
}
```
x??",812,"Caching can be very powerful indeed, but you need to understand the full path of data that is cached from source to destination to really appreciate its complexities and what can go wrong. Autoscaling...",qwen2.5:latest,2025-11-03 00:03:22,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Autoscaling,Reactive vs. Predictive Scaling,"#### Reactive vs. Predictive Scaling
Reactive scaling involves responding to changes in load or instance failures, while predictive scaling anticipates future needs based on historical data.

:p What are the advantages and disadvantages of reactive and predictive scaling?
??x
Reactive scaling allows for immediate response to unexpected events but can lead to overprovisioning during periods of low demand. Predictive scaling, using historical data, can prevent under-provisioning and reduce costs by maintaining optimal resource levels. However, it requires accurate forecasting and robust testing.
```java
public class LoadPredictor {
    public void predictLoad() {
        // Code to analyze historical load patterns
    }
}
```
x??",737,"Caching can be very powerful indeed, but you need to understand the full path of data that is cached from source to destination to really appreciate its complexities and what can go wrong. Autoscaling...",qwen2.5:latest,2025-11-03 00:03:22,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Autoscaling,Autoscaling for Failures,"#### Autoscaling for Failures
Using autoscaling to handle instance failures ensures that the system remains available by automatically replacing failed instances.

:p How does AWS manage instance failures through autoscaling?
??x
AWS allows you to set rules such as ""at least 5 instances in this group,"" ensuring that if one instance fails, a new one is automatically launched. This helps maintain system availability and reduces downtime due to individual instance failures.
```java
public class AwsAutoScaler {
    public void configureMinInstances(int minInstances) {
        // Code to set minimum instance requirement
    }
}
```
x??",638,"Caching can be very powerful indeed, but you need to understand the full path of data that is cached from source to destination to really appreciate its complexities and what can go wrong. Autoscaling...",qwen2.5:latest,2025-11-03 00:03:22,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,Autoscaling,Cost Management with Autoscaling,"#### Cost Management with Autoscaling
Autoscaling can help in cost management by allowing you to pay only for the computing resources used. However, careful observation and data analysis are essential.

:p What is a critical step when implementing autoscaling to ensure cost efficiency?
??x
A critical step is setting up comprehensive load tests to validate scaling rules. Without proper testing, it's difficult to predict how well your system will handle varying loads, leading to potential overprovisioning or underprovisioning.
```java
public class LoadTester {
    public void testScalability(int[] expectedLoads) {
        // Code to simulate different load scenarios and test scaling behavior
    }
}
```
x??

---",719,"Caching can be very powerful indeed, but you need to understand the full path of data that is cached from source to destination to really appreciate its complexities and what can go wrong. Autoscaling...",qwen2.5:latest,2025-11-03 00:03:22,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,CAP Theorem,CAP Theorem Overview,"---
#### CAP Theorem Overview
The CAP theorem states that in a distributed system, we can only achieve two of three desirable properties: consistency, availability, and partition tolerance. Since Eric Brewer published his original conjecture, it has been mathematically proven.

:p What does the CAP theorem state about distributed systems?
??x
The CAP theorem indicates that in any distributed system, you can only ensure at most two out of the following three qualities: Consistency (C), Availability (A), and Partition Tolerance (P). This means that if a network partition occurs, it is impossible to guarantee all three properties simultaneously.
x??",654,"CAP Theorem We’d like to have it all, but unfortunately we know we can’t. And when it comes to distributed systems like those we build using microservice architectures, we even have a mathematical pro...",qwen2.5:latest,2025-11-03 00:03:30,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,CAP Theorem,Network Partition Scenario,"#### Network Partition Scenario
In the scenario described, we have an inventory service deployed across two separate data centers. Each data center has its own database node, and these nodes try to synchronize their data.

:p What happens when the network link between the two data centers fails?
??x
When the network link between the two data centers fails, synchronization stops. Writes made to one database may not propagate to the other database. This leads to a situation where the databases become out of sync.
x??",520,"CAP Theorem We’d like to have it all, but unfortunately we know we can’t. And when it comes to distributed systems like those we build using microservice architectures, we even have a mathematical pro...",qwen2.5:latest,2025-11-03 00:03:30,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,CAP Theorem,Consistency and Availability in Failures,"#### Consistency and Availability in Failures
In the scenario with failed network links, reads and writes are done via the local database node, but replication is used to synchronize data between nodes.

:p What trade-offs might occur due to a network partition in this setup?
??x
Due to the network partition, you may have to trade off consistency and availability. For example:
- Consistency: Writes made to one database may not be reflected immediately in the other database.
- Availability: Since writes are only acknowledged by the local node, they might appear successful even if replication is delayed or fails.

To handle this, most databases support queuing techniques to ensure eventual consistency after network recovery.
x??",736,"CAP Theorem We’d like to have it all, but unfortunately we know we can’t. And when it comes to distributed systems like those we build using microservice architectures, we even have a mathematical pro...",qwen2.5:latest,2025-11-03 00:03:30,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,CAP Theorem,Queuing Technique for Recovery,"#### Queuing Technique for Recovery
Queuing mechanisms can be used to ensure that data can still be written and recovered from during a network failure. These queues allow writes to continue despite partition issues.

:p How do queuing techniques help in maintaining availability during network partitions?
??x
Queuing techniques provide a way to maintain availability by allowing writes to succeed even when replication is delayed or failed. Once the network recovers, these queued writes can be replayed to ensure eventual consistency.
x??",541,"CAP Theorem We’d like to have it all, but unfortunately we know we can’t. And when it comes to distributed systems like those we build using microservice architectures, we even have a mathematical pro...",qwen2.5:latest,2025-11-03 00:03:30,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,CAP Theorem,Handling Network Partitions,"#### Handling Network Partitions
Handling network partitions involves ensuring that the system can continue operating despite communication failures between parts of the system.

:p What are some strategies for maintaining availability during a network partition?
??x
Strategies include:
- Using queuing mechanisms to buffer writes until network recovery.
- Implementing optimistic replication where data is written locally and then propagated later.

These techniques help ensure that the system remains available, even when partitions occur.
x??",547,"CAP Theorem We’d like to have it all, but unfortunately we know we can’t. And when it comes to distributed systems like those we build using microservice architectures, we even have a mathematical pro...",qwen2.5:latest,2025-11-03 00:03:30,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,CAP Theorem,Example of Multiprimary Replication,"#### Example of Multiprimary Replication
Multiprimary replication involves setting up multiple primary nodes in different locations. This allows for continued operation despite network failures.

:p How does multiprimary replication affect data consistency and availability?
??x
Multiprimary replication can improve both availability and partition tolerance by allowing writes to any of the primary nodes, but it may reduce strict consistency guarantees as conflicts might arise if not managed properly.
x??

---",512,"CAP Theorem We’d like to have it all, but unfortunately we know we can’t. And when it comes to distributed systems like those we build using microservice architectures, we even have a mathematical pro...",qwen2.5:latest,2025-11-03 00:03:30,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,DNS,AP System,"---
#### AP System
In a distributed system, we may face partitions where network failures occur between database nodes. If we choose to keep availability and partition tolerance (AP), changes made on one node may not be immediately reflected on another due to the partition. This results in potentially stale data being served.
:p What is an AP system?
??x
An AP system, or an Availability-Partition Tolerance system, prioritizes availability and partition tolerance over consistency. In such a scenario, if there's a network failure between database nodes (DC1 and DC2), the node that doesn't receive updates will continue to serve potentially stale data until synchronization occurs.
```java
// Pseudocode for handling AP in a distributed system
public void handleAPPartition() {
    // Check if the current node can communicate with another node
    boolean canCommunicate = checkNetworkStatus();
    
    if (!canCommunicate) {
        // If communication is lost, accept stale data to ensure availability and partition tolerance
        serveStaleData();
    } else {
        // If communication is restored, synchronize data across nodes
        synchronizeData();
    }
}
```
x??",1186,"Sacrificing Consistency Let’s assume that we don’t shut the inventory service down entirely. If I make a change now to the data in DC1, the database in DC2 doesn’t see it. This means any requests made...",qwen2.5:latest,2025-11-03 00:03:41,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,DNS,Eventually Consistent Systems,"#### Eventually Consistent Systems
To maintain high availability in the face of network partitions, systems often adopt eventual consistency. This means that after a series of updates and potential delays in replication, all nodes will eventually reflect the same state. However, during the delay period, users might experience stale data.
:p What is an eventually consistent system?
??x
An eventually consistent system accepts reduced immediate consistency to maintain high availability (A) and partition tolerance (P). The key idea here is that updates will propagate through the network over time, but not instantaneously. Users may temporarily encounter old or inconsistent data until synchronization occurs.
```java
// Pseudocode for handling eventual consistency
public void updateDataEventuallyConsistent(String data) {
    // Update local copy of data immediately
    localDatabase.update(data);
    
    // Schedule an asynchronous replication task to other nodes
    scheduleReplicationTask();
}
```
x??",1013,"Sacrificing Consistency Let’s assume that we don’t shut the inventory service down entirely. If I make a change now to the data in DC1, the database in DC2 doesn’t see it. This means any requests made...",qwen2.5:latest,2025-11-03 00:03:41,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,DNS,CP System,"#### CP System
To ensure strong consistency across all nodes, the system must sacrifice availability (A) and may experience downtime during network partitions. This mode ensures that once a node is updated, all replicas will eventually reflect those updates.
:p What is a CP system?
??x
A CP (Consistency-Partition Tolerance) system prioritizes consistency over availability and partition tolerance. In the presence of a network failure, the system may temporarily shut down or limit its functionality to ensure that once an update is made, all nodes will eventually reflect it. This approach sacrifices availability during the partition period.
```java
// Pseudocode for handling CP in a distributed system
public void handleCPPartition() {
    // Check if the current node can communicate with another node
    boolean canCommunicate = checkNetworkStatus();
    
    if (!canCommunicate) {
        // If communication is lost, block all write requests until synchronization is possible
        rejectWriteRequests();
    } else {
        // If communication is restored, process and replicate writes to other nodes
        processAndReplicateWrites();
    }
}
```
x??",1169,"Sacrificing Consistency Let’s assume that we don’t shut the inventory service down entirely. If I make a change now to the data in DC1, the database in DC2 doesn’t see it. This means any requests made...",qwen2.5:latest,2025-11-03 00:03:41,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,DNS,Transactional Reads in Distributed Systems,"#### Transactional Reads in Distributed Systems
Consistency across multiple nodes in distributed systems requires coordination through transactions. However, initiating a transactional read involves locking mechanisms that can cause significant delays and potential deadlocks.
:p What are the challenges with transactional reads in distributed systems?
??x
Transactional reads in distributed systems pose significant challenges due to the need for cross-node coordination via locks. Ensuring that a read operation does not conflict with an ongoing write operation across multiple nodes is complex, especially given network partitions or failures. This can lead to performance issues and potential deadlocks.
```java
// Pseudocode for initiating a transactional read
public String performTransactionalRead(String key) {
    // Attempt to lock the remote node while reading from local node
    if (tryLockRemoteNode(key)) {
        try {
            return localDatabase.read(key);
        } finally {
            unlockRemoteNode(key); // Ensure the lock is released even on failure
        }
    } else {
        throw new LockAcquisitionException(""Could not acquire lock for transactional read"");
    }
}
```
x??

---",1218,"Sacrificing Consistency Let’s assume that we don’t shut the inventory service down entirely. If I make a change now to the data in DC1, the database in DC2 doesn’t see it. This means any requests made...",qwen2.5:latest,2025-11-03 00:03:41,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,DNS,Distributed Transactions and CAP Theorem,"#### Distributed Transactions and CAP Theorem
Background context: This section discusses the challenges of implementing distributed transactions, focusing on the CAP theorem. CAP stands for Consistency, Availability, and Partition Tolerance. The core issue is ensuring consistency across multiple nodes when a network partition occurs. In practice, it's highly advised not to invent your own distributed consistent data store but rather use existing solutions.
:p What does CAP stand for in the context of distributed systems?
??x
CAP stands for Consistency, Availability, and Partition Tolerance. These are three critical properties that a distributed system can have at most two out of these three at any given time according to the CAP theorem.
```java
// Example pseudocode to illustrate the concept
public class Node {
    public void handleRequest() {
        // Handle request logic here
    }
}
```
x??",910,Remember when we talked about distributed transactions in  Chapter 5 ? The core reason they are challenging is because of this problem with ensuring consistency across multiple nodes. Getting multinod...,qwen2.5:latest,2025-11-03 00:03:51,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,DNS,Importance of Using Existing Solutions for Distributed Transactions,"#### Importance of Using Existing Solutions for Distributed Transactions
Background context: The text emphasizes that inventing your own distributed consistent data store is highly risky and should be avoided. Instead, it's better to use existing solutions or aim for eventually consistent AP systems.
:p Why does the author strongly advise against inventing a CP (Consistency, Partition Tolerance) system?
??x
The author advises against inventing a CP system because building such a system from scratch is extremely challenging and risky. It requires deep understanding and expertise, often involving reading numerous research papers, obtaining a PhD, and still being prone to errors. Using existing solutions like Consul for a strongly consistent key/value store can significantly reduce these risks.
```java
// Example pseudocode for using an existing solution
public class DataStoreClient {
    public void useExistingSolution() {
        // Code to interact with an existing data store
    }
}
```
x??",1006,Remember when we talked about distributed transactions in  Chapter 5 ? The core reason they are challenging is because of this problem with ensuring consistency across multiple nodes. Getting multinod...,qwen2.5:latest,2025-11-03 00:03:51,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,DNS,Trade-Off Between AP and CP Systems,"#### Trade-Off Between AP and CP Systems
Background context: The text discusses the trade-offs between AP (Availability, Partition Tolerance) and CP (Consistency, Partition Tolerance) systems. It highlights that there is no one-size-fits-all solution and that understanding these trade-offs is crucial for making informed decisions.
:p What does the CAP theorem suggest about system design?
??x
The CAP theorem suggests that in a distributed system, you can have at most two out of the three properties: Consistency, Availability, and Partition Tolerance. This means that depending on your requirements, you might need to sacrifice one property for the others.
```java
// Example pseudocode illustrating trade-offs
public class SystemDesign {
    public void chooseSystemType() {
        // Logic to decide between AP or CP based on business needs
    }
}
```
x??",863,Remember when we talked about distributed transactions in  Chapter 5 ? The core reason they are challenging is because of this problem with ensuring consistency across multiple nodes. Getting multinod...,qwen2.5:latest,2025-11-03 00:03:51,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,DNS,Microservices and CAP Theorem,"#### Microservices and CAP Theorem
Background context: The text concludes by discussing how individual microservices within a system can have different requirements for consistency. It emphasizes that while the entire system might be designed as an eventually consistent AP system, certain critical services may need to ensure consistency.
:p How can a system design accommodate both AP and CP needs?
??x
A system can accommodate both AP and CP needs by designing individual microservices based on their specific requirements. For example, non-critical services like a catalog might be designed as an eventually consistent AP system, while critical services like inventory might need to ensure consistency.
```java
// Example pseudocode for service design
public class Microservice {
    public void designService() {
        // Logic to determine if the service needs to be CP or AP
    }
}
```
x??",899,Remember when we talked about distributed transactions in  Chapter 5 ? The core reason they are challenging is because of this problem with ensuring consistency across multiple nodes. Getting multinod...,qwen2.5:latest,2025-11-03 00:03:51,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,DNS,Understanding the Trade-Offs in Practice,"#### Understanding the Trade-Offs in Practice
Background context: The text underscores that understanding the trade-offs between AP and CP is essential. It advises that without knowing the specific business impact, it's difficult to make an informed decision about which system type to use.
:p How can businesses understand the trade-offs better?
??x
Businesses can understand the trade-offs better by thoroughly analyzing their specific requirements and the potential impacts of each design choice on their operations. This includes understanding how stale data or inconsistent records might affect different parts of their business processes.
```java
// Example pseudocode for decision-making
public class BusinessAnalysis {
    public void analyzeRequirements() {
        // Code to analyze business impact and make informed decisions
    }
}
```
x??

---",858,Remember when we talked about distributed transactions in  Chapter 5 ? The core reason they are challenging is because of this problem with ensuring consistency across multiple nodes. Getting multinod...,qwen2.5:latest,2025-11-03 00:03:51,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,DNS,Trade-Offs in CAP Theorem,"#### Trade-Offs in CAP Theorem
Background context explaining the concept. The CAP theorem states that a distributed system can exhibit at most two of the following properties: Consistency, Availability, and Partition Tolerance. However, real-world systems often allow for more nuanced trade-offs within individual service capabilities.
:p What is the main idea discussed regarding CAP in this text?
??x
The text explains that while the CAP theorem sets a mathematical limit on what can be achieved in distributed systems—stating that you can only have at most two of Consistency, Availability, and Partition Tolerance—it highlights that many systems allow for more flexible trade-offs within their services. For instance, in Apache Cassandra, different consistency levels (strict, quorum, or single node) are available based on the specific needs.
x??",851,"Really, what we have done is push the trade-offs around the CAP theorem down to individual service capabilities. Another complexity is that neither consistency nor availability is all or nothing. Many...",qwen2.5:latest,2025-11-03 00:04:02,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,DNS,Different Consistency Levels in Cassandra,"#### Different Consistency Levels in Cassandra
Background context explaining the concept. Cassandra offers various consistency levels to balance between strict consistency and availability in a distributed environment.
:p How does Cassandra allow for nuanced trade-offs between consistency and availability?
??x
Cassandra allows for different types of reads that enable varying degrees of consistency:

- **Strict Consistency**: Blocks until all replicas have responded with the same value, ensuring eventual consistency but potentially leading to long blocking times if one replica is unavailable.
```java
// Pseudocode for a strict consistency read in Cassandra
ConsistencyLevel ALL = ConsistencyLevel.ONE;
session.execute(""SELECT * FROM inventory WHERE album_id = ? "", albumId, ALL);
```

- **Quorum Read**: Blocks until a quorum of replicas have responded, providing a balance between performance and consistency.
```java
// Pseudocode for a quorum read in Cassandra
ConsistencyLevel QUORUM = ConsistencyLevel.ONE;
session.execute(""SELECT * FROM inventory WHERE album_id = ? "", albumId, QUORUM);
```

- **Single Node Read**: Does not block until a quorum of replicas have responded; returns the first available node's response.
```java
// Pseudocode for a single node read in Cassandra
ConsistencyLevel ONE = ConsistencyLevel.ONE;
session.execute(""SELECT * FROM inventory WHERE album_id = ? "", albumId, ONE);
```
x??",1420,"Really, what we have done is push the trade-offs around the CAP theorem down to individual service capabilities. Another complexity is that neither consistency nor availability is all or nothing. Many...",qwen2.5:latest,2025-11-03 00:04:02,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,DNS,Real-World vs. Electronic World Consistency,"#### Real-World vs. Electronic World Consistency
Background context explaining the concept. The text contrasts how electronic systems handle consistency with the unpredictability of real-world events.
:p How do real-world scenarios challenge the consistency assumptions in electronic systems?
??x
Real-world scenarios often introduce inconsistencies that electronic systems cannot account for because they are designed to operate within controlled environments where stopping the world and ensuring perfect consistency is feasible. For example, physical inventory counts can be affected by unforeseen events such as damage or theft, whereas an electronic system might only decrement a count when a sale transaction is successfully processed.

To handle such inconsistencies, some systems opt for AP (Availability and Partition Tolerance) strategies where occasional discrepancies are acceptable in exchange for better availability and scalability. For instance:
```java
// Pseudocode demonstrating handling of inconsistent states in a real-world inventory system
if (physicalInventoryCount < electronicInventoryCount) {
    notifyUser(""Item out of stock, please contact support."");
}
```
x??",1191,"Really, what we have done is push the trade-offs around the CAP theorem down to individual service capabilities. Another complexity is that neither consistency nor availability is all or nothing. Many...",qwen2.5:latest,2025-11-03 00:04:02,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,DNS,Service Discovery in Microservices,"#### Service Discovery in Microservices
Background context explaining the concept. As microservices architectures scale, managing where services are running and how to locate them becomes increasingly important for monitoring and usage.
:p What is service discovery in a microservices environment?
??x
Service discovery involves mechanisms that help distributed systems find and communicate with other services. In a microservices architecture, this is critical because services can be deployed dynamically across multiple instances or even different environments.

Service discovery tools like Consul, Eureka, or Kubernetes Service Discovery are commonly used to manage the locations of running services so that consuming microservices can reliably connect to them.
```java
// Pseudocode for a simple service discovery in Kubernetes
Map<String, String> labels = new HashMap<>();
labels.put(""app"", ""inventory"");
DiscoveryClient discoveryClient = new DiscoveryClient();
List<ServiceInstance> instances = discoveryClient.getInstances(""inventory-service"");
ServiceInstance instance = instances.get(0);
String uri = instance.getUri().toString();
```
x??

---",1154,"Really, what we have done is push the trade-offs around the CAP theorem down to individual service capabilities. Another complexity is that neither consistency nor availability is all or nothing. Many...",qwen2.5:latest,2025-11-03 00:04:02,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,DNS,DNS Service Discovery,"#### DNS Service Discovery
Background context explaining the concept. DNS allows associating a name with an IP address, facilitating service discovery by using names instead of IPs. It is particularly useful for managing different environments (e.g., dev, uat, prod) through templates or separate domain name servers.

:p What are some advantages and disadvantages of using DNS for service discovery?
??x
DNS offers several advantages:
- Well-understood and widely supported.
- Easy to use with most technology stacks.

However, it also has notable downsides:
- Updating DNS entries can be challenging in dynamic environments.
- TTL (Time To Live) values mean clients might hold onto old IP addresses for a while.
- Caching at various levels (network devices, JVMs, etc.) can further complicate service updates.

DNS is better suited for static setups with single nodes or where load balancers are used to manage instances dynamically.
x??",939,"Or perhaps you just want to make it easy for developers in your organization to know what APIs are available so they don’t reinvent the wheel. Broadly speaking, all of these use cases fall under the b...",qwen2.5:latest,2025-11-03 00:04:11,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,DNS,DNS and Load Balancer Integration,"#### DNS and Load Balancer Integration
Background context explaining the concept. Integrating DNS with a load balancer allows dynamic management of services by updating only the load balancer's configuration, which in turn updates the traffic distribution.

:p How can using a load balancer improve service discovery when compared to directly resolving IP addresses?
??x
Using a load balancer improves service discovery in several ways:
- Simplifies managing multiple instances behind a single endpoint.
- Allows for easier scaling and deployment of new instances without changing DNS records frequently.
- Provides redundancy and fault tolerance.

Code Example: Configuration for a hypothetical load balancer (e.g., Nginx) to route traffic to different backend services.
```nginx
http {
    upstream accounts_backend {
        server 192.0.2.1;
        server 192.0.2.2;
    }

    server {
        listen 80;

        location /accounts {
            proxy_pass http://accounts_backend;
        }
    }
}
```
x??",1014,"Or perhaps you just want to make it easy for developers in your organization to know what APIs are available so they don’t reinvent the wheel. Broadly speaking, all of these use cases fall under the b...",qwen2.5:latest,2025-11-03 00:04:11,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,DNS,DNS Round-Robining,"#### DNS Round-Robining
Background context explaining the concept. DNS round-robin involves distributing traffic across multiple servers by having a single DNS entry point to all backend services, which can be problematic if one of the backends becomes unavailable.

:p What are the main issues with using DNS round-robin for service discovery?
??x
The main issues with using DNS round-robin include:
- Clients may route requests to unhealthy or sick hosts.
- Lack of visibility into individual host health, making it difficult to reroute traffic away from a failing instance.
- Increased complexity in managing and troubleshooting network configurations.

Code Example: An example of how DNS round-robin might be implemented incorrectly (not recommended).
```dns
accounts.musiccorp.com.     IN A 192.0.2.1
accounts.musiccorp.com.     IN A 192.0.2.2
```
x??",857,"Or perhaps you just want to make it easy for developers in your organization to know what APIs are available so they don’t reinvent the wheel. Broadly speaking, all of these use cases fall under the b...",qwen2.5:latest,2025-11-03 00:04:11,
Sam-Newman---Building-Microservices-OReilly-Media-2015_processed,DNS,Conclusion on DNS for Service Discovery,"#### Conclusion on DNS for Service Discovery
Background context explaining the concept. While DNS is a reliable and well-understood method, it may not be the best fit for highly dynamic environments with frequently changing instances.

:p When might DNS still be a good choice for service discovery?
??x
DNS can still be a good choice when:
- The environment has static setups with no frequent deployments.
- There are few services that do not require high availability or load balancing.
- An existing infrastructure already supports DNS and the overhead of maintaining it is manageable.

However, in environments where instances are frequently deployed and destroyed, more advanced solutions like Consul may be necessary to ensure service discovery remains effective and up-to-date.
x??

---",793,"Or perhaps you just want to make it easy for developers in your organization to know what APIs are available so they don’t reinvent the wheel. Broadly speaking, all of these use cases fall under the b...",qwen2.5:latest,2025-11-03 00:04:11,
