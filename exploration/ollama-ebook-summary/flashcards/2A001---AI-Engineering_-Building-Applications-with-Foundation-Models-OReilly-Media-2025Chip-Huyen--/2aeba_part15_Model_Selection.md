# Flashcards: 2A001---AI-Engineering_-Building-Applications-with-Foundation-Models-OReilly-Media-2025Chip-Huyen--_processed (Part 15)

**Starting Chapter:** Model Selection. Model Selection Workflow

---

#### Model Selection Criteria
Background context explaining the importance of selecting models based on specific criteria for an application. The provided text outlines several key metrics and benchmarks that need to be considered, such as pass@1 HumanEval scores, factual consistency, and internal hallucination dataset performance.

:p What are some important metrics to consider when selecting a model?
??x
Some important metrics include pass@1 HumanEval score (benchmark should be >90%, ideal is >95%), factual consistency (internal GPT metric >0.8, internal hallucination dataset >0.9). These metrics help ensure the model performs well in real-world applications and maintains accuracy and reliability.
x??

---

#### Model Selection Workflow
Background context discussing the process of evaluating models for a specific application. The workflow involves filtering out unsuitable models based on hard attributes, narrowing down promising models using public information, running experiments with your own evaluation pipeline, and monitoring the model's performance in production.

:p What are the four steps involved in the model selection workflow?
??x
The four steps involved in the model selection workflow are:
1. Filter out models whose hard attributes don’t work for you.
2. Use publicly available information to narrow down promising models.
3. Run experiments with your own evaluation pipeline.
4. Continually monitor the model in production.

Each step helps ensure that the selected model meets the application's requirements and performs well over time.
x??

---

#### Hard vs Soft Attributes
Background context on differentiating between hard attributes, which are often determined by external factors or internal policies, and soft attributes, which can be improved through various techniques. Examples include model size, training data, privacy concerns, and performance optimization.

:p How do you differentiate between hard and soft attributes in the model selection process?
??x
In the model selection process, hard attributes are those that cannot be easily changed, such as licensing terms, training data sources, or internal policies regarding privacy. Soft attributes include elements like accuracy, toxicity, or factual consistency, which can potentially be improved through various techniques.

For example:
- Hard attribute: Using a model hosted by another company with fixed latency.
- Soft attribute: Optimizing the same model to reduce latency if you have access to it.

Understanding these distinctions helps in filtering and evaluating models more effectively based on your specific needs and constraints.
x??

---

#### Evaluation Workflow Overview
Background context explaining the high-level approach for evaluating AI systems, including filtering out unsuitable models, narrowing down promising candidates, conducting experiments, and monitoring performance post-deployment. The text provides a structured overview of these steps.

:p What is an overview of the evaluation workflow?
??x
The evaluation workflow involves:
1. Filtering out models whose hard attributes do not align with your needs.
2. Narrowing down promising models using benchmark performance and leaderboard rankings.
3. Running experiments with your own evaluation pipeline to find the best model.
4. Continually monitoring the model in production to detect failures and collect feedback.

This structured approach ensures that you select a model that meets your application's requirements while considering various factors such as cost, performance, and reliability.
x??

---

#### Public Training Data Argument
Background context explaining why making training data public is argued as important. The argument is that since models are often trained using data scraped from the internet, which was generated by the public, it's fair for the public to have access to the model’s training data.

:p Why should the public have the right to access a model's training data?
??x
The public has this right because the data used to train the models is often scraped from the internet and generated by the general public. Making the training data public ensures transparency and accountability, allowing for better understanding of how the model operates and potentially retraining or modifying it with new information.
x??

---

#### Iterative Process in Model Development
Explanation on the iterative nature of model development, where decisions may change based on newer information.

:p Describe an example of how decision-making is iterative in model development?
??x
In model development, you might initially decide to host open-source models due to their availability and cost-effectiveness. However, after thorough evaluation through public and private testing, you might find that these models don’t meet your performance requirements. Thus, you may need to switch to commercial APIs or even develop a custom model.

```java
public class ModelDevelopment {
    void iterateDecision() {
        boolean useOpenSource = true; // Initial decision based on availability
        while (!checkPerformance()) { // Check if current model meets the required performance
            if (performance < desiredPerformance) {
                useOpenSource = false; // Change to commercial APIs or custom models
            }
        }
    }

    private boolean checkPerformance() {
        // Logic to evaluate model's performance against criteria
        return true;
    }
}
```
x??

---

#### Model API vs. Hosted Models
Discussion on the decision of whether to use model APIs or host models yourself, and how this can impact the candidate model pool.

:p When is it better to use commercial model APIs over hosting open-source models?
??x
It’s better to use commercial model APIs when you need higher performance that may not be achievable with open-source models. Commercial APIs often have more advanced architectures and are continuously updated, which can provide better results compared to pre-existing open-source models.

```java
public class ModelSelection {
    void chooseModelAPI() {
        if (performanceEvaluation() < desiredPerformance) { // If the performance is insufficient
            useAPI = true; // Switch to commercial APIs for higher performance
        } else {
            useAPI = false; // Use open source models or custom development
        }
    }

    private boolean performanceEvaluation() {
        // Logic to evaluate if current model meets desired performance
        return false;
    }
}
```
x??

---

#### Open Source, Open Weight, and Model Licenses
Explanation on the terms "open source," "open weight," and "model licenses" and their implications.

:p What does it mean when a model is described as "open weight"?
??x
When a model is described as "open weight," it means that only the weights of the neural network are available to the public, but not the training data. This term is used in contrast to "open source models," which typically imply both the weights and the training data are made publicly available.

```java
public class ModelLicenses {
    String modelType(String description) {
        if (description.contains("weights only")) {
            return "Open weight";
        } else {
            return "Open source"; // Assuming it comes with open data as well
        }
    }
}
```
x??

---

#### Trust in Public Benchmarks
Discussion on why public benchmarks can’t be trusted and the need to design your own evaluation pipeline.

:p Why should you avoid using public benchmarks for model evaluation?
??x
Public benchmarks are often unreliable due to potential biases, data quality issues, or intentional manipulations by their creators. Therefore, it’s crucial to design your own evaluation pipeline that uses prompts and metrics you trust, ensuring the accuracy and reliability of your model's performance.

```java
public class BenchmarkEvaluation {
    void createCustomPipeline() {
        // Define custom prompts and metrics based on specific use cases
        List<String> prompts = new ArrayList<>();
        Map<String, String> metrics = new HashMap<>();
        
        for (String prompt : prompts) {
            evaluateModel(prompt);
        }
    }

    private void evaluateModel(String prompt) {
        // Logic to evaluate model performance against the given prompt using trusted metrics
    }
}
```
x??

---

#### Iterative Decision-Making in Model Build vs. Buy
Explanation on the iterative nature of choosing between building models yourself or using commercial APIs.

:p How does the decision-making process for model build vs. buy iterate?
??x
The decision-making process iterates based on performance evaluations and changing requirements. Initially, you might choose to use open-source models due to their cost-effectiveness. However, after thorough testing and evaluation, if these models don’t meet your performance needs, you may need to switch to commercial APIs or even develop a custom model.

```java
public class BuildVsBuyDecision {
    void iterateBuildVsBuy() {
        boolean useOpenSource = true; // Initial decision based on cost
        while (!performanceEvaluation()) { // Check if current model meets the required performance
            if (performance < desiredPerformance) {
                useOpenSource = false; // Switch to commercial APIs or custom development
            }
        }
    }

    private boolean performanceEvaluation() {
        // Logic to evaluate if current model meets desired performance
        return false;
    }
}
```
x??

---

#### Training Data Information and Legal Scrutiny
Background context: Model developers might hide training data information on purpose. This practice can prevent them from facing public scrutiny and potential lawsuits related to the use of such data.

:p What are the reasons for model developers to hide training data information?
??x
Model developers might hide training data information to avoid public scrutiny and potential legal actions that could arise from the use or misuse of this data. By keeping details about their training datasets confidential, they reduce the risk of facing questions or accusations regarding the ethical and legal implications associated with the model's development.
x??

---

#### Overview of Open Source Model Licenses
Background context: Different open source models are released under unique licenses, making it challenging to navigate licensing terms. Some examples include MIT, Apache 2.0, GNU GPL, BSD, Creative Commons, among others.

:p What are some common open-source model licenses mentioned in the text?
??x
Some common open-source model licenses mentioned in the text include:
- MIT (Massachusetts Institute of Technology)
- Apache 2.0
- GNU General Public License (GPL)
- Berkeley Software Distribution (BSD)
- Creative Commons

These licenses have different conditions and restrictions, which can affect how models are used.
x??

---

#### Commercial Use and Restrictions in Licenses
Background context: Many open-source models come with specific terms regarding commercial use and restrictions. For example, Llama 2 and Llama 3 require a special license for applications exceeding 700 million monthly active users.

:p Are there any conditions on the commercial use of open source models?
??x
Yes, many open-source models have specific conditions on their commercial use. For instance, both Llama 2 and Llama 3 from Meta require a special license if the application reaches more than 700 million monthly active users. Developers must seek permission from Meta to continue using the model under these circumstances.
x??

---

#### Using Model Outputs for Training Other Models
Background context: Some open-source models allow their outputs to be used in training other models, while others do not. This is an important consideration when selecting a model for development.

:p Can the output of one open-source model be used to train another?
??x
The ability to use the output of one open-source model to train another depends on the specific license terms associated with the original model. For example, the Llama licenses currently do not allow using their outputs to train or improve other models. Therefore, developers must ensure that they are compliant with all relevant licensing agreements before attempting such tasks.
x??

---

#### Difference Between Open Source Models and Model APIs
Background context: The term "model API" refers to the interface used by users to interact with an inference service that hosts a model. There are also other types of APIs like finetuning APIs and evaluation APIs.

:p What is the difference between open source models and model APIs?
??x
Open source models refer to the actual machine learning models themselves, often available for download or use under certain licenses. On the other hand, a model API is an interface that allows users to interact with these models through queries. For instance, when you make a request to a language model hosted by an inference service, what you are interacting with is the model API.

Here's an example of how this works in pseudocode:
```pseudocode
// Example of how a user might interact with a model API

function getUserResponse(query):
    // Connect to the inference service hosting the model
    connectToInferenceService()
    
    // Send query to the model API
    response = sendQuery(query)
    
    // Return the response received from the model
    return response

// Example usage
userQuery = "What is your favorite color?"
responseFromModelAPI = getUserResponse(userQuery)
print(responseFromModelAPI)
```
x??

---

#### Summary of Key Points on Open Source Models and Licensing
Background context: This section summarizes key points related to open-source models, including the challenges of navigating different licenses and the importance of understanding commercial use restrictions.

:p What are some important considerations when working with open source models?
??x
When working with open source models, it's crucial to consider:
- The specific license terms associated with each model.
- Whether the license allows for commercial use.
- Any restrictions on using the model’s outputs for training other models.
- Compliance with the data lineage and usage policies of the original dataset.

Understanding these factors is essential to ensure legal and ethical compliance when deploying or further developing open source AI models.
x??

#### Inference Service Interface
Inference services run models and provide user access. They are crucial for making machine learning models usable outside of their training environment.
:p What is an inference service, and why is it important?
??x
An inference service runs a model and provides an interface through which users can interact with the model. This service is vital because it bridges the gap between trained models and practical applications, allowing non-technical users to leverage AI capabilities without needing to understand or manage the underlying model.
For example:
```java
public class InferenceService {
    private Model model;

    public InferenceService(Model model) {
        this.model = model;
    }

    public String predict(String input) {
        return model.predict(input);
    }
}
```
x??

---

#### Open Source Models and APIs
Developers can open source models, make them accessible via APIs, or both. Many developers also provide commercial services.
:p How do developers typically manage the distribution of their models?
??x
Developers often choose to open source some weaker models while keeping better ones behind paywalls through APIs. This approach helps in gaining user trust and attracting contributions without compromising on the most advanced versions of the model.

For instance, OpenAI has both commercial models and open-sourced versions like GPT-2.
```java
public class ModelProvider {
    private String openSourceModel;
    private String commercialModel;

    public void openSource(String model) {
        openSourceModel = model;
    }

    public void commercialize(String model) {
        commercialModel = model;
    }
}
```
x??

---

#### Model APIs from Different Providers
Model APIs can be accessed through various providers such as OpenAI, cloud service providers like Azure and GCP, or third-party providers.
:p What are the different ways to access models via APIs?
??x
Models can be accessed via multiple API providers. For example:
- **OpenAI** for commercial models and open-source models like GPT-2.
- **Azure** for both OpenAI's models and its own proprietary models.
- **GCP (Google Cloud Platform)**, **AWS**, or third-party providers like Databricks Mosaic, Anyscale.

These APIs can offer the same model with varying features, constraints, and pricing. Here is a simplified example of accessing an API:
```java
public class ModelAPI {
    private String apiUrl;

    public void setApiUrl(String apiUrl) {
        this.apiUrl = apiUrl;
    }

    public String predict(String input) {
        // Code to make HTTP request to the apiUrl and return prediction
        return "Prediction from " + apiUrl;
    }
}
```
x??

---

#### Performance Variability Across APIs
Performance might differ slightly when using different APIs for the same model due to optimization techniques used.
:p How can performance vary across different API providers for the same model?
??x
Performance can vary because each provider may optimize the model differently. For example, Azure and GCP might use different techniques to speed up predictions or improve accuracy.

To illustrate:
```java
public class ModelPerformance {
    private String apiProvider;
    private double performanceRating;

    public void setApiProvider(String apiProvider) {
        this.apiProvider = apiProvider;
        // Set performance rating based on provider and model
    }

    public double getPerformance() {
        return performanceRating;
    }
}
```
x??

---

#### Use Cases for Self-Hosting vs. Using API Services
The choice between self-hosting a model or using an API depends on the specific use case.
:p What factors should be considered when deciding whether to host a model yourself or use an API?
??x
When deciding, consider the following:
- **Control and Security**: Do you need full control over the model's environment? Are there strict data privacy requirements?
- **Cost and Resources**: Can your organization handle hosting and maintaining models, or is it more cost-effective to use external APIs?
- **Scalability**: Do you expect high traffic that might strain your infrastructure?

For example:
```java
public class ModelHostingDecision {
    private boolean control;
    private boolean privacyRequirements;
    private double cost;

    public void setControl(boolean control) {
        this.control = control;
    }

    public void setPrivacyRequirements(boolean privacyRequirements) {
        this.privacyRequirements = privacyRequirements;
    }

    public void setCost(double cost) {
        this.cost = cost;
    }

    public String decideHosting() {
        if (control && !privacyRequirements && cost > 1000) {
            return "Self-host the model.";
        } else {
            return "Use API services.";
        }
    }
}
```
x??

#### Data Privacy Concerns
Data privacy is a significant concern for companies, especially those with strict policies. Leaking data outside an organization can lead to serious issues, such as information breaches or unintentional leaks of sensitive data.

:p What are some scenarios where data privacy poses a risk in AI model usage?
??x
Companies may face risks if they use externally hosted models because the provider might access and potentially misuse their data for training purposes. For example, Zoom's change in terms of service to allow the use of user-generated data for AI model training led to backlash from users.

Include code examples if relevant:
```java
public class DataPrivacy {
    // This is a pseudo-code representation of handling data privacy concerns.
    public void handleDataPrivacy(String data) {
        // Check if the data needs to be sanitized or anonymized before sending it to an external API.
        String sanitizedData = sanitizeData(data);
        if (sanitizedData != null) {
            sendToModelAPI(sanitizedData);
        } else {
            System.out.println("Data was not sanitized; cannot send.");
        }
    }

    private String sanitizeData(String data) {
        // Implementation to remove or mask sensitive information.
        return data.replaceAll("[P|I|R]_", "[*]");
    }
}
```
x??

---

#### Data Lineage and Copyright
Data lineage and copyright are crucial concerns in AI model development. The transparency of the training data used by models is often limited, leading to uncertainty about how the model was trained.

:p What issues can arise due to lack of transparency in the training data?
??x
Lack of transparency in the training data can lead to several issues:
- **Memorization**: AI models might memorize parts of their training set, which can be accidentally leaked or exploited.
- **IP Disputes**: Using copyrighted data without proper authorization can lead to legal disputes.

Code Example:
```java
public class DataLineage {
    // This is a pseudo-code representation of managing data lineage and copyright issues.
    public boolean checkDataLineageAndCopyright(String model, String[] trainingData) {
        for (String data : trainingData) {
            if (!isLegalToUse(data)) {
                System.out.println("Copyright violation detected.");
                return false;
            }
        }
        System.out.println("Data lineage and copyright are clear.");
        return true;
    }

    private boolean isLegalToUse(String data) {
        // Implementation to check if the data can be used legally.
        return true; // Placeholder implementation
    }
}
```
x??

---

#### Performance Considerations
Performance is a critical factor when choosing between on-premises models and hosted APIs. The speed, reliability, and scalability of AI models are essential for many applications.

:p How does performance impact the choice between on-premises and hosted models?
??x
Performance impacts the decision significantly because:
- **On-Premises Models**: Offer better control over infrastructure but require substantial IT resources.
- **Hosted APIs**: Provide ease of use and management but may introduce latency or reliability issues.

Code Example:
```java
public class PerformanceEvaluation {
    // This is a pseudo-code representation of evaluating performance for different model deployment options.
    public int evaluatePerformance(String deploymentOption, int workload) {
        if ("on-premises".equals(deploymentOption)) {
            return calculateOnPremisePerformance(workload);
        } else if ("hosted-API".equals(deploymentOption)) {
            return calculateHostedApiPerformance(workload);
        }
        return -1; // Placeholder for invalid option
    }

    private int calculateOnPremisePerformance(int workload) {
        // Logic to evaluate on-premises performance.
        return workload * 2; // Example calculation
    }

    private int calculateHostedApiPerformance(int workload) {
        // Logic to evaluate hosted API performance.
        return workload / 2; // Example calculation
    }
}
```
x??

---

#### Functionality and Costs
Functionality and costs are key factors when deciding between on-premises models and hosted APIs. The features provided by the model and the associated financial implications must be carefully weighed.

:p How do functionality and costs influence the choice of AI deployment?
??x
Functionality and costs influence the decision as follows:
- **On-Premises Models**: Require significant initial investment but offer more flexibility and control.
- **Hosted APIs**: Provide immediate access to advanced features at a cost, which can be beneficial for smaller organizations or those prioritizing speed over control.

Code Example:
```java
public class FunctionalityCosts {
    // This is a pseudo-code representation of evaluating functionality and costs for different deployment options.
    public void evaluateFunctionalityAndCost(String deploymentOption) {
        if ("on-premises".equals(deploymentOption)) {
            System.out.println("High initial investment required but offers more control over features.");
        } else if ("hosted-API".equals(deploymentOption)) {
            System.out.println("Immediate access to advanced features, but costs are higher and variable.");
        }
    }
}
```
x??

---

#### Control
Control is a key factor when choosing between on-premises models and hosted APIs. The ability to manage and customize the model locally can be crucial for sensitive applications.

:p Why is control important in AI deployment?
??x
Control is important because:
- **Customization**: On-premises models allow full customization, which might not be possible with hosted services.
- **Security**: Controlling the environment where data resides ensures better security and compliance adherence.

Code Example:
```java
public class ControlEvaluation {
    // This is a pseudo-code representation of evaluating control for different deployment options.
    public void evaluateControl(String deploymentOption) {
        if ("on-premises".equals(deploymentOption)) {
            System.out.println("Full customization and better security controls are available.");
        } else if ("hosted-API".equals(deploymentOption)) {
            System.out.println("Limited control over model settings, but easier to manage from a central location.");
        }
    }
}
```
x??

---

#### On-Device Deployment
On-device deployment is particularly important for scenarios where privacy and data security are paramount. Devices that process data locally can reduce the risk of unauthorized access.

:p How does on-device deployment address privacy concerns?
??x
On-device deployment addresses privacy concerns by:
- **Reducing Data Exposure**: Local processing minimizes the amount of sensitive information sent over networks.
- **Enhanced Security**: Data is processed directly on the device, reducing the risk of data breaches or misuse.

Code Example:
```java
public class OnDeviceDeployment {
    // This is a pseudo-code representation of implementing on-device deployment for privacy reasons.
    public void processOnDevice(String data) {
        if (isDataSensitive(data)) {
            processLocally(data);
        } else {
            sendToCloudAPI(data);
        }
    }

    private boolean isDataSensitive(String data) {
        // Logic to determine if the data needs to be processed locally.
        return true; // Placeholder implementation
    }

    private void processLocally(String data) {
        // Local processing logic.
        System.out.println("Processing data on device.");
    }

    private void sendToCloudAPI(String data) {
        // Cloud API invocation logic.
        System.out.println("Sending data to cloud API.");
    }
}
```
x??

---

#### Summary of Axes
The seven axes (data privacy, data lineage, performance, functionality, costs, control, and on-device deployment) provide a comprehensive framework for evaluating AI systems.

:p What are the key axes used in evaluating AI systems?
??x
The key axes used in evaluating AI systems include:
- **Data Privacy**: Ensuring data is not exposed to unauthorized entities.
- **Data Lineage and Copyright**: Tracking where and how data was sourced and used.
- **Performance**: Assessing speed, reliability, and scalability of models.
- **Functionality and Costs**: Evaluating the features and financial implications.
- **Control**: Managing local or remote deployment options for better customization and security.
- **On-Device Deployment**: Processing data locally to enhance privacy.

x??

---

#### Data Lineage and Open Models

Background context: Concerns over data lineage have driven some companies toward fully open models, where training data is made publicly available. This allows communities to inspect the data for safety before using it.

:p What are the main reasons behind the shift towards fully open models?
??x
The primary reason is to ensure transparency and safety of the training data by making it publicly accessible for inspection. This approach aims to build trust among users who want to verify that the data does not contain any harmful or unethical content before using it in their applications.
x??

---

#### Regulations and Auditable Information

Background context: As regulations evolve, there may be increased requirements for auditable information regarding models and training data. Commercial models could potentially provide certifications that save companies from the effort of thorough inspection.

:p How might commercial models benefit from this regulation?
??x
Commercial models can benefit by offering certifications that demonstrate compliance with regulatory standards. These certifications can help in reducing the burden on companies to conduct detailed audits themselves, thus saving time and resources.
x??

---

#### Open Source Models and Inference Services

Background context: Users prefer open source models because they provide more information and options. However, from a developer's perspective, investing millions into building models for others to make money is not ideal.

:p What are the incentives for model developers to use open source models?
??x
Model developers might be incentivized by the idea that open-source models can drive demand for inference and fine-tuning services, which can generate revenue. However, there is a risk of competitors leveraging these same open-source models for profit.
x??

---

#### Legal Risks with Open Source Models

Background context: Open source models often have limited legal resources compared to commercial models. Using an open source model that infringes on copyrights may not hold the developers accountable but could put users at risk.

:p What are the risks associated with using open source models?
??x
The primary risk is that if a user uses an open-source model that infringes on copyrights, they might face legal action instead of the model developers. This can lead to potential legal issues for users who are not aware of the limitations.
x??

---

#### Performance of Open Source Models

Background context: There has been a narrowing gap between performance of open source models and proprietary models over time. Many believe that eventually, an open-source model might perform as well or better than any proprietary model.

:p Why do some people still prefer proprietary models despite their potentially lower performance?
??x
Some people prefer proprietary models because they want to capitalize on the strongest available models themselves rather than sharing it with others who can benefit from it. This approach maximizes their returns.
x??

---

#### API Strategy for Companies

Background context: It is common practice for companies to keep their strongest models behind APIs and open source weaker models, ensuring that more powerful models are not used freely.

:p Why do companies often keep their strongest models behind APIs?
??x
Companies keep their strongest models behind APIs to control access and usage, ensuring that these models are leveraged in a way that maximizes the company's benefits. This strategy prevents competitors from easily using the most advanced models.
x??

---

#### Open Source vs Proprietary Models
Background context: The text discusses the advantages and disadvantages of open source models compared to proprietary ones. It highlights that for many use cases, open source models might be sufficient due to cost and functionality considerations.

:p Which scenario is more likely according to the text for the strongest open source model lagging behind the strongest proprietary model?
??x
The text suggests that it's likely that the strongest open source model will lag behind the strongest proprietary model for the foreseeable future. This is because open source developers don't receive user feedback, which commercial models benefit from.

x??

---

#### Functionalities Around a Model
Background context: The text outlines several functionalities required around a model to make it suitable for use cases. These include scalability, function calling, structured outputs, and output guardrails.

:p What are some key functionalities that need to be considered when working with models?
??x
Key functionalities include:
- Scalability: Ensuring the inference service supports traffic while maintaining desired latency and cost.
- Function calling: Allowing models to use external tools for applications like Retrieval-Augmented Generation (RAG) or agentic tasks.
- Structured outputs, such as generating JSON format.
- Output guardrails: Mitigating risks in generated responses.

x??

---

#### API Costs
Background context: The text highlights that while the largest companies might negotiate favorable terms with model providers, smaller companies often bear the brunt of API costs. Commercial APIs offer pre-built functionalities but restrict flexibility.

:p How do API costs affect smaller companies compared to larger ones?
??x
Smaller companies are more likely to be hit by API costs because they cannot negotiate as favorably as large companies. Large companies might be important enough for providers to secure better terms, whereas smaller companies have less bargaining power and thus higher costs.

x??

---

#### Finetuning Models
Background context: The text explains that proprietary models often restrict finetuning options, whereas open source models offer more flexibility. Different types of finetuning exist, but commercial providers may only support some.

:p What are the limitations on finetuning a proprietary model compared to an open source one?
??x
Proprietary models might not allow finetuning at all or require explicit permission from the provider. In contrast, open source models can be finetuned using third-party services or by the developer themselves, offering more flexibility in terms of partial and full finetuning.

x??

---

#### API Cost vs Engineering Cost
Background context: The text discusses when it might be more cost-effective to host a model internally rather than using an API provider. Hosting requires significant engineering effort but can be cheaper at large scales.

:p Under what conditions might a company prefer hosting its own models over using commercial APIs?
??x
A company might prefer hosting their own models if the usage is heavy and API costs become prohibitive. However, this option requires substantial time, talent, and engineering resources to set up and maintain.

x??

---

#### Model Optimization and Scaling

Background context: The need to optimize models for performance, scale inference services as needed, and provide guardrails around the model is critical. APIs can be costly, but custom engineering efforts are also significant.

:p What considerations should you make when optimizing a model for performance and scaling an inference service?
??x
When optimizing a model, focus on reducing latency and improving throughput while ensuring the accuracy of predictions remains high. For scaling, consider the load your system will handle at peak times and plan infrastructure accordingly. Guardrails are essential to prevent misuse; examples include blocking requests that generate harmful content.

```java
public class ModelScaler {
    public void optimizeAndScale() {
        // Optimize model for better performance
        optimizeModel();

        // Scale inference service based on predicted load
        scaleServiceBasedOnPredictedLoad();
        
        // Implement guardrails to prevent misuse
        applyGuardrails();
    }

    private void optimizeModel() {
        // Logic to reduce model size and improve performance
    }

    private void scaleServiceBasedOnPredictedLoad() {
        // Logic to scale service based on peak load predictions
    }

    private void applyGuardrails() {
        // Logic to implement safety measures, e.g., blocking harmful requests
    }
}
```
x??

---

#### Proprietary vs. Open Models

Background context: Deciding between proprietary and open models involves weighing ease of use, reliability, and control. Proprietary models are often easier to start with but may err more in over-censoring. Open models provide greater flexibility and community support.

:p What are the advantages and disadvantages of choosing a proprietary model versus an open-source model?
??x
Advantages of proprietary models include ease of use and standard API, making them simpler to integrate into existing systems. However, they often come with high costs and may err on the side of over-censoring due to safety guardrails imposed by providers.

Open-source models offer more control, customizability, and transparency. They benefit from a larger community that can provide support for issues encountered. However, they require more engineering effort for integration and may lack the reliability guarantees provided by proprietary APIs.

```java
public class ModelSelector {
    public String chooseModel(String criteria) {
        if (criteria.equals("ease_of_use")) {
            return "Proprietary";
        } else if (criteria.equals("control_and_customizability")) {
            return "Open-source";
        }
        // More logic can be added for other criteria
        return null;
    }
}
```
x??

---

#### Control, Access, and Transparency

Background context: Enterprises prefer open-source models due to control and customizability. This allows businesses to have more influence over their AI systems, which is crucial for sensitive applications.

:p Why might an enterprise prefer open-source models?
??x
Enterprises may prefer open-source models because they offer greater control and customization options. Open-source models allow enterprises to modify the code or integrate the model into existing systems as needed. This level of flexibility can be critical in sensitive applications where proprietary APIs might have limitations.

```java
public class EnterprisePreference {
    public String preferenceForModels() {
        return "Open-source";
    }
}
```
x??

---

#### Safety Guardrails and Over-Censoring

Background context: Model providers implement safety guardrails to prevent misuse, such as blocking requests that generate harmful content. However, these measures can limit the model's functionality in certain use cases.

:p What are safety guardrails, and how do they impact model usage?
??x
Safety guardrails are mechanisms implemented by model providers to prevent the generation of harmful or inappropriate content, such as racist jokes or images of real people. While these measures protect users and providers from potential lawsuits, they can limit a model's functionality in specific use cases where flexibility is required.

For example, generating realistic faces for a music video production might be restricted if the model blocks all face generation to avoid misuse.

```java
public class SafetyGuardrail {
    public boolean allowRequest(String request) {
        // Logic to check and block inappropriate requests
        return !request.contains("racist") && !request.contains("real person");
    }
}
```
x??

---

#### Historical Transparency Issues with Commercial Models

Background context: Historically, commercial models lack transparency in changes, versions, and roadmaps. This can lead to unpredictable updates that may break existing systems.

:p Why might historical transparency issues be a concern when using commercial AI models?
??x
Historical transparency issues with commercial AI models can lead to several concerns:
1. **Unannounced Updates:** Models are frequently updated without prior notice.
2. **Inconsistent Versions:** Lack of clear versioning and roadmaps makes it hard to track changes and dependencies.
3. **Predictability Issues:** Without a clear roadmap, businesses cannot plan their integration or updates effectively.

These issues can be particularly problematic for businesses that have built critical systems around these models.

```java
public class ModelTransparency {
    public boolean checkForUpdates() {
        // Check if the model has unannounced changes
        return true; // Assume there are unannounced changes for demonstration
    }
}
```
x??

---

#### Unpredictable Changes in AI Models
Background context: AI models can experience unexpected changes, which can affect their performance and reliability. This unpredictability is a challenge for industries that rely on consistent model behavior.
:p What are some reasons why unpredictable changes might occur in an AI model?
??x
Unpredictable changes in AI models can arise due to various factors such as updates from the model provider, changes in training data, or shifts in the underlying algorithms. These changes may not be communicated or controlled by the users of the model.
x??

---

#### Impact of Model Provider Changes on Use Cases
Background context: Model providers might discontinue support for certain use cases, industries, or countries. This can significantly impact applications that rely heavily on these models.
:p How does a model provider discontinuing support affect an application?
??x
If a model provider stops supporting your use case or industry, it can lead to disruptions in services and potential loss of functionality. For instance, if Italy banned OpenAI temporarily, users relying on the platform for specific applications might face service interruptions until alternative solutions are found.
x??

---

#### On-Device Deployment Considerations
Background context: Running models locally on devices is desirable for scenarios where internet access is limited or privacy concerns exist. This approach ensures data stays within the device and reduces dependency on external services.
:p What are some reasons to run a model on-device?
??x
Running a model on-device is beneficial in situations where:
- Reliable internet access is not available, making it challenging to send requests to remote servers.
- Privacy is paramount, such as when users want their data processed locally without transmitting it externally.
- Real-time performance is crucial and cannot be achieved through external APIs.
x??

---

#### Pros and Cons of Model APIs vs Self-hosting
Background context: There are trade-offs between using model APIs provided by third-party providers versus self-hosting models. Understanding these differences helps in making informed decisions based on specific needs.
:p What are the pros and cons of using model APIs compared to self-hosting?
??x
Using Model APIs:
- Pros: Best-performing closed-source models, more likely to support scaling and function calling, structured outputs, logprobs for classification tasks, evaluations, and interpretability.
- Cons: Data leakage risks due to sending data externally, fewer checks on data lineage or training data copyright.

Self-hosting Models:
- Pros: No need to send data externally, greater control over finetuning, quantization, and optimization.
- Cons: Higher cost in terms of talent, time, and engineering effort, API costs for third-party services.
x??

---

#### Fine-tuning Considerations
Background context: Finetuning models allows users to adapt existing models to their specific needs. However, the ability to do so depends on the provider's policies and the model's licensing.
:p What are the limitations of finetuning when using a model API?
??x
When using a model API, you might face limitations such as:
- Limited or no access to fine-tune models unless explicitly allowed by the provider.
- Potential restrictions on quantization and optimization due to licensing constraints.

Self-hosting offers more flexibility but requires additional expertise in:
- Engineering for hosting and maintaining the model.
- Optimizing and customizing the model according to specific needs.
x??

---

#### Self-Hosting Models vs. Commercial APIs

Background context: When deciding between self-hosting models and using commercial APIs, several factors come into play, such as control over the model, rate limits, transparency, edge use cases, and performance.

:p What are the main considerations when choosing between self-hosting a model or using a commercial API?
??x
When considering self-hosting a model versus using a commercial API, key factors include:

- **Control**: Self-hosting provides more control over the model's updates, changes, and versioning. You can inspect changes easily in open-source models.
- **Rate Limits**: Commercial APIs often have rate limits that may impact usage.
- **Risk of Losing Access**: There is a risk of losing access to commercial models if services change their policies or go out of business.
- **Lack of Transparency**: Changes and updates to pre-trained models from commercial providers might not be transparent, whereas self-hosted models offer more visibility.

In edge use cases, self-hosting allows running models offline without internet access but may require building custom APIs for integration. On the other hand, commercial APIs can run on devices without internet but might have limitations in customization and model updates.

```java
public class ModelSelectionDecision {
    private boolean control = true; // Self-hosting gives more control
    private boolean rateLimitsPresent = true; // Commercial APIs may have rate limits
    private boolean transparencyAvailable = false; // Pre-trained models' changes can be opaque

    public void evaluateCommercialVsSelfHosting() {
        if (control && !rateLimitsPresent && transparencyAvailable) {
            System.out.println("Consider self-hosting the model.");
        } else {
            System.out.println("Consider using a commercial API.");
        }
    }
}
```
x??

---

#### Public Benchmarks and Evaluation Harnesses

Background context: To compare different models, public benchmarks are crucial. These include tools like EleutherAI’s lm-evaluation-harness which supports over 400 benchmarks, or OpenAI’s evals that can run approximately 500 existing benchmarks.

:p How do you use a tool like EleutherAI’s lm-evaluation-harness to compare different models?
??x
To use EleutherAI's lm-evaluation-harness for comparing different models:

1. **Benchmark Selection**: Choose relevant benchmarks from the available set of over 400.
2. **Run Benchmarks**: Execute the benchmarks on your models to get performance data.
3. **Aggregate Results**: Aggregate the results to create a leaderboard that ranks the models based on their overall performance.

For example, if you want to compare two models A and B for code generation:

```java
public class BenchmarkEvaluation {
    private EvaluationHarness harness = new EvaluationHarness();

    public void evaluateModels(String modelA, String modelB) {
        // Run benchmarks on both models
        Map<String, Double> resultsModelA = harness.runBenchmark(modelA);
        Map<String, Double> resultsModelB = harness.runBenchmark(modelB);

        // Aggregate and compare the results
        double scoreA = aggregateResults(resultsModelA);
        double scoreB = aggregateResults(resultsModelB);

        if (scoreA > scoreB) {
            System.out.println("Model A performs better.");
        } else {
            System.out.println("Model B performs better.");
        }
    }

    private double aggregateResults(Map<String, Double> results) {
        // Simple average of scores
        return results.values().stream().mapToDouble(val -> val).average().orElse(0.0);
    }
}
```
x??

---

#### Benchmark Selection and Aggregation

Background context: Selecting the right benchmarks for evaluating models is critical to ensure that you choose a model suitable for your specific use case. Benchmark selection involves choosing relevant benchmarks from thousands available, while aggregation helps in ranking models based on performance.

:p How do you decide which benchmarks to include in a leaderboard?
??x
Deciding which benchmarks to include in a leaderboard requires considering the specific needs of your use cases and evaluating the relevance of each benchmark:

1. **Relevance**: Choose benchmarks that align with the capabilities required for your application (e.g., coding, toxicity detection).
2. **Compute Constraints**: Some benchmarks may be too expensive or resource-intensive to run frequently.
3. **Coverage**: Ensure a broad coverage of capabilities to get a comprehensive view of model performance.

For instance, if you are evaluating models for code generation and toxicity, include relevant benchmarks like COCODataset (for coding) and TOXICITY (for toxicity detection).

```java
public class BenchmarkSelection {
    private List<String> selectedBenchmarks = new ArrayList<>();

    public void selectRelevantBenchmarks() {
        // Add benchmark names that are relevant to the use case
        selectedBenchmarks.add("COCODataset");
        selectedBenchmarks.add("TOXICITY");

        System.out.println("Selected benchmarks: " + selectedBenchmarks);
    }
}
```
x??

---

#### Public Leaderboards

Background context: Public leaderboards rank models based on their performance across a subset of available benchmarks. These are useful for identifying promising models but may have limitations due to compute constraints.

:p How do public leaderboards help in model selection?
??x
Public leaderboards provide a structured way to compare and select models by ranking them based on performance metrics:

1. **Benchmark Aggregation**: Leaderboards aggregate results from multiple benchmarks to create a comprehensive score.
2. **Model Selection Guidance**: They offer insights into which models perform well across different aspects, helping you choose the most suitable model for your needs.

For example, if you are considering two models A and B for code generation:

- Model A performs better on COCODataset but worse on TOXICITY.
- Model B has a balanced performance across all benchmarks relevant to your use case.

You would need to decide based on the priorities of your application. If avoiding toxic content is critical, you might prefer Model A; otherwise, a balanced performance from Model B could be more appropriate.

```java
public class PublicLeaderboard {
    private Map<String, Double> modelScores = new HashMap<>();

    public void addModelScore(String modelName, double score) {
        modelScores.put(modelName, score);
    }

    public String selectBestModel(List<String> modelsToCompare) {
        String bestModel = "";
        double highestScore = 0.0;

        for (String model : modelsToCompare) {
            if (modelScores.getOrDefault(model, 0.0) > highestScore) {
                highestScore = modelScores.get(model);
                bestModel = model;
            }
        }

        return bestModel;
    }
}
```
x??

---

#### Hugging Face's Opt-Out from HumanEval
Hugging Face decided to opt out of HumanEval due to its high compute requirements. This decision highlights the complexity involved in evaluating large language models (LLMs) comprehensively.

:p Why did Hugging Face decide to opt out of HumanEval?
??x
Hugging Face opted out of HumanEval because it requires a significant amount of computational resources, which are not readily available for such extensive testing. This decision emphasizes the challenges in thoroughly assessing LLMs.
x??

---

#### Open LLM Leaderboard Initial Launch
In 2023, Hugging Face launched its Open LLM Leaderboard with four initial benchmarks to evaluate different aspects of LLM capabilities.

:p What were the initial four benchmarks included in Hugging Face's Open LLM Leaderboard?
??x
The initial four benchmarks in Hugging Face's Open LLM Leaderboard were:
1. ARC-C (Clark et al., 2018): Measuring the ability to solve complex, grade school-level science questions.
2. MMLU (Hendrycks et al., 2020): Measuring knowledge and reasoning capabilities in 57 subjects.
3. HellaSwag (Zellers et al., 2019): Measuring the ability to predict the completion of sentences or scenes in stories.
4. TruthfulQA (Lin et al., 2021): Focusing on generating truthful and non-misleading responses.

These benchmarks were chosen to cover a variety of reasoning and general knowledge across different fields.
x??

---

#### Expansion of Open LLM Leaderboard
By the end of 2023, Hugging Face expanded its Open LLM Leaderboard to include six benchmarks. This expansion aimed to provide a more comprehensive evaluation of LLMs.

:p What are the six benchmarks included in Hugging Face's updated Open LLM Leaderboard?
??x
The six benchmarks included in Hugging Face's updated Open LLM Leaderboard are:
1. ARC-C (Clark et al., 2018): Measuring the ability to solve complex, grade school-level science questions.
2. MMLU (Hendrycks et al., 2020): Measuring knowledge and reasoning capabilities in 57 subjects.
3. HellaSwag (Zellers et al., 2019): Measuring the ability to predict the completion of sentences or scenes in stories.
4. TruthfulQA (Lin et al., 2021): Focusing on generating truthful and non-misleading responses.
5. WinoGrande (Sakaguchi et al., 2019): Measuring the ability to solve challenging pronoun resolution problems requiring sophisticated commonsense reasoning.
6. GSM-8K (Grade School Math, OpenAI, 2021): Measuring the ability to solve a diverse set of math problems typically encountered in grade school curricula.

These benchmarks were chosen for their variety and coverage across different reasoning and knowledge domains.
x??

---

#### Reasoning Behind Benchmark Selection
Hugging Face chose these six benchmarks because they test a wide range of reasoning and general knowledge across various fields. This approach provides a more holistic evaluation compared to smaller sets of benchmarks.

:p Why did Hugging Face choose the specific benchmarks for their updated leaderboard?
??x
Hugging Face selected these benchmarks based on their ability to evaluate a variety of reasoning and general knowledge domains. The benchmarks cover:
- Complex science questions (ARC-C)
- Broad subjects including history, computer science, and law (MMLU)
- Sentence completion tasks involving common sense (HellaSwag)
- Truthfulness in responses (TruthfulQA)
- Challenging pronoun resolution problems (WinoGrande)
- Grade school-level math problems (GSM-8K)

These diverse benchmarks aim to provide a comprehensive assessment of the LLM's capabilities.
x??

---

#### Comparison with Other Leaderboards
Stanford’s HELM Leaderboard used ten benchmarks, including some shared with Hugging Face. This comparison highlights differences in benchmark selection and coverage.

:p How does Stanford's HELM Leaderboard compare to Hugging Face's Open LLM Leaderboard?
??x
Stanford's HELM Leaderboard expanded beyond the six benchmarks of Hugging Face’s Open LLM Leaderboard by including:
- A benchmark for competitive math (MATH)
- Legal, medical, and translation benchmarks (LegalBench, MedQA, WMT 2014)
- Reading comprehension benchmarks based on books or long stories (NarrativeQA and OpenBookQA)
- General question answering benchmarks with and without Wikipedia input (Natural Questions)

While both leaderboards aim for comprehensive evaluations, Stanford’s HELM Leaderboard offers a broader range of scenarios to test LLMs.
x??

---

#### Conclusion: Benchmark Selection
Public leaderboards generally balance coverage and the number of benchmarks to ensure a fair and comprehensive evaluation of different models.

:p What are the key considerations when selecting benchmarks for public leaderboards like Hugging Face's Open LLM Leaderboard?
??x
Key considerations in benchmark selection for public leaderboards such as Hugging Face’s Open LLM Leaderboard include:
- Coverage: Ensuring a wide range of reasoning and knowledge domains are tested.
- Relevance: Selecting benchmarks that accurately reflect the capabilities needed from LLMs.
- Simplicity vs. Complexity: Balancing between simple, easy-to-understand benchmarks versus complex, multi-faceted ones.

These considerations help in creating an evaluation framework that provides a well-rounded assessment of model performance across various tasks and scenarios.
x??

---

#### Benchmark Selection Challenges

Background context explaining the challenges and issues related to selecting benchmarks. The selection of benchmarks is crucial but often lacks transparency, leading to potential biases and redundancy.

:p What are some of the questions raised about benchmark selection?
??x
The questions raise several concerns regarding the criteria for choosing benchmarks. For instance:
- Why certain tasks like medical or legal reasoning are included in HELM Lite while others such as general science are not.
- The inconsistency in the number of tests, with math having two tests but no coding test.
- The absence of specific tests for summarization, tool use, toxicity detection, image search, etc.

These questions highlight the difficulties in defining and selecting benchmarks that cover a wide range of capabilities comprehensively.

x??

---

#### Transparency in Benchmark Selection

Background context explaining the importance of transparency in benchmark selection. Recent developments have led to more openness about how benchmarks are chosen and aggregated.

:p How has transparency improved regarding benchmark selection?
??x
Transparency has significantly improved with platforms like Hugging Face sharing detailed analyses on their benchmark selection processes. For instance, they shared a comprehensive analysis of benchmark correlations. This increased transparency is crucial for understanding the rationale behind the choices made in selecting benchmarks.

For example, when Hugging Face updated their leaderboard, they replaced benchmarks that were becoming saturated or close to saturation with more challenging and practical ones. This includes moving from GSM-8K to MATH lvl 5, which contains the most difficult questions from a competitive math benchmark, and updating MMLU to MMLU-PRO.

x??

---

#### Benchmark Correlation

Background context explaining why benchmark correlation is important in model evaluation. Strongly correlated benchmarks can exaggerate biases and may not provide diverse assessments of models’ capabilities.

:p Why is benchmark correlation an important consideration?
??x
Benchmark correlation is crucial because strongly correlated benchmarks can amplify existing biases within the model evaluations. If two or more benchmarks are highly correlated, they might be measuring similar aspects of a model's performance, leading to redundant information and potentially inflating certain types of bias.

For example, if a model performs well on one benchmark that tests factual consistency, it is likely to perform similarly on another closely related benchmark that also assesses factual accuracy. This redundancy can mask the true diversity in how different models handle various tasks.

x??

---

#### Example of Benchmark Correlation

Background context explaining an example of computing benchmark correlation scores using specific data from Hugging Face’s leaderboard.

:p What are the Pearson correlation scores among the benchmarks used by Hugging Face's leaderboard?
??x
In January 2024, Balázs Galambosi computed the Pearson correlation scores among six benchmarks used on Hugging Face’s leaderboard. These scores provide insights into how different benchmark tests correlate with each other.

Here is a summary of the correlations (hypothetical values):
- Benchmark A vs. Benchmark B: 0.75
- Benchmark C vs. Benchmark D: 0.82
- Benchmark E vs. Benchmark F: 0.69

These scores help in identifying which benchmarks measure similar aspects and might be redundant, thus guiding the selection of more diverse and representative tests.

x??

---

#### Hugging Face Leaderboard Updates

Background context explaining recent updates to Hugging Face’s leaderboard and the reasons behind these changes.

:p Why did Hugging Face update their leaderboard with new benchmarks?
??x
Hugging Face updated their leaderboard in June 2024, introducing more challenging and practical benchmarks. This was necessary because previous benchmarks had become saturated or close to saturation levels.

They replaced GSM-8K with MATH lvl 5, which includes the most difficult questions from a competitive math benchmark. Similarly, they updated MMLU to MMLU-PRO (Wang et al., 2024). The new benchmarks include:
- GPQA: A graduate-level Q&A benchmark
- MuSR: A chain-of-thought, multistep reasoning benchmark
- BBH (BIG-bench Hard): Another reasoning benchmark
- IFEval: An instruction-following benchmark

These updates ensure that the leaderboard reflects more advanced capabilities and practical applications of language models.

x??

---

#### Neverending Benchmark Concept

Background context explaining the concept of a neverending benchmark, where new levels can be procedurally generated as models level up.

:p What is the idea behind a "neverending benchmark"?
??x
The idea behind a "neverending benchmark" is to create a system that continuously generates new and increasingly challenging problems as models become more capable. This approach ensures that benchmarks remain relevant and diverse, providing a more comprehensive evaluation of model capabilities over time.

For instance, in gaming, there's the concept of procedurally generated levels where new challenges are created based on player progress. A similar approach could be applied to benchmarking language models by dynamically generating new, complex tasks as the models improve.

x??

---

#### Correlation Between Benchmarks
Background context explaining that different benchmarks test various aspects of reasoning and truthfulness, affecting their correlation. Mentioning the specific benchmark correlations provided in the table.
:p How are the benchmarks WinoGrande, MMLU, and ARC-C related to each other?
??x
These three benchmarks are strongly correlated because they all test reasoning capabilities. The high correlation among them (ARC-C: 0.8672 with MMLU, 0.8856 with WinoGrande) suggests that models performing well in one will likely perform well in the others.
For example:
```plaintext
ARC-C and MMLU have a correlation of 0.8672,
ARC-C and WinoGrande have a correlation of 0.8856.
```
x??

---

#### Correlation Between TruthfulQA and Other Benchmarks
Background context explaining the relationship between reasoning, math capabilities, and truthfulness as indicated by the correlations in the table.
:p How does TruthfulQA correlate with other benchmarks?
??x
TruthfulQA is only moderately correlated to other benchmarks. This suggests that improving a model's reasoning and math capabilities doesn't always improve its truthfulness, as shown by lower correlation scores (e.g., 0.4809 with MMLU, 0.4550 with WinoGrande).
For example:
```plaintext
Correlation between TruthfulQA and MMLU: 0.5507,
Correlation between TruthfulQA and WinoGrande: 0.4550.
```
x??

---

#### Averaging Benchmark Scores on Hugging Face Leaderboard
Background context explaining the method used by Hugging Face to aggregate model scores across different benchmarks.
:p How does Hugging Face rank models?
??x
Hugging Face ranks models by averaging their scores across all selected benchmarks. This means that a 80% score on any benchmark is treated equally, regardless of the difficulty or relevance of the benchmark to the task at hand.
For example:
```plaintext
A model's final score = (Score on ARC-C + Score on HellaSwag + ... + Score on GSM-8K) / Number of benchmarks.
```
x??

---

#### Mean Win Rate as an Alternative Ranking Method
Background context explaining the concept of mean win rate and how it differs from averaging benchmark scores.
:p What is the mean win rate, and why did HELM use this method?
??x
The mean win rate is a ranking method defined as "the fraction of times a model obtains a better score than another model, averaged across scenarios." This approach was chosen by the authors of HELM to avoid treating all benchmark scores equally, recognizing that different benchmarks might have varying significance.
For example:
```plaintext
Mean Win Rate = (Times Model A beats Model B + Times Model C beats Model D) / Total number of comparisons.
```
x??

---

#### Importance of Custom Leaderboards for Specific Applications
Background context explaining the need to create custom leaderboards tailored to specific applications and evaluating models based on relevant criteria.
:p Why are custom leaderboards important when evaluating models?
??x
Custom leaderboards are crucial because they allow you to rank models based on the specific capabilities needed for your application. For example, if you're developing a code generation tool, including benchmarks related to coding tasks will provide more relevant insights than general reasoning benchmarks.
For example:
```plaintext
If building a writing assistant, include creative writing benchmarks like MMLU or HellaSwag in the custom leaderboard.
```
x??

---

#### Reliability of Benchmarks
Background context explaining why it's important to evaluate and verify the reliability of benchmarks before using them for model evaluation.
:p Why should we assess the reliability of benchmarks?
??x
It is essential to assess the reliability of benchmarks because anyone can create and publish a benchmark, which might not always measure what you expect. Therefore, verifying whether a benchmark accurately measures the desired capabilities ensures that your evaluation results are meaningful.
For example:
```plaintext
Check the latest research on the benchmark’s validity and compare it with other established benchmarks in the same domain.
```
x??

---

#### Model Performance Perception
Background context: OpenAI updates its models frequently, and users often perceive a drop in performance for their specific use cases. A study by Stanford and UC Berkeley found significant changes in GPT-3.5 and GPT-4's performances between March 2023 and June 2023 on certain benchmarks.

:p Why do people think OpenAI’s models are getting worse with each update?
??x
People perceive a drop in performance because they evaluate the model based on their specific use case, which might not align perfectly with the benchmarks used by OpenAI. Additionally, different applications have different requirements and metrics for success.
x??

---

#### Evaluation Challenges
Background context: Evaluating AI models can be complex due to varying benchmark scores across different metrics like accuracy, F1, BLEU, etc. The evaluation process is expensive both in terms of cost and computational resources.

:p What are the main challenges in evaluating AI systems?
??x
Evaluating AI systems involves several challenges:
1. Varying benchmarks with different scoring methods.
2. Cost: Evaluations can be expensive due to the need for extensive GPU hours and commercial API fees.
3. Imperfect representation of application needs by public benchmarks.

Code Example for cost calculation (pseudocode):
```java
public class EvaluationCost {
    private double gpuHourlyRate;
    
    public EvaluationCost(double hourlyRate) {
        this.gpuHourlyRate = hourlyRate;
    }
    
    public double calculateTotalCost(int modelCount, int benchmarkCount, int hoursPerModelBenchmark) {
        return modelCount * benchmarkCount * hoursPerModelBenchmark * gpuHourlyRate;
    }
}
```
x??

---

#### Public Benchmark Contamination
Background context: Public benchmarks can be contaminated due to data leakage or training on the test set. This occurs when a model is evaluated using the same data it was trained on, leading to overly optimistic performance metrics.

:p What does contamination mean in the context of public benchmarks?
??x
Contamination refers to situations where a model's performance during evaluation is inflated because it has been trained and tested on the same dataset, which can lead to misleadingly high performance scores. This happens when the training data overlaps with or is identical to the validation or test datasets.

Code Example (pseudocode):
```java
public class DataContaminationCheck {
    private Set<String> trainingData;
    private Set<String> testData;

    public boolean checkContamination() {
        for (String dataPoint : testData) {
            if (trainingData.contains(dataPoint)) {
                return true; // Contamination detected
            }
        }
        return false;
    }
}
```
x??

---

#### Cost of Evaluation
Background context: Evaluating AI models on public benchmarks can be costly, requiring significant computational resources and financial investment.

:p What factors contribute to the high cost of evaluating AI models?
??x
The main factors contributing to the high cost are:
1. GPU hours required for model evaluation.
2. Commercial API fees if using paid APIs.
3. Labor costs associated with setting up and running benchmarks.

Code Example (pseudocode):
```java
public class EvaluationCostCalculator {
    private int modelCount;
    private int benchmarkCount;
    private double gpuHourlyRate;

    public EvaluationCostCalculator(int modelCount, int benchmarkCount, double gpuHourlyRate) {
        this.modelCount = modelCount;
        this.benchmarkCount = benchmarkCount;
        this.gpuHourlyRate = gpuHourlyRate;
    }
    
    public double calculateTotalCost(int hoursPerModelBenchmark) {
        return modelCount * benchmarkCount * hoursPerBenchmark * gpuHourlyRate;
    }
}
```
x??

---

#### Benchmark Aggregation
Background context: Public benchmarks provide scores in different units and scales, necessitating a method to aggregate these scores for model comparison.

:p How do you aggregate scores from different public benchmarks?
??x
To aggregate scores from different public benchmarks:
1. Assign weights based on the importance of each benchmark.
2. Normalize scores if they are not on the same scale.
3. Calculate an overall score by combining weighted normalized scores.

Code Example (pseudocode):
```java
public class BenchmarkAggregator {
    private Map<String, Double> benchmarkScores;
    private Map<String, Double> benchmarkWeights;

    public BenchmarkAggregator(Map<String, Double> scores, Map<String, Double> weights) {
        this.benchmarkScores = scores;
        this.benchmarkWeights = weights;
    }
    
    public double calculateOverallScore() {
        double overallScore = 0.0;
        for (Map.Entry<String, Double> entry : benchmarkScores.entrySet()) {
            overallScore += entry.getValue() * benchmarkWeights.get(entry.getKey());
        }
        return overallScore;
    }
}
```
x??

---

#### Model Memorization and Evaluation Scores
Background context: The model might memorize answers from training data, leading to misleadingly high evaluation scores. This can happen if a model is trained on benchmark datasets, causing it to perform well but not necessarily be useful for real-world applications.

:p What are the risks of a model being trained directly on benchmark data?
??x
The risk is that such a model might simply memorize answers from the training dataset rather than generalizing or understanding concepts deeply. This can result in high evaluation scores without practical utility, as demonstrated by Rylan Schaeffer's satirical paper.

```java
// Example of a simple check to see if an answer has been seen during training
public boolean hasAnswerBeenSeen(String question) {
    // Assume this method checks against the training data
    return trainingData.contains(question);
}
```
x??

---

#### Unintentional Data Contamination
Background context: Data contamination can occur unintentionally, such as when internet-scraped data includes benchmark samples. This undermines the trustworthiness of evaluation benchmarks because models may achieve high scores without genuine understanding.

:p How does unintentional data contamination typically happen?
??x
Unintentional data contamination happens during the scraping process where internet-scrapped training data might include publicly available benchmark samples. For instance, if math textbooks are used for training and also contain questions used in a benchmark evaluation.

```java
// Example of detecting potential data contamination using n-gram overlap
public boolean isContaminated(String sample) {
    List<String> tokens = tokenize(sample);
    for (String token : tokens) {
        if (trainingData.contains(token)) {
            return true;
        }
    }
    return false;
}
```
x??

---

#### Handling Data Contamination
Background context: Decontaminating data involves detecting and removing contaminated samples from the training dataset to ensure fair evaluation. However, this process is resource-intensive and often skipped due to practical constraints.

:p What methods can be used to detect data contamination?
??x
Methods include n-gram overlapping and perplexity:
- **n-gram Overlapping**: If a sequence of tokens in an evaluation sample matches the training data, it’s likely contaminated.
- **Perplexity**: Low perplexity indicates the model has seen this text before.

```java
// Example of calculating perplexity for detection
public double calculatePerplexity(String sample) {
    // Simplified version: assuming a method exists to compute perplexity
    return model.computePerplexity(sample);
}
```
x??

---

#### Impact on Evaluation Benchmarks
Background context: With the prevalence of data contamination, it’s challenging to compare different models fairly. This is because high-quality benchmark data can improve model performance but also leads to misleading scores.

:p Why do existing benchmarks become saturated quickly?
??x
Existing benchmarks become saturated quickly because training data often includes these public benchmarks. This means that once a model has seen the benchmark questions during training, it performs well on them even if it doesn’t truly understand the concepts.

```java
// Example of removing specific benchmarks from training data
public Model trainModel(List<String> trainingData) {
    List<String> cleanTrainingData = removeBenchmarks(trainingData);
    return new Model(cleanTrainingData).train();
}
```
x??

---

#### Practical Considerations for Model Developers
Background context: Model developers often face the challenge of balancing between maintaining benchmark standards and including high-quality data that might lead to contamination. They can mitigate this by removing benchmarks from training data.

:p What is a common practice among model developers to handle data contamination?
??x
A common practice is to remove specific benchmarks from training data before training models, ensuring they are evaluated on clean samples. This helps in maintaining the integrity of evaluation benchmarks while still allowing high-quality data to improve overall model performance.

```java
// Example of removing benchmark data before training
public List<String> excludeBenchmarks(List<String> allData) {
    return allData.stream().filter(data -> !isBenchmarkSample(data)).collect(Collectors.toList());
}
```
x??

---

