# Flashcards: 2A004---Hands-On-Large-Language-Models_processed (Part 32)

**Starting Chapter:** Augmented SBERT

---

#### Augmented SBERT Overview
Background context: The provided text introduces a method called Augmented SBERT, which is designed to fine-tune bi-encoder models like SBERT with limited labeled data. This approach uses a cross-encoder (fine-tuned on a small dataset) to generate additional labeled sentence pairs from unlabeled data, thus augmenting the training set.

:p What is Augmented SBERT and how does it work?
??x
Augmented SBERT is a method that aims to fine-tune embedding models with limited labeled data. The process involves:
1. Fine-tuning a cross-encoder (BERT) on a small, annotated dataset.
2. Creating new sentence pairs using the fine-tuned cross-encoder.
3. Labeling these newly created pairs.
4. Training a bi-encoder (SBERT) on both the original and newly labeled data.

The key steps in Augmented SBERT are:
1. **Fine-tuning Cross-Encoder**: Use a small annotated dataset to train BERT.
2. **Generate New Sentence Pairs**: Use the fine-tuned cross-encoder to create additional sentence pairs from unlabeled data.
3. **Label Silver Dataset**: Label the newly generated sentence pairs using the cross-encoder predictions.
4. **Train Bi-Encoder (SBERT)**: Combine both the original gold dataset and silver dataset for training.

This method helps in leveraging a small labeled dataset more effectively by generating additional labeled examples through domain-specific fine-tuning.
x??

---

#### Gold Dataset vs Silver Dataset
Background context: In Augmented SBERT, the distinction between gold and silver datasets is crucial. The gold dataset contains fully annotated data used as ground truth, while the silver dataset consists of predictions generated by a cross-encoder on unlabeled data.

:p What are the differences between a gold dataset and a silver dataset?
??x
A **gold dataset** is a small but fully annotated set that holds the ground truth. It is typically manually labeled to ensure accuracy and reliability. In contrast, a **silver dataset** is also fully annotated in terms of being generated through predictions by the cross-encoder, though it may not be the exact ground truth.

The key differences are:
1. **Gold Dataset**: Fully annotated with accurate labels.
2. **Silver Dataset**: Predicted labels from a cross-encoder on unlabeled data.

Both datasets play crucial roles: the gold dataset provides initial training and validation, while the silver dataset helps expand the labeled data for fine-tuning.
x??

---

#### Cross-Encoder Fine-Tuning Process
Background context: The process of fine-tuning a cross-encoder is detailed in the text. It involves using a small, annotated dataset to train BERT models which can then be used to generate additional sentence pairs.

:p What steps are involved in fine-tuning a cross-encoder for Augmented SBERT?
??x
The steps involved in fine-tuning a cross-encoder for Augmented SBERT include:

1. **Fine-Tuning Cross-Encoder**:
   - Use a small, annotated dataset (gold dataset) to train BERT.
2. **Generate New Sentence Pairs**:
   - Utilize the fine-tuned cross-encoder to create additional sentence pairs from unlabeled data.
3. **Label Silver Dataset**:
   - Use the cross-encoder to label these newly generated pairs.

This process helps in generating a larger labeled dataset (silver) which, when combined with the original gold dataset, improves the quality and quantity of training data for SBERT.
x??

---

#### Training Bi-Encoder on Augmented Data
Background context: After generating additional labeled sentence pairs using the cross-encoder, these are combined with the original gold dataset to train a bi-encoder (SBERT). This step ensures that both high-quality and larger quantities of labeled data are used for training.

:p How is the bi-encoder (SBERT) trained in Augmented SBERT?
??x
The bi-encoder (SBERT) is trained on an extended dataset derived from combining both the original gold dataset and the silver dataset generated through cross-encoder predictions. The steps involved are:

1. **Fine-Tuning Cross-Encoder**:
   - Use a small, annotated dataset to train BERT.
2. **Generate New Sentence Pairs**:
   - Utilize the fine-tuned cross-encoder to create additional sentence pairs from unlabeled data.
3. **Label Silver Dataset**:
   - Use the cross-encoder to label these newly generated pairs.
4. **Train Bi-Encoder (SBERT)**:
   - Combine both the original gold dataset and silver dataset for training SBERT.

This approach ensures that the bi-encoder model benefits from a richer set of labeled data, improving its performance on downstream tasks.
x??

---

#### Data Preparation for Cross-Encoder
Background context: The text describes preparing a dataset to train a cross-encoder model. A subset of 10,000 documents is taken from a larger dataset (50,000) to simulate limited annotated data. Each pair of sentences has labels based on entailment scores.
:p How many documents are used for the initial preparation step?
??x
10,000 documents are used for the initial preparation step.
x??

---

#### Cross-Encoder Training Setup
Background context: A cross-encoder is trained using a subset of 10,000 labeled sentence pairs. The model uses the `CrossEncoder` from `sentence_transformers.cross_encoder`.
:p What command is used to train the cross-encoder?
??x
The training is initiated with the following command:
```python
cross_encoder.fit(
    train_dataloader=gold_dataloader,
    epochs=1,
    show_progress_bar=True,
    warmup_steps=100,
    use_amp=False
)
```
x??

---

#### Silver Dataset Preparation
Background context: After training the cross-encoder, it is used to label a larger set of sentence pairs. This step creates a silver dataset from 400,000 unlabeled sentence pairs by predicting their labels.
:p How many documents are in the original full dataset?
??x
The original full dataset consists of 50,000 documents.
x??

---

#### Fine-Tuning an Embedding Model with Augmented Data
Background context: The text explains combining a gold and silver dataset to fine-tune an embedding model. This process uses the cross-encoder to label additional sentence pairs, thereby increasing the training data size without manual labeling of all sentences.
:p What is the purpose of creating the `silver` DataFrame?
??x
The purpose of creating the `silver` DataFrame is to store the labeled sentence pairs generated by predicting labels with the cross-encoder. This step allows for the use of a larger dataset, which was previously not manually labeled.
x??

---

#### Training Arguments Configuration
Background context: The text details setting up training arguments for fine-tuning an embedding model using the `SentenceTransformerTrainingArguments` class. These settings include parameters like number of epochs, batch size, and evaluation steps.
:p What are the key components in the training arguments?
??x
The key components in the training arguments include:
- `output_dir`: Directory where the model is saved.
- `num_train_epochs`: Number of training epochs.
- `per_device_train_batch_size`: Batch size for training.
- `eval_steps`: Evaluation steps during training.
- `logging_steps`: Logging frequency.

Example configuration:
```python
args = SentenceTransformerTrainingArguments(
    output_dir="augmented_embedding_model",
    num_train_epochs=1,
    per_device_train_batch_size=32,
    per_device_eval_batch_size=32,
    warmup_steps=100,
    fp16=True,
    eval_steps=100,
    logging_steps=100
)
```
x??

---

#### Model Evaluation Metrics
Background context: After training the model, it is evaluated using an evaluator that measures embedding similarity. The results include various metrics like Pearson and Spearman correlation coefficients for cosine similarity.
:p What are some of the evaluation metrics provided?
??x
The evaluation metrics provided include:
- Pearson Cosine
- Spearman Cosine
- Pearson Manhattan
- Spearman Manhattan
- Pearson Euclidean
- Spearman Euclidean
- Pearson Dot
- Spearman Dot
- Pearson Max
- Spearman Max

Example of evaluator results:
```python
evaluator(embedding_model)
{
    'pearson_cosine': 0.7101597020018693,
    'spearman_cosine': 0.7210536464320728,
    # ... other metrics
}
```
x??

---

#### TSDAE Overview
Background context: The Transformer-Based Sequential Denoising Auto-Encoder (TSDAE) is a method for unsupervised sentence embedding learning. It works by adding noise to sentences and then reconstructing them, with the goal of accurately representing the original sentence in an embedding space.
:p What is TSDAE?
??x
TSDAE is a technique used for creating sentence embeddings without labeled data. The method involves corrupting input sentences by removing certain words, encoding these corrupted sentences, and then trying to reconstruct the original sentences using an auto-encoder approach. This helps in learning meaningful sentence representations.
??x

---

#### Creating Flat Sentences
Background context: To prepare the dataset for TSDAE training, we need to create a flat list of sentences from our input data and ensure that no labels are present to mimic an unsupervised setting.

:p How do you create a flat list of sentences from the MNLI dataset?
??x
To create a flat list of sentences from the MNLI dataset:
```python
from tqdm import tqdm
from datasets import Dataset, load_dataset

# Load the MNLI dataset and select a subset for training
mnli = load_dataset("glue", "mnli", split="train").select(range(25_000))

# Extract premises and hypotheses into one list of sentences
flat_sentences = mnli["premise"] + mnli["hypothesis"]

# Create the TSDAE dataset by adding noise to these sentences
damaged_data = DenoisingAutoEncoderDataset(list(set(flat_sentences)))

# Prepare a dictionary to store damaged and original sentences
train_dataset = {"damaged_sentence": [], "original_sentence": []}

for data in tqdm(damaged_data):
    train_dataset["damaged_sentence"].append(data.texts[0])
    train_dataset["original_sentence"].append(data.texts[1])

# Convert the dictionary into a Dataset object
train_dataset = Dataset.from_dict(train_dataset)
```
This code snippet loads and processes the MNLI dataset to prepare it for TSDAE training.
??x

---

#### Defining Evaluators
Background context: After preparing the dataset, an evaluator needs to be defined to assess the quality of the embeddings generated by the model. The `EmbeddingSimilarityEvaluator` is used here to compare sentence pairs based on their cosine similarity.

:p How do you define an evaluator for TSDAE?
??x
To define an evaluator using the `EmbeddingSimilarityEvaluator`, follow these steps:
```python
from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator

# Load the validation dataset for STS-B
val_sts = load_dataset("glue", "stsb", split="validation")

# Create the evaluator
evaluator = EmbeddingSimilarityEvaluator(
    sentences1=val_sts["sentence1"],
    sentences2=val_sts["sentence2"],
    scores=[score / 5 for score in val_sts["label"]],
    main_similarity="cosine"
)
```
This code snippet sets up an evaluator that will be used to compare sentence pairs based on their cosine similarity.
??x

---

#### Training the TSDAE Model
Background context: The training process involves creating a model with a specific architecture and loss function, then using it to train the model. The key steps include defining the word embedding model, pooling strategy, loss function, and trainer arguments.

:p How do you train the TSDAE model?
??x
To train the TSDAE model, follow these steps:
```python
from sentence_transformers import models, SentenceTransformer
from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator
from sentence_transformers.losses import DenoisingAutoEncoderLoss
from sentence_transformers.trainer import SentenceTransformerTrainer

# Define the word embedding model and pooling strategy
word_embedding_model = models.Transformer("bert-base-uncased")
pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(), "cls")
embedding_model = SentenceTransformer(modules=[word_embedding_model, pooling_model])

# Define the loss function for denoising auto-encoder training
train_loss = losses.DenoisingAutoEncoderLoss(embedding_model, tie_encoder_decoder=True)
train_loss.decoder = train_loss.decoder.to("cuda")

# Set up the training arguments
args = SentenceTransformerTrainingArguments(
    output_dir="tsdae_embedding_model",
    num_train_epochs=1,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    warmup_steps=100,
    fp16=True,
    eval_steps=100,
    logging_steps=100
)

# Train the model
trainer = SentenceTransformerTrainer(
    model=embedding_model,
    args=args,
    train_dataset=train_dataset,
    loss=train_loss,
    evaluator=evaluator
)
trainer.train()
```
This code snippet defines and trains a TSDAE model using the specified configurations.
??x

---

#### Evaluating the Model
Background context: After training, the model's performance is evaluated to assess how well it can reconstruct original sentences from noisy inputs. The evaluation provides various metrics like Pearson and Spearman correlation coefficients.

:p How do you evaluate the trained TSDAE model?
??x
To evaluate the trained TSDAE model, use the following code:
```python
# Evaluate the model using the defined evaluator
results = evaluator(embedding_model)
print(results)
```
The output will provide various metrics such as Pearson and Spearman correlation coefficients for different similarity measures.
??x

#### Domain Adaptation Overview
Background context: When dealing with limited labeled data, unsupervised techniques like TSDAE are used to create an initial embedding model. However, these models often struggle with learning domain-specific concepts effectively compared to supervised methods. Domain adaptation aims to improve these embeddings by adapting them to a specific textual domain.
:p What is the main goal of domain adaptation in text embedding?
??x
The primary goal of domain adaptation is to update existing embedding models so that they can perform better on a target domain, which may contain different subjects and words compared to the source domain. This process involves pre-training with unsupervised techniques followed by fine-tuning using supervised or out-domain data.
x??

---

#### Adaptive Pretraining
Background context: Adaptive pretraining is a method used in domain adaptation where you first train an embedding model on your target domain using an unsupervised technique like TSDAE. This initial training helps the model learn general text representations before it can be fine-tuned with more specific data.
:p What does adaptive pretraining involve?
??x
Adaptive pretraining involves two main steps: 
1. Pre-training a model on the target domain using an unsupervised method such as TSDAE to generate initial embeddings.
2. Fine-tuning this model using either in-domain or out-of-domain data, depending on availability and preference.
```python
# Example of adaptive pretraining with TSDAE
def pretrain_tsdane(target_data):
    # Pre-train the model on target domain data
    model = TSDAE()
    model.fit(target_data)
    return model

model = pretrain_tsdane(target_domain_data)
```
x??

---

#### Fine-Tuning in Domain Adaptation
Background context: After adaptive pretraining, fine-tuning is performed to further improve the model's performance on the target domain. This can be done using either general supervised data or out-of-domain data depending on availability.
:p How does fine-tuning work in domain adaptation?
??x
Fine-tuning involves taking a pretrained model and training it further with additional data from the target domain, which may include labeled data (supervised) or unlabeled data (unsupervised). The objective is to refine the initial embeddings learned during pretraining.
```python
# Example of fine-tuning using Augmented SBERT for out-of-domain data
def fine_tune_augmented_sbert(model, augmented_data):
    # Fine-tune the model on augmented data
    aug_model = AugmentedSBert(model)
    aug_model.train(augmented_data)
    return aug_model

fine_tuned_model = fine_tune_augmented_sbert(model, out_of_domain_data)
```
x??

---

#### Contrastive Learning Basics
Background context: Contrastive learning is a common technique used in embedding models where the model learns from pairs or triples of documents that are either similar or dissimilar. This helps in understanding the structure and relationships within text data.
:p What is contrastive learning?
??x
Contrastive learning is an approach in which a model learns to distinguish between positive (similar) and negative (dissimilar) examples. It typically involves pairs or triples of documents, where the model tries to maximize the similarity score for similar items and minimize it for dissimilar ones.
```python
# Example contrastive loss function
def contrastive_loss(similarity_score):
    # Define a simple contrastive loss function
    return tf.nn.relu(margin - similarity_score) if positive_pair else 1.0

loss = contrastive_loss(similarity_score)
```
x??

---

#### Cosine Similarity Loss in TSDAE
Background context: In the context of TSDAE, cosine similarity is often used as a loss function to ensure that the learned embeddings are semantically meaningful and closely aligned with the source data.
:p What is the role of cosine similarity loss in TSDAE?
??x
Cosine similarity loss ensures that the embeddings produced by the model are semantically similar. It measures the cosine angle between vectors, which helps in maintaining a high degree of alignment between source and target representations during training.
```python
# Example of cosine similarity calculation
def cosine_similarity(vec1, vec2):
    return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))

similarity = cosine_similarity(source_vector, target_vector)
```
x??

---

#### MNR Loss in TSDAE
Background context: MNR loss is another type of loss function used in TSDAE to ensure that the embeddings are not only semantically similar but also diverse. It helps in learning more robust and generalizable representations.
:p What does MNR loss aim to achieve?
??x
MNR (Minimum Near-Neighbor) loss aims to encourage diversity among the nearest neighbors of each embedding by ensuring they are as different as possible from one another, thus promoting a richer representation space.
```python
# Example of MNR loss calculation
def mnr_loss(embeddings):
    # Calculate minimum distance between each embedding and its near-neighbor
    min_distances = [min([np.linalg.norm(e - other) for other in embeddings if e != other]) for e in embeddings]
    return np.mean(min_distances)

loss = mnr_loss(embeddings)
```
x??

---

