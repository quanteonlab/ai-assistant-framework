# Flashcards: 2A004---Hands-On-Large-Language-Models_processed (Part 26)

**Starting Chapter:** From Search to RAG

---

#### Mean Average Precision (MAP)
Background context explaining the concept. MAP is a metric used to evaluate the performance of information retrieval systems by considering the average precision score for every query in the test suite. It provides a single numerical value that allows for comparisons between different search systems.
:p What is mean average precision (MAP)?
??x
Mean Average Precision (MAP) takes into account the average precision scores for each query within the test suite and averages them to produce a single metric, which can be used to compare various search systems. This approach helps in evaluating how well a system ranks relevant documents relative to irrelevant ones.
```python
def calculate_map(relevance_scores):
    # relevance_scores is a list of (query_id, score) tuples
    map_score = 0
    for i, (query_id, score) in enumerate(relevance_scores, start=1):
        precision_at_i = sum(score > s[1] for s in relevance_scores[:i]) / i
        map_score += precision_at_i
    return map_score / len(set(query_id))
```
x??

---

#### Normalized Discounted Cumulative Gain (nDCG)
Background context explaining the concept. nDCG is another metric used to evaluate search systems, taking into account the relevance of documents in a more nuanced manner than binary relevance (relevant or not relevant). This metric considers how much each document contributes to the overall gain.
:p What is normalized discounted cumulative gain (nDCG)?
??x
Normalized Discounted Cumulative Gain (nDCG) is an evaluation metric for search systems that accounts for the varying levels of relevance among documents, providing a more nuanced comparison than binary relevance. It discounts lower ranked items less and considers how much each document contributes to the overall gain.
```python
def calculate_ndcg(relevance_scores, ideal_relevance_scores):
    # relevance_scores is a list of (query_id, doc_id, score) tuples
    dcg = 0
    idcg = sorted(ideal_relevance_scores, reverse=True)
    
    for i in range(len(relevance_scores)):
        score = relevance_scores[i][2]
        ideal_score = idcg[i]
        dcg += (2**score - 1) / np.log2(i + 2)
        idcg_val = (2**ideal_score - 1) / np.log2(i + 2)
    
    if not idcg:
        return 0
    else:
        return dcg / max(0, idcg[-1])
```
x??

---

#### Retrieval-Augmented Generation (RAG)
Background context explaining the concept. RAG combines search capabilities with generation capabilities to improve the factuality and accuracy of responses generated by LLMs. It involves two steps: a retrieval step where relevant documents are retrieved from a knowledge base, followed by a grounded generation step where an LLM is prompted to generate a response based on the retrieved information.
:p What is Retrieval-Augmented Generation (RAG)?
??x
Retrieval-Augmented Generation (RAG) is a method that enhances the performance of LLMs by integrating search capabilities. It involves two steps: first, retrieving relevant documents from a knowledge base using a search step, and then generating responses grounded in the retrieved information via a generation step. This approach helps reduce hallucinations and improves factuality.
```python
def rag_pipeline(query, knowledge_base):
    # Step 1: Search for relevant documents
    retrieved_docs = search_knowledge_base(query)
    
    # Step 2: Grounded generation using LLM
    context = prepare_context(retrieved_docs)
    answer = generate_answer(context, query)
    
    return answer
```
x??

---

#### Generative Search with RAG
Background context explaining the concept. In a generative search system that uses RAG, the question is presented to an LLM along with the top retrieved documents from the previous step of the search pipeline. The LLM then generates an answer based on this context.
:p How does generative search work in the context of RAG?
??x
In the context of RAG, a generative search system works by first using a search engine to retrieve relevant documents for a given query. These retrieved documents are then presented as context to an LLM, which generates a response based on this information. This grounded generation step helps ensure that the generated answer is factually correct and aligned with the provided context.
```python
def generative_search(query):
    # Step 1: Search for relevant documents
    retrieved_docs = search_knowledge_base(query)
    
    # Step 2: Grounded generation using LLM
    context = prepare_context(retrieved_docs)
    answer = generate_answer(context, query)
    
    return answer
```
x??

---

#### Embeddings and Semantic Search
Background context explaining the concept. The use of embeddings in semantic search involves converting text into numerical vectors to enable similarity comparisons. By comparing the embeddings of input queries with those of documents or other data points, relevant information can be identified.
:p How does embedding-based semantic search work?
??x
Embedding-based semantic search converts textual data into numerical vectors that capture the meaning and context of the text. This allows for efficient comparison and identification of similar content by calculating vector similarities. The process involves two main steps: generating embeddings for both queries and documents, followed by comparing these embeddings to find the most relevant documents.
```python
def semantic_search(query_embedding, document_embeddings):
    # Calculate cosine similarity between query and each document embedding
    similarities = [cosine_similarity(qe, de) for qe, de in zip(query_embedding, document_embeddings)]
    
    # Find the index of the highest similarity score
    most_relevant_doc_index = np.argmax(similarities)
    
    return most_relevant_doc_index
```
x??

#### Grounded Generation with an LLM API
Background context: This concept explains how to integrate retrieval-augmented generation (RAG) into a system using Cohere's managed language model. It involves retrieving relevant documents and generating grounded responses based on those documents.

:p How does the example create a grounded answer using Cohere’s managed LLM?
??x
The example creates a grounded answer by first performing an embedding search to retrieve top documents related to the query "income generated." Then, it passes these retrieved documents along with the original question to Cohere's `co.chat` endpoint. The model generates a response that references the specific text from the retrieved documents.

```python
query = "income generated"

# 1- Retrieval
results = search(query)

# 2- Grounded Generation
docs_dict = [{'text': text} for text in results['texts']]
response = co.chat(
    message=query,
    documents=docs_dict
)
print(response.text)
```
x??

---

#### RAG with Local Models
Background context: This concept illustrates how to implement a grounded generation step using local models, such as those from the Hugging Face model hub. It covers loading both text and embedding models, setting up a vector database, and creating an RAG pipeline.

:p How does the example load a quantized model for text generation?
??x
The example loads a quantized model named `Phi-3-mini-4k-instruct-fp16.gguf` using `llama.cpp`, `llama-cpp-python`, and `LangChain`. The code snippet demonstrates setting up the LLM with parameters such as `model_path`, `n_gpu_layers`, `max_tokens`, `n_ctx`, and `seed`.

```python
from langchain import LlamaCpp

# Load the text generation model
llm = LlamaCpp(
    model_path="Phi-3-mini-4k-instruct-fp16.gguf",
    n_gpu_layers=-1,
    max_tokens=500,
    n_ctx=2048,
    seed=42,
    verbose=False
)
```
x??

---

#### Setting Up an Embedding Model for RAG
Background context: This part explains how to load and utilize an embedding model, which is crucial for converting text into numerical representations that can be used in vector databases. It uses a small BAAI embedding model called `bge-small-en-v1.5`.

:p How does the example set up the embedding model?
??x
The example sets up the embedding model using the Hugging Face library and specifically loads the `thenlper/gte-small` model, which is then used to create a vector database.

```python
from langchain.embeddings.huggingface import HuggingFaceEmbeddings

# Load the embedding model
embedding_model = HuggingFaceEmbeddings(
    model_name='thenlper/gte-small'
)
```
x??

---

#### Creating the RAG Prompt Template
Background context: This concept details how to craft a prompt template for the retrieval-augmented generation pipeline. The prompt serves as an intermediary between the retrieved documents and the language model, allowing it to provide more grounded responses.

:p How does the example create a prompt template?
??x
The example creates a prompt template that includes placeholders for context (retrieved documents) and questions. This template is used by the `RetrievalQA` chain type from LangChain.

```python
from langchain import PromptTemplate

# Create a prompt template
template = """<|user|> Relevant information: {context} Provide a concise answer to the following question using the relevant information provided above: {question} <|end|> <|assistant|>"""

prompt = PromptTemplate(
    template=template,
    input_variables=['context', 'question']
)
```
x??

---

#### Implementing the RAG Pipeline
Background context: This section explains how to set up and invoke a retrieval-augmented generation pipeline using local models. It involves creating an `RetrievalQA` chain type that integrates the text generation model, embedding model, and vector database.

:p How does the example implement the RAG pipeline?
??x
The example implements the RAG pipeline by creating an instance of `RetrievalQA` from the LangChain library, specifying the local language model (`llm`), retrieval method, prompt template, and verbose mode. It then invokes the model with a specific query to generate a grounded response.

```python
from langchain import PromptTemplate
from langchain.chains import RetrievalQA

# RAG pipeline setup
rag = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type='stuff',
    retriever=db.as_retriever(),
    chain_type_kwargs={
        "prompt": prompt
    },
    verbose=True
)

# Invoke the model with a query
response = rag.invoke('Income generated')
print(response.text)
```
x??

---

#### Query Rewriting
Background context explaining how query rewriting helps RAG systems, especially chatbots. It involves transforming verbose user queries into more precise ones to improve retrieval accuracy.

:p What is the primary purpose of query rewriting in RAG systems?
??x
The primary purpose of query rewriting in RAG systems is to transform verbose or complex user queries into simpler and more precise queries that can be effectively processed by the retrieval system. This enhances the accuracy and relevance of the search results, making the overall conversation more efficient.

Example:
User Question: "We have an essay due tomorrow. We have to write about some animal. I love penguins. I could write about them. But I could also write about dolphins. Are they animals? Maybe. Let's do dolphins. Where do they live for example?"

Rewritten Query: "Where do dolphins live"

??x
The rewritten query is more direct and focused, making it easier for the retrieval system to find relevant information.

---
#### Multi-query RAG
Background context explaining how multi-query RAG extends single query rewriting by allowing the system to handle multiple queries if needed. This approach improves the chances of finding accurate and complete answers.

:p How does multi-query RAG differ from single-query RAG?
??x
Multi-query RAG differs from single-query RAG in that it handles cases where a single query is insufficient to answer a user's question comprehensively. Instead, multiple queries are generated based on different parts or aspects of the original question. This approach increases the likelihood of finding relevant information across different documents.

Example:
User Question: "Compare the financial results of Nvidia in 2020 vs. 2023"

Queries Generated:
- Query 1: “Nvidia 2020 financial results”
- Query 2: “Nvidia 2023 financial results”

??x
By generating multiple queries, the system can gather more detailed and comprehensive information from various sources.

---
#### Multi-hop RAG
Background context explaining how multi-hop RAG is used for sequential or follow-up queries to handle complex questions that require several steps of reasoning. It involves breaking down a question into smaller parts and searching sequentially for each part.

:p How does multi-hop RAG address complex user questions?
??x
Multi-hop RAG addresses complex user questions by breaking them down into simpler, sequential sub-questions (hops). This method allows the system to gather information step-by-step, ensuring that all necessary details are considered before generating a final response. Each hop builds upon the results of previous hops, leading to more accurate and comprehensive answers.

Example:
User Question: "Who are the largest car manufacturers in 2023? Do they each make EVs or not?"

Hops:
1. Step 1, Query 1: “largest car manufacturers 2023”
   - Results: Toyota, Volkswagen, Hyundai

2. Step 2, Query 1: "Toyota Motor Corporation electric vehicles"
   - Result: Yes

3. Step 2, Query 2: "Volkswagen AG electric vehicles"
   - Result: Yes

4. Step 2, Query 3: "Hyundai Motor Company electric vehicles"
   - Result: Yes

??x
By breaking down the question into smaller parts and addressing each part sequentially, multi-hop RAG ensures that all aspects of the user's query are thoroughly addressed.

---
#### Query Routing
Background context explaining how query routing allows the model to search multiple data sources based on the nature of the question. This improves the relevance of the search results by leveraging specific databases or systems for particular types of queries.

:p How does query routing enhance RAG performance?
??x
Query routing enhances RAG performance by enabling the system to search different data sources depending on the type of information requested. For example, if a user asks about HR-related information, the model can be directed to search an HR information system like Notion, while questions related to customer data would be routed to a CRM system such as Salesforce.

Example:
If the question is: "How many employees are in the marketing department?"

The query routing could instruct the system to search within the company’s HR information system (Notion) for this specific information.

??x
By directing queries to appropriate systems, query routing ensures that the relevant and most accurate data is retrieved, improving the overall performance of RAG systems.

