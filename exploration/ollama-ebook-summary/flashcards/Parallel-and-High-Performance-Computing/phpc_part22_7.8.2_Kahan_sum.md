# Flashcards: Parallel-and-High-Performance-Computing_processed (Part 22)

**Starting Chapter:** 7.8.2 Kahan summation implementation with OpenMP threading

---

#### Super-linear Speedup
Background context: Chapter 7 discusses performance enhancements using OpenMP, focusing on scenarios where the speedup is better than ideal scaling. This phenomenon can occur due to cache effects and optimizations that take advantage of smaller data sizes fitting into higher cache levels.

:p What is super-linear speedup?
??x
Super-linear speedup refers to a situation in parallel computing where the performance of an algorithm exceeds the expected linear improvement when adding more processing units (threads). This can happen because with fewer tasks, each thread operates on smaller chunks of data that fit better into higher cache levels, improving overall cache efficiency.

---
#### Kahan Summation Implementation
Background context: The Kahan summation algorithm is designed to improve the accuracy of floating-point summations by keeping track of lost low-order bits. Implementing this in a parallel environment requires careful handling due to loop-carried dependencies. The provided code demonstrates an OpenMP implementation that splits the task among threads, ensuring correct accumulation.

:p How does the Kahan summation algorithm handle floating-point summation inaccuracies?
??x
The Kahan summation algorithm addresses floating-point summation inaccuracies by maintaining a correction term that captures lost low-order bits during the summation process. This ensures that the accumulated sum is more accurate than traditional summation methods.

:p Explain the logic of the parallel implementation in GlobalSums/kahan_sum.c.
??x
The Kahan summation algorithm is implemented using OpenMP to handle multiple threads, ensuring correct accumulation while respecting loop-carried dependencies. The key steps are:

1. **Initialization**: Set up local variables for each thread and determine their range of work.
2. **Local Summation**: Each thread sums its assigned elements in parallel.
3. **Thread Barrier**: Synchronize the threads to collect results from all other threads.
4. **Global Summation**: Aggregate the partial sums from all threads into a global sum.

Here's the relevant code snippet with explanations:

```c
#include <stdlib.h>
#include <omp.h>

double do_kahan_sum(double* restrict var, long ncells) {
    struct esum_type {  // Kahan summation type structure
       double sum;
       double correction;
    };

    int nthreads = 1; 
    int thread_id = 0;

#ifdef _OPENMP  // Check if OpenMP is enabled
    nthreads = omp_get_num_threads();  // Get the number of threads
    thread_id = omp_get_thread_num();  // Get the current thread ID
#endif

    struct esum_type local; 
    local.sum = 0.0;
    local.correction = 0.0;

    int tbegin = ncells * (thread_id) / nthreads;     // Start index for this thread
    int tend   = ncells * (thread_id + 1) / nthreads; // End index for this thread

    for (long i = tbegin; i < tend; i++) { 
        double corrected_next_term = var[i] + local.correction;
        double new_sum             = local.sum + local.correction;
        local.correction   = corrected_next_term - (new_sum - local.sum);
        local.sum          = new_sum;
    }

    static struct esum_type *thread;      // Shared memory for thread results
    static double sum;                    // Global result

#ifdef _OPENMP  // If OpenMP is enabled
#pragma omp masked  // Masked pragma to allocate shared memory
    thread = malloc(nthreads*sizeof(struct esum_type));
#pragma omp barrier       // Explicit barrier

    thread[thread_id].sum = local.sum;
    thread[thread_id].correction = local.correction;

#pragma omp barrier        // Explicit barrier
#pragma omp masked
    {
       struct esum_type global;  // Global result storage
       global.sum = 0.0;
       global.correction = 0.0;

       for (int i = 0 ; i < nthreads ; i ++) { 
          double corrected_next_term = thread[i].sum +  thread[i].correction + global.correction;
          double new_sum    = global.sum + global.correction;
          global.correction = corrected_next_term - (new_sum - global.sum);
          global.sum = new_sum;
       }

       sum = global.sum + global.correction;  // Final result
       free(thread);  // Free allocated memory
    } // end omp masked
#pragma omp barrier        // Explicit barrier

#else  // If OpenMP is not enabled
    sum = local.sum + local.correction;  // Single-threaded fallback
#endif

    return(sum);
}
```

The code above demonstrates the parallel Kahan summation algorithm, where each thread handles a portion of the array and then combines their results in a synchronized manner. The use of OpenMP pragmas ensures proper synchronization between threads.

x??
```c
#include <stdlib.h>
#include <omp.h>

double do_kahan_sum(double* restrict var, long ncells) {
    struct esum_type {  // Kahan summation type structure
       double sum;
       double correction;
    };

    int nthreads = 1; 
    int thread_id = 0;

#ifdef _OPENMP  // Check if OpenMP is enabled
    nthreads = omp_get_num_threads();  // Get the number of threads
    thread_id = omp_get_thread_num();  // Get the current thread ID
#endif

    struct esum_type local; 
    local.sum = 0.0;
    local.correction = 0.0;

    int tbegin = ncells * (thread_id) / nthreads;     // Start index for this thread
    int tend   = ncells * (thread_id + 1) / nthreads; // End index for this thread

    for (long i = tbegin; i < tend; i++) { 
        double corrected_next_term = var[i] + local.correction;
        double new_sum             = local.sum + local.correction;
        local.correction   = corrected_next_term - (new_sum - local.sum);
        local.sum          = new_sum;
    }

    static struct esum_type *thread;      // Shared memory for thread results
    static double sum;                    // Global result

#ifdef _OPENMP  // If OpenMP is enabled
#pragma omp masked  // Masked pragma to allocate shared memory
    thread = malloc(nthreads*sizeof(struct esum_type));
#pragma omp barrier       // Explicit barrier

    thread[thread_id].sum = local.sum;
    thread[thread_id].correction = local.correction;

#pragma omp barrier        // Explicit barrier
#pragma omp masked
    {
       struct esum_type global;  // Global result storage
       global.sum = 0.0;
       global.correction = 0.0;

       for (int i = 0 ; i < nthreads ; i ++) { 
          double corrected_next_term = thread[i].sum +  thread[i].correction + global.correction;
          double new_sum    = global.sum + global.correction;
          global.correction = corrected_next_term - (new_sum - global.sum);
          global.sum = new_sum;
       }

       sum = global.sum + global.correction;  // Final result
       free(thread);  // Free allocated memory
    } // end omp masked
#pragma omp barrier        // Explicit barrier

#else  // If OpenMP is not enabled
    sum = local.sum + local.correction;  // Single-threaded fallback
#endif

    return(sum);
}
```
x??
---
#### Thread Range Calculation
Background context: Each thread in the parallel implementation must determine its range of work within the array. This ensures that each thread processes a unique subset of the data.

:p How does the code calculate the range for which each thread is responsible?
??x
The code calculates the range for which each thread is responsible by dividing the total number of elements into segments corresponding to the number of threads. The `tbegin` and `tend` variables determine the start and end indices for the current thread.

Here’s a detailed breakdown:
- **nthreads**: Number of threads.
- **thread_id**: Current thread ID.
- **ncells**: Total number of elements in the array.

For each thread, the range is calculated as follows:
1. **tbegin = ncells * (thread_id) / nthreads**: This gives the starting index for the current thread's segment.
2. **tend   = ncells * (thread_id + 1) / nthreads**: This gives the ending index just after the current thread’s segment.

:p Explain how the local variables are managed in each thread.
??x
Local variables in each thread need to be properly managed to ensure correct accumulation and avoid race conditions. Here's a detailed explanation:

- **local.sum** and **local.correction**: These store the partial sum and correction for the current thread’s segment of the array.
- **tbegin** and **tend**: These define the range of elements each thread processes.

During the summation, each thread updates its local variables based on the Kahan algorithm:
```c
double corrected_next_term = var[i] + local.correction;
double new_sum             = local.sum + local.correction;
local.correction   = corrected_next_term - (new_sum - local.sum);
local.sum          = new_sum;
```

After all threads complete their local summation, a barrier ensures synchronization before aggregating the results:
```c
#pragma omp barrier       // Explicit barrier

thread[thread_id].sum = local.sum;
thread[thread_id].correction = local.correction;

#pragma omp barrier        // Explicit barrier
```

Finally, the global sum is computed by combining all partial sums from the threads:
```c
#pragma omp masked
{
   struct esum_type global;  // Global result storage
   global.sum = 0.0;
   global.correction = 0.0;

   for (int i = 0 ; i < nthreads ; i ++) { 
      double corrected_next_term = thread[i].sum +  thread[i].correction + global.correction;
      double new_sum    = global.sum + global.correction;
      global.correction = corrected_next_term - (new_sum - global.sum);
      global.sum = new_sum;
   }

   sum = global.sum + global.correction;  // Final result
}
```

This ensures that the final result is accurate and free from race conditions.

x??
```c
#pragma omp barrier       // Explicit barrier

thread[thread_id].sum = local.sum;
thread[thread_id].correction = local.correction;

#pragma omp barrier        // Explicit barrier
```
x??

#### OpenMP Implementation of Prefix Scan Algorithm
Background context: The prefix scan operation is important for algorithms with irregular data, as it allows parallel computation by determining starting locations for each thread's portion of the data. This implementation uses OpenMP to distribute the workload among multiple threads effectively.

The algorithm consists of three main phases:
1. All threads calculate a prefix scan for their portion of the input array.
2. A single thread calculates the starting offset for each thread’s data.
3. All threads apply these offsets across the entire dataset.

:p What is the role of the `PrefixScan` function in this context?
??x
The `PrefixScan` function performs the parallel prefix scan operation on an array using OpenMP, allowing it to be used both serially and in a parallel environment. It first determines the number of threads and assigns each thread a specific range of data based on its ID.

```c
void PrefixScan(int *input, int *output, int length) {
    #ifdef _OPENMP
        int nthreads = omp_get_num_threads();
        int thread_id = omp_get_thread_num();
    #else
        int nthreads = 1;
        int thread_id = 0;
    #endif

    // Calculate the range for this thread's data
    int tbegin = length * (thread_id) / nthreads;
    int tend   = length * (thread_id + 1) / nthreads;

    if (tbegin < tend) {
        output[tbegin] = 0;  // Initialize starting point

        // Perform prefix scan within the thread's range
        for (int i = tbegin + 1; i < tend; i++) {
            output[i] = output[i - 1] + input[i - 1];
        }
    }

    if (nthreads == 1) return;

    #ifdef _OPENMP
    // Barrier to ensure all threads reach this point before proceeding
    #pragma omp barrier

    // Calculate starting offsets for other threads
    if (thread_id == 0) {
        for (int i = 1; i < nthreads; i++) {
            int ibegin = length * (i - 1) / nthreads;
            int iend   = length * (i)     / nthreads;

            if (ibegin < iend)
                output[iend] = output[ibegin] + input[iend - 1];
        }
    }

    // Barrier to ensure all threads reach this point before proceeding
    #pragma omp barrier

    // Apply the offset to the remaining elements
    #pragma omp simd
    for (int i = tbegin + 1; i < tend; i++) {
        output[i] += output[tbegin];
    }
    #endif
}
```
x??

---
#### Threaded Implementation Phases
Background context: The prefix scan operation can be divided into three phases, each addressing a different aspect of the computation. These phases ensure that data is processed in parallel while maintaining correct results.

1. All threads calculate their portion of the prefix scan.
2. A single thread calculates starting offsets for each thread's range.
3. All threads apply these offsets to complete the scan operation.

:p What are the three main phases of the threaded implementation of the prefix scan algorithm?
??x
The three main phases of the threaded implementation of the prefix scan algorithm are:
1. **All Threads Phase**: Each thread calculates a prefix scan for its portion of the data.
2. **Single Thread Phase**: A single thread (often the first) computes the starting offsets for each thread's data range.
3. **All Threads Phase Again**: All threads then apply these computed offsets to finalize the prefix scan operation.

These phases ensure that the algorithm is correctly parallelized, with initial values and offset calculations handled appropriately before final results are combined in a safe manner.
x??

---
#### OpenMP Directives Used
Background context: The `PrefixScan` function uses several OpenMP directives to manage thread synchronization and data processing. These include `#pragma omp barrier`, which ensures that all threads reach certain points, and `#pragma omp simd`, which helps optimize the loop for parallel execution.

:p What are the two main types of OpenMP directives used in the `PrefixScan` function?
??x
The two main types of OpenMP directives used in the `PrefixScan` function are:
1. **Barrier Directive (`#pragma omp barrier`)**: Ensures that all threads reach a certain point before any thread continues execution beyond it. This is crucial for maintaining correct results when multiple threads need to synchronize.
2. **Simd Directive (`#pragma omp simd`)**: Optimizes a loop for parallel execution by allowing compiler optimizations that take advantage of vector processing capabilities.

These directives help manage synchronization and optimize the performance of the algorithm, ensuring both correctness and efficiency.
x??

---
#### Performance Analysis
Background context: The provided analysis indicates that the `PrefixScan` function scales well with an increasing number of threads. For a Skylake Gold 6152 architecture, it peaks at about 44 threads, achieving 9.4 times faster performance compared to the serial version.

:p What is the theoretical parallelism scaling for the `PrefixScan` function?
??x
The theoretical parallelism scaling for the `PrefixScan` function can be approximated by the formula:
\[ \text{Parallel_timer} = 2 \times \frac{\text{serial_time}}{\text{nthreads}} \]

This suggests that the execution time decreases linearly with an increase in the number of threads, up to a certain point. For example, on a Skylake Gold 6152 architecture, this function scales well until around 44 threads, at which point it reaches its peak performance.

In practice, the actual scaling may vary depending on factors such as cache behavior and load balancing among threads.
x??

---
#### Profiling Tools for OpenMP
Background context: Robust OpenMP implementations require specialized tools to detect thread race conditions and identify performance bottlenecks. Common tools include Valgrind, Call Graph (cachegrind), Allinea/ARM Map, and Intel Inspector.

:p What are some essential tools for profiling and debugging an OpenMP application?
??x
Some essential tools for profiling and debugging an OpenMP application include:

1. **Valgrind**: A memory tool that helps detect uninitialized memory or out-of-bounds accesses in threads.
2. **Call Graph (cachegrind)**: Produces a call graph and profile of the application, helping to visualize function calls and identify performance bottlenecks.
3. **Allinea/ARM Map**: A high-level profiler to determine the overall cost of thread starts and barriers.
4. **Intel Inspector**: Detects thread race conditions.

These tools are crucial for understanding both memory behavior and performance issues in OpenMP applications, enabling developers to optimize their code effectively.
x??

---

#### Using Allinea/ARM MAP for Profiling OpenMP Applications
Allinea/ARM MAP is a tool that provides a high-level profile of your application, helping identify bottlenecks and memory usage efficiently. It's particularly useful for OpenMP applications as it highlights thread starts and waits, CPU utilization, and floating point operations.

:p What does Allinea/ARM MAP provide when profiling an OpenMP application?
??x
Allinea/ARM MAP provides a high-level profile that includes the cost of thread starts and waits, identifies bottlenecks in the application, and shows memory and CPU usage. It also helps compare performance changes before and after code modifications.

```cpp
// Example code snippet to demonstrate starting a thread using OpenMP
#include <omp.h>

int main() {
    int numThreads = 4;
    #pragma omp parallel num_threads(numThreads)
    {
        // Thread body here
    }
}
```
x??

---

#### Finding Thread Race Conditions with Intel® Inspector
Intel® Inspector is essential for detecting and pinpointing thread race conditions, which are critical for ensuring a robust production-quality application. Memory errors can cause applications to break as they scale, so catching these issues early helps prevent future problems.

:p How does Intel® Inspector assist in finding thread race conditions?
??x
Intel® Inspector uses advanced techniques to detect memory errors and thread race conditions. It provides detailed reports that highlight the locations of these race conditions, ensuring that developers can address them effectively before releasing their applications.

```cpp
// Example code snippet demonstrating potential race condition
int sharedVar = 0;

void threadFunction() {
    int localVar;
    // Critical section where a race condition might occur
    localVar = sharedVar + 1; // Potential race condition if accessed concurrently

    sharedVar = localVar; // Atomic operation is needed to avoid race conditions
}
```
x??

---

#### Importance of Regression Testing for OpenMP Implementations
Regression testing ensures the correctness of an application or subroutine before implementing OpenMP threading. This is crucial because a correct OpenMP implementation relies on proper working state and exercised sections of code.

:p Why is regression testing important in OpenMP implementations?
??x
Regression testing is essential because it verifies that changes made to the application do not introduce new bugs or regressions. It ensures that the threaded section of the code works correctly, maintaining the overall correctness and reliability of the application.

```java
// Example test cases for a function before and after OpenMP implementation
public void testFunction() {
    // Test case 1: Initial state
    assertEquals(expectedValue1, calculateValue(input1));
    
    // Test case 2: After applying OpenMP
    // Ensure that the parallel version of the function still works correctly
    assertEquals(expectedValue2, parallelCalculateValue(input2));
}
```
x??

---

