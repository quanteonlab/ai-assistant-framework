# Flashcards: Parallel-and-High-Performance-Computing_processed (Part 18)

**Starting Chapter:** 7.1.1 OpenMP concepts

---

---
#### OpenMP Overview
OpenMP is an open standard for shared-memory parallel programming, widely supported by hardware vendors and compilers. It was initially developed in the 1990s but gained prominence with the advent of multi-core systems in the late '90s.

:p What is OpenMP?
??x
OpenMP is a standard that allows developers to write parallel code for shared-memory systems using directives or pragmas, which are compiler-specific annotations. It simplifies parallel programming by providing an API that requires minimal changes to existing serial code.
x??

---
#### Version History of OpenMP
The development and evolution of OpenMP have been significant, with its first standard in 1997 following the widespread introduction of multi-core systems.

:p When did OpenMP first emerge as a standard?
??x
OpenMP was first standardized in 1997, driven by the increasing use of multi-core systems. Prior to this, it had several implementations by hardware vendors in the early 1990s.
x??

---
#### Ease of Use with OpenMP
One of the key benefits of OpenMP is its ease of use and quick implementation for adding parallelism to applications.

:p Why is OpenMP considered easy to use?
??x
OpenMP uses pragmas or directives within code, which are recognized by the compiler. This allows for parallelization without major structural changes to the application. The minimal increase in coding complexity typically seen with a few lines of pragma/directive makes it accessible even for beginners.
x??

---
#### Memory Models and OpenMP
OpenMP has a relaxed memory model that can lead to race conditions due to delayed updates of shared variables.

:p What is the relaxed memory model in OpenMP?
??x
The relaxed memory model means that changes to variables are not immediately reflected in all threads. This can cause race conditions, where different outcomes occur based on timing differences between threads accessing the same variable.
x??

---
#### Private and Shared Variables
In OpenMP, private variables are local to a thread, while shared variables can be modified by any thread.

:p How do private and shared variables differ in OpenMP?
??x
- **Private Variable**: Local to a single thread; not visible or modifiable by other threads.
- **Shared Variable**: Visible and modifiable by multiple threads. Variables declared as `private` are local to each thread, whereas those declared as `shared` can be accessed across all threads.

Example:
```c
#pragma omp parallel for private(i) shared(data)
for (int i = 0; i < n; ++i) {
    // Thread-specific work on data[i]
}
```
x??

---
#### Work Sharing and First Touch
Work sharing involves distributing tasks among threads, while first touch refers to the allocation of memory based on which thread accesses it first.

:p Explain what work sharing and first touch mean in OpenMP.
??x
- **Work Sharing**: Dividing a task into smaller units that can be executed concurrently by multiple threads.
- **First Touch**: Memory is allocated only when accessed for the first time, typically near the thread where it is first used. This reduces memory fragmentation but may introduce NUMA penalties on multi-node systems.

Example:
```c
int array[1024];
#pragma omp parallel for firstprivate(array)
for (int i = 0; i < 1024; ++i) {
    // Thread-specific work using array[i]
}
```
x??

---
#### OpenMP Barrier and Flush Operations
OpenMP barriers synchronize threads, ensuring that all locally modified values are flushed to main memory.

:p What is the role of an OpenMP barrier?
??x
An OpenMP barrier ensures that all threads reach a synchronized point before proceeding. It flushes any locally modified values to main memory, preventing race conditions and ensuring consistency among threads.
x??

---
#### NUMA Considerations with OpenMP
Non-Uniform Memory Access (NUMA) can affect performance on multi-node systems where different CPUs have access to different memory regions.

:p What is Non-Uniform Memory Access (NUMA)?
??x
NUMA occurs when the memory layout of a system is not uniform, meaning that some processors have faster access to certain memory regions than others. This can impact performance if data is frequently accessed across different NUMA domains.
x??

---
#### Summary of Key Concepts
This summary covers the essential aspects of OpenMP, including its ease of use, version history, and key programming concepts like private and shared variables.

:p What are the main points to remember about OpenMP?
??x
- **Ease of Use**: Minimal changes required through pragmas/directives.
- **Version History**: Developed in the 1990s, standardized in 1997.
- **Memory Model**: Relaxed with potential for race conditions due to delayed updates.
- **Variables**: Private (thread-specific) vs. Shared (accessible by all threads).
- **Work Sharing and First Touch**: Techniques for efficient memory management.
x??

---

---
#### OpenMP Parallel Pragma
OpenMP is a widely used API for parallel programming. The `#pragma omp parallel` directive creates a region of code where multiple threads can be spawned to work concurrently on different parts of the program. This allows tasks to be distributed among these threads, potentially improving performance.

:p What does the `#pragma omp parallel` directive do?
??x
The `#pragma omp parallel` directive starts a parallel region in your code, allowing for thread creation within this section. Each thread can execute its own copy of the code in the region, effectively splitting tasks among them.

Example usage:
```cpp
#include <omp.h>
int main() {
    #pragma omp parallel
    {
        // Code here will be executed by multiple threads.
    }
}
```
x??

---
#### OpenMP for Directive (Loop Work Sharing)
The `#pragma omp for` directive is used to distribute the iterations of a loop across multiple threads. This helps in parallelizing loops where each iteration can be processed independently.

:p How does the `#pragma omp for` directive work?
??x
The `#pragma omp for` directive tells OpenMP that the following loop should have its iterations distributed among available threads. The work is divided equally between the threads, and scheduling clauses like static, dynamic, guided, or auto can be used to control how iterations are assigned.

Example usage:
```cpp
#include <omp.h>
int main() {
    int arr[10];
    #pragma omp parallel for
    for(int i = 0; i < 10; ++i) {
        // Each thread processes one iteration of the loop.
    }
}
```
x??

---
#### OpenMP Combined Parallel and Work Sharing Directive
The `#pragma omp parallel for` directive combines both `parallel` and `for` directives. It first creates a region where multiple threads can be spawned, and then distributes the iterations of the following loop among these threads.

:p What does combining `#pragma omp parallel` with `#pragma omp for` do?
??x
Combining `#pragma omp parallel` with `#pragma omp for` allows you to both create a parallel region and distribute loop iterations across multiple threads. This is useful when the workload can be split into independent tasks that need to run in parallel.

Example usage:
```cpp
#include <omp.h>
int main() {
    int arr[10];
    #pragma omp parallel for
    for(int i = 0; i < 10; ++i) {
        // Each thread processes one iteration of the loop.
    }
}
```
x??

---
#### OpenMP Scheduling Clauses
Scheduling clauses like `static`, `dynamic`, `guided`, and `auto` control how iterations are assigned to threads in a work-sharing construct. These clauses help manage load balancing effectively.

:p What is the purpose of scheduling clauses in OpenMP?
??x
The purpose of scheduling clauses is to control how iterations are distributed among threads, which can significantly impact performance by managing load balance. Common clauses include:

- `static`: Splits work into chunks and assigns them sequentially.
- `dynamic`: Dynamically assigns chunks as needed.
- `guided`: A hybrid approach that starts with static and switches to dynamic if necessary.
- `auto`: Allows the runtime environment to decide on a scheduling method.

Example usage:
```cpp
#include <omp.h>
int main() {
    #pragma omp parallel for schedule(static)
    for(int i = 0; i < 10; ++i) {
        // Iterations are assigned statically.
    }
}
```
x??

---
#### OpenMP Reduction Directive
The `#pragma omp reduction` directive is used to perform reductions (like sum, min, max) among threads. This is useful when multiple threads need to combine their results.

:p What does the `#pragma omp reduction` do?
??x
The `#pragma omp reduction` directive combines values from different threads into a single value using specified operations like sum, minimum, or maximum. It is used in work-sharing constructs where thread-private copies of a variable are updated and need to be combined.

Example usage:
```cpp
#include <omp.h>
int main() {
    int arr[10];
    #pragma omp parallel for reduction(+:sum)
    for(int i = 0; i < 10; ++i) {
        sum += arr[i]; // Sum is updated in each thread and combined.
    }
}
```
x??

---
#### OpenMP Barrier Directive
The `#pragma omp barrier` directive creates a synchronization point where all threads must wait until they have reached this point. This ensures that no thread proceeds beyond the barrier before all others.

:p What does the `#pragma omp barrier` do?
??x
The `#pragma omp barrier` directive acts as a stop point in your parallel code, ensuring that all threads reach this point before any of them can proceed further. This is crucial for maintaining consistency and preventing race conditions.

Example usage:
```cpp
#include <omp.h>
int main() {
    #pragma omp parallel
    {
        // Thread-specific processing.
        #pragma omp barrier
        // Code here will only execute after all threads have reached this point.
    }
}
```
x??

---
#### OpenMP Serial Sections (masked)
The `#pragma omp masked` directive ensures that a block of code runs serially on thread zero without creating any barriers. It is useful for critical sections where only one thread should execute.

:p What does the `#pragma omp masked` do?
??x
The `#pragma omp masked` directive executes a region of code on thread zero and prevents other threads from executing it. This allows you to run specific parts serially within a parallel section without creating additional synchronization overhead.

Example usage:
```cpp
#include <omp.h>
int main() {
    #pragma omp parallel
    {
        #pragma omp masked
        {
            // Code here will be executed only on thread zero.
        }
    }
}
```
x??

---
#### OpenMP Master Directive (masked)
The `#pragma omp master` directive is a synonym for `#pragma omp masked`. It ensures that the following block of code runs on thread zero and prevents other threads from executing it.

:p What does the `#pragma omp master` do?
??x
The `#pragma omp master` directive executes a region of code only on thread zero, ensuring that no other threads can execute this block. This is useful for sections where only one thread should perform certain tasks.

Example usage:
```cpp
#include <omp.h>
int main() {
    #pragma omp parallel
    {
        #pragma omp master
        {
            // Code here will be executed only on thread zero.
        }
    }
}
```
x??

---
#### OpenMP Environment Variable for Thread Count
The `OMP_NUM_THREADS` environment variable can be used to set the number of threads to use in a parallel region. This allows you to dynamically adjust the thread count based on your needs.

:p How do you set the number of threads using an environment variable?
??x
You can set the number of threads by exporting the `OMP_NUM_THREADS` environment variable. For example, setting it to 16 would limit the number of threads to 16 for the duration of the program execution.

Example usage:
```sh
export OMP_NUM_THREADS=16
```
x??

---
#### OpenMP Function Call for Thread Count (omp_set_num_threads)
The `omp_set_num_threads` function can be called directly from your code to set the number of threads. This provides more control over thread allocation compared to using environment variables.

:p How do you set the number of threads using a function call?
??x
You can use the `omp_set_num_threads` function to explicitly set the number of threads that OpenMP should use. For example, setting it to 16 would limit the number of threads to 16 for the duration of your program.

Example usage:
```cpp
#include <omp.h>
int main() {
    omp_set_num_threads(16);
    // Rest of the code.
}
```
x??

---
#### Introduction to OpenMP Hello World Program
Background context: The provided C code demonstrates a simple "Hello, OpenMP" program. It includes essential components like including the OpenMP header and making function calls to get the number of threads and thread ID.

:p What is the purpose of the first `printf` statement in Listing 7.1?
??x
The initial `printf` statement prints "Goodbye slow serial world and Hello OpenMP," which serves as an introductory message indicating that the program is transitioning from a sequential execution to parallel execution using OpenMP.
x??

---
#### Declaring Variables for Thread Count and ID
Background context: The code declares variables `nthreads` and `thread_id` to store the number of threads and the current thread's identifier, respectively.

:p What are the roles of `nthreads` and `thread_id` in this program?
??x
The variable `nthreads` is used to store the total number of threads available for parallel execution. The variable `thread_id` holds the unique identifier (ID) of each thread within the range [0, nthreads-1]. These variables are essential for understanding and managing the threads.
x??

---
#### Adding a Parallel Region
Background context: To leverage multiple threads, the code introduces an OpenMP parallel region using the `#pragma omp parallel` directive. This directive spawns new threads that execute the enclosed block of code concurrently.

:p How does the `#pragma omp parallel` directive work in this program?
??x
The `#pragma omp parallel` directive signals to the compiler that the following blocks of code should be executed by multiple threads. In Listing 7.2, it initiates a parallel region where each thread executes the enclosed block independently.

```c
#pragma omp parallel >> Spawn threads >>
{
    nthreads = omp_get_num_threads();
    thread_id = omp_get_thread_num();
    // The rest of the code inside the parallel region
}
```
The `>> Spawn threads` annotation visually indicates that new threads are spawned.
x??

---
#### Implied Barrier
Background context: After the parallel region, there is an implied barrier. This means that all threads synchronize at this point before any thread continues executing beyond the end of the parallel region.

:p What is the role of the "Implied Barrier" in Listing 7.2?
??x
The "Implied Barrier" ensures that after the parallel region executes, all threads wait until each one has completed its part of the parallel block before proceeding further. This synchronization prevents race conditions and ensures that results are consistent across different threads.
x??

---
#### Race Condition Example
Background context: The provided output shows multiple threads reporting they have thread ID 3. This happens because `nthreads` and `thread_id` are shared variables, meaning their values can be overwritten by any thread during the execution of the parallel region.

:p Why do all threads report being thread number 3 in the output?
??x
All threads report being thread number 3 because `nthreads` and `thread_id` are shared between threads. When multiple threads execute the code inside the parallel region simultaneously, the last thread to write a value to these variables determines their final state for all threads.
x??

---

---
#### Parallel Region and Thread IDs
Background context: In threaded programs, thread IDs are crucial for identifying which thread is executing. Without proper handling, race conditions can occur due to shared variables being accessed by multiple threads simultaneously.

:p What happens if we don't define `thread_id` inside the parallel region in a multi-threaded program?
??x
When `thread_id` is defined outside the parallel region, all threads share the same variable, leading to inconsistent thread IDs and potential race conditions. Each thread writes its ID to the shared variable, but the final value can be unpredictable.

Code example:
```c
#pragma omp parallel // Spawn threads
{
    int nthreads = omp_get_num_threads();      // This is shared across all threads
    int thread_id = omp_get_thread_num();      // Thread-id is also shared and thus incorrect.
    printf("Goodbye slow serial world and Hello OpenMP. ");
    printf("I have %d thread(s) and my thread id is %d", nthreads, thread_id);
}
```
x??

---
#### Single Region to Minimize Output
Background context: To minimize output in a parallel region and ensure only one thread writes the data, we use the `#pragma omp single` directive. This ensures that only one thread executes the block of code within it.

:p How can you modify the code to ensure only one thread prints the message?
??x
By using `#pragma omp single`, only one thread will execute the block inside this region. Here, we define and use `nthreads` and `thread_id` inside the parallel region to make them private to each thread.

Code example:
```c
#pragma omp parallel // Spawn threads
{
    int nthreads = omp_get_num_threads();       // This is now local to each thread
    int thread_id = omp_get_thread_num();       // Thread-id is also local and thus correct.
    #pragma omp single                          // Only one thread will execute this block
    {
        printf("Number of threads is %d ", nthreads);
        printf("My thread id %d", thread_id);
    }
}
```
x??

---
#### OpenMP and Race Conditions
Background context: In the earlier example, shared variables like `nthreads` and `thread_id` can lead to race conditions because multiple threads might write to them concurrently. Using private or local variables helps avoid such issues.

:p What is a race condition in this context?
??x
A race condition occurs when the program's behavior depends on the sequence or timing of events, which can vary between different executions due to concurrent access and modification of shared data by multiple threads.

Code example:
```c
#pragma omp parallel // Spawn threads
{
    int nthreads = omp_get_num_threads();       // This is now local to each thread
    int thread_id = omp_get_thread_num();       // Thread-id is also local and thus correct.
    printf("Goodbye slow serial world and Hello OpenMP. ");
    printf("I have %d thread(s) and my thread id is %d", nthreads, thread_id);
}
```
x??

---

---
#### Masked vs Single Clauses
Background context explaining how masked and single clauses work. These clauses control which threads execute certain blocks of code, with different restrictions.

:p What is the difference between the `masked` and `single` OpenMP clauses?
??x
The `single` clause allows a block of code to be executed by exactly one thread, chosen by the compiler, while the `masked` clause restricts execution to thread 0. The `masked` clause does not include an implicit barrier at the end.

```c
#pragma omp single { ... } // Execution on one thread (chosen by the compiler)
#pragma omp masked { ... } // Only thread 0 can execute this block, no implicit barrier
```

x??
---

#### Parallel Region Variables and Scope
Explanation of how variables defined outside a parallel region are shared in the parallel region. The importance of defining variables at the smallest possible scope is discussed.

:p How do variables behave when defined outside a parallel region in OpenMP?
??x
Variables defined outside a parallel region are shared among all threads by default. It's recommended to define them only within the necessary scope to ensure correct behavior and avoid unintended interactions between threads. This helps in managing thread-private data effectively.

```c
int nthreads = omp_get_num_threads(); // Shared across threads
#pragma omp parallel {
   int thread_id = omp_get_thread_num(); // Thread-private variable
}
```

x??
---

#### Implied Barrier and Its Use
Explanation of the implied barrier that exists after an `#pragma omp parallel` directive. The importance of barriers in managing thread synchronization is discussed.

:p What is an implied barrier, and when does it exist in OpenMP?
??x
An implied barrier in OpenMP exists at the end of a parallel region created by `#pragma omp parallel`. It ensures that all threads have completed their work before any continue past this point. This helps manage synchronization between threads effectively.

```c
#pragma omp parallel {
   // Parallel code here
} // Implied barrier after this block
```

x??
---

#### Conditional Pragma for Thread Zero
Explanation of how to limit the execution of a statement to thread zero using a conditional within an OpenMP pragma.

:p How can you execute a piece of code only on thread 0 in an OpenMP parallel region?
??x
You can use a conditional inside an `#pragma omp` directive to restrict the execution to thread 0. This approach avoids the need for masked pragmas and leverages conditional statements within the parallel region.

```c
#pragma omp parallel {
   if (omp_get_thread_num() == 0) {
      printf("Thread zero specific code\n");
   }
}
```

x??
---

#### OpenMP Version Updates and Features
Overview of updates and new features in recent versions of OpenMP, including task parallelism, loop improvements, reduction operators, and vectorization.

:p What are some major features added to OpenMP over the last decade?
??x
Recent versions of OpenMP have introduced several key features:
- Task parallelism and improved loop parallelism (loop collapse and nested parallelism).
- New reduction operations like `min` and `max`.
- Support for thread binding.
- Vectorization capabilities.
- Offloading tasks to accelerators, such as GPUs.

```c
// Example of using a reduction operation in OpenMP
#pragma omp parallel for reduction(+:sum)
for (int i = 0; i < N; ++i) {
   sum += array[i];
}
```

x??
---

---
#### Loop-Level OpenMP Use Case
Loop-level OpenMP is typically used when your application requires modest speedup and has sufficient memory resources, meaning its requirements can be satisfied by a single hardware node. This scenario is ideal for applications where parallelism is needed at the loop level.

:p What are the characteristics of an application suitable for loop-level OpenMP?
??x
An application that needs modest parallelism, ample memory resources (low memory requirements), and where expensive calculations occur in just a few `for` or `do` loops would be suitable for using loop-level OpenMP. This approach is particularly effective when the benefits from parallelizing certain critical sections of code outweigh the overheads.

Version 4.0 introduced the SIMD directive, which supports vectorization; version 4.5 improved support for GPU devices with substantial improvements to accelerator device support; and version 5.0 further enhanced support for accelerator devices.
x??

---
#### High-Level OpenMP Use Case
High-level OpenMP is used when your application requires a significant speedup but can benefit from the integration of parallelism at higher levels, such as through data sharing or complex task management.

:p What are the characteristics of an application suitable for high-level OpenMP?
??x
Applications that require substantial speedups and can benefit from more sophisticated parallelism than simple loop-based parallelization might be good candidates for high-level OpenMP. This approach allows developers to manage complex interactions between threads, data sharing, and task management.

:p Provide a code example of using the `parallel` directive in C/C++.
??x
```c
#include <omp.h>

int main() {
    #pragma omp parallel
    {
        int thread_id = omp_get_thread_num();
        printf("Hello from thread %d\n", thread_id);
    }
    return 0;
}
```
This code uses the `#pragma omp parallel` directive to create a team of threads that execute the enclosed block. Each thread will print its ID.
x??

---
#### MPI Plus OpenMP Use Case
MPI plus OpenMP is used when your application needs both distributed memory (through MPI) and shared memory (through OpenMP) capabilities, allowing for more complex parallelization scenarios.

:p When would you use MPI plus OpenMP?
??x
MPI plus OpenMP is useful in applications that require a combination of distributed memory (handled by MPI) and shared memory (handled by OpenMP). This approach enables the efficient utilization of both local and remote resources within a cluster, making it suitable for large-scale parallel computing tasks.

:p Provide an example of integrating MPI and OpenMP.
??x
```c
#include <mpi.h>
#include <omp.h>

int main(int argc, char **argv) {
    int rank, size;
    #pragma omp parallel default(none) shared(size)
    {
        int thread_id = omp_get_thread_num();
        printf("Thread %d on process %d\n", thread_id, rank);
    }

    MPI_Init(&argc, &argv);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);
    MPI_Comm_size(MPI_COMM_WORLD, &size);

    #pragma omp parallel default(none) shared(rank)
    {
        int thread_id = omp_get_thread_num();
        printf("Thread %d on process %d\n", thread_id, rank);
    }

    MPI_Finalize();

    return 0;
}
```
This code integrates both OpenMP and MPI to manage threads and processes. The `#pragma omp parallel` directive is used within the context of an MPI application to create a team of OpenMP threads on each process.
x??

---

