# Flashcards: Parallel-and-High-Performance-Computing_processed (Part 28)

**Starting Chapter:** Part 3GPUs Built to accelerate

---

#### One-sided Communication in MPI
One-sided communication, introduced by adding `MPI_Puts` and `MPI_Gets`, differs from the traditional message-passing model where both sender and receiver must be active. In this new model, only one of them needs to conduct the operation. This feature enables more flexible and efficient parallel programming scenarios.
:p What is the key difference between one-sided communication in MPI compared to traditional point-to-point communication?
??x
One-sided communication allows for more flexibility because either the sender or receiver can initiate an operation without both being active participants, whereas in traditional message-passing models, both parties need to be engaged simultaneously.

For example:
```c
// C code snippet demonstrating one-sided communication using MPI_Put and MPI_Get
MPI_Put(buffer, count, datatype, dest_proc, dest_offset, src_proc, tag);
MPI_Get(buffer, count, datatype, src_proc, src_offset, dest_proc, tag);
```
x??

---

#### Blocking vs. Non-blocking Receives in Ghost Exchange
Blocking on receives can be problematic because it can cause processes to wait indefinitely if no message arrives, leading to potential hangs. In contrast, non-blocking receives use functions like `MPI_Irecv` and `MPI_Waitall`, allowing for more flexible control over data exchanges.
:p Why cannot we just block on receives as was done in the send/receive in the ghost exchange using pack or array buffer methods?
??x
Blocking on receives can lead to indefinite waiting if a message does not arrive, causing processes to hang. For example, consider the following scenario where a process blocks on `MPI_Recv` expecting a message that never comes:
```c
// C code snippet showing potential issue with blocking receive
status = MPI_Status;
MPI_Recv(buffer, count, datatype, source, tag, communicator, &status);
```
x??

---

#### Advantages of Blocking Receives in Ghost Exchange
Non-blocking receives can avoid hangs but may complicate the programming model. Blocking receives simplify the code and ensure that processes do not hang.
:p Is it safe to block on receives as shown in listing 8.8 in the vector type version of the ghost exchange?
??x
Blocking on receives is generally safer because it prevents indefinite waiting, ensuring that processes do not hang even if messages are delayed or lost.

For example:
```c
// C code snippet showing a safe blocking receive
status = MPI_Status;
MPI_Recv(buffer, count, datatype, source, tag, communicator, &status);
```
x??

---

#### Performance Impact of Blocking Receives in Ghost Exchange
Replacing `MPI_Waitall` with blocking receives in the ghost cell exchange example can affect performance and reliability.
:p Modify the ghost cell exchange vector type example in listing 8.21 to use blocking receives instead of a waitall. Is it faster? Does it always work?
??x
Using blocking receives might simplify the code but could impact performance, especially if messages are not guaranteed to arrive. The `MPI_Recv` function will block until a message arrives, which can introduce latency.

Example:
```c
// Pseudocode for blocking receive in ghost cell exchange
for (int i = 0; i < count; ++i) {
    MPI_Recv(buffer[i], size, datatype, source, tag, communicator, &status);
}
```
This approach may not be faster and does not guarantee consistent performance.

x??

---

#### Tag Usage in Ghost Exchange Routines
Using `MPI_ANY_TAG` can simplify the code by allowing any tag to be used, but explicit tags provide better control and performance.
:p Try replacing the explicit tags in one of the ghost exchange routines with MPI_ANY_TAG. Does it work? Is it any faster?
??x
Replacing explicit tags with `MPI_ANY_TAG` simplifies the code by allowing any message tag, but this might not always be optimal. Explicit tags provide better control and can help avoid race conditions or ensure proper message handling.

Example:
```c
// Original code using explicit tags
status = MPI_Status;
MPI_Recv(buffer, count, datatype, source, 0, communicator, &status);
```
Replacing with `MPI_ANY_TAG`:
```c
// Code using MPI_ANY_TAG
status = MPI_Status;
MPI_Recv(buffer, count, datatype, MPI_ANY_SOURCE, MPI_ANY_TAG, communicator, &status);
```
Using explicit tags can help in optimizing the program and ensuring proper message handling.

x??

---

#### Synchronized Timers vs. Unsynchronized Timers
Removing barriers for synchronized timers can affect performance measurements by introducing variability.
:p Remove the barriers for the synchronized timers in one of the ghost exchange examples. Run the code with original synchronized timers and unsynchronized timers.
??x
Removing barriers from synchronized timers can introduce variability into timing measurements, affecting the accuracy of performance metrics.

Example:
```c
// Original code with synchronized timers using barriers
for (int i = 0; i < count; ++i) {
    MPI_Walltime(&start_time[i], &dummy);
    // Exchange logic
    MPI_Barrier(communicator); // Barrier to synchronize all processes
}
```
Without barriers:
```c
// Code without barriers
for (int i = 0; i < count; ++i) {
    MPI_Walltime(&start_time[i], &dummy);
    // Exchange logic
}
```
Barriers are crucial for accurate timing and performance measurement.

x??

---

#### Adding Timer Statistics to Stream Triad Bandwidth Measurement Code
Adding timer statistics can provide more detailed insights into the performance of stream triad bandwidth measurements.
:p Add the timer statistics from listing 8.11 to the stream triad bandwidth measurement code in listing 8.17.
??x
Adding timer statistics involves measuring the time taken for specific operations within the stream triad bandwidth measurement code. This can provide a more detailed analysis of performance.

Example:
```c
// Code snippet adding timer statistics
for (int i = 0; i < count; ++i) {
    MPI_Walltime(&start_time[i], &dummy);
    // Perform stream triad operations
    MPI_Walltime(&end_time[i], &dummy);
}
```
This allows you to track the time taken for each operation, providing a comprehensive performance profile.

x??

---

#### Converting High-level OpenMP to Hybrid MPI Plus OpenMP Example
Converting high-level OpenMP to hybrid MPI plus OpenMP involves integrating both paradigms for better parallelism.
:p Apply the steps to convert high-level OpenMP to the hybrid MPI plus OpenMP example in the code that accompanies the chapter (HybridMPIPlusOpenMP directory). Experiment with vectorization, number of threads, and MPI ranks on your platform.
??x
Converting high-level OpenMP to a hybrid MPI plus OpenMP approach involves integrating both paradigms. This can be done by using OpenMP for thread parallelism within processes and MPI for process communication.

Example:
```c
// Pseudocode for hybrid MPI+OpenMP example
#pragma omp parallel shared(data)
{
    // Perform operations with OpenMP threads
}
MPI_Bcast(&count, 1, MPI_INT, 0, communicator);
```
Experimenting with vectorization (using intrinsics or compiler directives) and adjusting the number of threads and ranks can optimize performance.

x??

---

#### Summary of Key Concepts in MPI Programming
Key concepts in MPI programming include point-to-point communication, collective operations, ghost exchanges, and combining MPI with OpenMP for more parallelism.
:p Summarize key concepts in MPI programming discussed in this section?
??x
Key concepts in MPI programming include:
1. **Point-to-Point Communication**: Proper use of send/receive functions to avoid hangs and optimize performance.
2. **Collective Communication**: Using collective operations like `MPI_Bcast` for concise, safe, and efficient communication among processes.
3. **Ghost Exchanges**: Implementing subdomain exchanges using techniques like vector types to simulate global meshes.
4. **Hybrid Parallelism**: Combining MPI with OpenMP and vectorization to achieve higher levels of parallelism.

These concepts are essential for writing scalable and efficient parallel programs in MPI.

x??

---

#### Introduction to GPU Programming
The following chapters will cover the basics of GPU programming, starting from understanding GPU architecture and its benefits. You'll explore programming models, languages like OpenACC and OpenCL, and more advanced GPU languages.
:p What topics will be covered in the upcoming chapters on GPU computing?
??x
The upcoming chapters on GPU computing will cover:
1. **GPU Architecture**: Understanding the unique features and advantages of GPUs for general-purpose computation.
2. **Programming Models**: Developing a mental model for programming GPUs.
3. **GPU Programming Languages**: Exploring both low-level languages like CUDA, OpenCL, HIP, and high-level ones such as SYCL, Kokkos, and Raja.

These topics will provide a comprehensive introduction to GPU computing from basic examples to more advanced language implementations.

x??

---

---
#### Background on GPUs and Their Evolution
Background context explaining the development of GPUs from their initial focus on graphics to broader applications. Mention the term coined by Mark Harris (2002) as general-purpose graphics processing units (GPGPUs).
:p What is the evolution history and key term for GPUs?
??x
GPUs initially focused on accelerating computer animation but have evolved into versatile parallel accelerators used in various domains such as machine learning, high-performance computing, and bitcoin mining. In 2002, Mark Harris coined the term "General-Purpose Graphics Processing Units" (GPGPUs) to emphasize their broader applicability beyond graphics.
x??

---
#### Markets for GPUs
Provide details on the diverse markets that have emerged for GPUs, including Bitcoin mining, machine learning, and high-performance computing. Highlight the customization of GPU hardware for each market with examples like double-precision floating-point units and tensor operations.
:p Which are some major markets for GPUs?
??x
Major markets for GPUs include:
1. **Bitcoin Mining**: Utilizes GPUs to solve complex mathematical problems required for mining cryptocurrencies.
2. **Machine Learning**: Employs GPUs to perform large-scale matrix operations efficiently, essential for training deep learning models.
3. **High-Performance Computing (HPC)**: Uses GPUs for simulations and data analysis that require high computational power.

Customizations such as double-precision floating-point units and tensor operations are made to tailor GPU designs for each market segment.
x??

---
#### Comparison Between GPUs and CPUs
Explain the differences between GPUs and CPUs, focusing on their respective strengths in handling parallel vs. sequential tasks. Mention the price range of high-end GPUs and why they might not replace CPUs in all applications due to specialized operations better suited for CPUs.
:p How do GPUs and CPUs differ?
??x
GPUs excel at performing a large number of simultaneous parallel operations, making them ideal for tasks that can be broken down into many small, independent units. In contrast, CPUs are optimized for handling sequential tasks with high efficiency and precision.

High-end GPUs can command prices up to $10,000 but are not likely to replace CPUs entirely because some single-operation tasks are better suited for the CPU's specialized architecture.
x??

---
#### Speedup Achievable by GPUs
Highlight the significant speedup GPUs can provide over CPUs in parallelizable applications. Mention that while the exact speedup varies based on application and code quality, it is often around ten times.
:p How much speedup do GPUs typically offer compared to CPUs?
??x
GPUs can achieve a speedup of approximately ten times over CPUs for tasks that are highly parallelizable. However, this speedup varies significantly depending on the specific application and the quality of the code implementation.

For example:
```java
// Pseudocode demonstrating GPU-accelerated matrix multiplication
public class MatrixMul {
    public static void multiplyMatrices(double[][] A, double[][] B, double[][] C) {
        int n = A.length;
        for (int i = 0; i < n; i++) {
            for (int j = 0; j < n; j++) {
                for (int k = 0; k < n; k++) {
                    C[i][j] += A[i][k] * B[k][j];
                }
            }
        }
    }
}
```
x??

---
#### Future of GPUs in Parallel Computing
Discuss the ongoing significance of GPUs in parallel computing, including their ease of design and manufacture compared to CPUs. Mention the performance trends where GPUs have been improving faster than CPUs since 2012.
:p Why are GPUs important for the parallel computing community?
??x
GPUs are crucial for parallel computing because they are simpler to design and manufacture, leading to a shorter design cycle time (half that of CPUs). Since around 2012, GPU performance has been improving at about twice the rate of CPUs. This trend indicates that GPUs will continue to provide greater speedups for applications that can fit their massively parallel architecture.

For instance:
```java
// Pseudocode demonstrating a simple parallel task using GPU concepts
public class ParallelTask {
    public static void processTasks(int[] tasks) {
        int threads = 10; // Number of parallel threads
        for (int i = 0; i < threads; i++) {
            new Thread(() -> {
                for (int j = i; j < tasks.length; j += threads) {
                    System.out.println(tasks[j]);
                }
            }).start();
        }
    }
}
```
x??

---

#### Importance of Understanding GPU Hardware Design
Background context: In chapter 10, the essential parts of the hardware design for GPUs are discussed to help developers understand their functionality and limitations. This understanding is crucial before starting a project involving GPUs because many porting efforts have failed due to incorrect assumptions.

:p Why is it important to understand the hardware design of GPUs before starting a project?
??x
It is important to understand the hardware design of GPUs because moving only the most expensive loop to the GPU often does not result in significant speedups. Transferring data between the CPU and GPU can be expensive, so large parts of your application need to be ported to see any benefit. A performance model and analysis before implementation would help manage expectations.

---

#### Performance Modeling Before GPU Implementation
Background context: Developing an initial performance model and analysis is essential before implementing a GPU solution. This helps manage programmers' expectations and ensures they allocate sufficient time and effort for the project. Incorrect assumptions can lead to abandoned efforts when applications run slower than expected after porting.

:p What does the author suggest developers do before starting a GPU implementation?
??x
Developers should create a simple performance model and analysis before implementing on GPUs. This would help manage initial expectations and ensure that they plan adequately for the time and effort required, as simply moving expensive loops to the GPU may not result in significant speedups.

---

#### Programming Language Landscape for GPUs
Background context: The landscape of programming languages for GPUs is constantly evolving, making it challenging for application developers. While new languages are frequently released, many share common designs or dialects, reducing the complexity. Chapters 11 and 12 cover different language implementations.

:p What are the key observations about GPU programming languages mentioned in the text?
??x
The key observations are that while there is a constant evolution of programming languages for GPUs, most of these languages share similarities and common designs. This means they can be treated more like dialects rather than entirely new languages. Developers should initially choose a couple of languages to gain hands-on experience.

---

#### Setting Up Development Environment for GPU Programming
Background context: Access to hardware is one of the barriers in GPU programming, especially when setting up the development environment properly. Chapters 13 discusses different workflows and alternatives such as Docker containers and virtual machines (VMs) that can be used on laptops or desktops. Cloud services with GPUs are also mentioned for those without local hardware.

:p What challenges do developers face when setting up a GPU development environment?
??x
Developers often face the challenge of installing system software to support GPUs, which can be difficult. They may need to install specific software packages and use vendor-provided lists. However, these steps involve some trial and error. Using Docker containers or virtual machines can help set up an environment on laptops or desktops.

---

#### Example Cloud Services for GPU Development
Background context: For developers without local hardware, cloud services with GPUs are recommended. These include services from Google Cloud and Intel, which provide free trials and marketplace add-ons to set up HPC clusters.

:p What cloud services does the text recommend for GPU development?
??x
The text recommends using Google Cloud (with a $200-300 credit) or Intel's cloud version of oneAPI and DPCPP. These services offer free trials and allow setting up HPC clusters with GPUs, such as the Fluid Numerics Google Cloud Platform and Intel's cloud service for trial GPU usage.

---

#### Why GPUs are Important for High-Performance Computing (HPC)
Background context: Understanding why GPUs are essential for HPC involves recognizing their ability to perform a massive number of parallel operations, which surpasses that of conventional CPUs. This is due to the design of GPUs to handle large numbers of threads simultaneously.

:p What makes GPUs suitable for high-performance computing?
??x
GPUs excel in high-performance computing because they can execute thousands of threads concurrently, unlike CPUs which are optimized for a smaller number of threads but with higher computational intensity per thread. This parallelism allows GPUs to process data much faster when the workload is well-suited for parallel execution.
x??

---
#### Systems That Utilize GPU Acceleration
Background context: Many modern computing systems incorporate GPUs not just for graphical processing, but also for general-purpose computing due to their high-performance capabilities.

:p Which types of systems are commonly equipped with GPUs?
??x
Systems such as personal computers (especially those used for simulation or gaming), workstations, and HPC clusters often utilize GPUs. These systems benefit from the increased computational power provided by GPUs.
x??

---
#### Components of a GPU-Accelerated System
Background context: Understanding the hardware components of a GPU-accelerated system is crucial to effectively using GPUs for computing tasks.

:p What are the key hardware components in a GPU-accelerated system?
??x
The key hardware components include:
- CPU (main processor)
- CPU RAM (memory sticks or DIMMs containing DRAM)
- GPU (large peripheral card installed in a PCIe slot)
- GPU RAM (memory modules dedicated to the GPU)
- PCI bus (wiring that connects the GPU to other motherboard components)

The CPU and GPU have their own memory, and they communicate over the PCI bus.
x??

---
#### Terminology for GPUs
Background context: The terminology used can vary between different vendors. This chapter uses OpenCL standards but also notes common terms like those from NVIDIA.

:p What are some key terminologies related to GPUs?
??x
Key terminologies include:
- CPU (main processor installed in the motherboard socket)
- CPU RAM (memory sticks or DIMMs containing DRAM inserted into memory slots on the motherboard)
- GPU (large peripheral card in a PCIe slot)
- GPU RAM (memory modules dedicated for exclusive use by the GPU)
- PCI bus (wiring that connects peripherals to other components on the motherboard)

This terminology helps in understanding and discussing the hardware setup.
x??

---
#### Estimating Theoretical Performance of GPUs
Background context: To effectively utilize GPUs, it is important to understand their theoretical performance limits. This includes knowing how to calculate the maximum theoretical throughput.

:p How can you estimate the theoretical performance of a GPU?
??x
To estimate the theoretical performance of a GPU, consider its specifications such as the number of cores and the frequency at which they operate. Theoretical performance can be calculated using the following formula:

$$\text{Theoretical Performance} = (\text{Number of Cores}) \times (\text{Frequency (in Hz)})$$

For example, if a GPU has 2048 cores running at 1500 MHz, its theoretical performance would be $2048 \times 1500 = 3072000$ operations per second.
x??

---
#### Measuring Actual Performance with Micro-Benchmarks
Background context: While the theoretical performance gives an upper limit, actual performance can vary based on factors like memory bandwidth and communication overhead. Micro-benchmark applications are used to measure real-world performance.

:p How do you use micro-benchmarks to measure GPU performance?
??x
Micro-benchmarks allow for precise measurement of a GPU's actual performance by running small, well-defined tasks that stress specific hardware aspects. These tests can help identify bottlenecks in the system and provide insights into how efficiently the GPU is utilized.

For example, a simple micro-benchmark might involve performing a large number of matrix multiplications to test the GPUâ€™s floating-point performance.
x??

---
#### Applications Benefiting from GPU Acceleration
Background context: Certain types of applications are better suited for GPU acceleration due to their parallel nature and data-intensive requirements.

:p Which applications benefit most from GPU acceleration?
??x
Applications that can be accelerated using GPUs include:
- Machine learning and artificial intelligence (AI) training and inference
- Scientific simulations, such as molecular dynamics or fluid dynamics
- Data analytics and big data processing
- Graphics rendering for real-time applications like video games

These applications often involve large datasets and tasks that are well-suited for parallel processing.
x??

---
#### Goals for Achieving Performance Gains with GPUs
Background context: To effectively port an application to run on a GPU, it is important to understand the goals of achieving performance gains. This includes optimizing code for parallel execution and understanding memory hierarchies.

:p What should be your goals when porting an application to run on GPUs?
??x
Your goals when porting an application to run on GPUs include:
- Maximizing parallelism to leverage the large number of cores available in a GPU.
- Optimizing memory access patterns to reduce latency and improve bandwidth utilization.
- Minimizing communication overhead between the CPU and GPU.
- Ensuring that data is efficiently transferred between different levels of the memory hierarchy.

By focusing on these aspects, you can significantly enhance the performance of your application.
x??

#### Definition of Accelerator
Background context: An accelerator is a special-purpose device that supplements the main general-purpose CPU, speeding up certain operations. It can be either integrated on the CPU or as a dedicated peripheral card.

:p What is an accelerator?
??x
An accelerator is a specialized hardware component designed to speed up specific tasks relative to the primary CPU. For example, GPUs are accelerators used for graphics and now general-purpose computing tasks.
x??

---
#### Integrated GPU vs Dedicated GPU
Background context: Integrated GPUs are built directly into the CPU chip and share RAM resources with the CPU, while dedicated GPUs are attached via a PCI slot and have their own memory.

:p What is the difference between integrated GPUs and dedicated GPUs?
??x
Integrated GPUs are part of the CPU and share its memory. Dedicated GPUs have their own memory and can offload some computational tasks from the CPU.
x??

---
#### Integrated GPU on Intel Processors
Background context: Intel has traditionally included an integrated GPU with CPUs for budget markets, but recently Ice Lake processors have claimed to match AMD's integrated GPU performance.

:p What is the status of integrated GPUs in modern Intel processors?
??x
Modern Intel processors like Ice Lake can provide integrated GPU performance that is comparable to or even surpasses traditional AMD integrated GPUs.
x??

---
#### AMD Accelerated Processing Units (APUs)
Background context: APUs are a combination of CPU and GPU, sharing processor memory. They aim for cost-effective but high-performance systems in the mass market by eliminating PCI bus data transfer.

:p What are APUs?
??x
APUs combine CPUs and GPUs on the same chip, using shared memory to reduce performance bottlenecks and offering both processing power and graphics capabilities.
x??

---
#### Role of Integrated GPU in Commodity Systems
Background context: Many commodity desktops and laptops now have integrated GPUs that can provide modest performance boosts for scientific and data science applications.

:p What is the role of integrated GPUs in modern computing?
??x
Integrated GPUs in modern systems offer a relatively modest performance boost, reduce energy costs, and improve battery life. They are particularly useful for basic computational tasks.
x??

---
#### CUDA Programming Language
Background context: CUDA was introduced by NVIDIA in 2007 to enable general-purpose GPU programming.

:p What is CUDA?
??x
CUDA is a programming model and API developed by NVIDIA that allows developers to harness the power of GPUs for general-purpose computing, beyond just graphics.
x??

---
#### OpenCL Programming Language
Background context: OpenCL, released in 2009, is an open standard GPGPU language developed by Apple and other vendors.

:p What is OpenCL?
??x
OpenCL is a cross-platform framework for parallel programming of heterogeneous systems that can run on various CPUs and GPUs.
x??

---
#### Directive-Based APIs (OpenACC & OpenMP)
Background context: To simplify GPU programming, directive-based APIs like OpenACC and OpenMP with the new target directive were developed.

:p What are directive-based APIs?
??x
Directive-based APIs such as OpenACC and OpenMP with the new target directive allow programmers to specify parallel regions in code without deep knowledge of the underlying hardware.
x??

---
#### Example of Directive-Based API (OpenACC)
Background context: OpenACC directives can be used to instruct compilers on how to offload specific sections of code to the GPU.

:p How do OpenACC directives work?
??x
OpenACC directives, such as `acc` and `acc_kernel`, allow developers to annotate code sections for automatic parallelization by the compiler. For example:
```c++
// Example using OpenACC in C
#include <openacc.h>

void myFunction() {
    #pragma acc kernels
    for (int i = 0; i < N; i++) {
        // Some computation here
    }
}
```
x??

---
#### PCI Bus and Data Transfer
Background context: The PCI bus is a physical component that allows data transmission between the CPU and GPU. It can be a performance bottleneck if not optimized.

:p What is the PCI bus?
??x
The PCI bus (Peripheral Component Interconnect) is a physical interface allowing data to be transmitted between the CPU and GPU, potentially causing performance bottlenecks.
x??

---
#### Performance of Dedicated GPUs vs Integrated GPUs
Background context: Dedicated GPUs are generally considered superior for extreme performance tasks compared to integrated GPUs due to their dedicated memory.

:p Why are dedicated GPUs preferred over integrated GPUs?
??x
Dedicated GPUs provide better performance in extreme cases because they have their own memory and can handle more complex computational workloads, making them the undisputed champions for high-performance computing.
x??

---

