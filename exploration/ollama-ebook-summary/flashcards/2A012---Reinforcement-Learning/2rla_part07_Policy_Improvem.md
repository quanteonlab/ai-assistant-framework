# Flashcards: 2A012---Reinforcement-Learning_processed (Part 7)

**Starting Chapter:** Policy Improvement

---

#### Value Function and Policy Evaluation

**Background context**: This section discusses how to evaluate a policy using value functions. The value function $v_\pi(s)$ represents the expected return starting from state $s$ under policy $\pi$. In Example 4.1, an equiprobable random policy is used where all actions are equally likely.

:p What is the question about evaluating a policy's value function?
??x
To evaluate a policy's value function means to calculate the expected long-term return starting from each state $s $ under that policy$\pi $. The value of being in state $ s $and following policy$\pi$ can be represented by:
$$v_\pi(s) = E_{\pi}\left[\sum_{t=0}^{\infty} \gamma^t R_t | S_0=s\right]$$

For a deterministic policy, this simplifies to evaluating the state value function iteratively until it converges.
??x

---
#### Action-Value Function and Its Approximation

**Background context**: The action-value function $q_\pi(s,a)$ represents the expected return starting from state $s$ taking action $a$, then following policy $\pi$. It is a key component in determining whether to change policies. 

:p What is the question about the action-value function?
??x
The action-value function $q_\pi(s, a)$ gives the expected return for starting from state $s$ and taking action $a$, then following policy $\pi$:

$$q_\pi(s, a) = E_{\pi}\left[R_{t+1} + \gamma v_\pi(S_{t+1}) | S_t=s, A_t=a\right]$$

This is the expected sum of discounted future rewards starting from state $s $ and taking action$a$.
??x

---
#### Policy Improvement Theorem

**Background context**: This theorem states that if a new policy $\pi'$ improves on an existing policy $\pi$ in terms of the action-value function, then it will also yield a better expected return from all states.

:p What is the question about the policy improvement theorem?
??x
The policy improvement theorem asserts that if for every state $s $, the action $ a^*$that maximizes the action-value function under an existing policy $\pi$ satisfies:
$$q_\pi(s, a^*) > v_\pi(s)$$

Then it must be true that applying the new greedy policy (which is greedy with respect to $v_\pi $) will yield a value function$ v_{\pi'}(s) \geq v_\pi(s)$. If strict inequality holds for any state, then:

$$v_{\pi'}(s) > v_\pi(s)$$

Thus, the new policy is strictly better.
??x

---
#### Policy Improvement Algorithm for Stochastic Policies

**Background context**: For stochastic policies, the probability of taking each action in a given state can be used to calculate the expected return more flexibly. The policy improvement theorem holds true even when the policy is stochastic.

:p What is the question about stochastic policies?
??x
When dealing with stochastic policies, the new greedy policy $\pi'$ should select actions based on their expected returns under the current value function $v_\pi$. If multiple actions tie for the maximum action-value in state $ s$, these actions can share the probability of being selected. The value function for any such stochastic policy $\pi'$ will be better than or equal to that of the original policy.$$v_{\pi'}(s) = \max_a \sum_{a', s'} p(a' | s, a) [r(s, a, s') + \gamma v_\pi(s')] $$where $ p(a'|s,a)$ is the probability distribution over actions under the new policy.
??x

---
#### Example of Policy Improvement with Stochastic Policies

**Background context**: This example demonstrates how to apply policy improvement in a stochastic setting using the equiprobable random policy as an initial state. The goal is to find a better policy by making it greedy based on the value function.

:p What is the question about applying policy improvement for stochastic policies?
??x
In Example 4.1, the original policy $\pi $ is the equiprobable random policy where each action has equal probability. Policy improvement involves finding the new greedy policy that maximizes$q_\pi(s,a)$ for all states.

For state 15, if it transitions to itself with all actions except down which transitions to state 13:
- The value function $v_\pi(15)$ can be evaluated based on the original grid.
- When state 13's action "down" is changed to transition to state 15, the new policy must be re-evaluated.

By making $\pi'$ greedy with respect to $v_\pi$, we get a better policy.
??x

---
#### Policy Improvement for Deterministic Policies

**Background context**: When dealing with deterministic policies, the policy improvement involves selecting actions that maximize the immediate and future rewards. The process ensures that the new policy is at least as good as the old one.

:p What is the question about deterministic policies?
??x
For deterministic policies, the goal of policy improvement is to select the action $a $ in state$s$ that maximizes the expected return based on the current value function:
$$\pi'(s) = \arg\max_a q_\pi(s,a) = \arg\max_a E_{\pi}\left[R_{t+1} + \gamma v_\pi(S_{t+1}) | S_t=s, A_t=a\right]$$

If this new policy is as good as or better than the original policy, it means that $v_\pi = v_{\pi'}$ and both policies are optimal.

This process ensures that we get a strictly better policy unless the original policy was already optimal.
??x

--- 
Note: Each flashcard contains only one question and provides detailed explanations relevant to the concept discussed. Use of code examples is minimal as most concepts are explained through logic and formulas. ---

#### Policy Iteration Overview
Policy iteration is an algorithm for finding an optimal policy in a finite Markov Decision Process (MDP). It involves two main steps: policy evaluation and policy improvement. The process starts with an arbitrary policy, evaluates it to determine its value function, then improves the policy based on this value function, and repeats until convergence.

The key idea is that each iteration results in a strictly better or equivalent policy than the previous one, ensuring eventual optimality.
:p What does policy iteration involve?
??x
Policy iteration involves two main steps: first, performing a policy evaluation to determine the value function for the current policy, and second, improving the policy based on this new value function. This process is repeated until the policy no longer changes significantly between iterations, indicating that an optimal policy has been found.
x??

---
#### Policy Evaluation
During each iteration of policy iteration, the value function for the current policy needs to be computed. This is done using iterative methods such as value iteration or policy evaluation.

The update rule for state values during policy evaluation is given by:
$$v_{\pi}(s) \leftarrow \sum_{s'} P(s', r| s, a) [r + \gamma v_{\pi}(s')]$$where $ P(s', r| s, a)$is the probability of transitioning to state $ s'$and receiving reward $ r$from taking action $ a $ in state $ s $, and $\gamma$ is the discount factor.

The process continues until the value function converges within a specified tolerance.
:p What is the update rule for state values during policy evaluation?
??x
The update rule for state values during policy evaluation is:
$$v_{\pi}(s) \leftarrow \sum_{s'} P(s', r| s, a) [r + \gamma v_{\pi}(s')]$$

This means that the value of a state $s $ under policy$\pi $ is updated by summing over all possible next states$s'$, taking into account the transition probability and the immediate reward plus the discounted value of the next state.
x??

---
#### Policy Improvement
After evaluating a new policy, policy improvement involves updating the policy to maximize the expected return based on the current value function. The key idea is to choose actions that lead to higher values.

For each state $s$, the action to take is:
$$\pi'(s) = \arg\max_a \sum_{s'} P(s', r| s, a) [r + \gamma v_{\pi}(s')]$$where $ v_{\pi}$is the value function under policy $\pi$.

If the new action for state $s$ differs from the old one, the policy is considered unstable and another iteration of policy evaluation is needed.
:p What is the formula for determining the optimal action during policy improvement?
??x
The formula for determining the optimal action during policy improvement is:
$$\pi'(s) = \arg\max_a \sum_{s'} P(s', r| s, a) [r + \gamma v_{\pi}(s')]$$

This means that for each state $s $, the action$ a $that maximizes the expected return (immediate reward plus discounted future value under policy$\pi$) is chosen as the new policy's action.
x??

---
#### Policy Iteration Algorithm
The complete algorithm for policy iteration consists of multiple iterations, with each iteration involving policy evaluation and policy improvement.

1. **Initialization**: Start with an arbitrary policy $\pi(0)$.
2. **Policy Evaluation Loop**:
   - For each state $s$, update the value function until it converges.
3. **Policy Improvement**:
   - Update the policy for each state to find actions that maximize the expected return.
4. **Convergence Check**: If the policy does not change significantly, stop and return the current policy and value function; otherwise, go back to step 2.

The process is guaranteed to converge in a finite number of iterations due to the improvement property of policies.
:p What are the steps involved in the policy iteration algorithm?
??x
The steps involved in the policy iteration algorithm are:
1. **Initialization**: Start with an arbitrary policy $\pi(0)$.
2. **Policy Evaluation Loop**:
   - For each state $s$, update the value function until it converges.
3. **Policy Improvement**:
   - Update the policy for each state to find actions that maximize the expected return.
4. **Convergence Check**: If the policy does not change significantly, stop and return the current policy and value function; otherwise, go back to step 2.

The process is guaranteed to converge in a finite number of iterations due to the improvement property of policies.
x??

---
#### Jack’s Car Rental Example
Jack manages two car rental locations. Customers arrive according to Poisson distributions with different means for requests and returns at each location. The goal is to minimize the cost of moving cars while maximizing revenue from rentals.

The state space consists of pairs $(n_1, n_2)$, where $ n_1$and $ n_2$are the number of cars at locations 1 and 2, respectively. Actions involve moving between 0 and 5 cars overnight with a cost of$2 per car moved.

The discount factor is set to $\gamma = 0.9$.
:p What are the key elements in Jack’s Car Rental problem setup?
??x
The key elements in Jack’s Car Rental problem setup include:
- **State Space**: Pairs $(n_1, n_2)$ representing the number of cars at each location.
- **Actions**: Moving between 0 and 5 cars overnight with a cost of$2 per car moved.
- **Reward Structure**: Renting out a car earns $10, while moving a car incurs a cost of$2.
- **Transition Model**: Customers arrive according to Poisson distributions with different means for requests and returns at each location.
- **Discount Factor**: $\gamma = 0.9$ to account for the time value of money.
x??

---

#### Policy Iteration Overview
Policy iteration is a method for solving Markov Decision Processes (MDPs) that alternates between policy evaluation and policy improvement steps. The policy evaluation step computes the value function of a given policy, while the policy improvement step finds an improved policy based on this value function.

:p What does policy iteration do?
??x
Policy iteration involves alternating between two steps: policy evaluation and policy improvement. Policy evaluation calculates the state values under a current policy, and policy improvement uses these values to find a better policy.
x??

---

#### Convergence of Policy Iteration
In some cases, policy iteration can find an optimal policy in just one iteration because the policies become fixed after the first few steps.

:p In what scenario would policy iteration converge quickly?
??x
Policy iteration may converge very fast if there exist policies that are already close to optimality or when the problem has a structure that allows quick convergence. For example, in Jack's car rental problem, both the equiprobable random policy and the greedy policy derived from it might be optimal after just one iteration.
x??

---

#### Bug in Policy Iteration
Policy iteration may not terminate if the policy continually switches between equally good policies.

:p How can we modify policy iteration to ensure convergence?
??x
To ensure that policy iteration converges, you should add a termination condition based on small changes in value functions. This involves checking whether the change in value function values between iterations is below a certain threshold.
x??

---

#### Policy Iteration for Action Values
Policy iteration can be extended to consider action values (q-values) instead of state values.

:p How would policy iteration be defined for action values?
??x
For policy iteration using q-values, you first evaluate the q-function under the current policy and then improve the policy based on these q-values. The steps are similar but involve working with q-values rather than v-values.
x??

---

#### -Soft Policies
A policy is $\epsilon$-soft if it assigns a non-zero probability to each action in each state.

:p How would the policy iteration algorithm change for $\epsilon$-soft policies?
??x
For $\epsilon$-soft policies, you need to modify the steps as follows:
1. Policy Evaluation: Ensure that all actions are considered with at least $\epsilon/|A(s)|$ probability.
2. Policy Improvement: Use these probabilities in action selection.
3. Policy Iteration: The overall structure remains the same but involves handling the $\epsilon$-soft condition.

Example pseudocode:
```java
for each state s do
    for each action a in A(s) do
        if random() < epsilon / |A(s)| then
            select a with probability (1 - epsilon + epsilon/|A(s)|)
```
x??

---

#### Value Iteration Algorithm
Value iteration is an alternative to policy iteration that directly maximizes the value function.

:p What is value iteration and how does it work?
??x
Value iteration combines policy improvement and truncated policy evaluation into a single update rule. It iteratively updates the state values by taking the maximum over all possible actions, ensuring convergence without needing separate policy evaluation steps.
x??

---

#### Termination Condition for Value Iteration
Value iteration can stop when the change in value function is small enough.

:p How does value iteration determine when to terminate?
??x
Value iteration stops when the difference between successive iterations of the value function is less than a predefined threshold $\epsilon$. This ensures that the algorithm terminates while still providing an approximate solution.
x??

---

#### Gambler's Problem Example
The gambler’s problem involves betting on coin flips with a goal to reach $100.

:p What is the optimal policy in the gambler’s problem?
??x
In the gambler’s problem, the optimal policy is to bet all remaining money if it will push you over $100. For lower stakes, it’s more nuanced but generally involves betting enough to make significant progress towards the goal without risking too much.
x??

---

#### Implementation of Value Iteration for Gambler's Problem
Implementing value iteration requires handling the state and action spaces carefully.

:p How would you implement value iteration for the gambler’s problem?
??x
To implement value iteration for the gambler's problem, initialize values for each capital level. In each sweep, update the value of each state by considering all possible actions (bets) that can be made from that state. The optimal action is chosen to maximize the expected return.
```java
for each state s do
    v[s] = max_a(sum_{s',r} P(s',r|s,a)(r + v[s']))
```
x??

---

