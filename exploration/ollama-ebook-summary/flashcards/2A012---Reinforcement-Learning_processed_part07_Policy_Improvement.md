# Flashcards: 2A012---Reinforcement-Learning_processed (Part 7)

**Starting Chapter:** Policy Improvement

---

#### Exercise 4.1 - q⇡(11,down ) and q⇡(7,down )
Background context: In Example 4.1, the gridworld is a 4x4 environment with states numbered from 1 to 14, where actions up, down, right, and left are possible. The reward for every transition except the terminal state is -1. The final policy \( \pi \) is an equiprobable random policy, meaning each action has an equal probability of being chosen (0.25).

The formula to calculate the action value function \( q_{\pi}(s,a) \) is given by:
\[ q_{\pi}(s,a) = E[R_{t+1} + \gamma v_{\pi}(S_{t+1}) | S_t=s, A_t=a] \]
where \( R_{t+1} \) is the reward at time \( t+1 \), and \( \gamma \) is the discount factor (which is 0 in this case since it's an undiscounted task).

The policy \( \pi \) being random means that each action has a probability of 0.25.

:p What is the value of \( q_{\pi}(11, down ) \)?
??x
To find \( q_{\pi}(11, down ) \), we need to consider the possible transitions from state 11 when taking the "down" action and sum up their contributions weighted by their probabilities.

- If the agent moves down (from 11 to 12), it incurs a reward of -1. The next state is 12, and since \( \pi \) is random, each action from 12 has an equal probability.
- If the "down" move would take the agent out of bounds (which it does in this case), the agent stays in state 11.

Since the policy is equiprobable random:
\[ q_{\pi}(11, down ) = -1 + \frac{1}{4} v_{\pi}(12) + \frac{3}{4} (-1) \]

Given that \( v_{\pi}(s) \) for all states except the terminal state is approximately the expected number of steps to reach the terminal state, and since moving from 11 to 12 incurs an extra step:
\[ q_{\pi}(11, down ) = -1 + \frac{1}{4} (-2) + \frac{3}{4} (-1) = -1 -0.5 -0.75 = -2.25 \]

Thus,
??x
The value of \( q_{\pi}(11, down ) \) is approximately -2.25.
x??

---
#### Exercise 4.1 - v⇡(15)
Background context: A new state 15 is added to the gridworld below state 13. The original transitions are unchanged, and from state 13, moving down should take the agent to state 15.

The value function \( v_{\pi}(s) \) for a policy \( \pi \) can be calculated using the Bellman equation:
\[ v_{\pi}(s) = E[R_t + v_{\pi}(S_{t+1}) | S_t=s] \]

:p What is the value of \( v_{\pi}(15) \)?
??x
To find \( v_{\pi}(15) \), we need to consider all possible transitions from state 15 and their probabilities.

- If moving left, up, or right from state 15 takes the agent back to states 14, 12, or 16 (which is out of bounds).
- Since the policy \( \pi \) is random:
\[ v_{\pi}(15) = -1 + \frac{1}{3} (-1) + \frac{1}{3} (-1) + \frac{1}{3} (-2) \]

This simplifies to:
\[ v_{\pi}(15) = -1 - \frac{1}{3} - \frac{1}{3} - \frac{2}{3} = -1 - 1 = -2 \]

Thus,
??x
The value of \( v_{\pi}(15) \) is -2.
x??

---
#### Exercise 4.1 - v⇡(15) with state 13 changed
Background context: The dynamics of state 13 are changed such that action "down" from state 13 takes the agent to state 15, and all other actions remain unchanged.

:p What is the value of \( v_{\pi}(15) \)?
??x
With the new dynamics, moving down from state 13 now goes to state 15. The value function for state 15 can be calculated considering the transitions from states that lead to 15:

- If moving left, up, or right from a neighboring state (12, 14) leads back to these states.
- Since policy \( \pi \) is random:
\[ v_{\pi}(15) = -1 + \frac{1}{3} (-2) + \frac{1}{3} (-2) + \frac{1}{3} (-2) \]

This simplifies to:
\[ v_{\pi}(15) = -1 - \frac{2}{3} - \frac{2}{3} - \frac{2}{3} = -1 - 2 = -3 \]

Thus,
??x
The value of \( v_{\pi}(15) \) is -3.
x??

---
#### Exercise 4.1 - Difference in v⇡(s)
Background context: The original policy \( \pi \) being random results in a lower expected number of steps to the terminal state compared to the new policy which is greedy with respect to \( v_{\pi}(s) \).

:p What is the difference between \( v_{\pi}(s) \) and \( v_{\pi}^0(s) \)?
??x
The original value function \( v_{\pi}(s) \) for a random policy \( \pi \) has an upper bound of -14, meaning it takes up to 14 steps on average to reach the terminal state. The new greedy policy \( \pi^0 \) selects actions that maximize immediate reward and future values based on \( v_{\pi}(s) \).

Since the greedy policy reduces the number of steps needed to reach the terminal state, we have:
\[ v_{\pi}^0(s) \leq -3 \]
for all states \( s \), whereas for the random policy:
\[ v_{\pi}(s) \leq -14 \]

Thus,
??x
The difference between \( v_{\pi}(s) \) and \( v_{\pi}^0(s) \) is that \( v_{\pi}^0(s) \) provides a better estimate of the value, reducing the number of steps required to reach the terminal state.
x??

---
#### Policy Improvement for Stochastic Policies
Background context: The policy improvement theorem extends from deterministic policies to stochastic policies. In the stochastic case, if there are ties in policy improvement steps such as \( \arg\max_a q_{\pi}(s,a) \), then each maximizing action can be given a portion of the probability in the new greedy policy.

:p What is the process of policy improvement for stochastic policies?
??x
The process of policy improvement for stochastic policies involves creating a new policy that selects actions based on their expected immediate reward and future values according to \( v_{\pi}(s) \). If there are ties, each maximizing action can be given a probability in the new policy.

For example, if multiple actions achieve the maximum value:
\[ q_{\pi}(s,a_1) = q_{\pi}(s,a_2) = \max_a q_{\pi}(s,a) \]
then these actions can share the probability:
\[ \pi^0(a | s) = \frac{P}{k} \]
where \( P \) is a probability distribution, and \( k \) is the number of maximizing actions.

Thus,
??x
The process of policy improvement for stochastic policies involves creating a new greedy policy that selects actions with higher expected values according to \( v_{\pi}(s) \), and in case of ties, these actions share the probability proportionally.
x??

--- 
#### Bellman Optimality Equation
Background context: The Bellman optimality equation is used to find the optimal value function:
\[ v^*(s) = \max_a q_{\pi^*}(s,a) = \max_a E[R_{t+1} + v^*(S_{t+1}) | S_t=s, A_t=a] \]

:p How does policy improvement relate to the Bellman optimality equation?
??x
Policy improvement involves creating a new greedy policy \( \pi^0(s) = \arg\max_a q_{\pi}(s,a) \). If this new policy is not strictly better than the original, it implies that the original policy was already optimal. The Bellman optimality equation states:
\[ v^*(s) = \max_a E[R_{t+1} + v^*(S_{t+1}) | S_t=s, A_t=a] \]
Thus, if \( v_{\pi}(s) = v^*(s) \), the policy is optimal.

In this context:
\[ q_{\pi^0}(s,a) = E[R_{t+1} + v^*(S_{t+1}) | S_t=s, A_t=a] \]
and if \( v_{\pi}(s) = v^*(s) \), then:
\[ v^0(s) = \max_a q_{\pi}(s,a) = v^*(s) \]

Thus,
??x
Policy improvement leads to a new policy that is at least as good as the original, and if not better, it implies the original policy was already optimal.
x??

--- 
#### Greedy Policy Definition
Background context: The greedy policy \( \pi^0(s) = \arg\max_a q_{\pi}(s,a) \) takes actions that maximize immediate reward plus future expected values according to the current value function.

:p How is the greedy policy defined?
??x
The greedy policy \( \pi^0(s) \) is defined as:
\[ \pi^0(s) = \arg\max_a q_{\pi}(s,a) \]
which selects actions that maximize the expected immediate reward plus future expected values according to the current value function.

Thus,
??x
The greedy policy \( \pi^0(s) \) takes actions that maximize the expression \( E[R_{t+1} + v_{\pi}(S_{t+1}) | S_t=s, A_t=a] \).
x??

--- 
#### Policy Improvement Steps
Background context: Each step in the policy improvement process involves updating the policy to be greedy with respect to the current value function. Ties are handled by distributing probabilities among maximizing actions.

:p What is the general step-by-step process of policy improvement?
??x
The general steps for policy improvement include:
1. Calculate the action values \( q_{\pi}(s,a) \).
2. Construct a new greedy policy where each state selects the action that maximizes \( q_{\pi}(s,a) \):
\[ \pi^0(s) = \arg\max_a q_{\pi}(s,a) \]
3. If the new policy is not strictly better than the old one, then the old policy was optimal.

Thus,
??x
The process of policy improvement involves calculating action values and constructing a new greedy policy that selects actions maximizing immediate reward plus future expected values according to the current value function.
x??

--- 
#### Deterministic vs. Stochastic Policies in Policy Improvement
Background context: The policy improvement theorem applies similarly to both deterministic and stochastic policies, but in the case of stochastic policies, ties need to be handled by distributing probabilities among multiple maximizing actions.

:p How does the policy improvement process differ between deterministic and stochastic policies?
??x
The main difference lies in how ties are handled:
- **Deterministic Policies**: The action with the maximum \( q_{\pi}(s,a) \) is selected.
- **Stochastic Policies**: Ties can occur, so each maximizing action gets a portion of the probability. Any apportioning scheme is allowed as long as submaximal actions get zero probability.

Thus,
??x
In deterministic policies, ties are broken arbitrarily to select one action, while in stochastic policies, multiple maximizing actions share the probability.
x??

--- 
#### Bellman Optimality Equation and Policy Improvement
Background context: The Bellman optimality equation \( v^*(s) = \max_a q_{\pi^*}(s,a) \) is used to find the optimal policy. Policy improvement starts from a given policy and iteratively improves it by making it greedy.

:p How does the Bellman optimality equation relate to policy improvement?
??x
The Bellman optimality equation:
\[ v^*(s) = \max_a q_{\pi^*}(s,a) \]
is used to find the optimal value function. Policy improvement starts with an initial policy \( \pi \), and iteratively improves it by making it greedy with respect to the current value function.

If the new policy is not strictly better, then the original policy was already optimal:
\[ v_{\pi}(s) = v^*(s) \]
and thus:
\[ q_{\pi^0}(s,a) = E[R_{t+1} + v^*(S_{t+1}) | S_t=s, A_t=a] \]

Thus,
??x
The Bellman optimality equation helps in determining the optimal policy by comparing it to the greedy policy derived from the current value function.
x??

--- 
#### Policy Iteration Algorithm Overview
Background context: Policy iteration involves alternating between policy evaluation and policy improvement until an optimal policy is found.

:p What is the general outline of the policy iteration algorithm?
??x
The general outline of the policy iteration algorithm includes:
1. **Policy Evaluation**: Calculate \( v_{\pi}(s) \) for a given policy \( \pi \).
2. **Policy Improvement**: Update the policy to be greedy with respect to the current value function.
3. Repeat steps 1 and 2 until no further improvements are made.

Thus,
??x
The policy iteration algorithm alternates between evaluating the current policy and improving it, ensuring convergence to an optimal policy.
x??

--- 
#### Policy Evaluation and Improvement Steps
Background context: Policy evaluation calculates the value function for a given policy, while policy improvement updates the policy based on these values.

:p How are policy evaluation and policy improvement steps performed?
??x
- **Policy Evaluation**:
  - Start with initial \( v_{\pi}(s) \).
  - Use the Bellman expectation equation to iteratively update \( v_{\pi}(s) \):
    \[ v_{\pi}(s) = E[R_{t+1} + \gamma v_{\pi}(S_{t+1}) | S_t=s, A_t=a] \]

- **Policy Improvement**:
  - Calculate the action values \( q_{\pi}(s,a) \):
    \[ q_{\pi}(s,a) = E[R_{t+1} + \gamma v_{\pi}(S_{t+1}) | S_t=s, A_t=a] \]
  - Update the policy to be greedy:
    \[ \pi^0(s) = \arg\max_a q_{\pi}(s,a) \]

Thus,
??x
Policy evaluation updates the value function iteratively, while policy improvement makes the current policy greedy with respect to these values.
x??

#### Policy Iteration Overview
Background context: Policy iteration is a method for solving finite Markov Decision Processes (MDPs) where we iteratively improve policies and evaluate them to find an optimal policy. Each policy is guaranteed to be a strict improvement over the previous one, unless it is already optimal.

:p What does policy iteration involve in terms of improving and evaluating policies?
??x
Policy iteration involves two main steps: policy evaluation and policy improvement. In each step:
1. **Policy Evaluation**: The value function for the current policy is computed iteratively.
2. **Policy Improvement**: A new, better policy is derived from the evaluated value function.

Here's a simplified pseudocode for policy iteration:

```java
public class PolicyIteration {
    private double[] evaluatePolicy(Policy policy) {
        // Implementation to evaluate the given policy
        return values;
    }

    public Policy improvePolicy(State state, double[] values) {
        // Logic to derive new actions from evaluated value function
        return new Policy();
    }

    public void iterate() {
        Policy currentPolicy = initializePolicy();
        while (!isPolicyStable(currentPolicy)) {
            double[] values = evaluatePolicy(currentPolicy);
            Policy nextPolicy = improvePolicy(values);
            if (nextPolicy.equals(currentPolicy)) break;
            currentPolicy = nextPolicy;
        }
        // Return optimal policy and value function
    }
}
```
x??

---

#### Policy Evaluation Algorithm
Background context: Policy evaluation computes the state-value function for a given policy by iteratively updating the value of each state based on its expected future rewards under that policy.

:p What does the policy evaluation algorithm involve?
??x
The policy evaluation algorithm involves repeatedly estimating the state-value function until it converges. The update rule is:
\[ V(s) \leftarrow \sum_{s', r} p(s', r | s, a)V(s') + \gamma \cdot \text{reward}(r) \]
Where \(a = \pi(s)\), and \(\pi\) is the policy.

Here's an iterative version of the algorithm:

```java
public class PolicyEvaluation {
    private double evaluateState(State state, double[] nextValues) {
        // Sum up expected future rewards based on current actions and transitions
        return 0.0; // Placeholder for actual implementation
    }

    public void loopUntilConvergence(double[] values) {
        boolean stable = false;
        while (!stable) {
            stable = true;
            for (State state : states) {
                double previousValue = values[state];
                values[state] = evaluateState(state, values);
                if (Math.abs(previousValue - values[state]) > threshold) {
                    stable = false;
                }
            }
        }
    }
}
```
x??

---

#### Policy Improvement Criterion
Background context: Policy improvement is the step where a new policy is derived from an evaluated value function such that it improves upon the current policy in terms of expected future rewards.

:p What does the policy improvement criterion involve?
??x
The policy improvement criterion involves comparing the action-value functions for each state under the current and potential new policies. If any action leads to a higher expected reward, we should update the policy at that state.

Pseudocode:
```java
public class PolicyImprovement {
    public void improvePolicy(double[] values) {
        for (State state : states) {
            Action bestAction = null;
            double maxValue = Double.NEGATIVE_INFINITY;
            for (Action action : actions) {
                double value = 0.0;
                // Compute the expected future reward
                if (bestAction == null || value > maxValue) {
                    bestAction = action;
                    maxValue = value;
                }
            }
            policy[state] = bestAction; // Update policy with the new action
        }
    }
}
```
x??

---

#### Example of Jack's Car Rental Problem
Background context: The Jack’s car rental problem is a practical application of MDPs, where we need to decide how many cars to move between two locations overnight. This involves managing inventory and minimizing costs while maximizing profit.

:p What are the key elements in the policy iteration process for the Jack's car rental problem?
??x
Key elements include:
1. **State Representation**: Number of cars at each location.
2. **Action Space**: Net number of cars moved between locations.
3. **Transition Probabilities and Rewards**: Poisson distributions for requests and returns, with costs associated with moving cars.

Here is a simplified pseudocode for the Jack’s car rental problem:

```java
public class CarRentalMDP {
    private void evaluatePolicy(Policy policy) {
        // Evaluate current policy by computing state-value function
    }

    private Policy improvePolicy(double[] values) {
        // Improve policy based on value function
        return new Policy();
    }

    public void iterate() {
        Policy currentPolicy = initializePolicy();
        while (!isPolicyStable(currentPolicy)) {
            evaluatePolicy(currentPolicy);
            Policy nextPolicy = improvePolicy(values);
            if (nextPolicy.equals(currentPolicy)) break;
            currentPolicy = nextPolicy;
        }
    }
}
```
x??

---

#### Policy Iteration Convergence Bug

Background context: The policy iteration algorithm is discussed, and a subtle bug related to infinite loops when policies are equally good is mentioned. This bug can occur but is acceptable for pedagogical purposes.

:p What issue does the policy iteration algorithm face with policies that are equally good?
??x
The issue arises because if the policy evaluation step never updates the value function due to equally good policies, the loop may never terminate.
x??

---

#### Policy Iteration for Action Values

Background context: Policy iteration is extended to consider action values. The goal is to provide a complete algorithm analogous to computing \(v^\pi\).

:p How would you define policy iteration for action values?
??x
Policy iteration for action values involves iterating between policy evaluation and policy improvement, but now focusing on the action values. The core idea is to update the value of each state-action pair until convergence.

The pseudocode could look like this:
```pseudocode
function policyIterationForActionValues() {
    Initialize Q(s, a) arbitrarily for all states s and actions a in SxA

    while true do
        // Policy Evaluation Step
        while not converged(Q) do
            for each state s in S do
                for each action a in A do
                    Q(s, a) = Σ_{s'} P(s', r | s, a) * [r + γ * max_a' Q(s', a')]
        // Policy Improvement Step
        policy_stable = true
        for each state s in S do
            old_action = current_policy[s]
            new_action = argmax_a Q(s, a)
            if new_action != old_action then
                update current_policy[s] to new_action
                policy_stable = false
        end if
    end while

    return policy and value function Q
}
```
The goal is to ensure that the policy converges to an optimal one by iteratively updating the action values.
x??

---

#### -Soft Policies in Policy Iteration

Background context: The discussion shifts towards considering policies that are "-soft, meaning each action's probability in a state is at least "/|A(s)|.

:p How would you modify steps 3, 2, and 1 of the policy iteration algorithm for \(v^\pi\) when dealing with -soft policies?
??x
For step 3 (Policy Evaluation):
- Ensure that during value updates, each action's probability follows the "-soft condition. This means updating only actions within the range [p(s, a) - \epsilon/|A(s)|, p(s, a) + \epsilon/|A(s)|].

For step 2 (Policy Improvement):
- When selecting the greedy action for policy improvement, ensure that the probability distribution respects the "-soft condition.

For step 1 (Initialization):
- Initialize the policy to be -soft from the start or adjust initialization to ensure -softness.

These changes maintain the policy's -soft nature throughout iterations.
x??

---

#### Value Iteration Algorithm

Background context: Value iteration is introduced as a variant of policy iteration, where policy evaluation is truncated after one sweep. The goal is to find an optimal policy without waiting for exact convergence.

:p What is value iteration and how does it differ from regular policy iteration?
??x
Value iteration combines the steps of policy improvement and truncated policy evaluation into a single algorithm. It stops policy evaluation after just one update per state, making it more efficient than full policy evaluation in each step of policy iteration.

The update rule for value iteration is:
\[ v_{k+1}(s) = \max_a \sum_{s', r} p(s', r | s, a) [r + \gamma v_k(s')] \]
for all \(s \in S\).

This can be written as pseudocode:
```pseudocode
function valueIteration() {
    Initialize V arbitrarily for all states s in S

    while true do
        // Value Iteration Step (one sweep of policy evaluation)
        delta = 0
        for each state s in S do
            v_old = V(s)
            V(s) = max_a sum_{s', r} p(s', r | s, a) [r + γ * V(s')]
            if delta < |V(s) - v_old| then
                delta = |V(s) - v_old|
        end for

        // Policy Improvement Step (one sweep of policy improvement)
        policy_stable = true
        for each state s in S do
            old_action = current_policy[s]
            new_action = argmax_a V(s, a)
            if new_action != old_action then
                update current_policy[s] to new_action
                policy_stable = false
            end if
        end for

        if delta < threshold and policy_stable then break
    end while

    return optimal policy and value function V
}
```
This approach converges faster than full policy iteration by not waiting for exact convergence.
x??

---

#### Gambler’s Problem Example

Background context: The gambler's problem is used to illustrate concepts in dynamic programming, such as state transitions, action selection, and the optimal policy.

:p What does the value function and final policy look like for the gambler’s problem with ph=0.4?
??x
For \(ph = 0.4\), the value function and final policy can be visualized using successive sweeps of value iteration. The value function shows increasing values as capital increases, reflecting higher chances of winning.

The final policy suggests optimal betting strategies:
- When capital is low (e.g., $1 or $2), bet a small fraction to avoid losing all.
- As capital approaches the goal ($95 to $98), bet larger amounts to reach the target faster.
- At some point, like when capital is $47 to $50, the policy might suggest betting the entire amount.

This behavior ensures that the gambler maximizes their chances of reaching the goal state with minimal risk.
x??

---

