# Flashcards: 7A003--Performance-modeling-and-design-of-computer-systems_processed (Part 39)

**Starting Chapter:** 24.3 Optimal Server Farm Design

---

#### Task Assignment Policies for Server Farms Among Worst Performers

Background context: The text discusses task assignment policies for server farms with PS servers (Processor Shared servers) that perform poorly. In such scenarios, job size variability is not a significant issue, and some policies like JSQ are nearly insensitive to it.

:p What are the characteristics of the worst-performing server farms in terms of task assignment policies?
??x
The worst-performing server farms for PS servers exhibit poor performance despite having minimal job size variability. Policies such as JSQ show little sensitivity to changes in job sizes, suggesting that other factors contribute significantly to their suboptimal performance.
x??

---

#### Optimal Server Farm Design

Background context: The text introduces the theoretical question of designing an optimal server farm where both task assignment and scheduling policies can be chosen. This is more theoretical since typically, the operating system dictates the scheduling policy.

:p What are the assumptions made for designing an optimal server farm?
??x
The design assumes full preemption, known job sizes when they arrive, and the flexibility to use a central queue at the router if needed. The job size distribution has high variability, with mean job size \( E[S] \), and follows a Poisson arrival process with average rate \( \lambda \).
x??

---

#### Competitive Ratio in Worst-Case Analysis

Background context: In worst-case analysis, policies are evaluated based on their performance relative to the optimal policy across all possible arrival sequences. A higher competitive ratio indicates worse performance under the worst-case scenario.

:p What is a competitive ratio and how is it defined?
??x
A competitive ratio measures how well a given policy \( P \) performs compared to an optimal policy \( OPT \). It is defined as:
\[ \text{Competitive ratio of } P = \max_A r_P(A) \]
where \( r_P(A) = \frac{E[T(A)]_P}{E[T(A)]_{OPT}} \), and \( E[T(A)]_P \) is the expected response time of policy \( P \) on arrival sequence \( A \).

The competitive ratio indicates how poorly a policy performs under the worst possible scenario. A high competitive ratio means that the policy's performance degrades significantly in the worst-case.
x??

---

#### Stochastic Analysis vs Worst-Case Analysis

Background context: The text contrasts stochastic analysis, which assumes a Poisson arrival process and i.i.d. job sizes, with worst-case analysis, where an adversary can generate any sequence of arrivals.

:p How does worst-case analysis differ from stochastic analysis in the context of server farm design?
??x
In stochastic analysis, policies are evaluated under a specific probabilistic model (Poisson arrivals with i.i.d. job sizes), whereas in worst-case analysis, policies are tested against all possible arrival sequences generated by an adversary. Worst-case analysis provides a different ranking of policies compared to stochastic analysis because it considers the performance across every conceivable scenario.
x??

---

#### Theoretical vs Practical Considerations

Background context: The text highlights that while theoretical design allows flexibility in task assignment and scheduling, practical implementations are often constrained by existing operating systems.

:p Why is there almost no stochastic analysis in optimal server farm design?
??x
There is a lack of stochastic analysis in optimal server farm design because the field predominantly uses worst-case analysis with competitive ratios. This approach focuses on the robustness of policies under any potential arrival sequence, rather than probabilistic performance measures.
x??

---

#### Heterogeneous Servers

Background context: The text mentions that task assignment policies for heterogeneous servers are an open problem in this area.

:p Why is there a mention about heterogeneous servers being an interesting but unexplored area?
??x
Heterogeneous servers refer to systems with different types of processors or varying capacities. Task assignment policies need to account for these differences, and the text suggests that this aspect remains largely unexplored, making it an interesting open problem.
x??

---

#### SRPT Policy for Single Queue
Background context explaining that the Shortest Remaining Processing Time (SRPT) policy is optimal with respect to mean response time when there's a single queue and fully preemptible jobs, regardless of the arrival sequence.

:p What is the best scheduling policy on every arrival sequence for a single queue with fully preemptible jobs?
??x
The SRPT policy, which always runs the job with the shortest remaining processing time preemptively, is optimal in terms of mean response time. This result holds under any arrival sequence of job sizes and arrival times.
??x

---

#### Central-Queue-SRPT Policy for Server Farms
Background context explaining that the Central-Queue-SRPT policy extends the SRPT idea to server farms where k servers handle jobs with the shortest remaining processing time, maintaining a single queue.

:p What is the Central-Queue-SRPT policy in the context of server farm design?
??x
The Central-Queue-SRPT policy involves having k servers always serving those k jobs with the currently shortest remaining processing times. If a job arrives with shorter remaining time than the current job on any server, that arrival is immediately put into service at the appropriate server, and the prior job being served is returned to the queue.
??x

---

#### Limitations of Central-Queue-SRPT Policy
Background context explaining the limitations of the Central-Queue-SRPT policy by providing an example where it does not produce minimal mean response time.

:p Is the Central-Queue-SRPT policy optimal in the worst-case sense? Provide an example to illustrate.
??x
The Central-Queue-SRPT policy is not always optimal in the worst-case sense. An example with a 2-server system shows that at certain times, the optimal algorithm can pack jobs differently to achieve better performance than SRPT.

For instance:
- At time 0: 2 jobs of size 29 arrive, and 1 job of size 210.
- At time 210: 2 jobs of size 28 arrive, and 1 job of size 29.
- At time 210 + 29: 2 jobs of size 27 arrive, and 1 job of size 28.
- And so forth.

The optimal algorithm would pack the jobs differently to ensure both servers are fully utilized at all times, resulting in better mean response time than SRPT.
??x

---

#### Example of Optimal Algorithm
Background context explaining how an optimal algorithm can achieve better performance by packing jobs efficiently.

:p How does the optimal algorithm handle job arrivals and server utilization?
??x
The optimal algorithm packs jobs in such a way that both servers are fully utilized at all times. For example, consider a 2-server system:
- At time 0: Assign 2 jobs of size 29 to Server A and 1 job of size 210 to Server B.
- At time 210: Reassign the remaining work from Server A (jobs of size 28) to Server A and assign new jobs of size 29 to Server B, ensuring both servers are continuously utilized.

This packing strategy ensures minimal mean response time by keeping servers busy without idle periods.
??x

---

Each flashcard should cover a specific aspect of the provided text, helping with understanding and recall.

#### Central-Queue-SRPT Algorithm Performance
Background context: The Central-Queue-SRPT algorithm tries to run jobs with the smallest remaining time first. However, it often results in inefficient server utilization and poor job completion times due to its preemptive nature and lack of adaptability to varying job sizes.
:p How does the Central-Queue-SRPT algorithm perform when faced with a specific arrival sequence?
??x
The Central-Queue-SRPT algorithm tries to run smaller jobs first but often leaves one server idle while another runs larger jobs, leading to inefficiencies. This can result in preemption and incomplete job execution before new batches arrive.
---
#### Competitive Ratio of Central-Queue-SRPT
Background context: The competitive ratio of the Central-Queue-SRPT algorithm is proportional to \(\log\left(\frac{b}{s}\right)\), where \(b\) is the largest job size and \(s\) is the smallest job size. No online algorithm can improve on this by more than a constant factor.
:p What does the competitive ratio of Central-Queue-SRPT indicate?
??x
The competitive ratio indicates how well the Central-Queue-SRPT performs in comparison to an optimal offline algorithm. It shows that while it might not be optimal in worst-case scenarios, it is still among the best possible algorithms from a theoretical perspective.
---
#### Stochastic Analysis of Central-Queue-SRPT
Background context: No closed-form analysis exists for the stochastic behavior of Central-Queue-SRPT under Poisson arrivals and exponentially distributed job sizes. The closest analysis is on M/PH/k queues with preemptive priority classes, but results are numerical.
:p Why is there no known stochastic analysis for Central-Queue-SRPT?
??x
There is no known closed-form analysis because the problem's complexity does not allow for a straightforward analytical solution. Numerical methods can provide approximations, but they do not offer exact theoretical insights into its performance under varying job sizes and arrival patterns.
---
#### Immediate Dispatch and SRPT Scheduling
Background context: In scenarios where jobs must be immediately dispatched to hosts (e.g., web servers), running SRPT scheduling at each host is optimal. This ensures that short jobs are processed quickly, minimizing response times.
:p What task assignment policy works best for immediate dispatch in server farms?
??x
The IMD (Immediate Dispatch with Minimal Delay) algorithm is effective. It divides jobs into size classes and assigns incoming jobs to the server with the smallest number of jobs in that class, ensuring balanced workload distribution among servers.
---
#### IMD Algorithm Logic
Background context: The IMD algorithm aims to spread short jobs across all SRPT servers to maximize their processing efficiency by minimizing idle time on individual servers.
:p How does the IMD algorithm assign tasks?
??x
The IMD algorithm assigns each incoming job to the server with the smallest number of jobs in its size class. This ensures that no server is overloaded and allows for efficient SRPT scheduling.
```java
// Pseudocode for IMD Algorithm
public void assignTask(Job job) {
    Job[] servers = getServers();
    int bestServerIndex = -1;
    for (int i = 0; i < servers.length; i++) {
        if (bestServerIndex == -1 || servers[bestServerIndex].getJobCount(job.size()) > servers[i].getJobCount(job.size())) {
            bestServerIndex = i;
        }
    }
    servers[bestServerIndex].addJob(job);
}
```
x??
The pseudocode demonstrates how the IMD algorithm selects a server based on the job size class to balance the load and ensure efficient processing.

#### SITA and TAGS Policies for Server Farms
Background context: The size-based task assignment (SITA) policy was introduced by Harchol-Balter, Crovella, and Murta in [83]. It is used to reduce job size variability. A variant of SITA called TAGS was later proposed in [82] that does not require knowledge of the job size but still achieves similar response times.
:p What are the key differences between SITA and TAGS policies?
??x
The SITA policy requires knowing the size of the job, whereas TAGS does not. However, both aim to reduce variability in job sizes, leading to better performance under high variability conditions.
x??

---

#### Performance Comparison Between SITA and LWL
Background context: Many papers compared SITA to the Longest Work Left (LWL) policy, suggesting that as job size variability increases, SITA becomes far superior to LWL. However, recent work shows that this may not always be true under certain conditions.
:p What do the studies show about the performance of SITA and LWL?
??x
Studies initially suggested that SITA outperforms LWL significantly under high job size variability but later works like [90] found that in specific scenarios with certain job size distributions and loads, LWL can sometimes outperform SITA. This highlights the importance of understanding the specific characteristics of the workload.
x??

---

#### Hybrid Policy for Server Farms
Background context: The Hybrid policy combines elements of SITA and LWL to serve small jobs using a top server and any job using a bottom server in an M/G/2 system. It aims to leverage the strengths of both policies.
:p How does the Hybrid policy work?
??x
The Hybrid policy works by having one server (top) only handle small jobs, while another server (bottom) can serve all sizes of jobs. This combination leverages the benefits of SITA for small jobs and LWL for larger ones, optimizing resource usage.
x??

---

#### Mean Response Time Analysis for M/G/k
Background context: The mean response time for M/G/k systems remains an open problem in queueing theory. Many approximations exist but often rely on limited moments of job size distributions.
:p Why is the mean response time analysis challenging?
??x
The challenge lies in the fact that most existing closed-form approximations for mean waiting times in M/G/k systems use only the first two moments (mean and variance) of the job size distribution, which may not be sufficient. Advanced papers like [156] provide upper bounds on delay without relying solely on these limited moments.
x??

---

#### JSQ Policy Analysis
Background context: The Join-the-Shortest Queue (JSQ) policy is analyzed for server farms with FCFS scheduling at servers. While optimal for certain workloads, it performs poorly under highly variable job sizes and has received limited attention in the literature.
:p What are some of the challenges faced when analyzing JSQ policies?
??x
Challenges include limited analysis beyond two servers, reliance on exponential job size distributions, and approximations that may not be computationally efficient or generalize well to higher values of k. Additionally, state space truncation is often required, leading to inaccuracies.
x??

---

#### Approximation Methods for JSQ/FCFS Systems
Background context: Some studies approximate the number of busy servers using statistical methods like binomial distributions and estimate total jobs in a system with JSQ/FCFS policies. These approximations are surprisingly accurate but may not be exact or computationally efficient.
:p What is one method used to approximate the JSQ policy's performance?
??x
One method involves estimating the steady-state probability of an M/M/k queue (centralized model) and assuming that jobs are equally divided among each server, within a small margin. This approach provides surprisingly accurate results with errors typically in the 2% to 8% range.
x??

---

#### Server Farm with Size-Interval-Task-Assignment (SITA)
Background context: In a server farm, jobs are assigned to servers based on their size. Here, small jobs (<10 units) go to Host 1 and large jobs (≥10 units) go to Host 2. Jobs arrive according to a Poisson process with rate λ, and job sizes follow a power-law distribution.
:p What is the mean response time E[T] for this system?
??x
To derive the mean response time E[T], we need to consider the service times at each server and their respective probabilities of being small or large. The response time can be calculated as follows:
- For small jobs (S < 10), the service time is T1 = S / μ1, where μ1 is the service rate for Host 1.
- For large jobs (S ≥ 10), the service time is T2 = S / μ2, where μ2 is the service rate for Host 2.

The mean response time E[T] can be computed using the law of total expectation:
\[ E[T] = \lambda \left( \int_{0}^{10} \frac{t}{\mu_1} f_S(t) dt + \int_{10}^{\infty} \frac{t}{\mu_2} f_S(t) dt \right) \]
where \( f_S(t) \) is the probability density function of job sizes.

For a different power-law distribution, we can adjust the exponents accordingly. For instance, with P{S>x}=x−1.5, the mean response time would be:
\[ E[T] = \lambda \left( \int_{0}^{10} \frac{t}{\mu_1} t^{-1.5} dt + \int_{10}^{\infty} \frac{t}{\mu_2} t^{-1.5} dt \right) \]
??x
---

#### PS Server Farm with SITA Task Assignment
Background context: A server farm has two identical PS ( Processor Sharing ) hosts and uses a SITA task assignment policy to distribute jobs based on their size intervals.
:p Prove that the SITA cutoff which minimizes mean response time is that which balances load between the two hosts.
??x
To minimize the mean response time, we need to balance the load on both servers. The key idea is to find a job size threshold such that the fraction of jobs sent to each host is equal.

Let \( x \) be the cutoff value. If small jobs (size < \( x \)) are routed to Host 1 and large jobs (size ≥ \( x \)) are routed to Host 2, we need to balance their expected arrival rates:
\[ \text{Fraction of small jobs} = P(S < x) \]
\[ \text{Fraction of large jobs} = P(S \geq x) \]

For a balanced load, these fractions should be equal. Therefore, the optimal \( x \) is where:
\[ P(S < x) = 0.5 \]

This ensures that half the jobs are sent to each host, balancing the load and minimizing the mean response time.
??x
---

#### Hybrid Server Farm with SITA Task Assignment
Background context: A server farm has two identical hosts, one handling small jobs (S < 10) using FCFS scheduling and the other handling large jobs (S ≥ 10) using PS scheduling. The job size distribution is given, and the load balancing condition is that ρ = λE[S] / 2.
:p Derive an expression for E[T], the mean response time experienced by an arriving job, as a function of ρ, λ, fS(t), and FS(t).
??x
The mean response time \( E[T] \) can be derived using the properties of FCFS and PS queues.

For small jobs (S < 10):
- The arrival rate is \( \lambda P(S < 10) \)
- Service time is \( T_1 = S / μ_1 \), where \( μ_1 \) is the service rate for Host 1

The mean response time for FCFS:
\[ E[T_{\text{small}}] = \frac{\lambda P(S < 10)}{\mu_1} + \frac{\int_{0}^{10} \frac{t}{\mu_1^2} f_S(t) dt}{\mu_1 - \lambda P(S < 10)} \]

For large jobs (S ≥ 10):
- The arrival rate is \( \lambda P(S \geq 10) \)
- Service time is \( T_2 = S / μ_2 \), where \( μ_2 \) is the service rate for Host 2

The mean response time for PS:
\[ E[T_{\text{large}}] = \frac{\lambda P(S \geq 10)}{\mu_2} + \frac{\int_{10}^{\infty} \frac{t}{\mu_2^2} f_S(t) dt}{\mu_2 - \lambda P(S \geq 10)} \]

Combining these, the total mean response time is:
\[ E[T] = E[T_{\text{small}}] + E[T_{\text{large}}] \]
??x
---

#### Equivalence of LWL and M/G/k
Background context: The Local Wait List (LWL) and M/G/k systems are compared when fed the same arrival sequence under identical conditions.
:p Prove by induction that each job is served by the same server in both systems.
??x
To prove this, we use mathematical induction.

**Base Case:** For one job, it is trivially true since only one server will be used in both LWL and M/G/k.

**Inductive Step:**
Assume that for \( n \) jobs, each job is served by the same server in both systems. We need to show that adding a new job (job \( n+1 \)) maintains this property.

- In LWL, when a new job arrives, it will be added to the wait list of the same server as the previous job if possible.
- In M/G/k, each job follows its service sequence independently. However, under identical conditions and tied resolution, both systems treat jobs identically.

By induction, this holds for all \( n \) jobs, ensuring that each job is served by the same server in both LWL and M/G/k.
??x
---

#### One Fast Machine vs Two Slow Ones (Heavy-Tailed Job Sizes)
Background context: Comparing the performance of one fast machine versus two slow machines with heavy-tailed job sizes. The processing time on a fast machine is halved for large jobs, while small jobs have linear service times.
:p Compute E[TQ] as a function of λ in both cases and determine which case results in lower mean waiting time.
??x
For the single "fast" machine (M/G/1):
- Service rate \( \mu = 2s \) for large jobs, \( s \) for small jobs
- Arrival rate \( \lambda \)

Mean waiting time:
\[ E[T_{Q,1}] = \frac{\rho^2}{2(1-\rho)} + \frac{1}{\mu_1} \]
where \( \rho = \lambda / (3s) \).

For two slow machines (each with rate 0.5s):
- Each machine handles half the jobs
- Mean waiting time for each machine:
\[ E[T_{Q,2}] = \frac{\left(\frac{\lambda}{2}\right)^2}{1 - \frac{\lambda}{2}} + \frac{1}{0.5s} \]

Combining both machines' waiting times:
\[ E[T_{Q,2}] = 2 \times \left( \frac{\left(\frac{\lambda}{2}\right)^2}{1 - \frac{\lambda}{2}} + \frac{1}{0.5s} \right) \]

By comparing \( E[T_{Q,1}] \) and \( E[T_{Q,2}] \), we find that the single fast machine results in lower mean waiting time due to better utilization of resources.
??x
---

#### Load Balancing or Not (Bounded Pareto Job Size Distribution)
Background context: Evaluating whether load balancing between two identical FCFS hosts is always beneficial or if unbalancing can improve performance.
:p Determine the cutoff x under SITA-E and its impact on E[TQ].
??x
Under SITA-E, the cutoff \( x \) balances the load at the two hosts. Given:
\[ \rho = 0.5 \]
and a Bounded Pareto distribution with mean 3,000.

The cutoff \( x \) can be determined by solving for the median of the distribution to ensure equal expected loads on both servers. Once found, calculate \( E[TQ] \).

To find the optimal \( x \):
\[ P(S < x) = 0.5 \]

Using this value, compute the mean response time for each server and sum them up.

Unbalancing can be done by slightly adjusting \( x \) to see if it improves performance.
??x
--- 

---
These flashcards cover key concepts from the provided text, focusing on server farm task assignment policies, load balancing, and system comparison. Each card is designed to test understanding of specific aspects of the described scenarios. 
??x
---

