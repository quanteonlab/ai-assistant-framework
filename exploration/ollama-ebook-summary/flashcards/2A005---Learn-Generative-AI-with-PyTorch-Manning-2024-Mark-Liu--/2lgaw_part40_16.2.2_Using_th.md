# Flashcards: 2A005---Learn-Generative-AI-with-PyTorch-Manning-2024-Mark-Liu--_processed (Part 40)

**Starting Chapter:** 16.2.2 Using the OpenAI API in LangChain

---

#### Text-to-Speech (TTS) Integration with OpenAI API
Background context: Text-to-speech (TTS) technology converts written text into spoken words. This process is typically achieved through multimodal Transformers, where the input is text and the output is audio format. In this section, we will integrate TTS capabilities using the OpenAI API to generate an audio file from a given text.

:p How can you use the OpenAI API to convert a short text into speech?
??x
To use the OpenAI API for converting text into speech, follow these steps:

1. Import the necessary client.
2. Use the `client.audio.speech.create` method with the appropriate model and voice options.
3. Save the generated audio file.

Here is an example of how to do this in Python using the OpenAI API:

```python
import openai

# Initialize the OpenAI client
client = openai.OpenAI(api_key='YOUR_API_KEY')

response = client.audio.speech.create(
    model="tts-1-hd",
    voice="shimmer",
    input='''This is an audio file generated by      OpenAI's text to speech AI model.'''
)

# Save the response to a file
response.stream_to_file("files/speech.mp3")
```

The `model` parameter specifies the TTS model, and the `voice` parameter selects the voice option. In this example, we used "tts-1-hd" as the model and "shimmer" as the voice.

x??

---

#### Introduction to LangChain Library
Background context: LangChain is a Python library designed to facilitate the use of large language models (LLMs) in various applications. It provides tools and abstractions for building, deploying, and managing applications powered by LLMs like GPT-3, GPT-4, and others.

:p What is LangChain used for?
??x
LangChain is primarily used to build a "know-it-all" agent that can automatically retrieve real-time information from sources like Wolfram Alpha and Wikipedia. It simplifies the interaction with different LLMs by abstracting away the complexities of using multiple APIs, allowing developers to focus on building application logic.

x??

---

#### Need for LangChain Library
Background context: The objective is to build a zero-shot know-it-all agent that can generate content, retrieve real-time information, and answer factual questions without explicit instructions. This requires an agent that can intelligently decide which tools to use based on the task at hand.

:p Why is LangChain suitable for building such an agent?
??x
LangChain is suitable because it provides a modular architecture that allows easy integration of different components like LLMs, APIs (e.g., Wolfram Alpha and Wikipedia), and other tools. This enables the agent to leverage the strengths of various models and applications to answer questions effectively.

x??

---

#### Example with LangChain for Factual Question
Background context: Even advanced LLMs like GPT-4 struggle to provide real-time information or predictions about future events. For instance, querying who won the Best Actor Award in the 2024 Academy Awards would yield an inability to provide accurate data due to its nature.

:p Why did GPT-4 fail to answer the query about the 2024 Academy Awards?
??x
GPT-4 failed because it cannot access real-time information or make predictions about future events. Its training data does not include current events beyond a certain date, so it lacks the ability to provide accurate responses for recent or upcoming events.

x??

---

#### Combining LLMs with Wolfram Alpha and Wikipedia APIs
Background context: LangChain allows combining an LLM with APIs like Wolfram Alpha and Wikipedia to create a more capable agent. This integration enables the agent to use real-time information from Wolfram Alpha and factual data from Wikipedia, enhancing its ability to answer diverse queries.

:p How can you integrate an LLM with the Wolfram Alpha and Wikipedia APIs using LangChain?
??x
You can integrate an LLM with the Wolfram Alpha and Wikipedia APIs by creating a system where the agent first understands the query and then decides which tool (Wolfram Alpha or Wikipedia) to use based on the nature of the question. This involves setting up the LLM to interact with both APIs seamlessly.

For example, if you ask about recent facts, LangChain might route the query to Wikipedia; for scientific computations, it could go to Wolfram Alpha.

x??

---

#### Using LangChain to Invoke OpenAI API
Background context: The langchain-openai library allows you to use OpenAI GPTs with minimal prompt engineering. You only need to explain what you want the LLM to do in plain English.

:p How can we use the `langchain_openai` library to correct grammar errors in text?
??x
To correct grammar errors, you can follow these steps:
1. Import the necessary class from the langchain_openai library.
2. Initialize an instance of the OpenAI class with your API key.
3. Provide a clear and straightforward prompt explaining what needs to be done.

Example code:

```python
from langchain_openai import OpenAI

# Initialize the OpenAI model with your API key
llm = OpenAI(openai_api_key=openai_api_key)

# Define the prompt that explains the task in plain English
prompt = """
Correct the grammar errors in the text: 
i had went to stor buy phone. No good. returned get new phone.
"""

# Invoke the LLM with the provided prompt
res = llm.invoke(prompt)
print(res)
```

The output will be:

```
I went to the store to buy a phone, but it was no good. I returned it and got a new phone.
```

x??

---

#### Example of LangChain for OpenAI API
Background context: This example demonstrates how to use the langchain-openai library to correct grammar errors without extensive prompt engineering.

:p Can you provide an example where we ask the agent to name the capital city of Kentucky?
??x
Sure, here's an example:

```python
from langchain_openai import OpenAI

# Initialize the OpenAI model with your API key
llm = OpenAI(openai_api_key=openai_api_key)

# Define a prompt that asks for the capital city of Kentucky in plain English
prompt = """
What is the capital city of the state of Kentucky?
"""

# Invoke the LLM to get the response
res = llm.invoke(prompt)
print(res)
```

The output will be:

```
The capital city of Kentucky is Frankfort.
```

x??

---

#### Zero-shot Prompting
Background context: In zero-shot prompting, the model is given a task or question without any examples. The prompt typically includes a clear description of what is expected.

:p What does zero-shot prompting involve?
??x
In zero-shot prompting, the model receives a task or question directly and must generate a response based solely on its pre-existing knowledge and understanding. No prior examples are provided to guide the model.

Example:

```python
from langchain_openai import OpenAI

# Initialize the OpenAI model with your API key
llm = OpenAI(openai_api_key=openai_api_key)

# Define a prompt that describes the task without providing any examples
prompt = """
What is the capital city of the state of Kentucky?
"""

# Invoke the LLM to get the response
res = llm.invoke(prompt)
print(res)
```

x??

---

#### Few-shot Prompting
Background context: In few-shot prompting, multiple examples are provided to help the model understand the task better. This technique can improve accuracy by showing patterns or rules.

:p How does few-shot prompting work?
??x
Few-shot prompting involves providing several examples in the prompt to illustrate how to handle a specific task. The LLM uses these examples to infer the pattern and generate accurate responses.

Example:

```python
from langchain_openai import OpenAI

# Initialize the OpenAI model with your API key
llm = OpenAI(openai_api_key=openai_api_key)

# Provide multiple examples in the prompt
prompt = """
The movie is awesome. // Positive
It is so bad. // Negative
Wow, the movie was incredible. // Positive
How horrible the movie is. // Negative

How would you classify this sentence: "How horrible the movie is."
"""

# Invoke the LLM to get the response
res = llm.invoke(prompt)
print(res)
```

The output will be:

```
Negative
```

x??

---

#### One-shot Prompting
Background context: In one-shot prompting, a single example is provided to illustrate the task. The model learns from this single instance and generates responses accordingly.

:p What does one-shot prompting involve?
??x
In one-shot prompting, you provide a single example in the prompt to guide the LLM on how to handle the task. This example helps the model understand the structure or rules needed for generating an accurate response.

Example:

```python
from langchain_openai import OpenAI

# Initialize the OpenAI model with your API key
llm = OpenAI(openai_api_key=openai_api_key)

# Provide a single example in the prompt
prompt = """
Car -> Driver
Plane -> 
"""

# Invoke the LLM to get the response
res = llm.invoke(prompt)
print(res)
```

The output will be:

```
Pilot
```

x??

---

#### One-Shot Prompting Example
Background context explaining one-shot prompting. This involves providing a specific example to guide the LLM without using any previous examples.

:p What is an example of one-shot prompting?
??x
An example would be asking, "What is to a garden as a chef is to a kitchen?" and expecting the LLM to provide the analogy, such as "a gardener" or "botanist." This method relies on clear instructions without providing sample inputs.
x??

---

#### Zero-Shot Prompting Example
Background context explaining zero-shot prompting. This involves asking for an answer directly without any examples.

:p What is an example of a zero-shot prompt to determine the tone of a sentence?
??x
Example: "Is the tone in the sentence 'Today is a great day for me' positive, negative, or neutral?" The expected response from the LLM would be "Positive."
x??

---

#### Creating a Zero-Shot Know-It-All Agent with LangChain
Background context explaining how to create a zero-shot know-it-all agent that can handle various tasks using different APIs and tools.

:p What are the steps to create a zero-shot know-it-all agent in LangChain?
??x
1. Create an agent with only the Wolfram Alpha API for real-time information.
2. Add Wikipedia as a backup tool for factual questions.
3. Incorporate OpenAI GPT tools such as text summarizer, joke teller, and sentiment classifier.
4. Include image and code generation functionalities.

Example steps:
- Obtain Wolfram Alpha AppID: `https://account.wolfram.com/login/create/`
- Use the following code to set up the environment with OpenAI API key:
```python
os.environ['OPENAI_API_KEY'] = openai_api_key
from langchain.agents import load_tools, create_react_agent
```

- Load tools and create an agent:
```python
tool_names = ['wolfram-alpha']
tools = load_tools(tool_names)
agent = create_react_agent(llm=llm, tools=tools)
```
x??

---

#### Applying for a Wolfram Alpha API Key
Background context explaining the process to get access to the Wolfram Alpha API.

:p How do you obtain an API key for Wolfram Alpha?
??x
1. Create an account at `https://account.wolfram.com/login/create/`.
2. Go to `https://products.wolframalpha.com/api/` and click "Get API Access" in the bottom left corner.
3. Fill out the fields: Name, Description, select Simple API from the dropdown menu.
4. Click Submit to receive your AppID.

Example of setting up the environment:
```python
os.environ['WOLFRAM_ALPHA_APPID'] = 'your Wolfram Alpha AppID'
from langchain_community.utilities.wolfram_alpha import WolframAlphaAPIWrapper
wolfram = WolframAlphaAPIWrapper()
res = wolfram.run('how much is 23*55+123?')
print(res)
```
x??

---

#### Using the Wikipedia API in LangChain
Background context explaining how to use the Wikipedia API for factual queries.

:p How can you use the Wikipedia API in LangChain?
??x
Example code:
```python
from langchain.tools import WikipediaQueryRun
from langchain_community.utilities import WikipediaAPIWrapper

wikipedia = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())
res = wikipedia.run('University of Kentucky')
print(res)
```

The output would provide a summary of the University of Kentucky.
x??

---

#### Creating an Agent with ReAct in LangChain
Background context explaining how to create a reactive agent using tools like Wolfram Alpha and Wikipedia.

:p How do you create an agent in LangChain that uses only the Wolfram Alpha API?
??x
Example code:
```python
os.environ['OPENAI_API_KEY'] = openai_api_key
from langchain.agents import load_tools, create_react_agent

tool_names = ['wolfram-alpha']
tools = load_tools(tool_names)
agent = create_react_agent(llm=llm, tools=tools)

agent_executor = AgentExecutor(agent=agent, tools=tools,
                               handle_parsing_errors=True, verbose=True)

res = agent_executor.invoke({"input": "What is the temperature in Lexington, Kentucky now?"})
print(res["output"])
```

The output would show a chain of thoughts followed by the current temperature.
x??

---

#### Adding More Tools to an Agent
Background context explaining how to add more tools like Wikipedia and OpenAI GPT functionalities.

:p How can you add more tools such as Wikipedia and OpenAI GPT to the LangChain agent?
??x
Example code:
```python
tool_names += ['wikipedia']
tools = load_tools(tool_names)
agent = create_react_agent(llm=llm, tools=tools)

res = agent_executor.invoke({"input": "Who won the Best Actor Award in 2024 Academy Awards?"})
print(res["output"])
```

The output would use Wikipedia to find the winner of the Best Actor Award.
x??

---

