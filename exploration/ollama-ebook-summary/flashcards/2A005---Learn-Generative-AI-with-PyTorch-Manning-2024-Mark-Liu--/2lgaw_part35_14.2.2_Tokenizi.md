# Flashcards: 2A005---Learn-Generative-AI-with-PyTorch-Manning-2024-Mark-Liu--_processed (Part 35)

**Starting Chapter:** 14.2.2 Tokenizing MIDI files

---

#### Tokenizing Music Pieces

Background context: Before training a music Transformer, we need to convert raw MIDI files into sequences of tokens that can be processed by the model. This involves tokenization and indexing.

:p What is the first step in converting raw MIDI data into a numerical form for processing?
??x
The first step is to convert MIDI files into sequences of musical notes, which are then further tokenized into 1 of 388 unique events/tokens. After tokenization, each event/token is assigned a unique index (an integer), transforming the music pieces into sequences of integers.
```java
// Pseudocode for tokenizing and indexing
public class MusicTokenization {
    private static final int TOKEN_COUNT = 388; // Number of unique tokens

    public List<Integer> tokenizeAndIndex(List<MidiNote> midiNotes) {
        Map<MidiNote, Integer> tokenMap = new HashMap<>();
        int index = 0;
        
        for (MidiNote note : midiNotes) {
            if (!tokenMap.containsKey(note)) {
                tokenMap.put(note, index++);
            }
        }

        List<Integer> indexedTokens = new ArrayList<>();
        for (MidiNote note : midiNotes) {
            indexedTokens.add(tokenMap.get(note));
        }

        return indexedTokens;
    }
}
```
x??

---

#### Creating Input Sequences

Background context: To prepare the model for training, we create input sequences of a fixed length. These sequences are used to generate ground truth outputs that allow the model to predict the next token in the sequence.

:p How do we create input and output pairs for training the music Transformer?
??x
We create input-output pairs by taking a sequence of integers representing musical events and shifting it one index to the right, using the shifted version as the output. This forces the model to predict the next music token based on the current token and all previous tokens in the sequence.
```java
// Pseudocode for creating input sequences
public class SequencePreparation {
    public Pair<List<Integer>, List<Integer>> createInputOutputPairs(List<Integer> sequence, int maxLength) {
        if (sequence.size() <= maxLength) {
            return new Pair<>(sequence, sequence);
        }
        
        List<Integer> inputs = new ArrayList<>();
        for (int i = 0; i < sequence.size() - 1; i++) {
            inputs.add(sequence.get(i));
        }

        List<Integer> outputs = new ArrayList<>();
        for (int i = 1; i < sequence.size(); i++) {
            outputs.add(sequence.get(i));
        }

        return new Pair<>(inputs, outputs);
    }
}
```
x??

---

#### Training the Music Transformer

Background context: The training process involves feeding the model with input sequences and allowing it to predict the next token. This is done by sliding a window of input sequence one index to the right.

:p What is the role of cross-entropy loss in training the music Transformer?
??x
Cross-entropy loss measures the dissimilarity between the predicted probability distribution over possible event tokens and the actual target distribution. In the context of the music Transformer, it helps to optimize the model's parameters so that its predictions are as close as possible to the ground truth events.

The formula for cross-entropy loss is:

\[ L = -\sum_{i} y_i \log(p_i) \]

Where \( y_i \) is the target probability distribution and \( p_i \) is the predicted probability distribution over all event tokens.

```java
// Pseudocode for calculating cross-entropy loss
public class LossCalculation {
    public double calculateCrossEntropyLoss(List<Double> targets, List<Double> predictions) {
        double loss = 0.0;
        
        for (int i = 0; i < targets.size(); i++) {
            if (targets.get(i) != 0) { // Avoid log(0)
                loss -= Math.log(predictions.get(i)) * targets.get(i);
            }
        }

        return loss / targets.size();
    }
}
```
x??

---

#### Training Data Generation

Background context: The training data is generated by dividing the sequence of integers into smaller sequences of equal length. This allows the model to capture long-range dependencies among musical events.

:p How do we prepare the training data for the music Transformer?
??x
We prepare the training data by taking a sequence of tokens and splitting it into sequences of fixed length (2,048 indexes). For each split sequence, we shift one index to the right to create input-output pairs. The input is the sequence up until the last token, and the output is the last token.

```java
// Pseudocode for preparing training data
public class TrainingDataPreparation {
    public List<Pair<List<Integer>, List<Integer>>> prepareTrainingData(List<Integer> tokens) {
        List<Pair<List<Integer>, List<Integer>>> data = new ArrayList<>();
        int sequenceLength = 2048;

        if (tokens.size() <= sequenceLength) {
            return Collections.singletonList(new Pair<>(tokens.subList(0, tokens.size()), tokens));
        }

        for (int i = 0; i < tokens.size() - sequenceLength; i++) {
            List<Integer> inputSequence = tokens.subList(i, i + sequenceLength);
            List<Integer> outputSequence = new ArrayList<>();
            
            // Shift the output one index to the right
            for (int j = 1; j < inputSequence.size(); j++) {
                outputSequence.add(inputSequence.get(j));
            }

            data.add(new Pair<>(inputSequence, outputSequence));
        }

        return data;
    }
}
```
x??

---

#### Forward Pass in Training

Background context: During the training process, a forward pass involves feeding an input sequence through the music Transformer to make predictions. The model's parameters are updated based on the difference between predicted and actual outputs.

:p What happens during the forward pass of the music Transformer?
??x
During the forward pass, the input sequence is fed into the music Transformer, which processes it layer by layer. The transformer makes a prediction for each token in the sequence, using the current parameters in the model. This prediction is then compared with the actual output (ground truth) to compute the loss.

```java
// Pseudocode for forward pass
public class MusicTransformer {
    public void forwardPass(List<Integer> inputSequence) {
        // Process through layers of transformer
        List<Double> predictions = new ArrayList<>();
        
        for (int token : inputSequence) {
            // Predict next token based on current parameters
            double prediction = predictNextToken(token);
            predictions.add(prediction);
        }
    }

    private double predictNextToken(int currentToken) {
        // Logic to predict the next token using current model parameters
        return 0.5; // Placeholder for actual implementation
    }
}
```
x??

---

#### Cross-Entropy Loss Calculation
Cross-entropy loss is a common objective function used in classification tasks, including predicting the next token in a sequence. In this context, it measures how well the model's predicted probabilities match the true labels.

The cross-entropy loss \(L\) for one sample can be calculated as:
\[ L = -\sum_{i} y_i \log(p_i) \]
where:
- \(y_i\) is the ground truth (1 if the token at position \(i\) matches, 0 otherwise),
- \(p_i\) is the predicted probability that the model assigns to the true token.

The goal during training is to minimize this loss.

:p How do you calculate cross-entropy loss for a single sample?
??x
To calculate the cross-entropy loss for one sample:
\[ L = -\sum_{i} y_i \log(p_i) \]
where \(y_i\) indicates whether the true token matches the predicted probability \(p_i\).

For example, if you have four tokens and their ground truth labels are [1, 0, 0, 0] (assuming the first is correct), and your model predicts probabilities as [0.8, 0.1, 0.1, 0.0], then:
\[ L = -(1 \cdot \log(0.8) + 0 \cdot \log(0.1) + 0 \cdot \log(0.1) + 0 \cdot \log(0.0)) \]
\(L\) can be calculated as the negative log of the probability of the true token.

```python
import numpy as np

def cross_entropy_loss(y_true, y_pred):
    # Ensure numerical stability by clipping values
    y_pred = np.clip(y_pred, 1e-7, 1 - 1e-7)
    loss = -np.sum(y_true * np.log(y_pred))
    return loss

# Example usage:
y_true = np.array([1, 0, 0, 0])
y_pred = np.array([0.8, 0.1, 0.1, 0.0])
loss = cross_entropy_loss(y_true, y_pred)
print(f"Cross-Entropy Loss: {loss}")
```
x??

---

#### Tokenization and Indexing of Music Pieces
Tokenizing music involves converting musical events into discrete tokens that can be processed by a Transformer model. This is similar to tokenizing text in natural language processing (NLP).

:p What does tokenization involve when working with music pieces?
??x
Tokenization in the context of music involves breaking down a piece of music into individual events, such as notes and their attributes (e.g., pitch, duration), which are then converted into tokens. Each unique event is assigned an index.

For example:
- A note C4 for 0.5 seconds could be tokenized to "C4_0.5".

```python
def tokenize_midi(file_path):
    # This function would parse the MIDI file and convert it into a sequence of events.
    # Each unique event (e.g., "C4_0.5") is assigned an index.
    pass

# Example usage:
tokenized_sequence = tokenize_midi("/path/to/midi/file.mid")
print(f"Tokenized Sequence: {tokenized_sequence}")
```
x??

---

#### Training the Music Transformer
The training process for a music Transformer involves several steps, including tokenizing and indexing music pieces, preparing batches of input-output pairs, and adjusting model parameters to minimize loss.

:p What are the key steps in training a music Transformer?
??x
Key steps in training a music Transformer include:
1. **Tokenization**: Convert musical events into discrete tokens.
2. **Indexing**: Assign each unique event an index.
3. **Preprocessing Data**: Transform sequences of notes and their attributes into fixed-length input and output pairs.
4. **Batching**: Group pairs of input-output sequences into batches for training.
5. **Model Training**: Use these batches to train the model by adjusting parameters to minimize cross-entropy loss.

For example, you would start with a sequence like:
\[ \text{"C4_0.5", "D4_0.25", "E4_1.0"} \]
and transform it into an input-output pair for training.

```python
def prepare_data(file_paths):
    # This function reads and tokenizes the MIDI files, then prepares batches of inputs (x) and outputs (y).
    pass

# Example usage:
input_sequences, output_sequences = prepare_data(["/path/to/midi/file1.mid", "/path/to/midi/file2.mid"])
print(f"Input Sequences: {input_sequences}")
print(f"Output Sequences: {output_sequences}")
```
x??

---

#### MAESTRO Dataset and Training Data Preparation
The MAESTRO dataset provides piano performances that need to be tokenized, indexed, and split into train, validation, and test subsets. The dataset is structured with subfolders containing MIDI files.

:p What steps are involved in preparing training data from the MAESTRO dataset?
??x
Steps involved in preparing training data from the MAESTRO dataset include:
1. **Download Dataset**: Obtain the MAESTRO dataset from Googleâ€™s Magenta group.
2. **Unzip and Organize Files**: Extract the dataset files into a specific directory structure.
3. **Tokenization and Indexing**: Convert MIDI files to tokenized sequences.
4. **Split Data**: Divide data into training, validation, and test subsets.

For example:
1. Download and extract MAESTRO: 
   ```bash
   wget https://storage.googleapis.com/magentadata/datasets/maestro/v2.0.0/maestro-v2.0.0-midi.zip
   unzip maestro-v2.0.0-midi.zip
   ```

2. Organize files:
   ```python
   os.makedirs("files/maestro-v2.0.0/train", exist_ok=True)
   os.makedirs("files/maestro-v2.0.0/val", exist_ok=True)
   os.makedirs("files/maestro-v2.0.0/test", exist_ok=True)
   ```

3. Tokenize and index:
   ```python
   import utils.processor as processor

   def process_midi_files(directory):
       for file in os.listdir(directory):
           if file.endswith(".mid"):
               tokenized_sequence = processor.tokenize_midi(os.path.join(directory, file))
               # Save or use the tokenized sequence as needed.

   process_midi_files("files/maestro-v2.0.0/")
   ```

4. Split data:
   ```python
   train_files = [f for f in os.listdir("files/maestro-v2.0.0/") if "train" in f]
   val_files = [f for f in os.listdir("files/maestro-v2.0.0/") if "val" in f]
   test_files = [f for f in os.listdir("files/maestro-v2.0.0/") if "test" in f]

   # Further processing to split data into train, validation, and test subsets.
   ```

x??

---

#### Downloading and Organizing Maestro Dataset
Background context: The provided text discusses downloading the `maestro-v2.0.0.json` file from a GitHub repository to organize MIDI files into training, validation, and test subsets for use with a music Transformer model.

:p What is the purpose of using the `maestro-v2.0.0.json` file in organizing MIDI datasets?
??x
The `maestro-v2.0.0.json` file serves as an index to categorize each MIDI file into one of three subsets: train, validation, and test. This helps in splitting the dataset for training, validating, and testing purposes.

```python
import json

# Load JSON file containing metadata about Maestro dataset
with open("files/maestro-v2.0.0/maestro-v2.0.0.json", "r") as fb:
    maestro_json = json.load(fb)

# Example of how to iterate over the loaded data and prepare files
for x in maestro_json:
    mid = rf'files/maestro-v2.0.0/{x["midi_filename"]}'
    split_type = x["split"]
    
    # Prepare file name based on split type
    f_name = mid.split("/")[-1] + ".pickle"
    
    if(split_type == "train"):
        o_file = rf'files/maestro-v2.0.0/train/{f_name}'
    elif(split_type == "validation"):
        o_file = rf'files/maestro-v2.0.0/val/{f_name}'
    elif(split_type == "test"):
        o_file = rf'files/maestro-v2.0.0/test/{f_name}'
    
    # Encode MIDI file and save it to the appropriate folder
    prepped = encode_midi(mid)
    with open(o_file, "wb") as f:
        pickle.dump(prepped, f)
```
x??

---

#### Splitting Train, Validation, and Test Subsets
Background context: The code snippet shown in the text processes each MIDI file from the `maestro-v2.0.0.json` file, categorizing them into train, validation, or test subsets based on their metadata.

:p How can you verify the number of files in each subset after running the provided script?
??x
To verify the number of files in each subset (train, validation, and test), you can use Python's `os.listdir` function to count the files in each directory.

```python
import os

# Check the number of files in train set
train_size = len(os.listdir('files/maestro-v2.0.0/train'))
print(f"There are {train_size} files in the train set")

# Check the number of files in validation set
val_size = len(os.listdir('files/maestro-v2.0.0/val'))
print(f"There are {val_size} files in the validation set")

# Check the number of files in test set
test_size = len(os.listdir('files/maestro-v2.0.0/test'))
print(f"There are {test_size} files in the test set")
```
x??

---

#### Converting MIDI Files to Sequence of Music Notes
Background context: The code snippet demonstrates how to convert a MIDI file into a sequence of musical notes and then further tokenize these notes into discrete events for easier processing.

:p How does the provided code handle the conversion from a MIDI file to a sequence of music notes?
??x
The code converts a MIDI file into a series of musical notes by iterating through each instrument's notes and control changes. It uses helper functions to preprocess and process the notes, then converts these notes into events for easier handling.

```python
import pickle
from utils.processor import encode_midi
import pretty_midi

# Example of converting a MIDI file to a sequence of music notes
file = 'MIDI-Unprocessed_Chamber1_MID--AUDIO_07_R3_2018_wav--2'
name = rf'files/maestro-v2.0.0/2018/{file}.midi'

# Load the MIDI file using pretty_midi library
song = pretty_midi.PrettyMIDI(name)
events = []
notes = []

for inst in song.instruments:
    # Process notes and control changes for each instrument
    inst_notes = inst.notes
    ctrls = _control_preprocess([ctrl for ctrl in 
                                 inst.control_changes if ctrl.number == 64])
    
    notes += _note_preprocess(ctrls, inst_notes)
    dnotes = _divide_note(notes)
    dnotes.sort(key=lambda x: x.time)

# Example of the output
for i in range(5):
    print(dnotes[i])
```
x??

---

#### Tokenizing Events into Discrete Tokens
Background context: The code snippet further tokenizes the musical notes into discrete events to reduce complexity and make training more feasible.

:p How does the provided code convert musical notes into discrete tokens?
??x
The code converts each `SNote` (a note event) into a series of discrete events. It handles time shifts, velocities, and note activations by iterating through the sorted list of `SNote` objects and generating appropriate events based on their times and velocities.

```python
cur_time = 0
cur_vel = 0

for snote in dnotes:
    # Generate time shift events
    events += _make_time_sift_events(prev_time=cur_time,
                                     post_time=snote.time)
    
    # Convert note to event
    events += _snote2events(snote=snote, prev_vel=cur_vel)
    
    cur_time = snote.time
    cur_vel = snote.velocity

# Example of the output
indexes = [e.to_int() for e in events]
for i in range(15):
    print(events[i])
```
x??

#### Tokenizing Music Pieces
Tokenization involves converting music pieces into a sequence of unique events. These events include `note_on`, `note_off`, `time_shift`, and `velocity` types, with values ranging from 0 to 387. The total number of unique tokens is 388.
:p What are the four main event types used in tokenizing music pieces?
??x
The four main event types used in tokenizing music pieces are:
- `note_on`
- `note_off`
- `time_shift`
- `velocity`

These events represent different aspects of musical notes and timing. For example, a `note_on` event includes the note value (e.g., 74) and velocity (e.g., 17), while a `time_shift` indicates a time interval between events.
x??

---

#### Creating Training Data
Training data preparation involves converting music pieces into sequences of indexes for training a model. The function `create_xys()` is used to transform these sequences into `(x, y)` pairs suitable for sequence prediction tasks.
:p How does the `create_xys()` function prepare the training data?
??x
The `create_xys()` function prepares the training data by converting music pieces into `(x, y)` pairs. It reads each file in the specified folder, converts the music piece into a `LongTensor`, and then creates input (`x`) and output (`y`) sequences of 2048 indexes.

Here is how it works:
1. Initialize two full tensors of size (max_seq,), filled with index 389.
2. If the length of the music piece is less than or equal to `max_seq`, pad the sequence with index 389 and set the last element of `y` as 388 to signal the end of the sequence.
3. If the length exceeds `max_seq`, use only the first `max_seq` elements for `x` and the next `max_seq+1` elements for `y`.

```python
def create_xys(folder):
    files = [os.path.join(folder, f) for f in os.listdir(folder)]
    xys = []
    max_seq = 2048
    
    for f in files:
        with open(f, "rb") as fb:
            music = pickle.load(fb)
        music = torch.LongTensor(music)

        x = torch.full((max_seq,), 389, dtype=torch.long)
        y = torch.full((max_seq,), 389, dtype=torch.long)

        length = len(music)
        
        if length <= max_seq:
            print(length)
            x[:length] = music
            y[:length - 1] = music[1:]
            y[length - 1] = 388
        else:
            x = music[:max_seq]
            y = music[1:max_seq + 1]

        xys.append((x, y))
    return xys
```

This function ensures that both `x` and `y` sequences are of the same length (2048) for training.
x??

---

#### Preparing Training Data: Train Subset
The train subset of music pieces is processed to create `(x, y)` pairs suitable for training a model. The maximum sequence length (`max_seq`) is set to 2048.
:p How many music pieces in the train subset are shorter than or equal to 2048 indexes?
??x
Out of the 967 music pieces in the train subset, only 5 are shorter than or equal to 2048 indexes. These lengths are printed as part of the output.
x??

---

#### Preparing Training Data: Validation and Test Subsets
Validation and test subsets are also processed similarly using the `create_xys()` function.
:p How many music pieces in the validation subset exceed 2048 indexes?
??x
All music pieces in the validation subset exceed 2048 indexes. The output shows that there is no such piece, indicating they all are longer than 2048 indexes.
x??

---

#### Preparing Training Data: Example Output
An example of processing a file from the validation subset is shown to understand how `create_xys()` works with shorter sequences.
:p What does the shape and content of `val1` indicate?
??x
The output shows that the first music piece in the validation set (`val1`) has a length of 5, meaning it consists of only 5 indexes. The values printed are the actual index values from the sequence.

For example:
```python
val1, _ = val[0]
print(val1.shape)
print(val1)

# Output might look like this:
torch.Size([5])
tensor([389, 389, 389, 389, 389])
```

This indicates that `val1` is a tensor of shape (5,) filled with the padding index 389 because it was shorter than the maximum sequence length.
x??

#### Hyperparameters in the Music Transformer
Background context explaining the hyperparameters and their significance. The `Config()` class stores these hyperparameters for the music Transformer.

:p What are the hyperparameters defined in the Config() class, and what do they represent?

??x
The hyperparameters defined in the `Config()` class include:
- `n_layer`: Number of decoder layers (6).
- `n_head`: Number of parallel heads for causal self-attention (8).
- `n_embd`: Embedding dimension (512).
- `vocab_size`: Size of the vocabulary (390), including tokens for end-of-sequence and padding.
- `block_size`: Maximum length of input sequences (2,048).

These hyperparameters are crucial as they define the architecture and capacity of the music Transformer. The number of layers and heads influences the model's ability to capture complex patterns, while the embedding dimension affects the granularity of token representations.

```python
class Config():
    def __init__(self):
        self.n_layer = 6
        self.n_head = 8
        self.n_embd = 512
        self.vocab_size = 390
        self.block_size = 2048 
        self.embd_pdrop = 0.1
        self.resid_pdrop = 0.1
        self.attn_pdrop = 0.1
```
x??

---

#### Music Transformer Model Architecture
Background context explaining the model architecture, including feed-forward networks and causal self-attention mechanisms.

:p What is the structure of the music Transformer model defined in `ch14util.py`?

??x
The music Transformer model is structured as follows:
1. **Embedding Layers**: 
   - Word embedding (`wte`) for converting token IDs to embeddings.
   - Positional encoding (`wpe`) for adding positional information.

2. **Dropout Layer**: 
   - Applies dropout with a rate of 0.1 on the input embeddings and attention outputs.

3. **Decoder Blocks**: 
   - Each block consists of:
     - **Layer Normalization (LN_1)**: Standardizes the inputs to each layer.
     - **Residual Connection**: Adds the original input to the output after passing through the decoder block.
     - **Causal Self-Attention**: Computes attention over the sequence, ensuring no future information is used during prediction.

4. **Output Layer**:
   - Linear head that maps the final hidden state to a vocabulary size (390).

The model stack six such blocks on top of each other to form the main body. The input is a sequence of token IDs corresponding to musical events, which are passed through these layers to generate logits for the next token.

```python
class Model(nn.Module):
    def __init__(self, config: Config):
        super().__init__()
        self.transformer = nn.ModuleDict(dict(
            wte=nn.Embedding(config.vocab_size, config.n_embd),
            wpe=nn.Embedding(config.block_size, config.n_embd),
            drop=nn.Dropout(config.embd_pdrop),
            h=nn.Sequential(*[Block(config) for _ in range(config.n_layer)])
        ))
        
    def forward(self, x):
        # Input: Sequence of token IDs
        # Output: Logits for next token probabilities
```
x??

---

#### Causal Self-Attention Mechanism
Background context explaining the self-attention mechanism and its application to sequence modeling.

:p How does the causal self-attention mechanism work in the music Transformer?

??x
The causal self-attention mechanism in the music Transformer works as follows:
1. **Masking**: 
   - Ensures that when attending to a token at position `i`, only tokens from positions less than `i` can be attended to, preventing information leakage from the future.

2. **Query-Key-Value Computation**:
   - Each token is represented by three vectors: Query (`Q`), Key (`K`), and Value (`V`). These are computed using weight matrices.
   
3. **Attention Scores**:
   - The attention score for each pair of tokens is calculated as the dot product between `Q` and `K`, scaled appropriately, followed by a softmax to normalize these scores.

4. **Contextualized Representation**:
   - The final representation for each token is computed as a weighted sum of the values (`V`) using the attention scores.

```python
class CausalSelfAttention(nn.Module):
    def __init__(self, config: Config):
        super().__init__()
        n_embd = config.n_embd
        d_head = n_embd // config.n_head
        self.c_attn = nn.Linear(n_embd, 3 * n_embd)
        self.c_proj = nn.Linear(n_embd, n_embd)
        self.attn_dropout = nn.Dropout(config.attn_pdrop)
        self.resid_dropout = nn.Dropout(config.resid_pdrop)
        self.n_head = config.n_head
        self.n_embd = n_embd

    def forward(self, x):
        B, T, C = x.size()  # batch size, sequence length, embedding dimensionality (n_embd)

        q, k, v = self.c_attn(x).split(C, dim=2)
        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)
        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, nh, T, hs)

        v = v.view(B, T, self.n_head, C // self.n_head)  # (B, nh, T, hs)
        
        att = q @ k.transpose(-2, -1) * (C**-0.5)  # (B, nh, T, T)
        att = F.softmax(att, dim=-1)
        att = self.attn_dropout(att)

        y = att @ v  # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)
        y = y.transpose(1, 2).contiguous().view(B, T, C)  # re-assemble all head outputs side by side

        y = self.resid_dropout(self.c_proj(y))
```
x??

---

#### Building a Music Transformer Model
Background context explaining the process of constructing and training the model. The code outlines how to instantiate the `Model` class.

:p How is the music Transformer model instantiated and initialized?

??x
The music Transformer model is instantiated using the following steps:

1. **Import the Required Modules**:
   - Import the `Model` class from `utils.ch14util`.

2. **Initialize Configuration Object**:
   - Create an instance of the `Config()` class to store hyperparameters.

3. **Set Device for Training**:
   - Determine whether to use GPU or CPU based on availability.

4. **Instantiate Model and Move to Device**:
   - Instantiate the `Model` with the configuration object.
   - Move the model to the appropriate device (GPU/CPU).

5. **Count Parameters**:
   - Calculate the number of parameters in the model.

```python
from utils.ch14util import Model

config = Config()
device = "cuda" if torch.cuda.is_available() else "cpu"

model = Model(config)
model.to(device)

num = sum(p.numel() for p in model.transformer.parameters())
print(f"number of parameters: {num / 1e6:.2f}M")
print(model)
```

The output will provide the number of parameters and a summary of the model structure, indicating that it consists of embedding layers, decoder blocks, and other components.

```python
Model(
  (transformer): ModuleDict(
    (wte): Embedding(390, 512) 
    (wpe): Embedding(2048, 512)
    (drop): Dropout(p=0.1, inplace=False)
    (h): ModuleList(
      (0-5): 6 x Block(
        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
  )
)
```
x??

---

